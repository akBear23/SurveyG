File: /media/aiserver/New Volume/HDD_linux/bear/SurveyX/paper_data/KNOWLEDGE_GRAPH_EMBEDDING/core_papers/f42d060fb530a11daecd90695211c01a5c264f8d.pdf
Created: 2025-09-20T07:49:02.527369
Keywords: Continual Knowledge Graph Embedding (CKGE), Evolving Knowledge Graphs, Catastrophic forgetting, IncDE framework, Graph-structure-aware learning, Hierarchical ordering, Incremental distillation, Importance-aware knowledge preservation, Two-stage training, Node centrality, Breadth-First Search (BFS), Novel CKGE datasets, State-of-the-art performance
==================================================
INTRIGUING ABSTRACT:
==================================================
The dynamic nature of real-world Knowledge Graphs (KGs) poses a significant challenge for traditional Knowledge Graph Embedding (KGE) methods, which face prohibitive retraining costs and catastrophic forgetting when new knowledge emerges. Existing Continual KGE (CKGE) approaches largely overlook the explicit graph structure, leading to inefficient learning of new knowledge and ineffective preservation of old.

We introduce IncDE (Incremental Distillation for Continual KGE), a novel framework that fundamentally rethinks CKGE by explicitly leveraging the graph's inherent topology. IncDE pioneers a **hierarchical ordering** strategy, learning new triples in a semantically coherent sequence based on their structural proximity and entity importance. Complementing this, an **importance-aware incremental distillation** mechanism selectively preserves old entity representations layer-by-layer, dynamically weighting distillation loss based on graph centrality measures. Coupled with a robust two-stage training paradigm, IncDE effectively mitigates catastrophic forgetting and optimizes knowledge integration.

Extensive experiments on both established and newly proposed, more realistic datasets demonstrate IncDE's superior performance, consistently outperforming state-of-the-art baselines. Our work represents a critical advancement, offering a robust and scalable solution for managing evolving KGs and paving the way for more adaptive and intelligent knowledge systems.

==================================================
FULL SUMMARY:
==================================================
Here's a focused summary of the paper "Towards Continual Knowledge Graph Embedding via Incremental Distillation" by \cite{liu2024} for a literature review:

*   **Research Problem & Motivation**
    *   **Specific Technical Problem**: Traditional Knowledge Graph Embedding (KGE) methods struggle with evolving Knowledge Graphs (KGs), requiring costly retraining of the entire KG when new knowledge emerges. The Continual KGE (CKGE) task aims to efficiently learn emerging knowledge while preserving old knowledge.
    *   **Importance and Challenge**: Real-world KGs constantly evolve (e.g., DBpedia's growth). Retraining is computationally expensive and impractical. Existing CKGE methods largely ignore the explicit graph structure, leading to two main drawbacks: (1) learning new triples in a random order, which destroys the inherent semantics and structure of new KGs, and (2) preserving old triples with equal priority, which is ineffective against catastrophic forgetting.

*   **Related Work & Positioning**
    *   **Relation to Existing Approaches**: Existing CKGE methods fall into three categories: dynamic architecture-based, memory replay-based, and regularization-based.
    *   **Limitations of Previous Solutions**: These methods overlook the importance of learning new knowledge in an appropriate order for graph data. They also fail to effectively preserve *appropriate* old knowledge for better integration with new knowledge, often treating all old knowledge equally. Furthermore, many existing CKGE datasets restrict new triples to contain at least one old entity, which does not reflect real-world KG evolution where entirely new subgraphs can emerge.

*   **Technical Approach & Innovation**
    *   **Core Technical Method**: \cite{liu2024} proposes IncDE (Incremental Distillation for Continual KGE), a novel framework that explicitly leverages the graph structure for both learning new knowledge and preserving old knowledge.
    *   **Novelty/Difference**:
        *   **Hierarchical Ordering**: Introduces a strategy to optimize the learning sequence of new triples. This involves:
            *   **Inter-hierarchical ordering**: Divides new triples into layers based on their distance from the old KG using Breadth-First Search (BFS), prioritizing triples closer to existing knowledge.
            *   **Intra-hierarchical ordering**: Within each layer, triples are further sorted based on the importance of their entities and relations (measured by node centrality and betweenness centrality), ensuring critical structural elements are learned first.
        *   **Incremental Distillation Mechanism**: Devises a layer-by-layer distillation process for entity representations. If an entity appears in a previous layer, its representation is distilled from the nearest prior layer. This mechanism is importance-aware, dynamically weighting distillation loss based on the graph structure features (node and betweenness centrality) of entities.
        *   **Two-Stage Training Paradigm**: Employs a two-stage training strategy to prevent the over-corruption of old knowledge by under-trained new knowledge. In the first stage, old entity/relation embeddings are fixed while new ones are trained; in the second stage, all embeddings are fine-tuned.

*   **Key Technical Contributions**
    *   **Novel Algorithms/Methods**: Hierarchical ordering (inter- and intra-hierarchical) for structured learning of new knowledge. Importance-aware incremental distillation for effective and selective preservation of old knowledge.
    *   **System Design/Architectural Innovations**: A comprehensive framework (IncDE) that integrates graph-structure-aware ordering, distillation, and a two-stage training strategy for CKGE.
    *   **Theoretical Insights/Analysis**: The paper implicitly highlights the importance of graph topology (connectivity, centrality) in continual learning for KGs, moving beyond generic continual learning strategies.

*   **Experimental Validation**
    *   **Experiments Conducted**: Extensive experiments were performed on both existing datasets (FB15k-237, WN18RR) and three new datasets (ENTITY, RELATION, FACT, HYBRID) constructed by \cite{liu2024} to better simulate real-world KG evolution with varying scales of new knowledge. Ablation studies were conducted to evaluate the contribution of individual components.
    *   **Key Performance Metrics and Comparison Results**: Performance was evaluated using standard KGE metrics such as Mean Reciprocal Rank (MRR), Hits@1, Hits@3, and Hits@10. IncDE consistently outperformed state-of-the-art baselines. Notably, the incremental distillation mechanism alone contributed to significant improvements of 0.2%-6.5% in the MRR score. Further exploratory experiments confirmed IncDE's ability to proficiently learn new knowledge while effectively preserving old knowledge across all time steps.

*   **Limitations & Scope**
    *   **Technical Limitations/Assumptions**: The base KGE model used is TransE; while effective, the generalizability to more complex or recent KGE models (e.g., GNN-based) is not explicitly detailed. The computational cost of pre-calculating hierarchical ordering (BFS, centrality measures) is acknowledged but stated to be mitigated by pre-computation.
    *   **Scope of Applicability**: The method is designed for continual learning in evolving KGs, particularly those with emerging entities and relations, and is applicable to domains requiring dynamic KG updates (e.g., bio-medical, financial).

*   **Technical Significance**
    *   **Advancement of State-of-the-Art**: \cite{liu2024} significantly advances the technical state-of-the-art in CKGE by being the first to explicitly leverage the explicit graph structure for both learning new knowledge and preserving old knowledge, addressing critical shortcomings of previous methods.
    *   **Potential Impact**: The proposed methods (hierarchical ordering, importance-aware incremental distillation, two-stage training) offer robust solutions for managing evolving KGs, which is crucial for real-world applications. The introduction of new, more realistic datasets also provides valuable resources for future research in continual KGE.