\section{Introduction}
Knowledge Graphs (KGs) have emerged as a powerful and intuitive paradigm for representing structured knowledge, capturing real-world entities and their intricate relationships in a machine-readable format \cite{buehler2024, mathur2024}. Organized as collections of factual triples, typically in the form of (head entity, relation, tail entity), KGs provide a rich semantic foundation that underpins various artificial intelligence (AI) applications. From organizing vast scientific literature to enabling sophisticated reasoning in multi-agent systems, KGs offer a structured yet flexible framework for knowledge representation \cite{rubaiat2025, ghafarollahi2024}. However, the symbolic nature of KGs presents a fundamental challenge for computational models that operate in continuous vector spaces. This necessitates the transformation of symbolic knowledge into dense, low-dimensional vector embeddings, a process known as Knowledge Graph Embedding (KGE). KGE aims to represent entities and relations as vectors or matrices, such that the structural and semantic properties of the KG are preserved, thereby facilitating computational reasoning and generalization. This review delves into the foundational concepts of KGE, its critical role in advancing AI capabilities, and the diverse landscape of models and applications that have emerged in this rapidly evolving field.

\subsection{Background: Knowledge Graphs and Their Significance}
Knowledge Graphs serve as a cornerstone for intelligent systems by providing a structured representation of information, moving beyond unstructured text to explicitly define entities and their relationships \cite{buehler2024}. This structured format, typically comprising triples $(h, r, t)$, where $h$ is the head entity, $r$ is the relation, and $t$ is the tail entity, allows for clear semantic interpretation and facilitates automated reasoning. The significance of KGs spans numerous domains, from general-purpose knowledge bases like Freebase and WordNet to specialized applications in scientific discovery and materials engineering. For instance, in materials science, KGs can integrate diverse data sources, such as experimental results, simulation data, and theoretical models, to accelerate the design and discovery of novel materials \cite{ghafarollahi2024, ghafarollahi2025}. Similarly, in domains like agriculture, KGs can model complex relationships between crops, environmental factors, and suitable populations, as demonstrated by the TeaPle dataset for tea varieties \cite{li2025}. The explicit encoding of relationships within KGs enables systems to perform complex queries, infer new facts, and understand contextual information, which is crucial for developing robust and interpretable AI solutions. The ability to integrate and reason over heterogeneous information makes KGs indispensable for tasks requiring a deep understanding of domain-specific knowledge, thereby bridging the gap between human-understandable facts and machine-processable data structures \cite{mathur2024, rubaiat2025}.

\subsection{The Role of Knowledge Graph Embedding}
Despite their power, the symbolic nature of KGs poses a challenge for many machine learning algorithms that operate on continuous numerical data. Knowledge Graph Embedding (KGE) addresses this by projecting entities and relations into a continuous, low-dimensional vector space, allowing for efficient computation and generalization \cite{wang2014}. The fundamental problem in KGE is to learn representations (embeddings) that accurately capture the semantic and structural properties of the KG, such that plausible triples are assigned higher scores than implausible ones. Early KGE models, such as TransE, conceptualized relations as translation vectors in the entity embedding space, aiming for $h + r \approx t$ \cite{wang2014}. However, these models struggled with complex relation patterns like one-to-many, many-to-one, and reflexive relations. This limitation led to the development of more sophisticated models like TransH, which projects entities onto relation-specific hyperplanes before translation, thereby allowing entities to have different representations depending on the relation \cite{wang2014}. Further advancements introduced relation-adaptive translation functions, such as RatE, which utilize weighted products in complex space to enhance modeling capacity and explicitly alleviate embedding ambiguity in one-to-many relations \cite{huang2020}.

The importance of KGE extends to a wide array of AI tasks:
\begin{itemize}
    \item \textbf{Link Prediction:} This is a core application of KGE, aiming to predict missing entities or relations in incomplete KGs. Models like GNN-FTuckER combine Graph Neural Networks (GNNs) with tensor decomposition to capture rich semantic and structural information for tasks like identifying suitable populations for tea varieties \cite{li2025}. Similarly, in Temporal Knowledge Graphs (TKGs), models like TCompoundE and RotateQVS leverage compound geometric operations or quaternion vector spaces to capture complex temporal evolution patterns and predict missing facts over time \cite{ying2024, chen2022, sadeghian2021, chen2023, dileo2023}. The ability of KGE models to extrapolate to unseen data, driven by semantic evidence, further underscores their utility in completing dynamic and evolving knowledge bases \cite{li2021}.
    \item \textbf{Question Answering (QA):} KGE facilitates QA systems by enabling the retrieval and reasoning over relevant facts. By embedding knowledge, systems can efficiently match natural language queries to KG facts, even supporting logical query answering with fuzzy logic \cite{chen2021}.
    \item \textbf{Recommendation Systems:} KGE enhances recommendation systems by modeling user-item interactions and item attributes within a KG, allowing for more personalized and context-aware recommendations.
    \item \textbf{Scientific Discovery:} KGE plays a crucial role in accelerating scientific discovery by providing structured representations for complex scientific data. For example, KGE can be integrated into multi-agent AI systems for autonomous materials discovery, enabling rapid alloy design and property prediction by learning from atomistic simulations \cite{ghafarollahi2024, ghafarollahi2025}.
\end{itemize}
The ongoing development of KGE models, including those addressing temporal dynamics \cite{cai2024, cai2024} and continual learning challenges \cite{zhu2025, li2025, liu2024, sun2025}, highlights the field's commitment to creating robust and adaptable knowledge representation techniques.

\subsection{Scope and Organization of the Review}
This review aims to provide a comprehensive overview of Knowledge Graph Embedding, focusing on its foundational principles, diverse model architectures, and critical applications. We will synthesize information from recent advancements, critically evaluating the strengths and limitations of various approaches. The primary focus is on how KGE models represent entities and relations, their ability to capture complex patterns, and their performance in downstream AI tasks. We will explore different taxonomies of KGE models, including translation-based, semantic matching, and geometric models, as well as emerging directions such as temporal and continual KGE.

The remainder of this review is structured as follows: Section 2 will delve into the foundational KGE models, categorizing them by their underlying mechanisms and discussing their evolution. Section 3 will focus on advanced KGE architectures, including those leveraging neural networks and graph convolutional networks, and their capacity to capture richer semantic and structural information. Section 4 will explore specialized KGE paradigms, such as Temporal KGE and Continual KGE, addressing the challenges of dynamic and evolving knowledge graphs. Section 5 will discuss the critical applications of KGE across various AI domains, providing specific examples and performance benchmarks. Finally, Section 6 will offer a critical analysis of current challenges, limitations, and promising future research directions in the field of Knowledge Graph Embedding.