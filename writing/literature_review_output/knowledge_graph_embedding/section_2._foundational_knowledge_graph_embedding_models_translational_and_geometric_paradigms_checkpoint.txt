\section{Foundational Knowledge Graph Embedding Models: Translational and Geometric Paradigms}
The initial wave of Knowledge Graph Embedding (KGE) research laid the groundwork for representing symbolic knowledge in continuous vector spaces, primarily through the translational paradigm. These foundational models, exemplified by TransE, conceptualized relations as simple translation vectors, aiming to satisfy the geometric principle $h + r \approx t$ for a valid triple $(h, r, t)$. While elegant and efficient, these early approaches faced inherent limitations, particularly in handling complex relation patterns and managing embedding regularization. This section delves into the evolution of these foundational models, tracing their initial refinements within traditional Euclidean or vector spaces and highlighting pioneering efforts to transcend these boundaries by exploring non-Euclidean geometric structures. The journey from basic translation to sophisticated geometric transformations underscores a continuous pursuit of enhanced model capacity, improved expressiveness for diverse relation types, and more robust embedding regularization. This exploration forms the bedrock for understanding more advanced KGE techniques, demonstrating how fundamental challenges in knowledge representation were systematically addressed through innovative mathematical and geometric interpretations.

\noindent\textbf{Taxonomy Summary:} This section primarily covers models within the "Translational Models and Geometric Extensions" group. Specifically, it examines "Subgroup 1.1: Enhancing Relation Modeling within Euclidean Space" by discussing models like TransH and RatE, and "Subgroup 1.2: Exploring Non-Euclidean Embedding Spaces" through the lens of TorusE. The development direction highlighted here moves from refining core translation mechanics to exploring alternative embedding geometries.

\subsection{Core Translational Models and Their Initial Refinements}
The translational paradigm, pioneered by TransE, established a simple yet powerful principle for Knowledge Graph Embedding: the embedding of a head entity plus the embedding of a relation should approximately equal the embedding of the tail entity ($h + r \approx t$). This approach gained significant traction due to its computational efficiency and intuitive interpretability. However, TransE struggled with certain fundamental limitations. A major challenge was its inability to adequately model complex relation mapping properties, such as one-to-many, many-to-one, and many-to-many relations. In such scenarios, TransE's rigid translation often forced entities involved in these complex mappings to occupy similar points in the embedding space, leading to a loss of discriminative power. For instance, if a head entity has multiple tail entities via the same relation, TransE might struggle to differentiate between these tails.

Beyond the scoring function, the training process itself presented avenues for refinement. \cite{nayyeri2019} critically investigated TransE's limitations, arguing that the choice of the **loss function** is as crucial as the score function in encoding complex relation patterns. Their theoretical and empirical work demonstrated that a proper selection of the loss function could significantly mitigate TransE's shortcomings, particularly concerning many-to-many and symmetric relations. This insight highlighted that optimizing the learning objective, rather than solely redesigning the embedding operations, offers a powerful path for refining core translational models. This represents a crucial conceptual shift within the "core mechanics" phase of KGE development, emphasizing that the training objective is as critical as the scoring function in improving translation-based models, thereby opening a new avenue for research and optimization.

\subsection{Enhancing Relation Modeling within Euclidean Space}
To overcome the limitations of basic translational models in handling complex relation patterns, subsequent research focused on enriching the representation of relations within Euclidean or complex vector spaces. These efforts aimed to introduce more flexibility and capacity into the translation mechanism without sacrificing the inherent efficiency.

A significant step in this direction was **TransH** \cite{wang2014}, which addresses TransE's inability to efficiently model complex relation mapping properties (one-to-many, many-to-one, many-to-many). Instead of representing a relation as a simple vector, TransH models each relation $r$ as a **hyperplane** defined by a normal vector $w_r$, along with a translation vector $d_r$ that lies on this hyperplane. For a given triple $(h, r, t)$, both the head entity $h$ and tail entity $t$ are first projected onto the relation-specific hyperplane. The translation operation then occurs between these projected entities. This mechanism allows an entity to have different representations (projections) when involved in different relations, thereby effectively differentiating entities in complex mappings. \cite{wang2014} demonstrated that TransH delivers significant improvements in predictive accuracy over TransE on benchmark datasets like WordNet and Freebase, while maintaining comparable scalability and efficiency. This marked a crucial trade-off between model capacity and computational cost, falling under "Subgroup 1.1: Enhancing Relation Modeling within Euclidean Space."

Further advancements in this vein include **RatE (Relation-Adaptive Translating Embedding)** \cite{huang2020}. RatE operates in complex vector space and aims to enhance the expressive power of translational models by introducing a novel element-wise *weighted product* in its translation function. Unlike the rigid complex number multiplication in models like RotatE, RatE employs a learnable, relation-specific weight matrix $W(r)$ for each relation. This adaptive weighting allows for more flexible transformations, significantly improving the model's capacity to capture intricate relational semantics. Crucially, RatE explicitly addresses the problem of embedding ambiguity, particularly prevalent in one-to-many relations, by dynamically adjusting distances between tail entities. This prevents distinct entities from being assigned overly similar embeddings, a common pitfall in earlier models. \cite{huang2020} showed that RatE achieves state-of-the-art performance across multiple benchmark datasets, demonstrating its effectiveness in balancing enhanced modeling capacity with computational efficiency.

\subsection{Exploring Non-Euclidean Embedding Spaces}
While refinements within Euclidean and complex vector spaces significantly improved translational models, a fundamental limitation persisted: the need for explicit regularization. Models like TransE often forced entity embeddings onto a unit sphere to prevent divergence during training, a practice that could inadvertently warp the embedding space and hinder the model's ability to accurately capture the translation principle. This challenge motivated a pioneering shift towards exploring non-Euclidean geometric structures for KGE, aiming for spaces that intrinsically offer desirable properties like compactness.

**TorusE: Knowledge Graph Embedding on a Lie Group** \cite{ebisu2017} represents a groundbreaking effort in this direction. The paper identifies the regularization problem in TransE, where forcing embeddings onto a sphere warps the space and adversely affects link prediction accuracy. To address this, TorusE proposes embedding entities and relations not in a real or complex vector space, but on a **compact Lie group**, specifically a **torus**. The core insight is that the translation principle of TransE can be naturally defined on any Lie group. By choosing a compact Lie group like a torus, the inherent compactness of the space naturally regularizes the embeddings, preventing them from diverging without the need for explicit regularization. This avoids the adverse effects of regularization on embedding accuracy and interpretability. \cite{ebisu2017} highlights TorusE as the first model to embed objects on a space other than a real or complex vector space, marking a significant conceptual leap. Experiments demonstrated that TorusE outperforms state-of-the-art approaches, including TransE, DistMult, and ComplEx, on standard link prediction tasks, while also being scalable and faster than the original TransE. This innovation falls under "Subgroup 1.2: Exploring Non-Euclidean Embedding Spaces" and opened a new research direction for KGE, encouraging the exploration of other geometric spaces (e.g., hyperbolic spaces) for knowledge representation. The success of TorusE underscored the potential of leveraging advanced mathematical structures to fundamentally address challenges in KGE, moving beyond mere architectural tweaks to reconsider the very nature of the embedding space.