\section{Scalability and Efficiency for Large-Scale Knowledge Graphs}
The practical application of Knowledge Graph Embedding (KGE) models to real-world knowledge graphs (KGs) is often hampered by significant scalability and efficiency challenges. As KGs grow to encompass billions of entities and relations, the parameter storage cost and computational burden associated with traditional KGE methods become prohibitive. This section addresses these critical issues by reviewing advanced techniques designed to reduce model complexity and enhance operational efficiency. We explore parameter-efficient representation learning methods that mitigate the linear growth of embedding parameters and delve into strategies for managing large-scale, dynamic KGs, including those that implicitly support efficient deployment through continual learning. These advancements are crucial for enabling KGE models to move beyond benchmark datasets and into massive, evolving industrial and scientific applications, aligning with the broader challenges of Group 2: Addressing Scalability and Temporal Dynamics in KGEs, particularly Subgroup 2.1: Parameter-Efficient Representation Learning.

\subsection{Parameter-Efficient Representation Learning}
Traditional Knowledge Graph Embedding (KGE) models typically assign a unique vector embedding to every entity and relation in the graph. While effective for capturing semantic information, this approach leads to a parameter count that scales linearly with the number of entities and relations. For massive, real-world knowledge graphs, this results in exorbitant memory requirements and computational costs, hindering training and deployment. Parameter-efficient representation learning addresses this by devising methods that reduce the number of learnable parameters without sacrificing expressive power.

A significant stride in this direction is the **Entity-Agnostic Representation Learning (EARL)** method \cite{chen2023}. EARL tackles the parameter storage problem by fundamentally altering how entity embeddings are generated. Instead of learning and storing embeddings for all entities, EARL maintains embeddings for only a small, pre-defined set of "reserved entities." For all other entities, their representations are dynamically generated by universal, entity-agnostic encoders that process their contextual information, including connected relations, k-nearest reserved entities, and multi-hop neighbors. This innovative approach allows EARL to achieve a static and significantly lower parameter count, largely independent of the total number of entities in the knowledge graph. Experimental results demonstrate that EARL not only uses fewer parameters but also achieves superior performance on link prediction tasks, highlighting its effectiveness in balancing efficiency and accuracy \cite{chen2023}. This method directly falls under Subgroup 2.1: Parameter-Efficient Representation Learning, offering a novel paradigm for resource-constrained KGE.

Further advancements in parameter efficiency, particularly for dynamic and continually evolving knowledge graphs, are seen in techniques like **Incremental LoRA (Low-Rank Adaptation)** \cite{liu2024} and **Efficient Task-driven Tokens (ETT-CKGE)** \cite{zhu2025}. LoRA-based methods, originally popular in large language models, adapt a pre-trained model to new tasks or data by injecting small, low-rank matrices into existing layers, significantly reducing the number of trainable parameters compared to full fine-tuning. Applying this to KGEs, as in \cite{liu2024}, enables fast and continual learning by incrementally adapting to new knowledge without catastrophic forgetting, which is crucial for large-scale KGs that are constantly updated. Similarly, ETT-CKGE \cite{zhu2025} proposes using efficient task-driven tokens for continual KGE, suggesting a parameter-efficient mechanism to represent and update knowledge in a dynamic setting. These methods represent a shift towards adaptive and modular parameterization, ensuring that KGE models remain viable and efficient even as knowledge graphs expand and evolve.

\subsection{Strategies for Large-Scale Deployment and Dynamic KGs}
Deploying KGE models on truly massive knowledge graphs necessitates strategies that go beyond mere parameter reduction, encompassing efficient training, inference, and adaptation to dynamic changes. While traditional approaches often involve parallel computing and graph partitioning to distribute the computational load and memory footprint across multiple machines, recent research also focuses on continual learning paradigms to maintain efficiency for evolving large-scale KGs. These methods are critical for real-world applications where KGs are constantly updated with new facts, entities, and relations.

For static, large graphs, graph partitioning techniques divide the graph into smaller, manageable subgraphs that can be processed in parallel, either on a single machine with multiple GPUs or across a distributed cluster. This reduces the memory footprint per processing unit and allows for concurrent computation. While not explicitly detailed in the provided summaries, the underlying need for such strategies is evident in the context of large-scale systems like those described by \cite{ghafarollahi2024} and \cite{buehler2024}, which leverage GNNs and LLMs for scientific discovery over vast knowledge bases. These systems implicitly rely on efficient graph processing to handle the immense data volumes and complex interactions involved in tasks like alloy design or materials analysis. The "rapid predictive model" and "overcoming the computational bottleneck" mentioned in \cite{ghafarollahi2024} underscore the imperative for efficient underlying graph operations.

For dynamic knowledge graphs, which are prevalent in real-world scenarios, continual learning approaches are paramount for maintaining efficiency and scalability. Instead of retraining KGE models from scratch with every update, continual learning allows models to incrementally learn new information while retaining previously acquired knowledge, thereby saving significant computational resources. Methods like **Incremental Distillation** \cite{liu2024} and **Bayesian-Guided Continual KGE** \cite{li2025} are designed to address the challenge of catastrophic forgetting, a common issue in continual learning where new knowledge overwrites old. Incremental distillation, for instance, leverages knowledge transfer from an old model to a new one, ensuring that the model remains up-to-date without full retraining. Similarly, **Scale-Aware Gradual Evolution (SAGE)** \cite{li2025} and **A Generative Adaptive Replay Continual Learning Model** \cite{zhang2025} offer mechanisms for KGE models to adapt to growing and changing KGs efficiently. These continual learning strategies, by enabling efficient updates and reducing redundant computations, are indispensable for the scalable and sustainable deployment of KGE models in large-scale, dynamic environments, ensuring that the models remain relevant and performant over time.