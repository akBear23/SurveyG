\section{Addressing Dynamic Knowledge Graphs: Temporal and Continual KGE}
Knowledge graphs (KGs) are not static repositories of facts but rather dynamic entities that evolve continuously over time, with new entities, relations, and facts emerging, and existing ones changing or becoming obsolete. This inherent dynamism poses significant challenges for traditional Knowledge Graph Embedding (KGE) models, which are primarily designed for static graphs and assume a fixed set of entities and relations. Such static models struggle to capture the non-stationarity and temporal dependencies of real-world events, leading to outdated representations and poor performance on evolving data. Furthermore, retraining these models from scratch on continuously growing KGs is computationally prohibitive and prone to catastrophic forgetting, where newly learned information overwrites previously acquired knowledge.

Addressing these critical limitations, the field has seen the emergence of two pivotal research directions: Temporal Knowledge Graph Embedding (TKGE) and Continual Knowledge Graph Embedding (CKGE). TKGE methods explicitly model the temporal dimension of facts, capturing time-varying relationships and ensuring temporal consistency. They aim to represent entities, relations, and timestamps in a continuous vector space such that temporal facts $(h, r, t, \tau)$ (head, relation, tail, timestamp) can be accurately predicted and reasoned upon. In contrast, CKGE approaches focus on incrementally learning new knowledge from evolving KGs without forgetting old information, a phenomenon known as catastrophic forgetting. This involves developing strategies for efficient model updates that adapt to new facts while preserving the ability to reason over historical knowledge. This section delves into the methodologies, advancements, and challenges within these dynamic KGE paradigms, highlighting how they enable models to adapt to evolving knowledge, maintain robust performance over time, and overcome the limitations of static embedding approaches. We will critically analyze the design choices, underlying mechanisms, and comparative advantages of various TKGE and CKGE models, emphasizing their contributions to building more adaptive and intelligent knowledge systems.

\subsection{Temporal KGE: Modeling Time-Varying Facts}
Temporal Knowledge Graph Embedding (TKGE) aims to represent entities, relations, and timestamps in a continuous vector space such that temporal facts $(h, r, t, \tau)$ can be accurately predicted and reasoned upon. The core challenge lies in capturing the non-stationarity, heterogeneity, and complex temporal dependencies inherent in real-world events. Early KGE models largely ignored the temporal dimension, treating all facts as timeless. However, the recognition that facts are often time-sensitive and that relationships evolve has led to the development of sophisticated models that explicitly incorporate time into their embedding mechanisms. These models primarily achieve this through two main strategies: leveraging geometric transformations to model temporal evolution and enforcing temporal consistency and sensitivity in the learned embeddings. The goal is not just to predict facts, but to understand *when* and *how* relations hold or change, which is crucial for dynamic reasoning tasks like event forecasting and temporal question answering.

\subsubsection{Rotation-Based and Advanced Geometric Transformations}
A prominent and increasingly sophisticated approach in TKGE involves leveraging geometric transformations to model how entities and relations evolve over time. The fundamental idea is that temporal changes can be represented as transformations in the embedding space, allowing for a continuous and differentiable way to capture dynamics.

A foundational work in this area is \textbf{ChronoR} by \cite{sadeghian2021}. ChronoR introduced a novel model that employs a $k$-dimensional rotation transformation, uniquely parametrized by both relation and time. For a given fact $(h, r, t, \tau)$, ChronoR transforms the head entity's embedding using this specific rotation to align it with its tail entity's embedding. The model's scoring function is based on the inner product (cosine of the angle) between the transformed head and tail entities, which \cite{sadeghian2021} theoretically demonstrated to be a generalization of previously used complex-domain scoring functions (e.g., ComplEx's $Re(a \cdot b)$). This high-dimensional rotation, which relaxes the unit norm constraint to include scaling, effectively captures rich interactions between the temporal and multi-relational characteristics of a TKG. Furthermore, ChronoR introduced a novel regularization method inspired by tensor nuclear norms and a 4-norm temporal smoothness objective to encourage similar transformations for closer timestamps, enhancing temporal consistency. ChronoR demonstrated superior performance in temporal link prediction, particularly for facts within observed timestamps, highlighting the efficacy of rotation for temporal modeling.

Building upon the success of rotation-based models, subsequent research has explored more expressive algebraic spaces. \cite{chen2022} proposed \textbf{RotateQVS}, which extends the rotation concept by representing temporal entities as rotations in a Quaternion Vector Space. Relations are modeled as complex vectors in Hamilton's quaternion space, allowing for a more nuanced representation of relation patterns, including symmetry, asymmetry, inverse, and crucially, temporal evolution. The use of quaternions, which naturally encode 3D rotations and offer a richer mathematical structure than complex numbers, provides a more expressive framework for capturing intricate temporal dynamics. This approach not only improves modeling capacity but also offers improved interpretability compared to simpler rotation matrices by directly mapping relational transformations to quaternion operations.

Further advancing this, \cite{ying2024} introduced \textbf{TCompoundE}, a TKGE model that leverages compound geometric operations. Unlike ChronoR or RotateQVS which primarily rely on a single type of geometric operation (rotation), TCompoundE integrates *both* time-specific translation and scaling operations *within* relation-specific operations. Specifically, it applies time-specific translation ($T_\tau$) and scaling ($S_\tau$) to the relation-specific scaling component ($S_{\hat{r}}$), while keeping the relation-specific translation ($T_{\hat{r}}$) time-invariant. This allows TCompoundE to capture both dynamic (time-varying) and static (time-invariant) aspects of relations over time. The model's ability to combine multiple operations in a structured manner enables it to model a wider variety of relation patterns, including symmetric, asymmetric, inverse, and complex temporal evolution patterns, which single-operation models often struggle with \cite{ying2024}. TCompoundE's superior performance across benchmark datasets underscores the benefit of moving beyond monolithic transformations to a more modular and expressive compound approach.

The progression from ChronoR's k-dimensional rotations to RotateQVS's quaternion rotations and TCompoundE's compound operations illustrates a clear development direction: increasingly sophisticated geometric and algebraic transformations are being employed to model the continuous and dynamic nature of time and relations. While rotation-based methods excel at capturing cyclic or evolving patterns, compound operations offer greater flexibility by combining different types of transformations, potentially at the cost of increased model complexity and hyperparameter tuning. A critical challenge remains in determining the optimal geometric space and combination of operations for diverse temporal patterns, as well as ensuring the interpretability of these complex transformations.

\subsubsection{Temporal Consistency and Sensitivity}
Beyond the core embedding mechanisms, another critical aspect of TKGE is ensuring the temporal consistency of embeddings and capturing the varying importance of time for different facts. Facts do not always change abruptly; often, their evolution is smooth, and the relevance of a timestamp can differ significantly across relations.

\cite{dileo2023} addressed the challenge of temporal consistency by focusing on \textbf{temporal smoothness regularizers} for neural link predictors. This work systematically analyzed various regularization techniques, including those using linear functions and recurrent architectures, that enforce similar transformations for adjacent timestamps. By applying these regularizers to existing tensor factorization models like TNTComplEx, they demonstrated that explicitly encouraging embeddings to evolve smoothly over time significantly enhances model performance. This highlights that a continuous, gradual change in entity and relation representations, rather than abrupt shifts, often better reflects real-world temporal dynamics and improves predictive accuracy. The effectiveness of these regularizers suggests that incorporating prior knowledge about temporal continuity can be as crucial as the choice of the embedding function itself.

Complementing the focus on consistency, \cite{cai2024} introduced a model that operates within a complex space to address the crucial problem of \textbf{time sensitivity}. This model captures semantic characteristics with temporal sensitivity through transformation and an attention mechanism in its real part, while its imaginary part learns connections between fact elements without predefined weights. This approach allows the model to dynamically weigh the relevance of temporal information for specific facts, moving beyond uniform temporal modeling to context-aware temporal understanding. For instance, some relations might be highly sensitive to the exact timestamp (e.g., "was elected president on"), while others might be more enduring (e.g., "is a part of"). By employing an attention mechanism, the model can adaptively determine how much temporal information to incorporate, thereby improving the precision of temporal reasoning.

The comprehensive survey by \cite{cai2024} further consolidates these developments, defining TKGs, reviewing datasets and evaluation metrics, and proposing a taxonomy based on core technologies. It synthesizes diverse approaches, including those focusing on temporal consistency and sensitivity, and outlines future research directions, serving as a foundational resource for understanding the landscape and guiding further development in this rapidly evolving field. While temporal smoothness regularizers offer a global constraint on embedding evolution, time-sensitive models provide a more granular, context-dependent mechanism for leveraging temporal information. The trade-off often lies between the computational overhead of dynamic attention mechanisms and the generalizability of simpler regularization techniques. Future research will likely explore hybrid approaches that combine both global consistency and local sensitivity for more robust temporal modeling.

\subsection{Continual KGE: Incremental Learning and Catastrophic Forgetting Mitigation}
Continual Knowledge Graph Embedding (CKGE) addresses the critical challenge of incrementally learning new knowledge from evolving KGs while simultaneously preventing catastrophic forgetting of previously learned information. As KGs grow and change, retraining KGE models from scratch on the entire dataset becomes computationally prohibitive and inefficient. CKGE methods are designed to efficiently update embeddings by adapting to new facts without losing the ability to reason over old ones, a critical requirement for real-world, continuously updated knowledge bases like Wikipedia or biomedical databases. The core problem is to achieve a balance between *plasticity* (the ability to learn new information) and *stability* (the ability to retain old information), often referred to as the stability-plasticity dilemma. This section explores strategies that leverage knowledge distillation, parameter-efficient techniques, and adaptive learning to navigate this challenge.

\subsubsection{Distillation and Graph Structure Awareness}
A key strategy for mitigating catastrophic forgetting in CKGE is knowledge distillation, where a "teacher" model (representing old knowledge) guides the learning of a "student" model (learning new knowledge). However, generic distillation methods often overlook the explicit graph structure, which is crucial for efficient learning and preservation in KGs.

\cite{liu2024} proposed \textbf{Incremental Distillation (IncDE)}, a competitive method for CKGE that explicitly considers the full use of the explicit graph structure in KGs, a factor often overlooked by previous methods. IncDE introduces a hierarchical strategy to optimize the learning order of new triples based on graph structure features. This strategy involves two levels of ordering:
\begin{enumerate}
    \item \textbf{Inter-hierarchical Ordering}: New triples are divided into multiple layers using Breadth-First Search (BFS) expansion from the existing old graph. This prioritizes learning triples that are structurally closer to the already known knowledge.
    \item \textbf{Intra-hierarchical Ordering}: Within each layer, triples are further sorted based on the importance of their entities and relations, measured by node centrality and betweenness centrality. This ensures that critical structural elements are learned first, preserving the inherent semantics and structure of the new KGs.
\end{enumerate}
Crucially, IncDE devises a novel incremental distillation mechanism that facilitates the seamless transfer of entity representations from the previous layer to the next. This mechanism is importance-aware, dynamically weighting distillation loss based on the graph structure features (node and betweenness centrality) of entities. This ensures that more critical entities receive higher preservation priority, addressing the limitation of previous methods that preserved old triples with equal priority. The approach also employs a two-stage training paradigm to prevent the corruption of old knowledge by under-trained new knowledge: first, only new entity/relation representations are trained while old ones are fixed; then, all embeddings are fine-tuned. Experimental results demonstrate that IncDE consistently outperforms state-of-the-art baselines, with the distillation mechanism contributing significantly to performance improvements. This approach showcases how integrating graph structure awareness with sophisticated distillation techniques can effectively address the challenges of continual learning in dynamic KGs, offering a more principled way to balance plasticity and stability.

\subsubsection{Parameter-Efficient and Adaptive Learning Strategies}
The continuous growth of KGs also exacerbates the problem of parameter storage costs, as traditional KGE methods assign unique embeddings to every entity and relation, leading to a linear increase in parameters with KG size. To address this, parameter-efficient and adaptive learning strategies are crucial for CKGE models to dynamically adjust to new knowledge without prohibitive resource demands.

\cite{chen2023} introduced \textbf{Entity-Agnostic Representation Learning (EARL)}, a novel paradigm where embeddings are learned only for a small set of "reserved entities." For all other entities, their embeddings are dynamically generated by universal, entity-agnostic encoders that transform their distinguishable contextual information (from connected relations, k-nearest reserved entities, and multi-hop neighbors). This approach allows EARL to maintain a static, efficient, and significantly lower parameter count, independent of the total number of entities in the KG, while still achieving state-of-the-art performance on link prediction tasks. By decoupling entity count from parameter count, EARL offers a scalable solution for ever-growing KGs.

Beyond parameter efficiency, adaptive learning strategies are crucial for CKGE models to dynamically adjust to new knowledge. \cite{liu2024} further explored parameter-efficient continual learning with \textbf{Fast and Continual Knowledge Graph Embedding via Incremental LoRA}. This method leverages Low-Rank Adaptation (LoRA) to incrementally learn new knowledge by updating only a small number of parameters, significantly reducing computational overhead and memory footprint while preserving old knowledge. LoRA's effectiveness stems from its ability to inject trainable low-rank matrices into the model's layers, allowing for efficient adaptation without modifying the original pre-trained weights.

Similarly, \cite{li2025} proposed \textbf{Learning to Evolve: Bayesian-Guided Continual Knowledge Graph Embedding}, which uses a Bayesian approach to adaptively learn and update embeddings. This provides a principled way to quantify uncertainty and balance new and old information by maintaining posterior distributions over parameters, allowing for more robust and adaptive updates. Another adaptive strategy is presented by \cite{li2025} with \textbf{SAGE (Scale-Aware Gradual Evolution for Continual Knowledge Graph Embedding)}, which focuses on a scale-aware gradual evolution process to handle the varying impact of new knowledge. SAGE recognizes that new knowledge can have different "scales" of impact on the existing KG and adapts its learning rate and regularization accordingly. Furthermore, \cite{zhang2025} introduced a \textbf{Generative Adaptive Replay Continual Learning Model for Temporal Knowledge Graph Reasoning}, which uses generative models to replay past experiences. By synthesizing "pseudo-samples" from old knowledge, this approach prevents catastrophic forgetting in a more adaptive and data-efficient manner, reducing the need to store large amounts of old data.

These parameter-efficient and adaptive learning strategies represent critical advancements towards building scalable, robust, and intelligent KGE systems that can continuously learn and evolve with dynamic knowledge. While EARL offers a paradigm shift in parameter management, LoRA provides an efficient fine-tuning mechanism. Bayesian methods, scale-aware evolution, and generative replay offer different facets of adaptive learning, each with its own trade-offs between computational complexity, memory usage, and the effectiveness of forgetting mitigation. The challenge lies in combining these strategies to create truly general-purpose CKGE models that can handle diverse types of KG evolution with minimal resource expenditure.