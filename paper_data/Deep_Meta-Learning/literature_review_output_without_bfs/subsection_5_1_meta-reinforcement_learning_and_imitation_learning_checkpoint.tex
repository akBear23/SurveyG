\subsection{Meta-Reinforcement Learning and Imitation Learning}
Meta-Reinforcement Learning (Meta-RL) and Meta-Imitation Learning (Meta-IL) represent a critical evolution in equipping agents with the ability to rapidly adapt to novel tasks and environments. These paradigms directly confront the notorious challenges of sample inefficiency and limited generalization inherent in traditional reinforcement learning (RL) and imitation learning (IL) by enabling agents to "learn to learn" across a distribution of related tasks \cite{beck2023x24}. By acquiring a transferable skill or an efficient adaptation mechanism, meta-learning allows agents to quickly infer optimal behaviors, adapt to new reward functions, or acquire policies from minimal demonstrations, thereby accelerating learning and improving robustness in complex sequential decision-making scenarios.

The early trajectory of Meta-RL research explored implicit adaptation mechanisms, primarily leveraging Recurrent Neural Networks (RNNs). A seminal contribution by \cite{wang20167px} demonstrated that an LSTM, when trained across a diverse set of tasks with past actions, rewards, and observations as inputs, could implicitly learn an internal RL algorithm. This recurrent architecture effectively encoded task-specific information within its hidden state, allowing it to adapt its policy to new tasks without requiring explicit weight updates, a process termed "learning to reinforcement learn." While groundbreaking, this approach was initially demonstrated in simpler domains, raising questions about its scalability to complex, high-dimensional environments. Building on the concept of adaptive behaviors through architectural innovation, \cite{vecoven2018hc1} introduced Neuro-Modulated Networks (NMNs). These networks employed a neuromodulatory sub-network to dynamically tune the activation function parameters of a main policy network, offering a more scalable solution than simply increasing network depth or width. NMNs achieved faster and more stable adaptive behaviors compared to standard RNNs, though their performance could be sensitive to the choice of activation functions. Further pushing the boundaries of implicit algorithmic learning, \cite{xu2020txy} proposed FRODO, an algorithm that utilized meta-gradient descent to discover its own RL objective function online, parameterizing the update target with a neural network. This ambitious approach aimed to learn the very learning rule, but scaling such online objective learning to complex, real-world environments remains a significant practical challenge. These RNN-based methods, while powerful, often struggled with long-term credit assignment and the explicit representation of task uncertainty, paving the way for more explicit adaptation strategies \cite{finn2017vrt, sutton2022jss}.

A parallel and highly influential direction in meta-learning, particularly for rapid and explicit adaptation, emerged with gradient-based methods. Model-Agnostic Meta-Learning (MAML) \cite{finn20174c4} proved instrumental in this regard, extending its bi-level optimization framework to both Meta-RL and Meta-IL. For Meta-RL, MAML learns an initialization that can be quickly fine-tuned with a few gradient steps on a new task, significantly improving sample efficiency compared to learning from scratch. In the realm of Meta-Imitation Learning (Meta-IL), \cite{finn20174c4} pioneered one-shot visual imitation learning, enabling robots to acquire new skills from a single visual demonstration. This was a significant advancement, as it allowed end-to-end learning of visuomotor policies directly from raw pixel inputs, adapting via a few gradient updates. This approach addressed the critical data efficiency bottleneck in robotics, where collecting numerous demonstrations per task is often infeasible. However, a key challenge in imitation learning is the domain shift between human demonstrations and robot embodiments. \cite{yu2018nm7} addressed this by leveraging meta-learning to build prior knowledge for cross-domain transfer, facilitating more robust one-shot imitation. While gradient-based methods like MAML offer strong generalization capabilities, they can be computationally intensive and susceptible to meta-overfitting, where the meta-learner performs well on meta-training tasks but struggles with truly novel task distributions \cite{chen2021j5t}. To mitigate this, \cite{tseng2020m83} proposed regularizing meta-learning via gradient dropout, a simple yet effective method to alleviate overfitting during the inner-loop adaptation, thereby enhancing generalization to new tasks. More recent work by \cite{wang2024bhk} further investigates the underfitting/overfitting challenges in meta-learning, proposing a "Task Relation Learner" (TRLearner) to calibrate optimization by leveraging task similarities, which could be highly relevant for improving the robustness of gradient-based Meta-RL and Meta-IL.

To tackle increasingly complex and long-horizon tasks, meta-learning has also been integrated with hierarchical and skill-based approaches. These methods aim to decompose complex behaviors into reusable sub-skills, which can then be meta-learned and composed. \cite{yang2018p36} proposed a hierarchical deep reinforcement learning algorithm that simultaneously learned basic and compound skills, utilizing two levels of hierarchy with a meta critic overseeing basic skills. Building on this, \cite{xu2019brv} introduced Hierarchical Meta-Critic Networks for sample-efficient learning, providing transferable knowledge across tasks by sharing a global basic critic and a meta critic. This framework allowed for the distillation of meta-knowledge above the task level, enhancing adaptation. \cite{lan20196o7} further improved generalization by proposing Meta-RL with Task Embedding and Shared Policy, explicitly capturing shared information across tasks and meta-learning how to quickly abstract task-specific information. More recently, \cite{nam2022z75} devised a skill-based Meta-RL method that leverages prior experience extracted from offline datasets to learn reusable skills and meta-train a high-level policy. This enables efficient composition of learned skills into long-horizon behaviors, allowing for rapid adaptation to unseen target tasks with significantly fewer environment interactions.

The versatility of Meta-RL extends to various real-world applications, demonstrating its capacity for rapid adaptation in dynamic and resource-constrained settings. For instance, \cite{wang2020tae} developed a fast adaptive task offloading method in edge computing based on Meta-RL, which can adapt quickly to new environments with minimal gradient updates and samples. In robotics, \cite{visca20217nt} presented a deep meta-learning framework for energy-aware path planning for unmanned ground vehicles, allowing adaptation to unknown terrains. Furthermore, \cite{ma20243e9} proposed a Graph Convolutional Network based Multi-Objective Meta-Deep Q-Learning (GM2DQL) method for eco-routing, demonstrating rapid adaptation to dynamic traffic conditions. Beyond efficiency, Meta-RL is also being extended to safety-critical domains. \cite{khattar2024sr6} introduced a novel "CMDP-within-online" framework for Meta-Safe Reinforcement Learning, providing the first provable guarantees for task-averaged regret and constraint violations. This work is crucial for deploying RL agents in applications where both rapid adaptation and strict adherence to safety constraints are paramount. The integration of language instructions also shows promise, with \cite{bing2022xo7} presenting a meta-RL algorithm that utilizes language to shape task interpretation in robotic manipulation, offering a more intuitive way to specify new tasks.

In conclusion, the field of Meta-Reinforcement Learning and Imitation Learning has undergone a significant evolution, moving from early demonstrations of implicit algorithmic learning within recurrent networks to sophisticated frameworks that explicitly optimize for adaptability and leverage hierarchical structures. While substantial progress has been made in addressing sample inefficiency and generalization, future research will likely focus on improving robustness to out-of-distribution tasks in dynamic environments, bridging the sim-to-real gap more effectively, and scaling to even more diverse and open-ended task distributions. The integration of meta-learning with other advanced learning paradigms, particularly for safety and human-robot interaction, will be crucial for achieving truly autonomous and adaptable AI agents capable of operating effectively in complex, real-world scenarios.