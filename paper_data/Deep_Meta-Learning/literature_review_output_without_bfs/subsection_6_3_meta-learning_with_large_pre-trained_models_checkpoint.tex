\subsection{Meta-Learning with Large Pre-trained Models}
The advent of large pre-trained models, including Vision-Language Models (VLMs) and Large Language Models (LLMs), has fundamentally reshaped the landscape of few-shot learning, instigating a crucial paradigm shift in meta-learning. This subsection explores how meta-learning strategies are now predominantly employed to efficiently adapt, steer, or minimally tune these massive foundational models for novel tasks with limited data, moving beyond the traditional goal of learning optimal initial model weights.

Historically, meta-learning focused on learning generalizable initialization parameters or architectures that could quickly adapt to new tasks with a few gradient steps \cite{Finn_MAML_2017, Nichol_Reptile_2018}. Early surveys, such as \cite{huisman2020b7w}, noted an empirical correlation between larger network backbones and improved few-shot performance, implicitly hinting at the power of rich, pre-learned representations. This observation paved the way for integrating powerful pre-trained models into meta-learning frameworks. For instance, early work by \cite{holla20202od} demonstrated the efficacy of meta-learning with sparse experience replay for lifelong language learning, leveraging pre-trained BERT as a representation network to mitigate catastrophic forgetting. Similarly, \cite{li2023zn0} advanced few-shot text classification by proposing SEML, which enhances meta-learning with self-supervised information derived from unlabeled data, further enriching the feature representations learned by models like BERT. These initial integrations showcased meta-learning's ability to leverage pre-trained knowledge for specific adaptive challenges.

The true transformation, however, lies in the shift from fine-tuning entire models to efficiently interacting with or minimally tuning *frozen* foundational models. A seminal contribution in this area is "Learning to Prompt" (L2P) by \cite{Chen_L2P_2021}, which introduced a meta-learning approach where a meta-learner generates task-specific learnable prompts to guide a *frozen* Vision-Language Model for few-shot adaptation. This technique significantly reduces the number of parameters requiring fine-tuning, thereby minimizing computational cost and data requirements. Building upon this, \cite{wang2024dai} addressed a critical limitation of prompt tuning: overfitting to base classes and poor generalization to novel classes. They proposed "Learning to Learn Better Visual Prompts," which integrates a meta-learning-informed episodic training strategy (akin to MAML's inner-outer loop optimization) into prompt tuning. This enables the model to learn more generalizable prompt vectors that effectively transfer knowledge to unseen categories, demonstrating meta-learning's power in optimizing the *prompting strategy itself* for improved few-shot generalization. The practical impact of this paradigm is further exemplified by \cite{lupu20249p4}'s MAGICVFM, a stable adaptive controller for ground vehicles. This system integrates Visual Foundation Models (VFMs) and meta-learning to adapt only the last layer of a deep neural network based on VFM-derived visual features, showcasing efficient and robust adaptation of foundational models in safety-critical scenarios.

For Large Language Models (LLMs), meta-learning plays a crucial role in addressing their inherent data and computational demands, particularly for domain-specific adaptation \cite{lee2021jou}. While in-context learning (ICL) is an emergent capability of large transformers, exhibiting properties analogous to meta-learning by adapting to tasks from demonstrations without explicit weight updates, explicit meta-learning strategies are actively employed to enhance or steer this emergent behavior. \cite{Wang_Meta-Learning_2022} provides a comprehensive survey, highlighting how meta-learning underpins strategies like prompt-based learning, parameter-efficient fine-tuning (PEFT), and in-context learning to adapt these massive models with minimal data and computational overhead. This underscores a paradigm shift towards learning *how to interact with* or *efficiently tune* these powerful, pre-trained models rather than learning their initial weights from scratch. Furthermore, meta-learning with transformer-based models is being applied to real-world challenges like class incremental learning, where `\cite{kumar2024he9}` proposes a transformer-based aggregation function within a meta-learner to classify newly introduced classes without retraining, showcasing how meta-learning enables continuous adaptation for these large NLP models.

Beyond prompt tuning, meta-learning principles are being explored for other parameter-efficient fine-tuning (PEFT) techniques. For instance, meta-learning could be applied to optimize configurations for adapters (e.g., determining optimal LoRA ranks or placement) or to learn dynamic learning rate schedules for specific modules, further enhancing adaptation efficiency. The immense scale and complexity of foundational models also necessitate advancements in meta-optimization. \cite{ozkara2024nst} introduced Meta-Adaptive Optimizers (MADA), which meta-learn the most suitable optimizer dynamically during training. This approach is particularly beneficial for the complex optimization landscapes and high computational costs associated with fine-tuning large models, potentially leading to faster convergence or better generalization with fewer steps. Moreover, theoretical advancements, such as the analysis of optimal (even counter-intuitive negative) inner-loop learning rates in MAML for overparameterized models by \cite{bernacchia20211r0}, offer fundamental insights into the meta-optimization process. These insights are highly relevant for designing more robust and efficient meta-learning algorithms to adapt large pre-trained models, where overparameterization is the norm and optimal tuning strategies are critical for performance and computational efficiency.

The practical deployment of large foundation models, especially in sensitive domains, also highlights the critical role of meta-learning in distributed and privacy-preserving adaptation. The immense scale of these models, coupled with privacy concerns in real-world user data, makes centralized fine-tuning impractical. Federated meta-learning emerges as a critical enabling technology for privacy-preserving personalization, allowing large models to adapt to diverse client data without centralizing raw information. Examples include federated meta-learning frameworks for EV charging demand forecasting \cite{you2024xuq} and driver distraction detection \cite{liu2024jz5}, which enable collaborative learning across multiple clients while preserving data privacy, highly pertinent for deploying large models in sensitive, real-world environments.

In conclusion, meta-learning has undergone a significant evolution, transitioning from learning initial model parameters to developing sophisticated strategies for efficiently interacting with, steering, or minimally tuning large pre-trained models. This shift, driven by techniques like learning to generate optimal prompts and parameter-efficient fine-tuning, unlocks the immense potential of foundational models for rapid adaptation across a vast array of few-shot downstream applications. However, challenges remain in fully understanding the emergent properties of in-context learning, developing universally robust and parameter-efficient meta-learning strategies, scaling meta-training to encompass the full diversity of tasks that these increasingly capable foundational models can address, and advancing the theoretical understanding of meta-optimization in these overparameterized regimes.