\subsection{Differentiable Solvers and Hypernetworks for Parameter Adaptation}

The pursuit of highly flexible and rapid adaptation mechanisms in meta-learning has led to the exploration of advanced techniques that move beyond conventional gradient-based inner-loop optimization. This subsection delves into two prominent paradigms: leveraging differentiable closed-form solvers as base learners and employing hypernetworks to generate model parameters dynamically. These approaches enable meta-learners to optimize for rapid, interpretable adaptation tailored to specific problem structures, or to modulate entire network architectures based on task-specific information.

One significant direction involves integrating classical machine learning algorithms as differentiable components within a meta-learning framework. \textcite{bertinetto2018ur2} pioneered this by proposing a meta-learning approach where the base learner is a differentiable closed-form solver, such as Ridge Regression. Their method meta-learns both a deep feature extractor and the hyperparameters of the base solver end-to-end, efficiently backpropagating through the solver's closed-form solution (e.g., using the Woodbury identity for speed) to enable rapid adaptation in few-shot classification tasks. This allows for data-dependent adaptation at test time, offering more flexibility than similarity-based methods while being computationally more efficient than gradient-based meta-learning for a few adaptation steps.

Building upon the idea of meta-learning parameters for classical models, subsequent work has extended this to more complex scenarios. \textcite{wistuba2021wha} introduced Few-Shot Bayesian Optimization (FSBO) by meta-learning deep kernel Gaussian Processes (GPs) as surrogate models for hyperparameter optimization. Here, the deep kernel parameters are learned across a distribution of tasks, allowing the classical GP model to rapidly adapt its predictions to new tasks with very few evaluations. Further refining this, \textcite{chen2022z45} presented Adaptive Deep Kernel Fitting with Implicit Function Theorem (ADKF-IFT) for molecular property prediction. This framework uses bilevel optimization to meta-learn feature extractor parameters while adapting base kernel parameters per task, leveraging the Implicit Function Theorem for exact hypergradient computation, thereby achieving a robust balance between generalization and task-specific adaptation for deep kernel GPs. In a related vein, \textcite{lee2024snq} proposed Sequential Bayesian Meta-Continual Learning (SB-MCL), a biologically inspired framework that decouples deep representation learning from sequential knowledge integration. Here, neural networks are meta-learned to map complex data to a latent space suitable for simple statistical models, which then perform exact sequential Bayesian updates, effectively offloading continual learning to robust, forgetting-immune classical models.

Complementing differentiable solvers, hypernetworks offer another powerful mechanism for flexible parameter adaptation. Instead of optimizing the parameters of a base learner, hypernetworks are neural networks that generate the weights or parameters of another 'main' network, conditioned on task-specific information. This provides a mechanism for dynamic architecture generation or flexible parameter modulation without explicit gradient-based inner loops. \textcite{przewiezlikowski2022d4y} exemplify this with HyperMAML, which replaces MAML's gradient-based inner-loop adaptation with a trainable hypernetwork. The hypernetwork directly outputs weight updates for the base model based on support set embeddings and labels, enabling more substantial and flexible weight modifications in a single step, thereby offering a computationally efficient alternative to classical MAML.

Beyond these direct applications, the broader theme of learning flexible adaptation mechanisms also encompasses biologically inspired approaches. \textcite{lindsey202075a} explored Feedback and Local Plasticity (FLP), a meta-learning framework that learns biologically plausible local synaptic update rules and decoupled feedback weights for deep credit assignment. While not strictly a differentiable solver or hypernetwork, FLP meta-learns parameters (feedback weights, plasticity coefficients) that modulate the learning process itself, demonstrating how meta-learning can discover effective, local learning rules that excel in challenging scenarios like continual learning, often outperforming gradient-based meta-learners.

In summary, differentiable solvers and hypernetworks represent significant advancements in meta-learning, offering powerful alternatives to traditional gradient-based adaptation. Differentiable solvers provide interpretable and efficient adaptation by optimizing classical models, while hypernetworks enable dynamic parameter generation for flexible architectural and functional modulation. Biologically inspired approaches further expand this landscape by learning the very rules of adaptation. Despite these strides, challenges remain in scaling complex differentiable solvers to arbitrary models, ensuring the interpretability of hypernetwork-generated parameters, and fully understanding the theoretical underpinnings of learned, non-gradient-based adaptation rules. Future research may explore hybrid models that combine these strengths, or delve deeper into the biological plausibility and robustness of such learned adaptation mechanisms.