\subsection{Probabilistic Meta-Learning for Task Inference and Exploration}

A critical challenge in meta-reinforcement learning (meta-RL) is the efficient adaptation to novel tasks, particularly in environments characterized by inherent uncertainty. Advanced probabilistic meta-learning frameworks explicitly address this by modeling task uncertainty, enabling more efficient exploration and the development of Bayes-adaptive policies. These policies condition actions not just on the current state, but also on the agent's evolving belief about the underlying task, leading to more robust and uncertainty-aware adaptation.

A foundational contribution in this area is VariBAD (Variational Bayes-Adaptive Deep RL) by \cite{zintgraf2019zat}. VariBAD meta-learns an approximate Bayes-adaptive policy by jointly training a Variational Auto-Encoder (VAE) for posterior inference over latent MDP embeddings and a policy conditioned on this belief. This approach allows the agent to perform principled online exploration by continuously updating its belief about the task as it interacts with the environment, demonstrating superior exploratory behavior compared to methods like posterior sampling in tasks such as Gridworld navigation \cite{zintgraf2019zat}. However, VariBAD's reliance on on-policy experience during meta-training limited its sample efficiency, a common bottleneck in deep RL.

To address the sample inefficiency of on-policy meta-RL, \cite{rakelly2019m09} introduced PEARL (Probabilistic Embeddings for Actor-Critic RL). PEARL is an off-policy meta-RL algorithm that leverages probabilistic context variables to encode task-specific information, conditioning the policy on this latent variable. A key innovation is its permutation-invariant encoder for task inference, which processes past experience to estimate the posterior over context variables, enabling significantly improved meta-training sample efficiency (20-100X) and structured exploration through posterior sampling. By decoupling the data used for policy training from that for encoder training, PEARL effectively integrates probabilistic task inference with off-policy actor-critic methods, achieving higher asymptotic performance on continuous control benchmarks \cite{rakelly2019m09}.

The utility of probabilistic context variables extends beyond standard meta-RL to related problems like Inverse Reinforcement Learning (IRL). \cite{yu2019o41} proposed PEMIRL (Probabilistic Embeddings for Meta-Inverse Reinforcement Learning), which adapts the probabilistic context variable paradigm to infer reward functions from few, unstructured, and heterogeneous demonstrations. PEMIRL integrates a deep latent variable model with maximum entropy IRL, utilizing mutual information regularization between the probabilistic context variable and trajectories to ensure the learned reward function effectively uses the inferred context. This enables few-shot reward inference for new tasks without requiring explicit task groupings or labels, a significant step towards more practical IRL applications \cite{yu2019o41}.

Further pushing the boundaries of meta-learning under realistic constraints, \cite{dorfman2020mgv} tackled the critical problem of Offline Meta-Reinforcement Learning with BOReL (Bayesian Offline Reinforcement Learning). BOReL is an off-policy VariBAD variant designed to learn exploration strategies from static, pre-collected datasets, rather than requiring active online data collection. This work formalizes the concept of "MDP ambiguity," highlighting the inherent limitations of data identifiability when inferring task beliefs solely from offline data, and proposes strategies to mitigate it. BOReL demonstrates that effective meta-exploration can be learned from offline data, outperforming online baselines in some sparse reward tasks, which is crucial for applications where online interaction is costly or unsafe \cite{dorfman2020mgv}.

The principles of Bayes-adaptive meta-learning continue to inspire advancements in specific applications and challenging environments. For instance, \cite{zintgraf2021hoc} extended deep interactive Bayesian RL via meta-learning, enabling agents to learn about and adapt to other agents' unknown strategies in multi-agent settings by meta-learning approximate belief inference and Bayes-optimal behavior. Similarly, \cite{bing2022om0} addressed meta-RL in non-stationary and dynamic environments by introducing a training strategy and task representation based on Gaussian mixture models, achieving zero-shot adaptation and competitive performance in changing conditions. More recently, \cite{wang2024d09} proposed CBAMRL (Contrastive Learning-Based Bayes-Adaptive Meta-Reinforcement Learning) for active pantograph control in high-speed railways, employing a Bayes-adaptive strategy for zero-shot adaptation and a contrastive learning-based contextual encoder to represent complex task distributions, demonstrating rapid adaptation to unknown perturbations.

In conclusion, probabilistic meta-learning frameworks have significantly advanced the field by providing principled ways to model and leverage task uncertainty. From foundational methods like VariBAD to off-policy improvements in PEARL, extensions to Meta-IRL with PEMIRL, and the crucial offline learning capabilities of BOReL, these approaches enable more efficient exploration and robust adaptation in complex, partially observable environments. Despite these advancements, challenges remain in fully addressing MDP ambiguity in diverse offline datasets, scaling to extremely broad task distributions, and integrating these sophisticated probabilistic models with real-time, safety-critical applications while maintaining theoretical guarantees.