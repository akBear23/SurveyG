\subsection{Biologically Inspired Mechanisms for Adaptive Learning}

The quest for artificial intelligence systems that exhibit the efficiency, robustness, and flexibility characteristic of biological learning has led to a growing interest in biologically inspired meta-learning. This subsection explores approaches that draw directly from the brain's adaptive processes, aiming to overcome the fundamental biological implausibility of traditional deep learning paradigms, particularly backpropagation. Backpropagation, while highly effective, relies on non-local error signals, symmetric feedback weights, and precise derivative computations that are not observed in biological neural networks, leading to challenges in energy efficiency, online learning, and robustness in dynamic environments. By mimicking biological mechanisms such as neuromodulation, local synaptic plasticity, and sparse, event-driven computation, these methods seek to bridge the gap between artificial and natural intelligence, fostering more energy-efficient, robust, and flexible adaptive learning.

One prominent avenue of research focuses on **neuromodulatory control**, drawing inspiration from how neurochemicals like dopamine and serotonin dynamically alter neural processing in the brain. \cite{vecoven2018hc1} introduced Neuro-Modulated Networks (NMNs), where a dedicated "modulator" network learns to dynamically adjust the parameters of a "main" network's activation functions (e.g., slope and bias) based on the current task context. This mechanism allows the NMN to exhibit flexible, context-dependent responses, akin to how neuromodulators influence neural activity and learning in biological organisms. Crucially, the NMN architecture offers a scalable solution, as the number of modulated parameters scales linearly with the number of neurons, unlike some hypernetwork approaches where parameter generation can scale quadratically with connections. In meta-reinforcement learning tasks, NMNs demonstrated faster learning and more stable training compared to standard recurrent neural networks, highlighting their capacity for efficient adaptation to varying environmental parameters.

A second critical direction involves **meta-learning local synaptic plasticity rules**, directly addressing the biological implausibility of backpropagation's global credit assignment problem. Traditional backpropagation requires precise, non-local error signals and often symmetric feedback weights, which are biologically unrealistic. To circumvent this, \cite{lindsey202075a} proposed the "Feedback and Local Plasticity" (FLP) framework. In FLP, an outer meta-learning loop discovers local, Hebbian-like synaptic plasticity rules that update weights based solely on information available at the synapse (pre- and post-synaptic activity, and a local feedback signal). A key innovation is the use of *decoupled* feedback weights, which are distinct from the feedforward weights, thus avoiding the "weight transport problem" inherent in backpropagation. This decoupling, combined with local learning rules, significantly reduces the need for global communication and memory, contributing to potential energy efficiency. FLP demonstrated performance comparable to gradient-based meta-learners on few-shot classification and regression, but critically, it *significantly outperformed* them in continual learning scenarios, showcasing its robustness to catastrophic forgetting by enabling stable, online adaptation.

Further advancing the concept of meta-learned local plasticity, \cite{gu2019tvc} explored meta-learning a *parametric synaptic update rule* that is spatially localized to individual neurons. Their approach trains a meta-network to generate weight updates as a product of pre- and post-synaptic neuronal outputs, where the combination of inputs (bottom-up, top-down, recurrent) to each neuron for generating the learning signal is itself a meta-learned function. This framework directly addresses the challenge of discovering effective biologically plausible rules that can train deep networks, moving beyond hand-designed Hebbian rules that often struggle with complex tasks. By learning the structure of these local rules, \cite{gu2019tvc} demonstrated their ability to drive task-relevant learning for semi-supervised tasks, offering another compelling alternative to backpropagation's global update mechanisms.

Beyond continuous-valued networks, the field is also exploring **meta-learning in Spiking Neural Networks (SNNs)**, which are inherently more biologically realistic and energy-efficient due to their sparse, event-driven communication. SNNs process information through discrete spikes, mimicking the brain's communication style, and can operate at significantly lower power consumption compared to conventional ANNs. However, training deep SNNs effectively remains a challenge. \cite{schmidgall20238t4} introduced Meta-SpikePropamine, a bi-level optimization framework that meta-learns three-factor synaptic plasticity rules for SNNs. These three-factor rules, inspired by neuroscience, incorporate pre-synaptic activity, post-synaptic activity, and a global neuromodulatory signal (the "third factor") to modulate synaptic changes. By training SNNs with these meta-learned plasticity rules, Meta-SpikePropamine demonstrated the ability to solve challenging online learning problems, effectively bridging the gap between neuroscience-derived learning models and the performance capabilities of deep learning. This approach leverages the inherent energy efficiency of SNNs while developing sophisticated, biologically plausible learning mechanisms.

Collectively, these biologically inspired meta-learning approaches represent a significant paradigm shift. Neuromodulatory networks offer dynamic, context-dependent adaptation of neural properties, providing a flexible control mechanism. Meta-learned local plasticity rules (e.g., FLP, Gu et al.) directly tackle the credit assignment problem by discovering alternatives to backpropagation that are local, potentially more energy-efficient by reducing global communication, and robust for online and continual learning. Furthermore, integrating meta-learning with SNNs (e.g., Meta-SpikePropamine) pushes towards systems that are not only adaptive but also intrinsically more energy-efficient and biologically plausible in their computational substrate.

Despite these advancements, significant challenges remain. A critical question is the extent to which these meta-learned local rules and neuromodulatory strategies can generalize to entirely different network architectures or task distributions beyond their meta-training environment. The theoretical guarantees for robustness and generalization, particularly for complex, meta-learned plasticity rules, are often less developed than for gradient-based methods. Furthermore, while energy efficiency is a stated goal, rigorous comparative analyses of the actual energy consumption of these biologically inspired models versus conventional deep learning on large-scale tasks are still emerging. Future research will need to focus on scaling these approaches to more complex real-world scenarios, integrating multiple biological mechanisms (e.g., combining neuromodulation with local plasticity in SNNs), and developing robust theoretical frameworks to understand their generalization capabilities and computational trade-offs.