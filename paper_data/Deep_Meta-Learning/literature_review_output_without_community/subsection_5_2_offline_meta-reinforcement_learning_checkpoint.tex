\subsection{Offline Meta-Reinforcement Learning}

The paradigm of offline meta-reinforcement learning (meta-RL) represents a crucial advancement for scenarios where online interaction is either infeasible, too costly, or unsafe. This approach enables agents to learn to adapt to new tasks solely from static, pre-collected datasets without any further online interaction, thereby addressing a critical challenge for real-world RL deployment by leveraging existing data to train meta-learners that can then rapidly adapt to novel tasks.

While off-policy meta-RL methods, such as Probabilistic Embeddings for Actor-Critic RL (PEARL) \cite{rakelly2019m09}, significantly improved meta-training sample efficiency by decoupling data collection from policy updates, they still necessitated some online interaction for agents to adapt to truly novel tasks. This reliance on online experience for task inference or adaptation updates remained a bottleneck for many real-world applications. The critical challenge of entirely eliminating online interaction during meta-learning was directly addressed by \cite{dorfman2020mgv} with Bayes-Optimal Offline Reinforcement Learning (BOReL).

BOReL extends the probabilistic context variable framework to a fully offline setting, enabling agents to meta-learn adaptation strategies exclusively from static, pre-collected datasets \cite{dorfman2020mgv}. The core innovation of BOReL lies in its ability to train a meta-policy that can infer task-specific information and adapt its behavior solely by processing offline data, without any further online environment interaction. Technically, BOReL builds upon off-policy variants of algorithms like VariBAD, where a permutation-invariant belief encoder (often a VAE) processes historical context trajectories from the offline dataset. This encoder learns to approximate the posterior distribution over latent task variables, effectively augmenting each state in the offline trajectories with a neural belief estimate, transforming the data into a Bayes-Adaptive MDP (BAMDP) format. The meta-policy is then trained to be Bayes-optimal with respect to this inferred belief, ensuring robust adaptation to new tasks based purely on the static data. This method effectively bridges the gap between data-rich offline learning, where vast amounts of interaction data can be curated, and data-scarce online adaptation, where new tasks must be tackled with minimal or no new experience.

A significant challenge identified and formalized by BOReL is **MDP ambiguity** \cite{dorfman2020mgv}. This occurs when the offline dataset does not contain sufficient information to distinguish between different underlying MDPs (i.e., different tasks), making it impossible for the meta-learner to form an accurate belief about the current task. This problem is particularly acute when the offline data primarily consists of exploitative trajectories from conventional agents, lacking the diverse exploration needed to identify task specifics. BOReL proposes principled data collection strategies (e.g., ensuring identifying state-action pairs are visited) and a "reward relabeling trick" to mitigate ambiguity when it stems solely from reward function differences. This highlights a crucial limitation: the effectiveness of offline meta-RL is highly dependent on the quality and diversity of the multi-task offline dataset, especially regarding its coverage of task-identifying information.

Beyond inferring task context, offline meta-learning has also been explored for extracting reusable components. \cite{nam2022z75} propose a skill-based meta-RL method that leverages prior experience from offline datasets to enable meta-learning on long-horizon, sparse-reward tasks. Their approach involves (1) extracting reusable skills and a skill prior from offline datasets, (2) meta-training a high-level policy to efficiently compose these learned skills into long-horizon behaviors, and (3) rapidly adapting the meta-trained policy to solve unseen target tasks. This demonstrates an alternative paradigm for utilizing offline data, not just for task inference, but for learning a transferable library of fundamental behaviors that can be quickly recombined for novel, complex tasks, significantly reducing the need for online interactions during adaptation.

More recently, the capabilities of offline meta-learning have been extended to highly complex, real-world adaptive control problems. \cite{lupu20249p4} introduced MAGICVFM (Meta-Learning Adaptation for Ground Interaction Control With Visual Foundation Models), which integrates offline meta-learning with visual foundation models (VFMs) and online composite adaptive control. In this framework, offline meta-learning is leveraged to train a robust adaptive controller that processes terrain images via VFMs, feeding features into a deep neural network for residual dynamics. This meta-learned system can then adapt its last layer in real-time for stable control of off-road vehicles, providing mathematical guarantees of exponential stability and robustness. This demonstrates how offline meta-learning can be a cornerstone for building highly adaptive and robust embodied AI systems, by pre-training general adaptation capabilities from diverse offline datasets and then enabling rapid, safe, and stable online refinement, even in safety-critical domains.

In conclusion, offline meta-RL has evolved from foundational concepts of learned RL algorithms to sophisticated frameworks capable of leveraging static data for rapid and robust adaptation. Methods like BOReL have been pivotal in demonstrating the feasibility of learning to adapt without any online interaction during meta-training, while skill-based approaches broaden the utility of offline data. Recent advancements like MAGICVFM showcase the practical utility of this paradigm in complex, safety-critical applications. However, critical challenges remain, including enhancing robustness to out-of-distribution offline data, addressing the inherent limitations posed by MDP ambiguity, and developing stronger theoretical guarantees for complex offline adaptation. Future directions will likely focus on further integrating with large foundation models and exploring methods to robustly handle imperfect or limited offline datasets to unlock even broader real-world deployment capabilities across diverse and dynamic environments.