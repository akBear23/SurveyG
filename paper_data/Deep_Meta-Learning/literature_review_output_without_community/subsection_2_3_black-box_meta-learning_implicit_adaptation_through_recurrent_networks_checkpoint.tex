\subsection{Black-Box Meta-Learning: Implicit Adaptation through Recurrent Networks}

Early explorations into meta-learning leveraged the inherent capacity of recurrent neural networks (RNNs) and other black-box architectures to implicitly learn adaptation algorithms. These methods train a single, often recurrent, model to process task-specific data sequentially, allowing its internal state to evolve and adapt to the nuances of a new task without relying on explicit gradient-based meta-optimization steps during adaptation. This approach offers significant flexibility, enabling the discovery of complex, non-linear adaptation strategies, particularly in domains like reinforcement learning where sequential decision-making is paramount. However, this flexibility often comes at the cost of reduced interpretability regarding the precise mechanisms of the learned adaptation process.

A seminal contribution in this area is the work by \cite{wang20167px}, which introduced deep meta-reinforcement learning (deep meta-RL). This framework demonstrated that a recurrent neural network, specifically an LSTM, could implicitly learn a full-fledged reinforcement learning procedure. The LSTM is trained using a standard deep RL algorithm (e.g., Advantage Actor-Critic) across a distribution of related tasks. Crucially, during an episode, the network receives not only the current observation but also auxiliary inputs indicating the action taken on the previous step and the reward received from that action. Through this sequential processing, the recurrent dynamics of the LSTM itself learn to implement an internal RL algorithm, adapting its policy based on the history of interactions within an episode, even when the network's weights are frozen. This implicit adaptation allows the learned RL algorithm to develop its own exploration strategies and policy update rules, which are tailored to exploit the underlying structure of the training domain for rapid adaptation. Experiments in various bandit tasks and simple Markov Decision Problems showed that this meta-RL approach could outperform established bandit algorithms like UCB and Thompson sampling, validating its ability to learn effective exploration-exploitation strategies.

Beyond learning an entire RL algorithm, other black-box approaches have explored different forms of implicit adaptation. For instance, \cite{vecoven2018hc1} introduced Neuro-Modulated Networks (NMNs), where a "modulator" network, often recurrent, dynamically adjusts the weights or activations of a "main" network. This mechanism allows the system to learn adaptive behaviors by implicitly controlling the computational flow and processing within the main network based on task context. The modulator network, acting as a black box, learns to infer and apply appropriate modulations, enabling flexible responses to unforeseen problems without explicit meta-optimization of the main network's parameters. These neuromodulatory approaches represent another facet of implicit adaptation, where a learned, dynamic control signal facilitates task-specific adjustments.

While offering remarkable flexibility and the ability to discover novel adaptation strategies, black-box meta-learning methods inherently face challenges. The primary limitation is the reduced interpretability of the learned adaptation process; understanding *how* the RNN's internal state translates into a specific adaptation strategy remains opaque. Furthermore, while these models can generalize well to new tasks drawn from the *same distribution* as the training tasks, their ability to adapt to significantly out-of-distribution tasks can be limited, as the learned implicit algorithm is specialized to the statistics of the training environment. The computational intensity of training complex recurrent architectures for meta-learning, especially for very long sequences or high-dimensional state spaces, also poses scalability concerns. These limitations, particularly the lack of interpretability and the desire for more principled adaptation, have motivated the development of more explicit meta-learning paradigms, such as gradient-based meta-optimization, which offer clearer insights into the adaptation process and often provide stronger generalization guarantees. Nevertheless, black-box recurrent approaches remain a foundational and powerful pathway, particularly in domains like meta-reinforcement learning, where the sequential nature of tasks aligns naturally with recurrent processing.