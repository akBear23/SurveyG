\subsection{Continual Adaptation in Model-Based Meta-Reinforcement Learning}

Agents operating in dynamic and evolving environments critically require the ability to continually adapt their behavior and internal understanding of the world. Meta-learning, particularly when integrated with model-based reinforcement learning (MBRL), offers a powerful paradigm for achieving persistent online adaptation, enabling agents to refine their environmental models and prevent catastrophic forgetting.

Early explorations into meta-reinforcement learning laid the groundwork for adaptive agents by demonstrating that neural networks could implicitly learn generalizable learning algorithms. For instance, \textcite{wang20167px} introduced a deep meta-RL framework where a recurrent neural network (RNN), trained across a distribution of tasks, implicitly learns an entire reinforcement learning procedure within its recurrent dynamics. This foundational work showed that an agent could learn an internal mechanism for rapid adaptation, including exploration and policy updates, based on observed rewards and actions, thereby setting the stage for more explicit model-based adaptation.

Building upon this, the Model-Based Online Learner (MOLe) proposed by \textcite{nagabandi2018esl} directly addresses continual online adaptation in MBRL by combining Model-Agnostic Meta-Learning (MAML) with an Expectation-Maximization (EM) algorithm and a Dirichlet Process-based mixture model. MOLe enables agents to learn a distribution of dynamics models and continuously refine their internal representations of the environment by either identifying the best-fitting model from the mixture or adapting existing ones to new dynamics \cite{nagabandi2018esl}. This approach is crucial for preventing catastrophic forgetting, as it maintains a diverse set of learned models, allowing for persistent improvement in performance over extended periods in non-stationary environments.

Further advancing model-based adaptation, \textcite{zintgraf2019zat} introduced VariBAD, a method for Bayes-adaptive deep RL that meta-learns a variational inference network. This network is trained to infer a latent context variable that effectively represents the current Markov Decision Process (MDP), allowing the agent to adapt its policy based on a continually refined internal model (belief) of the task \cite{zintgraf2019zat}. By performing efficient posterior inference over task parameters, VariBAD facilitates principled exploration and rapid adaptation to new tasks within a family, significantly enhancing the agent's ability to adapt its understanding of the environment in a principled, Bayesian manner.

More recently, the principles of continual adaptation in model-based settings have been extended to complex embodied AI systems. \textcite{lupu20249p4} presented MAGICVFM, a framework that integrates Visual Foundation Models (VFMs) with offline meta-learning and online composite adaptive control for robust ground interaction. This method processes terrain images via VFMs to extract features, which are then used to adapt a neural network's last layer in real-time to model residual dynamics, effectively continually refining an internal model of terrain interaction \cite{lupu20249p4}. MAGICVFM provides mathematical guarantees of exponential stability and robustness, demonstrating how agents can achieve safe and effective long-term operation in dynamic physical environments by continuously adapting their internal models based on rich sensory inputs.

In summary, the literature demonstrates a clear progression towards enabling robust and continual adaptation in model-based meta-reinforcement learning. From implicit learning of RL algorithms to explicit mixture models for dynamics and principled belief-based adaptation, these methods empower agents to persistently improve performance, adapt to changing dynamics, and tackle novel situations without succumbing to catastrophic forgetting. Future directions include developing more sophisticated and interpretable model representations, enhancing sample efficiency for real-world deployment, and integrating stronger theoretical guarantees for safety and robustness in highly dynamic and unpredictable environments.