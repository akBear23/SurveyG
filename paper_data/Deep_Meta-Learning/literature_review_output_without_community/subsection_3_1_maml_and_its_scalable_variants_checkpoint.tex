\subsection{MAML and its Scalable Variants}

The advent of Model-Agnostic Meta-Learning (MAML) marked a significant milestone in gradient-based meta-learning, offering a principled approach for deep neural networks to rapidly adapt to new tasks with minimal data \cite{Finn2017}. MAML's core innovation lies in learning a set of initial parameters such that a few gradient steps on a new task yield substantial performance improvements. This is achieved by meta-optimizing these initial parameters, requiring the computation of second-order derivatives (or their approximations) to ensure the learned initialization is "sensitive" to rapid adaptation \cite{Finn2017}. While theoretically powerful and model-agnostic, this reliance on second-order derivatives introduced significant practical limitations, including high computational cost, substantial memory consumption, and increased implementation complexity, particularly when applied to very deep and wide neural networks. These challenges restricted MAML's applicability to larger models and datasets, hindering its broader adoption in real-world scenarios.

To address these practical bottlenecks, scalable alternatives emerged, primarily focusing on reducing the computational overhead. A prominent example is Reptile, which proposed a significantly simpler, first-order meta-learning algorithm \cite{Nichol2018}. Instead of complex second-order optimization, Reptile iteratively samples a task, performs several gradient steps to obtain task-specific parameters, and then moves the global meta-parameters (the initialization) *towards* these task-specific parameters using only first-order gradients. This approach was demonstrated to be a first-order approximation of MAML's objective, effectively achieving similar meta-learning goals with vastly improved efficiency \cite{Nichol2018}. Reptile's simplicity and computational efficiency significantly lowered the barrier to entry for applying meta-learning to deep networks, making it more practical for large-scale deep learning problems and fostering wider adoption.

Beyond first-order approximations, further advancements sought to enhance both efficiency and generalization by decoupling meta-learning from high-dimensional parameter optimization. One such direction involved learning in a low-dimensional latent space, where meta-parameters are generated or adapted more efficiently than directly optimizing the full high-dimensional model parameters. This approach improves efficiency by reducing the dimensionality of the meta-learning problem and can enhance generalization by learning more abstract, transferable representations. For instance, in complex control tasks, methods like Neural-Fly leverage similar principles by learning shared representations and confining task-specific adaptations to a low-dimensional space \cite{oconnell2022twd}. This allows for rapid online adaptation in dynamic environments, such as agile flight in strong winds, by updating a small set of linear coefficients that mix basis elements derived from pretrained representations \cite{oconnell2022twd}. The ability to adapt quickly and precisely in such challenging conditions underscores the practical benefits of these scalable meta-learning approaches.

The development of these scalable variants has significantly broadened the applicability of gradient-based meta-learning. For example, in the domain of No-Reference Image Quality Assessment (NR-IQA), where annotated data is scarce, meta-learning has been successfully applied to learn meta-knowledge shared across various distortion types \cite{zhu2020rb5}. The MetaIQA framework utilizes deep meta-learning to acquire a quality prior model that can then be quickly fine-tuned on a target NR-IQA task, demonstrating strong generalization from synthetic to authentic distortions \cite{zhu2020rb5}. This exemplifies how efficient meta-learning, by learning transferable prior knowledge, can address data scarcity problems in specialized applications. The evolution from the computationally intensive original MAML to its more scalable and efficient variants, through first-order approximations and latent space learning, has thus been crucial in making gradient-based meta-learning a practical and powerful tool for a diverse range of real-world scenarios, from few-shot learning to complex control and specialized data-scarce domains.

Despite these advancements, challenges remain in ensuring robust generalization to truly novel, out-of-distribution tasks, and in developing meta-learning algorithms that can continually adapt without catastrophic forgetting. Future research directions will likely continue to explore more sophisticated ways to balance efficiency with robust generalization, particularly in dynamic and open-ended learning environments.