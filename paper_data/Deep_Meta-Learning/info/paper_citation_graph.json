{
  "nodes": [
    {
      "id": "7b201e42e32430d951458916810a7dbf1e946a6d",
      "title": "Regularizing Meta-Learning via Gradient Dropout",
      "abstract": "With the growing attention on learning-to-learn new tasks using only a few examples, meta-learning has been widely used in numerous problems such as few-shot classification, reinforcement learning, and domain generalization. However, meta-learning models are prone to overfitting when there are no sufficient training tasks for the meta-learners to generalize. Although existing approaches such as Dropout are widely used to address the overfitting problem, these methods are typically designed for regularizing models of a single task in supervised training. In this paper, we introduce a simple yet effective method to alleviate the risk of overfitting for gradient-based meta-learning. Specifically, during the gradient-based adaptation stage, we randomly drop the gradient in the inner-loop optimization of each parameter in deep neural networks, such that the augmented gradients improve generalization to new tasks. We present a general form of the proposed gradient dropout regularization and show that this term can be sampled from either the Bernoulli or Gaussian distribution. To validate the proposed method, we conduct extensive experiments and analysis on numerous computer vision tasks, demonstrating that the gradient dropout regularization mitigates the overfitting problem and improves the performance upon various gradient-based meta-learning frameworks.",
      "authors": [
        "Hung-Yu Tseng",
        "Yi-Wen Chen",
        "Yi-Hsuan Tsai",
        "Sifei Liu",
        "Yen-Yu Lin",
        "Ming-Hsuan Yang"
      ],
      "year": 2020,
      "citation_count": 36,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/7b201e42e32430d951458916810a7dbf1e946a6d",
      "pdf_link": "",
      "venue": "Asian Conference on Computer Vision",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "a962dc06a19c08bb76184bde864e7f1e2e502150",
      "title": "Rethinking Meta-Learning from a Learning Lens",
      "abstract": "Meta-learning seeks to learn a well-generalized model initialization from training tasks to solve unseen tasks. From the\"learning to learn\"perspective, the quality of the initialization is modeled with one-step gradient decent in the inner loop. However, contrary to theoretical expectations, our empirical analysis reveals that this may expose meta-learning to underfitting. To bridge the gap between theoretical understanding and practical implementation, we reconsider meta-learning from the\"Learning\"lens. We propose that the meta-learning model comprises two interrelated components: parameters for model initialization and a meta-layer for task-specific fine-tuning. These components will lead to the risks of overfitting and underfitting depending on tasks, and their solutions, fewer parameters vs. more meta-layer, are often in conflict. To address this, we aim to regulate the task information the model receives without modifying the data or model structure. Our theoretical analysis indicates that models adapted to different tasks can mutually reinforce each other, highlighting the effective information. Based on this insight, we propose TRLearner, a plug-and-play method that leverages task relation to calibrate meta-learning. It first extracts task relation matrices and then applies relation-aware consistency regularization to guide optimization. Extensive theoretical and empirical evaluations demonstrate its effectiveness.",
      "authors": [
        "Jingyao Wang",
        "Wenwen Qiang",
        "Jiangmeng Li",
        "Lingyu Si",
        "Changwen Zheng"
      ],
      "year": 2024,
      "citation_count": 3,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/a962dc06a19c08bb76184bde864e7f1e2e502150",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "79aa092bb37f5ab75d93195f2a5288a51bb8f21d",
      "title": "MetaLoc: Learning to Learn Wireless Localization",
      "abstract": "Existing localization methods that intensively leverage the environment-specific received signal strength (RSS) or channel state information (CSI) of wireless signals are rather accurate in certain environments. However, these methods, whether based on pure statistical signal processing or data-driven approaches, often struggle to generalize to new environments, which results in considerable time and effort being wasted. To address this challenge, we propose MetaLoc, which is the first fingerprinting-based localization framework that leverages the Model-Agnostic Meta-Learning (MAML). Specifically, built on a deep neural network with strong representation capabilities, MetaLoc is trained on historical data sourced from well-calibrated environments, employing a two-loop optimization mechanism to obtain the meta-parameters. These meta-parameters act as the initialization for quick adaptation in new environments, reducing the need for much human effort. The framework introduces two paradigms for the optimization of meta-parameters: a centralized paradigm that simplifies the process by sharing data from all historical environments, and a distributed paradigm that maintains data privacy by training meta-parameters for each specific environment separately. Furthermore, the advanced distributed paradigm modifies the vanilla MAML loss function to ensure that the reduction of loss occurs in a consistent direction across various training domains, thus facilitating faster convergence during training. Our experiments on both synthetic and real datasets demonstrate that MetaLoc outperforms baseline methods in terms of localization accuracy, robustness, and cost-effectiveness. The code and datasets used in this study are publicly available at: https://github.com/WU-Dongze/MetaLoc.",
      "authors": [
        "Jun Gao",
        "Dongze Wu",
        "Feng Yin",
        "Qinglei Kong",
        "Lexi Xu",
        "Shuguang Cui"
      ],
      "year": 2022,
      "citation_count": 73,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/79aa092bb37f5ab75d93195f2a5288a51bb8f21d",
      "pdf_link": "",
      "venue": "IEEE Journal on Selected Areas in Communications",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "020bb2ba5f3923858cd6882ba5c5a44ea8041ab6",
      "title": "Meta-Learning in Neural Networks: A Survey",
      "abstract": "The field of meta-learning, or learning-to-learn, has seen a dramatic rise in interest in recent years. Contrary to conventional approaches to AI where tasks are solved from scratch using a fixed learning algorithm, meta-learning aims to improve the learning algorithm itself, given the experience of multiple learning episodes. This paradigm provides an opportunity to tackle many conventional challenges of deep learning, including data and computation bottlenecks, as well as generalization. This survey describes the contemporary meta-learning landscape. We first discuss definitions of meta-learning and position it with respect to related fields, such as transfer learning and hyperparameter optimization. We then propose a new taxonomy that provides a more comprehensive breakdown of the space of meta-learning methods today. We survey promising applications and successes of meta-learning such as few-shot learning and reinforcement learning. Finally, we discuss outstanding challenges and promising areas for future research.",
      "authors": [
        "Timothy M. Hospedales",
        "Antreas Antoniou",
        "P. Micaelli",
        "A. Storkey"
      ],
      "year": 2020,
      "citation_count": 2147,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/020bb2ba5f3923858cd6882ba5c5a44ea8041ab6",
      "pdf_link": "",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "e1a8bf47ef1c51298f5bcf957062c6ee63a96bfb",
      "title": "Graph Convolutional Network Based Multi-Objective Meta-Deep Q-Learning for Eco-Routing",
      "abstract": "Route selection can greatly affect vehicle fuel consumption and emissions. Finding the most fuel/energy-efficient route is known as the eco-routing problem. Existing eco-routing solutions do not effectively consider the critical traffic signal information and rely on fuel consumption models that may not be sufficiently accurate. To address the eco-routing problem in a signalized traffic network, this paper proposes a graph convolutional network based multi-objective meta-deep Q-learning (GM2DQL) method. The problem is formulated as dynamic multi-objective Markov decision processes (MOMDP) and is tackled through deep reinforcement learning and meta-learning. We identify that graph convolutional network (GCN) is an efficient and suitable feature representation for a signalized traffic network. GM2DQL can explore the optimal routes with respect to drivers’ different preferences on saving fuel and travel time. Through GM2DQL, the agent is trained under a series of learning environments that are characterized by historical vehicle trajectories, fuel consumption data, and traffic signal data in the remote data center. The vehicle requesting eco-routing service can download the model that represents the action value function of the historical dynamic driving conditions. The model in the vehicle can quickly adapt to the most recent driving condition through online one-shot learning and predict the optimal eco-routes for the subsequent unseen driving conditions of the signalized traffic network. Extensive proof-of-concept experiments validate that GM2DQL can effectively discover optimal eco-routes. It saves up to 71% travel time and 62% fuel, compared to the conventional shortest-path routing strategy that is widely used in navigation systems.",
      "authors": [
        "Xin Ma",
        "Yuanchang Xie",
        "Chunxiao Chigan"
      ],
      "year": 2024,
      "citation_count": 3,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/e1a8bf47ef1c51298f5bcf957062c6ee63a96bfb",
      "pdf_link": "",
      "venue": "IEEE transactions on intelligent transportation systems (Print)",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "f4eff7c0127a2ef92c441f028c3bb15b64cabcc8",
      "title": "One-Shot Imitation from Observing Humans via Domain-Adaptive Meta-Learning",
      "abstract": "Humans and animals are capable of learning a new behavior by observing others perform the skill just once. We consider the problem of allowing a robot to do the same -- learning from a raw video pixels of a human, even when there is substantial domain shift in the perspective, environment, and embodiment between the robot and the observed human. Prior approaches to this problem have hand-specified how human and robot actions correspond and often relied on explicit human pose detection systems. In this work, we present an approach for one-shot learning from a video of a human by using human and robot demonstration data from a variety of previous tasks to build up prior knowledge through meta-learning. Then, combining this prior knowledge and only a single video demonstration from a human, the robot can perform the task that the human demonstrated. We show experiments on both a PR2 arm and a Sawyer arm, demonstrating that after meta-learning, the robot can learn to place, push, and pick-and-place new objects using just one video of a human performing the manipulation.",
      "authors": [
        "Tianhe Yu",
        "Chelsea Finn",
        "Annie Xie",
        "Sudeep Dasari",
        "Tianhao Zhang",
        "P. Abbeel",
        "S. Levine"
      ],
      "year": 2018,
      "citation_count": 367,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/f4eff7c0127a2ef92c441f028c3bb15b64cabcc8",
      "pdf_link": "",
      "venue": "Robotics: Science and Systems",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "b82a9500246176a6b32598ded8d2b96d1e29f61c",
      "title": "Bayesian Meta-Learning: Toward Fast Adaptation in Neural Network Positioning Techniques",
      "abstract": "Neural network positioning technology, as one of the mainstream in indoor Wi-Fi positioning systems, is playing an increasingly important role in location-based services. The main challenge is that the samples are prone to be outdated as the indoor environment changes or the wireless signal varies over time, i.e., the samples’ Age of Information (AoI) is large, which leads to the trained model not being available. However, recollecting data to retrain the model is both time-consuming and labor-intensive. To address the above problem, this article proposes a fast adaptation approach based on Bayesian meta-learning that makes the pretrained model acquire a learned learning capability so that it can quickly learn new tasks based on the acquisition of existing knowledge. Specifically, first, a model-agnostic learning scheme is introduced to guide the learning process, which could automatically learn the optimal model parameters and hyperparameter settings. Second, to mitigate the effects of model uncertainty, especially to prevent the overfitting situation based on a limited number of samples, we combine the Stein variational gradient descent (SVGD) with the model-agnostic learning scheme, i.e., Bayesian meta-learning. Compared with traditional meta-learning algorithms, the proposed method makes the training more robust by inferring the Bayesian posterior from a probabilistic perspective. Extensive experimental results show that the proposed approach effectively overcomes the impact of large AoI on localization performance while decreasing labor consumption significantly.",
      "authors": [
        "Q. Pu",
        "Youkun Chen",
        "Mu Zhou",
        "Joseph K. Y. Ng",
        "Rui Cai"
      ],
      "year": 2024,
      "citation_count": 3,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/b82a9500246176a6b32598ded8d2b96d1e29f61c",
      "pdf_link": "",
      "venue": "IEEE Internet of Things Journal",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "a9f1e05bb2f6f2eeaf92fe1fc5ca3f0eb498f673",
      "title": "Learning Deep Time-index Models for Time Series Forecasting",
      "abstract": "Deep learning has been actively applied to time series forecasting, leading to a deluge of new methods, belonging to the class of historical-value models. Yet, despite the attractive properties of time-index models, such as being able to model the continuous nature of underlying time series dynamics, little attention has been given to them. Indeed, while naive deep time-index models are far more expressive than the manually predefined function representations of classical time-index models, they are inadequate for forecasting, being unable to generalize to unseen time steps due to the lack of inductive bias. In this paper, we propose DeepTime, a meta-optimization framework to learn deep time-index models which overcome these limitations, yielding an efficient and accurate forecasting model. Extensive experiments on real world datasets in the long sequence time-series forecasting setting demonstrate that our approach achieves competitive results with state-of-the-art methods, and is highly efficient. Code is available at https://github.com/salesforce/DeepTime.",
      "authors": [
        "Gerald Woo",
        "Chenghao Liu",
        "Doyen Sahoo",
        "Akshat Kumar",
        "S. Hoi"
      ],
      "year": 2022,
      "citation_count": 32,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/a9f1e05bb2f6f2eeaf92fe1fc5ca3f0eb498f673",
      "pdf_link": "",
      "venue": "International Conference on Machine Learning",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "434d8baa964856bcf4bbe9d1bf49dc70ac2128ab",
      "title": "Few Shot Learning For Point Cloud Data Using Model Agnostic Meta Learning",
      "abstract": "The ability of deep neural networks to extract complex statistics and learn high level features from vast datasets is proven. Yet current deep learning approaches suffer from poor sample efficiency in stark contrast to human perception. Few shot learning algorithms such as matching networks or Model Agnostic Meta Learning (MAML) mitigate this problem, enabling fast learning with few examples. In this paper, we extend the MAML algorithm to point cloud data using a PointNet Architecture. We construct N$\\times$K-shot classification tasks from the ModelNet40 point cloud dataset to show that this method performs classification as well as supervised deep learning methods with the added benefit of being able to adapt after a single gradient step on a single N$\\times$K task. We empirically search for optimal values of N and K for few shot classification and show our method to achieve 90% meta test accuracy compared to traditional PointNet with 89.2%. We also adapt a meta-trained PointNet to a support set of 9, N=3, K=3, never before seen point clouds which are drawn from an entirely different dataset, ShapeNet. Once adapted the model achieves 7.1/9 classification accuracy on average across 100 query sets of the same classes with new, unique instances. This result far exceeds the supervised Stochastic Gradient Descent (SGD) training result of 3.1/9 accuracy on the query sets which is equivalent to a random baseline.",
      "authors": [
        "R. Puri",
        "A. Zakhor",
        "Raul Puri"
      ],
      "year": 2020,
      "citation_count": 11,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/434d8baa964856bcf4bbe9d1bf49dc70ac2128ab",
      "pdf_link": "",
      "venue": "International Conference on Information Photonics",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "0341673e7ed78a096b9b9b51fbdbeff08beed660",
      "title": "Meta Soft Label Generation for Noisy Labels",
      "abstract": "The existence of noisy labels in the dataset causes significant performance degradation for deep neural networks (DNNs). To address this problem, we propose a Meta Soft Label Generation algorithm called MSLG, which can jointly generate soft labels using meta-learning techniques and learn DNN parameters in an end-to-end fashion. Our approach adapts the meta-learning paradigm to estimate optimal label distribution by checking gradient directions on both noisy training data and noise-free meta-data. In order to iteratively update soft labels, meta-gradient descent step is performed on estimated labels, which would minimize the loss of noise-free meta samples. In each iteration, the base classifier is trained on estimated meta labels. MSLG is model-agnostic and can be added on top of any existing model at hand with ease. We performed extensive experiments on CIFAR10, Clothing1M and Food101N datasets. Results show that our approach outperforms other state-of-the-art methods by a large margin. Our code is available at https://github.com/gorkemalgan/MSLG_noisy_label.",
      "authors": [
        "G. Algan",
        "I. Ulusoy"
      ],
      "year": 2020,
      "citation_count": 43,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/0341673e7ed78a096b9b9b51fbdbeff08beed660",
      "pdf_link": "",
      "venue": "International Conference on Pattern Recognition",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "9adc67e027edfea39a7904d96f7d436cd3ec3dff",
      "title": "MetaRockETC: Adaptive Encrypted Traffic Classification in Complex Network Environments via Time Series Analysis and Meta-Learning",
      "abstract": "Encrypted Traffic Classification (ETC) is crucial for network security management and Quality of Service (QoS) improvement. There have been many attempts to tackle various ETC tasks, however, which generally suffer from task dependency and limited adaptability, falling short of meeting practical requirements. Under the realistic assumptions of complex network environments, diverse encryption techniques and ever-changing application landscapes coexist. It is highly desirable to learn the generic encrypted traffic representations to investigate the common knowledge across different ETC tasks and rapidly adapt to the dynamic shifts. To fill the gap, we propose MetaRockETC, a generic encrypted traffic classification framework, which extracts protocol-agnostic features to learn the common knowledge and rapidly adapt to novel ETC tasks and evolving network environments. In MetaRockETC, we first model packet length sequences of encrypted sessions as multivariate time series and perform random convolution kernel transformations to summarize discriminatory behavioral patterns across channels. By integrating MetaRockETC into an advanced Model-Agnostic Meta-Learning (MAML) framework, we learn a task-adaptive loss function to facilitate better generalization and transferability across diverse ETC tasks. Extensive experimental results demonstrate the superiority of MetaRockETC in both across-task and few-shot scenarios, highlighting its potential to provide a practical solution for encrypted traffic classification in real-world scenarios.",
      "authors": [
        "Jianjin Zhao",
        "Qi Li",
        "Yueping Hong",
        "Meng Shen"
      ],
      "year": 2024,
      "citation_count": 11,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/9adc67e027edfea39a7904d96f7d436cd3ec3dff",
      "pdf_link": "",
      "venue": "IEEE Transactions on Network and Service Management",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "bf8d58faf972ad0a1026c0a7c5577c07996ef3a7",
      "title": "Hierarchical Deep Reinforcement Learning for Continuous Action Control",
      "abstract": "Robotic control in a continuous action space has long been a challenging topic. This is especially true when controlling robots to solve compound tasks, as both basic skills and compound skills need to be learned. In this paper, we propose a hierarchical deep reinforcement learning algorithm to learn basic skills and compound skills simultaneously. In the proposed algorithm, compound skills and basic skills are learned by two levels of hierarchy. In the first level of hierarchy, each basic skill is handled by its own actor, overseen by a shared basic critic. Then, in the second level of hierarchy, compound skills are learned by a meta critic by reusing basic skills. The proposed algorithm was evaluated on a Pioneer 3AT robot in three different navigation scenarios with fully observable tasks. The simulations were built in Gazebo 2 in a robot operating system Indigo environment. The results show that the proposed algorithm can learn both high performance basic skills and compound skills through the same learning process. The compound skills learned outperform those learned by a discrete action space deep reinforcement learning algorithm.",
      "authors": [
        "Zhaoyang Yang",
        "K. Merrick",
        "Lianwen Jin",
        "H. Abbass"
      ],
      "year": 2018,
      "citation_count": 158,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/bf8d58faf972ad0a1026c0a7c5577c07996ef3a7",
      "pdf_link": "",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "aea3f03299ff0cfea9b394f5559aa1c173f9876f",
      "title": "Few-Shot Bayesian Optimization with Deep Kernel Surrogates",
      "abstract": "Hyperparameter optimization (HPO) is a central pillar in the automation of machine learning solutions and is mainly performed via Bayesian optimization, where a parametric surrogate is learned to approximate the black box response function (e.g. validation error). Unfortunately, evaluating the response function is computationally intensive. As a remedy, earlier work emphasizes the need for transfer learning surrogates which learn to optimize hyperparameters for an algorithm from other tasks. In contrast to previous work, we propose to rethink HPO as a few-shot learning problem in which we train a shared deep surrogate model to quickly adapt (with few response evaluations) to the response function of a new task. We propose the use of a deep kernel network for a Gaussian process surrogate that is meta-learned in an end-to-end fashion in order to jointly approximate the response functions of a collection of training data sets. As a result, the novel few-shot optimization of our deep kernel surrogate leads to new state-of-the-art results at HPO compared to several recent methods on diverse metadata sets.",
      "authors": [
        "Martin Wistuba",
        "Josif Grabocka"
      ],
      "year": 2021,
      "citation_count": 73,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/aea3f03299ff0cfea9b394f5559aa1c173f9876f",
      "pdf_link": "",
      "venue": "International Conference on Learning Representations",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "98b41528c58e6f5b7b28be5b54029e52ca90c4ab",
      "title": "Learning to Learn: Hierarchical Meta-Critic Networks",
      "abstract": "In recent years, deep reinforcement learning methods have achieved impressive performance in many different fields, including playing games, robotics, and dialogue systems. However, there are still a lot of restrictions here, one of which is the demand for massive amounts of sampled data. In this paper, a hierarchical meta-learning method based on the actor-critic algorithm is proposed for sample efficient learning. This method provides the transferable knowledge that can efficiently train an actor on a new task with a few trials. Specifically, a global basic critic, meta critic, and task specified network are shared within a distribution of tasks and are capable of criticizing any actor trying to solve any specified task. The hierarchical framework is applied to a critic network in the actor-critic algorithm for distilling meta-knowledge above the task level and addressing distinct tasks. The proposed method is evaluated on multiple classic control tasks with reinforcement learning algorithms, including the start-of-the-art meta-learning methods. The experimental results statistically demonstrate that the proposed method achieves state-of-the-art performance and attains better results with more depth of meta critic network.",
      "authors": [
        "Zhixiong Xu",
        "Lei Cao",
        "Xi-liang Chen"
      ],
      "year": 2019,
      "citation_count": 16,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/98b41528c58e6f5b7b28be5b54029e52ca90c4ab",
      "pdf_link": "",
      "venue": "IEEE Access",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "7bd95a62fd6320730cbb24a0e4fafac97d840652",
      "title": "Meta Reinforcement Learning with Task Embedding and Shared Policy",
      "abstract": "Despite significant progress, deep reinforcement learning (RL) suffers from data-inefficiency and limited generalization. Recent efforts apply meta-learning to learn a meta-learner from a set of RL tasks such that a novel but related task could be solved quickly. Though specific in some ways, different tasks in meta-RL are generally similar at a high level. However, most meta-RL methods do not explicitly and adequately model the specific and shared information among different tasks, which limits their ability to learn training tasks and to generalize to novel tasks. In this paper, we propose to capture the shared information on the one hand and meta-learn how to quickly abstract the specific information about a task on the other hand. Methodologically, we train an SGD meta-learner to quickly optimize a task encoder for each task, which generates a task embedding based on past experience. Meanwhile, we learn a policy which is shared across all tasks and conditioned on task embeddings. Empirical results on four simulated tasks demonstrate that our method has better learning capacity on both training and novel tasks and attains up to 3 to 4 times higher returns compared to baselines.",
      "authors": [
        "Lin Lan",
        "Zhenguo Li",
        "X. Guan",
        "P. Wang"
      ],
      "year": 2019,
      "citation_count": 51,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/7bd95a62fd6320730cbb24a0e4fafac97d840652",
      "pdf_link": "",
      "venue": "International Joint Conference on Artificial Intelligence",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "17b6829678802a20e51558ec28c5369414defe42",
      "title": "Data Valuation using Reinforcement Learning",
      "abstract": "Quantifying the value of data is a fundamental problem in machine learning. Data valuation has multiple important use cases: (1) building insights about the learning task, (2) domain adaptation, (3) corrupted sample discovery, and (4) robust learning. To adaptively learn data values jointly with the target task predictor model, we propose a meta learning framework which we name Data Valuation using Reinforcement Learning (DVRL). We employ a data value estimator (modeled by a deep neural network) to learn how likely each datum is used in training of the predictor model. We train the data value estimator using a reinforcement signal of the reward obtained on a small validation set that reflects performance on the target task. We demonstrate that DVRL yields superior data value estimates compared to alternative methods across different types of datasets and in a diverse set of application scenarios. The corrupted sample discovery performance of DVRL is close to optimal in many regimes (i.e. as if the noisy samples were known apriori), and for domain adaptation and robust learning DVRL significantly outperforms state-of-the-art by 14.6% and 10.8%, respectively.",
      "authors": [
        "Jinsung Yoon",
        "Sercan Ö. Arik",
        "Tomas Pfister"
      ],
      "year": 2019,
      "citation_count": 194,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/17b6829678802a20e51558ec28c5369414defe42",
      "pdf_link": "",
      "venue": "International Conference on Machine Learning",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "a1c68c32b11d83c9d48c48163f2a445ce359069e",
      "title": "A CMDP-within-online framework for Meta-Safe Reinforcement Learning",
      "abstract": "Meta-reinforcement learning has widely been used as a learning-to-learn framework to solve unseen tasks with limited experience. However, the aspect of constraint violations has not been adequately addressed in the existing works, making their application restricted in real-world settings. In this paper, we study the problem of meta-safe reinforcement learning (Meta-SRL) through the CMDP-within-online framework to establish the first provable guarantees in this important setting. We obtain task-averaged regret bounds for the reward maximization (optimality gap) and constraint violations using gradient-based meta-learning and show that the task-averaged optimality gap and constraint satisfaction improve with task-similarity in a static environment or task-relatedness in a dynamic environment. Several technical challenges arise when making this framework practical. To this end, we propose a meta-algorithm that performs inexact online learning on the upper bounds of within-task optimality gap and constraint violations estimated by off-policy stationary distribution corrections. Furthermore, we enable the learning rates to be adapted for every task and extend our approach to settings with a competing dynamically changing oracle. Finally, experiments are conducted to demonstrate the effectiveness of our approach.",
      "authors": [
        "Vanshaj Khattar",
        "Yuhao Ding",
        "Bilgehan Sel",
        "J. Lavaei",
        "Ming Jin"
      ],
      "year": 2024,
      "citation_count": 15,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/a1c68c32b11d83c9d48c48163f2a445ce359069e",
      "pdf_link": "",
      "venue": "International Conference on Learning Representations",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "282a380fb5ac26d99667224cef8c630f6882704f",
      "title": "Learning to reinforcement learn",
      "abstract": "In recent years deep reinforcement learning (RL) systems have attained superhuman performance in a number of challenging task domains. However, a major limitation of such applications is their demand for massive amounts of training data. A critical present objective is thus to develop deep RL methods that can adapt rapidly to new tasks. In the present work we introduce a novel approach to this challenge, which we refer to as deep meta-reinforcement learning. Previous work has shown that recurrent networks can support meta-learning in a fully supervised context. We extend this approach to the RL setting. What emerges is a system that is trained using one RL algorithm, but whose recurrent dynamics implement a second, quite separate RL procedure. This second, learned RL algorithm can differ from the original one in arbitrary ways. Importantly, because it is learned, it is configured to exploit structure in the training domain. We unpack these points in a series of seven proof-of-concept experiments, each of which examines a key aspect of deep meta-RL. We consider prospects for extending and scaling up the approach, and also point out some potentially important implications for neuroscience.",
      "authors": [
        "Jane X. Wang",
        "Z. Kurth-Nelson",
        "Hubert Soyer",
        "Joel Z. Leibo",
        "Dhruva Tirumala",
        "R. Munos",
        "C. Blundell",
        "D. Kumaran",
        "M. Botvinick"
      ],
      "year": 2016,
      "citation_count": 1007,
      "layer": 1,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/282a380fb5ac26d99667224cef8c630f6882704f",
      "pdf_link": "",
      "venue": "Annual Meeting of the Cognitive Science Society",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "bef33d15c3e8d433261f97f7001cc41a5ae0ec32",
      "title": "MADA: Meta-Adaptive Optimizers through hyper-gradient Descent",
      "abstract": "Following the introduction of Adam, several novel adaptive optimizers for deep learning have been proposed. These optimizers typically excel in some tasks but may not outperform Adam uniformly across all tasks. In this work, we introduce Meta-Adaptive Optimizers (MADA), a unified optimizer framework that can generalize several known optimizers and dynamically learn the most suitable one during training. The key idea in MADA is to parameterize the space of optimizers and dynamically search through it using hyper-gradient descent during training. We empirically compare MADA to other popular optimizers on vision and language tasks, and find that MADA consistently outperforms Adam and other popular optimizers, and is robust against sub-optimally tuned hyper-parameters. MADA achieves a greater validation performance improvement over Adam compared to other popular optimizers during GPT-2 training and fine-tuning. We also propose AVGrad, a modification of AMSGrad that replaces the maximum operator with averaging, which is more suitable for hyper-gradient optimization. Finally, we provide a convergence analysis to show that parameterized interpolations of optimizers can improve their error bounds (up to constants), hinting at an advantage for meta-optimizers.",
      "authors": [
        "Kaan Ozkara",
        "Can Karakus",
        "Parameswaran Raman",
        "Mingyi Hong",
        "Shoham Sabach",
        "B. Kveton",
        "V. Cevher"
      ],
      "year": 2024,
      "citation_count": 6,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/bef33d15c3e8d433261f97f7001cc41a5ae0ec32",
      "pdf_link": "",
      "venue": "International Conference on Machine Learning",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "35ad6ba10006975c2bc67ecefaa9ee6af2453bdc",
      "title": "Deep Meta-Learning: Learning to Learn in the Concept Space",
      "abstract": "Few-shot learning remains challenging for meta-learning that learns a learning algorithm (meta-learner) from many related tasks. In this work, we argue that this is due to the lack of a good representation for meta-learning, and propose deep meta-learning to integrate the representation power of deep learning into meta-learning. The framework is composed of three modules, a concept generator, a meta-learner, and a concept discriminator, which are learned jointly. The concept generator, e.g. a deep residual net, extracts a representation for each instance that captures its high-level concept, on which the meta-learner performs few-shot learning, and the concept discriminator recognizes the concepts. By learning to learn in the concept space rather than in the complicated instance space, deep meta-learning can substantially improve vanilla meta-learning, which is demonstrated on various few-shot image recognition problems. For example, on 5-way-1-shot image recognition on CIFAR-100 and CUB-200, it improves Matching Nets from 50.53% and 56.53% to 58.18% and 63.47%, improves MAML from 49.28% and 50.45% to 56.65% and 64.63%, and improves Meta-SGD from 53.83% and 53.34% to 61.62% and 66.95%, respectively.",
      "authors": [
        "Fengwei Zhou",
        "Bin Wu",
        "Zhenguo Li"
      ],
      "year": 2018,
      "citation_count": 124,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/35ad6ba10006975c2bc67ecefaa9ee6af2453bdc",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "361e953f792a585496834ee14216b94d0ce9ae74",
      "title": "VariBAD: A Very Good Method for Bayes-Adaptive Deep RL via Meta-Learning",
      "abstract": "Trading off exploration and exploitation in an unknown environment is key to maximising expected return during learning. A Bayes-optimal policy, which does so optimally, conditions its actions not only on the environment state but on the agent's uncertainty about the environment. Computing a Bayes-optimal policy is however intractable for all but the smallest tasks. In this paper, we introduce variational Bayes-Adaptive Deep RL (variBAD), a way to meta-learn to perform approximate inference in an unknown environment, and incorporate task uncertainty directly during action selection. In a grid-world domain, we illustrate how variBAD performs structured online exploration as a function of task uncertainty. We further evaluate variBAD on MuJoCo domains widely used in meta-RL and show that it achieves higher online return than existing methods.",
      "authors": [
        "Luisa M. Zintgraf",
        "Kyriacos Shiarlis",
        "Maximilian Igl",
        "Sebastian Schulze",
        "Y. Gal",
        "Katja Hofmann",
        "S. Whiteson"
      ],
      "year": 2019,
      "citation_count": 290,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/361e953f792a585496834ee14216b94d0ce9ae74",
      "pdf_link": "",
      "venue": "International Conference on Learning Representations",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "3bd02411eaa798a158aa780a4d75d0cbfa0af790",
      "title": "Deep Metric Learning via Adaptive Learnable Assessment",
      "abstract": "In this paper, we propose a deep metric learning via adaptive learnable assessment (DML-ALA) method for image retrieval and clustering, which aims to learn a sample assessment strategy to maximize the generalization of the trained metric. Unlike existing deep metric learning methods that usually utilize a fixed sampling strategy like hard negative mining, we propose a sequence-aware learnable assessor which re-weights each training example to train the metric towards good generalization. We formulate the learning of this assessor as a meta-learning problem, where we employ an episode-based training scheme and update the assessor at each iteration to adapt to the current model status. We construct each episode by sampling two subsets of disjoint labels to simulate the procedure of training and testing and use the performance of one-gradient-updated metric on the validation subset as the meta-objective of the assessor. Experimental results on the widely used CUB-200-2011, Cars196, and Stanford Online Products datasets demonstrate the effectiveness of the proposed approach.",
      "authors": [
        "Wenzhao Zheng",
        "Jiwen Lu",
        "Jie Zhou"
      ],
      "year": 2020,
      "citation_count": 25,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/3bd02411eaa798a158aa780a4d75d0cbfa0af790",
      "pdf_link": "",
      "venue": "Computer Vision and Pattern Recognition",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "6ac73bcb953640dcc9c5b7f730f57ad135593d8e",
      "title": "Learning to Learn across Diverse Data Biases in Deep Face Recognition",
      "abstract": "Convolutional Neural Networks have achieved remarkable success in face recognition, in part due to the abundant availability of data. However, the data used for training CNNs is often imbalanced. Prior works largely focus on the long-tailed nature of face datasets in data volume per identity, or focus on single bias variation. In this paper, we show that many bias variations such as ethnicity, head pose, occlusion and blur can jointly affect the accuracy significantly. We propose a sample level weighting approach termed Multi-variation Cosine Margin (MvCoM), to simultaneously consider the multiple variation factors, which orthogonally enhances the face recognition losses to incorporate the importance of training samples. Further, we leverage a learning to learn approach, guided by a held-out meta learning set and use an additive modeling to predict the MvCoM. Extensive experiments on challenging face recognition benchmarks demonstrate the advantages of our method in jointly handling imbalances due to multiple variations.",
      "authors": [
        "Chang Liu",
        "Xiang Yu",
        "Yao-Hung Hubert Tsai",
        "M. Faraki",
        "Ramin Moslemi",
        "Manmohan Chandraker",
        "Y. Fu"
      ],
      "year": 2022,
      "citation_count": 21,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/6ac73bcb953640dcc9c5b7f730f57ad135593d8e",
      "pdf_link": "",
      "venue": "Computer Vision and Pattern Recognition",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "290357314d0c339bcce31cfbe6b29aa50f89b026",
      "title": "Learning What and Where to Transfer",
      "abstract": "As the application of deep learning has expanded to real-world problems with insufficient volume of training data, transfer learning recently has gained much attention as means of improving the performance in such small-data regime. However, when existing methods are applied between heterogeneous architectures and tasks, it becomes more important to manage their detailed configurations and often requires exhaustive tuning on them for the desired performance. To address the issue, we propose a novel transfer learning approach based on meta-learning that can automatically learn what knowledge to transfer from the source network to where in the target network. Given source and target networks, we propose an efficient training scheme to learn meta-networks that decide (a) which pairs of layers between the source and target networks should be matched for knowledge transfer and (b) which features and how much knowledge from each feature should be transferred. We validate our meta-transfer approach against recent transfer learning methods on various datasets and network architectures, on which our automated scheme significantly outperforms the prior baselines that find \"what and where to transfer\" in a hand-crafted manner.",
      "authors": [
        "Yunhun Jang",
        "Hankook Lee",
        "Sung Ju Hwang",
        "Jinwoo Shin"
      ],
      "year": 2019,
      "citation_count": 154,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/290357314d0c339bcce31cfbe6b29aa50f89b026",
      "pdf_link": "",
      "venue": "International Conference on Machine Learning",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "47da3a722b007cef7238299a075c0595fed8632e",
      "title": "Meta-Learning Biologically Plausible Semi-Supervised Update Rules",
      "abstract": "The question of how neurons embedded in a network update their synaptic weights to collectively achieve behavioral goals is a longstanding problem in systems neuroscience. Since Hebb’s hypothesis [10] that cells that fire together strengthen their connections, cellular studies [6] have shed light on potential synaptic mechanisms underlying learning. These mechanisms have directly driven the careful hand design of biologically plausible models of learning and memory in computational neuroscience [1]. However, these hand designed rules have yet to achieve satisfying success training large neural networks, and are dramatically outperformed by biologically implausible approaches such as backprop. We propose an alternative paradigm for designing biologically plausible learning rules: using meta-learning to learn a parametric synaptic update rule which is capable of training deep networks. We demonstrate this approach by meta-learning an update rule for semi-supervised tasks, where sparse labels are provided to a deep network but the majority of inputs are unlabeled. The meta-learned plasticity rule integrates bottom-up, top-down, and recurrent inputs to each neuron, and generates weight updates as the product of pre- and post- synaptic neuronal outputs. The way in which the inputs to each neuron are combined to produce a learning signal, however, is itself a meta-learned function, parameterized by a neural network. Critically, the meta-learned update rule integrates only neuron-local information when proposing updates–that is, our learning rule is spatially localized to individual neurons. After meta-learning, the resulting synaptic update rule is capable of driving task-relevant learning for semi-supervised tasks. We demonstrate this capability on two simple classification problems. In general, we believe meta-learning to be a powerful approach to finding more effective synaptic plasticity rules, which will motivate new hypotheses for biological neural networks, and new algorithms for artificial neural networks.",
      "authors": [
        "Keren Gu",
        "S. Greydanus",
        "Luke Metz",
        "Niru Maheswaranathan",
        "Jascha Narain Sohl-Dickstein"
      ],
      "year": 2019,
      "citation_count": 15,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/47da3a722b007cef7238299a075c0595fed8632e",
      "pdf_link": "",
      "venue": "bioRxiv",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "737ee2562b31437146de4df7e2948d1027ef2ecd",
      "title": "Meta-Learning with Sparse Experience Replay for Lifelong Language Learning",
      "abstract": "Lifelong learning requires models that can continuously learn from sequential streams of data without suffering catastrophic forgetting due to shifts in data distributions. Deep learning models have thrived in the non-sequential learning paradigm; however, when used to learn a sequence of tasks, they fail to retain past knowledge and learn incrementally. We propose a novel approach to lifelong learning of language tasks based on meta-learning with sparse experience replay that directly optimizes to prevent forgetting. We show that under the realistic setting of performing a single pass on a stream of tasks and without any task identifiers, our method obtains state-of-the-art results on lifelong text classification and relation extraction. We analyze the effectiveness of our approach and further demonstrate its low computational and space complexity.",
      "authors": [
        "Nithin Holla",
        "Pushkar Mishra",
        "H. Yannakoudakis",
        "Ekaterina Shutova"
      ],
      "year": 2020,
      "citation_count": 22,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/737ee2562b31437146de4df7e2948d1027ef2ecd",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "f8ee167e718cb152d816f06d42c66efec729a536",
      "title": "Domain Generalization through Meta-Learning: A Survey",
      "abstract": "Deep neural networks (DNNs) have revolutionized artificial intelligence but often lack performance when faced with out-of-distribution data, a common scenario due to the inevitable domain shifts in real-world applications. This limitation stems from the common assumption that training and testing data share the same distribution-an assumption frequently violated in practice. Despite their effectiveness with large amounts of data and computational power, DNNs struggle with distributional shifts and limited labeled data, leading to overfitting and poor generalization across various tasks and domains. Meta-learning presents a promising approach by employing algorithms that acquire transferable knowledge across various tasks for fast adaptation, eliminating the need to learn each task from scratch. This survey paper delves into the realm of meta-learning with a focus on its contribution to domain generalization. We first clarify the concept of meta-learning for domain generalization and introduce a novel taxonomy based on the feature extraction strategy and the classifier learning methodology, offering a granular view of methodologies. Additionally, we present a decision graph to assist readers in navigating the taxonomy based on data availability and domain shifts, enabling them to select and develop a proper model tailored to their specific problem requirements. Through an exhaustive review of existing methods and underlying theories, we map out the fundamentals of the field. Our survey provides practical insights and an informed discussion on promising research directions.",
      "authors": [
        "Arsham Gholamzadeh Khoee",
        "Yinan Yu",
        "R. Feldt"
      ],
      "year": 2024,
      "citation_count": 28,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/f8ee167e718cb152d816f06d42c66efec729a536",
      "pdf_link": "",
      "venue": "Artificial Intelligence Review",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "4da500f57577b4ee207ab80b4cde0e1ee338f948",
      "title": "Attention Multisource Fusion-Based Deep Few-Shot Learning for Hyperspectral Image Classification",
      "abstract": "Recently, deep learning-based methods outperform others in hyperspectral image (HSI) classification. However, the deep learning methods require sufficient labeled samples to improve performance, which is unfeasible in practice. The training labels are usually limited in HSIs that need to be classified (namely target domain), while other available labels in multisource HSIs (namely source domain) are not utilized effectively. To mitigate these issues, an attention multisource fusion method of few-shot learning (AMF-FSL) is proposed for small-sized HSI classification. AMF-FSL is an implementation of few-shot learning (FSL) in the meta-learning field, which can transfer the learned ability of classification from multiple source data to target data. The process of learning to classify in AMF-FSL is not restricted by the traditional requirement of the same distribution between the source and target domains, which can learn from the source domain and apply it to a different distribution in the target domain. Moreover, the multisource domain adaption in AMF-FSL has the capacity of extracting features from fused homogeneous and heterogeneous data in the source domain, which can improve the generalization of the classification model in the cross domains. Specifically, the multisource domain adaption contains three modules, namely the target-based class alignment, domain attention assignment, and multisource data fusion, which are responsible for aligning the class space, paying band-level attention, and merging the distributions of homogeneous and heterogeneous data in the source domain. The experimental results demonstrate the effectiveness of the multisource domain adaption and the superiority of AMF-FSL over other state-of-the-art methods in small-sized HSI classification.",
      "authors": [
        "Xuejian Liang",
        "Yehui Zhang",
        "Junping Zhang"
      ],
      "year": 2021,
      "citation_count": 25,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/4da500f57577b4ee207ab80b4cde0e1ee338f948",
      "pdf_link": "",
      "venue": "IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "eb8dba325534da472170293b054596a17558c7f2",
      "title": "Few-Shot Graph Learning for Molecular Property Prediction",
      "abstract": "The recent success of graph neural networks has significantly boosted molecular property prediction, advancing activities such as drug discovery. The existing deep neural network methods usually require large training dataset for each property, impairing their performance in cases (especially for new molecular properties) with a limited amount of experimental data, which are common in real situations. To this end, we propose Meta-MGNN, a novel model for few-shot molecular property prediction. Meta-MGNN applies molecular graph neural network to learn molecular representations and builds a meta-learning framework for model optimization. To exploit unlabeled molecular information and address task heterogeneity of different molecular properties, Meta-MGNN further incorporates molecular structures, attribute based self-supervised modules and self-attentive task weights into the former framework, strengthening the whole learning model. Extensive experiments on two public multi-property datasets demonstrate that Meta-MGNN outperforms a variety of state-of-the-art methods.",
      "authors": [
        "Zhichun Guo",
        "Chuxu Zhang",
        "W. Yu",
        "John E. Herr",
        "O. Wiest",
        "Meng Jiang",
        "N. Chawla"
      ],
      "year": 2021,
      "citation_count": 188,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/eb8dba325534da472170293b054596a17558c7f2",
      "pdf_link": "",
      "venue": "The Web Conference",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "332c44793b70776b9b966128c52e694222b1ab73",
      "title": "A survey of deep meta-learning",
      "abstract": "Deep neural networks can achieve great successes when presented with large data sets and sufficient computational resources. However, their ability to learn new concepts quickly is limited. Meta-learning is one approach to address this issue, by enabling the network to learn how to learn. The field of Deep Meta-Learning advances at great speed, but lacks a unified, in-depth overview of current techniques. With this work, we aim to bridge this gap. After providing the reader with a theoretical foundation, we investigate and summarize key methods, which are categorized into (i) metric-, (ii) model-, and (iii) optimization-based techniques. In addition, we identify the main open challenges, such as performance evaluations on heterogeneous benchmarks, and reduction of the computational costs of meta-learning.",
      "authors": [
        "M. Huisman",
        "Jan N. van Rijn",
        "A. Plaat"
      ],
      "year": 2020,
      "citation_count": 356,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/332c44793b70776b9b966128c52e694222b1ab73",
      "pdf_link": "",
      "venue": "Artificial Intelligence Review",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "2bdebf2fb0f5c21907fcaae6d87c7ba5811e778a",
      "title": "Meta-Learning and Universality: Deep Representations and Gradient Descent can Approximate any Learning Algorithm",
      "abstract": "Learning to learn is a powerful paradigm for enabling models to learn from data more effectively and efficiently. A popular approach to meta-learning is to train a recurrent model to read in a training dataset as input and output the parameters of a learned model, or output predictions for new test inputs. Alternatively, a more recent approach to meta-learning aims to acquire deep representations that can be effectively fine-tuned, via standard gradient descent, to new tasks. In this paper, we consider the meta-learning problem from the perspective of universality, formalizing the notion of learning algorithm approximation and comparing the expressive power of the aforementioned recurrent models to the more recent approaches that embed gradient descent into the meta-learner. In particular, we seek to answer the following question: does deep representation combined with standard gradient descent have sufficient capacity to approximate any learning algorithm? We find that this is indeed true, and further find, in our experiments, that gradient-based meta-learning consistently leads to learning strategies that generalize more widely compared to those represented by recurrent models.",
      "authors": [
        "Chelsea Finn",
        "S. Levine"
      ],
      "year": 2017,
      "citation_count": 223,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/2bdebf2fb0f5c21907fcaae6d87c7ba5811e778a",
      "pdf_link": "",
      "venue": "International Conference on Learning Representations",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "d0eb13325d77e50a60102139e84484a9beaf62ff",
      "title": "A Comprehensive Overview and Survey of Recent Advances in Meta-Learning",
      "abstract": "This article reviews meta-learning also known as learning-to-learn which seeks rapid and accurate model adaptation to unseen tasks with applications in strong AI, few-shot learning, natural language processing and robotics. Unlike deep learning, meta-learning can be applied to few-shot high-dimensional datasets and considers further improving model generalization to unseen tasks. Deep learning is focused upon in-sample prediction and meta-learning concerns model adaptation for out-of-sample prediction. Meta-learning can continually perform self-improvement to achieve highly autonomous AI. Meta-learning may serve as an additional generalization block complementary for original deep learning model. Meta-learning seeks adaptation of machine learning models to unseen tasks which are vastly different from trained tasks. Meta-learning with coevolution between agent and environment provides solutions for complex tasks unsolvable by training from scratch. Meta-learning methodology covers a wide range of great minds and thoughts. We briefly summarize meta-learning methodologies into the following categories: black-box meta-learning, metric-based meta-learning, layered meta-learning and Bayesian meta-learning framework. Recent applications concentrate upon the integration of meta-learning with other machine learning framework to provide feasible integrated problem solutions. We briefly present recent meta-learning advances and discuss potential future research directions.",
      "authors": [
        "Huimin Peng"
      ],
      "year": 2020,
      "citation_count": 36,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/d0eb13325d77e50a60102139e84484a9beaf62ff",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "b2058b849f29e99ed4052e2d82b248acc4d6685f",
      "title": "Automatic Modulation Classification via Meta-Learning",
      "abstract": "Internet of Things (IoT) networks are often subject to many malicious attacks in untrusted environments, and automatic modulation classification (AMC) is an effective way to combat IoT physical-layer threats. However, most existing AMC methods assume sufficient labeled signals and invariant signal distribution, which is often impossible in untrusted environments. In this article, a new meta-learning method is proposed for a few-shot AMC with distribution bias. First, a multi-frequency octave ResNet (MFOR) is constructed to learn coarse (low-frequency) and fine (high-frequency) features, which can efficiently identify the modulation type of the signal while saving computational resources. Second, a large number of classification-related meta-tasks are established for training MFOR to explore general knowledge in signal classification, and then transfer it to the AMC. Different with deep neural networks (DNNs) that learn a mapping by multiple instances, the MFOR with meta-learning (denoted as M-MFOR) can improve the generalization ability of new AMC tasks with very few instances and distribution bias. Furthermore, we find that the distribution bias between data can be reduced by adjusting the normalized distribution and propose a class-related mixup. Extensive experiments are taken on several datasets to investigate the effectiveness of M-MFOR. The results show its feasibility and superiority over existing methods.",
      "authors": [
        "Xiaoyang Hao",
        "Zhixi Feng",
        "Shuyuan Yang",
        "Min Wang",
        "Licheng Jiao"
      ],
      "year": 2023,
      "citation_count": 29,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/b2058b849f29e99ed4052e2d82b248acc4d6685f",
      "pdf_link": "",
      "venue": "IEEE Internet of Things Journal",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "26b07c6309ef12034571f20973097691a22d7116",
      "title": "Meta-learning by the Baldwin effect",
      "abstract": "The scope of the Baldwin effect was recently called into question by two papers that closely examined the seminal work of Hinton and Nowlan. To this date there has been no demonstration of its necessity in empirically challenging tasks. Here we show that the Baldwin effect is capable of evolving few-shot supervised and reinforcement learning mechanisms, by shaping the hyperparameters and the initial parameters of deep learning algorithms. Furthermore it can genetically accommodate strong learning biases on the same set of problems as a recent machine learning algorithm called MAML \"Model Agnostic Meta-Learning\" which uses second-order gradients instead of evolution to learn a set of reference parameters (initial weights) that can allow rapid adaptation to tasks sampled from a distribution. Whilst in simple cases MAML is more data efficient than the Baldwin effect, the Baldwin effect is more general in that it does not require gradients to be backpropagated to the reference parameters or hyperparameters, and permits effectively any number of gradient updates in the inner loop. The Baldwin effect learns strong learning dependent biases, rather than purely genetically accommodating fixed behaviours in a learning independent manner.",
      "authors": [
        "Chrisantha Fernando",
        "Jakub Sygnowski",
        "Simon Osindero",
        "Jane X. Wang",
        "T. Schaul",
        "Denis Teplyashin",
        "P. Sprechmann",
        "A. Pritzel",
        "Andrei A. Rusu"
      ],
      "year": 2018,
      "citation_count": 40,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/26b07c6309ef12034571f20973097691a22d7116",
      "pdf_link": "",
      "venue": "Annual Conference on Genetic and Evolutionary Computation",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "42de54e614110c0c0a0bbbfee045e11e53eb4a7d",
      "title": "Deep Online Learning via Meta-Learning: Continual Adaptation for Model-Based RL",
      "abstract": "Humans and animals can learn complex predictive models that allow them to accurately and reliably reason about real-world phenomena, and they can adapt such models extremely quickly in the face of unexpected changes. Deep neural network models allow us to represent very complex functions, but lack this capacity for rapid online adaptation. The goal in this paper is to develop a method for continual online learning from an incoming stream of data, using deep neural network models. We formulate an online learning procedure that uses stochastic gradient descent to update model parameters, and an expectation maximization algorithm with a Chinese restaurant process prior to develop and maintain a mixture of models to handle non-stationary task distributions. This allows for all models to be adapted as necessary, with new models instantiated for task changes and old models recalled when previously seen tasks are encountered again. Furthermore, we observe that meta-learning can be used to meta-train a model such that this direct online adaptation with SGD is effective, which is otherwise not the case for large function approximators. In this work, we apply our meta-learning for online learning (MOLe) approach to model-based reinforcement learning, where adapting the predictive model is critical for control; we demonstrate that MOLe outperforms alternative prior methods, and enables effective continuous adaptation in non-stationary task distributions such as varying terrains, motor failures, and unexpected disturbances.",
      "authors": [
        "Anusha Nagabandi",
        "Chelsea Finn",
        "S. Levine"
      ],
      "year": 2018,
      "citation_count": 196,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/42de54e614110c0c0a0bbbfee045e11e53eb4a7d",
      "pdf_link": "",
      "venue": "International Conference on Learning Representations",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "5a8a079d30d40fc24565db7f1687d22dc323d24e",
      "title": "A new meta-transfer learning method with freezing operation for few-shot bearing fault diagnosis",
      "abstract": "Deep learning for bearing fault diagnosis often requires a large quantity of comprehensive data to give support in the field of rotating machinery fault diagnosis. However, large-quantity datasets for model training are difficult to obtain in actual working environments. Therefore, bearing fault diagnosis problems under practical working conditions are often considered few-shot problems. Meta-learning can be adopted to solve these few-shot problems. Traditional meta-learning methods, however, can lead to model overfitting, and shallow neural networks are usually used to avoid overfitting. As a result, the features extracted by the shallow neural network are insufficiently rich to exploit the optimal performance of the model. A few-shot fault diagnosis method based on meta-learning, named meta-transfer learning with freezing operation (MTLFO), is proposed in this study to solve these problems. MTLFO can learn new knowledge rapidly through a small number of samples. The hyperparameter self-regulation ability of meta-learning is adopted by MTLFO, and a freezing operation is used to deal with the neuronal nature of meta-learning to ensure that the neurons from different tasks are transferred by utilizing scaling and shifting. MTLFO avoids the overfitting problem in traditional meta-learning methods and presents more advantages in solving few-shot problems in fault diagnosis compared with other types of methods.",
      "authors": [
        "Peiqi Wang",
        "Jingde Li",
        "Shubei Wang",
        "Fusheng Zhang",
        "Juanjuan Shi",
        "Changqing Shen"
      ],
      "year": 2023,
      "citation_count": 21,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/5a8a079d30d40fc24565db7f1687d22dc323d24e",
      "pdf_link": "",
      "venue": "Measurement science and technology",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "b1493cac304d3fea710b375fa09e4b943a8a7de9",
      "title": "Meta Continual Learning",
      "abstract": "Using neural networks in practical settings would benefit from the ability of the networks to learn new tasks throughout their lifetimes without forgetting the previous tasks. This ability is limited in the current deep neural networks by a problem called catastrophic forgetting, where training on new tasks tends to severely degrade performance on previous tasks. One way to lessen the impact of the forgetting problem is to constrain parameters that are important to previous tasks to stay close to the optimal parameters. Recently, multiple competitive approaches for computing the importance of the parameters with respect to the previous tasks have been presented. In this paper, we propose a learning to optimize algorithm for mitigating catastrophic forgetting. Instead of trying to formulate a new constraint function ourselves, we propose to train another neural network to predict parameter update steps that respect the importance of parameters to the previous tasks. In the proposed meta-training scheme, the update predictor is trained to minimize loss on a combination of current and past tasks. We show experimentally that the proposed approach works in the continual learning setting.",
      "authors": [
        "Risto Vuorio",
        "D.-Y. Cho",
        "Daejoong Kim",
        "Jiwon Kim"
      ],
      "year": 2018,
      "citation_count": 28,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/b1493cac304d3fea710b375fa09e4b943a8a7de9",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "a38500c3448189abd05e72e35332224b96e24a32",
      "title": "Overcoming Catastrophic Forgetting Using Sparse Coding and Meta Learning",
      "abstract": "Continuous learning occurs naturally in human beings. However, Deep Learning methods suffer from a problem known as Catastrophic Forgetting (CF) that consists of a model drastically decreasing its performance on previously learned tasks when it is sequentially trained on new tasks. This situation, known as task interference, occurs when a network modifies relevant weight values as it learns a new task. In this work, we propose two main strategies to face the problem of task interference in convolutional neural networks. First, we use a sparse coding technique to adaptively allocate model capacity to different tasks avoiding interference between them. Specifically, we use a strategy based on group sparse regularization to specialize groups of parameters to learn each task. Afterward, by adding binary masks, we can freeze these groups of parameters, using the rest of the network to learn new tasks. Second, we use a meta learning technique to foster knowledge transfer among tasks, encouraging weight reusability instead of overwriting. Specifically, we use an optimization strategy based on episodic training to foster learning weights that are expected to be useful to solve future tasks. Together, these two strategies help us to avoid interference by preserving compatibility with previous and future weight values. Using this approach, we achieve state-of-the-art results on popular benchmarks used to test techniques to avoid CF. In particular, we conduct an ablation study to identify the contribution of each component of the proposed method, demonstrating its ability to avoid retroactive interference with previous tasks and to promote knowledge transfer to future tasks.",
      "authors": [
        "J. Hurtado",
        "Hans Lobel",
        "Álvaro Soto"
      ],
      "year": 2021,
      "citation_count": 13,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/a38500c3448189abd05e72e35332224b96e24a32",
      "pdf_link": "",
      "venue": "IEEE Access",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "b1c2df70f8c287c98e8735d82185bdadf2d4d24b",
      "title": "FMGCN: Federated Meta Learning-Augmented Graph Convolutional Network for EV Charging Demand Forecasting",
      "abstract": "Recent booming successes of electric vehicles (EVs) motivate emerging exploration of spatio-temporal (ST) EV charging demand forecasting to inform policy making. Recent studies have contributed to remarkable accuracy improvement by developing deep learning methods. However, when they access massive amounts of data and frequently exchange data through the Internet of Things (IoT), data silos and inefficient training emerge as main challenges. To tackle these challenges, this study proposes an integrated approach for regional EV charging demand forecasting, named federated meta learning-based graph convolutional network, which consists of two modules, namely, 1) ST learning module, which introduces spatial and temporal attentions to capture the underlying charging patterns between different regions and cities effectively and 2) distributed pretraining module, which incorporates federated learning and meta-learning to enhance the adaptivity and generalisability of the forecasting model. A comprehensive evaluation based on a real-world data set of 25246 public EV charging piles shows that the proposed model outperforms other representative models with 1) an average improvement of 29.9% in forecasting errors; 2) an acceleration of 65% in convergence speed; and 3) a sound adaptability to support varying charging demand.",
      "authors": [
        "Linlin You",
        "Qiyang Chen",
        "Haohao Qu",
        "Rui Zhu",
        "Jinyue Yan",
        "Paolo Santi",
        "Carlo Ratti"
      ],
      "year": 2024,
      "citation_count": 13,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/b1c2df70f8c287c98e8735d82185bdadf2d4d24b",
      "pdf_link": "",
      "venue": "IEEE Internet of Things Journal",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "b237deb6c0234378238a6ee49b229b1299b7efe6",
      "title": "LeMON: Learning to Learn Multi-Operator Networks",
      "abstract": "Single-operator learning involves training a deep neural network to learn a specific operator, whereas recent work in multi-operator learning uses an operator embedding structure to train a single neural network on data from multiple operators. Thus, multi-operator learning is capable of predicting a range of operators within one model. In this work, we propose pretraining and fine-tuning strategies for solving PDEs using multi-operator learning. One key aspect is that by increasing the number of families of operators used in pretraining, a PDE foundation model can be fine-tuned to downstream tasks involving new PDEs with a limited number of samples, thus outperforming single operator neural networks. Specifically, a multi-operator learning model pre-trained with data from diverse PDE families can predict unseen operators after fine-tuning with only a limited number of operators from the new family, enabling them to serve as a data-free PDE solver. We also show that the proposed training and fine-tuning method is able to predict new operators in zero-shot prediction without samples. Additionally, we introduce a PDE-agnostic meta-learning algorithm to improve the adaptability of the model to various PDEs by providing a better parameter initialization process. To address the needs of applications with limited computing resources, we explore low-rank adaptation methods that reduce computational costs while enhancing solver accuracy. Lastly, by examining the scaling law with respect to the number of operator families, we establish and highlight its potential for broad adaptation in PDE-solving tasks.",
      "authors": [
        "Jingmin Sun",
        "Zecheng Zhang",
        "Hayden Schaeffer"
      ],
      "year": 2024,
      "citation_count": 8,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/b237deb6c0234378238a6ee49b229b1299b7efe6",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "3b32351004d1628329b875576323a7b1767e9e5a",
      "title": "AFM3D: An Asynchronous Federated Meta-Learning Framework for Driver Distraction Detection",
      "abstract": "Driver Distraction Detection (3D) is of great significance in helping intelligent vehicles decide whether to remind drivers or take over the driving task and avoid traffic accidents. However, the current centralized learning paradigm of 3D has become unpractical because of rising limitations on data sharing and increasing concerns about user privacy. In this context, 3D is further facing three emerging challenges, namely data islands, data heterogeneity, and the straggler issue. To jointly address these three issues and make the 3D model training and deployment more practical and efficient, this paper proposes an Asynchronous Federated Meta-learning framework called AFM3D. Specifically, AFM3D bridges data islands through Federated Learning (FL), a novel distributed learning paradigm that enables multiple clients (i.e., private vehicles with individual data of drivers) to learn a global model collaboratively without data exchange. Moreover, AFM3D further utilizes meta-learning to tackle data heterogeneity by training a meta-model that can adapt to new driver data quickly with satisfactory performance. Finally, AFM3D is designed to operate in an asynchronous mode to reduce delays caused by stragglers and achieve efficient learning. A temporally weighted aggregation strategy is also designed to handle stale models commonly encountered in the asynchronous mode and in turn, optimize the aggregation direction. Extensive experiment results show that AFM3D can boost performance in terms of model accuracy, recall, F1 score, test loss, and learning speed by 7.61%, 7.44%, 7.95%, 9.95%, and 50.91%, respectively, against five state-of-the-art methods.",
      "authors": [
        "Sheng Liu",
        "Linlin You",
        "Rui Zhu",
        "Bing Liu",
        "Rui Liu",
        "Han Yu",
        "Chau Yuen"
      ],
      "year": 2024,
      "citation_count": 7,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/3b32351004d1628329b875576323a7b1767e9e5a",
      "pdf_link": "",
      "venue": "IEEE transactions on intelligent transportation systems (Print)",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "557e9371711c7409c78c96a6a2bea290a28cb365",
      "title": "MetaIQA: Deep Meta-Learning for No-Reference Image Quality Assessment",
      "abstract": "Recently, increasing interest has been drawn in exploiting deep convolutional neural networks (DCNNs) for no-reference image quality assessment (NR-IQA). Despite of the notable success achieved, there is a broad consensus that training DCNNs heavily relies on massive annotated data. Unfortunately, IQA is a typical small sample problem. Therefore, most of the existing DCNN-based IQA metrics operate based on pre-trained networks. However, these pre-trained networks are not designed for IQA task, leading to generalization problem when evaluating different types of distortions. With this motivation, this paper presents a no-reference IQA metric based on deep meta-learning. The underlying idea is to learn the meta-knowledge shared by human when evaluating the quality of images with various distortions, which can then be adapted to unknown distortions easily. Specifically, we first collect a number of NR-IQA tasks for different distortions. Then meta-learning is adopted to learn the prior knowledge shared by diversified distortions. Finally, the quality prior model is fine-tuned on a target NR-IQA task for quickly obtaining the quality model. Extensive experiments demonstrate that the proposed metric outperforms the state-of-the-arts by a large margin. Furthermore, the meta-model learned from synthetic distortions can also be easily generalized to authentic distortions, which is highly desired in real-world applications of IQA metrics.",
      "authors": [
        "Hancheng Zhu",
        "Leida Li",
        "Jinjian Wu",
        "W. Dong",
        "Guangming Shi"
      ],
      "year": 2020,
      "citation_count": 304,
      "layer": 1,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/557e9371711c7409c78c96a6a2bea290a28cb365",
      "pdf_link": "",
      "venue": "Computer Vision and Pattern Recognition",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "eeb0407b2f47857fe7b44c948c08ef23469a8ad2",
      "title": "An Adversarial Meta-Training Framework for Cross-Domain Few-Shot Learning",
      "abstract": "Meta-learning provides a promising way for deep learning models to efficiently learn in few-shot learning. With this capacity, many deep learning systems can be applied in many real applications. However, many existing meta-learning based few-shot learning systems suffer from vulnerable generalization when new tasks are from unseen domains (a.k.a, cross-domain few-shot learning). In this work, we consider this problem from the perspective of designing a model-agnostic meta-training framework to improve the generalization of existing meta-learning methods in cross-domain few-shot learning. In this way, compared with focusing on elaborately designing modules for a specific meta-learning model, our method is endowed with the ability to be compatible with different meta-learning models in various few-shot problems. To achieve this goal, a novel adversarial meta-training framework is proposed. The proposed framework utilizes max-min episodic iteration. In the episode of maximization, our framework focuses on how to dynamically generate appropriate pseudo tasks which benefit learning cross-domain knowledge. In the episode of minimization, our method aims to solve how to help meta-learning model learn cross-task and robust meta-knowledge. To comprehensively evaluate our framework, experiments are conducted on two few-shot learning settings, three meta-learning models, and eight datasets. These results demonstrate that our method is applicable to various meta-learning models in different few-shot learning problems. The superiority of our method is verified compared with existing state-of-the-art methods.",
      "authors": [
        "Pinzhuo Tian",
        "Shaorong Xie"
      ],
      "year": 2023,
      "citation_count": 19,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/eeb0407b2f47857fe7b44c948c08ef23469a8ad2",
      "pdf_link": "",
      "venue": "IEEE transactions on multimedia",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "71c00beb70d83eab08f1cf6c32f48c112bd9bfdf",
      "title": "Towards Robust and Interpretable EMG-based Hand Gesture Recognition using Deep Metric Meta Learning",
      "abstract": "Current electromyography (EMG) pattern recognition (PR) models have been shown to generalize poorly in unconstrained environments, setting back their adoption in applications such as hand gesture control. This problem is often due to limited training data, exacerbated by the use of supervised classification frameworks that are known to be suboptimal in such settings. In this work, we propose a shift to deep metric-based meta-learning in EMG PR to supervise the creation of meaningful and interpretable representations. We use a Siamese Deep Convolutional Neural Network (SDCNN) and contrastive triplet loss to learn an EMG feature embedding space that captures the distribution of the different classes. A nearest-centroid approach is subsequently employed for inference, relying on how closely a test sample aligns with the established data distributions. We derive a robust class proximity-based confidence estimator that leads to a better rejection of incorrect decisions, i.e. false positives, especially when operating beyond the training data domain. We show our approach's efficacy by testing the trained SDCNN's predictions and confidence estimations on unseen data, both in and out of the training domain. The evaluation metrics include the accuracy-rejection curve and the Kullback-Leibler divergence between the confidence distributions of accurate and inaccurate predictions. Outperforming comparable models on both metrics, our results demonstrate that the proposed meta-learning approach improves the classifier's precision in active decisions (after rejection), thus leading to better generalization and applicability.",
      "authors": [
        "S. Tam",
        "S. T. P. Raghu",
        "Étienne Buteau",
        "Erik J. Scheme",
        "Mounir Boukadoum",
        "Alexandre Campeau-Lecours",
        "Benoit Gosselin"
      ],
      "year": 2024,
      "citation_count": 4,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/71c00beb70d83eab08f1cf6c32f48c112bd9bfdf",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "2e4316e7c38373d068f8ff55f26ff83dfc4238b8",
      "title": "Learning to Learn From Noisy Labeled Data",
      "abstract": "Despite the success of deep neural networks (DNNs) in image classification tasks, the human-level performance relies on massive training data with high-quality manual annotations, which are expensive and time-consuming to collect. There exist many inexpensive data sources on the web, but they tend to contain inaccurate labels. Training on noisy labeled datasets causes performance degradation because DNNs can easily overfit to the label noise. To overcome this problem, we propose a noise-tolerant training algorithm, where a meta-learning update is performed prior to conventional gradient update. The proposed meta-learning method simulates actual training by generating synthetic noisy labels, and train the model such that after one gradient update using each set of synthetic noisy labels, the model does not overfit to the specific noise. We conduct extensive experiments on the noisy CIFAR-10 dataset and the Clothing1M dataset. The results demonstrate the advantageous performance of the proposed method compared to several state-of-the-art baselines.",
      "authors": [
        "Junnan Li",
        "Yongkang Wong",
        "Qi Zhao",
        "Mohan Kankanhalli"
      ],
      "year": 2018,
      "citation_count": 338,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/2e4316e7c38373d068f8ff55f26ff83dfc4238b8",
      "pdf_link": "",
      "venue": "Computer Vision and Pattern Recognition",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "d8d680aea59295c020b9d53d78dd8d954a876845",
      "title": "Meta-Transfer Learning for Few-Shot Learning",
      "abstract": "Meta-learning has been proposed as a framework to address the challenging few-shot learning setting. The key idea is to leverage a large number of similar few-shot tasks in order to learn how to adapt a base-learner to a new task for which only a few labeled samples are available. As deep neural networks (DNNs) tend to overfit using a few samples only, meta-learning typically uses shallow neural networks (SNNs), thus limiting its effectiveness. In this paper we propose a novel few-shot learning method called meta-transfer learning (MTL) which learns to adapt a deep NN for few shot learning tasks. Specifically, \"meta\" refers to training multiple tasks, and \"transfer\" is achieved by learning scaling and shifting functions of DNN weights for each task. In addition, we introduce the hard task (HT) meta-batch scheme as an effective learning curriculum for MTL. We conduct experiments using (5-class, 1-shot) and (5-class, 5-shot) recognition tasks on two challenging few-shot learning benchmarks: miniImageNet and Fewshot-CIFAR100. Extensive comparisons to related works validate that our meta-transfer learning approach trained with the proposed HT meta-batch scheme achieves top performance. An ablation study also shows that both components contribute to fast convergence and high accuracy.",
      "authors": [
        "Qianru Sun",
        "Yaoyao Liu",
        "Tat-Seng Chua",
        "B. Schiele"
      ],
      "year": 2018,
      "citation_count": 1112,
      "layer": 1,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/d8d680aea59295c020b9d53d78dd8d954a876845",
      "pdf_link": "",
      "venue": "Computer Vision and Pattern Recognition",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "759ae1234d46e2d1399ce9d642724738a766ed22",
      "title": "Meta-Gradient Reinforcement Learning with an Objective Discovered Online",
      "abstract": "Deep reinforcement learning includes a broad family of algorithms that parameterise an internal representation, such as a value function or policy, by a deep neural network. Each algorithm optimises its parameters with respect to an objective, such as Q-learning or policy gradient, that defines its semantics. In this work, we propose an algorithm based on meta-gradient descent that discovers its own objective, flexibly parameterised by a deep neural network, solely from interactive experience with its environment. Over time, this allows the agent to learn how to learn increasingly effectively. Furthermore, because the objective is discovered online, it can adapt to changes over time. We demonstrate that the algorithm discovers how to address several important issues in RL, such as bootstrapping, non-stationarity, and off-policy learning. On the Atari Learning Environment, the meta-gradient algorithm adapts over time to learn with greater efficiency, eventually outperforming the median score of a strong actor-critic baseline.",
      "authors": [
        "Zhongwen Xu",
        "H. V. Hasselt",
        "Matteo Hessel",
        "Junhyuk Oh",
        "Satinder Singh",
        "David Silver"
      ],
      "year": 2020,
      "citation_count": 79,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/759ae1234d46e2d1399ce9d642724738a766ed22",
      "pdf_link": "",
      "venue": "Neural Information Processing Systems",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "7d0216a7331ee4031fe488c8ff1da2adfcc59a0c",
      "title": "Meta-Learning for Multi-Family Android Malware Classification",
      "abstract": "With the emergence of smartphones, Android has become a widely used mobile operating system. However, it is vulnerable when encountering various types of attacks. Every day, new malware threatens the security of users’ devices and private data. Many methods have been proposed to classify malicious applications, utilizing static or dynamic analysis for classification. However, previous methods still suffer from unsatisfactory performance due to two challenges. First, they are unable to address the imbalanced data distribution problem, leading to poor performance for malware families with few members. Second, they are unable to address the zero-day malware (zero-day malware refers to malicious applications that exploit unknown vulnerabilities) classification problem. In this article, we introduce an innovative meta-learning approach for multi-family Android malware classification named Meta-MAMC, which uses meta-learning technology to learn meta-knowledge (i.e., the similarities and differences among different malware families) of few-family samples and combines new sampling algorithms to solve the above challenges. Meta-MAMC integrates (i) the meta-knowledge contained within the dataset to guide models in learning to identify unknown malware; and (ii) more accurate and diverse tasks based on novel sampling strategies, as well as directly adapting meta-learning to a new few-sample and zero-sample task to classify families. We have evaluated Meta-MAMC on two popular datasets and a corpus of real-world Android applications. The results demonstrate its efficacy in accurately classifying malicious applications belonging to certain malware families, even achieving 100% classification in some families.",
      "authors": [
        "Yao Li",
        "Dawei Yuan",
        "Tao Zhang",
        "Haipeng Cai",
        "David Lo",
        "Cuiyun Gao",
        "Xiapu Luo",
        "He Jiang"
      ],
      "year": 2024,
      "citation_count": 9,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/7d0216a7331ee4031fe488c8ff1da2adfcc59a0c",
      "pdf_link": "",
      "venue": "ACM Transactions on Software Engineering and Methodology",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "f6271880cc1d7ff6514672366fe124fdb1212fb2",
      "title": "Learning to Learn with Feedback and Local Plasticity",
      "abstract": "Interest in biologically inspired alternatives to backpropagation is driven by the desire to both advance connections between deep learning and neuroscience and address backpropagation's shortcomings on tasks such as online, continual learning. However, local synaptic learning rules like those employed by the brain have so far failed to match the performance of backpropagation in deep networks. In this study, we employ meta-learning to discover networks that learn using feedback connections and local, biologically inspired learning rules. Importantly, the feedback connections are not tied to the feedforward weights, avoiding biologically implausible weight transport. Our experiments show that meta-trained networks effectively use feedback connections to perform online credit assignment in multi-layer architectures. Surprisingly, this approach matches or exceeds a state-of-the-art gradient-based online meta-learning algorithm on regression and classification tasks, excelling in particular at continual learning. Analysis of the weight updates employed by these models reveals that they differ qualitatively from gradient descent in a way that reduces interference between updates. Our results suggest the existence of a class of biologically plausible learning mechanisms that not only match gradient descent-based learning, but also overcome its limitations.",
      "authors": [
        "Jack W Lindsey",
        "Ashok Litwin-Kumar"
      ],
      "year": 2020,
      "citation_count": 33,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/f6271880cc1d7ff6514672366fe124fdb1212fb2",
      "pdf_link": "",
      "venue": "Neural Information Processing Systems",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "d726e991e68ed892bd4c42c8c8150ebc71ae1b9e",
      "title": "Towards Real Unsupervised Anomaly Detection Via Confident Meta-Learning",
      "abstract": "So-called unsupervised anomaly detection is better described as semi-supervised, as it assumes all training data are nominal. This assumption simplifies training but requires manual data curation, introducing bias and limiting adaptability. We propose Confident Meta-learning (CoMet), a novel training strategy that enables deep anomaly detection models to learn from uncurated datasets where nominal and anomalous samples coexist, eliminating the need for explicit filtering. Our approach integrates Soft Confident Learning, which assigns lower weights to low-confidence samples, and Meta-Learning, which stabilizes training by regularizing updates based on training validation loss covariance. This prevents overfitting and enhances robustness to noisy data. CoMet is model-agnostic and can be applied to any anomaly detection method trainable via gradient descent. Experiments on MVTec-AD, VIADUCT, and KSDD2 with two state-of-the-art models demonstrate the effectiveness of our approach, consistently improving over the baseline methods, remaining insensitive to anomalies in the training set, and setting a new state-of-the-art across all datasets.",
      "authors": [
        "Muhammad Aqeel",
        "Shakiba Sharifi",
        "Marco Cristani",
        "Francesco Setti"
      ],
      "year": 2025,
      "citation_count": 5,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/d726e991e68ed892bd4c42c8c8150ebc71ae1b9e",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "0e8827f439152cf1f5670a3ab391db6148abc7e0",
      "title": "MetaLoc: Learning to Learn Indoor RSS Fingerprinting Localization over Multiple Scenarios",
      "abstract": "The existing indoor fingerprinting methods based on received signal strength (RSS) are rather accurate after intensive offline calibration for a specific scenario, but the well-calibrated localization model (can be a pure statistical one or a data-driven one) will present poor generalization ability in a new scenario, which results in big loss in knowledge and human effort. To break the scenario-specific localization bottleneck, we propose a new-fashioned data-driven fingerprinting method for localization based on meta-learning, named by MetaLoc, that can adapt itself rapidly to a new, possibly unseen, scenario with very little calibration work. Specifically, the underlying localization model is taken to be a deep neural network (NN), and we train an optimal set of group-specific meta-parameters by leveraging historical data collected from diverse well-calibrated indoor scenarios and the maximum mean discrepancy criterion. Simulation results confirm that the meta-parameters obtained for MetaLoc achieves very rapid adaptation to new scenarios, competitive localization accuracy, and high resistance to significantly reduced reference points (RPs), saving a lot of calibration effort.",
      "authors": [
        "Jun Gao",
        "Ceyao Zhang",
        "Qinglei Kong",
        "F. Yin",
        "Lexi Xu",
        "K. Niu"
      ],
      "year": 2022,
      "citation_count": 19,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/0e8827f439152cf1f5670a3ab391db6148abc7e0",
      "pdf_link": "",
      "venue": "ICC 2022 - IEEE International Conference on Communications",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "1d421d179a2520ba23dc1375fe2989e4ba79b437",
      "title": "MetaFlux: Meta-learning global carbon fluxes from sparse spatiotemporal observations",
      "abstract": "We provide a global, long-term carbon flux dataset of gross primary production and ecosystem respiration generated using meta-learning, called MetaFlux . The idea behind meta-learning stems from the need to learn efficiently given sparse data by learning how to learn broad features across tasks to better infer other poorly sampled ones. Using meta-trained ensemble of deep models, we generate global carbon products on daily and monthly timescales at a 0.25-degree spatial resolution from 2001 to 2021, through a combination of reanalysis and remote-sensing products. Site-level validation finds that MetaFlux ensembles have lower validation error by 5–7% compared to their non-meta-trained counterparts. In addition, they are more robust to extreme observations, with 4–24% lower errors. We also checked for seasonality, interannual variability, and correlation to solar-induced fluorescence of the upscaled product and found that MetaFlux outperformed other machine-learning based carbon product, especially in the tropics and semi-arids by 10–40%. Overall, MetaFlux can be used to study a wide range of biogeochemical processes.",
      "authors": [
        "Juan Nathaniel",
        "Jiangong Liu",
        "P. Gentine"
      ],
      "year": 2023,
      "citation_count": 25,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/1d421d179a2520ba23dc1375fe2989e4ba79b437",
      "pdf_link": "",
      "venue": "Scientific Data",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "208cd4b25768f0096fb2e80e7690473da0e2a563",
      "title": "Meta-learning with differentiable closed-form solvers",
      "abstract": "Adapting deep networks to new concepts from a few examples is challenging, due to the high computational requirements of standard fine-tuning procedures. Most work on few-shot learning has thus focused on simple learning techniques for adaptation, such as nearest neighbours or gradient descent. Nonetheless, the machine learning literature contains a wealth of methods that learn non-deep models very efficiently. In this paper, we propose to use these fast convergent methods as the main adaptation mechanism for few-shot learning. The main idea is to teach a deep network to use standard machine learning tools, such as ridge regression, as part of its own internal model, enabling it to quickly adapt to novel data. This requires back-propagating errors through the solver steps. While normally the cost of the matrix operations involved in such a process would be significant, by using the Woodbury identity we can make the small number of examples work to our advantage. We propose both closed-form and iterative solvers, based on ridge regression and logistic regression components. Our methods constitute a simple and novel approach to the problem of few-shot learning and achieve performance competitive with or superior to the state of the art on three benchmarks.",
      "authors": [
        "Luca Bertinetto",
        "João F. Henriques",
        "Philip H. S. Torr",
        "A. Vedaldi"
      ],
      "year": 2018,
      "citation_count": 962,
      "layer": 1,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/208cd4b25768f0096fb2e80e7690473da0e2a563",
      "pdf_link": "",
      "venue": "International Conference on Learning Representations",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "37a349a7a46a9339cb59ac02f81d3848a62d3885",
      "title": "Learning to Continually Learn with the Bayesian Principle",
      "abstract": "In the present era of deep learning, continual learning research is mainly focused on mitigating forgetting when training a neural network with stochastic gradient descent on a non-stationary stream of data. On the other hand, in the more classical literature of statistical machine learning, many models have sequential Bayesian update rules that yield the same learning outcome as the batch training, i.e., they are completely immune to catastrophic forgetting. However, they are often overly simple to model complex real-world data. In this work, we adopt the meta-learning paradigm to combine the strong representational power of neural networks and simple statistical models' robustness to forgetting. In our novel meta-continual learning framework, continual learning takes place only in statistical models via ideal sequential Bayesian update rules, while neural networks are meta-learned to bridge the raw data and the statistical models. Since the neural networks remain fixed during continual learning, they are protected from catastrophic forgetting. This approach not only achieves significantly improved performance but also exhibits excellent scalability. Since our approach is domain-agnostic and model-agnostic, it can be applied to a wide range of problems and easily integrated with existing model architectures.",
      "authors": [
        "Soochan Lee",
        "Hyeonseong Jeon",
        "Jaehyeon Son",
        "Gunhee Kim"
      ],
      "year": 2024,
      "citation_count": 5,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/37a349a7a46a9339cb59ac02f81d3848a62d3885",
      "pdf_link": "",
      "venue": "International Conference on Machine Learning",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "a968524df2c59fb0ed8892603546f55b731d6439",
      "title": "Ada-VSR: Adaptive Video Super-Resolution with Meta-Learning",
      "abstract": "Most of the existing works in supervised spatio-temporal video super-resolution (STVSR) heavily rely on a large-scale external dataset consisting of paired low-resolution low-frame rate (LR-LFR) and high-resolution high-frame-rate (HR-HFR) videos. Despite their remarkable performance, these methods make a prior assumption that the low-resolution video is obtained by down-scaling the high-resolution video using a known degradation kernel, which does not hold in practical settings. Another problem with these methods is that they cannot exploit instance-specific internal information of a video at testing time. Recently, deep internal learning approaches have gained attention due to their ability to utilize the instance-specific statistics of a video. However, these methods have a large inference time as they require thousands of gradient updates to learn the intrinsic structure of the data. In this work, we present Adaptive VideoSuper-Resolution (Ada-VSR) which leverages external, as well as internal, information through meta-transfer learning and internal learning, respectively. Specifically, meta-learning is employed to obtain adaptive parameters, using a large-scale external dataset, that can adapt quickly to the novel condition (degradation model) of the given test video during the internal learning task, thereby exploiting external and internal information of a video for super-resolution. The model trained using our approach can quickly adapt to a specific video condition with only a few gradient updates, which reduces the inference time significantly. Extensive experiments on standard datasets demonstrate that our method performs favorably against various state-of-the-art approaches.",
      "authors": [
        "Akash Gupta",
        "P. Jonnalagedda",
        "B. Bhanu",
        "A. Roy-Chowdhury"
      ],
      "year": 2021,
      "citation_count": 7,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/a968524df2c59fb0ed8892603546f55b731d6439",
      "pdf_link": "",
      "venue": "ACM Multimedia",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "756b3e51e8ac2951bfd7d5b5322f1502442eab8e",
      "title": "Few Shot Scene Classification in Remote Sensing using Meta-Agnostic Machine",
      "abstract": "Scene classification has become an important research topic in remote sensing (RS) field. The typical solution relies on labeling a large enough set of the RS scenes manually using expert opinion, then training the algorithm on this set to learn how to classify other new scenes correctly. The best performance deep learning models required a large labeled dataset for training. Accordingly, there is a great need to develop an intelligent machine learning algorithm that can learn to classify RS datasets containing new unseen classes from a few labeled samples only, which is known as few-shot machine learning. In this work, we develop a deep few-shot learning method for the classification of RS scenes. The proposed method is based on Model Agnostic Meta-Learning (MAML), one of the recently introduced and most popular meta-learning algorithms. In this paper, we report preliminary results using the UC Merced, OPTIMAL-31, and AID RS scene datasets.",
      "authors": [
        "D. Alajaji",
        "H. Alhichri"
      ],
      "year": 2020,
      "citation_count": 23,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/756b3e51e8ac2951bfd7d5b5322f1502442eab8e",
      "pdf_link": "",
      "venue": "2020 6th Conference on Data Science and Machine Learning Applications (CDMA)",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "fe10bf13aeb8728a955f1f8fd312ce77773b59ec",
      "title": "MetaABR: A Meta-Learning Approach on Adaptative Bitrate Selection for Video Streaming",
      "abstract": "Video streaming is one of the most popular Internet applications that makes up a large amount of Internet traffic. A fundamental mechanism in video streaming is adaptive bitrate (ABR) selection which decides the proper compression level for each chunk of a video to optimize the users’ quality of experience (QoE). The existing ABR algorithms require significant tuning and do not generalize to diverse network conditions and personalized QoE objectives. In this article, we propose a novel framework for meta-learning based ABR design and discuss challenges of deploying learning based ABR mechanism in real-world video streaming systems. We utilize the proposed framework to design MetaABR, a novel adaptive bitrate selection algorithm based on meta-reinforcement learning to maximize users’ QoE. By jointly training multiple learning tasks with a shared meta-critic, it can provide transferrable meta-knowledge to supervise bitrate selection across tasks, and can be applied to efficiently learn a new task in unseen environment with only a few trials. We implement MetaABR on an emulation platform which connects to the Linux network protocol stack through virtual network interfaces. Extensive experiments based on real-world traces and wireless testbed show that MetaABR achieves the best comprehensive QoE compared with the state-of-the-art ABR algorithms in a variety of network environments.",
      "authors": [
        "Wenzhong Li",
        "Xiang Li",
        "Yeting Xu",
        "Yezhou Yang",
        "Sanglu Lu"
      ],
      "year": 2024,
      "citation_count": 20,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/fe10bf13aeb8728a955f1f8fd312ce77773b59ec",
      "pdf_link": "",
      "venue": "IEEE Transactions on Mobile Computing",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "7f567f1e8972ff31a7ced59c329e7d75da645baf",
      "title": "Learning to Explore with Meta-Policy Gradient",
      "abstract": "The performance of off-policy learning, including deep Q-learning and deep deterministic policy gradient (DDPG), critically depends on the choice of the exploration policy. Existing exploration methods are mostly based on adding noise to the on-going actor policy and can only explore \\emph{local} regions close to what the actor policy dictates. In this work, we develop a simple meta-policy gradient algorithm that allows us to adaptively learn the exploration policy in DDPG. Our algorithm allows us to train flexible exploration behaviors that are independent of the actor policy, yielding a \\emph{global exploration} that significantly speeds up the learning process. With an extensive study, we show that our method significantly improves the sample-efficiency of DDPG on a variety of reinforcement learning tasks.",
      "authors": [
        "Tianbing Xu",
        "Qiang Liu",
        "Liang Zhao",
        "Jian Peng"
      ],
      "year": 2018,
      "citation_count": 54,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/7f567f1e8972ff31a7ced59c329e7d75da645baf",
      "pdf_link": "",
      "venue": "International Conference on Machine Learning",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "754878242a3b480b2ca9031bff623f2c557f2caa",
      "title": "Deep Interactive Bayesian Reinforcement Learning via Meta-Learning",
      "abstract": "Agents that interact with other agents often do not know a priori what the other agents' strategies are, but have to maximise their own online return while interacting with and learning about others. The optimal adaptive behaviour under uncertainty over the other agents' strategies w.r.t. some prior can in principle be computed using the Interactive Bayesian Reinforcement Learning framework. Unfortunately, doing so is intractable in most settings, and existing approximation methods are restricted to small tasks. To overcome this, we propose to meta-learn approximate belief inference and Bayes-optimal behaviour for a given prior. To model beliefs over other agents, we combine sequential and hierarchical Variational Auto-Encoders, and meta-train this inference model alongside the policy. We show empirically that our approach outperforms existing methods that use a model-free approach, sample from the approximate posterior, maintain memory-free models of others, or do not fully utilise the known structure of the environment.",
      "authors": [
        "Luisa M. Zintgraf",
        "Sam Devlin",
        "K. Ciosek",
        "S. Whiteson",
        "Katja Hofmann"
      ],
      "year": 2021,
      "citation_count": 48,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/754878242a3b480b2ca9031bff623f2c557f2caa",
      "pdf_link": "",
      "venue": "Adaptive Agents and Multi-Agent Systems",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "5af8c7c650e9ec50d91a16be287ce54b16075fe7",
      "title": "A Tutorial on Meta-Reinforcement Learning",
      "abstract": "While deep reinforcement learning (RL) has fueled multiple high-profile successes in machine learning, it is held back from more widespread adoption by its often poor data efficiency and the limited generality of the policies it produces. A promising approach for alleviating these limitations is to cast the development of better RL algorithms as a machine learning problem itself in a process called meta-RL. Meta-RL is most commonly studied in a problem setting where, given a distribution of tasks, the goal is to learn a policy that is capable of adapting to any new task from the task distribution with as little data as possible. In this survey, we describe the meta-RL problem setting in detail as well as its major variations. We discuss how, at a high level, meta-RL research can be clustered based on the presence of a task distribution and the learning budget available for each individual task. Using these clusters, we then survey meta-RL algorithms and applications. We conclude by presenting the open problems on the path to making meta-RL part of the standard toolbox for a deep RL practitioner.",
      "authors": [
        "Jacob Beck",
        "Risto Vuorio",
        "E. Liu",
        "Zheng Xiong",
        "Luisa M. Zintgraf",
        "Chelsea Finn",
        "S. Whiteson"
      ],
      "year": 2023,
      "citation_count": 121,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/5af8c7c650e9ec50d91a16be287ce54b16075fe7",
      "pdf_link": "",
      "venue": "Found. Trends Mach. Learn.",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "b8a16fd8d823cfe683c19d58bec77a023b5bf1ef",
      "title": "Multi-task Magnetic Resonance Imaging Reconstruction using Meta-learning",
      "abstract": "Using single-task deep learning methods to reconstruct Magnetic Resonance Imaging (MRI) data acquired with different imaging sequences is inherently challenging. The trained deep learning model typically lacks generalizability, and the dissimilarity among image datasets with different types of contrast leads to suboptimal learning performance. This paper proposes a meta-learning approach to efficiently learn image features from multiple MRI datasets. Our algorithm can perform multi-task learning to simultaneously reconstruct MRI images acquired using different imaging sequences with various image contrasts. We have developed a proximal gradient descent-inspired optimization method to learn image features across image and k-space domains, ensuring high-performance learning for every image contrast. Meanwhile, meta-learning, a \"learning-to-learn\" process, is incorporated into our framework to improve the learning of mutual features embedded in multiple image contrasts. The experimental results reveal that our proposed multi-task meta-learning approach surpasses state-of-the-art single-task learning methods at high acceleration rates. Our meta-learning consistently delivers accurate and detailed reconstructions, achieves the lowest pixel-wise errors, and significantly enhances qualitative performance across all tested acceleration rates. We have demonstrated the ability of our new meta-learning reconstruction method to successfully reconstruct highly-undersampled k-space data from multiple MRI datasets simultaneously, outperforming other compelling reconstruction methods previously developed for single-task learning.",
      "authors": [
        "Wanyu Bian",
        "Albert Jang",
        "Fang Liu"
      ],
      "year": 2024,
      "citation_count": 9,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/b8a16fd8d823cfe683c19d58bec77a023b5bf1ef",
      "pdf_link": "",
      "venue": "Magnetic Resonance Imaging",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "cf70392a3b1ae92fdb1b70448aaddcbd03726d3d",
      "title": "YOLOv9: Learning What You Want to Learn Using Programmable Gradient Information",
      "abstract": "Today's deep learning methods focus on how to design the most appropriate objective functions so that the prediction results of the model can be closest to the ground truth. Meanwhile, an appropriate architecture that can facilitate acquisition of enough information for prediction has to be designed. Existing methods ignore a fact that when input data undergoes layer-by-layer feature extraction and spatial transformation, large amount of information will be lost. This paper will delve into the important issues of data loss when data is transmitted through deep networks, namely information bottleneck and reversible functions. We proposed the concept of programmable gradient information (PGI) to cope with the various changes required by deep networks to achieve multiple objectives. PGI can provide complete input information for the target task to calculate objective function, so that reliable gradient information can be obtained to update network weights. In addition, a new lightweight network architecture -- Generalized Efficient Layer Aggregation Network (GELAN), based on gradient path planning is designed. GELAN's architecture confirms that PGI has gained superior results on lightweight models. We verified the proposed GELAN and PGI on MS COCO dataset based object detection. The results show that GELAN only uses conventional convolution operators to achieve better parameter utilization than the state-of-the-art methods developed based on depth-wise convolution. PGI can be used for variety of models from lightweight to large. It can be used to obtain complete information, so that train-from-scratch models can achieve better results than state-of-the-art models pre-trained using large datasets, the comparison results are shown in Figure 1. The source codes are at: https://github.com/WongKinYiu/yolov9.",
      "authors": [
        "Chien-Yao Wang",
        "I-Hau Yeh",
        "Hongpeng Liao"
      ],
      "year": 2024,
      "citation_count": 1831,
      "layer": 1,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/cf70392a3b1ae92fdb1b70448aaddcbd03726d3d",
      "pdf_link": "",
      "venue": "European Conference on Computer Vision",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "5ffee7480bdb997a0f8452829016eee71cb8bbce",
      "title": "Learning to learn by yourself: Unsupervised meta‐learning with self‐knowledge distillation for COVID‐19 diagnosis from pneumonia cases",
      "abstract": "The goal of diagnosing the coronavirus disease 2019 (COVID‐19) from suspected pneumonia cases, that is, recognizing COVID‐19 from chest X‐ray or computed tomography (CT) images, is to improve diagnostic accuracy, leading to faster intervention. The most important and challenging problem here is to design an effective and robust diagnosis model. To this end, there are three challenges to overcome: (1) The lack of training samples limits the success of existing deep‐learning‐based methods. (2) Many public COVID‐19 data sets contain only a few images without fine‐grained labels. (3) Due to the explosive growth of suspected cases, it is urgent and important to diagnose not only COVID‐19 cases but also the cases of other types of pneumonia that are similar to the symptoms of COVID‐19. To address these issues, we propose a novel framework called Unsupervised Meta‐Learning with Self‐Knowledge Distillation to address the problem of differentiating COVID‐19 from pneumonia cases. During training, our model cannot use any true labels and aims to gain the ability of learning to learn by itself. In particular, we first present a deep diagnosis model based on a relation network to capture and memorize the relation among different images. Second, to enhance the performance of our model, we design a self‐knowledge distillation mechanism that distills knowledge within our model itself. Our network is divided into several parts, and the knowledge in the deeper parts is squeezed into the shallow ones. The final results are derived from our model by learning to compare the features of images. Experimental results demonstrate that our approach achieves significantly higher performance than other state‐of‐the‐art methods. Moreover, we construct a new COVID‐19 pneumonia data set based on text mining, consisting of 2696 COVID‐19 images (347 X‐ray + 2349 CT), 10,155 images (9661 X‐ray + 494 CT) about other types of pneumonia, and the fine‐grained labels of all. Our data set considers not only a bacterial infection or viral infection which causes pneumonia but also a viral infection derived from the influenza virus or coronavirus.",
      "authors": [
        "Wenbo Zheng",
        "Lan Yan",
        "Chao Gou",
        "Zhi-Cheng Zhang",
        "J. Zhang",
        "Ming Hu",
        "Fei-Yue Wang"
      ],
      "year": 2021,
      "citation_count": 21,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/5ffee7480bdb997a0f8452829016eee71cb8bbce",
      "pdf_link": "",
      "venue": "International Journal of Intelligent Systems",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "1845ece5be61f96292d0b3ea3ecec251b2510909",
      "title": "Lessons learned from the NeurIPS 2021 MetaDL challenge: Backbone fine-tuning without episodic meta-learning dominates for few-shot learning image classification",
      "abstract": "Although deep neural networks are capable of achieving performance superior to humans on various tasks, they are notorious for requiring large amounts of data and computing resources, restricting their success to domains where such resources are available. Metalearning methods can address this problem by transferring knowledge from related tasks, thus reducing the amount of data and computing resources needed to learn new tasks. We organize the MetaDL competition series, which provide opportunities for research groups all over the world to create and experimentally assess new meta-(deep)learning solutions for real problems. In this paper, authored collaboratively between the competition organizers and the top-ranked participants, we describe the design of the competition, the datasets, the best experimental results, as well as the top-ranked methods in the NeurIPS 2021 challenge, which attracted 15 active teams who made it to the final phase (by outperforming the baseline), making over 100 code submissions during the feedback phase. The solutions of the top participants have been open-sourced. The lessons learned include that learning good representations is essential for effective transfer learning.",
      "authors": [
        "Adrian El Baz",
        "André Carvalho",
        "Hong Chen",
        "Fábio Ferreira",
        "H. Gouk",
        "S. Hu",
        "F. Hutter",
        "Zhengying Liu",
        "F. Mohr",
        "J. V. Rijn",
        "Xin Wang",
        "Isabelle M Guyon"
      ],
      "year": 2022,
      "citation_count": 12,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/1845ece5be61f96292d0b3ea3ecec251b2510909",
      "pdf_link": "",
      "venue": "Neural Information Processing Systems",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "2d8d30eb2f6d554d13be811d8cee541387573bd9",
      "title": "Deep Metric Learning Based on Meta-Mining Strategy With Semiglobal Information",
      "abstract": "Recently, deep metric learning (DML) has achieved great success. Some existing DML methods propose adaptive sample mining strategies, which learn to weight the samples, leading to interesting performance. However, these methods suffer from a small memory (e.g., one training batch), limiting their efficacy. In this work, we introduce a data-driven method, meta-mining strategy with semiglobal information (MMSI), to apply meta-learning to learn to weight samples during the whole training, leading to an adaptive mining strategy. To introduce richer information than one training batch only, we elaborately take advantage of the validation set of meta-learning by implicitly adding additional validation sample information to training. Furthermore, motivated by the latest self-supervised learning, we introduce a dictionary (memory) that maintains very large and diverse information. Together with the validation set, this dictionary presents much richer information to the training, leading to promising performance. In addition, we propose a new theoretical framework that can formulate pairwise and tripletwise metric learning loss functions in a unified framework. This framework brings new insights to society and facilitates us to generalize our MMSI to many existing DML methods. We conduct extensive experiments on three public datasets, CUB200-2011, Cars-196, and Stanford Online Products (SOP). Results show that our method can achieve the state of the art or very competitive performance. Our source codes have been made available at https://github.com/NUST-Machine-Intelligence-Laboratory/MMSI.",
      "authors": [
        "Xiruo Jiang",
        "Sheng Liu",
        "Xili Dai",
        "Guosheng Hu",
        "Xingguo Huang",
        "Yazhou Yao",
        "Guosen Xie",
        "Ling Shao"
      ],
      "year": 2022,
      "citation_count": 16,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/2d8d30eb2f6d554d13be811d8cee541387573bd9",
      "pdf_link": "",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "4bf9f88d438c7d978fb854eba686cf3933879df1",
      "title": "Meta-Learning without Memorization",
      "abstract": "The ability to learn new concepts with small amounts of data is a critical aspect of intelligence that has proven challenging for deep learning methods. Meta-learning has emerged as a promising technique for leveraging data from previous tasks to enable efficient learning of new tasks. However, most meta-learning algorithms implicitly require that the meta-training tasks be mutually-exclusive, such that no single model can solve all of the tasks at once. For example, when creating tasks for few-shot image classification, prior work uses a per-task random assignment of image classes to N-way classification labels. If this is not done, the meta-learner can ignore the task training data and learn a single model that performs all of the meta-training tasks zero-shot, but does not adapt effectively to new image classes. This requirement means that the user must take great care in designing the tasks, for example by shuffling labels or removing task identifying information from the inputs. In some domains, this makes meta-learning entirely inapplicable. In this paper, we address this challenge by designing a meta-regularization objective using information theory that places precedence on data-driven adaptation. This causes the meta-learner to decide what must be learned from the task training data and what should be inferred from the task testing input. By doing so, our algorithm can successfully use data from non-mutually-exclusive tasks to efficiently adapt to novel tasks. We demonstrate its applicability to both contextual and gradient-based meta-learning algorithms, and apply it in practical settings where applying standard meta-learning has been difficult. Our approach substantially outperforms standard meta-learning algorithms in these settings.",
      "authors": [
        "Mingzhang Yin",
        "G. Tucker",
        "Mingyuan Zhou",
        "S. Levine",
        "Chelsea Finn"
      ],
      "year": 2019,
      "citation_count": 189,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/4bf9f88d438c7d978fb854eba686cf3933879df1",
      "pdf_link": "",
      "venue": "International Conference on Learning Representations",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "2dc6799265db441bfa53eb9346cf67fec9a27e39",
      "title": "Few-shot short utterance speaker verification using meta-learning",
      "abstract": "Short utterance speaker verification (SV) in the actual application is the task of accepting or rejecting the identity claim of a speaker based on a few enrollment utterances. Traditional methods have used deep neural networks to extract speaker representations for verification. Recently, several meta-learning approaches have learned a deep distance metric to distinguish speakers within meta-tasks. Among them, a prototypical network learns a metric space that may be used to compute the distance to the prototype center of speakers, in order to classify speaker identity. We use emphasized channel attention, propagation and aggregation in TDNN (ECAPA-TDNN) to implement the necessary function for the prototypical network, which is a nonlinear mapping from the input space to the metric space for either few-shot SV task. In addition, optimizing only for speakers in given meta-tasks cannot be sufficient to learn distinctive speaker features. Thus, we used an episodic training strategy, in which the classes of the support and query sets correspond to the classes of the entire training set, further improving the model performance. The proposed model outperforms comparison models on the VoxCeleb1 dataset and has a wide range of practical applications.",
      "authors": [
        "Weijie Wang",
        "Hong Zhao",
        "Yikun Yang",
        "Youkang Chang",
        "Haojie You"
      ],
      "year": 2023,
      "citation_count": 8,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/2dc6799265db441bfa53eb9346cf67fec9a27e39",
      "pdf_link": "",
      "venue": "PeerJ Computer Science",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "684b36d780bda6e7c4a4c99aa03390466d476476",
      "title": "Few-Shot Cross-Domain Object Detection With Instance-Level Prototype-Based Meta-Learning",
      "abstract": "In typical unsupervised domain adaptive object detection, it is assumed that extensive unlabeled training data from the target domain can be easily obtained. However, in some access-constrained scenarios, massive target data cannot be guaranteed, but acquiring only a few target samples and annotating them may costs less. Therefore, inspired by the meta-learning success in few-shot tasks, we propose an Instance-level Prototype learning Network (IPNet) for solving the domain adaptive object detection under the supervised few-shot scenario in this work. To compensate for the target domain data deficiency, we fuse cropped instances from labeled images in both domains to learn a representative prototype for each class, by enforcing features of the same class’s instances but from different domains to be as close as possible. These prototypes are further employed to discriminate various features’ salience in an image, and separate foreground and background regions for respective domain alignment. Extensive experiments are conducted on several cross-domain scenarios, and their results show the consistent accuracy gains of the IPNet over state-of-the-art methods, e.g., 10.4% mAP increase on Cityscapes-to-FoggyCityscapes setting and 3.0% mAP increase on Sim10k-to-Cityscapes setting.",
      "authors": [
        "Lin Zhang",
        "Bo Zhang",
        "Botian Shi",
        "Jiayuan Fan",
        "Tao Chen"
      ],
      "year": 2024,
      "citation_count": 7,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/684b36d780bda6e7c4a4c99aa03390466d476476",
      "pdf_link": "",
      "venue": "IEEE transactions on circuits and systems for video technology (Print)",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "c8905a4c9c5cbeff6e905687c5077e8af47b8ce4",
      "title": "Meta-Learning Based Domain Prior With Application to Optical-ISAR Image Translation",
      "abstract": "This paper focuses on generating Inverse Synthetic Aperture Radar (ISAR) images from optical images, in particular, for orbit space targets. ISAR images are widely applied in space target observation and classification tasks, whereas, limited to the expensive cost of ISAR sample collection, training deep learning-based ISAR image classifiers with insufficient samples and generating ISAR samples from emulation optical images via image translation techniques have attracted increasing attention. Image translation has highlighted significant success and popularity in computer vision, remote sensing and data generation societies. However, most of the existing methods are implemented under the discipline of extracting the explicit pixel-level features and do not perform effectively while entailing translation to domains with specific implicit features, such as ISAR image does. We propose a meta-learning based domain prior to implicit feature modelling and apply it to CycleGAN and UNIT models to realize effective translations between the ISAR and optical domains. Two representative implicit features, ISAR scattering distribution feature from the physical domain and the classification identifying feature from the task domain, are elaborately formulated with explicit modelling in statistic form. A meta-learning based training scheme is introduced to leverage the mutual knowledge of domain priors across different samples, and thus allows few-shot learning capacity with dramatically reduced training samples. Extensive simulations validate that the obtained ISAR images have better visible-authenticity and training-effectiveness than the existing image translation approaches on various synthetic datasets. Source codes are available at https://github.com/XYLGroup/MLDP.",
      "authors": [
        "Huaizhang Liao",
        "Jingyuan Xia",
        "Zhixiong Yang",
        "Fulin Pan",
        "Zhen Liu",
        "Yongxiang Liu"
      ],
      "year": 2024,
      "citation_count": 26,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/c8905a4c9c5cbeff6e905687c5077e8af47b8ce4",
      "pdf_link": "",
      "venue": "IEEE transactions on circuits and systems for video technology (Print)",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "c40a927a558ad5a5ffe254605ed3bfebd18be39c",
      "title": "Fast Adaptive Task Offloading in Edge Computing Based on Meta Reinforcement Learning",
      "abstract": "Multi-access edge computing (MEC) aims to extend cloud service to the network edge to reduce network traffic and service latency. A fundamental problem in MEC is how to efficiently offload heterogeneous tasks of mobile applications from user equipment (UE) to MEC hosts. Recently, many deep reinforcement learning (DRL)-based methods have been proposed to learn offloading policies through interacting with the MEC environment that consists of UE, wireless channels, and MEC hosts. However, these methods have weak adaptability to new environments because they have low sample efficiency and need full retraining to learn updated policies for new environments. To overcome this weakness, we propose a task offloading method based on meta reinforcement learning, which can adapt fast to new environments with a small number of gradient updates and samples. We model mobile applications as Directed Acyclic Graphs (DAGs) and the offloading policy by a custom sequence-to-sequence (seq2seq) neural network. To efficiently train the seq2seq network, we propose a method that synergizes the first order approximation and clipped surrogate objective. The experimental results demonstrate that this new offloading method can reduce the latency by up to 25 percent compared to three baselines while being able to adapt fast to new environments.",
      "authors": [
        "Jin Wang",
        "Jia Hu",
        "G. Min",
        "Albert Y. Zomaya",
        "N. Georgalas"
      ],
      "year": 2020,
      "citation_count": 288,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/c40a927a558ad5a5ffe254605ed3bfebd18be39c",
      "pdf_link": "",
      "venue": "IEEE Transactions on Parallel and Distributed Systems",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "069cce0ffad769451fe6008e67a6cd27f9eaa281",
      "title": "Few-Shot Hyperspectral Image Classification Using Meta Learning and Regularized Finetuning",
      "abstract": "The use of deep learning (DL)-based hyperspectral image (HSI) classification has been made remarkable progress in recent years. However, obtaining sufficient labeled samples for training DL models remains a challenge. Transfer learning is effective in addressing the problem of HSI classification with limited labeled samples. However, cross-domain HSI classification using transfer learning remain difficult, as differences in ground object categories between two datasets make it challenging to transfer and learn accurate. To address this issue, we propose a simple yet effective method for HSI classification using model-agnostic meta-learning (MAML) and Regularized Fine-tuning (MRFSL). Our method uses optimized 3-D convolutional neural networks (3D-CNNs) model, aided by MAML and cutout data augmentation to enable cross-domain transfer learning and carry out the HSI classification with limited target samples. Experiments conducted on three HSI datasets demonstrate that the MRFSL method achieves excellent results compared to existing methods. Specifically, the overall accuracy (OA) of our proposed MRFSL method reached 91.81%, 71.04%, and 88.35%, when only five labeled samples for each category were randomly extracted from the Salinas, Indian Pines (IPs), and University of Pavia (UP) datasets, respectively.",
      "authors": [
        "Wenmei Li",
        "Qing Liu",
        "Yu Zhang",
        "Yu Wang",
        "Yuan Yuan",
        "Yan Jia",
        "Yuhong He"
      ],
      "year": 2023,
      "citation_count": 7,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/069cce0ffad769451fe6008e67a6cd27f9eaa281",
      "pdf_link": "",
      "venue": "IEEE Transactions on Geoscience and Remote Sensing",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "8291dcc23a6daf3afc976acba07b8b47aa0caebe",
      "title": "Generalizable No-Reference Image Quality Assessment via Deep Meta-Learning",
      "abstract": "Recently, researchers have shown great interest in using convolutional neural networks (CNNs) for no-reference image quality assessment (NR-IQA). Due to the lack of big training data, the efforts of existing metrics in optimizing CNN-based NR-IQA models remain limited. Furthermore, the diversity of distortions in images result in the generalization problem of NR-IQA models when trained with known distortions and tested on unseen distortions, which is an easy task for human. Hence, we propose a NR-IQA metric via deep meta-learning, which is highly generalizable in the face of unseen distortions. The fundamental idea is to learn the meta-knowledge shared by human when evaluating the quality of images with diversified distortions. Specifically, we define NR-IQA of different distortions as a series of tasks and propose a task selection strategy to build two task sets, which are characterized by synthetic to synthetic and synthetic to authentic distortions, respectively. Based on these two task sets, an optimization-based meta-learning is proposed to learn the generalized NR-IQA model, which can be directly used to evaluate the quality of images with unseen distortions. Extensive experiments demonstrate that our NR-IQA metric outperforms the state-of-the-arts in terms of both evaluation performance and generalization ability.",
      "authors": [
        "Hancheng Zhu",
        "Leida Li",
        "Jinjian Wu",
        "W. Dong",
        "Guangming Shi"
      ],
      "year": 2022,
      "citation_count": 48,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/8291dcc23a6daf3afc976acba07b8b47aa0caebe",
      "pdf_link": "",
      "venue": "IEEE transactions on circuits and systems for video technology (Print)",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "66c2031ebf6407e50e309f4a989497353927859b",
      "title": "Introducing neuromodulation in deep neural networks to learn adaptive behaviours",
      "abstract": "Animals excel at adapting their intentions, attention, and actions to the environment, making them remarkably efficient at interacting with a rich, unpredictable and ever-changing external world, a property that intelligent machines currently lack. Such an adaptation property relies heavily on cellular neuromodulation, the biological mechanism that dynamically controls intrinsic properties of neurons and their response to external stimuli in a context-dependent manner. In this paper, we take inspiration from cellular neuromodulation to construct a new deep neural network architecture that is specifically designed to learn adaptive behaviours. The network adaptation capabilities are tested on navigation benchmarks in a meta-reinforcement learning context and compared with state-of-the-art approaches. Results show that neuromodulation is capable of adapting an agent to different tasks and that neuromodulation-based approaches provide a promising way of improving adaptation of artificial systems.",
      "authors": [
        "Nicolas Vecoven",
        "D. Ernst",
        "Antoine Wehenkel",
        "G. Drion"
      ],
      "year": 2018,
      "citation_count": 44,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/66c2031ebf6407e50e309f4a989497353927859b",
      "pdf_link": "",
      "venue": "PLoS ONE",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "0d13ff7e27e0a1fb06c57efaeacfc90ccaef3452",
      "title": "ELITE: Robust Deep Anomaly Detection with Meta Gradient",
      "abstract": "Deep Learning techniques have been widely used in detecting anomalies from complex data. Most of these techniques are either unsupervised or semi-supervised because of a lack of a large number of labeled anomalies. However, they typically rely on a clean training data not polluted by anomalies to learn the distribution of the normal data. Otherwise, the learned distribution tends to be distorted and hence ineffective in distinguishing between normal and abnormal data. To solve this problem, we propose a novel approach called ELITE that uses a small number of labeled examples to infer the anomalies hidden in the training samples. It then turns these anomalies into useful signals that help to better detect anomalies from user data. Unlike the classical semi-supervised classification strategy which uses labeled examples as training data, ELITE uses them as validation set. It leverages the gradient of the validation loss to predict if one training sample is abnormal. The intuition is that correctly identifying the hidden anomalies could produce a better deep anomaly model with reduced validation loss. Our experiments on public benchmark datasets show that ELITE achieves up to 30% improvement in ROC AUC comparing to the state-of-the-art, yet robust to polluted training data.",
      "authors": [
        "Huayi Zhang",
        "Lei Cao",
        "Peter M. VanNostrand",
        "S. Madden",
        "Elke A. Rundensteiner"
      ],
      "year": 2021,
      "citation_count": 30,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/0d13ff7e27e0a1fb06c57efaeacfc90ccaef3452",
      "pdf_link": "",
      "venue": "Knowledge Discovery and Data Mining",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "272b071e05e960ef3adab2bc8a078fd165b268d5",
      "title": "General Hyperspectral Image Super-Resolution via Meta-Transfer Learning",
      "abstract": "Recent advances in deep learning-based methods have led to significant progress in the hyperspectral super-resolution (SR). However, the scarcity and the high dimension of data have hindered further development since deep models require sufficient data to learn stable patterns. Moreover, the huge domain differences between hyperspectral image (HSI) datasets pose a significant challenge in generalizability. To address these problems, we present a general hyperspectral SR framework via meta-transfer learning (MTL). We randomly sample various spectral ranges for SR tasks during MTL, allowing the model to accumulate diverse task experiences. Additionally, we implement a task schedule to gradually expand the number of bands, bridging the significant domain differences between datasets. By leveraging multiple datasets, we are able to achieve better performance and greater generalizability, making it applicable under various circumstances. Meanwhile, as a general framework, our scheme can be applied to existing methods to obtain performance improvements. In addition, we design an advanced network architecture based on the multifusion features to further improve the performance. Experiments demonstrate that our method not only achieves superior performance in both qualitative and quantitative terms but also can adapt robustly to a new and difficult sample, where few epochs can yield quite considerable results.",
      "authors": [
        "Yingsong Cheng",
        "Xinya Wang",
        "Yong Ma",
        "Xiaoguang Mei",
        "Minghui Wu",
        "Jiayi Ma"
      ],
      "year": 2024,
      "citation_count": 4,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/272b071e05e960ef3adab2bc8a078fd165b268d5",
      "pdf_link": "",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "9e1ea0a5a29a19baf531aa5d9f32ca51d240d575",
      "title": "Self-Supervised Deep Visual Odometry With Online Adaptation",
      "abstract": "Self-supervised VO methods have shown great success in jointly estimating camera pose and depth from videos. However, like most data-driven methods, existing VO networks suffer from a notable decrease in performance when confronted with scenes different from the training data, which makes them unsuitable for practical applications. In this paper, we propose an online meta-learning algorithm to enable VO networks to continuously adapt to new environments in a self-supervised manner. The proposed method utilizes convolutional long short-term memory (convLSTM) to aggregate rich spatial-temporal information in the past. The network is able to memorize and learn from its past experience for better estimation and fast adaptation to the current frame. When running VO in the open world, in order to deal with the changing environment, we propose an online feature alignment method by aligning feature distributions at different time. Our VO network is able to seamlessly adapt to different environments. Extensive experiments on unseen outdoor scenes, virtual to real world and outdoor to indoor environments demonstrate that our method consistently outperforms state-of-the-art self-supervised VO baselines considerably.",
      "authors": [
        "Shunkai Li",
        "Xin Wang",
        "Yingdian Cao",
        "Fei Xue",
        "Zike Yan",
        "H. Zha"
      ],
      "year": 2020,
      "citation_count": 73,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/9e1ea0a5a29a19baf531aa5d9f32ca51d240d575",
      "pdf_link": "",
      "venue": "Computer Vision and Pattern Recognition",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "3904315e2eca50d0086e4b7273f7fd707c652230",
      "title": "Meta-Learning with Memory-Augmented Neural Networks",
      "abstract": "",
      "authors": [
        "Adam Santoro",
        "Sergey Bartunov",
        "M. Botvinick",
        "D. Wierstra",
        "T. Lillicrap"
      ],
      "year": 2016,
      "citation_count": 1861,
      "layer": 1,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/3904315e2eca50d0086e4b7273f7fd707c652230",
      "pdf_link": "",
      "venue": "International Conference on Machine Learning",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "c09d2846ff81c5dbdf4f8662bd2d9bb4dd61b396",
      "title": "Heterogeneity-Informed Meta-Parameter Learning for Spatiotemporal Time Series Forecasting",
      "abstract": "Spatiotemporal time series forecasting plays a key role in a wide range of real-world applications. While significant progress has been made in this area, fully capturing and leveraging spatiotemporal heterogeneity remains a fundamental challenge. Therefore, we propose a novel Heterogeneity-Informed Meta-Parameter Learning scheme. Specifically, our approach implicitly captures spatiotemporal heterogeneity through learning spatial and temporal embeddings, which can be viewed as a clustering process. Then, a novel spatiotemporal meta-parameter learning paradigm is proposed to learn spatiotemporal-specific parameters from meta-parameter pools, which is informed by the captured heterogeneity. Based on these ideas, we develop a Heterogeneity-Informed Spatiotemporal Meta-Network (HimNet) for spatiotemporal time series forecasting. Extensive experiments on five widely-used benchmarks demonstrate our method achieves state-of-the-art performance while exhibiting superior interpretability. Our code is available at https://github.com/XDZhelheim/HimNet.",
      "authors": [
        "Zheng Dong",
        "Renhe Jiang",
        "Haotian Gao",
        "Hangchen Liu",
        "Jinliang Deng",
        "Qingsong Wen",
        "Xuan Song"
      ],
      "year": 2024,
      "citation_count": 27,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/c09d2846ff81c5dbdf4f8662bd2d9bb4dd61b396",
      "pdf_link": "",
      "venue": "Knowledge Discovery and Data Mining",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "c67de8be8b033362e94d98dcefae88e4b75dd6c7",
      "title": "Adaptive Label Noise Cleaning with Meta-Supervision for Deep Face Recognition",
      "abstract": "The training of a deep face recognition system usually faces the interference of label noise in the training data. However, it is difficult to obtain a high-precision cleaning model to remove these noises. In this paper, we propose an adaptive label noise cleaning algorithm based on meta-learning for face recognition datasets, which can learn the distribution of the data to be cleaned and make automatic adjustments based on class differences. It first learns re-liable cleaning knowledge from well-labeled noisy data, then gradually transfers it to the target data with meta-supervision to improve performance. A threshold adapter module is also proposed to address the drift problem in transfer learning methods. Extensive experiments clean two noisy in-the-wild face recognition datasets and show the effectiveness of the proposed method to reach state-of-the-art performance on the IJB-C face recognition benchmark.",
      "authors": [
        "Yaobin Zhang",
        "Weihong Deng",
        "Yaoyao Zhong",
        "Jiani Hu",
        "Dongchao Wen"
      ],
      "year": 2021,
      "citation_count": 10,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/c67de8be8b033362e94d98dcefae88e4b75dd6c7",
      "pdf_link": "",
      "venue": "IEEE International Conference on Computer Vision",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "dea00783b876b41e852adc0ad1954e1005324edd",
      "title": "Many or Few Samples?: Comparing Transfer, Contrastive and Meta-Learning in Encrypted Traffic Classification",
      "abstract": "The popularity of Deep Learning (DL), coupled with network traffic visibility reduction due to the increased adoption of HTTPS, QUIC, and DNS-SEC, re-ignited interest towards Traffic Classification (TC). However, to tame the dependency from task-specific large labeled datasets, we need to find better ways to learn representations that are valid across tasks. In this work we investigate this problem comparing transfer learning, meta-learning and contrastive learning against reference Machine Learning (ML) tree-based and monolithic DL models (16 methods total). Using two publicly available datasets, namely MIRAGE19 (40 classes) and AppClassNet (500 classes), we show that ($i$) by using DL methods on large datasets we can obtain more general representations with (i i) contrastive learning methods yielding the best performance and (iii) meta-learning the worst one. While (iv) tree-based models can be impractical for large tasks but fit well small tasks, (v) DL methods that reuse better learned representations are closing their performance gap against trees also for small tasks.",
      "authors": [
        "Idio Guarino",
        "Chao Wang",
        "A. Finamore",
        "A. Pescapé",
        "Dario Rossi"
      ],
      "year": 2023,
      "citation_count": 11,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/dea00783b876b41e852adc0ad1954e1005324edd",
      "pdf_link": "",
      "venue": "Traffic Monitoring and Analysis",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "0be1e53ccf4320e6e140523a75d55bac57d4d3e2",
      "title": "A Feedback Semi-Supervised Learning With Meta-Gradient for Intrusion Detection",
      "abstract": "The widespread popularity of the Internet of human life has been accompanied by a significant increase in the cost of protecting private data from malicious attacks. Researchers have proposed many deep learning-based intrusion detection methods. However, traditional methods rely on a large number of unpolluted data to learn benign data distributions, while nonidentical distributions will affect the performance in distinguishing normal and abnormal data. To address this problem, this article proposes an intrusion detection method feedback semi-supervised learning with meta-gradient for intrusion detection (FSMG) based on feedback deep semi-supervised learning. FSMG constructs a lightweight evaluation network with slight data augmentation and nonprocessing on the same input, respectively, and uses a small amount of labeled data to infer nonidentically distributed data flows hidden in the training dataset. Then, FSMG converts malicious flows into useful information, continuously track and update the model, reducing data labeling errors. Furthermore, for the updating of gradients in the model, a bilevel nested optimization is used to ensure the model converges within $O({\\mathrm{C}}/{\\sqrt{T}})$. Unlike other semisupervised algorithms, FSMG uses labeled data and nonidentically distributed unlabeled data proportionally to construct the training dataset, achieving higher classification accuracy, and better robustness even with an 80% polluted rate.",
      "authors": [
        "Shaokang Cai",
        "Dezhi Han",
        "Dun Li"
      ],
      "year": 2023,
      "citation_count": 12,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/0be1e53ccf4320e6e140523a75d55bac57d4d3e2",
      "pdf_link": "",
      "venue": "IEEE Systems Journal",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "bbe13b72314fffcc2f35b0660195f2f6607c00a0",
      "title": "Few-shot Autoregressive Density Estimation: Towards Learning to Learn Distributions",
      "abstract": "Deep autoregressive models have shown state-of-the-art performance in density estimation for natural images on large-scale datasets such as ImageNet. However, such models require many thousands of gradient-based weight updates and unique image examples for training. Ideally, the models would rapidly learn visual concepts from only a handful of examples, similar to the manner in which humans learns across many vision tasks. In this paper, we show how 1) neural attention and 2) meta learning techniques can be used in combination with autoregressive models to enable effective few-shot density estimation. Our proposed modifications to PixelCNN result in state-of-the art few-shot density estimation on the Omniglot dataset. Furthermore, we visualize the learned attention policy and find that it learns intuitive algorithms for simple tasks such as image mirroring on ImageNet and handwriting on Omniglot without supervision. Finally, we extend the model to natural images and demonstrate few-shot image generation on the Stanford Online Products dataset.",
      "authors": [
        "Scott E. Reed",
        "Yutian Chen",
        "T. Paine",
        "Aäron van den Oord",
        "S. Eslami",
        "Danilo Jimenez Rezende",
        "O. Vinyals",
        "Nando de Freitas"
      ],
      "year": 2017,
      "citation_count": 88,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/bbe13b72314fffcc2f35b0660195f2f6607c00a0",
      "pdf_link": "",
      "venue": "International Conference on Learning Representations",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "91e6d31e3bb634007dbc3abc3d84da01412fea17",
      "title": "Neural-Fly enables rapid learning for agile flight in strong winds",
      "abstract": "Executing safe and precise flight maneuvers in dynamic high-speed winds is important for the ongoing commoditization of uninhabited aerial vehicles (UAVs). However, because the relationship between various wind conditions and its effect on aircraft maneuverability is not well understood, it is challenging to design effective robot controllers using traditional control design methods. We present Neural-Fly, a learning-based approach that allows rapid online adaptation by incorporating pretrained representations through deep learning. Neural-Fly builds on two key observations that aerodynamics in different wind conditions share a common representation and that the wind-specific part lies in a low-dimensional space. To that end, Neural-Fly uses a proposed learning algorithm, domain adversarially invariant meta-learning (DAIML), to learn the shared representation, only using 12 minutes of flight data. With the learned representation as a basis, Neural-Fly then uses a composite adaptation law to update a set of linear coefficients for mixing the basis elements. When evaluated under challenging wind conditions generated with the Caltech Real Weather Wind Tunnel, with wind speeds up to 43.6 kilometers/hour (12.1 meters/second), Neural-Fly achieves precise flight control with substantially smaller tracking error than stateof-the-art nonlinear and adaptive controllers. In addition to strong empirical performance, the exponential stability of Neural-Fly results in robustness guarantees. Last, our control design extrapolates to unseen wind conditions, is shown to be effective for outdoor flights with only onboard sensors, and can transfer across drones with minimal performance degradation. Description A drone adapts to dynamic wind conditions using deep neural networks in real time with stability guarantees.",
      "authors": [
        "M. O'Connell",
        "Guanya Shi",
        "Xichen Shi",
        "K. Azizzadenesheli",
        "Anima Anandkumar",
        "Yisong Yue",
        "Soon-Jo Chung"
      ],
      "year": 2022,
      "citation_count": 212,
      "layer": 1,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/91e6d31e3bb634007dbc3abc3d84da01412fea17",
      "pdf_link": "",
      "venue": "Science Robotics",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "190ae56a68a94620d7ddfdc7c4b1424673f78b97",
      "title": "Cross-Domain Deep Code Search with Meta Learning",
      "abstract": "Recently, pre-trained programming language models such as Code-BERT have demonstrated substantial gains in code search. Despite their success, they rely on the availability of large amounts of parallel data to fine-tune the semantic mappings between queries and code. This restricts their practicality in domain-specific languages with relatively scarce and expensive data. In this paper, we propose CDCS, a novel approach for domain-specific code search. CDCS employs a transfer learning framework where an initial program representation model is pre-trained on a large corpus of common programming languages (such as Java and Python), and is further adapted to domain-specific languages such as Solidity and SQL. Un-like cross-language CodeBERT, which is directly fine-tuned in the target language, CDCS adapts a few-shot meta-learning algorithm called MAML to learn the good initialization of model parameters, which can be best reused in a domain-specific language. We evaluate the proposed approach on two domain-specific languages, namely Solidity and SQL, with model transferred from two widely used languages (Python and Java). Experimental results show that CDCS significantly outperforms conventional pre-trained code models that are directly fine-tuned in domain-specific languages, and it is particularly effective for scarce data.",
      "authors": [
        "Yitian Chai",
        "Hongyu Zhang",
        "Beijun Shen",
        "Xiaodong Gu"
      ],
      "year": 2022,
      "citation_count": 43,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/190ae56a68a94620d7ddfdc7c4b1424673f78b97",
      "pdf_link": "",
      "venue": "International Conference on Software Engineering",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "c317d2faa26b38250960cf3d2e6cf095b9d5b92d",
      "title": "Learning to Learn to Disambiguate: Meta-Learning for Few-Shot Word Sense Disambiguation",
      "abstract": "The success of deep learning methods hinges on the availability of large training datasets annotated for the task of interest. In contrast to human intelligence, these methods lack versatility and struggle to learn and adapt quickly to new tasks, where labeled data is scarce. Meta-learning aims to solve this problem by training a model on a large number of few-shot tasks, with an objective to learn new tasks quickly from a small number of examples. In this paper, we propose a meta-learning framework for few-shot word sense disambiguation (WSD), where the goal is to learn to disambiguate unseen words from only a few labeled instances. Meta-learning approaches have so far been typically tested in an N-way, K-shot classification setting where each task has N classes with K examples per class. Owing to its nature, WSD deviates from this controlled setup and requires the models to handle a large number of highly unbalanced classes. We extend several popular meta-learning approaches to this scenario, and analyze their strengths and weaknesses in this new challenging setting.",
      "authors": [
        "Nithin Holla",
        "Pushkar Mishra",
        "H. Yannakoudakis",
        "Ekaterina Shutova"
      ],
      "year": 2020,
      "citation_count": 29,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/c317d2faa26b38250960cf3d2e6cf095b9d5b92d",
      "pdf_link": "",
      "venue": "Findings",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "b6efb87e4b609fb67304f73b8ee9c1984fce5e88",
      "title": "VL-Meta: Vision-Language Models for Multimodal Meta-Learning",
      "abstract": "Multimodal learning is a promising area in artificial intelligence (AI) that can make the model understand different kinds of data. Existing works are trying to re-train a new model based on pre-trained models that requires much data, computation power, and time. However, it is difficult to achieve in low-resource or small-sample situations. Therefore, we propose VL-Meta, Vision Language Models for Multimodal Meta Learning. It (1) presents the vision-language mapper and multimodal fusion mapper, which are light model structures, to use the existing pre-trained models to make models understand images to language feature space and save training data, computation power, and time; (2) constructs the meta-task pool that can only use a small amount of data to construct enough training data and improve the generalization of the model to learn the data knowledge and task knowledge; (3) proposes the token-level training that can align inputs with the outputs during training to improve the model performance; and (4) adopts the multi-task fusion loss to learn the different abilities for the models. It achieves a good performance on the Visual Question Answering (VQA) task, which shows the feasibility and effectiveness of the model. This solution can help blind or visually impaired individuals obtain visual information.",
      "authors": [
        "Han Ma",
        "Baoyu Fan",
        "B. Ng",
        "C. Lam"
      ],
      "year": 2024,
      "citation_count": 6,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/b6efb87e4b609fb67304f73b8ee9c1984fce5e88",
      "pdf_link": "",
      "venue": "Mathematics",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "588c69df5e7920db0037db76c41f933ee16c290d",
      "title": "Meta-GPS++: Enhancing Graph Meta-Learning with Contrastive Learning and Self-Training",
      "abstract": "Node classification is an essential problem in graph learning. However, many models typically obtain unsatisfactory performance when applied to few-shot scenarios. Some studies have attempted to combine meta-learning with graph neural networks to solve few-shot node classification on graphs. Despite their promising performance, some limitations remain. First, they employ the node encoding mechanism of homophilic graphs to learn node embeddings, even in heterophilic graphs. Second, existing models based on meta-learning ignore the interference of randomness in the learning process. Third, they are trained using only limited labeled nodes within the specific task, without explicitly utilizing numerous unlabeled nodes. Finally, they treat almost all sampled tasks equally without customizing them for their uniqueness. To address these issues, we propose a novel framework for few-shot node classification called Meta-GPS \\(++\\) . Specifically, we first adopt an efficient method to learn discriminative node representations on homophilic and heterophilic graphs. Then, we leverage a prototype-based approach to initialize parameters and contrastive learning for regularizing the distribution of node embeddings. Moreover, we apply self-training to extract valuable information from unlabeled nodes. Additionally, we adopt S \\({}^{2}\\) (scaling and shifting) transformation to learn transferable knowledge from diverse tasks. The results on real-world datasets show the superiority of Meta-GPS \\(++\\) . Our code is available here.",
      "authors": [
        "Yonghao Liu",
        "Mengyu Li",
        "Ximing Li",
        "Lan Huang",
        "Fausto Giunchiglia",
        "Yanchun Liang",
        "Xiaoyue Feng",
        "Renchu Guan"
      ],
      "year": 2024,
      "citation_count": 4,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/588c69df5e7920db0037db76c41f933ee16c290d",
      "pdf_link": "",
      "venue": "ACM Transactions on Knowledge Discovery from Data",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "5ad8802447f81bd8574a3bee0c2d1a6456d1533b",
      "title": "Explainable Artificial Intelligence (xAI) Approaches and Deep Meta-Learning Models",
      "abstract": "The explainable artificial intelligence (xAI) is one of the interesting issues that has emerged recently. Many researchers are trying to deal with the subject with different dimensions and interesting results that have come out. However, we are still at the beginning of the way to understand these types of models. The forthcoming years are expected to be years in which the openness of deep learning models is discussed. In classical artificial intelligence approaches, we frequently encounter deep learning methods available today. These deep learning methods can yield highly effective results according to the data set size, data set quality, the methods used in feature extraction, the hyper parameter set used in deep learning models, the activation functions, and the optimization algorithms. However, there are important shortcomings that current deep learning models are currently inadequate. These artificial neural network-based models are black box models that generalize the data transmitted to it and learn from the data. Therefore, the relational link between input and output is not observable. This is an important open point in artificial neural networks and deep learning models. For these reasons, it is necessary to make serious efforts on the explainability and interpretability of black box models.",
      "authors": [
        "Evren Daglarli"
      ],
      "year": 2020,
      "citation_count": 25,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/5ad8802447f81bd8574a3bee0c2d1a6456d1533b",
      "pdf_link": "",
      "venue": "Advances and Applications in Deep Learning",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "8f12add50397f697631b3fff04608d5efa957867",
      "title": "Adaptation and Learning to Learn (ALL): An Integrated Approach for Small-Sample Parking Occupancy Prediction",
      "abstract": "Parking occupancy prediction (POP) plays a vital role in many parking-related smart services for better parking management. However, an issue hinders its mass deployment: many parking facilities cannot collect enough data to feed data-hungry machine learning models. To tackle the challenges in small-sample POP, we propose an approach named Adaptation and Learning to Learn (ALL) by adopting the capability of advanced deep learning and federated learning. ALL integrates two novel ideas: (1) Adaptation: by leveraging the Asynchronous Advantage Actor-Critic (A3C) reinforcement learning technique, an auto-selector module is implemented, which can group and select data-scarce parks automatically as supporting sources to enable the knowledge adaptation in model training; and (2) Learning to learn: by applying federated meta-learning on selected supporting sources, a meta-learner module is designed, which can train a high-performance local prediction model in a collaborative and privacy-preserving manner. Results of an evaluation with 42 parking lots in two Chinese cities (Shenzhen and Guangzhou) show that, compared to state-of-the-art baselines: (1) the auto-selector can reduce the model variance by about 17.8%; (2) the meta-learner can train a converged model 102× faster; and (3) finally, ALL can boost the forecasting performance by about 29.8%. Through the integration of advanced machine learning methods, i.e., reinforcement learning, meta-learning, and federated learning, the proposed approach ALL represents a significant step forward in solving small-sample issues in parking occupancy prediction.",
      "authors": [
        "H. Qu",
        "Sheng Liu",
        "Jun Li",
        "Yuren Zhou",
        "R. Liu"
      ],
      "year": 2022,
      "citation_count": 14,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/8f12add50397f697631b3fff04608d5efa957867",
      "pdf_link": "",
      "venue": "Mathematics",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "205770123d5779da5470ae58cf446bc3e9cfc195",
      "title": "MAGICVFM-Meta-Learning Adaptation for Ground Interaction Control With Visual Foundation Models",
      "abstract": "Control of off-road vehicles is challenging due to the complex dynamic interactions with the terrain. Accurate modeling of these interactions is important to optimize driving performance, but the relevant physical phenomena, such as slip, are too complex to model from first principles. Therefore, we present an offline meta-learning algorithm to construct a rapidly-tunable model of residual dynamics and disturbances. Our model processes terrain images into features using a visual foundation model (VFM), then maps these features and the vehicle state to an estimate of the current actuation matrix using a deep neural network (DNN). We then combine this model with composite adaptive control to modify the last layer of the DNN in real time, accounting for the remaining terrain interactions not captured during offline training. We provide mathematical guarantees of stability and robustness for our controller, and demonstrate the effectiveness of our method through simulations and hardware experiments with a tracked vehicle and a car-like robot. We evaluate our method outdoors on different slopes with varying slippage and actuator degradation disturbances, and compare against an adaptive controller that does not use the VFM terrain features. We show significant improvement over the baseline in both hardware experimentation and simulation.",
      "authors": [
        "E. Lupu",
        "Fengze Xie",
        "James A. Preiss",
        "Jedidiah Alindogan",
        "Matthew Anderson",
        "Soon-Jo Chung"
      ],
      "year": 2024,
      "citation_count": 9,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/205770123d5779da5470ae58cf446bc3e9cfc195",
      "pdf_link": "",
      "venue": "IEEE Transactions on robotics",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "38b547a2cf81bacd30cbb322e7279091753604dc",
      "title": "Transductive Episodic-Wise Adaptive Metric for Few-Shot Learning",
      "abstract": "Few-shot learning, which aims at extracting new concepts rapidly from extremely few examples of novel classes, has been featured into the meta-learning paradigm recently. Yet, the key challenge of how to learn a generalizable classifier with the capability of adapting to specific tasks with severely limited data still remains in this domain. To this end, we propose a Transductive Episodic-wise Adaptive Metric (TEAM) framework for few-shot learning, by integrating the meta-learning paradigm with both deep metric learning and transductive inference. With exploring the pairwise constraints and regularization prior within each task, we explicitly formulate the adaptation procedure into a standard semi-definite programming problem. By solving the problem with its closed-form solution on the fly with the setup of transduction, our approach efficiently tailors an episodic-wise metric for each task to adapt all features from a shared task-agnostic embedding space into a more discriminative task-specific metric space. Moreover, we further leverage an attention-based bi-directional similarity strategy for extracting the more robust relationship between queries and prototypes. Extensive experiments on three benchmark datasets show that our framework is superior to other existing approaches and achieves the state-of-the-art performance in the few-shot literature.",
      "authors": [
        "Limeng Qiao",
        "Yemin Shi",
        "Jia Li",
        "Yaowei Wang",
        "Tiejun Huang",
        "Yonghong Tian"
      ],
      "year": 2019,
      "citation_count": 186,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/38b547a2cf81bacd30cbb322e7279091753604dc",
      "pdf_link": "",
      "venue": "IEEE International Conference on Computer Vision",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "13b00c6c8e6fd35a540b08904824aff0d6b66897",
      "title": "Meta-Inverse Reinforcement Learning with Probabilistic Context Variables",
      "abstract": "Reinforcement learning demands a reward function, which is often difficult to provide or design in real world applications. While inverse reinforcement learning (IRL) holds promise for automatically learning reward functions from demonstrations, several major challenges remain. First, existing IRL methods learn reward functions from scratch, requiring large numbers of demonstrations to correctly infer the reward for each task the agent may need to perform. Second, and more subtly, existing methods typically assume demonstrations for one, isolated behavior or task, while in practice, it is significantly more natural and scalable to provide datasets of heterogeneous behaviors. To this end, we propose a deep latent variable model that is capable of learning rewards from unstructured, multi-task demonstration data, and critically, use this experience to infer robust rewards for new, structurally-similar tasks from a single demonstration. Our experiments on multiple continuous control tasks demonstrate the effectiveness of our approach compared to state-of-the-art imitation and inverse reinforcement learning methods.",
      "authors": [
        "Lantao Yu",
        "Tianhe Yu",
        "Chelsea Finn",
        "Stefano Ermon"
      ],
      "year": 2019,
      "citation_count": 75,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/13b00c6c8e6fd35a540b08904824aff0d6b66897",
      "pdf_link": "",
      "venue": "Neural Information Processing Systems",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "52f37e9bd84547db2ecefed420715f312827c398",
      "title": "Property-Guided Few-Shot Learning for Molecular Property Prediction With Dual-View Encoder and Relation Graph Learning Network",
      "abstract": "Molecular property prediction is an important task in drug discovery. However, experimental data for many drug molecules are limited, especially for novel molecular structures or rare diseases which affect the accuracy of many deep learning methods that rely on large training datasets. To this end, we propose PG-DERN, a novel few-shot learning model for molecular property prediction. A dual-view encoder is introduced to learn a meaningful molecular representation by integrating information from node and subgraph. Next, a relation graph learning module is proposed to construct a relation graph based on the similarity between molecules, which improves the efficiency of information propagation and the accuracy of property prediction. In addition, we use a MAML-based meta-learning strategy to learn well-initialized meta-parameters. In order to guide the tuning of meta-parameters, a property-guided feature augmentation module is designed to transfer information from similar properties to the novel property to improve the comprehensiveness of the feature representation of molecules with novel property. A series of comparative experiments on four benchmark datasets demonstrate that the proposed PG-DERN outperforms state-of-the-art methods.",
      "authors": [
        "Lianwei Zhang",
        "Dongjiang Niu",
        "Beiyi Zhang",
        "Qiang Zhang",
        "Zhen Li"
      ],
      "year": 2024,
      "citation_count": 4,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/52f37e9bd84547db2ecefed420715f312827c398",
      "pdf_link": "",
      "venue": "IEEE journal of biomedical and health informatics",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "23bac2542b145bf2fcd17d7fa0a02ae03d0a45f7",
      "title": "Skill-based Meta-Reinforcement Learning",
      "abstract": "While deep reinforcement learning methods have shown impressive results in robot learning, their sample inefficiency makes the learning of complex, long-horizon behaviors with real robot systems infeasible. To mitigate this issue, meta-reinforcement learning methods aim to enable fast learning on novel tasks by learning how to learn. Yet, the application has been limited to short-horizon tasks with dense rewards. To enable learning long-horizon behaviors, recent works have explored leveraging prior experience in the form of offline datasets without reward or task annotations. While these approaches yield improved sample efficiency, millions of interactions with environments are still required to solve complex tasks. In this work, we devise a method that enables meta-learning on long-horizon, sparse-reward tasks, allowing us to solve unseen target tasks with orders of magnitude fewer environment interactions. Our core idea is to leverage prior experience extracted from offline datasets during meta-learning. Specifically, we propose to (1) extract reusable skills and a skill prior from offline datasets, (2) meta-train a high-level policy that learns to efficiently compose learned skills into long-horizon behaviors, and (3) rapidly adapt the meta-trained policy to solve an unseen target task. Experimental results on continuous control tasks in navigation and manipulation demonstrate that the proposed method can efficiently solve long-horizon novel target tasks by combining the strengths of meta-learning and the usage of offline datasets, while prior approaches in RL, meta-RL, and multi-task RL require substantially more environment interactions to solve the tasks.",
      "authors": [
        "Taewook Nam",
        "Shao-Hua Sun",
        "Karl Pertsch",
        "S. Hwang",
        "Joseph J. Lim"
      ],
      "year": 2022,
      "citation_count": 49,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/23bac2542b145bf2fcd17d7fa0a02ae03d0a45f7",
      "pdf_link": "",
      "venue": "International Conference on Learning Representations",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "4454a763c891afb3fb8fa6567a367d05b1938e97",
      "title": "Meta-learning with negative learning rates",
      "abstract": "Deep learning models require a large amount of data to perform well. When data is scarce for a target task, we can transfer the knowledge gained by training on similar tasks to quickly learn the target. A successful approach is meta-learning, or learning to learn a distribution of tasks, where learning is represented by an outer loop, and to learn by an inner loop of gradient descent. However, a number of recent empirical studies argue that the inner loop is unnecessary and more simple models work equally well or even better. We study the performance of MAML as a function of the learning rate of the inner loop, where zero learning rate implies that there is no inner loop. Using random matrix theory and exact solutions of linear models, we calculate an algebraic expression for the test loss of MAML applied to mixed linear regression and nonlinear regression with overparameterized models. Surprisingly, while the optimal learning rate for adaptation is positive, we find that the optimal learning rate for training is always negative, a setting that has never been considered before. Therefore, not only does the performance increase by decreasing the learning rate to zero, as suggested by recent work, but it can be increased even further by decreasing the learning rate to negative values. These results help clarify under what circumstances meta-learning performs best.",
      "authors": [
        "A. Bernacchia"
      ],
      "year": 2021,
      "citation_count": 17,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/4454a763c891afb3fb8fa6567a367d05b1938e97",
      "pdf_link": "",
      "venue": "International Conference on Learning Representations",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "7ecab7276a1a0360ad594ae08a0fa91a26ecb025",
      "title": "A Few-Shot Malicious Encrypted Traffic Detection Approach Based on Model-Agnostic Meta-Learning",
      "abstract": "Existing malicious encrypted traffic detection approaches need to be trained with many samples to achieve effective detection of a specified class of encrypted traffic data. With the rapid development of encryption technology, various new types of encrypted traffic are emerging and difficult to label. Therefore, it is an urgent problem to train a deep learning model using only a small number of samples to detect new classes of malicious encrypted traffic. This paper proposes a few-shot malicious encrypted traffic detection (FMETD) approach based on model-agnostic meta-learning (MAML), integrating feature selection and classification into an end-to-end framework. The FMETD approach first converts the raw traffic data into two-dimensional grayscale images. Then, FMETD trains a deep learning model (2D-CNN) using the MAML, which is to learn an optimal set of model initialization parameters for the model from a set of classification tasks consisting of grayscale images. The model with this set of parameters can detect new classes of maliciously encrypted traffic data efficiently with a few samples by a few iterations steps. The experimental results show that the FMETD approach has 99.8% accuracy for two-class classification encrypted traffic and 98.5% average accuracy for multi-classification. When the number of grayscale images of each class in the support set and validation set is reduced to 20, the accuracy of our approach to multi-class classification is 97.9% for new classes of traffic.",
      "authors": [
        "Zhiqiang Wang",
        "Man Li",
        "H. Ou",
        "Shufang Pang",
        "Z. Yue"
      ],
      "year": 2023,
      "citation_count": 5,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/7ecab7276a1a0360ad594ae08a0fa91a26ecb025",
      "pdf_link": "",
      "venue": "Security and Communication Networks",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "2e3e8a56981df1e33d93284be43f81704abc5795",
      "title": "SEML: Self-Supervised Information-Enhanced Meta-learning for Few-Shot Text Classification",
      "abstract": "Training a deep-learning text classification model usually requires a large amount of labeled data, yet labeling data are usually labor-intensive and time-consuming. Few-shot text classification focuses on predicting unknown samples using only a few labeled samples. Recently, metric-based meta-learning methods have achieved promising results in few-shot text classification. They use episodic training in labeled samples to enhance the model’s generalization ability. However, existing models only focus on learning from a few labeled samples but neglect to learn from a large number of unlabeled samples. In this paper, we exploit the knowledge learned by the model in unlabeled samples to improve the generalization performance of the meta-network. Specifically, we introduce a novel knowledge distillation method that expands and enriches the meta-learning representation with self-supervised information. Meanwhile, we design a graph aggregation method that efficiently interacts the query set information with the support set information in each task and outputs a more discriminative representation. We conducted experiments on three public few-shot text classification datasets. The experimental results show that our model performs better than the state-of-the-art models in 5-way 1-shot and 5-way 5-shot cases.",
      "authors": [
        "Hui Li",
        "Guimin Huang",
        "Yiqun Li",
        "Xiaowei Zhang",
        "Yabing Wang",
        "Jun Li"
      ],
      "year": 2023,
      "citation_count": 5,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/2e3e8a56981df1e33d93284be43f81704abc5795",
      "pdf_link": "",
      "venue": "International Journal of Computational Intelligence Systems",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "d700cd5e6fec5d138abf754fe463443ef5f47a95",
      "title": "Z-Score Normalization, Hubness, and Few-Shot Learning",
      "abstract": "The goal of few-shot learning (FSL) is to recognize a set of novel classes with only few labeled samples by exploiting a large set of abundant base class samples. Adopting a meta-learning framework, most recent FSL methods meta-learn a deep feature embedding network, and during inference classify novel class samples using nearest neighbor in the learned high-dimensional embedding space. This means that these methods are prone to the hubness problem, that is, a certain class prototype becomes the nearest neighbor of many test instances regardless which classes they belong to. However, this problem is largely ignored in existing FSL studies. In this work, for the first time we show that many FSL methods indeed suffer from the hubness problem. To mitigate its negative effects, we further propose to employ z-score feature normalization, a simple yet effective trans-formation, during meta-training. A theoretical analysis is provided on why it helps. Extensive experiments are then conducted to show that with z-score normalization, the performance of many recent FSL methods can be boosted, resulting in new state-of-the-art on three benchmarks.",
      "authors": [
        "Nanyi Fei",
        "Yizhao Gao",
        "Zhiwu Lu",
        "Tao Xiang"
      ],
      "year": 2021,
      "citation_count": 105,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/d700cd5e6fec5d138abf754fe463443ef5f47a95",
      "pdf_link": "",
      "venue": "IEEE International Conference on Computer Vision",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "3b479d5df78cd259989d8bbac5d68926d846d3ba",
      "title": "Meta Learning for Few-Shot Joint Intent Detection and Slot-Filling",
      "abstract": "Intent detection and slot filling are the two main tasks in natural language understanding module in goal oriented conversational agents. Models which optimize these two objectives simultaneously within a single network (joint models) have proven themselves to be superior to mono-objective networks. However, these data-intensive deep learning approaches have not been successful in catering the demand of the industry for adaptable, multilingual dialogue systems. To this end, we cast joint intent detection as an n-way k-shot classification problem and establish it within meta learning setup. Our approach is motivated by the success of meta learning on few-shot image classification tasks. We empirically demonstrate that, our approach can meta-learn a prior from similar tasks under highly resource constrained settings which enable rapid inference on target tasks. First, we show the adaptability of proposed approach by meta learning n-way k-shot joint intent detection using set of intents and evaluating on a completely new set of intents. Second, we exemplify the cross-lingual adaptability by learning a prior, utilizing English utterances and evaluating on Spanish and Thai utterances. Compared to random initialization, our method significantly improves the accuracy in both intent detection and slot-filling.",
      "authors": [
        "H. S. Bhathiya",
        "Uthayasanker Thayasivam"
      ],
      "year": 2020,
      "citation_count": 22,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/3b479d5df78cd259989d8bbac5d68926d846d3ba",
      "pdf_link": "",
      "venue": "International Conference on Machine Learning Technologies",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "8d6d320dbe9bdd6a1ea844faaf3dc0f5fff2543b",
      "title": "Contrastive Learning-Based Bayes-Adaptive Meta-Reinforcement Learning for Active Pantograph Control in High-Speed Railways",
      "abstract": "The fluctuation of pantograph–catenary contact force seriously affects the current collection quality, maintenance cost, and operation safety of high-speed trains. In recent years, agents developed with deep reinforcement learning (DRL) technology have achieved significant success. However, most of these jobs are restricted to narrow task distributions and stationary environments, which require a large amount of training data and cannot adapt quickly to the new task. We propose a contrastive learning-based Bayes-adaptive meta-reinforcement learning (CBAMRL) algorithm that addresses these limitations, enabling agents to learn new skills from a few transitions and adapt to the new environment. We first introduce a Bayes-adaptive training strategy, achieving zero-shot adaptation in nonstationary environments with high sample efficiency and competitive asymptotic performance. We proposed a contrastive learning-based contextual encoder to represent complex task distributions with similar structures, providing compact and sufficient task representation without modeling irrelevant dependencies. We evaluate the proposed method on a validated pantograph–catenary system (PCS) benchmark. Compared to the state-of-the-art DRL approach and traditional solutions, the experiment result demonstrates that the proposed algorithm can swiftly adapt to new operating circumstances and unknown perturbations with well-structured task representation and zero-shot adaptation.",
      "authors": [
        "Hui Wang",
        "Zhiwei Han",
        "Xufan Wang",
        "Yanbo Wu",
        "Zhigang Liu"
      ],
      "year": 2024,
      "citation_count": 8,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/8d6d320dbe9bdd6a1ea844faaf3dc0f5fff2543b",
      "pdf_link": "",
      "venue": "IEEE Transactions on Transportation Electrification",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "1aed7b21007519b7482ac4a005d5f2e0c7a1d047",
      "title": "Meta-learning Adaptive Deep Kernel Gaussian Processes for Molecular Property Prediction",
      "abstract": "We propose Adaptive Deep Kernel Fitting with Implicit Function Theorem (ADKF-IFT), a novel framework for learning deep kernel Gaussian processes (GPs) by interpolating between meta-learning and conventional deep kernel learning. Our approach employs a bilevel optimization objective where we meta-learn generally useful feature representations across tasks, in the sense that task-specific GP models estimated on top of such features achieve the lowest possible predictive loss on average. We solve the resulting nested optimization problem using the implicit function theorem (IFT). We show that our ADKF-IFT framework contains previously proposed Deep Kernel Learning (DKL) and Deep Kernel Transfer (DKT) as special cases. Although ADKF-IFT is a completely general method, we argue that it is especially well-suited for drug discovery problems and demonstrate that it significantly outperforms previous state-of-the-art methods on a variety of real-world few-shot molecular property prediction tasks and out-of-domain molecular property prediction and optimization tasks.",
      "authors": [
        "Wenlin Chen",
        "Austin Tripp",
        "José Miguel Hernández-Lobato"
      ],
      "year": 2022,
      "citation_count": 24,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/1aed7b21007519b7482ac4a005d5f2e0c7a1d047",
      "pdf_link": "",
      "venue": "International Conference on Learning Representations",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "287b8037b0f75caec9fab471dd48fe0b81090f74",
      "title": "A Unified Meta-Learning Framework for Fair Ranking With Curriculum Learning",
      "abstract": "In recent information retrieval systems, it is observed that the datasets used to train machine learning models can be biased, leading to systematic discrimination against certain demographic groups, which means the ranking utility of specific groups is often lower than others in a biased dataset. Training models on these datasets will further decrease the exposure of the minority groups. To address this problem, we propose a Meta Curriculum-based Fair Ranking framework (MCFR) which could alleviate the data bias issue through the weighted loss using gradient-based learning to learn. Specifically, we optimize a meta learner from a sampled dataset (meta-dataset), and meanwhile train a ranking model on the whole (biased) dataset. The meta-dataset is sampled with a curriculum learning scheduler to guide the meta learner's training to gradually mitigate the skewness towards biased attributes. The meta learner serves as a weighting function to make the ranking loss focus more on the minority group. We formulate the proposed MCFR as a bilevel optimization problem and solve it using gradients through gradients. Extensive experiments on real-world datasets demonstrate that our approach can be used as a generic framework to work with various ranking losses and fairness metrics.",
      "authors": [
        "Yuan Wang",
        "Zhiqiang Tao",
        "Yi Fang"
      ],
      "year": 2024,
      "citation_count": 6,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/287b8037b0f75caec9fab471dd48fe0b81090f74",
      "pdf_link": "",
      "venue": "IEEE Transactions on Knowledge and Data Engineering",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "e95e3a314cab21171e206cd0824fe93c1c47677c",
      "title": "iTAML: An Incremental Task-Agnostic Meta-learning Approach",
      "abstract": "Humans can continuously learn new knowledge as their experience grows. In contrast, previous learning in deep neural networks can quickly fade out when they are trained on a new task. In this paper, we hypothesize this problem can be avoided by learning a set of generalized parameters, that are neither specific to old nor new tasks. In this pursuit, we introduce a novel meta-learning approach that seeks to maintain an equilibrium between all the encountered tasks. This is ensured by a new meta-update rule which avoids catastrophic forgetting. In comparison to previous meta-learning techniques, our approach is task-agnostic. When presented with a continuum of data, our model automatically identifies the task and quickly adapts to it with just a single update. We perform extensive experiments on five datasets in a class-incremental setting, leading to significant improvements over the state of the art methods (e.g., a 21.3% boost on CIFAR100 with 10 incremental tasks). Specifically, on large-scale datasets that generally prove difficult cases for incremental learning, our approach delivers absolute gains as high as 19.1% and 7.4% on ImageNet and MS-Celeb datasets, respectively.",
      "authors": [
        "Jathushan Rajasegaran",
        "Salman Hameed Khan",
        "Munawar Hayat",
        "F. Khan",
        "M. Shah"
      ],
      "year": 2020,
      "citation_count": 159,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/e95e3a314cab21171e206cd0824fe93c1c47677c",
      "pdf_link": "",
      "venue": "Computer Vision and Pattern Recognition",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "15561ab20c298e113b0008b7a029486a422e7ca3",
      "title": "Bilevel Programming for Hyperparameter Optimization and Meta-Learning",
      "abstract": "We introduce a framework based on bilevel programming that unifies gradient-based hyperparameter optimization and meta-learning. We show that an approximate version of the bilevel problem can be solved by taking into explicit account the optimization dynamics for the inner objective. Depending on the specific setting, the outer variables take either the meaning of hyperparameters in a supervised learning problem or parameters of a meta-learner. We provide sufficient conditions under which solutions of the approximate problem converge to those of the exact problem. We instantiate our approach for meta-learning in the case of deep learning where representation layers are treated as hyperparameters shared across a set of training episodes. In experiments, we confirm our theoretical findings, present encouraging results for few-shot learning and contrast the bilevel approach against classical approaches for learning-to-learn.",
      "authors": [
        "Luca Franceschi",
        "P. Frasconi",
        "Saverio Salzo",
        "Riccardo Grazzi",
        "M. Pontil"
      ],
      "year": 2018,
      "citation_count": 762,
      "layer": 1,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/15561ab20c298e113b0008b7a029486a422e7ca3",
      "pdf_link": "",
      "venue": "International Conference on Machine Learning",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "f68020d22d9895d0d7f173b14961459395f96861",
      "title": "Deep Relation Network for Hyperspectral Image Few-Shot Classification",
      "abstract": "Deep learning has achieved great success in hyperspectral image classification. However, when processing new hyperspectral images, the existing deep learning models must be retrained from scratch with sufficient samples, which is inefficient and undesirable in practical tasks. This paper aims to explore how to accurately classify new hyperspectral images with only a few labeled samples, i.e., the hyperspectral images few-shot classification. Specifically, we design a new deep classification model based on relational network and train it with the idea of meta-learning. Firstly, the feature learning module and the relation learning module of the model can make full use of the spatial–spectral information in hyperspectral images and carry out relation learning by comparing the similarity between samples. Secondly, the task-based learning strategy can enable the model to continuously enhance its ability to learn how to learn with a large number of tasks randomly generated from different data sets. Benefitting from the above two points, the proposed method has excellent generalization ability and can obtain satisfactory classification results with only a few labeled samples. In order to verify the performance of the proposed method, experiments were carried out on three public data sets. The results indicate that the proposed method can achieve better classification results than the traditional semisupervised support vector machine and semisupervised deep learning models.",
      "authors": [
        "Kuiliang Gao",
        "Bing Liu",
        "Xuchu Yu",
        "Jinchun Qin",
        "Pengqiang Zhang",
        "Xiong Tan"
      ],
      "year": 2020,
      "citation_count": 143,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/f68020d22d9895d0d7f173b14961459395f96861",
      "pdf_link": "",
      "venue": "Remote Sensing",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "859e953bba919a6f989d440b6c23ab19a8cb855b",
      "title": "HyperMAML: Few-Shot Adaptation of Deep Models with Hypernetworks",
      "abstract": "The aim of Few-Shot learning methods is to train models which can easily adapt to previously unseen tasks, based on small amounts of data. One of the most popular and elegant Few-Shot learning approaches is Model-Agnostic Meta-Learning (MAML). The main idea behind this method is to learn the general weights of the meta-model, which are further adapted to specific problems in a small number of gradient steps. However, the model's main limitation lies in the fact that the update procedure is realized by gradient-based optimisation. In consequence, MAML cannot always modify weights to the essential level in one or even a few gradient iterations. On the other hand, using many gradient steps results in a complex and time-consuming optimization procedure, which is hard to train in practice, and may lead to overfitting. In this paper, we propose HyperMAML, a novel generalization of MAML, where the training of the update procedure is also part of the model. Namely, in HyperMAML, instead of updating the weights with gradient descent, we use for this purpose a trainable Hypernetwork. Consequently, in this framework, the model can generate significant updates whose range is not limited to a fixed number of gradient steps. Experiments show that HyperMAML consistently outperforms MAML and performs comparably to other state-of-the-art techniques in a number of standard Few-Shot learning benchmarks.",
      "authors": [
        "Marcin Przewiezlikowski",
        "P. Przybysz",
        "J. Tabor",
        "Maciej Ziȩba",
        "P. Spurek"
      ],
      "year": 2022,
      "citation_count": 21,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/859e953bba919a6f989d440b6c23ab19a8cb855b",
      "pdf_link": "",
      "venue": "Neurocomputing",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "04396f17e2bdc848300b8670104895b0b3fee84f",
      "title": "When Meta-Learning Meets Online and Continual Learning: A Survey",
      "abstract": "Over the past decade, deep neural networks have demonstrated significant success using the training scheme that involves mini-batch stochastic gradient descent on extensive datasets. Expanding upon this accomplishment, there has been a surge in research exploring the application of neural networks in other learning scenarios. One notable framework that has garnered significant attention is meta-learning. Often described as “learning to learn,” meta-learning is a data-driven approach to optimize the learning algorithm. Other branches of interest are continual learning and online learning, both of which involve incrementally updating a model with streaming data. While these frameworks were initially developed independently, recent works have started investigating their combinations, proposing novel problem settings and learning algorithms. However, due to the elevated complexity and lack of unified terminology, discerning differences between the learning frameworks can be challenging even for experienced researchers. To facilitate a clear understanding, this paper provides a comprehensive survey that organizes various problem settings using consistent terminology and formal descriptions. By offering an overview of these learning paradigms, our work aims to foster further advancements in this promising area of research.",
      "authors": [
        "Jaehyeon Son",
        "Soochan Lee",
        "Gunhee Kim"
      ],
      "year": 2023,
      "citation_count": 16,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/04396f17e2bdc848300b8670104895b0b3fee84f",
      "pdf_link": "",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "31eba23839649c21c3e462a7568b6b72041d4b5c",
      "title": "Meta-Reinforcement Learning in Non-Stationary and Dynamic Environments",
      "abstract": "In recent years, the subject of deep reinforcement learning (DRL) has developed very rapidly, and is now applied in various fields, such as decision making and control tasks. However, artificial agents trained with RL algorithms require great amounts of training data, unlike humans that are able to learn new skills from very few examples. The concept of meta-reinforcement learning (meta-RL) has been recently proposed to enable agents to learn similar but new skills from a small amount of experience by leveraging a set of tasks with a shared structure. Due to the task representation learning strategy with few-shot adaptation, most recent work is limited to narrow task distributions and stationary environments, where tasks do not change within episodes. In this work, we address those limitations and introduce a training strategy that is applicable to non-stationary environments, as well as a task representation based on Gaussian mixture models to model clustered task distributions. We evaluate our method on several continuous robotic control benchmarks. Compared with state-of-the-art literature that is only applicable to stationary environments with few-shot adaption, our algorithm first achieves competitive asymptotic performance and superior sample efficiency in stationary environments with zero-shot adaption. Second, our algorithm learns to perform successfully in non-stationary settings as well as a continual learning setting, while learning well-structured task representations. Last, our algorithm learns basic distinct behaviors and well-structured task representations in task distributions with multiple qualitatively distinct tasks.",
      "authors": [
        "Zhenshan Bing",
        "David Lerch",
        "Kai Huang",
        "Alois Knoll"
      ],
      "year": 2022,
      "citation_count": 53,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/31eba23839649c21c3e462a7568b6b72041d4b5c",
      "pdf_link": "",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "bfb22a7d0f64625b897ebfe3a3f7498d5c71cbb1",
      "title": "Providing URLLC Service in Multi-STAR-RIS Assisted and Full-Duplex Cellular Wireless Systems: A Meta-Learning Approach",
      "abstract": "The Simultaneously Transmitting and Reflecting Reconfigurable Intelligent Surface (STAR-RIS) technology is an innovative approach that aims to enhance the performance of sixth-generation (6G) wireless networks. This study focuses on a multi-STAR-RIS and full-duplex (FD) communication system aimed at providing ultra-reliable low-latency communication (URLLC) services. To maximize the total uplink (UL) and downlink (DL) rates, beamforming and combining vectors at the base station (BS), the transmit power of UL users, the amplitude attenuations, and phase shifts of the STAR-RISs are jointly optimized. These optimizations take into account the maximum transmit power constraints of the BS and UL users, as well as the quality of service requirements of UL and DL users. Given the non-convex nature of the optimization problem, this study proposes a novel deep reinforcement learning algorithm called Meta DDPG, which combines meta-learning and deep deterministic policy gradient. Numerical results demonstrate that a multi-STAR-RIS assisted system can obtain a higher system total rate compared to the conventional multi-RIS assisted system.",
      "authors": [
        "Y. Eghbali",
        "Shiva Kazemi Taskou",
        "Mohammad Robat Mili",
        "M. Rasti",
        "Ekram Hossain"
      ],
      "year": 2024,
      "citation_count": 8,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/bfb22a7d0f64625b897ebfe3a3f7498d5c71cbb1",
      "pdf_link": "",
      "venue": "IEEE Communications Letters",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "c267e53f823a2ef9e9e6bbc26196d68b789fb4c2",
      "title": "Learning to Learn Better Visual Prompts",
      "abstract": "Prompt tuning provides a low-cost way of adapting vision-language models (VLMs) for various downstream vision tasks without requiring updating the huge pre-trained parameters. Dispensing with the conventional manual crafting of prompts, the recent prompt tuning method of Context Optimization (CoOp) introduces adaptable vectors as text prompts. Nevertheless, several previous works point out that the CoOp-based approaches are easy to overfit to the base classes and hard to generalize to novel classes. In this paper, we reckon that the prompt tuning works well only in the base classes because of the limited capacity of the adaptable vectors. The scale of the pre-trained model is hundreds times the scale of the adaptable vector, thus the learned vector has a very limited ability to absorb the knowledge of novel classes. To minimize this excessive overfitting of textual knowledge on the base class, we view prompt tuning as learning to learn (LoL) and learn the prompt in the way of meta-learning, the training manner of dividing the base classes into many different subclasses could fully exert the limited capacity of prompt tuning and thus transfer it power to recognize the novel classes. To be specific, we initially perform fine-tuning on the base class based on the CoOp method for pre-trained CLIP. Subsequently, predicated on the fine-tuned CLIP model, we carry out further fine-tuning in an N-way K-shot manner from the perspective of meta-learning on the base classes. We finally apply the learned textual vector and VLM for unseen classes.Extensive experiments on benchmark datasets validate the efficacy of our meta-learning-informed prompt tuning, affirming its role as a robust optimization strategy for VLMs.",
      "authors": [
        "Fengxiang Wang",
        "Wanrong Huang",
        "Shaowu Yang",
        "Qi Fan",
        "Long Lan"
      ],
      "year": 2024,
      "citation_count": 12,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/c267e53f823a2ef9e9e6bbc26196d68b789fb4c2",
      "pdf_link": "",
      "venue": "AAAI Conference on Artificial Intelligence",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "61b03c891489247bcb5ad432b4d485784a274fb4",
      "title": "Deep Meta-Learning Energy-Aware Path Planner for Unmanned Ground Vehicles in Unknown Terrains",
      "abstract": "This paper presents an adaptive energy-aware prediction and planning framework for vehicles navigating over terrains with varying and unknown properties. A novel feature of the method is the use of a deep meta-learning framework to learn a prior energy model, which can efficiently adapt to the local terrain conditions based on small quantities of exteroceptive and proprioceptive data. A meta-adaptive heuristic function is also proposed for the integration of the energy model into an A* path planner. The performance of the proposed approach is assessed in a 3D-body dynamic simulator over several typologies of deformable terrains and compared with alternative machine learning solutions. We provide evidence of the advantages of the proposed method to adapt to unforeseen terrain conditions, thereby yielding more informed estimations and energy-efficient paths when navigating on unknown terrains.",
      "authors": [
        "Marco Visca",
        "R. Powell",
        "Yang Gao",
        "Saber Fallah"
      ],
      "year": 2021,
      "citation_count": 8,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/61b03c891489247bcb5ad432b4d485784a274fb4",
      "pdf_link": "",
      "venue": "IEEE Access",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "4625628163a2ee0e6cd320cd7a14b4ccded2a631",
      "title": "Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables",
      "abstract": "Deep reinforcement learning algorithms require large amounts of experience to learn an individual task. While in principle meta-reinforcement learning (meta-RL) algorithms enable agents to learn new skills from small amounts of experience, several major challenges preclude their practicality. Current methods rely heavily on on-policy experience, limiting their sample efficiency. The also lack mechanisms to reason about task uncertainty when adapting to new tasks, limiting their effectiveness in sparse reward problems. In this paper, we address these challenges by developing an off-policy meta-RL algorithm that disentangles task inference and control. In our approach, we perform online probabilistic filtering of latent task variables to infer how to solve a new task from small amounts of experience. This probabilistic interpretation enables posterior sampling for structured and efficient exploration. We demonstrate how to integrate these task variables with off-policy RL algorithms to achieve both meta-training and adaptation efficiency. Our method outperforms prior algorithms in sample efficiency by 20-100X as well as in asymptotic performance on several meta-RL benchmarks.",
      "authors": [
        "Kate Rakelly",
        "Aurick Zhou",
        "Deirdre Quillen",
        "Chelsea Finn",
        "S. Levine"
      ],
      "year": 2019,
      "citation_count": 694,
      "layer": 1,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/4625628163a2ee0e6cd320cd7a14b4ccded2a631",
      "pdf_link": "",
      "venue": "International Conference on Machine Learning",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "16a3dd5ab3e8570f6083ddf6f88aa5e916450fef",
      "title": "Generative Teaching Networks: Accelerating Neural Architecture Search by Learning to Generate Synthetic Training Data",
      "abstract": "This paper investigates the intriguing question of whether we can create learning algorithms that automatically generate training data, learning environments, and curricula in order to help AI agents rapidly learn. We show that such algorithms are possible via Generative Teaching Networks (GTNs), a general approach that is, in theory, applicable to supervised, unsupervised, and reinforcement learning, although our experiments only focus on the supervised case. GTNs are deep neural networks that generate data and/or training environments that a learner (e.g. a freshly initialized neural network) trains on for a few SGD steps before being tested on a target task. We then differentiate through the entire learning process via meta-gradients to update the GTN parameters to improve performance on the target task. GTNs have the beneficial property that they can theoretically generate any type of data or training environment, making their potential impact large. This paper introduces GTNs, discusses their potential, and showcases that they can substantially accelerate learning. We also demonstrate a practical and exciting application of GTNs: accelerating the evaluation of candidate architectures for neural architecture search (NAS), which is rate-limited by such evaluations, enabling massive speed-ups in NAS. GTN-NAS improves the NAS state of the art, finding higher performing architectures when controlling for the search proposal mechanism. GTN-NAS also is competitive with the overall state of the art approaches, which achieve top performance while using orders of magnitude less computation than typical NAS methods. Speculating forward, GTNs may represent a first step toward the ambitious goal of algorithms that generate their own training data and, in doing so, open a variety of interesting new research questions and directions.",
      "authors": [
        "F. Such",
        "Aditya Rawal",
        "J. Lehman",
        "Kenneth O. Stanley",
        "J. Clune"
      ],
      "year": 2019,
      "citation_count": 164,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/16a3dd5ab3e8570f6083ddf6f88aa5e916450fef",
      "pdf_link": "",
      "venue": "International Conference on Machine Learning",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "9342fce9c5a69f545a778ca7e885ba9d63af928f",
      "title": "Offline Meta Reinforcement Learning",
      "abstract": "Consider the following problem, which we term Offline Meta Reinforcement Learning (OMRL): given the complete training histories of $N$ conventional RL agents, trained on $N$ different tasks, design a learning agent that can quickly maximize reward in a new, unseen task from the same task distribution. In particular, while each conventional RL agent explored and exploited its own different task, the OMRL agent must identify regularities in the data that lead to effective exploration/exploitation in the unseen task. To solve OMRL, we take a Bayesian RL (BRL) view, and seek to learn a Bayes-optimal policy from the offline data. We extend the recently proposed VariBAD BRL algorithm to the off-policy setting, and demonstrate learning of Bayes-optimal exploration strategies from offline data using deep neural networks. Furthermore, when applied to the online meta-RL setting (agent simultaneously collects data and improves its meta-RL policy), our method is significantly more sample efficient than the conventional VariBAD.",
      "authors": [
        "Ron Dorfman",
        "Aviv Tamar"
      ],
      "year": 2020,
      "citation_count": 20,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/9342fce9c5a69f545a778ca7e885ba9d63af928f",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "4e0a735ee8f7606dd13633a88de15f6dfe3348ac",
      "title": "Deep Meta Metric Learning",
      "abstract": "In this paper, we present a deep meta metric learning (DMML) approach for visual recognition. Unlike most existing deep metric learning methods formulating the learning process by an overall objective, our DMML formulates the metric learning in a meta way, and proves that softmax and triplet loss are consistent in the meta space. Specifically, we sample some subsets from the original training set and learn metrics across different subsets. In each sampled sub-task, we split the training data into a support set as well as a query set, and learn the set-based distance, instead of sample-based one, to verify the query cell from multiple support cells. In addition, we introduce hard sample mining for set-based distance to encourage the intra-class compactness. Experimental results on three visual recognition applications including person re-identification, vehicle re-identification and face verification show that the proposed DMML method outperforms most existing approaches.",
      "authors": [
        "Guangyi Chen",
        "Tianren Zhang",
        "Jiwen Lu",
        "Jie Zhou"
      ],
      "year": 2019,
      "citation_count": 61,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/4e0a735ee8f7606dd13633a88de15f6dfe3348ac",
      "pdf_link": "",
      "venue": "IEEE International Conference on Computer Vision",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "b0d8165eecf2aa04a85e701d0c6bb4edd4b3811b",
      "title": "A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning",
      "abstract": "To achieve general intelligence, agents must learn how to interact with others in a shared environment: this is the challenge of multiagent reinforcement learning (MARL). The simplest form is independent reinforcement learning (InRL), where each agent treats its experience as part of its (non-stationary) environment. In this paper, we first observe that policies learned using InRL can overfit to the other agents' policies during training, failing to sufficiently generalize during execution. We introduce a new metric, joint-policy correlation, to quantify this effect. We describe an algorithm for general MARL, based on approximate best responses to mixtures of policies generated using deep reinforcement learning, and empirical game-theoretic analysis to compute meta-strategies for policy selection. The algorithm generalizes previous ones such as InRL, iterated best response, double oracle, and fictitious play. Then, we present a scalable implementation which reduces the memory requirement using decoupled meta-solvers. Finally, we demonstrate the generality of the resulting policies in two partially observable settings: gridworld coordination games and poker.",
      "authors": [
        "Marc Lanctot",
        "V. Zambaldi",
        "A. Gruslys",
        "Angeliki Lazaridou",
        "K. Tuyls",
        "J. Pérolat",
        "David Silver",
        "T. Graepel"
      ],
      "year": 2017,
      "citation_count": 660,
      "layer": 1,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/b0d8165eecf2aa04a85e701d0c6bb4edd4b3811b",
      "pdf_link": "",
      "venue": "Neural Information Processing Systems",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "482c0cbfffa77154e3c879c497f50b605297d5bc",
      "title": "One-Shot Visual Imitation Learning via Meta-Learning",
      "abstract": "In order for a robot to be a generalist that can perform a wide range of jobs, it must be able to acquire a wide variety of skills quickly and efficiently in complex unstructured environments. High-capacity models such as deep neural networks can enable a robot to represent complex skills, but learning each skill from scratch then becomes infeasible. In this work, we present a meta-imitation learning method that enables a robot to learn how to learn more efficiently, allowing it to acquire new skills from just a single demonstration. Unlike prior methods for one-shot imitation, our method can scale to raw pixel inputs and requires data from significantly fewer prior tasks for effective learning of new skills. Our experiments on both simulated and real robot platforms demonstrate the ability to learn new tasks, end-to-end, from a single visual demonstration.",
      "authors": [
        "Chelsea Finn",
        "Tianhe Yu",
        "Tianhao Zhang",
        "P. Abbeel",
        "S. Levine"
      ],
      "year": 2017,
      "citation_count": 577,
      "layer": 1,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/482c0cbfffa77154e3c879c497f50b605297d5bc",
      "pdf_link": "",
      "venue": "Conference on Robot Learning",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "24411be9cbb7ca4bc27fb6e3285601405e39061f",
      "title": "Single neuromorphic memristor closely emulates multiple synaptic mechanisms for energy efficient neural networks",
      "abstract": "Biological neural networks do not only include long-term memory and weight multiplication capabilities, as commonly assumed in artificial neural networks, but also more complex functions such as short-term memory, short-term plasticity, and meta-plasticity - all collocated within each synapse. Here, we demonstrate memristive nano-devices based on SrTiO3 that inherently emulate all these synaptic functions. These memristors operate in a non-filamentary, low conductance regime, which enables stable and energy efficient operation. They can act as multi-functional hardware synapses in a class of bio-inspired deep neural networks (DNN) that make use of both long- and short-term synaptic dynamics and are capable of meta-learning or learning-to-learn. The resulting bio-inspired DNN is then trained to play the video game Atari Pong, a complex reinforcement learning task in a dynamic environment. Our analysis shows that the energy consumption of the DNN with multi-functional memristive synapses decreases by about two orders of magnitude as compared to a pure GPU implementation. Based on this finding, we infer that memristive devices with a better emulation of the synaptic functionalities do not only broaden the applicability of neuromorphic computing, but could also improve the performance and energy costs of certain artificial intelligence applications.",
      "authors": [
        "C. Weilenmann",
        "A. Ziogas",
        "T. Zellweger",
        "K. Portner",
        "Marko Mladenovi'c",
        "M. Kaniselvan",
        "Timoleon Moraitis",
        "Mathieu Luisier",
        "A. Emboras"
      ],
      "year": 2024,
      "citation_count": 33,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/24411be9cbb7ca4bc27fb6e3285601405e39061f",
      "pdf_link": "",
      "venue": "Nature Communications",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "e35e0ad5959c3160d66309c3c1e10df9b4352c6d",
      "title": "Self-supervised Knowledge Distillation for Few-shot Learning",
      "abstract": "Real-world contains an overwhelmingly large number of object classes, learning all of which at once is infeasible. Few shot learning is a promising learning paradigm due to its ability to learn out of order distributions quickly with only a few samples. Recent works [7, 41] show that simply learning a good feature embedding can outperform more sophisticated meta-learning and metric learning algorithms for few-shot learning. In this paper, we propose a simple approach to improve the representation capacity of deep neural networks for few-shot learning tasks. We follow a two-stage learning process: First, we train a neural network to maximize the entropy of the feature embedding, thus creating an optimal output manifold using a self-supervised auxiliary loss. In the second stage, we minimize the entropy on feature embedding by bringing self-supervised twins together, while constraining the manifold with student-teacher distillation. Our experiments show that, even in the first stage, self-supervision can outperform current state-of-the-art methods, with further gains achieved by our second stage distillation process. Our codes are available at: this https URL.",
      "authors": [
        "Jathushan Rajasegaran",
        "Salman Hameed Khan",
        "Munawar Hayat",
        "F. Khan",
        "M. Shah"
      ],
      "year": 2020,
      "citation_count": 100,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/e35e0ad5959c3160d66309c3c1e10df9b4352c6d",
      "pdf_link": "",
      "venue": "British Machine Vision Conference",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "d1ad1bfa0bb76002b10e7f211b937842baeb28d9",
      "title": "Meta-Reinforcement Learning via Language Instructions",
      "abstract": "Although deep reinforcement learning has recently been very successful at learning complex behaviors, it requires a tremendous amount of data to learn a task. One of the fundamental reasons causing this limitation lies in the nature of the trial-and-error learning paradigm of reinforcement learning, where the agent communicates with the environment and pro-gresses in the learning only relying on the reward signal. This is implicit and rather insufficient to learn a task well. On the con-trary, humans are usually taught new skills via natural language instructions. Utilizing language instructions for robotic motion control to improve the adaptability is a recently emerged topic and challenging. In this paper, we present a meta-RL algorithm that addresses the challenge of learning skills with language instructions in multiple manipulation tasks. On the one hand, our algorithm utilizes the language instructions to shape its in-terpretation of the task, on the other hand, it still learns to solve task in a trial-and-error process. We evaluate our algorithm on the robotic manipulation benchmark (Meta-World) and it significantly outperforms state-of-the-art methods in terms of training and testing task success rates. Codes are available at https://tumi6robot.wixsite.com/million.",
      "authors": [
        "Zhenshan Bing",
        "A. Koch",
        "Xiangtong Yao",
        "Kai Huang",
        "Alois Knoll"
      ],
      "year": 2022,
      "citation_count": 21,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/d1ad1bfa0bb76002b10e7f211b937842baeb28d9",
      "pdf_link": "",
      "venue": "IEEE International Conference on Robotics and Automation",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "06b8e82542d1873928d007548a23d3b77daa11f8",
      "title": "Urban Traffic Prediction from Spatio-Temporal Data Using Deep Meta Learning",
      "abstract": "Predicting urban traffic is of great importance to intelligent transportation systems and public safety, yet is very challenging because of two aspects: 1) complex spatio-temporal correlations of urban traffic, including spatial correlations between locations along with temporal correlations among timestamps; 2) diversity of such spatio-temporal correlations, which vary from location to location and depend on the surrounding geographical information, e.g., points of interests and road networks. To tackle these challenges, we proposed a deep-meta-learning based model, entitled ST-MetaNet, to collectively predict traffic in all location at once. ST-MetaNet employs a sequence-to-sequence architecture, consisting of an encoder to learn historical information and a decoder to make predictions step by step. In specific, the encoder and decoder have the same network structure, consisting of a recurrent neural network to encode the traffic, a meta graph attention network to capture diverse spatial correlations, and a meta recurrent neural network to consider diverse temporal correlations. Extensive experiments were conducted based on two real-world datasets to illustrate the effectiveness of ST-MetaNet beyond several state-of-the-art methods.",
      "authors": [
        "Zheyi Pan",
        "Yuxuan Liang",
        "Weifeng Wang",
        "Yong Yu",
        "Yu Zheng",
        "Junbo Zhang"
      ],
      "year": 2019,
      "citation_count": 557,
      "layer": 1,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/06b8e82542d1873928d007548a23d3b77daa11f8",
      "pdf_link": "",
      "venue": "Knowledge Discovery and Data Mining",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "bfe284e4338e62f0a61bb33398353efd687f206f",
      "title": "Learning to Compare: Relation Network for Few-Shot Learning",
      "abstract": "We present a conceptually simple, flexible, and general framework for few-shot learning, where a classifier must learn to recognise new classes given only few examples from each. Our method, called the Relation Network (RN), is trained end-to-end from scratch. During meta-learning, it learns to learn a deep distance metric to compare a small number of images within episodes, each of which is designed to simulate the few-shot setting. Once trained, a RN is able to classify images of new classes by computing relation scores between query images and the few examples of each new class without further updating the network. Besides providing improved performance on few-shot learning, our framework is easily extended to zero-shot learning. Extensive experiments on five benchmarks demonstrate that our simple approach provides a unified and effective approach for both of these two tasks.",
      "authors": [
        "Flood Sung",
        "Yongxin Yang",
        "Li Zhang",
        "T. Xiang",
        "Philip H. S. Torr",
        "Timothy M. Hospedales"
      ],
      "year": 2017,
      "citation_count": 4190,
      "layer": 1,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/bfe284e4338e62f0a61bb33398353efd687f206f",
      "pdf_link": "",
      "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "e5638e677d40c9ea67401b9b5241f381a73be6fd",
      "title": "A Full-Reference Image Quality Assessment Method via Deep Meta-Learning and Conformer",
      "abstract": "In this paper, a full-reference image quality assessment (FR-IQA) model based on deep meta-learning and Conformer is proposed. We combine the Conformer architecture with a Siamese network to extract the feature vectors of the reference and distorted images and calculate the similarity of these feature vectors as the predicted score of the image. We use meta-learning to help the model identify different types of image distortion. First, because the information taken as input by the human visual system (HVS) ranges in scale from local to global, we use a Conformer network as a feature extractor to obtain the global and local features of the pristine and distorted images and use a Siamese network to reduce the number of parameters in our model. Second, we use meta-learning to carry out bilevel gradient descent from the query set to the support set in the training stage and fine-tune the model parameters on a few images with unknown distortion types in the testing stage to improve the generalization ability of the model. Experiments show that our method is competitive with existing FR-IQA methods on three standard IQA datasets.",
      "authors": [
        "Shujun Lang",
        "Xu Liu",
        "Mingliang Zhou",
        "Jun Luo",
        "Huayan Pu",
        "Zhuang Xu",
        "Jason Wang",
        "Xuekai Wei",
        "Taiping Zhang",
        "Yong Feng",
        "Zhaowei Shang"
      ],
      "year": 2024,
      "citation_count": 12,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/e5638e677d40c9ea67401b9b5241f381a73be6fd",
      "pdf_link": "",
      "venue": "IEEE transactions on broadcasting",
      "paper_type": "",
      "keywords": []
    }
  ],
  "edges": [
    {
      "source": "020bb2ba5f3923858cd6882ba5c5a44ea8041ab6",
      "target": "d726e991e68ed892bd4c42c8c8150ebc71ae1b9e",
      "weight": 0.23948388747623006
    },
    {
      "source": "020bb2ba5f3923858cd6882ba5c5a44ea8041ab6",
      "target": "a962dc06a19c08bb76184bde864e7f1e2e502150",
      "weight": 0.2997781384880144
    },
    {
      "source": "020bb2ba5f3923858cd6882ba5c5a44ea8041ab6",
      "target": "3b32351004d1628329b875576323a7b1767e9e5a",
      "weight": 0.28430531542580123
    },
    {
      "source": "020bb2ba5f3923858cd6882ba5c5a44ea8041ab6",
      "target": "205770123d5779da5470ae58cf446bc3e9cfc195",
      "weight": 0.2187522845440028
    },
    {
      "source": "020bb2ba5f3923858cd6882ba5c5a44ea8041ab6",
      "target": "a1c68c32b11d83c9d48c48163f2a445ce359069e",
      "weight": 0.29515856078123776
    },
    {
      "source": "020bb2ba5f3923858cd6882ba5c5a44ea8041ab6",
      "target": "7d0216a7331ee4031fe488c8ff1da2adfcc59a0c",
      "weight": 0.265493753623483
    },
    {
      "source": "020bb2ba5f3923858cd6882ba5c5a44ea8041ab6",
      "target": "f8ee167e718cb152d816f06d42c66efec729a536",
      "weight": 0.386542013355416
    },
    {
      "source": "020bb2ba5f3923858cd6882ba5c5a44ea8041ab6",
      "target": "b8a16fd8d823cfe683c19d58bec77a023b5bf1ef",
      "weight": 0.33058101904245374
    },
    {
      "source": "020bb2ba5f3923858cd6882ba5c5a44ea8041ab6",
      "target": "52f37e9bd84547db2ecefed420715f312827c398",
      "weight": 0.24983240056100006
    },
    {
      "source": "020bb2ba5f3923858cd6882ba5c5a44ea8041ab6",
      "target": "c267e53f823a2ef9e9e6bbc26196d68b789fb4c2",
      "weight": 0.25635505451016405
    },
    {
      "source": "020bb2ba5f3923858cd6882ba5c5a44ea8041ab6",
      "target": "e5638e677d40c9ea67401b9b5241f381a73be6fd",
      "weight": 0.2393419304685005
    },
    {
      "source": "020bb2ba5f3923858cd6882ba5c5a44ea8041ab6",
      "target": "bfb22a7d0f64625b897ebfe3a3f7498d5c71cbb1",
      "weight": 0.2460672147674759
    },
    {
      "source": "020bb2ba5f3923858cd6882ba5c5a44ea8041ab6",
      "target": "24411be9cbb7ca4bc27fb6e3285601405e39061f",
      "weight": 0.23317465101578821
    },
    {
      "source": "020bb2ba5f3923858cd6882ba5c5a44ea8041ab6",
      "target": "bef33d15c3e8d433261f97f7001cc41a5ae0ec32",
      "weight": 0.2270498605380456
    },
    {
      "source": "020bb2ba5f3923858cd6882ba5c5a44ea8041ab6",
      "target": "b6efb87e4b609fb67304f73b8ee9c1984fce5e88",
      "weight": 0.28957335682493657
    },
    {
      "source": "020bb2ba5f3923858cd6882ba5c5a44ea8041ab6",
      "target": "04396f17e2bdc848300b8670104895b0b3fee84f",
      "weight": 0.37968540077979684
    },
    {
      "source": "020bb2ba5f3923858cd6882ba5c5a44ea8041ab6",
      "target": "1d421d179a2520ba23dc1375fe2989e4ba79b437",
      "weight": 0.25079423019443
    },
    {
      "source": "020bb2ba5f3923858cd6882ba5c5a44ea8041ab6",
      "target": "2e3e8a56981df1e33d93284be43f81704abc5795",
      "weight": 0.2832908737081379
    },
    {
      "source": "f4eff7c0127a2ef92c441f028c3bb15b64cabcc8",
      "target": "23bac2542b145bf2fcd17d7fa0a02ae03d0a45f7",
      "weight": 0.33679750937674807
    },
    {
      "source": "f4eff7c0127a2ef92c441f028c3bb15b64cabcc8",
      "target": "d0eb13325d77e50a60102139e84484a9beaf62ff",
      "weight": 0.06759447786978492
    },
    {
      "source": "f4eff7c0127a2ef92c441f028c3bb15b64cabcc8",
      "target": "13b00c6c8e6fd35a540b08904824aff0d6b66897",
      "weight": 0.27813139747703886
    },
    {
      "source": "bf8d58faf972ad0a1026c0a7c5577c07996ef3a7",
      "target": "98b41528c58e6f5b7b28be5b54029e52ca90c4ab",
      "weight": 0.386152846941694
    },
    {
      "source": "aea3f03299ff0cfea9b394f5559aa1c173f9876f",
      "target": "1aed7b21007519b7482ac4a005d5f2e0c7a1d047",
      "weight": 0.3639699213217824
    },
    {
      "source": "7bd95a62fd6320730cbb24a0e4fafac97d840652",
      "target": "e1a8bf47ef1c51298f5bcf957062c6ee63a96bfb",
      "weight": 0.2322110993591371
    },
    {
      "source": "7bd95a62fd6320730cbb24a0e4fafac97d840652",
      "target": "31eba23839649c21c3e462a7568b6b72041d4b5c",
      "weight": 0.4252747274160067
    },
    {
      "source": "7bd95a62fd6320730cbb24a0e4fafac97d840652",
      "target": "361e953f792a585496834ee14216b94d0ce9ae74",
      "weight": 0.12599882928061337
    },
    {
      "source": "282a380fb5ac26d99667224cef8c630f6882704f",
      "target": "e1a8bf47ef1c51298f5bcf957062c6ee63a96bfb",
      "weight": 0.24637261280008388
    },
    {
      "source": "282a380fb5ac26d99667224cef8c630f6882704f",
      "target": "d1ad1bfa0bb76002b10e7f211b937842baeb28d9",
      "weight": 0.3165166703314234
    },
    {
      "source": "282a380fb5ac26d99667224cef8c630f6882704f",
      "target": "31eba23839649c21c3e462a7568b6b72041d4b5c",
      "weight": 0.3913337988240491
    },
    {
      "source": "282a380fb5ac26d99667224cef8c630f6882704f",
      "target": "23bac2542b145bf2fcd17d7fa0a02ae03d0a45f7",
      "weight": 0.351060506120086
    },
    {
      "source": "282a380fb5ac26d99667224cef8c630f6882704f",
      "target": "754878242a3b480b2ca9031bff623f2c557f2caa",
      "weight": 0.04242776273793238
    },
    {
      "source": "282a380fb5ac26d99667224cef8c630f6882704f",
      "target": "332c44793b70776b9b966128c52e694222b1ab73",
      "weight": 0.2522135357806651
    },
    {
      "source": "282a380fb5ac26d99667224cef8c630f6882704f",
      "target": "9342fce9c5a69f545a778ca7e885ba9d63af928f",
      "weight": 0.4667479523806285
    },
    {
      "source": "282a380fb5ac26d99667224cef8c630f6882704f",
      "target": "759ae1234d46e2d1399ce9d642724738a766ed22",
      "weight": 0.3315448571908714
    },
    {
      "source": "282a380fb5ac26d99667224cef8c630f6882704f",
      "target": "f6271880cc1d7ff6514672366fe124fdb1212fb2",
      "weight": 0.2647892194031909
    },
    {
      "source": "282a380fb5ac26d99667224cef8c630f6882704f",
      "target": "c317d2faa26b38250960cf3d2e6cf095b9d5b92d",
      "weight": 0.08998216102436067
    },
    {
      "source": "282a380fb5ac26d99667224cef8c630f6882704f",
      "target": "d0eb13325d77e50a60102139e84484a9beaf62ff",
      "weight": 0.08622560270631381
    },
    {
      "source": "282a380fb5ac26d99667224cef8c630f6882704f",
      "target": "020bb2ba5f3923858cd6882ba5c5a44ea8041ab6",
      "weight": 0.07678332414893953
    },
    {
      "source": "282a380fb5ac26d99667224cef8c630f6882704f",
      "target": "361e953f792a585496834ee14216b94d0ce9ae74",
      "weight": 0.12526873583083117
    },
    {
      "source": "282a380fb5ac26d99667224cef8c630f6882704f",
      "target": "4625628163a2ee0e6cd320cd7a14b4ccded2a631",
      "weight": 0.44831411356089923
    },
    {
      "source": "282a380fb5ac26d99667224cef8c630f6882704f",
      "target": "66c2031ebf6407e50e309f4a989497353927859b",
      "weight": 0.23299440649932232
    },
    {
      "source": "282a380fb5ac26d99667224cef8c630f6882704f",
      "target": "42de54e614110c0c0a0bbbfee045e11e53eb4a7d",
      "weight": 0.2717740772723139
    },
    {
      "source": "282a380fb5ac26d99667224cef8c630f6882704f",
      "target": "26b07c6309ef12034571f20973097691a22d7116",
      "weight": 0.24531798834539112
    },
    {
      "source": "282a380fb5ac26d99667224cef8c630f6882704f",
      "target": "7f567f1e8972ff31a7ced59c329e7d75da645baf",
      "weight": 0.24086739304014754
    },
    {
      "source": "282a380fb5ac26d99667224cef8c630f6882704f",
      "target": "35ad6ba10006975c2bc67ecefaa9ee6af2453bdc",
      "weight": 0.28504531703960345
    },
    {
      "source": "282a380fb5ac26d99667224cef8c630f6882704f",
      "target": "2bdebf2fb0f5c21907fcaae6d87c7ba5811e778a",
      "weight": 0.335998109738244
    },
    {
      "source": "282a380fb5ac26d99667224cef8c630f6882704f",
      "target": "98b41528c58e6f5b7b28be5b54029e52ca90c4ab",
      "weight": 0.2839037754842302
    },
    {
      "source": "35ad6ba10006975c2bc67ecefaa9ee6af2453bdc",
      "target": "e5638e677d40c9ea67401b9b5241f381a73be6fd",
      "weight": 0.2408510318978625
    },
    {
      "source": "35ad6ba10006975c2bc67ecefaa9ee6af2453bdc",
      "target": "38b547a2cf81bacd30cbb322e7279091753604dc",
      "weight": 0.2835976434740862
    },
    {
      "source": "361e953f792a585496834ee14216b94d0ce9ae74",
      "target": "23bac2542b145bf2fcd17d7fa0a02ae03d0a45f7",
      "weight": 0.28973646117082447
    },
    {
      "source": "361e953f792a585496834ee14216b94d0ce9ae74",
      "target": "754878242a3b480b2ca9031bff623f2c557f2caa",
      "weight": 0.24197482912233934
    },
    {
      "source": "361e953f792a585496834ee14216b94d0ce9ae74",
      "target": "9342fce9c5a69f545a778ca7e885ba9d63af928f",
      "weight": 0.5471139512986654
    },
    {
      "source": "3bd02411eaa798a158aa780a4d75d0cbfa0af790",
      "target": "2d8d30eb2f6d554d13be811d8cee541387573bd9",
      "weight": 0.41404526187156837
    },
    {
      "source": "eb8dba325534da472170293b054596a17558c7f2",
      "target": "1aed7b21007519b7482ac4a005d5f2e0c7a1d047",
      "weight": 0.44910715253422
    },
    {
      "source": "332c44793b70776b9b966128c52e694222b1ab73",
      "target": "71c00beb70d83eab08f1cf6c32f48c112bd9bfdf",
      "weight": 0.24876815677963154
    },
    {
      "source": "332c44793b70776b9b966128c52e694222b1ab73",
      "target": "f8ee167e718cb152d816f06d42c66efec729a536",
      "weight": 0.2787243058371487
    },
    {
      "source": "332c44793b70776b9b966128c52e694222b1ab73",
      "target": "b6efb87e4b609fb67304f73b8ee9c1984fce5e88",
      "weight": 0.24380981906946197
    },
    {
      "source": "332c44793b70776b9b966128c52e694222b1ab73",
      "target": "b2058b849f29e99ed4052e2d82b248acc4d6685f",
      "weight": 0.2801149801359855
    },
    {
      "source": "332c44793b70776b9b966128c52e694222b1ab73",
      "target": "1845ece5be61f96292d0b3ea3ecec251b2510909",
      "weight": 0.288373014514157
    },
    {
      "source": "2bdebf2fb0f5c21907fcaae6d87c7ba5811e778a",
      "target": "c8905a4c9c5cbeff6e905687c5077e8af47b8ce4",
      "weight": 0.24772514798747353
    },
    {
      "source": "2bdebf2fb0f5c21907fcaae6d87c7ba5811e778a",
      "target": "04396f17e2bdc848300b8670104895b0b3fee84f",
      "weight": 0.31871858296740485
    },
    {
      "source": "2bdebf2fb0f5c21907fcaae6d87c7ba5811e778a",
      "target": "a968524df2c59fb0ed8892603546f55b731d6439",
      "weight": 0.2709388389533327
    },
    {
      "source": "2bdebf2fb0f5c21907fcaae6d87c7ba5811e778a",
      "target": "4454a763c891afb3fb8fa6567a367d05b1938e97",
      "weight": 0.33718376984704745
    },
    {
      "source": "2bdebf2fb0f5c21907fcaae6d87c7ba5811e778a",
      "target": "332c44793b70776b9b966128c52e694222b1ab73",
      "weight": 0.2796870207652048
    },
    {
      "source": "2bdebf2fb0f5c21907fcaae6d87c7ba5811e778a",
      "target": "f6271880cc1d7ff6514672366fe124fdb1212fb2",
      "weight": 0.33262557653452346
    },
    {
      "source": "2bdebf2fb0f5c21907fcaae6d87c7ba5811e778a",
      "target": "9e1ea0a5a29a19baf531aa5d9f32ca51d240d575",
      "weight": 0.22922723427474234
    },
    {
      "source": "2bdebf2fb0f5c21907fcaae6d87c7ba5811e778a",
      "target": "020bb2ba5f3923858cd6882ba5c5a44ea8041ab6",
      "weight": 0.13961370016177604
    },
    {
      "source": "2bdebf2fb0f5c21907fcaae6d87c7ba5811e778a",
      "target": "16a3dd5ab3e8570f6083ddf6f88aa5e916450fef",
      "weight": 0.25124948535841424
    },
    {
      "source": "2bdebf2fb0f5c21907fcaae6d87c7ba5811e778a",
      "target": "208cd4b25768f0096fb2e80e7690473da0e2a563",
      "weight": 0.3437494139910322
    },
    {
      "source": "2bdebf2fb0f5c21907fcaae6d87c7ba5811e778a",
      "target": "eeb0407b2f47857fe7b44c948c08ef23469a8ad2",
      "weight": 0.3736924381161041
    },
    {
      "source": "d0eb13325d77e50a60102139e84484a9beaf62ff",
      "target": "61b03c891489247bcb5ad432b4d485784a274fb4",
      "weight": 0.292152812989261
    },
    {
      "source": "26b07c6309ef12034571f20973097691a22d7116",
      "target": "fe10bf13aeb8728a955f1f8fd312ce77773b59ec",
      "weight": 0.22068488579745976
    },
    {
      "source": "26b07c6309ef12034571f20973097691a22d7116",
      "target": "020bb2ba5f3923858cd6882ba5c5a44ea8041ab6",
      "weight": 0.06969074870602424
    },
    {
      "source": "42de54e614110c0c0a0bbbfee045e11e53eb4a7d",
      "target": "e1a8bf47ef1c51298f5bcf957062c6ee63a96bfb",
      "weight": 0.25269947896173195
    },
    {
      "source": "42de54e614110c0c0a0bbbfee045e11e53eb4a7d",
      "target": "04396f17e2bdc848300b8670104895b0b3fee84f",
      "weight": 0.3018198132631487
    },
    {
      "source": "42de54e614110c0c0a0bbbfee045e11e53eb4a7d",
      "target": "31eba23839649c21c3e462a7568b6b72041d4b5c",
      "weight": 0.43397254727185786
    },
    {
      "source": "42de54e614110c0c0a0bbbfee045e11e53eb4a7d",
      "target": "23bac2542b145bf2fcd17d7fa0a02ae03d0a45f7",
      "weight": 0.29309765554805894
    },
    {
      "source": "42de54e614110c0c0a0bbbfee045e11e53eb4a7d",
      "target": "d0eb13325d77e50a60102139e84484a9beaf62ff",
      "weight": 0.13596311006527
    },
    {
      "source": "b1493cac304d3fea710b375fa09e4b943a8a7de9",
      "target": "020bb2ba5f3923858cd6882ba5c5a44ea8041ab6",
      "weight": 0.08098568295495204
    },
    {
      "source": "b1493cac304d3fea710b375fa09e4b943a8a7de9",
      "target": "a38500c3448189abd05e72e35332224b96e24a32",
      "weight": 0.41951604732492964
    },
    {
      "source": "3b32351004d1628329b875576323a7b1767e9e5a",
      "target": "b1c2df70f8c287c98e8735d82185bdadf2d4d24b",
      "weight": 0.30432080932831784
    },
    {
      "source": "557e9371711c7409c78c96a6a2bea290a28cb365",
      "target": "e5638e677d40c9ea67401b9b5241f381a73be6fd",
      "weight": 0.4626226535709153
    },
    {
      "source": "557e9371711c7409c78c96a6a2bea290a28cb365",
      "target": "8291dcc23a6daf3afc976acba07b8b47aa0caebe",
      "weight": 0.982161890316823
    },
    {
      "source": "2e4316e7c38373d068f8ff55f26ff83dfc4238b8",
      "target": "c67de8be8b033362e94d98dcefae88e4b75dd6c7",
      "weight": 0.4160234437609946
    },
    {
      "source": "2e4316e7c38373d068f8ff55f26ff83dfc4238b8",
      "target": "0341673e7ed78a096b9b9b51fbdbeff08beed660",
      "weight": 0.5815889786049075
    },
    {
      "source": "2e4316e7c38373d068f8ff55f26ff83dfc4238b8",
      "target": "020bb2ba5f3923858cd6882ba5c5a44ea8041ab6",
      "weight": 0.05212851746227527
    },
    {
      "source": "2e4316e7c38373d068f8ff55f26ff83dfc4238b8",
      "target": "17b6829678802a20e51558ec28c5369414defe42",
      "weight": 0.2993403850142398
    },
    {
      "source": "d8d680aea59295c020b9d53d78dd8d954a876845",
      "target": "b237deb6c0234378238a6ee49b229b1299b7efe6",
      "weight": 0.08872697582496138
    },
    {
      "source": "d8d680aea59295c020b9d53d78dd8d954a876845",
      "target": "c8905a4c9c5cbeff6e905687c5077e8af47b8ce4",
      "weight": 0.27500620971613976
    },
    {
      "source": "d8d680aea59295c020b9d53d78dd8d954a876845",
      "target": "272b071e05e960ef3adab2bc8a078fd165b268d5",
      "weight": 0.34313883833447256
    },
    {
      "source": "d8d680aea59295c020b9d53d78dd8d954a876845",
      "target": "1d421d179a2520ba23dc1375fe2989e4ba79b437",
      "weight": 0.2630725656575494
    },
    {
      "source": "d8d680aea59295c020b9d53d78dd8d954a876845",
      "target": "2dc6799265db441bfa53eb9346cf67fec9a27e39",
      "weight": 0.26956952784125626
    },
    {
      "source": "d8d680aea59295c020b9d53d78dd8d954a876845",
      "target": "5a8a079d30d40fc24565db7f1687d22dc323d24e",
      "weight": 0.3764790705774421
    },
    {
      "source": "d8d680aea59295c020b9d53d78dd8d954a876845",
      "target": "190ae56a68a94620d7ddfdc7c4b1424673f78b97",
      "weight": 0.25255173532117925
    },
    {
      "source": "d8d680aea59295c020b9d53d78dd8d954a876845",
      "target": "c67de8be8b033362e94d98dcefae88e4b75dd6c7",
      "weight": 0.3150822515120839
    },
    {
      "source": "d8d680aea59295c020b9d53d78dd8d954a876845",
      "target": "d700cd5e6fec5d138abf754fe463443ef5f47a95",
      "weight": 0.3281908872012602
    },
    {
      "source": "d8d680aea59295c020b9d53d78dd8d954a876845",
      "target": "a968524df2c59fb0ed8892603546f55b731d6439",
      "weight": 0.2732260942069162
    },
    {
      "source": "d8d680aea59295c020b9d53d78dd8d954a876845",
      "target": "e35e0ad5959c3160d66309c3c1e10df9b4352c6d",
      "weight": 0.35529512559692505
    },
    {
      "source": "d8d680aea59295c020b9d53d78dd8d954a876845",
      "target": "020bb2ba5f3923858cd6882ba5c5a44ea8041ab6",
      "weight": 0.16584925727245847
    },
    {
      "source": "d8d680aea59295c020b9d53d78dd8d954a876845",
      "target": "756b3e51e8ac2951bfd7d5b5322f1502442eab8e",
      "weight": 0.30795331706261786
    },
    {
      "source": "f6271880cc1d7ff6514672366fe124fdb1212fb2",
      "target": "47da3a722b007cef7238299a075c0595fed8632e",
      "weight": 0.47790405002725095
    },
    {
      "source": "0e8827f439152cf1f5670a3ab391db6148abc7e0",
      "target": "b82a9500246176a6b32598ded8d2b96d1e29f61c",
      "weight": 0.34918476600338233
    },
    {
      "source": "0e8827f439152cf1f5670a3ab391db6148abc7e0",
      "target": "79aa092bb37f5ab75d93195f2a5288a51bb8f21d",
      "weight": 0.60746310297681
    },
    {
      "source": "208cd4b25768f0096fb2e80e7690473da0e2a563",
      "target": "04396f17e2bdc848300b8670104895b0b3fee84f",
      "weight": 0.2856724005022172
    },
    {
      "source": "208cd4b25768f0096fb2e80e7690473da0e2a563",
      "target": "dea00783b876b41e852adc0ad1954e1005324edd",
      "weight": 0.27748057883752775
    },
    {
      "source": "208cd4b25768f0096fb2e80e7690473da0e2a563",
      "target": "a9f1e05bb2f6f2eeaf92fe1fc5ca3f0eb498f673",
      "weight": 0.2515221130743271
    },
    {
      "source": "208cd4b25768f0096fb2e80e7690473da0e2a563",
      "target": "859e953bba919a6f989d440b6c23ab19a8cb855b",
      "weight": 0.3972041647165711
    },
    {
      "source": "208cd4b25768f0096fb2e80e7690473da0e2a563",
      "target": "d700cd5e6fec5d138abf754fe463443ef5f47a95",
      "weight": 0.3103769342520063
    },
    {
      "source": "208cd4b25768f0096fb2e80e7690473da0e2a563",
      "target": "5ffee7480bdb997a0f8452829016eee71cb8bbce",
      "weight": 0.24499617788922123
    },
    {
      "source": "208cd4b25768f0096fb2e80e7690473da0e2a563",
      "target": "eb8dba325534da472170293b054596a17558c7f2",
      "weight": 0.2397050255621664
    },
    {
      "source": "208cd4b25768f0096fb2e80e7690473da0e2a563",
      "target": "4454a763c891afb3fb8fa6567a367d05b1938e97",
      "weight": 0.3383770041891765
    },
    {
      "source": "208cd4b25768f0096fb2e80e7690473da0e2a563",
      "target": "332c44793b70776b9b966128c52e694222b1ab73",
      "weight": 0.35289115536641097
    },
    {
      "source": "208cd4b25768f0096fb2e80e7690473da0e2a563",
      "target": "434d8baa964856bcf4bbe9d1bf49dc70ac2128ab",
      "weight": 0.3049729336215951
    },
    {
      "source": "208cd4b25768f0096fb2e80e7690473da0e2a563",
      "target": "e35e0ad5959c3160d66309c3c1e10df9b4352c6d",
      "weight": 0.30962919749402273
    },
    {
      "source": "208cd4b25768f0096fb2e80e7690473da0e2a563",
      "target": "d0eb13325d77e50a60102139e84484a9beaf62ff",
      "weight": 0.13849770292732028
    },
    {
      "source": "208cd4b25768f0096fb2e80e7690473da0e2a563",
      "target": "020bb2ba5f3923858cd6882ba5c5a44ea8041ab6",
      "weight": 0.07341479399947419
    },
    {
      "source": "37a349a7a46a9339cb59ac02f81d3848a62d3885",
      "target": "04396f17e2bdc848300b8670104895b0b3fee84f",
      "weight": 0.3458809324609541
    },
    {
      "source": "7f567f1e8972ff31a7ced59c329e7d75da645baf",
      "target": "020bb2ba5f3923858cd6882ba5c5a44ea8041ab6",
      "weight": 0.058016147900491026
    },
    {
      "source": "7f567f1e8972ff31a7ced59c329e7d75da645baf",
      "target": "7bd95a62fd6320730cbb24a0e4fafac97d840652",
      "weight": 0.2722110721011842
    },
    {
      "source": "7f567f1e8972ff31a7ced59c329e7d75da645baf",
      "target": "4625628163a2ee0e6cd320cd7a14b4ccded2a631",
      "weight": 0.3869957780351858
    },
    {
      "source": "4bf9f88d438c7d978fb854eba686cf3933879df1",
      "target": "a962dc06a19c08bb76184bde864e7f1e2e502150",
      "weight": 0.3872673657433996
    },
    {
      "source": "4bf9f88d438c7d978fb854eba686cf3933879df1",
      "target": "7ecab7276a1a0360ad594ae08a0fa91a26ecb025",
      "weight": 0.293332123428702
    },
    {
      "source": "4bf9f88d438c7d978fb854eba686cf3933879df1",
      "target": "a9f1e05bb2f6f2eeaf92fe1fc5ca3f0eb498f673",
      "weight": 0.23066941078687075
    },
    {
      "source": "4bf9f88d438c7d978fb854eba686cf3933879df1",
      "target": "332c44793b70776b9b966128c52e694222b1ab73",
      "weight": 0.2979523895898869
    },
    {
      "source": "4bf9f88d438c7d978fb854eba686cf3933879df1",
      "target": "c317d2faa26b38250960cf3d2e6cf095b9d5b92d",
      "weight": 0.21664283621500338
    },
    {
      "source": "4bf9f88d438c7d978fb854eba686cf3933879df1",
      "target": "020bb2ba5f3923858cd6882ba5c5a44ea8041ab6",
      "weight": 0.13479508041726324
    },
    {
      "source": "0d13ff7e27e0a1fb06c57efaeacfc90ccaef3452",
      "target": "0be1e53ccf4320e6e140523a75d55bac57d4d3e2",
      "weight": 0.418621975513757
    },
    {
      "source": "c67de8be8b033362e94d98dcefae88e4b75dd6c7",
      "target": "6ac73bcb953640dcc9c5b7f730f57ad135593d8e",
      "weight": 0.43861283068373613
    },
    {
      "source": "dea00783b876b41e852adc0ad1954e1005324edd",
      "target": "9adc67e027edfea39a7904d96f7d436cd3ec3dff",
      "weight": 0.32214346710088837
    },
    {
      "source": "bbe13b72314fffcc2f35b0660195f2f6607c00a0",
      "target": "434d8baa964856bcf4bbe9d1bf49dc70ac2128ab",
      "weight": 0.30424440324018776
    },
    {
      "source": "bbe13b72314fffcc2f35b0660195f2f6607c00a0",
      "target": "3b479d5df78cd259989d8bbac5d68926d846d3ba",
      "weight": 0.2793466786664707
    },
    {
      "source": "bbe13b72314fffcc2f35b0660195f2f6607c00a0",
      "target": "020bb2ba5f3923858cd6882ba5c5a44ea8041ab6",
      "weight": 0.03201902518915155
    },
    {
      "source": "91e6d31e3bb634007dbc3abc3d84da01412fea17",
      "target": "205770123d5779da5470ae58cf446bc3e9cfc195",
      "weight": 0.298147018821448
    },
    {
      "source": "c317d2faa26b38250960cf3d2e6cf095b9d5b92d",
      "target": "737ee2562b31437146de4df7e2948d1027ef2ecd",
      "weight": 0.30439788926792327
    },
    {
      "source": "8f12add50397f697631b3fff04608d5efa957867",
      "target": "3b32351004d1628329b875576323a7b1767e9e5a",
      "weight": 0.3418728998644782
    },
    {
      "source": "8f12add50397f697631b3fff04608d5efa957867",
      "target": "b1c2df70f8c287c98e8735d82185bdadf2d4d24b",
      "weight": 0.32869801372580915
    },
    {
      "source": "38b547a2cf81bacd30cbb322e7279091753604dc",
      "target": "d700cd5e6fec5d138abf754fe463443ef5f47a95",
      "weight": 0.28837985319241183
    },
    {
      "source": "38b547a2cf81bacd30cbb322e7279091753604dc",
      "target": "e35e0ad5959c3160d66309c3c1e10df9b4352c6d",
      "weight": 0.31375109187996897
    },
    {
      "source": "e95e3a314cab21171e206cd0824fe93c1c47677c",
      "target": "04396f17e2bdc848300b8670104895b0b3fee84f",
      "weight": 0.2748726163047898
    },
    {
      "source": "e95e3a314cab21171e206cd0824fe93c1c47677c",
      "target": "a38500c3448189abd05e72e35332224b96e24a32",
      "weight": 0.3418785862803124
    },
    {
      "source": "15561ab20c298e113b0008b7a029486a422e7ca3",
      "target": "287b8037b0f75caec9fab471dd48fe0b81090f74",
      "weight": 0.3074052583206959
    },
    {
      "source": "15561ab20c298e113b0008b7a029486a422e7ca3",
      "target": "020bb2ba5f3923858cd6882ba5c5a44ea8041ab6",
      "weight": 0.13749421962827316
    },
    {
      "source": "15561ab20c298e113b0008b7a029486a422e7ca3",
      "target": "290357314d0c339bcce31cfbe6b29aa50f89b026",
      "weight": 0.2554500427480403
    },
    {
      "source": "15561ab20c298e113b0008b7a029486a422e7ca3",
      "target": "d8d680aea59295c020b9d53d78dd8d954a876845",
      "weight": 0.30080336699975163
    },
    {
      "source": "f68020d22d9895d0d7f173b14961459395f96861",
      "target": "069cce0ffad769451fe6008e67a6cd27f9eaa281",
      "weight": 0.44810897830042107
    },
    {
      "source": "f68020d22d9895d0d7f173b14961459395f96861",
      "target": "4da500f57577b4ee207ab80b4cde0e1ee338f948",
      "weight": 0.33747469825124465
    },
    {
      "source": "04396f17e2bdc848300b8670104895b0b3fee84f",
      "target": "37a349a7a46a9339cb59ac02f81d3848a62d3885",
      "weight": 0.3458809324609541
    },
    {
      "source": "31eba23839649c21c3e462a7568b6b72041d4b5c",
      "target": "04396f17e2bdc848300b8670104895b0b3fee84f",
      "weight": 0.2749170538379936
    },
    {
      "source": "31eba23839649c21c3e462a7568b6b72041d4b5c",
      "target": "d1ad1bfa0bb76002b10e7f211b937842baeb28d9",
      "weight": 0.4015419443733298
    },
    {
      "source": "61b03c891489247bcb5ad432b4d485784a274fb4",
      "target": "205770123d5779da5470ae58cf446bc3e9cfc195",
      "weight": 0.35322431107313396
    },
    {
      "source": "4625628163a2ee0e6cd320cd7a14b4ccded2a631",
      "target": "e1a8bf47ef1c51298f5bcf957062c6ee63a96bfb",
      "weight": 0.22248937335405528
    },
    {
      "source": "4625628163a2ee0e6cd320cd7a14b4ccded2a631",
      "target": "8d6d320dbe9bdd6a1ea844faaf3dc0f5fff2543b",
      "weight": 0.3703821135538362
    },
    {
      "source": "4625628163a2ee0e6cd320cd7a14b4ccded2a631",
      "target": "fe10bf13aeb8728a955f1f8fd312ce77773b59ec",
      "weight": 0.27279547312515745
    },
    {
      "source": "4625628163a2ee0e6cd320cd7a14b4ccded2a631",
      "target": "d1ad1bfa0bb76002b10e7f211b937842baeb28d9",
      "weight": 0.3435992754131413
    },
    {
      "source": "4625628163a2ee0e6cd320cd7a14b4ccded2a631",
      "target": "31eba23839649c21c3e462a7568b6b72041d4b5c",
      "weight": 0.501270308376335
    },
    {
      "source": "4625628163a2ee0e6cd320cd7a14b4ccded2a631",
      "target": "23bac2542b145bf2fcd17d7fa0a02ae03d0a45f7",
      "weight": 0.4407171895312262
    },
    {
      "source": "4625628163a2ee0e6cd320cd7a14b4ccded2a631",
      "target": "9342fce9c5a69f545a778ca7e885ba9d63af928f",
      "weight": 0.46882087717299503
    },
    {
      "source": "4625628163a2ee0e6cd320cd7a14b4ccded2a631",
      "target": "c40a927a558ad5a5ffe254605ed3bfebd18be39c",
      "weight": 0.28013008866135114
    },
    {
      "source": "4625628163a2ee0e6cd320cd7a14b4ccded2a631",
      "target": "5ad8802447f81bd8574a3bee0c2d1a6456d1533b",
      "weight": 0.22899446441642124
    },
    {
      "source": "4625628163a2ee0e6cd320cd7a14b4ccded2a631",
      "target": "d0eb13325d77e50a60102139e84484a9beaf62ff",
      "weight": 0.08937014560545634
    },
    {
      "source": "4625628163a2ee0e6cd320cd7a14b4ccded2a631",
      "target": "7b201e42e32430d951458916810a7dbf1e946a6d",
      "weight": 0.2734395207839559
    },
    {
      "source": "4625628163a2ee0e6cd320cd7a14b4ccded2a631",
      "target": "020bb2ba5f3923858cd6882ba5c5a44ea8041ab6",
      "weight": 0.10284974826449307
    },
    {
      "source": "4625628163a2ee0e6cd320cd7a14b4ccded2a631",
      "target": "4bf9f88d438c7d978fb854eba686cf3933879df1",
      "weight": 0.3951691604396542
    },
    {
      "source": "4625628163a2ee0e6cd320cd7a14b4ccded2a631",
      "target": "361e953f792a585496834ee14216b94d0ce9ae74",
      "weight": 0.19567432222896422
    },
    {
      "source": "4625628163a2ee0e6cd320cd7a14b4ccded2a631",
      "target": "13b00c6c8e6fd35a540b08904824aff0d6b66897",
      "weight": 0.3793773075908714
    },
    {
      "source": "4625628163a2ee0e6cd320cd7a14b4ccded2a631",
      "target": "eeb0407b2f47857fe7b44c948c08ef23469a8ad2",
      "weight": 0.2831069850025595
    },
    {
      "source": "4e0a735ee8f7606dd13633a88de15f6dfe3348ac",
      "target": "2d8d30eb2f6d554d13be811d8cee541387573bd9",
      "weight": 0.35300354834567743
    },
    {
      "source": "482c0cbfffa77154e3c879c497f50b605297d5bc",
      "target": "61b03c891489247bcb5ad432b4d485784a274fb4",
      "weight": 0.23839031279014816
    },
    {
      "source": "482c0cbfffa77154e3c879c497f50b605297d5bc",
      "target": "d0eb13325d77e50a60102139e84484a9beaf62ff",
      "weight": 0.11144084841437676
    },
    {
      "source": "482c0cbfffa77154e3c879c497f50b605297d5bc",
      "target": "13b00c6c8e6fd35a540b08904824aff0d6b66897",
      "weight": 0.3617254272542103
    },
    {
      "source": "482c0cbfffa77154e3c879c497f50b605297d5bc",
      "target": "f4eff7c0127a2ef92c441f028c3bb15b64cabcc8",
      "weight": 0.5345872592765584
    },
    {
      "source": "482c0cbfffa77154e3c879c497f50b605297d5bc",
      "target": "2bdebf2fb0f5c21907fcaae6d87c7ba5811e778a",
      "weight": 0.3011259283609454
    },
    {
      "source": "482c0cbfffa77154e3c879c497f50b605297d5bc",
      "target": "bbe13b72314fffcc2f35b0660195f2f6607c00a0",
      "weight": 0.2577477934809643
    },
    {
      "source": "06b8e82542d1873928d007548a23d3b77daa11f8",
      "target": "c09d2846ff81c5dbdf4f8662bd2d9bb4dd61b396",
      "weight": 0.24774161424517824
    },
    {
      "source": "bfe284e4338e62f0a61bb33398353efd687f206f",
      "target": "d726e991e68ed892bd4c42c8c8150ebc71ae1b9e",
      "weight": 0.24583108804981862
    },
    {
      "source": "bfe284e4338e62f0a61bb33398353efd687f206f",
      "target": "a962dc06a19c08bb76184bde864e7f1e2e502150",
      "weight": 0.29992364165703095
    },
    {
      "source": "bfe284e4338e62f0a61bb33398353efd687f206f",
      "target": "b237deb6c0234378238a6ee49b229b1299b7efe6",
      "weight": 0.06990948000971108
    },
    {
      "source": "bfe284e4338e62f0a61bb33398353efd687f206f",
      "target": "588c69df5e7920db0037db76c41f933ee16c290d",
      "weight": 0.2677423656362247
    },
    {
      "source": "bfe284e4338e62f0a61bb33398353efd687f206f",
      "target": "52f37e9bd84547db2ecefed420715f312827c398",
      "weight": 0.27957394023275994
    },
    {
      "source": "bfe284e4338e62f0a61bb33398353efd687f206f",
      "target": "c267e53f823a2ef9e9e6bbc26196d68b789fb4c2",
      "weight": 0.3107923901830494
    },
    {
      "source": "bfe284e4338e62f0a61bb33398353efd687f206f",
      "target": "e5638e677d40c9ea67401b9b5241f381a73be6fd",
      "weight": 0.2829634462843774
    },
    {
      "source": "bfe284e4338e62f0a61bb33398353efd687f206f",
      "target": "04396f17e2bdc848300b8670104895b0b3fee84f",
      "weight": 0.27780490821351433
    },
    {
      "source": "bfe284e4338e62f0a61bb33398353efd687f206f",
      "target": "b2058b849f29e99ed4052e2d82b248acc4d6685f",
      "weight": 0.2525164257483661
    }
  ]
}