\subsection{Probabilistic Meta-Learning for Task Inference and Uncertainty Quantification}
Probabilistic meta-learning represents a crucial paradigm for developing robust and trustworthy AI systems, particularly in scenarios demanding not only accurate predictions but also reliable uncertainty estimates. This approach focuses on modeling distributions over functions, enabling models to infer task-specific latent variables and condition predictions on these beliefs, thereby facilitating Bayes-adaptive decision-making in uncertain environments.

The foundational concept of modeling distributions over functions was significantly advanced by \textcite{garnelo2018} with the introduction of Conditional Neural Processes (CNPs). CNPs learn to map a set of context points to a distribution over functions, providing both a prediction and a measure of uncertainty at any target point. Building upon this, \textcite{kim2019} proposed Attentive Neural Processes (ANPs), which integrate attention mechanisms into the CNP framework. ANPs allow the model to selectively weigh the importance of different context points, significantly enhancing performance and flexibility in capturing intricate function structures compared to their non-attentive predecessors.

Extending these probabilistic principles to reinforcement learning (RL), where task uncertainty is paramount for efficient exploration, \textcite{zintgraf2019zat} introduced VariBAD (Variational Bayes-Adaptive Deep RL). VariBAD meta-learns an approximate Bayes-adaptive policy by jointly training a Variational Auto-Encoder (VAE) for posterior inference over latent MDP embeddings and a policy conditioned on this belief. This allows the agent to perform structured online exploration by explicitly reasoning about task uncertainty, a significant step towards tractable Bayes-optimal policies. However, VariBAD's reliance on on-policy experience limited its sample efficiency during meta-training.

To address this, \textcite{rakelly2019m09} developed PEARL (Probabilistic Embeddings for Actor-Critic RL), an off-policy meta-RL algorithm that dramatically improves meta-training sample efficiency. PEARL leverages probabilistic context variables and a permutation-invariant encoder to infer task-specific information, enabling structured exploration through posterior sampling. This decoupling of task inference from policy optimization, coupled with off-policy learning, allowed for substantial gains in data efficiency and asymptotic performance. The utility of probabilistic context variables was further demonstrated in Meta-Inverse Reinforcement Learning (IRL) by \textcite{yu2019o41}. Their PEMIRL method enables few-shot reward inference from unstructured, heterogeneous demonstrations by using probabilistic context variables and mutual information regularization, effectively learning robust reward functions without explicit task labels.

A critical challenge in real-world applications is learning from static, offline datasets. \textcite{dorfman2020mgv} tackled this with BOReL (Bayesian Offline Reinforcement Learning), an off-policy VariBAD variant designed for Offline Meta-RL. BOReL learns exploration strategies from static datasets, formalizing the concept of "MDP ambiguity" where offline data might not contain sufficient information to distinguish between different underlying tasks. This work highlights the inherent limitations of data identifiability in offline settings while providing practical solutions for learning meta-exploration from pre-collected data.

Recent advancements continue to refine probabilistic meta-learning for increasingly complex scenarios. \textcite{zintgraf2021hoc} extended the Bayes-adaptive framework to interactive settings, proposing a meta-learning approach for Deep Interactive Bayesian Reinforcement Learning that models beliefs over other agents' strategies. For non-stationary and dynamic environments, \textcite{bing2022om0} introduced a training strategy applicable to such settings, utilizing Gaussian Mixture Models for clustered task distributions to achieve zero-shot adaptation. Furthermore, \textcite{wang2024d09} proposed a Contrastive Learning-Based Bayes-Adaptive Meta-Reinforcement Learning (CBAMRL) algorithm, which employs contrastive learning for more compact and sufficient task representations, leading to robust zero-shot adaptation in applications like active pantograph control.

In summary, probabilistic meta-learning has evolved from foundational models of function distributions to sophisticated frameworks that enable Bayes-adaptive behavior, efficient exploration, and robust task inference in complex RL and IRL settings. While significant progress has been made in quantifying uncertainty and adapting to new tasks with limited data, challenges remain in scaling these methods to extremely high-dimensional observation spaces, ensuring robust inference in highly ambiguous or severely limited offline data regimes, and developing more theoretically grounded approaches for handling non-stationarity and concept drift in continuous learning.