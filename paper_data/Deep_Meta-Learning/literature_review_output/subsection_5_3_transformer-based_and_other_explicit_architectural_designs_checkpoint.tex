\subsection{Transformer-Based and Other Explicit Architectural Designs}

Building upon the model-based meta-learning paradigm, this subsection focuses on explicit architectural designs that intrinsically support rapid adaptation and in-context learning without relying on explicit gradient updates during task-specific adaptation. These approaches prioritize designing models whose very structure facilitates swift knowledge integration and adaptation. We explore Transformer-based architectures leveraging attention mechanisms, hypernetworks for dynamic parameter generation, and modular networks for context-dependent component selection.

The Transformer architecture, renowned for its powerful self-attention mechanism, has become a cornerstone for achieving in-context learning and emergent meta-learning capabilities. Unlike optimization-based methods that perform explicit gradient updates for task-specific adaptation, Transformers can process a support set (task examples) as part of their input sequence. This allows the model to implicitly learn a task-specific function or adapt its behavior purely through forward passes, effectively inferring the task from context \cite{d8d680aea59295c020b9d53d78dd8d954a876845, 282a380fb5ac26d99667224cef8c630f6882704f}. This phenomenon is particularly prominent in large language models (LLMs) and Vision Transformers (ViTs), where few-shot prompting—concatenating task instructions and input-output examples to the query—enables generalization to new inputs for the same task without any weight updates. The self-attention mechanism dynamically weighs the relevance of different input tokens, allowing the model to 'attend' to support examples to infer task structure and adapt predictions for the query, mimicking a meta-learning process. This implicit adaptation is a key characteristic that aligns Transformers with model-based meta-learning.

Beyond these emergent properties, Transformers are also explicitly integrated into meta-learning frameworks. For instance, \cite{kumar2024he9} proposes a Transformer-based aggregation function within a meta-learner specifically for real-world class incremental learning. This design concurrently processes the entire support and query sets, using attention to prioritize crucial samples and enhance the impact of relevant information during inference. Crucially, this method enables the meta-learner to complete class incremental tasks without retraining or explicit gradient updates to its core parameters during the incremental learning phase, demonstrating strong generalization even without specific training for this exact task. The strength of Transformer-based meta-learners lies in their ability to scale to vast datasets, learn highly expressive representations, and inherently handle sequential or contextual information. However, their computational cost, especially for very long contexts, and the heavy reliance on extensive pre-training data remain significant considerations for broader applicability, particularly in data-scarce meta-learning scenarios where pre-training on massive datasets is not feasible. Moreover, the 'black-box' nature of how Transformers perform in-context adaptation can hinder interpretability compared to more explicit parameter generation or module selection mechanisms.

Hypernetworks represent another explicit architectural design enabling rapid, gradient-free adaptation by dynamically generating parameters for a 'main' network. In this paradigm, a meta-learner (the hypernetwork) produces task-specific weights or biases for a base model based on a given task description or support set. This mechanism allows for adaptation without requiring explicit gradient descent on the base model's parameters during the inner loop, shifting the learning burden from parameter optimization to parameter generation.

A prominent example is HyperMAML \cite{przewiezlikowski2022d4y}, which generalizes Model-Agnostic Meta-Learning (MAML) by replacing its gradient-based inner-loop adaptation with a trainable hypernetwork. While MAML relies on a few gradient steps to adapt a universal initialization, HyperMAML employs a hypernetwork to directly output weight updates ($\Delta\theta$) for the base model. This hypernetwork takes as input encoded support set embeddings, the base model's predictions on the support set, and the ground-truth labels. The critical innovation is that the hypernetwork generates these updates in a single step, *without requiring loss calculation or gradient backpropagation during the task-specific adaptation phase*. The task-specific parameters are simply computed as $\theta' = \theta + \Delta\theta$. This approach offers a more powerful and efficient adaptation mechanism, allowing for more significant and flexible weight modifications than MAML's limited gradient steps, while also being more computationally efficient by avoiding second-order optimization. Although the meta-training process still optimizes the hypernetwork and universal weights via gradient descent, the *task-specific adaptation* itself is gradient-free for the base model. This design addresses MAML's limitations regarding insufficient updates and computational overhead, offering a biologically plausible alternative to gradient-based adaptation. Other works, such as those employing Feature-wise Linear Modulation (FiLM) layers, can also be seen as a form of hypernetwork where a small network generates affine transformation parameters for feature maps, allowing for conditional computation and adaptation based on context \cite{perez2018film}. The primary challenge with hypernetworks lies in their own meta-training complexity and ensuring the hypernetwork has sufficient capacity to generate high-quality parameters for a diverse range of tasks without overfitting.

Modular networks offer a distinct approach to explicit architectural adaptation by dynamically reconfiguring their structure or selecting components based on the task at hand. Instead of adapting parameters through gradients or generating them, these approaches adapt the *architecture* itself. A meta-learner can be designed to learn a policy for selecting, composing, or routing a subset of pre-trained modules, or even generating new modules, tailored to a novel task. This allows for efficient knowledge transfer by leveraging a library of specialized functionalities. The core idea is that the meta-learner learns *which* components are relevant for a given task and orchestrates their usage, with the adaptation to a new task primarily involving a forward pass through the selected or configured modules, without gradient updates to the modules themselves during this phase.

For instance, Neural Module Networks (NMNs) \cite{andreas2016neural, andreas2017learning} demonstrate this principle by dynamically assembling differentiable modules (e.g., for "find," "relate," "query") into a task-specific network structure based on a natural language query. While the initial training of individual modules and the meta-learner (which learns to parse queries into network layouts) involves gradients, the *execution* for a new query (task) involves a forward pass through the dynamically constructed network. Similarly, architectures employing Mixture-of-Experts (MoE) can be viewed through a meta-learning lens, where a "gating network" acts as a meta-learner, learning to select or weight the outputs of different "expert" sub-networks based on the input or task context \cite{shazeer2017outrageously}. For new tasks, the gating network routes the input to the most appropriate experts, effectively adapting the computation path without modifying the expert weights. This paradigm offers advantages in interpretability, as the activated modules can reveal the model's reasoning, and efficiency, as only relevant components are engaged. However, challenges include learning effective composition or routing policies, ensuring seamless integration of diverse modules, and managing the combinatorial complexity of module selection for highly flexible systems.

These explicit architectural designs—Transformers, hypernetworks, and modular networks—represent distinct yet complementary strategies within model-based meta-learning to achieve rapid adaptation without explicit gradient updates during the inner loop. Transformers excel at implicit in-context learning by leveraging their attention mechanisms to dynamically interpret task examples within a sequence, demonstrating emergent meta-learning capabilities, particularly in large pre-trained models. Their strength lies in handling complex sequential dependencies and scaling to massive datasets, but their adaptation mechanism is often opaque, and their computational cost for very long contexts can be prohibitive. Hypernetworks, conversely, focus on generating task-specific parameters, offering a direct and potentially more expressive way to modify a base model's weights in a single step, circumventing the limitations of gradient-based adaptation. They provide finer-grained control over adaptation than implicit Transformer mechanisms but introduce their own meta-training complexity and capacity challenges. Modular networks aim for structural adaptation, reconfiguring or selecting functional components to suit new tasks, which can lead to highly efficient and interpretable solutions by activating only relevant parts of the model. However, learning robust composition or routing policies and managing the combinatorial explosion of module interactions remain significant hurdles.

A critical comparison reveals trade-offs across these paradigms. Transformers, especially large pre-trained ones, demand vast computational resources for initial training and rely on the implicit emergent properties of attention, making their adaptation mechanism less interpretable. Their generalization to truly out-of-distribution tasks can be limited by the diversity of their pre-training data. Hypernetworks offer a more explicit and potentially more powerful adaptation than MAML-like gradient steps, but their meta-training can be complex, and the hypernetwork's capacity must be carefully balanced to avoid underfitting or overfitting the parameter generation task. Modular networks offer the highest potential for interpretability and efficiency during adaptation, as only relevant components are activated. However, the meta-learning problem shifts to learning an effective *composition policy*, which can be challenging to generalize across vastly different tasks. The data requirements also vary: large Transformers benefit immensely from massive pre-training, while hypernetworks and modular networks typically require meta-training on a distribution of tasks, which can still be substantial. Future research directions include combining the strengths of these approaches, such as using Transformers as powerful meta-learners to control hypernetworks or orchestrate modular compositions, to achieve more robust, efficient, and interpretable rapid adaptation.