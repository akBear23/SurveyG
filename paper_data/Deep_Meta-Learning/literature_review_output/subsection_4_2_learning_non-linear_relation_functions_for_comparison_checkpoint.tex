\subsection{Learning Non-Linear Relation Functions for Comparison}

The effectiveness of metric-based meta-learning in few-shot scenarios critically depends on its capacity to accurately quantify similarity between examples. While foundational methods like Prototypical Networks \cite{community_8} and Matching Networks \cite{community_5} learn robust embedding spaces, they often rely on fixed distance metrics (e.g., Euclidean or cosine similarity) or implicit attention mechanisms for comparison. This reliance can be a significant limitation, as complex, non-linear relationships inherent in diverse data may not be adequately captured by simple, pre-defined metrics \cite{hospedales2020m37, peng20209of}. This challenge spurred a pivotal advancement in the field: the development of methods that explicitly learn deep, non-linear "relation functions" to compute similarity scores, thereby enabling more flexible and expressive comparisons. This approach fundamentally shifts the focus from merely learning better representations for a fixed metric to learning the comparison mechanism itself.

A seminal contribution in this area is the **Relation Network (RN)**, proposed by \cite{sung2017nc5}. This work introduced an end-to-end meta-learning framework designed to move beyond fixed or linear similarity measures. The RN architecture comprises two main components: an embedding module ($f_\phi$) and a relation module ($g_\psi$). The embedding module extracts feature maps from both query and support examples. Crucially, these feature maps are then concatenated and fed into the relation module ($g_\psi$), which is typically a deep convolutional neural network (CNN) or a multi-layer perceptron (MLP). This $g_\psi$ learns to output a scalar "relation score" (between 0 and 1) that directly indicates the similarity between the input pair. The network is meta-trained with a Mean Squared Error (MSE) loss, regressing scores to 1 for matching pairs and 0 for non-matching pairs. This explicit learning of a non-linear comparison function allows the model to discover intricate relational patterns that simple Euclidean distances cannot, demonstrating superior performance in few-shot and zero-shot classification by adapting the comparison mechanism itself \cite{sung2017nc5}. Unlike Matching Networks, which use an attention mechanism to weigh support examples, the RN explicitly learns a *function* to compute a similarity score for *any given pair* of embedded examples, offering a more direct and adaptable comparison.

The core principle of learning an explicit, non-linear relation function has been successfully extended and adapted across various domains and data types. For instance, \cite{gao2020h75} applied the Relation Network paradigm to hyperspectral image few-shot classification. Their method leverages the spatial-spectral information inherent in hyperspectral images, where the embedding module extracts rich features, and the subsequent relation learning module performs comparisons. By meta-training with a task-based learning strategy, their approach demonstrates excellent generalization ability, achieving satisfactory classification results with limited labeled samples. This highlights the transferability of the RN's core idea to specialized data modalities, where the learned relation function can capture complex spectral and spatial similarities.

Further advancing the sophistication of learned relation functions, \cite{zhang2024ycr} introduced PG-DERN for few-shot molecular property prediction. This method employs a dual-view encoder for molecular representation and, more relevantly to this discussion, a novel **relation graph learning module**. Instead of simply concatenating feature vectors, this module constructs a "relation graph" based on learned similarities between molecules. This allows for more efficient information propagation and a richer understanding of complex, multi-faceted relationships between molecular structures, which are crucial for accurate property prediction. While PG-DERN also incorporates a MAML-based meta-learning strategy, its innovation in learning a structured, graph-based relation function for comparison represents a significant evolution from the simpler CNN/MLP relation modules of earlier RNs, showcasing how the concept of "relation learning" can be extended to model intricate, non-Euclidean structures between instances.

The evolution from fixed distance metrics to learned, non-linear relation functions marks a critical paradigm shift in metric-based meta-learning. The foundational Relation Networks \cite{sung2017nc5} provided a robust framework for explicitly learning similarity scores, demonstrating that a deeper, learned comparator offers greater generalizability than pre-defined metrics. Subsequent works have built upon this by adapting the relation learning module to diverse data types, such as hyperspectral images \cite{gao2020h75}, and by introducing more sophisticated, structured relation learning mechanisms, like graph-based approaches for molecular data \cite{zhang2024ycr}. This trajectory underscores a continuous drive towards more flexible, expressive, and domain-aware comparison mechanisms, enabling meta-learning models to capture increasingly intricate patterns and adapt effectively to novel tasks and domains. Future research in this area may explore even more complex, hierarchical relation structures, integrate attention mechanisms more deeply within the relation module, or investigate how these learned comparison functions can be made more interpretable.