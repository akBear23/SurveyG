\subsection{Efficient and Safe Meta-Reinforcement Learning}

Deploying adaptive reinforcement learning (RL) agents in real-world, safety-critical scenarios necessitates advanced meta-learning techniques that not only achieve rapid adaptation and sample efficiency but also provide robust guarantees for safe operation. This subsection reviews the progression of meta-RL towards off-policy learning, efficient adaptation from static datasets, and, crucially, the integration of provable safety mechanisms.

Early meta-reinforcement learning (meta-RL) approaches, while demonstrating the ability to learn adaptation mechanisms \cite{wang20167px}, often suffered from severe sample inefficiency due to their reliance on on-policy data and lacked robust mechanisms for reasoning about task uncertainty. A significant step towards addressing these limitations was the introduction of \textit{Probabilistic Embeddings for Actor-Critic RL} (PEARL) \cite{rakelly2019m09}. PEARL pioneered an off-policy meta-RL algorithm that leverages probabilistic context variables to encode task-specific information, enabling explicit reasoning about uncertainty. Its key innovations include a permutation-invariant encoder for online probabilistic filtering and a decoupled off-policy training strategy, which together yield substantial (20-100x) improvements in meta-training sample efficiency and facilitate structured exploration through posterior sampling. Building on the idea of learning task uncertainty, \textit{VariBAD} \cite{zintgraf2019zat} further advanced Bayes-adaptive deep RL by meta-learning an approximate Bayes-optimal policy. VariBAD jointly trains a variational auto-encoder for posterior inference over latent MDP embeddings and a policy conditioned on this belief, allowing for efficient, structured online exploration without requiring privileged task information. The versatility of probabilistic context variables was further demonstrated by \textit{PEMIRL} \cite{yu2019o41}, which extended this paradigm to Meta-Inverse Reinforcement Learning, enabling few-shot reward inference from unstructured, heterogeneous demonstrations by regularizing mutual information between context and trajectories.

While PEARL and VariBAD enhanced online sample efficiency, real-world applications often demand learning from static, pre-collected datasets. This led to the critical development of offline meta-reinforcement learning. \textit{BOReL} \cite{dorfman2020mgv} tackled this challenge by proposing an off-policy VariBAD variant specifically designed for offline learning. BOReL formalized the concept of "MDP ambiguity," highlighting the inherent limitations of data identifiability when learning exploration strategies from static, exploitative datasets. It demonstrated how to learn effective exploration from such data, marking a crucial step towards meta-RL deployment in data-scarce environments. This offline meta-learning capability has recently been leveraged in complex adaptive control systems. For instance, \textit{MAGICVFM} \cite{lupu20249p4} integrates Visual Foundation Models with offline meta-learning and composite adaptive control for real-time, stable ground vehicle control in challenging terrains. This approach showcases meta-learning's role in robust physical system autonomy, providing mathematical guarantees of exponential stability and robustness, which are vital for reliable operation. Other works have also explored efficient adaptation in dynamic settings, such as \textit{MetaABR} \cite{li20246fg} for adaptive bitrate selection in video streaming and methods for fast adaptive task offloading in edge computing \cite{wang2020tae}.

Beyond efficiency and offline capabilities, the paramount concern of safety in real-world deployment has driven the emergence of meta-safe RL frameworks. A significant advancement in this direction is the \textit{CMDP-within-online framework for Meta-Safe Reinforcement Learning} \cite{khattar2024sr6}. This groundbreaking work introduces the first provable guarantees for task-averaged regret and constraint violations in meta-RL. By encapsulating each task as a Constrained Markov Decision Process (CMDP) and employing an online meta-learner that updates policies and learning rates based on inexact upper bounds on optimality gaps and constraint violations, it provides a critical step towards deploying adaptive RL agents in safety-critical scenarios. This framework is designed to ensure adherence to constraints, which is paramount for reliable operation. Further illustrating the need for robust and safe adaptation in dynamic environments, \textit{CBAMRL} \cite{wang2024d09} proposes a contrastive learning-based Bayes-adaptive meta-RL algorithm for active pantograph control in high-speed railways. This method achieves zero-shot adaptation in non-stationary environments and provides well-structured task representations, implicitly contributing to operational safety by maintaining stable contact force. Similarly, \textit{GM2DQL} \cite{ma20243e9} addresses eco-routing with multi-objective meta-deep Q-learning, balancing fuel efficiency and travel time, where safety is an implicit concern through optimal route selection.

The trajectory of meta-reinforcement learning has thus evolved from foundational efforts in learning adaptive algorithms to sophisticated frameworks that prioritize not only sample efficiency and off-policy learning but also, critically, safety. The progression from probabilistic context variables in PEARL and VariBAD to offline learning with BOReL, and finally to provably safe meta-RL with frameworks like \cite{khattar2024sr6}, underscores a concerted effort to bridge the gap between theoretical adaptability and practical, reliable deployment. Future research will likely focus on scaling these provable safety guarantees to more complex, high-dimensional systems, integrating them with the adaptive capabilities of large foundation models, and ensuring robustness against unforeseen domain shifts, thereby paving the way for truly autonomous and trustworthy AI agents in safety-critical applications.