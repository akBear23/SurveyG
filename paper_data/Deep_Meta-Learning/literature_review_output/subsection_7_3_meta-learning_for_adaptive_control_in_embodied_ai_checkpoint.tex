\subsection{Meta-Learning for Adaptive Control in Embodied AI}

Achieving robust and stable control for embodied AI systems, particularly in robotics, presents significant challenges due to complex, often unmodeled dynamics, external disturbances, and varying environmental conditions. Traditional control methods often struggle with these uncertainties, requiring precise system models or extensive manual tuning. Meta-learning offers a powerful paradigm to enable rapid adaptation and enhance the reliability of autonomous systems in such intricate physical environments by learning to quickly adjust control policies or system models. The critical distinction in this domain lies in moving beyond mere performance optimization to providing mathematical stability guarantees essential for safe and predictable real-world deployment.

Early explorations into meta-learning for robotics primarily focused on improving data efficiency and rapid skill acquisition within the reinforcement learning (RL) paradigm. \cite{finn20174c4} pioneered the application of Model-Agnostic Meta-Learning (MAML) to visual imitation learning, demonstrating how a robot could adapt a policy from raw pixel inputs using a single visual demonstration. This work highlighted meta-learning's capacity for few-shot skill acquisition and policy adaptation. Similarly, \cite{yu2018nm7} extended this by proposing domain-adaptive meta-learning for one-shot imitation from human demonstrations, enabling robots to learn new behaviors despite significant domain shifts in perspective, environment, and embodiment. While these foundational efforts established meta-learning's utility for rapid task adaptation and data-efficient policy learning, they primarily operated within the RL framework, focusing on optimizing task performance rather than explicitly addressing real-time control stability or providing formal guarantees against unmodeled physical dynamics. The critical aspects of robust control, such as Lyapunov stability or disturbance rejection, remained largely outside their scope.

As the field matured, the focus expanded towards enabling robust adaptation in dynamic and non-stationary environments, bridging the gap towards adaptive control. \cite{bing2022om0} introduced a meta-reinforcement learning strategy applicable to non-stationary environments, demonstrating competitive asymptotic performance and superior sample efficiency in continuous robotic control benchmarks. Their approach, leveraging Gaussian mixture models for task representation, allows agents to learn basic distinct behaviors and adapt to changing conditions, crucial for operating in unpredictable physical settings. Complementing this, \cite{meng2024nqq} addressed the sim-to-real transfer problem for meta-policies in robotics using Progressive Neural Networks (MetaPNN). By meta-training a policy for multiple source tasks and transferring it via PNN, their method effectively bridges the reality gap with mismatched dynamics, allowing robots to adapt to new situations in the real world with improved learning efficiency and performance. These works underscore meta-learning's ability to handle environmental variability and dynamics discrepancies, laying important groundwork for more explicit adaptive control.

A significant advancement in directly integrating meta-learning with adaptive control theory for stability and robustness guarantees has emerged. \cite{oconnell2022twd} introduced Neural-Fly, a learning-based approach for agile flight in strong, dynamic winds, which is a prime example of adaptive control for embodied AI. Neural-Fly incorporates pretrained representations through deep learning, leveraging the observation that aerodynamics in different wind conditions share a common, low-dimensional representation. It employs a learning algorithm called Domain Adversarially Invariant Meta-Learning (DAIML) to learn this shared representation from minimal flight data (e.g., 12 minutes). Crucially, with this learned representation as a basis, Neural-Fly uses a *composite adaptation law* to update a set of linear coefficients online. This design not only achieves precise flight control with substantially smaller tracking error than state-of-the-art nonlinear and adaptive controllers but also provides *exponential stability* and *robustness guarantees*. This methodology demonstrates how meta-learning can facilitate rapid online adaptation while adhering to rigorous control-theoretic principles, even extrapolating to unseen wind conditions and transferring across different drones.

Building on this trajectory, \cite{lupu20249p4} presented MAGICVFM, a framework that integrates meta-learning with visual foundation models (VFMs) and composite adaptive control for real-time terrain adaptation in ground vehicles. This addresses the complex challenge of controlling off-road vehicles amidst dynamic, unmodeled ground interactions like slippage. MAGICVFM leverages VFMs to extract rich, generalizable visual features from terrain, which are then fed into a Deep Neural Network (DNN) trained offline using meta-learning to model residual dynamics. A key innovation is that only the final layer of this DNN is adapted online by a composite adaptive controller, which provides *mathematical guarantees of exponential stability and robustness* for real-time control. This approach exemplifies how meta-learning facilitates rapid adaptation to unmodeled dynamics by learning an adaptable model of the environment, while simultaneously offering critical stability assurances through its integration with established control theory. The synergy between advanced perception (VFMs), meta-learning for adaptable modeling, and robust adaptive control is crucial for pushing the boundaries of autonomous systems in complex physical environments.

In conclusion, meta-learning has evolved from foundational policy and reward adaptation techniques to sophisticated frameworks that enable robust and stable control for embodied AI. The integration of meta-learning with composite adaptive control, as demonstrated by Neural-Fly \cite{oconnell2022twd} for UAVs and MAGICVFM \cite{lupu20249p4} for ground vehicles, represents a significant advancement. These approaches provide both rapid adaptation to unmodeled dynamics and mathematical stability guarantees, bridging advanced perception, learning, and control. Future directions will likely focus on extending these guarantees to a wider range of complex physical interactions, exploring the integration of even more diverse multi-modal foundation models for enhanced environmental understanding, and developing tighter theoretical links between meta-learning's generalization capabilities and the formal stability properties of adaptive control systems. This continuous advancement is crucial for pushing the boundaries of autonomous systems in complex, real-world physical environments.