\subsection{Meta-Learning for Adapting Large Foundation Models}

The emergence of large pre-trained models, including Vision-Language Models (VLMs) and Large Language Models (LLMs), has fundamentally reshaped artificial intelligence. These models, often termed "foundation models," possess vast knowledge acquired from extensive pre-training. However, efficiently adapting these massive models to new, often data-scarce, downstream tasks remains a significant challenge. Meta-learning provides a powerful paradigm to address this by enabling models to "learn to learn," facilitating rapid and generalizable adaptation with minimal data and computational overhead.

Historically, meta-learning focused on learning optimal initial model parameters for faster adaptation. Model-Agnostic Meta-Learning (MAML) \cite{Finn_MAML_2017} and its first-order approximation, Reptile \cite{Nichol_Reptile_2018}, exemplify this by training a model's initialization such that a few gradient steps on a new task yield strong performance. While foundational, these gradient-based methods often incur high computational costs due to bi-level optimization and second-order derivatives, limiting their direct scalability to the immense parameter counts of modern foundation models. This computational bottleneck necessitated a shift in the meta-learning paradigm: instead of learning initial weights for an entire model, the focus moved to learning *how to efficiently interact with* or *tune* these largely frozen, pre-trained giants.

This shift has prominently manifested in parameter-efficient fine-tuning (PEFT) methods, where meta-learning guides the adaptation of a small subset of parameters or external modules. A leading approach is "Learning to Prompt" (L2P) \cite{Chen_L2P_2021}, where a meta-learner is trained to generate task-specific, learnable prompts that guide a frozen VLM for few-shot adaptation. This significantly reduces the number of parameters requiring fine-tuning, making adaptation highly efficient. For LLMs, meta-learning has been extensively surveyed as a key strategy for prompt-based learning, PEFT, and in-context learning, enabling adaptation with minimal data and computational cost \cite{Wang_Meta-Learning_2022, lee2021jou}. Recent advancements, such as "Learning to Learn Better Visual Prompts" (LoL) \cite{wang2024dai}, integrate meta-learning's N-way K-shot episodic training into prompt tuning for VLMs. This meta-training strategy explicitly addresses overfitting issues common in prompt tuning, significantly enhancing generalization from base classes to novel, unseen classes by optimizing the prompt learning process itself.

Beyond prompt-based methods, meta-learning is also being applied to other critical PEFT techniques. Low-Rank Adaptation (LoRA), which fine-tunes low-rank incremental update matrices on top of frozen pre-trained weights, has proven highly effective. However, LoRA's performance can depend heavily on the choice of matrix rank. To address this, AutoLoRA \cite{zhang2024a5a} introduces a meta-learning framework that automatically identifies the optimal rank for each LoRA layer. By learning selection variables associated with rank-1 matrices, AutoLoRA dynamically determines which components to retain, thereby improving both efficiency and performance compared to uniform or exhaustively searched rank assignments. Similarly, meta-learning can be applied to other PEFT modules like adapters, where the meta-learner could optimize adapter initializations or architectures for faster downstream convergence. This broader application of meta-learning to various PEFT strategies highlights its versatility in making foundation models adaptable. For instance, in transformer-based NLP models, meta-learning can facilitate class incremental learning by enabling models to generalize to newly introduced classes without full retraining, emphasizing its role in continuous adaptation for large models \cite{kumar2024he9}.

The choice between meta-learning prompts and meta-learning lightweight modules (like LoRA or adapters) involves trade-offs. Prompt-based methods primarily modify the input context, offering simplicity and often requiring minimal changes to the foundation model architecture. However, their capacity to encode complex task-specific information might be limited. In contrast, meta-learning PEFT modules directly modify the model's internal representations or computation, potentially offering greater flexibility and expressiveness for adaptation, albeit with slightly more architectural complexity. Both approaches leverage meta-learning to optimize *how* the foundation model learns, rather than *what* it learns, making them crucial for efficient deployment.

Furthermore, meta-learning plays a role in enabling foundation models to adapt to complex, real-time scenarios. For instance, the MAGICVFM framework \cite{lupu20249p4} demonstrates meta-learning's utility in adapting Visual Foundation Models (VFMs) for real-time ground interaction control in autonomous vehicles. By combining offline meta-learning with online composite adaptive control, it adaptively modifies only the last layer of a deep neural network, providing mathematically guaranteed stability and efficient adaptation to dynamic, unmodeled terrains. While robust generalization to out-of-distribution (OOD) data and domain shifts is a critical challenge for foundation models, meta-learning contributes to this by designing training strategies that promote this capability. For a detailed discussion on meta-learning strategies for domain generalization and OOD robustness, including methods like adversarial meta-training frameworks for cross-domain few-shot learning \cite{tian2023iyh}, refer to Subsection 6.3.

In conclusion, meta-learning is indispensable for unlocking the full potential of large foundation models. It has evolved from learning initial model weights to developing sophisticated strategies for parameter-efficient adaptation, such as prompt learning and selective module tuning. While significant progress has been made in improving generalization and efficiency, challenges remain in developing universally robust and theoretically grounded meta-adaptation strategies that can consistently perform across extreme domain shifts and ensure safety in critical applications. Future research will likely focus on deeper theoretical understandings of meta-learning's interaction with foundation models, exploring more adaptive and context-aware prompt generation, and developing methods that offer stronger guarantees for out-of-distribution generalization and safety in real-world deployment.