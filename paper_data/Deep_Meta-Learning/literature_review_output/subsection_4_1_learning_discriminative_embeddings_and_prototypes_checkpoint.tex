\subsection*{Learning Discriminative Embeddings and Prototypes}
Few-shot learning presents a significant challenge where models must generalize to novel categories with minimal labeled examples, a core problem that metric-based meta-learning aims to address \cite{huisman2020b7w, peng20209of}. This paradigm focuses on learning an effective embedding space where examples from the same class are close, and examples from different classes are far apart, coupled with robust comparison mechanisms. These methods are crucial for few-shot classification as they prioritize learning representations that generalize well to unseen classes, moving beyond rote memorization to capture transferable semantic properties and enable rapid adaptation to new tasks.

A foundational approach in this domain is \textit{Matching Networks} \cite{Vinyals2016}, which introduced an end-to-end differentiable neural network capable of performing one-shot learning. This method learns an embedding function that maps input examples into a feature space. Crucially, it employs an attention mechanism to compare a query example with all examples in a support set. The classification of the query is then determined by a weighted sum of the labels of the support examples, where weights are derived from the learned similarity scores. This effectively functions as a non-parametric classifier, adapting its decision boundary based on the support set. While innovative for its direct, end-to-end learning of a comparison mechanism, the attention over all support examples can become computationally intensive and less efficient for larger support sets, limiting its scalability.

Building upon the concept of learning a discriminative embedding space, \textit{Prototypical Networks} \cite{Snell2017} offered a significant simplification and improvement, directly addressing some of the complexities of Matching Networks. Instead of comparing a query to every individual support example, Prototypical Networks learn an embedding space where each class is represented by a single "prototype." This prototype is typically computed as the centroid (mean) of its support examples in the learned embedding space. Classification then becomes a straightforward nearest-neighbor problem, assigning a query example to the class whose prototype is closest in the embedding space. This approach demonstrated improved robustness and interpretability due to its simpler comparison mechanism and often achieved competitive or superior performance with greater efficiency, making it a highly influential method in few-shot learning.

However, this reliance on a single centroid per class is a strong inductive bias that can be detrimental when representing classes with complex, non-convex, or multi-modal feature distributions in the embedding space. For instance, if a class has two distinct sub-categories, a single centroid might not accurately represent either, leading to misclassifications. To address this single-centroid limitation, subsequent work has explored representing classes with more flexible distributions, such as Gaussian prototypes, thereby capturing intra-class variance more effectively. Furthermore, nearest-neighbor classification, as employed by Prototypical Networks, is susceptible to the "hubness problem" in high-dimensional embedding spaces, where certain prototypes become nearest neighbors to a disproportionately large number of query examples regardless of their true class \cite{fei20211x6}. This phenomenon can degrade classification performance by creating "hubs" that attract many unrelated samples, reducing the discriminative power of the learned embeddings.

Recognizing that the quality of the learned embedding space is paramount for these methods, subsequent research has focused on enhancing the feature extraction process itself. For example, \textit{Deep Meta-Learning: Learning to Learn in the Concept Space} \cite{zhou20188lr} improves upon Matching Networks and other meta-learners by introducing a "concept generator" that extracts high-level representations, allowing the meta-learner to operate in a more abstract and discriminative concept space. Similarly, \textit{Self-supervised Knowledge Distillation for Few-shot Learning} \cite{rajasegaran2020glw} proposes a two-stage process to improve representation capacity, first maximizing entropy of the feature embedding with a self-supervised auxiliary loss, then minimizing it via distillation, demonstrating that robust feature learning alone can significantly boost few-shot performance. The importance of the backbone network's complexity in achieving high performance in few-shot classification has also been empirically noted, suggesting that larger, more powerful feature extractors often lead to better embeddings \cite{huisman2020b7w}.

The effectiveness of these prototype-based and attention-based methods is highly dependent not only on the quality of the embedding function but also on the selection of support examples and the robustness of the metric learning process itself. Consequently, a significant line of research has focused on meta-learning the metric learning process to be robust to sub-optimal sampling and to adaptively refine the comparison mechanism. \textit{Deep Meta Metric Learning (DMML)} \cite{chen2019oep} formulates metric learning in a meta-way, sampling subsets from the training data to learn robust metrics across different sub-tasks. It introduces the concept of set-based distance and hard sample mining for these sets, demonstrating that common loss functions can be made consistent in a meta-space. This approach aims to learn a metric that is inherently more robust and generalizable by optimizing the metric learning process itself across diverse meta-tasks.

Addressing the challenge of sub-optimal sample selection, which is critical for effective metric learning, \textit{Deep Metric Learning via Adaptive Learnable Assessment (DML-ALA)} \cite{zheng20200ig} proposes a meta-learning approach to learn an adaptive sample assessment strategy. This method employs a sequence-aware learnable assessor that re-weights each training example to maximize the generalization of the trained metric. By adapting to the current model status through an episode-based training scheme, DML-ALA ensures that the most informative samples contribute more to the metric learning process, thereby improving the discriminative power of the learned embeddings. Expanding on this, \textit{Deep Metric Learning Based on Meta-Mining Strategy With Semiglobal Information (MMSI)} \cite{jiang20220tg} further refines the meta-mining strategy. It incorporates richer "semiglobal information" from a validation set and a memory dictionary, allowing for a more comprehensive and adaptive sample weighting during training. This approach aims to overcome the limitations of relying solely on information from a single training batch, leading to state-of-the-art performance by providing a more informed and stable meta-learning signal for sample selection.

The utility of learning discriminative embeddings and prototypes extends to more complex and challenging tasks beyond simple image classification. For instance, \textit{Few-Shot Cross-Domain Object Detection With Instance-Level Prototype-Based Meta-Learning (IPNet)} \cite{zhang2024mf0} applies instance-level prototype learning in a challenging cross-domain object detection scenario. IPNet fuses cropped instances from both source and target domains to learn representative prototypes for each class, demonstrating how prototype-based methods can be effectively adapted to compensate for data deficiency and mitigate domain shifts. Similarly, for few-shot short utterance speaker verification, a meta-learning approach integrating an Emphasized Channel Attention, Propagation and Aggregation in TDNN (ECAPA-TDNN) within a Prototypical Network (ETP) has shown superior performance by combining a robust feature extractor with an episodic training strategy that includes a global classification objective \cite{wang2023x5w}. These applications highlight the versatility of the core concept in pushing the boundaries of few-shot learning into more complex recognition and verification tasks.

In conclusion, the evolution of learning discriminative embeddings and prototypes has progressed from foundational attention-based comparison mechanisms in Matching Networks to simplified, efficient, and interpretable prototype-based representations in Prototypical Networks. While these methods established the efficacy of learning robust embedding spaces, critical analysis reveals limitations such as the strong inductive bias of single centroids and the hubness problem inherent in nearest-neighbor classification. Subsequent advancements have focused on enhancing the quality of the embedding function itself and, more recently, on meta-learning the metric learning process through adaptive sample assessment and mining strategies (e.g., DMML, DML-ALA, MMSI). While these methods have significantly advanced few-shot classification by focusing on robust representations and efficient comparison, challenges remain in scaling to extremely diverse tasks, balancing the simplicity and expressiveness of comparison mechanisms, and developing even more adaptive metric learning strategies that can generalize across a broader spectrum of data distributions and task complexities, particularly in scenarios with significant domain shifts or highly imbalanced data.