\subsection{Learning to Reinforcement Learn and One-Shot Imitation}

The quest for intelligent agents capable of rapid adaptation and data-efficient skill acquisition in dynamic environments has led to the emergence of meta-learning as a powerful paradigm for sequential decision-making. This subsection delves into foundational works that pioneered the application of meta-learning to reinforcement learning (RL) and imitation learning, enabling agents to implicitly learn learning algorithms and acquire new skills from minimal demonstrations.

Early explorations into meta-learning demonstrated that recurrent neural networks (RNNs) could implicitly learn simple learning algorithms in supervised settings, effectively functioning as "meta-learners" by processing sequences of inputs and targets \cite{santoro2016323}. Building upon this insight, \textcite{wang20167px} introduced the seminal concept of "learning to reinforcement learn," extending RNN-based meta-learning to the more complex domain of sequential decision-making. Their work demonstrated that an LSTM, trained with a standard deep RL algorithm across a distribution of related tasks, could implicitly learn a distinct RL algorithm within its recurrent dynamics. By receiving previous actions and rewards as auxiliary inputs, the network's internal state effectively encoded a task-specific policy and exploration strategy, allowing it to adapt rapidly to new tasks from the training distribution without explicit weight updates during adaptation. While a proof-of-concept on constrained environments like bandit problems and simple Markov Decision Processes, this work fundamentally shifted the perspective from hand-designing RL algorithms to learning them directly from experience.

Concurrent advancements extended meta-learning to the realm of imitation learning, particularly for robotic skill acquisition. \textcite{finn20174c4} presented a groundbreaking application of Model-Agnostic Meta-Learning (MAML) to one-shot visual imitation learning. This approach enabled robots to acquire new visuomotor skills from a single visual demonstration by adapting a policy through gradient updates. The core innovation lay in training a policy initialization that, when fine-tuned with a single demonstration, could quickly converge to a proficient task-specific policy. To facilitate this, the authors introduced architectural modifications such as a two-head network and bias transformation, enhancing the stability and representational power of gradient-based meta-learning for end-to-end learning from raw pixel inputs. This work significantly addressed the data inefficiency of traditional imitation learning, paving the way for more generalist robots capable of rapid skill transfer. However, it did not explicitly address challenges like compounding errors or domain shift between demonstration and execution.

These foundational works laid critical groundwork for subsequent advancements in meta-RL and meta-imitation learning. The implicit RL algorithm learned by RNNs in \textcite{wang20167px} showcased the potential for flexible adaptation, but its black-box nature made analysis and generalization beyond the training distribution challenging. Similarly, \textcite{finn20174c4}'s MAML-based approach, while powerful for one-shot imitation, could still be sample-inefficient during the meta-training phase and faced limitations in complex, high-dimensional RL settings. Addressing these limitations, later works like \textcite{rakelly2019m09} introduced more sophisticated off-policy meta-RL algorithms, such as PEARL, which leveraged probabilistic context variables to explicitly model task uncertainty and enable structured exploration, significantly improving meta-training sample efficiency. This marked a progression from implicitly learned algorithms to more principled, probabilistic approaches for task inference and policy adaptation.

In conclusion, these early applications of meta-learning to sequential decision-making established a powerful paradigm for data-efficient policy acquisition and rapid skill transfer. By demonstrating that agents could either implicitly learn an RL algorithm through recurrent dynamics \cite{wang20167px} or quickly adapt a policy through gradient-based meta-learning from a single demonstration \cite{finn20174c4}, these works fundamentally challenged the traditional approach of learning each task from scratch. They highlighted the potential for agents to "learn to learn" in dynamic, interactive environments, setting the stage for future research into more robust, sample-efficient, and generalizable meta-learning algorithms for complex real-world challenges. Despite their success, challenges such as scalability to highly complex environments, theoretical guarantees for learned algorithms, and robust generalization to entirely novel tasks remain active areas of research.