\subsection{Learning Explicit Optimizers and Differentiable Solvers}

Beyond learning a mere initialization, advanced meta-learning techniques focus on explicitly learning the optimization process itself or integrating differentiable solvers as adaptable base learners. These approaches aim to achieve greater flexibility and fine-grained control over the inner-loop adaptation, thereby enhancing the meta-learner's ability to optimize effectively for novel tasks.

The concept of learning an optimization algorithm itself, rather than relying on fixed gradient descent rules, represents a significant leap in meta-learning. This idea is rooted in the theoretical finding by \cite{finn2017vrt} that deep representations combined with standard gradient descent can approximate any learning algorithm, providing a strong universality argument for trainable optimizers. Historically, the broader field of meta-gradient methods, which learn meta-parameters like step sizes, has been explored for years \cite{sutton2022jss}. A seminal contribution to explicitly learning optimizers came from \cite{andrychowicz2016learning}, who proposed using a recurrent neural network (RNN), specifically an LSTM, as a learned optimizer. This LSTM takes gradients and previous hidden states as input and outputs parameter updates, effectively learning an update rule from data. Building on this, \cite{ravi2017optimization} introduced the Meta-Learner LSTM, applying this concept to few-shot learning by training an LSTM to generate updates for a base learner, demonstrating rapid adaptation to new tasks. Another key advancement, Meta-SGD \cite{li2017meta}, further refined this by meta-learning not only the initialization but also per-parameter learning rates and update directions, offering fine-grained control over the inner-loop adaptation process. These early works established the viability of replacing hand-designed optimizers with learned, data-driven counterparts, moving beyond the limitations of fixed update rules.

Further advancements have explored more sophisticated learned optimization mechanisms. \cite{franceschi2018u1q} provided a unifying framework based on bilevel programming, demonstrating how gradient-based hyperparameter optimization and meta-learning can be framed to explicitly account for inner-objective optimization dynamics, which is crucial for understanding learned optimizers. HyperMAML \cite{przewiezlikowski2022d4y} addresses limitations of traditional MAML by replacing its gradient-based inner-loop adaptation with a trainable hypernetwork. This hypernetwork directly generates weight updates in a single step, offering more substantial and efficient modifications without the computational overhead of second-order optimization, and allowing for more flexible, non-gradient-based adaptation. Learned optimizers have proven particularly valuable in challenging domains like continual learning, where mitigating catastrophic forgetting is paramount. \cite{vuorio2018gwb} proposed a meta-continual learning approach where a neural network predicts parameter update steps specifically designed to prevent forgetting. Similarly, \cite{gu2019tvc} explored meta-learning biologically plausible synaptic update rules that integrate neuron-local information, moving towards more interpretable and biologically inspired learning mechanisms. The Feedback and Local Plasticity (FLP) framework introduced by \cite{lindsey202075a} further advanced this by meta-learning decoupled feedback weights and local plasticity rules, demonstrating effective deep credit assignment and superior performance in continual learning compared to gradient-based baselines. For incremental learning, \cite{rajasegaran2020llk} proposed iTAML, a task-agnostic meta-learning approach that introduces a novel meta-update rule to maintain equilibrium across encountered tasks and prevent catastrophic forgetting. While not a learned optimizer itself, the theoretical analysis of MAML's inner-loop learning rate by \cite{bernacchia20211r0}, revealing the counter-intuitive finding that an optimal *negative* learning rate can exist during meta-training in overparameterized models, highlights the complexities of fixed optimization schedules and implicitly motivates the need for adaptive, learned update rules that can transcend such limitations.

An alternative yet complementary direction involves integrating differentiable closed-form or iterative solvers as the inner-loop adaptation mechanism. This approach leverages the stability and efficiency of classical machine learning algorithms while allowing for end-to-end meta-learning. \cite{bertinetto2018ur2} pioneered this by proposing a meta-learning framework where the base learner is a differentiable closed-form solver, such as Ridge Regression or Logistic Regression. By efficiently backpropagating through these solvers using techniques like the Woodbury identity, their method achieved competitive performance in few-shot learning while offering greater flexibility than similarity-based methods and faster convergence than gradient-based ones. Extending this concept to more complex models, \cite{wistuba2021wha} developed Few-Shot Bayesian Optimization with Deep Kernel Surrogates, where a deep kernel network for Gaussian Processes is meta-learned to rapidly adapt to new hyperparameter optimization tasks with limited evaluations. This allows for robust predictions with well-calibrated uncertainty. Building on this, \cite{chen2022z45} introduced Adaptive Deep Kernel Fitting with Implicit Function Theorem (ADKF-IFT) for Deep Kernel Gaussian Processes, which unifies previous approaches by meta-learning a subset of parameters while adapting others per task using exact hypergradients computed via the Implicit Function Theorem. This provides fine-grained control over the adaptation process, balancing generalization and task-specificity effectively for molecular property prediction. Most recently, \cite{lee2024snq} proposed Sequential Bayesian Meta-Continual Learning (SB-MCL), a novel framework that meta-learns neural networks to bridge raw data to simple statistical models that perform *exact* sequential Bayesian updates. This innovative approach inherently prevents catastrophic forgetting in continual learning by offloading sequential knowledge integration to robust statistical models, representing a sophisticated form of learned, differentiable adaptation.

In summary, the progression from learning initializations to explicitly learning optimization processes and integrating differentiable solvers marks a significant evolution in meta-learning. Learned optimizers, from foundational RNN-based approaches to hypernetworks, offer enhanced flexibility and fine-grained control over parameter updates, particularly beneficial in scenarios requiring complex adaptation or resistance to forgetting. Differentiable solvers, on the other hand, provide stability and efficiency by leveraging well-understood classical algorithms within an end-to-end meta-learning framework. These methods collectively represent a powerful paradigm for designing meta-learners that can adapt more effectively and robustly than those relying solely on initial parameter learning. Future research will likely explore hybrid approaches that combine the strengths of learned optimizers with the guarantees of differentiable solvers, extend these techniques to even more complex and safety-critical domains, and further deepen the theoretical understanding of the meta-optimization landscape to unlock new capabilities in adaptable AI systems.