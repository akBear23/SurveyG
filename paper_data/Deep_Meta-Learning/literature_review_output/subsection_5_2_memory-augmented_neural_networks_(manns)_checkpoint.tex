\subsection{Memory-Augmented Neural Networks (MANNs)}

Model-based meta-learning aims to design neural network architectures that are intrinsically capable of rapid adaptation and in-context learning, moving beyond simple parameter adaptation. A prominent approach within this paradigm involves Memory-Augmented Neural Networks (MANNs), which integrate external memory modules to store and retrieve task-relevant information in a differentiable manner. This architectural innovation allows meta-learners to leverage explicit memory mechanisms for rapid adaptation and few-shot learning.

The foundational concept of MANNs draws inspiration from the idea of equipping neural networks with a "working memory" similar to a computer, enabling them to read from and write to an external data store. Key architectures in this domain include Neural Turing Machines (NTMs) \cite{graves2014neural} and Differentiable Neural Computers (DNCs) \cite{graves2016hybrid}. These models extend traditional recurrent neural networks by coupling them with a large, addressable memory matrix. The network learns to control memory access through differentiable read and write heads, allowing gradient-based optimization of both the network's internal parameters and its memory interaction strategies. This differentiable nature is crucial, as it permits end-to-end training of the entire system, enabling the meta-learner to acquire sophisticated strategies for information storage and retrieval tailored to new tasks.

A seminal work in applying this concept to meta-learning is the Memory-Augmented Neural Network (MANN) proposed by \cite{santoro2016323}. This paper demonstrated how an external memory module, inspired by NTMs, could enable a neural network to perform one-shot learning. The MANN learns to store representations of individual examples or features encountered during a new task in its external memory. When presented with a query example from the same task, the network can then retrieve relevant information from memory to make an accurate prediction, effectively "remembering" the task-specific knowledge without requiring extensive gradient updates to its internal weights. This mechanism facilitates rapid adaptation by allowing the meta-learner to leverage an explicit, episodic memory of past experiences within the current task.

The integration of external memory in MANNs offers several distinct advantages for meta-learning. Firstly, it provides a mechanism for sophisticated in-context learning, where the network can process a sequence of examples from a new task and dynamically update its internal state and external memory to build a rich understanding of the task's specifics. This allows for knowledge transfer across diverse tasks, as the network learns general strategies for how to encode and retrieve useful information, rather than just learning task-specific parameters. Secondly, MANNs can mitigate catastrophic forgetting, a common challenge in traditional neural networks, by storing distinct task-relevant information in separate memory locations or by learning to selectively overwrite memory. This enables the network to quickly adapt to new tasks while retaining knowledge from previous ones.

Despite their powerful capabilities, MANNs face inherent challenges. The complexity of training memory-augmented networks, particularly in optimizing the intricate interactions between the controller network and the memory module, can be substantial. Furthermore, the scalability of memory access mechanisms and the overall memory size can become a limitation for very large or complex datasets, as managing and searching through vast memory stores can be computationally intensive. These challenges have led to continued research into more efficient memory architectures and alternative model-based meta-learning approaches that leverage different forms of in-context learning, such as those based on attention and temporal convolutions, or probabilistic function approximation, to achieve rapid adaptation with potentially simpler or more scalable designs.