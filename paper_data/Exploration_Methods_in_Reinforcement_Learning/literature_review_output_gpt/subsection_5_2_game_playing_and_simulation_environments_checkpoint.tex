\subsection{Game Playing and Simulation Environments}

The exploration methods in game playing and simulation environments pose a significant challenge for reinforcement learning (RL) agents, particularly in scenarios characterized by sparse rewards and complex state-action spaces. As agents navigate these environments, the need for effective exploration strategies becomes paramount to ensure they can discover rewarding states and actions efficiently. Recent advancements in exploration techniques, including population-based methods and meta-learning, have led to notable improvements in agent performance across various competitive games.

A seminal contribution to the exploration landscape is the work by \cite{ding2023whs}, which introduces the concept of Incremental Reinforcement Learning (Incremental RL) alongside the Dual-Adaptive $\epsilon$-greedy Exploration (DAE) method. This framework addresses the challenge of expanding state and action spaces, a common issue in dynamic environments. DAE employs a Meta Policy to adaptively determine exploration probabilities based on the convergence of state value estimates, while an Explorer component prioritizes actions that have been least tried. This dual mechanism significantly enhances exploration efficiency, allowing agents to adapt to new states and actions without incurring the high computational costs associated with retraining from scratch.

Building on the foundations of exploration in RL, \cite{conti2017cr2} explores the integration of novelty-seeking strategies within Evolution Strategies (ES). By leveraging populations of agents that engage in directed exploration through novelty search and quality diversity algorithms, this approach demonstrates improved performance on sparse reward tasks. The synergy between DAE's adaptive exploration and the novelty-driven exploration in ES highlights a critical advancement in addressing local optima and enhancing exploration in complex environments.

Further expanding on exploration methodologies, \cite{houthooft2016yee} introduces Variational Information Maximizing Exploration (VIME), which utilizes Bayesian neural networks to maximize information gain about the environment. VIME effectively modifies the reward structure of the MDP to encourage exploration in high-dimensional spaces, demonstrating superior performance compared to traditional heuristics. This work complements the findings of \cite{ding2023whs} by providing a principled approach to exploration that addresses the inherent uncertainties in dynamic environments.

In the context of multi-agent systems, \cite{yang2021ngm} surveys various exploration strategies, emphasizing the challenges of sample inefficiency and the need for effective exploration in both single-agent and multi-agent settings. The paper categorizes existing methods into uncertainty-oriented and intrinsic motivation-oriented exploration, highlighting the importance of balancing exploration and exploitation in complex environments. This comprehensive analysis underscores the necessity for adaptive exploration strategies that can generalize across different scenarios, a theme echoed in the incremental learning framework proposed by \cite{ding2023whs}.

Despite these advancements, challenges remain in effectively balancing exploration strategies with the demands of real-time learning in dynamic environments. For instance, while \cite{mavrin2019iqm} proposes a distributional RL approach that utilizes upper quantiles for exploration bonuses, it still faces limitations in environments with non-stationary dynamics. The need for exploration methods that can adapt to continual shifts in state and action spaces is a critical area for future research.

In conclusion, the exploration methods developed in recent literature reflect a significant evolution from traditional approaches, focusing on adaptive strategies that enhance agent performance in complex game environments. However, unresolved issues, such as the integration of exploration strategies in non-stationary environments and the balance between exploration and exploitation, present opportunities for further investigation. Future directions may include the development of more robust frameworks that combine the strengths of various exploration techniques, ultimately paving the way for more capable and adaptable RL agents in real-world applications.
```