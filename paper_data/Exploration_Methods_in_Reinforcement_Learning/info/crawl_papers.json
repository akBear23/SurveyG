[
  {
    "id": "c28ec2a40a2c77e20d64cf1c85dc931106df8e83",
    "title": "Overcoming Exploration in Reinforcement Learning with Demonstrations",
    "authors": [
      "Ashvin Nair",
      "Bob McGrew",
      "Marcin Andrychowicz",
      "Wojciech Zaremba",
      "P. Abbeel"
    ],
    "year": 2017,
    "citationCount": 814,
    "abstract": "Exploration in environments with sparse rewards has been a persistent problem in reinforcement learning (RL). Many tasks are natural to specify with a sparse reward, and manually shaping a reward function can result in suboptimal performance. However, finding a non-zero reward is exponentially more difficult with increasing task horizon or action dimensionality. This puts many real-world tasks out of practical reach of RL methods. In this work, we use demonstrations to overcome the exploration problem and successfully learn to perform long-horizon, multi-step robotics tasks with continuous control such as stacking blocks with a robot arm. Our method, which builds on top of Deep Deterministic Policy Gradients and Hindsight Experience Replay, provides an order of magnitude of speedup over RL on simulated robotics tasks. It is simple to implement and makes only the additional assumption that we can collect a small set of demonstrations. Furthermore, our method is able to solve tasks not solvable by either RL or behavior cloning alone, and often ends up outperforming the demonstrator policy.",
    "url": "https://www.semanticscholar.org/paper/c28ec2a40a2c77e20d64cf1c85dc931106df8e83",
    "pdf_url": "https://arxiv.org/pdf/1709.10089.pdf",
    "venue": "IEEE International Conference on Robotics and Automation",
    "publicationDate": "2017-09-28",
    "externalIds": {
      "ArXiv": "1709.10089",
      "MAG": "2756826236",
      "DBLP": "journals/corr/abs-1709-10089",
      "DOI": "10.1109/ICRA.2018.8463162",
      "CorpusId": 3543784
    },
    "references": [
      {
        "paperId": "1bead9000a719cb258bac7320228055aee650d2c",
        "title": "Leveraging Demonstrations for Deep Reinforcement Learning on Robotics Problems with Sparse Rewards"
      },
      {
        "paperId": "9862caed8ee93321c78b0196e0b7eef516b545ba",
        "title": "Reverse Curriculum Generation for Reinforcement Learning"
      },
      {
        "paperId": "429ed4c9845d0abd1f8204e1d7705919559bc2a2",
        "title": "Hindsight Experience Replay"
      },
      {
        "paperId": "e3b0ea7209731c47b582215c6c67f9c691ad9863",
        "title": "Deep Q-learning From Demonstrations"
      },
      {
        "paperId": "a7fb199f85943b3fb6b5f7e9f1680b2e2a445cce",
        "title": "Learning from Demonstrations for Real World Reinforcement Learning"
      },
      {
        "paperId": "017b07fe36e8d43965b2125f6170a97c9d747fca",
        "title": "Data-efficient Deep Reinforcement Learning for Dexterous Manipulation"
      },
      {
        "paperId": "5c57bb5630835a05eb1c3d0df3e12d6180d75de2",
        "title": "One-Shot Imitation Learning"
      },
      {
        "paperId": "5ce030f1650145a103527e883e7a9d9a25c45547",
        "title": "A Machine Learning Approach to Visual Perception of Forest Trails for Mobile Robots"
      },
      {
        "paperId": "e37b999f0c96d7136db07b0185b837d5decd599a",
        "title": "Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates"
      },
      {
        "paperId": "0e3cc46583217ec81e87045a4f9ae3478a008227",
        "title": "End to End Learning for Self-Driving Cars"
      },
      {
        "paperId": "494e2d5b40dcebde349f9872c7317e5003f9c5d2",
        "title": "Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection"
      },
      {
        "paperId": "04162cb8cfaa0f7e37586823ff4ad0bff09ed21d",
        "title": "Guided Cost Learning: Deep Inverse Optimal Control via Policy Optimization"
      },
      {
        "paperId": "69e76e16740ed69f4dc55361a3d319ac2f1293dd",
        "title": "Asynchronous Methods for Deep Reinforcement Learning"
      },
      {
        "paperId": "846aedd869a00c09b40f1f1f35673cb22bc87490",
        "title": "Mastering the game of Go with deep neural networks and tree search"
      },
      {
        "paperId": "f03b4ff1b4943691cec703b508c0a91f2d97a881",
        "title": "Supersizing self-supervision: Learning to grasp from 50K tries and 700 robot hours"
      },
      {
        "paperId": "024006d4c2a89f7acacc6e4438d156525b60a98f",
        "title": "Continuous control with deep reinforcement learning"
      },
      {
        "paperId": "5dc2a215bd7cd5bdd3a0baa8c967575632696fac",
        "title": "Universal Value Function Approximators"
      },
      {
        "paperId": "b6b8a1b80891c96c28cc6340267b58186157e536",
        "title": "End-to-End Training of Deep Visuomotor Policies"
      },
      {
        "paperId": "340f48901f72278f6bf78a04ee5b01df208cc508",
        "title": "Human-level control through deep reinforcement learning"
      },
      {
        "paperId": "449532187c94af3dd3aa55e16d2c50f7854d2199",
        "title": "Trust Region Policy Optimization"
      },
      {
        "paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8",
        "title": "Adam: A Method for Stochastic Optimization"
      },
      {
        "paperId": "680345b8799e659b60018c77cddcfff2b781d529",
        "title": "Combined task and motion planning through an extensible planner-independent interface layer"
      },
      {
        "paperId": "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5",
        "title": "Neural Machine Translation by Jointly Learning to Align and Translate"
      },
      {
        "paperId": "8932789178defaa6eafcb054da7b0ac6acf004f7",
        "title": "Learning from Limited Demonstrations"
      },
      {
        "paperId": "b354ee518bfc1ac0d8ac447eece9edb69e92eae1",
        "title": "MuJoCo: A physics engine for model-based control"
      },
      {
        "paperId": "60b7d47758a71978e74edff6dd8dea4d9c791d7a",
        "title": "PILCO: A Model-Based and Data-Efficient Approach to Policy Search"
      },
      {
        "paperId": "4f185ec16ce9c4e2d01d4acb0f9b46fe91b1b1eb",
        "title": "Learning to Control a Low-Cost Manipulator using Data-Efficient Reinforcement Learning"
      },
      {
        "paperId": "9b7ae896675c71ac50fa1fbc555cb19f80863f0e",
        "title": "Hierarchical task and motion planning in the now"
      },
      {
        "paperId": "79ab3c49903ec8cb339437ccf5cf998607fc313e",
        "title": "A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning"
      },
      {
        "paperId": "e0456b744af84f07cb5e750217463214f96c921e",
        "title": "Relative Entropy Policy Search"
      },
      {
        "paperId": "c0018e7a8c3412ac567dad8e93685c5078ae7fbe",
        "title": "Learning locomotion over rough terrain using terrain templates"
      },
      {
        "paperId": "c8221c054459e37edbf313668523d667fe5c1536",
        "title": "Maximum Entropy Inverse Reinforcement Learning"
      },
      {
        "paperId": "ffced5b53ad956474a12d73b5cbfd38355dfb70a",
        "title": "Reinforcement learning of motor skills with policy gradients"
      },
      {
        "paperId": "f65020fc3b1692d7989e099d6b6e698be5a50a93",
        "title": "Apprenticeship learning via inverse reinforcement learning"
      },
      {
        "paperId": "92e51de612ea7c1c7dd3b5ff1cb9eb01b1c8475b",
        "title": "Learning from demonstration and adaptation of biped locomotion"
      },
      {
        "paperId": "be2f6bb4e262d7015b931c12eb1559827ba597dc",
        "title": "Robot learning from demonstration"
      },
      {
        "paperId": "dd6960d576da0d769ca30de6eb6607a66806211f",
        "title": "Algorithms for Inverse Reinforcement Learning"
      },
      {
        "paperId": "7e81dac6260c4768af3a28ac21c78c5a38a5f7d0",
        "title": "Probabilistic roadmaps for path planning in high-dimensional configuration spaces"
      },
      {
        "paperId": "29abaa5bee5bb297949fd262c4bfd1569328b900",
        "title": "DEMONSTRATION"
      },
      {
        "paperId": "7786bc6c25ba38ff0135f1bdad192f6b3c4ad0b3",
        "title": "ALVINN, an autonomous land vehicle in a neural network"
      },
      {
        "paperId": "de4961a6431b9553d9f13319236570a9f033fcab",
        "title": "Understanding natural language"
      },
      {
        "paperId": "4d6a87d76ec0c0379f0afbf24f84bba848c6246e",
        "title": "Noname manuscript No. (will be inserted by the editor) Policy Search for Motor Primitives in Robotics"
      }
    ],
    "cited_by": [
      {
        "paperId": "ca8fb30e4425d6fde21695716f71d3f1433c9287",
        "title": "On Discovering Algorithms for Adversarial Imitation Learning"
      },
      {
        "paperId": "0bc6d2da78e2deefc1b87f80ab8d6b2c41bfb69c",
        "title": "Sequence Pathfinder for Multi-Agent Pickup and Delivery in the Warehouse"
      },
      {
        "paperId": "245f8ee0fa638ebbf0349ac2d5b9fa36d2143443",
        "title": "Zero-shot Whole-Body Manipulation with a Large-Scale Soft Robotic Torso via Guided Reinforcement Learning"
      },
      {
        "paperId": "31d5ffc672866721cfff4f1bb673c220ded8f004",
        "title": "Tackling GNARLy Problems: Graph Neural Algorithmic Reasoning Reimagined through Reinforcement Learning"
      },
      {
        "paperId": "543f87b15b888cb0a2381c5b36141256ad5ec84e",
        "title": "LLM-Guided Task- and Affordance-Level Exploration in Reinforcement Learning"
      },
      {
        "paperId": "ba1f70ea8347a229dd083d92c442b9eb3d2f89b6",
        "title": "Uncertainty-Based Smooth Policy Regularisation for Reinforcement Learning with Few Demonstrations"
      },
      {
        "paperId": "d815c3b036febd337af2eebf0c68cb8c3a3d53c4",
        "title": "Trust-Calibrated Human-in-the-Loop Reinforcement Learning for Safe and Efficient Autonomous Navigation"
      },
      {
        "paperId": "ff55393f87cd7d93b5103cdc0e7f8ee2384d1b76",
        "title": "Dexplore: Scalable Neural Control for Dexterous Manipulation from Reference-Scoped Exploration"
      },
      {
        "paperId": "b6e5f61ed2c12f259e60c6b7237c78175c4a477b",
        "title": "Imitation-relaxation reinforcement learning for sparse badminton strikes via dynamic trajectory generation"
      },
      {
        "paperId": "7cefb07ebc9bb3ea7c667937d1016ba4758858df",
        "title": "HAEPO: History-Aggregated Exploratory Policy Optimization"
      },
      {
        "paperId": "a0132748f31498c569220b32d32400f7ad5d092b",
        "title": "An Efficient Open World Environment for Multi-Agent Social Learning"
      },
      {
        "paperId": "929eb043e19cc7f2f9e93429f393c39acb7043d6",
        "title": "Mixture of Autoencoder Experts Guidance using Unlabeled and Incomplete Data for Exploration in Reinforcement Learning"
      },
      {
        "paperId": "3983740e1949b1b9e80184b9adfd0792a38ecd7d",
        "title": "Inverse Reinforcement Learning Meets Large Language Model Post-Training: Basics, Advances, and Opportunities"
      },
      {
        "paperId": "b38af7b162f9c4bef03975f3832410b2456718fd",
        "title": "EXPO: Stable Reinforcement Learning with Expressive Policies"
      },
      {
        "paperId": "7a219e065b1099f5c482b345635f7ab0a56a013d",
        "title": "Efficient Multi-task Reinforcement Learning with Cross-Task Policy Guidance"
      },
      {
        "paperId": "f04517cd932e1812944694e8204fea4a663ed26b",
        "title": "Learning Agile Tensile Perching for Aerial Robots from Demonstrations"
      },
      {
        "paperId": "096d4a024a818d497291ba0371ef4d924b52b892",
        "title": "Accelerated Online Reinforcement Learning using Auxiliary Start State Distributions"
      },
      {
        "paperId": "12ae1183f891f0369cba7840647bb266cfae9f4f",
        "title": "Robust Visuomotor Control for Humanoid Loco-Manipulation Using Hybrid Reinforcement Learning"
      },
      {
        "paperId": "c67785a778c4cacd3356f99cc85384980466433c",
        "title": "Inverse Skill Learning From Demonstrations via Reinforcement Learning"
      },
      {
        "paperId": "507dd245030099afc1f5a416ac5a9547837e4cbe",
        "title": "Scaffolding Dexterous Manipulation with Vision-Language Models"
      },
      {
        "paperId": "ab91cc77a6a5865ef6d32fcbfba28e648b930a8d",
        "title": "Accelerating Residual Reinforcement Learning with Uncertainty Estimation"
      },
      {
        "paperId": "287e3a04a9398679358a021cf2e53e67dace74c1",
        "title": "From Simulation to Reality: Deep Reinforcement Learning for Autonomous Underwater Vehicle Docking"
      },
      {
        "paperId": "37483be6933d29635e6a5483ba3f1ee2b3dcafc8",
        "title": "MOORL: A Framework for Integrating Offline-Online Reinforcement Learning"
      },
      {
        "paperId": "845e7b768dd3b846be8e2aa5a001d57fcc635a90",
        "title": "Uncertainty Prioritized Experience Replay"
      },
      {
        "paperId": "6e19a3ed0f582faa74d3909aaf74d8997100739f",
        "title": "Reinforcement Learning via Implicit Imitation Guidance"
      },
      {
        "paperId": "27ef41b1a69202ee6562c90aaec175cc47ec534e",
        "title": "Think Twice, Act Once: A Co-Evolution Framework of LLM and RL for Large-Scale Decision Making"
      },
      {
        "paperId": "34d8817c85fd7f351e75a6bcbdd851a559877300",
        "title": "Trajectory First: A Curriculum for Discovering Diverse Policies"
      },
      {
        "paperId": "0fc969805f12b1b1b03fb9c7de73074c31d5bbf6",
        "title": "Intelligent Eco-Driving Control for Urban CAVs Using a Model-Based Controller Assisted Deep Reinforcement Learning"
      },
      {
        "paperId": "1705a7abc5ec333a70397807ad323642744508ec",
        "title": "Interactive Imitation Learning for Dexterous Robotic Manipulation: Challenges and Perspectives - A Survey"
      },
      {
        "paperId": "6cf69bada16e23b9ab912b1d15afabdbefcd4e50",
        "title": "Diffusion-Based Generative System Surrogates for Scalable Learning-Driven Optimization in Virtual Playgrounds"
      },
      {
        "paperId": "203c272f8be5cde57e6ef3b9afa252bcf5dfb3c9",
        "title": "Intelligent Bidding Strategies in Local Energy Markets using the Advantage Actor-Critic Algorithm"
      },
      {
        "paperId": "86e37a42e52f6aa73a31e88a299ec099a1b0cc7b",
        "title": "UBG: An Unreal BattleGround Benchmark With Object-Aware Hierarchical Proximal Policy Optimization"
      },
      {
        "paperId": "96fab8486200d5ec068c9430b90ecd486163ad9e",
        "title": "Augmenting Online RL with Offline Data is All You Need: A Unified Hybrid RL Algorithm Design and Analysis"
      },
      {
        "paperId": "962eedb6cf3bd41f6481eb35030f7a2046521e4e",
        "title": "Improving the Data-efficiency of Reinforcement Learning by Warm-starting with LLM"
      },
      {
        "paperId": "fb2d335b7260a3c3de372d781cae83f6c3df2eec",
        "title": "Behavior Cloning based Deep Reinforcement Learning for Control of Grid-Connected Photovoltaic Inverter"
      },
      {
        "paperId": "33a7bb26825560a0b13c955152ff14f27979c026",
        "title": "VLM Q-Learning: Aligning Vision-Language Models for Interactive Decision-Making"
      },
      {
        "paperId": "66385540204950877aa44c3be43cf2a567d422db",
        "title": "Dynamic Action Interpolation: A Universal Approach for Accelerating Reinforcement Learning with Expert Guidance"
      },
      {
        "paperId": "f0870be65f480a57d28f792f89246cae90431a5f",
        "title": "Improving RL Exploration for LLM Reasoning through Retrospective Replay"
      },
      {
        "paperId": "7cf3f86186c5c3e63c62eba57597cd85cf8e5344",
        "title": "Deep imitative reinforcement learning with gradient conflict-free for decision-making in autonomous vehicles"
      },
      {
        "paperId": "d1b25f2404ed9f8eb20fed834e20e111565fed2a",
        "title": "CONTHER: Human-Like Contextual Robot Learning via Hindsight Experience Replay and Transformers without Expert Demonstrations"
      },
      {
        "paperId": "b7de4a78bca27a80381b0bca047aed18d3b32dfe",
        "title": "CCDP: Composition of Conditional Diffusion Policies with Guided Sampling"
      },
      {
        "paperId": "1f17175b5321c318699198a1b2e02d84ea137869",
        "title": "Reinforcement learning-based motion imitation for physiologically plausible musculoskeletal motor control"
      },
      {
        "paperId": "462396a1cbef6a3b71fed11fd0e2da7c6d877984",
        "title": "Active Robot Curriculum Learning from Online Human Demonstrations"
      },
      {
        "paperId": "fddd061ddfd54176cb7ba3f8bf2fa69d02330333",
        "title": "Diffusion Stabilizer Policy for Automated Surgical Robot Manipulations"
      },
      {
        "paperId": "33f24a2ac0248a72627b1ea42651e5b0700f34b6",
        "title": "Multi-Stage Manipulation with Demonstration-Augmented Reward, Policy, and World Model Learning"
      },
      {
        "paperId": "ab37bdd10e7a892bd64c9a416e7acfca807d19af",
        "title": "Deep Reinforcement Learning Based Intelligent Resource Allocation in Hybrid Vehicle Scenario"
      },
      {
        "paperId": "02dfba49e62d9f1d796e0eaa3500bb51bf5e657a",
        "title": "Physics-Driven Data Generation for Contact-Rich Manipulation via Trajectory Optimization"
      },
      {
        "paperId": "6834fe5dc02d368cb543b169dddee64543ce2950",
        "title": "Safe Multi-Agent Navigation Guided by Goal-Conditioned Safe Reinforcement Learning"
      },
      {
        "paperId": "0541e064cce0248543203945429abb4562546c85",
        "title": "NatSGLD: A Dataset with Speech, Gesture, Logic, and Demonstration for Robot Learning in Natural Human-Robot Interaction"
      },
      {
        "paperId": "5d4e1169a52f16a86e81760f860b991a41cbe3ee",
        "title": "Compliant Motion Planning Integrating Human Skill for Robotic Arm Collecting Tomato Bunch Based on Improved DDPG"
      },
      {
        "paperId": "19b460d0065662f331c06ea05dbe8218d277e346",
        "title": "MILE: Model-Based Intervention Learning"
      },
      {
        "paperId": "c0b68738f3e46e7b9dacbe566f0c31104a78b616",
        "title": "Active Advantage-Aligned Online Reinforcement Learning with Offline Data"
      },
      {
        "paperId": "84fc714f22535f82c3fbfa28f2883292d2a02167",
        "title": "ConRFT: A Reinforced Fine-tuning Method for VLA Models via Consistency Policy"
      },
      {
        "paperId": "c5c5a0d4cfe702e2c432789bb0034efdec7af678",
        "title": "Learning from Suboptimal Data in Continuous Control via Auto-Regressive Soft Q-Network"
      },
      {
        "paperId": "156706de01ccb16512c1794bf73823c62d5838e7",
        "title": "Reinforcement learning based automatic block decomposition of solid models for hexahedral meshing"
      },
      {
        "paperId": "c3cd8042268a35436a1e1cbbe745fd2da138d873",
        "title": "RbRL2.0: Integrated Reward and Policy Learning for Rating-based Reinforcement Learning"
      },
      {
        "paperId": "4827ccb844c5e29e5b5316449aa4e65e8f38f6e3",
        "title": "Federated Reinforcement Learning for smart and privacy-preserving energy management of residential microgrids clusters"
      },
      {
        "paperId": "f99a6202c2d26c5b97ea89a177ca1fc7fbc052c9",
        "title": "Safety Decision-Making for Autonomous Vehicles Integrating Passenger Physiological States by fNIRS"
      },
      {
        "paperId": "47ebee2dfc357b44ff78933eb76750e1a8703a59",
        "title": "AAM-SEALS: Developing Aerial-Aquatic Manipulators in SEa, Air, and Land Simulator"
      },
      {
        "paperId": "6ef32591af268d810c3e9272921803d1f4098467",
        "title": "When Can Proxies Improve the Sample Complexity of Preference Learning?"
      },
      {
        "paperId": "3a2e880d36fbca0fdcfb6adc1a98cc7f12319931",
        "title": "SORREL: Suboptimal-Demonstration-Guided Reinforcement Learning for Learning to Branch"
      },
      {
        "paperId": "466b7a6e2b12330b6bb23f4af64732b4e77ca794",
        "title": "Policy Decorator: Model-Agnostic Online Refinement for Large Policy Model"
      },
      {
        "paperId": "aeb1f4ab2160135fc4ff32e8cae20d23e31234ac",
        "title": "Marvel: Accelerating Safe Online Reinforcement Learning with Finetuned Offline Policy"
      },
      {
        "paperId": "150faac21853b10fc54f4698d1d872a15da422f1",
        "title": "Dense Dynamics-Aware Reward Synthesis: Integrating Prior Experience with Demonstrations"
      },
      {
        "paperId": "131bce9acaebcc3ecaad3e3252898e50d1606ae0",
        "title": "Reinforcement Learning Approaches In Open-Ended Environments"
      },
      {
        "paperId": "a05a03089e1f8b7aa7dd0e2d140ca03dd04dcc56",
        "title": "AMAGO-2: Breaking the Multi-Task Barrier in Meta-Reinforcement Learning with Transformers"
      },
      {
        "paperId": "8431320f50a398d5ae6ad8158e374342f48fbb63",
        "title": "Learning World Models for Unconstrained Goal Navigation"
      },
      {
        "paperId": "16f525fc8c4bf4f5a6dab4c74e019e5a9d0d9d57",
        "title": "SoftCTRL: Soft conservative KL-control of Transformer Reinforcement Learning for Autonomous Driving"
      },
      {
        "paperId": "e02478f97ed90766d87530b35647e55d1edf6f64",
        "title": "Robot Policy Learning with Temporal Optimal Transport Reward"
      },
      {
        "paperId": "6a07440cd5d56b5e6bcdaf36784748804cc8032c",
        "title": "Solving Minimum-Cost Reach Avoid using Reinforcement Learning"
      },
      {
        "paperId": "b2203c382daacadf88c02c0a635c466e777e4ff7",
        "title": "SAMG: Offline-to-Online Reinforcement Learning via State-Action-Conditional Offline Model Guidance"
      },
      {
        "paperId": "d89ddb912b9680df66ba6a2ff2f8eb3716dc44b5",
        "title": "SPIRE: Synergistic Planning, Imitation, and Reinforcement Learning for Long-Horizon Manipulation"
      },
      {
        "paperId": "acd370650c2bd09fd8f67abc3e87217e25ad7e82",
        "title": "D-MARL: A Dynamic Communication-Based Action Space Enhancement for Multi Agent Reinforcement Learning Exploration of Large Scale Unknown Environments"
      },
      {
        "paperId": "53615928032912900a93b4825f04bf1b549e1fbf",
        "title": "Cross-Observability Learning for Vehicle Routing Problems"
      },
      {
        "paperId": "123d8f72a57e7f956d383de500d5abf32380fd68",
        "title": "Reinforcement Learning From Imperfect Corrective Actions And Proxy Rewards"
      },
      {
        "paperId": "a7a335450bb864f66bc5263bd79afbc435d12bdf",
        "title": "Unsupervised Skill Discovery for Robotic Manipulation through Automatic Task Generation"
      },
      {
        "paperId": "56b5ff3972351528deea16ce7c5d07a7a895fc58",
        "title": "Real-time power scheduling through reinforcement learning from demonstrations"
      },
      {
        "paperId": "6ba90cbe89ef4e6bc547f9f461359570a6f4f32c",
        "title": "Autonomous Driving at Unsignalized Intersections: A Review of Decision-Making Challenges and Reinforcement Learning-Based Solutions"
      },
      {
        "paperId": "a64a2db9cf5bf27b5c7cc0da0c04bdc3f043c54f",
        "title": "Demo2Test: Transfer Testing of Agent in Competitive Environment with Failure Demonstrations"
      },
      {
        "paperId": "1696e9ba819e67ae9f549b7f41ec06df98dd16d6",
        "title": "DemoStart: Demonstration-Led Auto-Curriculum Applied to Sim-to-Real with Multi-Fingered Robots"
      },
      {
        "paperId": "843d8906ab712b14fb4590404c939d3319aa98e6",
        "title": "Goal-Reaching Policy Learning from Non-Expert Observations via Effective Subgoal Guidance"
      },
      {
        "paperId": "ca09eae7d0ef0a74ff7ec75c0bc7918c2a891c3e",
        "title": "Surgical Task Automation Using Actor-Critic Frameworks and Self-Supervised Imitation Learning"
      },
      {
        "paperId": "04bf8de46df777e50cfcec6209178d4dece7bea4",
        "title": "Accelerating Virtual Fixture Estimation for Robot Manipulation using Reinforcement Learning and Human Demonstrations"
      },
      {
        "paperId": "342a3e1b9bc9c09fc549efd44bbe825048792901",
        "title": "The Evolution of Reinforcement Learning in Quantitative Finance: A Survey"
      },
      {
        "paperId": "374b00384c80b76d0d1134ca1b5bd3c24a1134a4",
        "title": "PLANRL: A Motion Planning and Imitation Learning Framework to Bootstrap Reinforcement Learning"
      },
      {
        "paperId": "b57959aa56bbd089333215641c6867023471a2e4",
        "title": "Reinforcement Learning-Based Skill Acquisition Study for Linearly Heated Robots"
      },
      {
        "paperId": "93b41399a5d733db856ade26d9947d4c49b77850",
        "title": "Jacta: A Versatile Planner for Learning Dexterous and Whole-body Manipulation"
      },
      {
        "paperId": "bb16d52a23c9bd680c444c5bd132c769c930bbf6",
        "title": "SynthRL: Cross-domain Synthesizer Sound Matching via Reinforcement Learning"
      },
      {
        "paperId": "d8f75adfc48528833a6fc5fa0dcc6e856016a243",
        "title": "From Imitation to Refinement - Residual Rl for Precise Assembly"
      },
      {
        "paperId": "06ca5fab665b49f44ed697b003691552d25270cd",
        "title": "Skill enhancement learning with knowledge distillation"
      },
      {
        "paperId": "e3ec6fa351268f017709b7ab869c21ad7711559c",
        "title": "Rocket Landing Control with Random Annealing Jump Start Reinforcement Learning"
      },
      {
        "paperId": "a7df1639acd0bddadbfa94764bb4d8006fe46ce0",
        "title": "Obstacle-free Trajectory Planning of an Uncertain Space Manipulator: Learning from a Fixed-Based Manipulator"
      },
      {
        "paperId": "d1f3bd0da1be39c57d24c445a265ff12c75b3c76",
        "title": "Efficient Imitation Without Demonstrations via Value-Penalized Auxiliary Control from Examples"
      },
      {
        "paperId": "656fd716acf2196c1b22bd9007f54fab4a999a8d",
        "title": "Model Based Reinforcement Learning Pre-Trained with Various State Data"
      },
      {
        "paperId": "f344e82d0d4f5e97c3d6176fd4b953a8c6c72d45",
        "title": "SurgicAI: A Hierarchical Platform for Fine-Grained Surgical Policy Learning and Benchmarking"
      },
      {
        "paperId": "5dce7116746cc0f5b09f5b27d3ea42caa27a1120",
        "title": "DIPPER: Direct Preference Optimization to Accelerate Primitive-Enabled Hierarchical Reinforcement Learning"
      },
      {
        "paperId": "6cdc80906656f3044de9759a58666bbe0b7b4129",
        "title": "Exploring demonstration pre-training with improved Deep Q-learning"
      },
      {
        "paperId": "93fd8d387432002238822a4e551515af1ce34e6f",
        "title": "LGR2: Language Guided Reward Relabeling for Accelerating Hierarchical Reinforcement Learning"
      },
      {
        "paperId": "ed7d213de959004feab13b1f713a6116a9dfa320",
        "title": "ATraDiff: Accelerating Online Reinforcement Learning with Imaginary Trajectories"
      },
      {
        "paperId": "1a4238a1dd6068f59b3daa8923a6612a2f98cd12",
        "title": "DEER: A Delay-Resilient Framework for Reinforcement Learning with Variable Delays"
      },
      {
        "paperId": "9ac3f4e74e9837cb47f43211cf143d4c7115e7f1",
        "title": "``Give Me an Example Like This'': Episodic Active Reinforcement Learning from Demonstrations"
      },
      {
        "paperId": "67a5bea98fa51759b43daf508b27b11aecbf6eb7",
        "title": "Outperforming the tutor: Expert-infused deep reinforcement learning for dynamic portfolio selection of diverse assets"
      },
      {
        "paperId": "cf5610741bb071e46f4b1fb301462748e29ec059",
        "title": "Hierarchical reinforcement learning from imperfect demonstrations through reachable coverage-based subgoal filtering"
      },
      {
        "paperId": "430c28e14b9303268ef64bfbe3f0b24f4846f32f",
        "title": "Learning from Random Demonstrations: Offline Reinforcement Learning with Importance-Sampled Diffusion Models"
      },
      {
        "paperId": "9ca54942f9c99c528aeaae7f220e3e970d2744e8",
        "title": "VICtoR: Learning Hierarchical Vision-Instruction Correlation Rewards for Long-horizon Manipulation"
      },
      {
        "paperId": "34d30f8997ea244204cc07d83dca9468a2005cc9",
        "title": "Models That Prove Their Own Correctness"
      },
      {
        "paperId": "dc7f6c4b42c4c639d62c8de5a3d228d155bf84fe",
        "title": "Inverse-RLignment: Inverse Reinforcement Learning from Demonstrations for LLM Alignment"
      },
      {
        "paperId": "f2916fb464578fb8944a3dff5a3f00b7539ccacb",
        "title": "UAV Swarm Cooperative Dynamic Target Search: A MAPPO-Based Discrete Optimal Control Method"
      },
      {
        "paperId": "015562270ff19914fec984aa2a653808ffc95150",
        "title": "Composable Interaction Primitives: A Structured Policy Class for Efficiently Learning Sustained-Contact Manipulation Skills"
      },
      {
        "paperId": "feb6f6a2f7de192baf2ae67e6cdb40c20f530393",
        "title": "Multi-objective Cross-task Learning via Goal-conditioned GPT-based Decision Transformers for Surgical Robot Task Automation"
      },
      {
        "paperId": "7fb8fcf9368a985e9c1338527a6f0d83b411b72d",
        "title": "Improving Offline Reinforcement Learning with Inaccurate Simulators"
      },
      {
        "paperId": "0fecc3a349cf4bc0f430cb8b9a69e8540ee10905",
        "title": "Reverse Forward Curriculum Learning for Extreme Sample and Demonstration Efficiency in Reinforcement Learning"
      },
      {
        "paperId": "1282382ba1570c306fb7a3f6b82f4b57fd93f28d",
        "title": "Path planning of intelligent radar anti-jamming matrix based on Q-learning algorithm"
      },
      {
        "paperId": "449896dafde44a950fdf1bb1cc91a29a7979726e",
        "title": "Learning Robot Soccer from Egocentric Vision with Deep Reinforcement Learning"
      },
      {
        "paperId": "18c8c0934139544826b53ce4be35e59c16a32934",
        "title": "Accelerating Multi-step Sparse Reward Reinforcement Learning"
      },
      {
        "paperId": "db2c2e9286e8f07c9c0472961dae4ecedc9dab17",
        "title": "DiffuseLoco: Real-Time Legged Locomotion Control with Diffusion from Offline Datasets"
      },
      {
        "paperId": "6073196bd50b000571dbdfc46418304a4aff6591",
        "title": "Countering Reward Over-optimization in LLM with Demonstration-Guided Reinforcement Learning"
      },
      {
        "paperId": "d47e9ea7c7d29431ca05b1dd83079110a4f2014f",
        "title": "Extended residual learning with one-shot imitation learning for robotic assembly in semi-structured environment"
      },
      {
        "paperId": "6a472311da2e19f44e7999acc8d8aac4fd59b5a2",
        "title": "Distilling Privileged Information for Dubins Traveling Salesman Problems with Neighborhoods"
      },
      {
        "paperId": "5b3a3f599674d7bba97f5120bd8e5737be757333",
        "title": "PIPER: Primitive-Informed Preference-based Hierarchical Reinforcement Learning via Hindsight Relabeling"
      },
      {
        "paperId": "af649b7481dc27f88ca1f98193e9f73110a6bffe",
        "title": "Provable Interactive Learning with Hindsight Instruction Feedback"
      },
      {
        "paperId": "2389fafc2a97e13fa810c4014babe73bd886c06f",
        "title": "Offline Reinforcement Learning with Failure Under Sparse Reward Environments"
      },
      {
        "paperId": "f036c481cb86c96228c66e2e3e9778ba4730435c",
        "title": "Dataset Reset Policy Optimization for RLHF"
      },
      {
        "paperId": "d7645259883ff3fd3bed22bafc2cad1f89c207dd",
        "title": "Multi-Task Reinforcement Learning with Cost-based HTN Planning"
      },
      {
        "paperId": "ed8246ebe105c0b52441fb5e25efbc3260354605",
        "title": "Sequential Decision Making with Expert Demonstrations under Unobserved Heterogeneity"
      },
      {
        "paperId": "dfd427eddb4be1da98d0eb593b1fad221576b1cc",
        "title": "Learning Prehensile Dexterity by Imitating and Emulating State-Only Observations"
      },
      {
        "paperId": "033b96503d85082445fc724b91d5ab252934418f",
        "title": "Demonstration Guided Multi-Objective Reinforcement Learning"
      },
      {
        "paperId": "1e0babe1537739895b49b8d260325a7e0572024f",
        "title": "Learning from Demonstration Framework for Multi-Robot Systems Using Interaction Keypoints and Soft Actor-Critic Methods"
      },
      {
        "paperId": "516aabc7484f109280560be204cd5a800a86bf71",
        "title": "Deep Generative Adversarial Reinforcement Learning for Semi-Supervised Segmentation of Low-Contrast and Small Objects in Medical Images"
      },
      {
        "paperId": "4186f74da6d793c91c5ca4d8a10cf2f8638eade6",
        "title": "RLfOLD: Reinforcement Learning from Online Demonstrations in Urban Autonomous Driving"
      },
      {
        "paperId": "2852426a094360457a5aef545c53d0f9d5a3732f",
        "title": "Supervised Fine-Tuning as Inverse Reinforcement Learning"
      },
      {
        "paperId": "000429c966f527613c700947a0912b10ce3f359d",
        "title": "Grasp Anything: Combining Teacher-Augmented Policy Gradient Learning with Instance Segmentation to Grasp Arbitrary Objects"
      },
      {
        "paperId": "4cbf53465e2af3aa4f0079167a0474fc713f3ce0",
        "title": "RLingua: Improving Reinforcement Learning Sample Efficiency in Robotic Manipulations With Large Language Models"
      },
      {
        "paperId": "400abf91d03681d27dfbe6263928e6d55664d914",
        "title": "Reconciling Reality through Simulation: A Real-to-Sim-to-Real Approach for Robust Manipulation"
      },
      {
        "paperId": "8965da806f1cbe46190cbf34e60a81613807806a",
        "title": "Blockchain-Assisted Demonstration Cloning for Multiagent Deep Reinforcement Learning"
      },
      {
        "paperId": "6a8d04f7f47954f72944db4b37652352857e5ef8",
        "title": "Snapshot Reinforcement Learning: Leveraging Prior Trajectories for Efficiency"
      },
      {
        "paperId": "1d759e71640b235d7252af45d7ce3ca4f4be65d2",
        "title": "Enhancing Reinforcement Learning Agents with Local Guides"
      },
      {
        "paperId": "38c728d05bdf75553f44efcb95883bbf99fc7162",
        "title": "Single-Reset Divide & Conquer Imitation Learning"
      },
      {
        "paperId": "7d6168fbd3ed72f9098573007f4b8c2ec9e576b9",
        "title": "Training Large Language Models for Reasoning through Reverse Curriculum Reinforcement Learning"
      },
      {
        "paperId": "fef99d99507a8fcad9e211bd9aade1ced0f97b41",
        "title": "R\u00d7R: Rapid eXploration for Reinforcement Learning via Sampling-based Reset Distributions and Imitation Pre-training"
      },
      {
        "paperId": "65d9de7b4cd8ead2c8ad8e0ae84ee7cfa2b54115",
        "title": "RESPRECT: Speeding-up Multi-Fingered Grasping With Residual Reinforcement Learning"
      },
      {
        "paperId": "ae6e0fdaf5088b0c588e4dba91b17fdc6f6ba6ac",
        "title": "Introducing PetriRL: An Innovative Framework for JSSP Resolution Integrating Petri nets and Event-based Reinforcement Learning"
      },
      {
        "paperId": "30c6f6db70338ededf8663b749020adb94737080",
        "title": "Human as AI Mentor: Enhanced Human-in-the-loop Reinforcement Learning for Safe and Efficient Autonomous Driving"
      },
      {
        "paperId": "f49afbb0dbf654f4e311daa621db05a123faee58",
        "title": "Guiding Offline Reinforcement Learning Using a Safety Expert"
      },
      {
        "paperId": "c31dd05cd19402a64d03f721ea2a110c55a80d04",
        "title": "Embodied Human Activity Recognition"
      },
      {
        "paperId": "b6a4df2f937e562fd5fedced00938d76d3ba3828",
        "title": "Automatic enhancement of vascular configuration for self-healing concrete through reinforcement learning approach"
      },
      {
        "paperId": "80ef2a08c550407aa7732e73886a1ad5892cfdf5",
        "title": "Explicit-Implicit Subgoal Planning for Long-Horizon Tasks With Sparse Rewards"
      },
      {
        "paperId": "24a8f2b230883bdafbbc6560d6b37081c347d035",
        "title": "Deep Reinforcement Learning for Quantitative Trading"
      },
      {
        "paperId": "aa83690628456d1b197716f8d45f08f8f2d48ffc",
        "title": "Human-AI Collaboration in Real-World Complex Environment with Reinforcement Learning"
      },
      {
        "paperId": "a492d59aaddc35860fed62c7fdd8d967c923f820",
        "title": "Open-Source Reinforcement Learning Environments Implemented in MuJoCo with Franka Manipulator"
      },
      {
        "paperId": "0a4c5c39cd2ad9558ebe5bbb6c6ed9e3dfb62905",
        "title": "Human-Machine Teaming for UAVs: An Experimentation Platform"
      },
      {
        "paperId": "a891444122d12deadd6f023bd3c8db7aef04c835",
        "title": "Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation"
      },
      {
        "paperId": "ce5839f371fb230c155c8eaa1ca68d7965eb1f85",
        "title": "A Q-learning approach to the continuous control problem of robot inverted pendulum balancing"
      },
      {
        "paperId": "096013115cc78698a1305b8bbd7bdbadfab21b20",
        "title": "Adaptive algorithms for shaping behavior"
      },
      {
        "paperId": "1780eafb4e46a7684647019dbf77095ba0a37395",
        "title": "Exploring the Use of Deep Reinforcement Learning Algorithms for Wound-Approaching Trajectories in Robot-Assisted Minimally Invasive Surgery"
      },
      {
        "paperId": "c3bcd40df07b78eaf6713de957aeffb799da9152",
        "title": "Regularity as Intrinsic Reward for Free Play"
      },
      {
        "paperId": "927fde98aad557340799c9d41cc3a406e9d5a62b",
        "title": "Safely Learn to Fly Aircraft From Human: An Offline\u2013Online Reinforcement Learning Strategy and Its Application to Aircraft Stall Recovery"
      },
      {
        "paperId": "b7384bfd7a569db3f09ec31095b81841663f0fbd",
        "title": "SOZIL: Self-Optimal Zero-Shot Imitation Learning"
      },
      {
        "paperId": "65731c720d2575ced7fa5134d408cffdbfce37ca",
        "title": "Replay across Experiments: A Natural Extension of Off-Policy RL"
      },
      {
        "paperId": "738b8695e52756833aaf4b25cf230c44f87e6e0c",
        "title": "RLIF: Interactive Imitation Learning as Reinforcement Learning"
      },
      {
        "paperId": "7431639df325753ab7a8abd5734fd7430410da2e",
        "title": "RoboAuditor: Goal-Oriented Robotic System for Assessing Energy-intensive Indoor Appliance via Visual Language Models"
      },
      {
        "paperId": "f4f9dda3a53f078eac7878a89322450609d6df10",
        "title": "Signal Temporal Logic-Guided Apprenticeship Learning"
      },
      {
        "paperId": "05603c65d813105eb136e867abfc412ea1735cf0",
        "title": "Accelerating Exploration with Unlabeled Prior Data"
      },
      {
        "paperId": "00f776a588e13e4c9278e91f7c10c536f6dc8c2d",
        "title": "Imitation Bootstrapped Reinforcement Learning"
      },
      {
        "paperId": "eecaecff044faf713b0db36a918c94635637b9c7",
        "title": "Soft imitation reinforcement learning with value decomposition for portfolio management"
      },
      {
        "paperId": "230c29618749b1a33d33847b8cb8d370ec56e988",
        "title": "Enhanced Generalization Through Prioritization and Diversity in Self-Imitation Reinforcement Learning Over Procedural Environments with Sparse Rewards"
      },
      {
        "paperId": "6cc2b9d5e679e41306d05871e3e140ce357d378e",
        "title": "Embedding expert demonstrations into clustering buffer for effective deep reinforcement learning"
      },
      {
        "paperId": "57472831768428ad28533a57cac1e68dd18a8d9b",
        "title": "6-DoF Stability Field via Diffusion Models"
      },
      {
        "paperId": "c963a7be3da1165595866942cb17481a2a91c89a",
        "title": "Learning agility and adaptive legged locomotion via curricular hindsight reinforcement learning"
      },
      {
        "paperId": "990d36239a4f87a3c74fac4464ccd2888b741c75",
        "title": "Plan-Guided Reinforcement Learning for Whole-Body Manipulation"
      },
      {
        "paperId": "b6f4b18fcf03b3ebe51dd686ea6fc8138329ac25",
        "title": "Deep Reinforcement Learning With Explicit Context Representation"
      },
      {
        "paperId": "79de4f387abf91610870a54588bc10a100a4d0b1",
        "title": "Interpretable Imitation Learning with Symbolic Rewards"
      },
      {
        "paperId": "f782d5f569eaa4eb2d66e0cb95bdba0941dbb8b0",
        "title": "Reinforcement Learning in the Era of LLMs: What is Essential? What is needed? An RL Perspective on RLHF, Prompting, and Beyond"
      },
      {
        "paperId": "578c3a5e9afdb8d68e65b5796484460393062dcb",
        "title": "Subtask-masked curriculum learning for reinforcement learning with application to UAV maneuver decision-making"
      },
      {
        "paperId": "945bd17b6ca2e60db29d91e98ea6a3f86a5e478c",
        "title": "Augmentation Enables One-Shot Generalization in Learning from Demonstration for Contact-Rich Manipulation"
      },
      {
        "paperId": "d6a7fe545d88d412a19b8d30bc7caf7c7f20258a",
        "title": "Secondary crash mitigation controller after rear-end collisions using reinforcement learning"
      },
      {
        "paperId": "bac356bfbc54145d37f187dfd717d577abab541b",
        "title": "Deep-reinforcement-learning-based robot motion strategies for grabbing objects from human hands"
      },
      {
        "paperId": "673b233912697e61f680432191bd3cf182bfa92f",
        "title": "Imitation-Guided Multimodal Policy Generation from Behaviourally Diverse Demonstrations"
      },
      {
        "paperId": "e134bb9a135a6ef4c23d06b701a79db0e18d8296",
        "title": "Guided Reward Design in Continuous Reinforcement Learning for Quantum Control"
      },
      {
        "paperId": "266c7d38ebfdc4424020fd071348dece1fafffab",
        "title": "Feature-Space Reinforcement Learning for Robotic Manipulation"
      },
      {
        "paperId": "0fa5bb936ec4d99f118268c2f058d059adc4e0b5",
        "title": "C\u00b7ASE: Learning Conditional Adversarial Skill Embeddings for Physics-based Characters"
      },
      {
        "paperId": "99d5d92c9b281497d0982a2f6865eb4a98452bf8",
        "title": "Optimizing Crowd-Aware Multi-Agent Path Finding through Local Communication with Graph Neural Networks"
      },
      {
        "paperId": "9572fa00e594862f1704b90f9f218069cf83af30",
        "title": "One ACT Play: Single Demonstration Behavior Cloning with Action Chunking Transformers"
      },
      {
        "paperId": "cd391facabf5005419b79997b2ef8473644a8192",
        "title": "Query-Dependent Prompt Evaluation and Optimization with Offline Inverse RL"
      },
      {
        "paperId": "99e2b4e7c7d62292bd783a03db07abb51ce9b7b3",
        "title": "Hundreds Guide Millions: Adaptive Offline Reinforcement Learning With Expert Guidance"
      },
      {
        "paperId": "89931304bd4b7faf6ff8e3ff1579f7151938d15a",
        "title": "Autonomous Soft Tissue Retraction Using Demonstration-Guided Reinforcement Learning"
      },
      {
        "paperId": "612591e9c4c331ebe6feb2511f3685f055c85216",
        "title": "Automated Curriculum Reinforcement Learning in Path-finding Scenarios"
      },
      {
        "paperId": "29e5156bf13a2e75d7c368960516d833dcea2e50",
        "title": "An Anthropomorphic Framework for Learning-Based Visual Servoing to Reach Unseen Objects"
      },
      {
        "paperId": "517ccb9ac129af6e0fad9bd476f577bb5e66a140",
        "title": "Robotic Stacking of Irregular Objects with Load Position Identification and Compensation"
      },
      {
        "paperId": "b65cd9857c72c917eec19a0514cad9885ee7620a",
        "title": "Transformer Decoder-Based Enhanced Exploration Method to Alleviate Initial Exploration Problems in Reinforcement Learning"
      },
      {
        "paperId": "b1f843c14c8fed4395d84d621d7be7c958280a1f",
        "title": "FoX: Formation-aware exploration in multi-agent reinforcement learning"
      },
      {
        "paperId": "590431c109b9f24a25c1e8f0c79343fc795392d8",
        "title": "Improving Sample Efficiency of Multiagent Reinforcement Learning With Nonexpert Policy for Flocking Control"
      },
      {
        "paperId": "f2ab1cabe8b78bd5073d69ade6dc9b85eef5b535",
        "title": "Artificial intelligence in the autonomous navigation of endovascular interventions: a systematic review"
      },
      {
        "paperId": "ea5a46dcffe2f65290550fa6c87bbc153f648093",
        "title": "Multiagent Deep Reinforcement Learning With Demonstration Cloning for Target Localization"
      },
      {
        "paperId": "44b4a633e20f64f0aed51d2b1984c0f9bf1185c4",
        "title": "Multi-Agent Advisor Q-Learning (Extended Abstract)"
      },
      {
        "paperId": "4c1ae44f053a09a79c942b32cc0e0e2cd2cbef3b",
        "title": "Value-Informed Skill Chaining for Policy Learning of Long-Horizon Tasks with Surgical Robot"
      },
      {
        "paperId": "8c6f5575e0959dbf1e598a31b1f0536eae693554",
        "title": "Using Implicit Behavior Cloning and Dynamic Movement Primitive to Facilitate Reinforcement Learning for Robot Motion Planning"
      },
      {
        "paperId": "8ba93052c60a266b31e121fd06e8ce9cbd9b1bc0",
        "title": "The SocialAI School: Insights from Developmental Psychology Towards Artificial Socio-Cultural Agents"
      },
      {
        "paperId": "e533815a25c4172cd27bfeac2ba627f49530dfcc",
        "title": "Application of Reinforcement Learning to UR10 Positioning for Prioritized Multi-Step Inspection in NVIDIA Omniverse"
      },
      {
        "paperId": "c7d37504b150d59c7d7193be7560df442fecca74",
        "title": "Budgeting Counterfactual for Offline RL"
      },
      {
        "paperId": "015e7c959715ca1062843979e898a19c47ac916b",
        "title": "Sequential Neural Barriers for Scalable Dynamic Obstacle Avoidance"
      },
      {
        "paperId": "f0f65fd4b61a2b9eebadc24a5472ce27d405ca3f",
        "title": "Autonomous target tracking of multi-UAV: A two-stage deep reinforcement learning approach with expert experience"
      },
      {
        "paperId": "aa62cb25f20bf38fb4d1787ab29d1bd1136a5684",
        "title": "A deep multi-agent reinforcement learning framework for autonomous aerial navigation to grasping points on loads"
      },
      {
        "paperId": "bf518e5b5f8e10babed4479dc0d1f1c79e77699b",
        "title": "Action and Trajectory Planning for Urban Autonomous Driving with Hierarchical Reinforcement Learning"
      },
      {
        "paperId": "71bee9c699cad8649c3cfa61a6e2e53ea5129058",
        "title": "Personified multi-ship collision avoidance navigation on knowledge distillation"
      },
      {
        "paperId": "a497bace44792275c78e52fc158c81e3c15e93ef",
        "title": "Learning to Generate Better Than Your LLM"
      },
      {
        "paperId": "6205098f7f6103d76aca8358e1f6dd5d7fde029f",
        "title": "Reinforcement Learning in Robotic Motion Planning by Combined Experience-based Planning and Self-Imitation Learning"
      },
      {
        "paperId": "6046a423a059cea4c4b4382f21b6123cbe6ddf69",
        "title": "PEAR: Primitive enabled Adaptive Relabeling for boosting Hierarchical Reinforcement Learning"
      },
      {
        "paperId": "7d77396bb8ab048a55a48be795d1a810ff2bb9c1",
        "title": "Confidence-Controlled Exploration: Efficient Sparse-Reward Policy Learning for Robot Navigation"
      },
      {
        "paperId": "4bf9975b0a33fbaf102af9d7559a736f460b6184",
        "title": "A Grasp Pose is All You Need: Learning Multi-Fingered Grasping with Deep Reinforcement Learning from Vision and Touch"
      },
      {
        "paperId": "60128ea6bd7fa6cd72099d1682ccf33d5b47b3c6",
        "title": "Boosting Offline Reinforcement Learning with Action Preference Query"
      },
      {
        "paperId": "dde533b729a0355f64a44d4f51d45a63358230e4",
        "title": "Efficient Lane-changing Behavior Planning via Reinforcement Learning with Imitation Learning Initialization"
      },
      {
        "paperId": "b626167360b31b6d5d84d339135ce39db798774a",
        "title": "CSGP: Closed-Loop Safe Grasp Planning via Attention-Based Deep Reinforcement Learning From Demonstrations"
      },
      {
        "paperId": "eb6f1d3488b5c27f47425fcd694399e704bc402e",
        "title": "Dealing with Sparse Rewards in Continuous Control Robotics via Heavy-Tailed Policy Optimization"
      },
      {
        "paperId": "86499dc6939a0ed3682d5845d29202c03d97072b",
        "title": "Learning Deposition Policies for Fused Multi-Material 3D Printing"
      },
      {
        "paperId": "d3319d9e5705de57e7dd50d10832fa2d16d49dc9",
        "title": "Using Learning Curve Predictions to Learn from Incorrect Feedback"
      },
      {
        "paperId": "78e1d49db2867299942fbd0a93a2bed016608689",
        "title": "Adaptive action supervision in reinforcement learning from real-world multi-agent demonstrations"
      },
      {
        "paperId": "0f5e5d2e14279137c59cf7a23b3bfed8405bbcda",
        "title": "Reward-agnostic Fine-tuning: Provable Statistical Benefits of Hybrid Reinforcement Learning"
      },
      {
        "paperId": "d4cb61a81ee3183be66ae73da2dbd0cde3b8445b",
        "title": "Reinforcement Learning with Reward Shaping and Hybrid Exploration in Sparse Reward Scenes"
      },
      {
        "paperId": "2f40434d9bac62088118b8ff26d946ac6009b0ec",
        "title": "A reinforcement learning algorithm acquires demonstration from the training agent by dividing the task space"
      },
      {
        "paperId": "d64c40b2a0a4aa46cc04ae3355a6b56cae5fda1b",
        "title": "RESPONSIVITAS PEMERINTAH MELALUI ELEKTRONIK GOVERNMENT (E-GOV) DI KOTA MAKASSAR"
      },
      {
        "paperId": "06f7c730009a9cf5975350bceefa7987441be6bf",
        "title": "Aiding reinforcement learning for set point control"
      },
      {
        "paperId": "05e3dd7135500150a38a362bece1d42c5b4d5740",
        "title": "Learning and Adapting Agile Locomotion Skills by Transferring Experience"
      },
      {
        "paperId": "f811132d3a737ee1c71b52706f0cd78b8904f056",
        "title": "Learning Diverse Policies with Soft Self-Generated Guidance"
      },
      {
        "paperId": "33faa621880be8690f92848379c9bbdf46a8a5c0",
        "title": "Sample-Efficient Reinforcement Learning with Symmetry-Guided Demonstrations for Robotic Manipulation"
      },
      {
        "paperId": "75763147a1b13c875d564b567a9e0d1f7c8df94f",
        "title": "Adaptive Hybrid Optimization Learning-Based Accurate Motion Planning of Multi-Joint Arm"
      },
      {
        "paperId": "d46b3680bfc1811c5cdf5d8d2336b9dedda67a37",
        "title": "CRISP: Curriculum Inducing Primitive Informed Subgoal Prediction for Hierarchical Reinforcement Learning"
      },
      {
        "paperId": "72a6983126822e20973ab1774e19c69dd7359a2a",
        "title": "TAG: Teacher-Advice Mechanism With Gaussian Process for Reinforcement Learning"
      },
      {
        "paperId": "ceaedeaabc9a8be28f9f4f9cf277c26dd47b6744",
        "title": "Constrained Exploration in Reinforcement Learning with Optimality Preservation"
      },
      {
        "paperId": "aee25ec357bfb8b4422e04630bc46c8f5a03a695",
        "title": "A Survey on Deep Reinforcement Learning Algorithms for Robotic Manipulation"
      },
      {
        "paperId": "1830d2d493f0fa219254e125f1b46e2f20597c12",
        "title": "End-to-end deep learning-based framework for path planning and collision checking: bin-picking application"
      },
      {
        "paperId": "ae95f453398024df8b5811da7af44257ba5f96d0",
        "title": "Learning Complicated Manipulation Skills Via Deterministic Policy with Limited Demonstrations"
      },
      {
        "paperId": "377f8a1eb52568062fc6460e44d7bfea4ce2ab78",
        "title": "Modeling a Conversational Agent using BDI Framework"
      },
      {
        "paperId": "5045c2a64a4ea41d8da9ee8eb279498db1ff3d78",
        "title": "Planning Goals for Exploration"
      },
      {
        "paperId": "18bc10da4a1162da1baf2e0a09e97f486a342423",
        "title": "Boosting Reinforcement Learning and Planning with Demonstrations: A Survey"
      },
      {
        "paperId": "3721c70ed660a73592ae4adad46de5cba6abd63d",
        "title": "Information-Directed Policy Search in Sparse-Reward Settings via the Occupancy Information Ratio"
      },
      {
        "paperId": "60acb595049c3c364f1c809ed958cb035422c3b9",
        "title": "Recent Advances of Deep Robotic Affordance Learning: A Reinforcement Learning Perspective"
      },
      {
        "paperId": "99435b8d230ee465c3b6947e6f8b7aaa02477038",
        "title": "Sample-Efficient Real-Time Planning with Curiosity Cross-Entropy Method and Contrastive Learning"
      },
      {
        "paperId": "27d74d7ca8455fe6d068eec457302150173bec1d",
        "title": "End-to-End Learning of Deep Visuomotor Policy for Needle Picking"
      },
      {
        "paperId": "4eda9565531a1f6b2c320f3a3a6ca4a8a32cf722",
        "title": "Sampling-based Exploration for Reinforcement Learning of Dexterous Manipulation"
      },
      {
        "paperId": "86ab0829befe266f1e741fb0a2ee9a0bc6645e27",
        "title": "Seq2Seq Imitation Learning for Tactile Feedback-based Manipulation"
      },
      {
        "paperId": "99b17f5d5b0f3bd9ee1d7b0baca4e5c2e8a48438",
        "title": "Trajectory Generation for Space Manipulators Capturing Moving Targets Using Transfer Learning"
      },
      {
        "paperId": "18837de696bd7349fa3f315faf4fa4034a513104",
        "title": "Self-Improving Robots: End-to-End Autonomous Visuomotor Reinforcement Learning"
      },
      {
        "paperId": "eeff37d3d522583e75a516a12615fd356ca26682",
        "title": "Expert-Free Online Transfer Learning in Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "96a19cd83080d70e10f3e6fd5af327a1263f20d6",
        "title": "Teach a Robot to FISH: Versatile Imitation from One Minute of Demonstrations"
      },
      {
        "paperId": "dc744d27a13da837f332b542434a4bc877b0fd17",
        "title": "The Virtues of Laziness in Model-based RL: A Unified Objective and Algorithms"
      },
      {
        "paperId": "8fed1e5ab5fc485d2def1ef8e81c33e11ccd78bb",
        "title": "Deep multi-agent fusion Q-Network for graph generation"
      },
      {
        "paperId": "56d3f1c0e0be6ad9dc29750c2828199a80d59da2",
        "title": "A review of recent trend in motion planning of industrial robots"
      },
      {
        "paperId": "9e13ffe77bc1ae88dc9e89ddf6fdaa12f15c165f",
        "title": "Data-Driven Robotic Manipulation of Cloth-like Deformable Objects: The Present, Challenges and Future Prospects"
      },
      {
        "paperId": "03c7f9a6b743eb1c9127b8d4552147b2b1a0c1e1",
        "title": "Demonstration-Guided Reinforcement Learning with Efficient Exploration for Task Automation of Surgical Robot"
      },
      {
        "paperId": "4c3dc49abcdb1122472d15f1f8fde4bbabc3859f",
        "title": "Natural Language-conditioned Reinforcement Learning with Inside-out Task Language Development and Translation"
      },
      {
        "paperId": "bd38cbbb346a347cb5b60ac4a133b3d73cb44e07",
        "title": "Efficient Online Reinforcement Learning with Offline Data"
      },
      {
        "paperId": "7f270c9b098727675c9d8b893e362b561d61f27e",
        "title": "Policy Expansion for Bridging Offline-to-Online Reinforcement Learning"
      },
      {
        "paperId": "69fa746f5362dc65df435a1cd2807e97ec091122",
        "title": "QMP: Q-switch Mixture of Policies for Multi-Task Behavior Sharing"
      },
      {
        "paperId": "1b7bd0d241f3224135688386d41a50365eb3e6c5",
        "title": "HTTP adaptive streaming scheme based on reinforcement learning with edge computing assistance"
      },
      {
        "paperId": "6b48246a05ef0cbe05b1ce72f807a87fff9cc28e",
        "title": "Hu-bot: promoting the cooperation between humans and mobile robots"
      },
      {
        "paperId": "b530925ea809ae25be3e86fb5d17e3fa924f7ff6",
        "title": "Learning From Guided Play: Improving Exploration for Adversarial Imitation Learning With Simple Auxiliary Tasks"
      },
      {
        "paperId": "c91b5677ecb034a6a2ca69509b604965c776d5d6",
        "title": "On Pathologies in KL-Regularized Reinforcement Learning from Expert Demonstrations"
      },
      {
        "paperId": "ca137b4f6ba25ca2f26952044c6a2d06ae3e607f",
        "title": "Cross-Domain Transfer via Semantic Skill Imitation"
      },
      {
        "paperId": "b2004f4f19bc6ccae8e4afc554f39142870100f5",
        "title": "MoDem: Accelerating Visual Model-Based Reinforcement Learning with Demonstrations"
      },
      {
        "paperId": "5bb567f9b6ecdac86345495871636fde48adc410",
        "title": "Off-Policy Deep Reinforcement Learning Algorithms for Handling Various Robotic Manipulator Tasks"
      },
      {
        "paperId": "782c3235b6e9a3f9711274eedffa114792657f93",
        "title": "Model-based trajectory stitching for improved behavioural cloning and its applications"
      },
      {
        "paperId": "15017bb278faf92c3efe3152770ed50a00fcfd83",
        "title": "Deep Reinforcement Learning-Based Control for Stomach Coverage Scanning of Wireless Capsule Endoscopy"
      },
      {
        "paperId": "687a52b69e4c91d992a4b8e7dbbe7f333083d7d2",
        "title": "Redundant robot control with learning from expert demonstrations"
      },
      {
        "paperId": "0d709a92f5dfec905ded281d13958c8a1aa58ae5",
        "title": "Extended-Self Recognition for Autonomous Agent Based on Controllability and Predictability"
      },
      {
        "paperId": "f18015a05a834fc0068ad38b7bfdd6d2fbd8cb6b",
        "title": "Reinforcement learning with Demonstrations from Mismatched Task under Sparse Reward"
      },
      {
        "paperId": "9c841140a6208a82a8f0a3ddd38a3b29f567366e",
        "title": "Computationally Efficient Reinforcement Learning: Targeted Exploration leveraging Simple Rules"
      },
      {
        "paperId": "87361e44f253a3044b02b947c0a2370ff4f1e3e7",
        "title": "Towards Improving Exploration in Self-Imitation Learning using Intrinsic Motivation"
      },
      {
        "paperId": "b9ecf9fb3b2d5a70451fa30911142e55848fde2d",
        "title": "Model-based Trajectory Stitching for Improved Offline Reinforcement Learning"
      },
      {
        "paperId": "43523144783262f25528420763a9d05c3a593b65",
        "title": "Improving TD3-BC: Relaxed Policy Constraint for Offline Learning and Stable Online Fine-Tuning"
      },
      {
        "paperId": "f4536d6a8ee83759967251647fda2c6b07a23e41",
        "title": "Deep Reinforcement Learning Guided Improvement Heuristic for Job Shop Scheduling"
      },
      {
        "paperId": "de05478bbc0031904ccbeb6ef18726c5a40bb266",
        "title": "ToolFlowNet: Robotic Manipulation with Tools via Predicting Tool Flow from Point Clouds"
      },
      {
        "paperId": "6d7e11f7cb280df884e017002d2e4eb30985629e",
        "title": "Leveraging Offline Data in Online Reinforcement Learning"
      },
      {
        "paperId": "adb0ae3a3c581c3dda0f2f3eb6af316cc602d699",
        "title": "Behavior Prior Representation learning for Offline Reinforcement Learning"
      },
      {
        "paperId": "be0288847c13f3ef286524899bb95709eb1213e8",
        "title": "TD3 with Reverse KL Regularizer for Offline Reinforcement Learning from Mixed Datasets"
      },
      {
        "paperId": "80c0be3a092ca1ecd7bee20eafcc4f1c4e564738",
        "title": "Empowerment-driven Policy Gradient Learning with Counterfactual Augmentation in Recommender Systems"
      },
      {
        "paperId": "44002dc3b68b0ccb9c20759289e1451fad7fc49f",
        "title": "Reinforcement Learning for Solving Robotic Reaching Tasks in the Neurorobotics Platform"
      },
      {
        "paperId": "a541a27ec339b5a9ad98ce200d70c00aed582f27",
        "title": "A Method for High-Value Driving Demonstration Data Generation Based on One-Dimensional Deep Convolutional Generative Adversarial Networks"
      },
      {
        "paperId": "594cc7ce6690ad6d3dd79ea57391ab1bd4d41119",
        "title": "Goal Exploration Augmentation via Pre-trained Skills for Sparse-Reward Long-Horizon Goal-Conditioned Reinforcement Learning"
      },
      {
        "paperId": "8c3957874cc07a849238c6526d67505fb6ace59a",
        "title": "Heuristic Reward Function for Reinforcement Learning Based Manipulator Motion Planning"
      },
      {
        "paperId": "a3b0edd8866d0eca40ce47221de596e4c9d440af",
        "title": "D-Shape: Demonstration-Shaped Reinforcement Learning via Goal Conditioning"
      },
      {
        "paperId": "aa65704a16138790678e2b9b59ae679b6c9353d7",
        "title": "Knowledge-Guided Exploration in Deep Reinforcement Learning"
      },
      {
        "paperId": "fc4c08a07637634fe156da49a866d3a5c4afe4a6",
        "title": "LEAGUE: Guided Skill Learning and Abstraction for Long-Horizon Manipulation"
      },
      {
        "paperId": "bdb12ecda298e6b0cba82e4837d3dbab8bab52b8",
        "title": "Analyzing and Overcoming Degradation in Warm-Start Reinforcement Learning"
      },
      {
        "paperId": "57542739d3e2ccf29cd10bbce1fb9a3b291bfd57",
        "title": "Cut-and-Approximate: 3D Shape Reconstruction from Planar Cross-sections with Deep Reinforcement Learning"
      },
      {
        "paperId": "f7e6206f58261fb12dc104d80a5768443ded5a1a",
        "title": "Rate-Splitting for Intelligent Reflecting Surface-Aided Multiuser VR Streaming"
      },
      {
        "paperId": "4705fb99684f1bb0e35177be38acd3ab173c99cd",
        "title": "Research on Robotic Arm Based on YOLO"
      },
      {
        "paperId": "3ebac906c03bc55a1f95c38421ed4f561f36a506",
        "title": "Task Phasing: Automated Curriculum Learning from Demonstrations"
      },
      {
        "paperId": "1b9c5622b905a3da9733d2c306a87374101eb479",
        "title": "The Pump Scheduling Problem: A Real-World Scenario for Reinforcement Learning"
      },
      {
        "paperId": "4c09d6969f451d288d8a188aa7e48a2af38d1911",
        "title": "Planning for Sample Efficient Imitation Learning"
      },
      {
        "paperId": "e199e494ca2006a350c51cecfd70659d55e14675",
        "title": "You Only Live Once: Single-Life Reinforcement Learning"
      },
      {
        "paperId": "4532aa9a2d51e4568f7fe9f2b54e2d4867622c5a",
        "title": "Robust Imitation of a Few Demonstrations with a Backwards Model"
      },
      {
        "paperId": "bccf155b51c1da59e891c494521643e1657a08f6",
        "title": "Guided Deep Reinforcement Learning based on RBF-ARX Pseudo LQR in Single Stage Inverted Pendulum"
      },
      {
        "paperId": "ad4707bb87c6fc087e09d9f6609665b53835c899",
        "title": "Learning Skills From Demonstrations: A Trend From Motion Primitives to Experience Abstraction"
      },
      {
        "paperId": "a1e7be62296af87f5a5786c313d22fa9af143bcd",
        "title": "Monte Carlo Augmented Actor-Critic for Sparse Reward Deep Reinforcement Learning from Suboptimal Demonstrations"
      },
      {
        "paperId": "2d33f309f7e92c75434a2bb16f70d6ec65ab7d2a",
        "title": "Hybrid RL: Using Both Offline and Online Data Can Make RL Efficient"
      },
      {
        "paperId": "382f6bd017c4ee1c543febdf2bfb267176596088",
        "title": "Augmentation for Learning From Demonstration with Environmental Constraints"
      },
      {
        "paperId": "7842368bf1afde87deb63333871760ae848f01f9",
        "title": "Mastering the Game of No-Press Diplomacy via Human-Regularized Reinforcement Learning and Planning"
      },
      {
        "paperId": "c44fb1f8e7e66c220f136178f3a92a2a5b837c5d",
        "title": "Reinforcement Learning to Efficiently Recover Control Performance of Robots Using Imitation Learning After Failure"
      },
      {
        "paperId": "579cd3b80cb633fd23ff6d6b25dd754a197b3e9e",
        "title": "Flexible Attention-Based Multi-Policy Fusion for Efficient Deep Reinforcement Learning"
      },
      {
        "paperId": "96776d78267b3a366aa37ebe4765050c1a55e0fd",
        "title": "Handling Sparse Rewards in Reinforcement Learning Using Model Predictive Control"
      },
      {
        "paperId": "0f5063db98a8758126503a1e28ce2bfd758b12d5",
        "title": "Learning Depth Vision-Based Personalized Robot Navigation From Dynamic Demonstrations in Virtual Reality"
      },
      {
        "paperId": "cd9d7ebdd6ac590400271b5e903e730df1f9f671",
        "title": "Bayesian Q-learning With Imperfect Expert Demonstrations"
      },
      {
        "paperId": "3fbe2e8413df0207c26ff393c9aaa8488e3ca4c3",
        "title": "VIP: Towards Universal Visual Reward and Representation via Value-Implicit Pre-Training"
      },
      {
        "paperId": "35f8eb09776abebad8a963b59978d673ab97301a",
        "title": "Understanding Hindsight Goal Relabeling from a Divergence Minimization Perspective"
      },
      {
        "paperId": "6bc761321b836c8eef8602ceb2357cb28807b564",
        "title": "Enhanced Meta Reinforcement Learning using Demonstrations in Sparse Reward Environments"
      },
      {
        "paperId": "a17a7256c04afee68f9aa0b7bfdc67fbca998b9c",
        "title": "Accelerating Reinforcement Learning for Autonomous Driving Using Task-Agnostic and Ego-Centric Motion Skills"
      },
      {
        "paperId": "d64aca6a2f9dc1ebd5c22f447969eda271e78f78",
        "title": "Minimizing Human Assistance: Augmenting a Single Demonstration for Deep Reinforcement Learning"
      },
      {
        "paperId": "951da1a5a0c2bfb6f2d16dccfc488acb71f1447f",
        "title": "An Open Tele-Impedance Framework to Generate Data for Contact-Rich Tasks in Robotic Manipulation"
      },
      {
        "paperId": "436e34f3fd0f0a99c0d449a896c004f0643f8e85",
        "title": "First-order Policy Optimization for Robust Markov Decision Process"
      },
      {
        "paperId": "5815a68809dbedb72515cf930f6011761c8d8ea8",
        "title": "Soft Action Priors: Towards Robust Policy Transfer"
      },
      {
        "paperId": "27b8919ddb25bb6ff5d83668688cc1d1b1fa44b5",
        "title": "Active Predicting Coding: Brain-Inspired Reinforcement Learning for Sparse Reward Robotic Control Problems"
      },
      {
        "paperId": "90855a4e85b920f7fb5cf99753c9452e42ff3c18",
        "title": "Intrinsically motivated reinforcement learning based recommendation with counterfactual data augmentation"
      },
      {
        "paperId": "ec0c9cdd961d0c07572e83d16843a5c8ce95e3ec",
        "title": "Sub-optimal Policy Aided Multi-Agent Reinforcement Learning for Flocking Control"
      },
      {
        "paperId": "451ac09cff243176db6437fd92b1059a8b8e1e74",
        "title": "Sample-Efficient Multi-Agent Reinforcement Learning with Demonstrations for Flocking Control"
      },
      {
        "paperId": "35c756a9100a8ab4573f6fd8f3d91d20e074e3cd",
        "title": "Multi-State-Space Reasoning Reinforcement Learning for Long-Horizon RFID-Based Robotic Searching and Planning Tasks"
      },
      {
        "paperId": "91382df81f9de79824712f2739f3821fe4b74239",
        "title": "MetaTrader: An Reinforcement Learning Approach Integrating Diverse Policies for Portfolio Optimization"
      },
      {
        "paperId": "73b26f7da3a7a7c7f317dae3f74fda0780745477",
        "title": "Failed Goal Aware Hindsight Experience Replay"
      },
      {
        "paperId": "46bcc3a154611516fdf79118067c883e3b725b00",
        "title": "Deep Reinforcement Learning for Peg-in-hole Assembly Task Via Information Utilization Method"
      },
      {
        "paperId": "ec8d50285072082d608dd50e82d3fea701aa3788",
        "title": "A Sample Efficiency Improved Method via Hierarchical Reinforcement Learning Networks"
      },
      {
        "paperId": "57a752e11258cdfa658b8ab3ab87ad926f249c77",
        "title": "Turning Mathematics Problems into Games: Reinforcement Learning and Gr\u00f6bner bases together solve Integer Feasibility Problems"
      },
      {
        "paperId": "104213c5ead6ed90419aa230ee2f73ce8f793a5a",
        "title": "Some Supervision Required: Incorporating Oracle Policies in Reinforcement Learning via Epistemic Uncertainty Metrics"
      },
      {
        "paperId": "ae40437fb4733c9c4364475fb7b86e9c35d147dc",
        "title": "Multi-goal Reinforcement Learning via Exploring Successor Matching"
      },
      {
        "paperId": "40bb4f69c5fb8149b35d412e50771fd0503acf5e",
        "title": "Managing Shaping Complexity in Reinforcement Learning with State Machines - Using Robotic Tasks with Unspecified Repetition as an Example"
      },
      {
        "paperId": "916967189d9520cb5c0c2a0b52c5cfc0abb19a3f",
        "title": "Impact Makes a Sound and Sound Makes an Impact: Sound Guides Representations and Explorations"
      },
      {
        "paperId": "2a88d01f3079e68ad9b5bcb1ebe56da25679e331",
        "title": "Relay Hindsight Experience Replay: Self-guided continual reinforcement learning for sequential object manipulation tasks with sparse rewards"
      },
      {
        "paperId": "5ed51a8cca6f4da8ee4030c54ce5ca1c1b8701e9",
        "title": "Robot Policy Learning from Demonstration Using Advantage Weighting and Early Termination"
      },
      {
        "paperId": "c3b849ac59741497ca70e9ceabb9367b50b22f42",
        "title": "Improved Policy Optimization for Online Imitation Learning"
      },
      {
        "paperId": "37083bc97534bd6d361bb97ece4c7bdb09164943",
        "title": "Learning Deformable Object Manipulation From Expert Demonstrations"
      },
      {
        "paperId": "a95cfea5cc9ce2731beb4cd264efe8155b13cd3e",
        "title": "Abstract Demonstrations and Adaptive Exploration for Efficient and Stable Multi-step Sparse Reward Reinforcement Learning"
      },
      {
        "paperId": "4ad958b6ecd194632f697ea31dea4f0067950b31",
        "title": "Urban Traffic Signal Control with Reinforcement Learning from Demonstration Data"
      },
      {
        "paperId": "2b29032f899fbc6d5f1a761c5ac45b8629589900",
        "title": "Incorporating External Knowledge Reasoning for Vision-and-Language Navigation with Assistant\u2019s Help"
      },
      {
        "paperId": "0ac948bc087603176b47c4af29ef7f240d27b541",
        "title": "Don't Start From Scratch: Leveraging Prior Data to Automate Robotic Reinforcement Learning"
      },
      {
        "paperId": "87c7f02df2a61b4b71a0324d3b4597a232360c00",
        "title": "HTRON: Efficient Outdoor Navigation with Sparse Rewards via Heavy Tailed Adaptive Reinforce Algorithm"
      },
      {
        "paperId": "275a14fc8e5c929a6fcd85a155a2c50404776d7b",
        "title": "Domain Adapting Deep Reinforcement Learning for Real-World Speech Emotion Recognition"
      },
      {
        "paperId": "2d5edc038a177bed2f58e5374f38a411d8538f0b",
        "title": "Watch and Match: Supercharging Imitation with Regularized Optimal Transport"
      },
      {
        "paperId": "a94aaf192fc1d46d697e4d7eb3e999021ec88b46",
        "title": "Phasic Self-Imitative Reduction for Sparse-Reward Goal-Conditioned Reinforcement Learning"
      },
      {
        "paperId": "8b9ffb367f8670f4c736fbdb4349800c959bca39",
        "title": "Dealing with Sparse Rewards in Continuous Control Robotics via Heavy-Tailed Policies"
      },
      {
        "paperId": "94f7eca9f9ccffedf771288784c35c2678320483",
        "title": "Achieving Goals Using Reward Shaping and Curriculum Learning"
      },
      {
        "paperId": "5e86a1e80cd7a84a5ff316f59345f00c402bddb5",
        "title": "Reincarnating Reinforcement Learning: Reusing Prior Computation to Accelerate Progress"
      },
      {
        "paperId": "0412360a27a92f621c889f4ee2701c5e805e0d6e",
        "title": "Critic Sequential Monte Carlo"
      },
      {
        "paperId": "44df7090a9b18b19196018b5705dfc9153e068ed",
        "title": "BulletArm: An Open-Source Robotic Manipulation Benchmark and Learning Framework"
      },
      {
        "paperId": "82af29ad844d4cb5a58cde1cd9a9644cd7d14152",
        "title": "Tutorial on Course-of-Action (COA) Attack Search Methods in Computer Networks"
      },
      {
        "paperId": "2decff836d5a433fa917a1f9e37466a490c84abd",
        "title": "SFP: State-free Priors for Exploration in Off-Policy Reinforcement Learning"
      },
      {
        "paperId": "2a1300702c5e73d36a1b97e3db77e6f59dc459ab",
        "title": "From Model-Centric to Data-Centric AI: A Paradigm Shift or Rather a Complementary Approach?"
      },
      {
        "paperId": "7e781f77ba346662dcac2c7c1e976ed431a602c8",
        "title": "Efficient Reinforcement Learning from Demonstration Using Local Ensemble and Reparameterization with Split and Merge of Expert Policies"
      },
      {
        "paperId": "2c3b8644beb419f8363362c3be96db8a9fbc9fb5",
        "title": "Stable Object Reorientation using Contact Plane Registration"
      },
      {
        "paperId": "08a2ae4cfffa3decdc5bc9008859f3c53650e75d",
        "title": "A Safe Training Approach for Deep Reinforcement Learning-based Traffic Engineering"
      },
      {
        "paperId": "14903cf4df35936b8a5c1dfa638c59727be6112d",
        "title": "A State-Distribution Matching Approach to Non-Episodic Reinforcement Learning"
      },
      {
        "paperId": "ec5f3cad41691ec5fc12cb6a87f72cee5b4c3e5f",
        "title": "Learning intraoperative organ manipulation with context-based reinforcement learning"
      },
      {
        "paperId": "5f20881359f6433ced3b586e293ff1131d24f6e6",
        "title": "Counterfactual harm"
      },
      {
        "paperId": "3cd46314a0ad311ec108e5d604a31c43a4de22ec",
        "title": "AMBF-RL: A real-time simulation based Reinforcement Learning toolkit for Medical Robotics"
      },
      {
        "paperId": "ca8034b13394a07affd5da8170e33a93f6ae47a4",
        "title": "Learning Design and Construction with Varying-Sized Materials via Prioritized Memory Resets"
      },
      {
        "paperId": "f3a63a840185fa8c5e0db4bbe12afa7d3de7d029",
        "title": "Jump-Start Reinforcement Learning"
      },
      {
        "paperId": "db5b3c8e5659be8fa8a95c70e47d99ebd5c351cd",
        "title": "ESNI: Domestic Robots Design for Elderly and Disabled People"
      },
      {
        "paperId": "0c39ee9b544054a277b90da7cc4c3d16b220c965",
        "title": "Learning Personalized Human-Aware Robot Navigation Using Virtual Reality Demonstrations from a User Study"
      },
      {
        "paperId": "9f2ae2efaaef1e670455333e886e66ce209a57c6",
        "title": "LORM: a novel reinforcement learning framework for biped gait control"
      },
      {
        "paperId": "651468a69da74dab716cebbd179a5cbb8e672c14",
        "title": "Self-Imitation Learning from Demonstrations"
      },
      {
        "paperId": "784c1e1b02ac95b31189ac12d12a4fa909634052",
        "title": "Combining imitation and deep reinforcement learning to human-level performance on a virtual foraging task"
      },
      {
        "paperId": "0a4933c17a63bfbc1cbaf97b56bf9d25205fd9d4",
        "title": "Crash mitigation controller for unavoidable T-bone collisions using reinforcement learning."
      },
      {
        "paperId": "da1e404ae194dcbcce198f5335f49f090403bbd7",
        "title": "Exploration in Deep Reinforcement Learning: A Survey"
      },
      {
        "paperId": "5ee839d965a1596298895ace7d003b98e165962c",
        "title": "Exploring with Sticky Mittens: Reinforcement Learning with Expert Interventions via Option Templates"
      },
      {
        "paperId": "7f712d58084e32ddc1b0cd60932f8bc0a0916330",
        "title": "Rethinking Goal-conditioned Supervised Learning and Its Connection to Offline RL"
      },
      {
        "paperId": "00438218d81c2d50fc96592e16c07ae720440bb6",
        "title": "Reinforcement Learning with Sparse Rewards using Guidance from Offline Demonstration"
      },
      {
        "paperId": "cef96faa16c992042d8bcfbf8b763496c84b211d",
        "title": "DexVIP: Learning Dexterous Grasping with Human Hand Pose Priors from Video"
      },
      {
        "paperId": "28ddab3a41c6c7bf0b847695dddd0d65ef40a5ec",
        "title": "Overcoming Exploration: Deep Reinforcement Learning for Continuous Control in Cluttered Environments From Temporal Logic Specifications"
      },
      {
        "paperId": "d520a0fc90931ba021ee8f2ca4a5ae823db4fcf2",
        "title": "Learning Task-Parameterized Skills from Few Demonstrations"
      },
      {
        "paperId": "b1e30f99171e1727e34dc2e7625fd884aa5fcd29",
        "title": "Goal-Conditioned Reinforcement Learning: Problems and Solutions"
      },
      {
        "paperId": "660d660c56e120b4e36229e5531f0b38fbf3e0cd",
        "title": "Evolutionary Action Selection for Gradient-based Policy Learning"
      },
      {
        "paperId": "69951c75efb9ff615fb31eb4b6b42d4cd1ded58f",
        "title": "Reward Relabelling for combined Reinforcement and Imitation Learning on sparse-reward tasks"
      },
      {
        "paperId": "b5e124ace60793321b82ff4243f505e5cc2ca1b1",
        "title": "An Adaptive Imitation Learning Framework for Robotic Complex Contact-Rich Insertion Tasks"
      },
      {
        "paperId": "bb815fdbf1c2b2bf10ed22586fa5ddfbbcf91af5",
        "title": "Offline reinforcement learning with anderson acceleration for robotic tasks"
      },
      {
        "paperId": "d4a6b2a917773040030132371d8fd226f98baff0",
        "title": "HER-PDQN: A Reinforcement Learning Approach for UAV Navigation with Hybrid Action Spaces and Sparse Rewards"
      },
      {
        "paperId": "2e563300f1d359d7ffb6c244753725d5e83ee35c",
        "title": "Actor-critic reinforcement learning for bidding in bilateral negotiation"
      },
      {
        "paperId": "26d127834f54949f8b2b3555a4222ede21098377",
        "title": "An Underwater Soft Claw Based on Bionic Principle"
      },
      {
        "paperId": "9d2038e233042790929123ff62354eb18ee52e47",
        "title": "Learning from Guided Play: A Scheduled Hierarchical Approach for Improving Exploration in Adversarial Imitation Learning"
      },
      {
        "paperId": "d7bebb71635cb818d2f5e0ca0a70434283deb4b6",
        "title": "Modeling Strong and Human-Like Gameplay with KL-Regularized Search"
      },
      {
        "paperId": "51517d5d900a7c0cd33e210c48cb27ef3b96e5a9",
        "title": "JueWu-MC: Playing Minecraft with Sample-efficient Hierarchical Reinforcement Learning"
      },
      {
        "paperId": "e724029bebc6afc060c4a0dacb7231f71d48d5f7",
        "title": "Optimal Control Combining Emulation and Imitation to Acquire Physical Assistance Skills"
      },
      {
        "paperId": "bb77b81c4e817859fb831378d3c35ad0ea31bf4b",
        "title": "Wish you were here: Hindsight Goal Selection for long-horizon dexterous manipulation"
      },
      {
        "paperId": "ba107fdb842ee0218dffa34d7d7da7a20289a900",
        "title": "Deep reinforcement learning-based rehabilitation robot trajectory planning with optimized reward functions"
      },
      {
        "paperId": "e115a0b6f766264cee117319f02ce76a93e92da6",
        "title": "A Heuristic Algorithm with Falling Prediction Mechanism for Pose Planning in Irregular Objects Oriented Vertical Stacking Task"
      },
      {
        "paperId": "662d90425f2ed5d633d84b003c167ee938fb9631",
        "title": "Kernel-based diffusion approximated Markov decision processes for autonomous navigation and control on unstructured terrains"
      },
      {
        "paperId": "6f12106b68ce6d7bb4a00b65eca66d2e2020ce5b",
        "title": "Improving Learning from Demonstrations by Learning from Experience"
      },
      {
        "paperId": "7b599fdcbe89b694f8314e8dd77a097000dedabb",
        "title": "Learning Multi-Stage Tasks with One Demonstration via Self-Replay"
      },
      {
        "paperId": "fe609b4a2ab1ee99c50602f7ac1a2df8496356b5",
        "title": "Reinforcement Learning for Mobile Robotics Exploration: A Survey"
      },
      {
        "paperId": "1dcb8d9b305a75e5c5ae7e7d22a5e6a28db1faf4",
        "title": "Distilling Motion Planner Augmented Policies into Visual Control Policies for Robot Manipulation"
      },
      {
        "paperId": "62272403114c67a85e6fde9e428334d89e143485",
        "title": "AW-Opt: Learning Robotic Skills with Imitation and Reinforcement at Scale"
      },
      {
        "paperId": "bf999fea75655c26ec61110e182f72c9a78f6c89",
        "title": "Deep reinforcement learning control of electric vehicle charging in the presence of photovoltaic generation"
      },
      {
        "paperId": "f9ad2c9781ee0d8afa86c654756763270848962f",
        "title": "Achieving Safe Deep Reinforcement Learning via Environment Comprehension Mechanism"
      },
      {
        "paperId": "b25cccde97f1f38e574c12d9e97136040b561887",
        "title": "Quantum-enhanced reinforcement learning for control: a preliminary study"
      },
      {
        "paperId": "16a020853ab3ac442dfa2dce18966c0ed1c8da6e",
        "title": "LIDAR: learning from imperfect demonstrations with advantage rectification"
      },
      {
        "paperId": "00806bf90617d274ecf5fdc8c66844b7e8a68e93",
        "title": "Hindsight Goal Ranking on Replay Buffer for Sparse Reward Environment"
      },
      {
        "paperId": "c47962f2456143b83d4a1932b308b378830b5bfd",
        "title": "Learning from demonstrations with SACR2: Soft Actor-Critic with Reward Relabeling"
      },
      {
        "paperId": "87d0c65af9ccb1054afbf125e4ee8dbf3dcbfab1",
        "title": "Learning Diverse Policies in MOBA Games via Macro-Goals"
      },
      {
        "paperId": "2d1d65fd9b53bb97fdc46af4a6faa3e154178b3c",
        "title": "Multi-Agent Advisor Q-Learning"
      },
      {
        "paperId": "0510dd1e5b10f747dd1c0a9df87f930d4fbf58ef",
        "title": "Efficient Robotic Manipulation Through Offline-to-Online Reinforcement Learning and Goal-Aware State Information"
      },
      {
        "paperId": "274e32e992bb5ce70feb1a474ea9a4cc7886dad8",
        "title": "Model-free reinforcement learning from expert demonstrations: a survey"
      },
      {
        "paperId": "c0bdac3c09f7b9ec99932f4cf82cf3bd10ba87fa",
        "title": "Unobstructed Programming-by-Demonstration for Force-Based Assembly Utilizing External Force-Torque Sensors"
      },
      {
        "paperId": "0cfcabcaf746139bd82d8b7494c14e714e3227ae",
        "title": "Learning from Ambiguous Demonstrations with Self-Explanation Guided Reinforcement Learning"
      },
      {
        "paperId": "96bec8e33d877af3f174bc5ef70bf14d9c4e759f",
        "title": "A Closer Look at Advantage-Filtered Behavioral Cloning in High-Noise Datasets"
      },
      {
        "paperId": "a54ff2352a7b9bd6f4c9265d33ee5b55e6240a2a",
        "title": "Augmenting Reinforcement Learning with Behavior Primitives for Diverse Manipulation Tasks"
      },
      {
        "paperId": "b35925115846b5d5cda8a985c4e55a7f98cf1196",
        "title": "Deep Reinforcement Learning for Stock Recommendation"
      },
      {
        "paperId": "c2ec144b633e2dcbf889da95c711b483803e8350",
        "title": "Hierarchical learning from human preferences and curiosity"
      },
      {
        "paperId": "0eada7dcf0f92adf668253461435ade904284b7b",
        "title": "Trajectory-based Split Hindsight Reverse Curriculum Learning"
      },
      {
        "paperId": "bd6b23afe1a8f5c06e8fb89d8dca7250e191d85b",
        "title": "Prioritized Experience-Based Reinforcement Learning With Human Guidance for Autonomous Driving"
      },
      {
        "paperId": "e932ea6cd35f603137c69f174a81f83484f25b29",
        "title": "Efficiently Training On-Policy Actor-Critic Networks in Robotic Deep Reinforcement Learning with Demonstration-like Sampled Exploration"
      },
      {
        "paperId": "12bbeb7955b7af84c188c016f14dda158404655d",
        "title": "A noncontact robot demonstration method with human supervision"
      },
      {
        "paperId": "e5c3b520d7415e1268cf6c6e2f7e618248e11662",
        "title": "DCUR: Data Curriculum for Teaching via Samples with Reinforcement Learning"
      },
      {
        "paperId": "9e145a6fc514c0187a5307051e658f23b6d41572",
        "title": "OPIRL: Sample Efficient Off-Policy Inverse Reinforcement Learning via Distribution Matching"
      },
      {
        "paperId": "8938cf6f2b2a04bec7b31527c074feb5e711cb64",
        "title": "Reinforcement Learning for Statistical Process Control in Manufacturing"
      },
      {
        "paperId": "5e9d6c0d58a85b20c784d487155704ade4d0a274",
        "title": "Privacy Preserving Load Control of Residential Microgrid via Deep Reinforcement Learning"
      },
      {
        "paperId": "6f4ca4a3fd6071787083d067cf420e468e930d62",
        "title": "SurRoL: An Open-source Reinforcement Learning Centered and dVRK Compatible Platform for Surgical Robot Learning"
      },
      {
        "paperId": "694cac5ed2db350922d404cea214ab5f38a1e3ee",
        "title": "Goal-driven active learning"
      },
      {
        "paperId": "a1aeb9e492e8bf5d7cdf2cb1b90921921fe62c60",
        "title": "A data-efficient goal-directed deep reinforcement learning method for robot visuomotor skill"
      },
      {
        "paperId": "5079fb5db4953605753aeeefebe0edd979b030a0",
        "title": "Adaptive exploration policy for exploration\u2013exploitation tradeoff in continuous action control optimization"
      },
      {
        "paperId": "f4b1c103ccc117470cf60d17ea978b8dc12d1685",
        "title": "Tolerance-Guided Policy Learning for Adaptable and Transferrable Delicate Industrial Insertion"
      },
      {
        "paperId": "fe7382db243694c67c667cf2ec80072577d2372b",
        "title": "Rule-Aware Reinforcement Learning for Knowledge Graph Reasoning"
      },
      {
        "paperId": "b5c67d148c1936f4b68f3d8920f3471ee500fe1c",
        "title": "Reinforced Imitation Learning by Free Energy Principle"
      },
      {
        "paperId": "105f44c9d445de2b93d1297c2d5ac10cc776d654",
        "title": "Demonstration-Guided Reinforcement Learning with Learned Skills"
      },
      {
        "paperId": "0a50454605da864cc4e1ac949e7a43055be11717",
        "title": "Recent Advances in Deep Reinforcement Learning Applications for Solving Partially Observable Markov Decision Processes (POMDP) Problems: Part 1 - Fundamentals and Applications in Games, Robotics and Natural Language Processing"
      },
      {
        "paperId": "441b1fd5f97a858af63190a335b17675b159751a",
        "title": "Mixing Human Demonstrations with Self-Exploration in Experience Replay for Deep Reinforcement Learning"
      },
      {
        "paperId": "45afe2d85f2896ce569be0d27678edcff68017e2",
        "title": "Hierarchical Policies for Cluttered-Scene Grasping with Latent Plans"
      },
      {
        "paperId": "408916ecd31db45abeba61e92b766bc600550a4e",
        "title": "Support Rather Than Assault \u2013 Cooperative Agents in Minecraft"
      },
      {
        "paperId": "97e783e0dfb8b7c43ed3c37d4db7598d99cab1a6",
        "title": "Planning-Augmented Hierarchical Reinforcement Learning"
      },
      {
        "paperId": "d1da8fa082b16714780f63fa8275529d425bca6a",
        "title": "Imitation Learning: Progress, Taxonomies and Challenges"
      },
      {
        "paperId": "8786bb16bea2e1a4d9952d0abae5f0591959b3b7",
        "title": "Automatic Curricula via Expert Demonstrations"
      },
      {
        "paperId": "d051076c1356c3f9fc573d70b9a384984386c3bf",
        "title": "Residual Reinforcement Learning from Demonstrations"
      },
      {
        "paperId": "c879b25308026d6538e52b27bcf4fd3cb60855f3",
        "title": "A Minimalist Approach to Offline Reinforcement Learning"
      },
      {
        "paperId": "2d8279e9f71ccda897d89f200c912a27c82c3356",
        "title": "Packet Drop Probability-Optimal Cross-layer Scheduling: Dealing with Curse of Sparsity using Prioritized Experience Replay"
      },
      {
        "paperId": "6b7d21cf7355e08d53cb2ad17c144002fca3f2c8",
        "title": "Q-attention: Enabling Efficient Learning for Vision-based Robotic Manipulation"
      },
      {
        "paperId": "b666facd47d8878bf6a1e481c9e39dd40a017b1c",
        "title": "Learning from Demonstration without Demonstrations"
      },
      {
        "paperId": "0ea89ca0929b9323ba3d65bc6bce654d37cdcea4",
        "title": "Hierarchical Learning from Demonstrations for Long-Horizon Tasks"
      },
      {
        "paperId": "7af3961b49435f2225e5d6d56a5542cebcfeb1a1",
        "title": "Automated Generation of Robot Trajectories for Assembly Processes Requiring Only Sparse Manual Input"
      },
      {
        "paperId": "d399d37e086e43b9d0e5022dd06d773ead0ffc35",
        "title": "Transfer Learning and Curriculum Learning in Sokoban"
      },
      {
        "paperId": "d030baeb69f68a09a90370bf7cea0b72a58c3d5c",
        "title": "Manipulation skill learning on multi-step complex task based on explicit and implicit curriculum learning"
      },
      {
        "paperId": "d8e087c88ee0a553c9212bfcb1a194260a06db43",
        "title": "A self-guided approach for navigation in a minimalistic foraging robotic swarm"
      },
      {
        "paperId": "a8e8c037136750a2e989e60c9a56dad082fe1269",
        "title": "PP-PG: Combining Parameter Perturbation with Policy Gradient Methods for Effective and Efficient Explorations in Deep Reinforcement Learning"
      },
      {
        "paperId": "1563b3cf96bd3ffb257ee89cf2f1120b41f37d6e",
        "title": "Coarse-to-Fine Imitation Learning: Robot Manipulation from a Single Demonstration"
      },
      {
        "paperId": "c87507d6c45b6ef4b56bdd3d481fd59e9f27b565",
        "title": "Hybrid Imitation Learning Framework for Robotic Manipulation Tasks"
      },
      {
        "paperId": "4868b1779076164308171694aefd02eb3076afa1",
        "title": "Imitation-Learning-Enabled Vehicular Edge Computing: Toward Online Task Scheduling"
      },
      {
        "paperId": "bae901b2011089906a0d43446cf92e9891ecf594",
        "title": "Reinforcement learning for robot research: A comprehensive review and open issues"
      },
      {
        "paperId": "f68cfbf3caf3de5324b297510d8f52e14dee8e09",
        "title": "Pre-training with asynchronous supervised learning for reinforcement learning based autonomous driving"
      },
      {
        "paperId": "11b721931a25a52ebdea82d92055535e574adb3d",
        "title": "Match Plan Generation in Web Search with Parameterized Action Reinforcement Learning"
      },
      {
        "paperId": "bec834d11bc738eab0d380956cc8673d634821eb",
        "title": "Human-in-the-Loop Deep Reinforcement Learning with Application to Autonomous Driving"
      },
      {
        "paperId": "2d48c903c37cd0776e8b1defb8ec108ce3276aac",
        "title": "Self-Imitation Learning by Planning"
      },
      {
        "paperId": "bb213772d6323464ad08ec07c4555f56c57d4a39",
        "title": "CLAMGen: Closed-Loop Arm Motion Generation via Multi-view Vision-Based RL"
      },
      {
        "paperId": "6100d5db2e8908a0b2a9549476b30b248cbc2e18",
        "title": "Robust Multi-Modal Policies for Industrial Assembly via Reinforcement Learning and Demonstrations: A Large-Scale Study"
      },
      {
        "paperId": "4d75db2bdbdb395c105a8acb9bdd3793b8789902",
        "title": "Human-Inspired Multi-Agent Navigation using Knowledge Distillation"
      },
      {
        "paperId": "27e4c7ec8deec0d1562e25479639721f610f4556",
        "title": "[CASPI] Causal-aware Safe Policy Improvement for Task-oriented Dialogue"
      },
      {
        "paperId": "54e1040e7f9f7d6b52abf03a1353b5a377802396",
        "title": "Robotic Imitation of Human Assembly Skills Using Hybrid Trajectory and Force Learning"
      },
      {
        "paperId": "29f05531de4426cf0b88bace56b47bf0ac9ce0a2",
        "title": "Deep reinforcement learning in medical imaging: A literature review"
      },
      {
        "paperId": "c7b01136da36d68e229f268e5489c81c94586481",
        "title": "Hierarchical Reinforcement Learning With Universal Policies for Multistep Robotic Manipulation"
      },
      {
        "paperId": "1fcfdfe06b13b36eff963d9f86f377cfcfec938a",
        "title": "PsiPhi-Learning: Reinforcement Learning with Demonstrations using Successor Features and Inverse Temporal Difference Learning"
      },
      {
        "paperId": "488fe992c5953af742481b7b73fb028be698a8a6",
        "title": "Identifying efficient curricula for reinforcement learning in complex environments with a fixed computational budget"
      },
      {
        "paperId": "1543d2d35cb8d8a597abc0635967f4f4facc33a2",
        "title": "VisuoSpatial Foresight for physical sequential fabric manipulation"
      },
      {
        "paperId": "40b6d7d961772f7fe2e23683da960ce7c7e5d059",
        "title": "Learning Memory-Dependent Continuous Control from Demonstrations"
      },
      {
        "paperId": "9ae2eca0b43cbad7a16771baf0439e9d01bc11b6",
        "title": "A Collaborative Control Method of Dual-Arm Robots Based on Deep Reinforcement Learning"
      },
      {
        "paperId": "d9866f95765c0a505bc749c6a256e4df080554d0",
        "title": "SCAPE: Learning Stiffness Control from Augmented Position Control Experiences"
      },
      {
        "paperId": "6c7d9f857d5264113553d89b1edc1588288a2f61",
        "title": "Transferring Domain Knowledge with an Adviser in Continuous Tasks"
      },
      {
        "paperId": "2f2e2ac0832d1ceab3b8d09a05099db3528a18d8",
        "title": "Towards Hierarchical Task Decomposition using Deep Reinforcement Learning for Pick and Place Subtasks"
      },
      {
        "paperId": "400811ee31020a3f002551476dac25973e13035e",
        "title": "How to train your robot with deep reinforcement learning: lessons we have learned"
      },
      {
        "paperId": "c9ad9676a07c16e595390fe10ac30d66eddbe18a",
        "title": "Vision-Based Deep Reinforcement Learning For UR5 Robot Motion Control"
      },
      {
        "paperId": "12ce3a14da5a7e22bcb3b14452dd9d3bb8f5cf36",
        "title": "Asymmetric self-play for automatic goal discovery in robotic manipulation"
      },
      {
        "paperId": "b7f7d441a56a012fd3e5b1f1da37ac77708645a8",
        "title": "Deep Learning in Robotics: Survey on Model Structures and Training Strategies"
      },
      {
        "paperId": "c31ac9a97c2e40e57ffccfc1c500304b8c697abe",
        "title": "POPO: Pessimistic Offline Policy Optimization"
      },
      {
        "paperId": "3b0b15dcef21d2ac69ba85300c3dbd95f1182f29",
        "title": "Locally Persistent Exploration in Continuous Control Tasks with Sparse Rewards"
      },
      {
        "paperId": "af921a181be30830afc60fdbb619b087788bda8b",
        "title": "Auto-Agent-Distiller: Towards Efficient Deep Reinforcement Learning Agents via Neural Architecture Search"
      },
      {
        "paperId": "ec8eee61f42e07228bc78faac9817cff3e000ebf",
        "title": "Augmenting Policy Learning with Routines Discovered from a Single Demonstration"
      },
      {
        "paperId": "a638594a57de24bca143e55397073a8d27b0aa98",
        "title": "Autotelic Agents with Intrinsically Motivated Goal-Conditioned Reinforcement Learning: A Short Survey"
      },
      {
        "paperId": "34eb252cc43ba3577a1724792b035f7b5f378e37",
        "title": "Intrinsically Motivated Goal-Conditioned Reinforcement Learning: a Short Survey"
      },
      {
        "paperId": "c5ab9777142def8ab2ced2fd7df663a3fe39120c",
        "title": "Learning Visual Robotic Control Efficiently with Contrastive Pre-training and Data Augmentation"
      },
      {
        "paperId": "b9e4bccd2e69132af961fceeb51d418ee106e3ff",
        "title": "Active Hierarchical Imitation and Reinforcement Learning"
      },
      {
        "paperId": "5c9815f7d907b79b088cdf49665be9b8dc89e5e7",
        "title": "A Framework for Efficient Robotic Manipulation"
      },
      {
        "paperId": "5432409e4b7f2226081f8ba32c238ad6c8b8bff3",
        "title": "Relative control of an underactuated spacecraft using reinforcement learning"
      },
      {
        "paperId": "a7418f5f5e20917f473be3762e205c8a776456c1",
        "title": "Reward Learning From Very Few Demonstrations"
      },
      {
        "paperId": "23472dde7735dbe88b300662a0421aca52692075",
        "title": "Offline Learning from Demonstrations and Unlabeled Experience"
      },
      {
        "paperId": "16b03f654b259eaeb744b185191490a953af3d5f",
        "title": "Learning of Long-Horizon Sparse-Reward Robotic Manipulator Tasks With Base Controllers"
      },
      {
        "paperId": "8bf9dec401de484819530b2b2fa0f62faff2f986",
        "title": "REPAINT: Knowledge Transfer in Deep Reinforcement Learning"
      },
      {
        "paperId": "3a634e5d24d097847cf3858096b5a7faf6c8e917",
        "title": "Achieving Sample-Efficient and Online-Training-Safe Deep Reinforcement Learning with Base Controllers"
      },
      {
        "paperId": "f5275f5eb6569ddb5ba9a959ede09875d56e3bac",
        "title": "Parrot: Data-Driven Behavioral Priors for Reinforcement Learning"
      },
      {
        "paperId": "17403af8c126f4699c8c302b442a4462025c1603",
        "title": "Real-time artificial intelligence for accelerator control: A study at the Fermilab Booster"
      },
      {
        "paperId": "7d99f5dc92678f61576427f92adba9bef43dfd65",
        "title": "Reinforcement Learning with Videos: Combining Offline Observations with Interaction"
      },
      {
        "paperId": "9663d77591f7307d918875d59e95e963eb4149bc",
        "title": "Behavior Planning at Urban Intersections through Hierarchical Reinforcement Learning*"
      },
      {
        "paperId": "a5f103845d5d23098cb0afc3b1d6f2808c0b0ab2",
        "title": "Bimanual Regrasping for Suture Needles using Reinforcement Learning for Rapid Motion Planning"
      },
      {
        "paperId": "d0b69a1885781e38de0f2cb81651fb852799ba3a",
        "title": "Sample-efficient Reinforcement Learning in Robotic Table Tennis"
      },
      {
        "paperId": "a66df6a55c25e8f131502c922000b9197debe14a",
        "title": "Shaping Rewards for Reinforcement Learning with Imperfect Demonstrations using Generative Models"
      },
      {
        "paperId": "9fbc546e6a39446d5795a20ef851948cc0d420ea",
        "title": "\"What, not how\": Solving an under-actuated insertion task from scratch"
      },
      {
        "paperId": "a15d706d3ceed54466b25fe8c75136249991c4d3",
        "title": "Episodic Self-Imitation Learning with Hindsight"
      },
      {
        "paperId": "e7c71aec3d99708471c3d2e2d8505b1701964e27",
        "title": "Decentralized Multi-Agent Pursuit Using Deep Reinforcement Learning"
      },
      {
        "paperId": "201fd36b79dff8d46ec49ce1531cadcf7330332b",
        "title": "Hindsight Experience Replay with Kronecker Product Approximate Curvature"
      },
      {
        "paperId": "548dad02f2b1c16663f8229d1fa52d358869a83a",
        "title": "Policy learning in SE(3) action spaces"
      },
      {
        "paperId": "091810ee7c0a2a550f84c16dfef796d736298454",
        "title": "Goal-Auxiliary Actor-Critic for 6D Robotic Grasping with Point Clouds"
      },
      {
        "paperId": "580d85d3ce5475a2931b2147648b1fb79ef03358",
        "title": "Align-RUDDER: Learning From Few Demonstrations by Reward Redistribution"
      },
      {
        "paperId": "b20aea31b21cd3a2b23e89cb4d0fa6470a5eecc3",
        "title": "Lucid dreaming for experience replay: refreshing past states with the current policy"
      },
      {
        "paperId": "d66612637efdec74acaf10e26c4a3d8a22dc784a",
        "title": "Multi-agent reinforcement learning algorithm to solve a partially-observable multi-agent problem in disaster response"
      },
      {
        "paperId": "b61d9470276c6658b7bc25a2701f5be89efddae3",
        "title": "Evolutionary Selective Imitation: Interpretable Agents by Imitation Learning Without a Demonstrator"
      },
      {
        "paperId": "f8492a321d66c381637b693a24af994af41b3cdf",
        "title": "Transfer Learning in Deep Reinforcement Learning: A Survey"
      },
      {
        "paperId": "db5a49608d8320c7751c74c40a3121e774251614",
        "title": "WMat algorithm based on Q-Learning algorithm in taxi-v2 game"
      },
      {
        "paperId": "a8922aa79040e225738a024d3cb4e421150103ec",
        "title": "Natural object manipulation using anthropomorphic robotic hand through deep reinforcement learning and deep grasping probability network"
      },
      {
        "paperId": "441c95150940e67bd044831ea69b5310c2cca395",
        "title": "Physically Embedded Planning Problems: New Challenges for Reinforcement Learning"
      },
      {
        "paperId": "f6313aa0f6b7c5f9875d755a4cfbb119cba90d8e",
        "title": "Solving Challenging Dexterous Manipulation Tasks With Trajectory Optimisation and Reinforcement Learning"
      },
      {
        "paperId": "9c60e97762aeea4b5b7a01c835975b749772227b",
        "title": "Generating attentive goals for prioritized hindsight reinforcement learning"
      },
      {
        "paperId": "5dbe5543b4f32fff2b0ca9ea6ed5ff3b7377d967",
        "title": "Hybrid of Reinforcement and Imitation Learning for Human-Like Agents"
      },
      {
        "paperId": "a1257d485970d89125795e1c9308cd4bb54309c5",
        "title": "Variable Compliance Control for Robotic Peg-in-Hole Assembly: A Deep Reinforcement Learning Approach"
      },
      {
        "paperId": "c864b7a2bd0d82c1c091b9bae2a33f1ef927f6d2",
        "title": "REMAX: Relational Representation for Multi-Agent Exploration"
      },
      {
        "paperId": "124b683a6f0f8ac8e13e204dfcd30a5497ab581d",
        "title": "Overcoming Model Bias for Robust Offline Deep Reinforcement Learning"
      },
      {
        "paperId": "46ec3dd1e50746c77e0ed18b6748d1274a1f623d",
        "title": "Deep Reinforcement Learning based Local Planner for UAV Obstacle Avoidance using Demonstration Data"
      },
      {
        "paperId": "6c7adc331a5970faebb9bb2420e30164a4871c6b",
        "title": "Follow the Object: Curriculum Learning for Manipulation Tasks with Imagined Goals"
      },
      {
        "paperId": "0ced04ad3dd659f660ad9de9abe53c46efd06ac4",
        "title": "Parallel Multi-Environment Shaping Algorithm for Complex Multi-step Task"
      },
      {
        "paperId": "c7bb51450805f8a33a63e7fbd42b5c8bf84eb3d8",
        "title": "Reinforcement Learning with Converging Goal Space and Binary Reward Function"
      },
      {
        "paperId": "dcb6fa098b20efd1e6a9f40fc34ffff1d2bba889",
        "title": "Complex Robotic Manipulation via Graph-Based Hindsight Goal Generation"
      },
      {
        "paperId": "3f618d2500477729926ce7d9d311e7e69477dc07",
        "title": "Bridging the Imitation Gap by Adaptive Insubordination"
      },
      {
        "paperId": "b1b0f9c33e4b3253a27e651d3ed47d29867d4df4",
        "title": "Learning Abstract Models for Strategic Exploration and Fast Reward Transfer"
      },
      {
        "paperId": "72260c19441259404ed24003d9e27588fb3613ae",
        "title": "Maximum Entropy Gain Exploration for Long Horizon Multi-goal Reinforcement Learning"
      },
      {
        "paperId": "69fb9608411f878b4634f6b30ddcabd9fd7e7f20",
        "title": "Robot learning from demonstration for path planning: A review"
      },
      {
        "paperId": "066841efe8d79b4998b7af92eb1f5cf3a3f5ebcd",
        "title": "A conceptual framework for externally-influenced agents: an assisted reinforcement learning review"
      },
      {
        "paperId": "e423c07b36936ddce137bce009b318f2c2741be5",
        "title": "Adaptive Procedural Task Generation for Hard-Exploration Problems"
      },
      {
        "paperId": "57a121e51b4cd1cd2a81506ce32196217b439a46",
        "title": "Reinforcement Learning based Control of Imitative Policies for Near-Accident Driving"
      },
      {
        "paperId": "7acbdb961f67d50fef359066f2a1d7755cf16ee2",
        "title": "Critic Regularized Regression"
      },
      {
        "paperId": "0272b14dd471fe7b81df703af1b71d7600b77215",
        "title": "Accelerating Online Reinforcement Learning with Offline Datasets"
      },
      {
        "paperId": "b1d8bb041f504ed4a593638f093c45c5c1cba9d7",
        "title": "Learning synergies based in-hand manipulation with reward shaping"
      },
      {
        "paperId": "dfca81607959eb7f101911288025a598bc5a6d18",
        "title": "Learning to Play by Imitating Humans"
      },
      {
        "paperId": "34c44797d32b6e05d19f80af6d60c701e72cb6b4",
        "title": "Teaching NICO How to Grasp: An Empirical Study on Crossmodal Social Interaction as a Key Factor for Robots Learning From Humans"
      },
      {
        "paperId": "55cf90ee141f0ab3e59b7d1521ce0cef572330db",
        "title": "Investigating exploration for deep reinforcement learning of concentric tube robot control"
      },
      {
        "paperId": "2e1ac521d53e09e90fd9b4bf949f667d756853de",
        "title": "Acme: A Research Framework for Distributed Reinforcement Learning"
      },
      {
        "paperId": "c26f10c4954aebf2f9001cebac412b63c0381dff",
        "title": "Learning visual servo policies via planner cloning"
      },
      {
        "paperId": "f441e637980a8b427474dbdc0141f38dd78bb831",
        "title": "Recent Advances in Robot Learning from Demonstration"
      },
      {
        "paperId": "b4873e3a17058c81a8d2bba838cdd0c415ee80e7",
        "title": "Guided Uncertainty-Aware Policy Optimization: Combining Learning and Model-Based Strategies for Sample-Efficient Policy Learning"
      },
      {
        "paperId": "a7cca565d47880e83b3dc409501ad8bded27993a",
        "title": "Attentive Task-Net: Self Supervised Task-Attention Network for Imitation Learning using Video Demonstration"
      },
      {
        "paperId": "d99772c59a94242cd12a61c5b4e7224bbcfe7d90",
        "title": "TRASS: Time Reversal as Self-Supervision"
      },
      {
        "paperId": "0043212391ddf7d689c434efd0daa9960225e3b0",
        "title": "Knowledge-Guided Reinforcement Learning Control for Robotic Lower Limb Prosthesis"
      },
      {
        "paperId": "913e9384a1440dbf860f6d8597953187237fa1b4",
        "title": "Disagreement-Regularized Imitation Learning"
      },
      {
        "paperId": "5f5f3c5a395f66691bc47e7877e83168cf0454bf",
        "title": "Meta-Reinforcement Learning for Robotic Industrial Insertion Tasks"
      },
      {
        "paperId": "616ac6772508e72084360158078058dfa8bda7e7",
        "title": "First return, then explore"
      },
      {
        "paperId": "36e739a71e408e19d1c37ea4873370f293bfceb2",
        "title": "Evolutionary Stochastic Policy Distillation"
      },
      {
        "paperId": "de93c8aed64229571b03e40b36499d4f07ce875d",
        "title": "PBCS : Efficient Exploration and Exploitation Using a Synergy between Reinforcement Learning and Motion Planning"
      },
      {
        "paperId": "6874b196102f1de902bef1679ec26337bde6891a",
        "title": "Policy Gradient From Demonstration and Curiosity"
      },
      {
        "paperId": "c748dc8700684d90efe228043264ea5baac68c18",
        "title": "Flexible and Efficient Long-Range Planning Through Curious Exploration"
      },
      {
        "paperId": "a8cbef54001aa2077631827f10ca89a702e01a07",
        "title": "Generalize Robot Learning From Demonstration to Variant Scenarios With Evolutionary Policy Gradient"
      },
      {
        "paperId": "325488b29a33ba8e3f061dda9aed7b2df52f0447",
        "title": "A Game Theoretic Framework for Model Based Reinforcement Learning"
      },
      {
        "paperId": "1ee2cf270b6f6db61e11e96f7d73f7a106a724eb",
        "title": "Optical Coherence Tomography-Guided Robotic Ophthalmic Microsurgery via Reinforcement Learning from Demonstration"
      },
      {
        "paperId": "0256975b4448b8945fc44ff71b22ddc3ca95731d",
        "title": "A Confrontation Decision-Making Method with Deep Reinforcement Learning and Knowledge Transfer for Multi-Agent System"
      },
      {
        "paperId": "15098ba616a6d5eca666a303ce3d532f613d19f4",
        "title": "Learning visual policies for building 3D shape categories"
      },
      {
        "paperId": "4421bf55471b941bb5a85c7ed128fbf1f955d636",
        "title": "Self-Punishment and Reward Backfill for Deep Q-Learning"
      },
      {
        "paperId": "3a9d030daf272945ce87abf6afdb63068f2ec26d",
        "title": "Adaptive Quantitative Trading: An Imitative Deep Reinforcement Learning Approach"
      },
      {
        "paperId": "f73ce2a24c44045a078bd06c0b06326c78e789ac",
        "title": "Learning Sparse Rewarded Tasks from Sub-Optimal Demonstrations"
      },
      {
        "paperId": "7206d27bbb73b376dbf6992435bdac66a5994ab1",
        "title": "Goal-Conditioned End-to-End Visuomotor Control for Versatile Skill Primitives"
      },
      {
        "paperId": "369aea426e4602ab0963da50c1ee88aa08754fdf",
        "title": "VisuoSpatial Foresight for Multi-Step, Multi-Task Fabric Manipulation"
      },
      {
        "paperId": "9f31b5dd1d9407de3ae8aa0f88bc8cfe9392f398",
        "title": "Human-in-the-Loop Methods for Data-Driven and Reinforcement Learning Systems"
      },
      {
        "paperId": "7b82f0682614da9d11d23ba7733439d7a2b1d19f",
        "title": "Dynamic Experience Replay"
      },
      {
        "paperId": "4c79edba0079d9b64359a085a96661c3d125b159",
        "title": "Exploration-efficient Deep Reinforcement Learning with Demonstration Guidance for Robot Control"
      },
      {
        "paperId": "44ef2860f3778dadbc373cad901b4ac647b46f57",
        "title": "Learning Precise 3D Manipulation from Multiple Uncalibrated Cameras"
      },
      {
        "paperId": "4dc50d9c88d4d4b802363785424bd58f7dcff458",
        "title": "A Comparative Study of Model-Free Reinforcement Learning Approaches"
      },
      {
        "paperId": "263f24dbb1ef66b6031a388d436c959023c11a90",
        "title": "Accelerating Reinforcement Learning for Reaching Using Continuous Curriculum Learning"
      },
      {
        "paperId": "c8d224bb179c749edcea764e690c2cdaa65b9e93",
        "title": "Periodic Intra-Ensemble Knowledge Distillation for Reinforcement Learning"
      },
      {
        "paperId": "44323ec1ea9c1432966ea0d5c2ef78f4b8e2a619",
        "title": "A Framework for Learning From Demonstration With Minimal Human Effort"
      },
      {
        "paperId": "a42744f40650673b910a6fa925a217491a074ed9",
        "title": "On Simple Reactive Neural Networks for Behaviour-Based Reinforcement Learning"
      },
      {
        "paperId": "52c77192011106c2f2e3911f0e8ed7bb9f5ac7a2",
        "title": "Augmenting GAIL with BC for sample efficient imitation learning"
      },
      {
        "paperId": "58d55473fafcaa1f8106052dbe6987863ebc8388",
        "title": "A Survey of Reinforcement Learning Techniques: Strategies, Recent Development, and Future Directions"
      },
      {
        "paperId": "2b8fa3aabf3e8a0cf7e492fbb15f8bc69cce8595",
        "title": "Learning from Demonstrations and Human Evaluative Feedbacks: Handling Sparsity and Imperfection Using Inverse Reinforcement Learning Approach"
      },
      {
        "paperId": "92668600e733b937469060ef6b4025e30f5f557c",
        "title": "Constrained-Space Optimization and Reinforcement Learning for Complex Tasks"
      },
      {
        "paperId": "ac97d53ea5d16062a38bf062f92ca54240c9c36f",
        "title": "Transferring Human Manipulation Knowledge to Robots with Inverse Reinforcement Learning*"
      },
      {
        "paperId": "ecb852d120b1b50b2b0cd5a91d154b3c1ff2167d",
        "title": "Federated Imitation Learning: A Novel Framework for Cloud Robotic Systems With Heterogeneous Sensor Data"
      },
      {
        "paperId": "d4a28c5c2c37d57501d4529a135fb1f9cd8e3234",
        "title": "Towards Practical Multi-Object Manipulation using Relational Reinforcement Learning"
      },
      {
        "paperId": "25faeb46c6201e4702ec364ee905bf66ac29821a",
        "title": "Hierarchical Deep Q-Network from imperfect demonstrations in Minecraft"
      },
      {
        "paperId": "e3e04b0855beb24d4ab112cc7f864161a72375f8",
        "title": "Addressing Reward Engineering for Deep Reinforcement Learning on Multi-stage Task"
      },
      {
        "paperId": "3b1c7687ce4644063b69718bdb18e26ea0d33278",
        "title": "Learning from Demonstration Based on a Classification of Task Parameters and Trajectory Optimization"
      },
      {
        "paperId": "8112e04e71b83f996f6098bc6ae2b4856128fbce",
        "title": "Automated curriculum generation for Policy Gradients from Demonstrations"
      },
      {
        "paperId": "8d9c98a819371464bd1c25a2af2e66b69ed40bfe",
        "title": "Accelerating Reinforcement Learning with Suboptimal Guidance"
      },
      {
        "paperId": "5e9764f45e7ea6206594deb94753a5cad4e31a1a",
        "title": "IRIS: Implicit Reinforcement without Interaction at Scale for Learning Control from Offline Robot Manipulation Data"
      },
      {
        "paperId": "d18a394df825556b139aa8d0c120e4f75d96ec21",
        "title": "Task-Oriented Deep Reinforcement Learning for Robotic Skill Acquisition and Control"
      },
      {
        "paperId": "4cd888a537e719ceaeb253afeb601df870e21732",
        "title": "Hierarchical Reinforcement Learning Method for Autonomous Vehicle Behavior Planning"
      },
      {
        "paperId": "f657c8d1fa6aaf8263ade59ad4f0b7c65af4accb",
        "title": "Multi-step Pick-and-Place Tasks Using Object-centric Dense Correspondences"
      },
      {
        "paperId": "f58a77a92b795241943b7ff1740dfcc58039589c",
        "title": "TendencyRL: Multi-stage Discriminative Hints for Efficient Goal-Oriented Reverse Curriculum Learning"
      },
      {
        "paperId": "d849aec0e90d31068cac27d6f7258a6b513af512",
        "title": "Dynamic Cloth Manipulation with Deep Reinforcement Learning"
      },
      {
        "paperId": "361c00b22e29d0816ca896513d2c165e26399821",
        "title": "Grandmaster level in StarCraft II using multi-agent reinforcement learning"
      },
      {
        "paperId": "38420ad6f2c499caddc59f77660a23a299d1d011",
        "title": "ZPD Teaching Strategies for Deep Reinforcement Learning from Demonstrations"
      },
      {
        "paperId": "8c54e8575e7c17a4097838305915e6e7b00fd4af",
        "title": "Relay Policy Learning: Solving Long-Horizon Tasks via Imitation and Reinforcement Learning"
      },
      {
        "paperId": "377079e2a90e19b1e667853fc9aa4caf3d899489",
        "title": "Self-Supervised Sim-to-Real Adaptation for Visual Robotic Manipulation"
      },
      {
        "paperId": "7e12836510909097f592cbec8c2f646f1b3790e5",
        "title": "OffWorld Gym: open-access physical robotics environment for real-world reinforcement learning benchmark and research"
      },
      {
        "paperId": "0172bd210eaaaa4423c95684802d366ec22001dd",
        "title": "Adaptive Curriculum Generation from Demonstrations for Sim-to-Real Visuomotor Control"
      },
      {
        "paperId": "1fcef49a0b3bdf0cc8d7b3ddf3dc1704eac98e58",
        "title": "Reinforcement learning for robotic manipulation using simulated locomotion demonstrations"
      },
      {
        "paperId": "7cde7e40e573bc5a5e31010d82b696cfdcfc16c4",
        "title": "Integrating Behavior Cloning and Reinforcement Learning for Improved Performance in Sparse Reward Environments"
      },
      {
        "paperId": "956c771c624ea9c1ce1247a99ec23d64fc432299",
        "title": "DeepMNavigate: Deep Reinforced Multi-Robot Navigation Unifying Local & Global Collision Avoidance"
      },
      {
        "paperId": "1ea418e97bca641d10ff2a08cd99aee57e6dae94",
        "title": "Accelerated Robot Learning via Human Brain Signals"
      },
      {
        "paperId": "b0fc897a79214e7b72d96ab67219bbc142ed276c",
        "title": "Reinforcement Learning for Multi-Objective Optimization of Online Decisions in High-Dimensional Systems"
      },
      {
        "paperId": "c92780cd2c90b0393efb5d5b3a49b1ec9503df11",
        "title": "Policy Continuation with Hindsight Inverse Dynamics"
      },
      {
        "paperId": "78aea5a51feb90e988a4d00302616e56d1be5eb0",
        "title": "Scaling data-driven robotics with reward sketching and batch reinforcement learning"
      },
      {
        "paperId": "cac78ad6696d6b6370942679f7d0e4425ef4b3e7",
        "title": "A Framework for Data-Driven Robotics"
      },
      {
        "paperId": "8a14669c7d9541cf072889f97020bd956a54eb51",
        "title": "Demonstration actor critic"
      },
      {
        "paperId": "2f9d13cbb9bc01b3261f8ff6afbace923eefa341",
        "title": "Reinforcement Learning with Probabilistically Complete Exploration"
      },
      {
        "paperId": "eb00b8453b23d4f6f142378e2fb0f0a9e6f9c5e2",
        "title": "Visual Tracking by Means of Deep Reinforcement Learning and an Expert Demonstrator"
      },
      {
        "paperId": "67f8477b2af5a045bceff059b77773496c3377a7",
        "title": "State Representation Learning from Demonstration"
      },
      {
        "paperId": "83275ca4a2fb40c2c3fa281b181fb644fd583848",
        "title": "AC-Teach: A Bayesian Actor-Critic Method for Policy Learning with an Ensemble of Suboptimal Teachers"
      },
      {
        "paperId": "428e2bf31b1e6ef6b5f2e982d39575158bded349",
        "title": "Making Efficient Use of Demonstrations to Solve Hard Exploration Problems"
      },
      {
        "paperId": "abbd25a8903f9a911aa07af654151871368c7811",
        "title": "Deep Reinforcement Learning for the Navigation of Neurovascular Catheters"
      },
      {
        "paperId": "8097efa4b0c5490575c92fd915a40e593ea3ccfa",
        "title": "Human-like Object Grasping and Relocation for an Anthropomorphic Robotic Hand with Natural Hand Pose Priors in Deep Reinforcement Learning"
      },
      {
        "paperId": "0ee7bddcdd52388c3c01df560c35bfe7d9b9c15f",
        "title": "Learning to combine primitive skills: A step towards versatile robotic manipulation \u00a7"
      },
      {
        "paperId": "1f1e51350458358274e0ad86ea1bfc88b92b1b6a",
        "title": "Combining learned skills and reinforcement learning for robotic manipulations"
      },
      {
        "paperId": "2ef99cf62de0f1d2e929ad11f346a5f825287739",
        "title": "Unmanned Aerial Vehicles Path Planning Based on Deep Reinforcement Learning"
      },
      {
        "paperId": "bc32aab2970d62a097cee080c8bbd25837a5443b",
        "title": "Learning Self-Correctable Policies and Value Functions from Demonstrations with Negative Sampling"
      },
      {
        "paperId": "195c930e5b52d8072aadf9def7111ecf97e610b2",
        "title": "Learning a Behavioral Repertoire from Demonstrations"
      },
      {
        "paperId": "bc5a6bdfa230ec4f7b7ebcf0b514a89970eb8c02",
        "title": "Integration of imitation learning using GAIL and reinforcement learning using task-achievement rewards via probabilistic graphical model"
      },
      {
        "paperId": "a87b805fd1b2da7cc1401e34df695056c278f2ea",
        "title": "Co-training for Policy Learning"
      },
      {
        "paperId": "8e0d90978773345f1230d77a20082dcf95b31c16",
        "title": "Low-resource learning in complex games"
      },
      {
        "paperId": "20ba7adabf7684b1c6fc3079b32674a4a269cab0",
        "title": "Task Dependent Trajectory Learning from Multiple Demonstrations Using Movement Primitives"
      },
      {
        "paperId": "fff2bcc0348dffdb7661640d51738d4716a6304c",
        "title": "RIDM: Reinforced Inverse Dynamics Modeling for Learning from a Single Observed Demonstration"
      },
      {
        "paperId": "b668ba0900ddcdacd0a07ff9983172f525c3c4d6",
        "title": "Goal-conditioned Imitation Learning"
      },
      {
        "paperId": "e0889fcee1acd985af76a3907d5d0029bf260be9",
        "title": "Search on the Replay Buffer: Bridging Planning and Reinforcement Learning"
      },
      {
        "paperId": "4af16a5e81e28e920d7844f470d97bab77a909d6",
        "title": "Curiosity-Driven Multi-Criteria Hindsight Experience Replay"
      },
      {
        "paperId": "00753de4e5553de8a569e951f531bd683d8dcb16",
        "title": "Watch, Try, Learn: Meta-Learning from Demonstrations and Reward"
      },
      {
        "paperId": "150ca7461ebd503efacd11eb3a24458687d38e32",
        "title": "Autonomous Reinforcement Learning of Multiple Interrelated Tasks"
      },
      {
        "paperId": "ed9d466e671895ee9e651db30c761f81933f3c1a",
        "title": "An Automatic Robot Skills Learning System from Robot\u2019s Real-World Demonstrations"
      },
      {
        "paperId": "51fe965c579a689d1dc466f2313227a07eb22126",
        "title": "Safety Augmented Value Estimation From Demonstrations (SAVED): Safe Deep Model-Based RL for Sparse Cost Robotic Tasks"
      },
      {
        "paperId": "4a417479df70d216e19c99d4a5408e41488d3621",
        "title": "Extending Deep Model Predictive Control with Safety Augmented Value Estimation from Demonstrations"
      },
      {
        "paperId": "801eb90352e603b1573a04a36804417d1c9496f0",
        "title": "Learning latent state representation for speeding up exploration"
      },
      {
        "paperId": "98f65707cdadaaa2d2dc127d6b3fc6d3b68bf03b",
        "title": "Exact-K Recommendation via Maximal Clique Optimization"
      },
      {
        "paperId": "b95494460d7f5c6da4e370c42a5110b3fe01a223",
        "title": "Pretrain Soft Q-Learning with Imperfect Demonstrations"
      },
      {
        "paperId": "27d5c1a6b35a591739c10a04e5fc1a96b10e3177",
        "title": "MaMiC: Macro and Micro Curriculum for Robotic Reinforcement Learning"
      },
      {
        "paperId": "92e0c1697bc4630903501185d11676f9542c9c80",
        "title": "Deep Reinforcement Learning for Industrial Insertion Tasks with Visual Inputs and Natural Rewards"
      },
      {
        "paperId": "9bec2a4576bb0a1bb5c80852c70a7fdf5dfad5e2",
        "title": "Autonomous Open-Ended Learning of Interdependent Tasks"
      },
      {
        "paperId": "6570c7cab46d2b0f3315f5abfbbd209140529c8b",
        "title": "Hierarchical Policy Learning is Sensitive to Goal Space Design"
      },
      {
        "paperId": "b48390dcdaa931919e1a1f36ffe404117b44f63f",
        "title": "Online Hybrid Learning to Speed Up Deep Reinforcement Learning Method for Commercial Aircraft Control"
      },
      {
        "paperId": "6b0d807e205cf26c3fc0d378942ab60ea50e5513",
        "title": "Learning Deep Visuomotor Policies for Dexterous Hand Manipulation"
      },
      {
        "paperId": "db5dd02e2ca2b27b8870d547e4886a12b26ffc20",
        "title": "Bayesian Gaussian Mixture Model for Robotic Policy Imitation"
      },
      {
        "paperId": "b97e99e246a47fa2184032da3a9503b58e9be5f6",
        "title": "Supervised Reinforcement Learning via Value Function"
      },
      {
        "paperId": "fbe22105c3dd0fa3220e58b4a167129f1e548581",
        "title": "Generative Exploration and Exploitation"
      },
      {
        "paperId": "374adc06806697f5fc6cede97fa4dfdbc5494a0d",
        "title": "Few-Shot Bayesian Imitation Learning with Logical Program Policies"
      },
      {
        "paperId": "ec1204806b1e189587fb8d15edd7cafea6e32e36",
        "title": "Improvisation through Physical Understanding: Using Novel Objects as Tools with Visual Foresight"
      },
      {
        "paperId": "970398fa7759b0559d8a6267975b6ed5ccd3f7a9",
        "title": "Safer Deep RL with Shallow MCTS: A Case Study in Pommerman"
      },
      {
        "paperId": "226bf05765f706bc71f604679295d4df0970b0a7",
        "title": "Reinforced Imitation in Heterogeneous Action Space"
      },
      {
        "paperId": "3a6705ca23f01d0a115e70cc6b9a36f9b071556c",
        "title": "Multi-Preference Actor Critic"
      },
      {
        "paperId": "65b072f590433f2c09aa2a8d566bcaa9c3b5b9a0",
        "title": "Guided Meta-Policy Search"
      },
      {
        "paperId": "dafee29eba0094dbcecca697b3164cb8ac76fddc",
        "title": "Deep Reinforcement Learning with Feedback-based Exploration"
      },
      {
        "paperId": "6006100c01372a07abb92698f6621f48b17a4b5e",
        "title": "Trajectory Optimization for Unknown Constrained Systems using Reinforcement Learning"
      },
      {
        "paperId": "15704ce8121737e3eb109a210d4b72e2fe1a0ad7",
        "title": "Hybrid Reinforcement Learning with Expert State Sequences"
      },
      {
        "paperId": "8c247678b0f652b9c8f01d200767b6b5c5199cc0",
        "title": "Open-Sourced Reinforcement Learning Environments for Surgical Robotics"
      },
      {
        "paperId": "9539673a4711775f7b8b30830293b6d19b01edcd",
        "title": "Certified Reinforcement Learning with Logic Guidance"
      },
      {
        "paperId": "754cdcf031965d49c7b7470db35ff26e4d309da0",
        "title": "Learning good policies from suboptimal demonstrations"
      },
      {
        "paperId": "e3fb7aa0bdd5bd85b101f2d22c451ee2765ff07e",
        "title": "Model-based Deep Reinforcement Learning for Dynamic Portfolio Optimization"
      },
      {
        "paperId": "e6f98139a2932e874c4ea8ce90a70a92e9d45228",
        "title": "Deep Reinforcement Learning for Soft, Flexible Robots: Brief Review with Impending Challenges"
      },
      {
        "paperId": "03fdf3abf8d6bb3ff35dc87742ad66722997caeb",
        "title": "Vision-Based Navigation With Language-Based Assistance via Imitation Learning With Indirect Intervention"
      },
      {
        "paperId": "5d4ebbaa1ef8feeac885e2869f45c0276c18834f",
        "title": "Learning Montezuma's Revenge from a Single Demonstration"
      },
      {
        "paperId": "ae4d32f05cf40e4cc01c69d7787149a258c95eda",
        "title": "Residual Reinforcement Learning for Robot Control"
      },
      {
        "paperId": "5285cb8faada5de8a92a47622950f6cfd476ac1d",
        "title": "Off-Policy Deep Reinforcement Learning without Exploration"
      },
      {
        "paperId": "5c72b6c5300348b603b315f5025ace9a015fe75f",
        "title": "Using Monte Carlo Tree Search as a Demonstrator within Asynchronous Deep RL"
      },
      {
        "paperId": "d1bcc3f6063b60f760542d76d2fbdf11d182c67a",
        "title": "Deep Reinforcement Learning for Soft Robotic Applications: Brief Overview with Impending Challenges"
      },
      {
        "paperId": "3e6cde685fdf321d7edf9319f7b07c01ff79c11a",
        "title": "Reward learning from human preferences and demonstrations in Atari"
      },
      {
        "paperId": "3994dbf6fb01e537a9d9d74b2cfb70b11a0c1009",
        "title": "Coordinating Disaster Emergency Response with Heuristic Reinforcement Learning"
      },
      {
        "paperId": "31ed72a18ed8a1008786130af9f1d61761cff4f3",
        "title": "DQN-TAMER: Human-in-the-Loop Reinforcement Learning with Intractable Feedback"
      },
      {
        "paperId": "55d917a92f0e8044b99c22b743acbc8e05735d3e",
        "title": "Sample-Efficient Learning of Nonprehensile Manipulation Policies via Physics-Based Informed State Distributions"
      },
      {
        "paperId": "cda470bede832f2965e594f9bdee79d6973a91e9",
        "title": "ROBOTURK: A Crowdsourcing Platform for Robotic Skill Learning through Imitation"
      },
      {
        "paperId": "3f56ac0e4b881d25268e83961b93ee95f2807bfb",
        "title": "CURIOUS: Intrinsically Motivated Modular Multi-Goal Reinforcement Learning"
      },
      {
        "paperId": "de1609a446456b1b0e575daf8985d3727c5e967d",
        "title": "CURIOUS: Intrinsically Motivated Multi-Task, Multi-Goal Reinforcement Learning"
      },
      {
        "paperId": "64ecc055b5b84d5f0843ad3eef98deedee4bba85",
        "title": "Dexterous Manipulation with Deep Reinforcement Learning: Efficient, General, and Low-Cost"
      },
      {
        "paperId": "618ed26b8323791ac0db534b555cb4aeec9aca3e",
        "title": "A Practical Approach to Insertion with Variable Socket Position Using Deep Reinforcement Learning"
      },
      {
        "paperId": "78745d33e99038bcdb9025d1af866a8f4deeb1b5",
        "title": "Time Reversal as Self-Supervision"
      },
      {
        "paperId": "fb35e39246ff599614bc4c4a9db5ef4438af4c08",
        "title": "Stage-Wise Learning of Reaching Using Little Prior Knowledge"
      },
      {
        "paperId": "776f3d2250285ac03b2019ecf18668fcdd72a9ce",
        "title": "One-Shot High-Fidelity Imitation: Training Large-Scale Deep Nets with RL"
      },
      {
        "paperId": "cc82b74a6f8eb35e10646dafa9507805d333ccc0",
        "title": "Cooperative and Competitive Reinforcement and Imitation Learning for a Mixture of Heterogeneous Learning Modules"
      },
      {
        "paperId": "45371436c11e5f6f42bb2b8524a1c663794ff1fe",
        "title": "Where Off-Policy Deep Reinforcement Learning Fails"
      },
      {
        "paperId": "365ee41de6d92d6557864e183ca104cb677a0122",
        "title": "Automata Guided Reinforcement Learning With Demonstrations"
      },
      {
        "paperId": "4fe9142a47b35638edcc59013ad5e9e87bd93ea7",
        "title": "PRIMAL: Pathfinding via Reinforcement and Imitation Multi-Agent Learning"
      },
      {
        "paperId": "62d76acb8eb4a8890b7fba143843a908a247531f",
        "title": "Discriminator-Actor-Critic: Addressing Sample Inefficiency and Reward Bias in Adversarial Imitation Learning"
      },
      {
        "paperId": "876053977063ab843dd24c78425cbad1779a62ed",
        "title": "Discriminator-Actor-Critic: Addressing Sample Inefficiency and Reward Bias in Adversarial Imitation Learning"
      },
      {
        "paperId": "44e197f8dbbd5b92f900337d6d18168c74ce2c3c",
        "title": "Addressing Sample Inefficiency and Reward Bias in Inverse Reinforcement Learning"
      },
      {
        "paperId": "184ea5682d5ae0e745ecc65907162f7ab0074efc",
        "title": "Task-Oriented Hand Motion Retargeting for Dexterous Manipulation Imitation"
      },
      {
        "paperId": "9b4d48eca82e8a3907b0e941a7ebd6976c95d6ae",
        "title": "Backplay: \"Man muss immer umkehren\""
      },
      {
        "paperId": "79143e26369d898563155fec3f0dcb2603f98ea0",
        "title": "Industrial Human-Robot Collaboration"
      },
      {
        "paperId": "7007b1a11b3e689396aab6711a1767de0f69d042",
        "title": "Race from Pixels: Evolving Neural Network Controller for Vision-Based Car Driving"
      },
      {
        "paperId": "c979efe1f0a8b0b343ea332368e5b51dc153c522",
        "title": "Policy Optimization with Demonstrations"
      },
      {
        "paperId": "8037db97f3cadb842ffa4bee83d59878fe7974d0",
        "title": "Sim-to-Real Reinforcement Learning for Deformable Object Manipulation"
      },
      {
        "paperId": "d397f4cf400f6ffcb1b8e3db27bb75966a0513cf",
        "title": "Self-Imitation Learning"
      },
      {
        "paperId": "170643ea1794c4285a491bcea7dede2d35806726",
        "title": "AGIL: Learning Attention from Human for Visuomotor Tasks"
      },
      {
        "paperId": "b4c8aef6cd1946d7aacd9524286637d2f825160b",
        "title": "Observe and Look Further: Achieving Consistent Performance on Atari"
      },
      {
        "paperId": "653bdfb3c35621ee04ee5d5253dc7e3a422d69e1",
        "title": "Learning Self-Imitating Diverse Policies"
      },
      {
        "paperId": "395ea8a62d84c8dd85a8dadfc3043cf2228e38c5",
        "title": "Imitating Latent Policies from Observation"
      },
      {
        "paperId": "58a44c2fcbe282a0b8fe354e33703458640b4c28",
        "title": "Fast Policy Learning through Imitation and Reinforcement"
      },
      {
        "paperId": "5a9c6942180bf1a8762e7980647909afbef56460",
        "title": "Deep learning based approaches for imitation learning"
      },
      {
        "paperId": "1b9ce6abc0f3024b88fcd4dbd0c10cf5bcf7d38d",
        "title": "DeepMimic"
      },
      {
        "paperId": "0f27f67779d50ab9ececfde980dc74542431cb5e",
        "title": "Imitation Learning with Concurrent Actions in 3D Games"
      },
      {
        "paperId": "fb9693183bc74568c72188431c18cb2b07c87213",
        "title": "Hierarchical Imitation and Reinforcement Learning"
      },
      {
        "paperId": "cab81775baae7ba2d056ebbc60437f2e03358ca3",
        "title": "Learning by Playing - Solving Sparse Reward Tasks from Scratch"
      },
      {
        "paperId": "d356a5603f14c7a6873272774782d7812871f952",
        "title": "Reinforcement and Imitation Learning for Diverse Visuomotor Skills"
      },
      {
        "paperId": "7dd8fc595afdd2097b43a5af9a8d9f5e97a65ec1",
        "title": "Reinforcement Learning on Web Interfaces Using Workflow-Guided Exploration"
      },
      {
        "paperId": "d07fe76aa2ca2337c8aa6001c8cfe296fa824109",
        "title": "Truncated Horizon Policy Search: Combining Reinforcement Learning & Imitation Learning"
      },
      {
        "paperId": "904307cb58795241b22cfaa34f560e610997f5c1",
        "title": "Divide-and-Conquer Reinforcement Learning"
      },
      {
        "paperId": "e010ba3ff5744604cdbfe44a733e2a98649ee907",
        "title": "Learning Complex Dexterous Manipulation with Deep Reinforcement Learning and Demonstrations"
      },
      {
        "paperId": "1fdf0319b4a1db62c611980351e5f4c2f08958cd",
        "title": "GEP-PG: Decoupling Exploration and Exploitation in Deep Reinforcement Learning Algorithms"
      },
      {
        "paperId": "8d218715b292f423799bb574e24bd03a6fb861dd",
        "title": "A Survey of Deep Network Solutions for Learning Control in Robotics: From Reinforcement to Imitation"
      },
      {
        "paperId": "939f77089ecc4b753b59a44fa42afca1374a4af1",
        "title": "Learn-Gen-Plan: Bridging the Gap Between Vision Language Models and Real-World Long-Horizon Dexterous Manipulations"
      },
      {
        "paperId": "fc2413e670f81d79cd49601674225d976d89b516",
        "title": "Autonomous PID Tuning: Two-Phase Reinforcement Learning Through Adversarial Imitation Learning Under Imperfect Demonstrations"
      },
      {
        "paperId": "a25a58dbde248737f64cf5fae2aa7df8c7f5d80d",
        "title": "Modifiedment the Performance of Q-learning Algorithm Based on Parameters Setting for Optimal Path Planning"
      },
      {
        "paperId": "aa42b63924737bce4e168c6450a960382450b506",
        "title": "Leveraging Offline Data in Linear Latent Bandits"
      },
      {
        "paperId": "02930d9a116eaec470a31c6a758386276e090f55",
        "title": "A Systematic Literature Review on AI Safety: Identifying Trends, Challenges, and Future Directions"
      },
      {
        "paperId": "90bc755dbb04186ad92a83cd62514cf63b86834a",
        "title": "SAMG: State-Action-Aware Offline-to-Online Reinforcement Learning with Offline Model Guidance"
      },
      {
        "paperId": "68c2489182356842ff742e10dfaf76d6b6431300",
        "title": "Safe and Efficient Robot Learning by Biasing Exploration Towards Expert Demonstrations"
      },
      {
        "paperId": "cda17df410fdb43467b3830784fc1fdc050d4289",
        "title": "End-to-End Deep Visual Control for Mastering Needle-Picking Skills With World Models and Behavior Cloning"
      },
      {
        "paperId": "3c1f943d8edcab7d7df8229fdae8632af4cba666",
        "title": "CRISP: Curriculum inducing Primitive Informed Subgoal Prediction for Hierarchical Reinforcement Learning"
      },
      {
        "paperId": "0c50965fe451b5fc8f7c76511af6853c331d89da",
        "title": "Learning Depth Vision-Based Personalized Robot Navigation Using Latent State Representations"
      },
      {
        "paperId": "2dd9d0a2047320f44972e77c612aa491635b19db",
        "title": "Active Reinforcement Learning from Demonstration in Continuous Action Spaces"
      },
      {
        "paperId": "b451feeabf6a9ce867b7ec1b009cc638b542afcf",
        "title": "Deep Reinforcement Learning with Implicit Imitation for Lane-Free Autonomous Driving"
      },
      {
        "paperId": "8e3d4f0bf86baaf6db2f02b37cad82fdc862fdcb",
        "title": "REBEL: A Regularization-Based Solution for Reward Overoptimization in Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "ce7be018cc30bd2bb195e7f97849ef004fdc33e9",
        "title": "Two-Level Actor-Critic Using Multiple Teachers"
      },
      {
        "paperId": "b6b80286c50bd3d69b57bea2c2420e27dd055ad7",
        "title": "Overleaf Example"
      },
      {
        "paperId": "96ecf468d53457e492080cd61be6345409a69999",
        "title": "Towards Minimax Optimal Reward-free Reinforcement Learning in Linear MDPs"
      },
      {
        "paperId": "4795d08143c24abec132dc7ad3641e230d15c851",
        "title": "Hierarchical Planning for Rope Manipulation using Knot Theory and a Learned Inverse Model"
      },
      {
        "paperId": "767174a176d34419aa3b0c693cbe84fe1b9b3411",
        "title": "Ada-NAV: Adaptive Trajectory-Based Sample Efficient Policy Learning for Robotic Navigation"
      },
      {
        "paperId": "bc2e48b13978f5560ce3b238116a34093f861314",
        "title": "Efficient Multi-Task Reinforcement Learning via Selective Behavior Sharing"
      },
      {
        "paperId": "3c66bdfcddbcde02854582a8f97ca8d302924ad8",
        "title": "Vid2Act: Activate Offline Videos for Visual RL"
      },
      {
        "paperId": "546d141ecf6175e52ace36fb65d751030bb90616",
        "title": "Implicit Subgoal Planning with Variational Autoencoders for Long-Horizon Sparse Reward Robotic Tasks"
      },
      {
        "paperId": "666d4aed971e87ca46c35d4f01fce0d78b8a9793",
        "title": "Overleaf Example"
      },
      {
        "paperId": "439be199f09ffc411d9a709d6c89c5446c735b6e",
        "title": "Meta-Residual Policy Learning: Zero-Trial Robot Skill Adaptation via Knowledge Fusion"
      },
      {
        "paperId": "5bc1125502fc43cc5acc6b034aa58cccc92ab96c",
        "title": "Overcoming Exploration: Deep Reinforcement Learning in Complex Environments from Temporal Logic Specifications"
      },
      {
        "paperId": "e32eb52d152ee93fa57a894e748ccb6feacc820b",
        "title": "First do no harm: counterfactual objective functions for safe & ethical AI"
      },
      {
        "paperId": "ef777c461af99290142714acd87fd0530c295845",
        "title": "TempoRL: Temporal Priors for Exploration in Off-Policy Reinforcement Learning"
      },
      {
        "paperId": "d7988ae63e038b09c1c20ccc48f5462b76bad96b",
        "title": "Autonomous Swarm Shepherding Using Curriculum-Based Reinforcement Learning"
      },
      {
        "paperId": "33c1087b86025c7bc919f2fda817a491c0c350ab",
        "title": "Beyond Tabula Rasa: Reincarnating Reinforcement Learning"
      },
      {
        "paperId": "b6c846570b72e9095c11d46958ca7ec43c243bc6",
        "title": "Effects of Reward Shaping on Curriculum Learning in Goal Conditioned Tasks"
      },
      {
        "paperId": "f165170d0cff5760d465b8903d2184648d5d553f",
        "title": "Teaching Personalized Robot Navigation through Virtual Reality Demonstrations: A Learning Framework and User Study"
      },
      {
        "paperId": "902cb60429f8d5ed9a7becd0f6878c6d1f043628",
        "title": "Domain Adapting Speech Emotion Recognition modals to real-world scenario with Deep Reinforcement Learning"
      },
      {
        "paperId": "de436f69427e922c3c38d88791feb4a37e57f0cf",
        "title": "Relay Hindsight Experience Replay: Continual Reinforcement Learning for Robot Manipulation Tasks with Sparse Rewards"
      },
      {
        "paperId": "82022e51d007c953ba02e01070c5e7a165377b59",
        "title": "Understanding Hindsight Goal Relabeling Requires Rethinking Divergence Minimization"
      },
      {
        "paperId": "6b8187250d2466a47f67964429e4d7121c6f991f",
        "title": "Active Inference Integrated with Imitation Learning for Autonomous Driving"
      },
      {
        "paperId": "3f14defdaf32541312c1378ac4c1dcf1e685789b",
        "title": "Squeezing more value out of your historical data: data-augmented behavioural cloning as launchpad for reinforcement learning"
      },
      {
        "paperId": "e6a0fa87b8fe61f1f4bc542c167aa1f8e331329b",
        "title": "K EEP C ALM AND C ARRY O FFLINE : P OLICY R EFINEMENT IN O FFLINE R EINFORCEMENT L EARNING"
      },
      {
        "paperId": "06318722ebbac1d9b59e2408ecd3994eadb0ec6b",
        "title": "Deep Reinforcement Learning Task Assignment Based on Domain Knowledge"
      },
      {
        "paperId": "85f15232f9dc83d920cca51a4091ebd0903bb3a0",
        "title": "Autonomous scene exploration using experience enhancement"
      },
      {
        "paperId": "9eb5e294b047e399a131f86df4e88274d796e7e5",
        "title": "Efficient Reinforcement Learning (ERL): Targeted Exploration Through Action Saturation"
      },
      {
        "paperId": "973f123c58fbcb21cb8e6cff1d0894f5c53b0cc6",
        "title": "Deep Reinforcement Learning for Robotic Control with Multi-Fidelity Models"
      },
      {
        "paperId": "13b120fad7ebe58f4d2257a1ee7901c30219feaf",
        "title": "Policy Transfer via Enhanced Action Space"
      },
      {
        "paperId": "0f8296ca42d21daa709944ea041ec93552919a47",
        "title": "Will it Blend? Learning to Coexecute Subskills"
      },
      {
        "paperId": "b59d4d86b432f8c91327f3246042420fe312e0c9",
        "title": "Speeding-up Continual Learning through Information Gains in Novel Experiences"
      },
      {
        "paperId": "9010370a3dad5ba31c6748cf5f02ed6b3992b28f",
        "title": "Towards Universal Visual Reward and Representation via Value-Implicit Pre-Training"
      },
      {
        "paperId": "dc2d82aabe20585bb42f1c0e4cb59ee1c2f0a41d",
        "title": "Cluster-based Sampling in Hindsight Experience Replay for Robot Control"
      },
      {
        "paperId": "10f9a6c45a6ce04956032a0595b809db9163c868",
        "title": "Overcoming Exploration: Deep Reinforcement Learning for Continuous Navigation in Complex Environments from Temporal Logic Speci\ufb01cations"
      },
      {
        "paperId": "b3b63ceccfce398cfc85927fa7dfa5f27088a8b4",
        "title": "Accelerating Multi-Goal Reinforcement Learning by Learning from Demonstrations for Robotic Manipulation Tasks"
      },
      {
        "paperId": "261ada5d3d3a7653faf91194c10f61a098908171",
        "title": "Replay Buffer start Strategy a ) Strategy b ) EncoderEncoder Encoder Encoder EncoderGoal Goal DBTaskDemonstrationsif successful Online Goal Selection"
      },
      {
        "paperId": "a922fdc66286ded6480e330a9364c45cf02e7b91",
        "title": "W ISH YOU WERE HERE : H INDSIGHT G OAL S ELECTION FOR LONG - HORIZON DEXTEROUS MANIPULATION"
      },
      {
        "paperId": "77b6eb9a171f149ba7c0998f59bb1741d4814ce6",
        "title": "Interpretable Policy Specification and Synthesis through Natural Language and RL"
      },
      {
        "paperId": "6bc7238d8b27de65aa862529ea7b833babf340da",
        "title": "Ensemble Bootstrapped Deep Deterministic Policy Gradient for Vision-Based Robotic Grasping"
      },
      {
        "paperId": "fb9486b9398132dec86e2b3d9cdf586621c227af",
        "title": "Hybrid Trajectory and Force Learning of Complex Assembly Tasks: A Combined Learning Framework"
      },
      {
        "paperId": "3e7044375e70afe3c314c59a5439b242aec08712",
        "title": "Imitation Learning: Progress, Taxonomies and Opportunities"
      },
      {
        "paperId": "52bf6897fade8088fd825aa266ed713e21404f7b",
        "title": "Transfer Learning with Demonstration Forgetting for Robotic Manipulator"
      },
      {
        "paperId": "727d2d5fe17a29f7b32117645ba4c2d2d6309f54",
        "title": "Learning to Compose Behavior Primitives for Near-Decomposable Manipulation Tasks"
      },
      {
        "paperId": "96402a85bec09486998b623f67f13e2e99f300d7",
        "title": "Prioritized Experience-based Reinforcement Learning with Human Guidance: Methdology and Application to Autonomous Driving"
      },
      {
        "paperId": "764b6780f61677fc66468235be7d8e9c3981f8c7",
        "title": "A Comparison of Two Reinforcement Learning Algorithms for Robotic Pick and Place with Non-Visual Sensing"
      },
      {
        "paperId": "c384e0b77e2c542ef870644d6f92f872578c4b6c",
        "title": "Re-exploration of \u03b5-Greedy in Deep Reinforcement Learning"
      },
      {
        "paperId": "71e15ff25b382afb433f7373cfe16d0647957d1d",
        "title": "Redundancy Resolution as Action Bias in Policy Search for Robotic Manipulation"
      },
      {
        "paperId": "0047f2c7052e1aa02803fe5f9c4fd743f3990567",
        "title": "Demonstration-Guided Q-Learning"
      },
      {
        "paperId": "eae3a8c4d8201489ec8bfca397a2c9b2b6a3128d",
        "title": "pu ta \u00e7\u00e3 o End-to-End Visual Obstacle Avoidance for a Robotic Manipulator using Deep Reinforcement Learning"
      },
      {
        "paperId": "3e2e99f2e6157f7b9944602b8a18d65177bf3526",
        "title": "Evaluation of Human Demonstration Augmented Deep Reinforcement Learning Policies via Object Manipulation with an Anthropomorphic Robot Hand"
      },
      {
        "paperId": "2128c5dc3537d423910387963b64c69aafcf8635",
        "title": "Stochastic Policy Optimization with Heuristic Information for Robot Learning"
      },
      {
        "paperId": "3d6ae7ada6368cd14b61ba89c64705ac7ede1f7a",
        "title": "Learning Search Guidance from Failures with Eliminable Edge Sets"
      },
      {
        "paperId": "ffaa75ef6ee5ac43671076c00cc911081dd8c02f",
        "title": "Dynamic Robotic Cloth Manipulation"
      },
      {
        "paperId": "91329f5b8e60ca064b58649ddac3487b7100ed9b",
        "title": "Artificial Neural Networks and Machine Learning \u2013 ICANN 2020: 29th International Conference on Artificial Neural Networks, Bratislava, Slovakia, September 15\u201318, 2020, Proceedings, Part II"
      },
      {
        "paperId": "905866bb7ec4070d696a79788adc727858227994",
        "title": "Effect of Demonstrations for Deep Reinforcement Learning-Based Control of Concentric Tube Robots"
      },
      {
        "paperId": "6f854a55bd35cfd5308a79480448de936aa9fd55",
        "title": "Bootstrapping Baysian Inverse Reinforcement Learning in Robotics through VR Demonstration"
      },
      {
        "paperId": "faba141483180e53f461c6035ce95041cfed9a8f",
        "title": "Playing Games with Implicit Human Feedback"
      },
      {
        "paperId": "3be26bc9dfa1b83c0484e91c2b8145f1bdb8e22d",
        "title": "End to End Learning in Autonomous Driving Systems"
      },
      {
        "paperId": "d3c4791dc083f4a2f0481a59620421166b976ed4",
        "title": "OCT Guided Robotic Ophthalmic Microsurgery via Reinforcement Learning from Demonstration"
      },
      {
        "paperId": "ea42d5e2a9a8430444b58b1ef8ed801e27ffb537",
        "title": "Investigation of Near-accident Car-driving Scenario using Deep Imitation Learning and Reinforcement Learning"
      },
      {
        "paperId": "26ef18d87b200c5a00b0592609cba7b527a4d8d6",
        "title": "Distributed Evolution"
      },
      {
        "paperId": "24138fe78c47e12dd6c7734600210040a49be418",
        "title": "Batch Prioritization in Multigoal Reinforcement Learning"
      },
      {
        "paperId": "b870d15756cf728ed652ece2bfb19bb51aa89687",
        "title": "Review of Deep Reinforcement Learning-Based Object Grasping: Techniques, Open Challenges, and Recommendations"
      },
      {
        "paperId": "01313ad53e08a173e4329daaf402cd8bb1698f81",
        "title": "Towards Adaptive Enterprise"
      },
      {
        "paperId": "33682275f7b6004c42586eab5118c9d8ec277431",
        "title": "Deep Reinforcement Learning for Traffic Signal Control: A Review"
      },
      {
        "paperId": "e0dfecf181ec4524c6bf9c2e516521c0582d7a99",
        "title": "Smart Start and HER for a Directed and Persistent Reinforcement Learning Exploration in Discrete Environment"
      },
      {
        "paperId": "300bd40c90eb2feed6f8f2aa487df50b92e22213",
        "title": "A TTENTION - DRIVEN R OBOTIC M ANIPULATION"
      },
      {
        "paperId": "cbdc16d0e6a62062e07d677e041f6ef4b007f072",
        "title": "Teleoperation System for Teaching Dexterous Manipulation"
      },
      {
        "paperId": "86b6a60fafe039dedd36c42b3dc220d40cf8bc9d",
        "title": "Demonstration Guided Actor-Critic Deep Reinforcement Learning for Fast Teaching of Robots in Dynamic Environments"
      },
      {
        "paperId": "a0f54431e5d2a9357191e0db576d79ef3209734d",
        "title": "A LIGN -RUDDER: L EARNING F ROM F EW D EMON - STRATIONS BY R EWARD R EDISTRIBUTION"
      },
      {
        "paperId": "a5ccad014463d55e740d7910e8e33a2eb269d0c0",
        "title": "Imitation Learning"
      },
      {
        "paperId": "98bda1861201f3baa39992db7078e08f680e32ed",
        "title": "Apprentissage par Reinforcement profond guide\u0301 par ensemble de politiques sources"
      },
      {
        "paperId": "d1060aabd99a96f191be228b7176377c3a6e8de9",
        "title": "Towards Practical Robot Manipulation using Relational Reinforcement Learning"
      },
      {
        "paperId": "cd5da832b632a383ad306b25a67d7e97dec04ae2",
        "title": "Combining Model-Based with Learning-Based Approaches for Autonomous Manipulation"
      },
      {
        "paperId": "05550696a0a1f2ca9c444aada0011019d9ce84f9",
        "title": "Deep Reinforcement Learning With Optimized Reward Functions for Robotic Trajectory Planning"
      },
      {
        "paperId": "2ad7467f34745f17266019d0d75656609a7c5cdf",
        "title": "Rewards Prediction-Based Credit Assignment for Reinforcement Learning With Sparse Binary Rewards"
      },
      {
        "paperId": "5ea079d76dd5b067cebe8037184acd047cff8344",
        "title": "HOW USEFUL IS QUANTILIZATION FOR MITIGATING SPECIFICATION-GAMING?"
      },
      {
        "paperId": "a284d21befa6fe145dce747353c7e7fe22d70bb0",
        "title": "Transferring Human Manipulation Knowledge to Industrial Robots Using Reinforcement Learning"
      },
      {
        "paperId": "ce6654a4488dc94eb3a60eba5b2f1f5b3875e76d",
        "title": "Robot Learning from Demonstration: A Review of Recent Advances"
      },
      {
        "paperId": "c0b1d43f03c4d959a0bf57ab03ff6ef83f849872",
        "title": "Intrinsically Motivated and Interactive Reinforcement Learning: a Developmental Approach. (Apprentissage par Renforcement Intrins\u00e8quement Motiv\u00e9 et Interactif: une approche d\u00e9veloppementale)"
      },
      {
        "paperId": "f52449c24b55852bff87205f50d30339166ba8df",
        "title": "Policy Continuation and Policy Evolution with Hindsight Inverse Dynamics"
      },
      {
        "paperId": "770d79c89ca27de2efd143b16a81fe98497820a5",
        "title": "Swarm-inspired Reinforcement Learning via Collaborative Inter-agent Knowledge Distillation"
      },
      {
        "paperId": "a1a536d92551b6690761e36f413f53e8ce923d33",
        "title": "Sense, Think, Grasp: A study on visual and tactile information processing for autonomous manipulation"
      },
      {
        "paperId": "1abe6a0b0731129b5ba40e7af92fd86e48c7c033",
        "title": "Reinforcement Learning for Dexterity Transfer Between Manipulators"
      },
      {
        "paperId": "f27c73a0996d2ed89c052d82689bb1759705017f",
        "title": "Deep Reinforcement Learning for Adaptive Human Robotic Collaboration"
      },
      {
        "paperId": "d47c25951e15a0d51aa2f132bed189d0c8e420fc",
        "title": "Integration of Imitation Learning using GAIL and Reinforcement Learning using Task-achievement Rewards via Probabilistic Generative Model"
      },
      {
        "paperId": "5c90f7c36065bcf1c2e75252a1c87001cab8d985",
        "title": "ICLR 2019 Expert Replay Buffer ActorEnvironment Discriminator Policy Replay Buffer Absorbing State Wrapper s a Critic"
      },
      {
        "paperId": "b6091229cf81a7736b805bb2753cc32cdc41f38e",
        "title": "Model-based Deep Reinforcement Learning for Financial Portfolio Optimization"
      },
      {
        "paperId": "a3082db4b1020de3d3d73395fbc9204ac4add1a5",
        "title": "Continuous control for robot based on deep reinforcement learning"
      },
      {
        "paperId": "c8702335d72eaeb2319d0dd3ddfb9a226aa1e337",
        "title": "Fast Robot Learning using Prospection and Experimental Knowledge : A Cognitive Approach with Narrative-Enabled Episodic Memories and Symbolic Knowledge"
      },
      {
        "paperId": "e2d0a5005d945278b5b0b0b57261f9a09a47f43c",
        "title": "Shaping Rewards for Combined Reinforcement and Imitation Learning using Generative Models"
      },
      {
        "paperId": "77a31a4601444a3f7aeed15061b08684d0bea92b",
        "title": "Exploration-Exploitation Trade-off in Deep Reinforcement Learning"
      },
      {
        "paperId": "4b574c1f9d1d8667b6ce8ccf8a96145b3215b3bb",
        "title": "UGV Navigation Optimization Aided by Reinforcement Learning-Based Path Tracking"
      },
      {
        "paperId": "c9ac9632fcfb250b878e834140d2c5096c5284f5",
        "title": "Curriculum Distillation to Teach Playing Atari"
      },
      {
        "paperId": "6dad90f530cb9aa0deec5fa232155dab539d1b49",
        "title": "Action Permissibility in Deep Reinforcement Learning and Application to Autonomous Driving"
      },
      {
        "paperId": "1e9617fed4fa147e07ae68776ef70aaf386b52c9",
        "title": "Efficient Deep Reinforcement Learning via Planning, Generalization, and Improved Exploration"
      },
      {
        "paperId": "8849e2c68a33515e98e9b188d518a86edf5d418e",
        "title": "Predict futures using video prediction model Training Time Test Time Collect Kinesthetic Demonstrations Autonomous Random Data"
      },
      {
        "paperId": null,
        "title": "Deep Reinforcement Learning Off-policy Algorithms and Benchmark for Solving Various Robotic Manipulator Tasks"
      },
      {
        "paperId": "a69e2e4abf3006f5d448b65a2e92ffb3dd986e66",
        "title": "Research Article Learning Diverse Policies with Soft Self-Generated Guidance"
      },
      {
        "paperId": "7ae2c602a163d0afd05bfbd7bd1f8820133a227e",
        "title": "Interactive Imitation Learning as Reinforcement Learning"
      },
      {
        "paperId": "e9dbab1db7d69f14534028f21d7759d2ffb7e4a7",
        "title": "Optimizing Crowd-Aware Multi-Agent Path Finding through Local Broadcasting with Graph Neural Networks"
      },
      {
        "paperId": "96e4756aa71bf003603fb999bae634a5840292d1",
        "title": "Real-Time Legged Locomotion Control with Diffusion from Offline Datasets"
      },
      {
        "paperId": "8c39be17f63e310c8052af533d519fa61761de6b",
        "title": "PBCS: E\ufb00icient Exploration and Exploitation Using a Synergy Between Reinforcement Learning and Motion Planning"
      },
      {
        "paperId": "c6277e0dce2989d1c672498c1306a8b2a2349fd1",
        "title": "WIP: Human-AI interactions in real-world complex environments using a comprehensive reinforcement learning framework"
      },
      {
        "paperId": "52a44c6eaa20ef6c54d7c2761a0016c3a7fd982b",
        "title": "Inverse Reinforcement Learning from Demonstrations for LLM Alignment"
      },
      {
        "paperId": "6170a539eca8ce30a1e23bcea3bded24f17f561a",
        "title": "O FFLINE R EINFORCEMENT L EARNING WITH C LOSED - LOOP P OLICY E VALUATION AND D IFFUSION W ORLD - M ODEL A DAPTATION"
      },
      {
        "paperId": "abcfdf85b669324ed631638e2335441a30d5ce73",
        "title": "An Efficient Open World Benchmark for Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "641589804241c21da823a03d656ad8b6aec918ca",
        "title": "Imitation guided Automated Red Teaming"
      },
      {
        "paperId": "db42ceedc831ff27794c8989182bb800f0cc7ba5",
        "title": "Transfer Learning, Reinforcement Learning for Adaptive Control Optimization under Distribution Shift"
      },
      {
        "paperId": "2a42e68da6fe3b8d6e3b492b800bd83b3bdf59a0",
        "title": "Deep Reinforcement Algorithms in RIS-Empowered Wireless Communication Systems"
      },
      {
        "paperId": "e5b7522071d1fd462f3ca3aa7781ae6499e45b93",
        "title": "Goal-Conditioned Reinforcement Learning for Surgical Robotic Manipulation"
      }
    ],
    "score": 101.75
  },
  {
    "id": "0fcb2034e31a2bc2f12a2b1363d0d77baf445fdf",
    "title": "#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning",
    "authors": [
      "Haoran Tang",
      "Rein Houthooft",
      "Davis Foote",
      "Adam Stooke",
      "Xi Chen",
      "Yan Duan",
      "John Schulman",
      "F. Turck",
      "P. Abbeel"
    ],
    "year": 2016,
    "citationCount": 798,
    "abstract": "Count-based exploration algorithms are known to perform near-optimally when used in conjunction with tabular reinforcement learning (RL) methods for solving small discrete Markov decision processes (MDPs). It is generally thought that count-based methods cannot be applied in high-dimensional state spaces, since most states will only occur once. Recent deep RL exploration strategies are able to deal with high-dimensional continuous state spaces through complex heuristics, often relying on optimism in the face of uncertainty or intrinsic motivation. In this work, we describe a surprising finding: a simple generalization of the classic count-based approach can reach near state-of-the-art performance on various high-dimensional and/or continuous deep RL benchmarks. States are mapped to hash codes, which allows to count their occurrences with a hash table. These counts are then used to compute a reward bonus according to the classic count-based exploration theory. We find that simple hash functions can achieve surprisingly good results on many challenging tasks. Furthermore, we show that a domain-dependent learned hash code may further improve these results. Detailed analysis reveals important aspects of a good hash function: 1) having appropriate granularity and 2) encoding information relevant to solving the MDP. This exploration strategy achieves near state-of-the-art performance on both continuous control tasks and Atari 2600 games, hence providing a simple yet powerful baseline for solving MDPs that require considerable exploration.",
    "url": "https://www.semanticscholar.org/paper/0fcb2034e31a2bc2f12a2b1363d0d77baf445fdf",
    "pdf_url": "https://arxiv.org/pdf/1611.04717.pdf",
    "venue": "Neural Information Processing Systems",
    "publicationDate": "2016-11-15",
    "externalIds": {
      "MAG": "2949475445",
      "ArXiv": "1611.04717",
      "DBLP": "journals/corr/TangHFSCDSTA16",
      "CorpusId": 14357699
    },
    "references": [
      {
        "paperId": "4ba25cb493ac7a03fc15d3b936257c9a6c689c1d",
        "title": "Strategic Attentive Writer for Learning Macro-Actions"
      },
      {
        "paperId": "6e90fd78e8a3b98af3954aae5209703aa966603e",
        "title": "Unifying Count-Based Exploration and Intrinsic Motivation"
      },
      {
        "paperId": "317cd4522b1f4a6f889743578143bb8823623f8b",
        "title": "VIME: Variational Information Maximizing Exploration"
      },
      {
        "paperId": "bda89e0d181eda7e49ea831225eda86d075e111c",
        "title": "Towards Conceptual Compression"
      },
      {
        "paperId": "1464776f20e2bccb6182f183b5ff2e15b0ae5e56",
        "title": "Benchmarking Deep Reinforcement Learning for Continuous Control"
      },
      {
        "paperId": "2b3ec72786da1853fc379e9494a73bcce88f47b8",
        "title": "Exploratory Gradient Boosting for Reinforcement Learning in Complex Domains"
      },
      {
        "paperId": "53d8cd298711a85dd89c9814c989f7fae7500d87",
        "title": "Learning functions across many orders of magnitudes"
      },
      {
        "paperId": "4931c91f4b30eb122def1e697abc096f14c48987",
        "title": "Learning values across many orders of magnitude"
      },
      {
        "paperId": "4b63e34276aa98d5345efa7fe09bb06d8a9d8f52",
        "title": "Deep Exploration via Bootstrapped DQN"
      },
      {
        "paperId": "69e76e16740ed69f4dc55361a3d319ac2f1293dd",
        "title": "Asynchronous Methods for Deep Reinforcement Learning"
      },
      {
        "paperId": "41f1d50c85d3180476c4c7b3eea121278b0d8474",
        "title": "Pixel Recurrent Neural Networks"
      },
      {
        "paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d",
        "title": "Deep Residual Learning for Image Recognition"
      },
      {
        "paperId": "4c05d7caa357148f0bbd61720bdd35f0bc05eb81",
        "title": "Dueling Network Architectures for Deep Reinforcement Learning"
      },
      {
        "paperId": "ea6d73a33ac2109ed095963e25b857a9350449bd",
        "title": "Bayesian Reinforcement Learning: A Survey"
      },
      {
        "paperId": "3b9732bb07dc99bde5e1f9f75251c6ea5039373e",
        "title": "Deep Reinforcement Learning with Double Q-Learning"
      },
      {
        "paperId": "024006d4c2a89f7acacc6e4438d156525b60a98f",
        "title": "Continuous control with deep reinforcement learning"
      },
      {
        "paperId": "d5ed07113ddcd038062525a5a54550c012ac9a74",
        "title": "Massively Parallel Methods for Deep Reinforcement Learning"
      },
      {
        "paperId": "2470fcf0f89082de874ac9133ccb3a8667dd89a8",
        "title": "Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models"
      },
      {
        "paperId": "340f48901f72278f6bf78a04ee5b01df208cc508",
        "title": "Human-level control through deep reinforcement learning"
      },
      {
        "paperId": "449532187c94af3dd3aa55e16d2c50f7854d2199",
        "title": "Trust Region Policy Optimization"
      },
      {
        "paperId": "995c5f5e62614fcb4d2796ad2faab969da51713e",
        "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"
      },
      {
        "paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8",
        "title": "Adam: A Method for Stochastic Optimization"
      },
      {
        "paperId": "19cb18335c0ef439882d9e72ffa237c52886801c",
        "title": "Bayes-Adaptive Simulation-based Search with Value Function Approximation"
      },
      {
        "paperId": "eb42cf88027de515750f230b23b1a057dc782108",
        "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition"
      },
      {
        "paperId": "1389772b8a0f9c7fc43057f9da41a7d0ebf0308b",
        "title": "Generalization and Exploration via Randomized Value Functions"
      },
      {
        "paperId": "d9eeb1277a4cea35a2c1a147dfcd4b41b83ecacd",
        "title": "PAC Optimal Exploration in Continuous Space Markov Decision Processes"
      },
      {
        "paperId": "abd1c342495432171beb7ca8fd9551ef13cbd0ff",
        "title": "ImageNet classification with deep convolutional neural networks"
      },
      {
        "paperId": "f82e4ff4f003581330338aaae71f60316e58dd26",
        "title": "The Arcade Learning Environment: An Evaluation Platform for General Agents"
      },
      {
        "paperId": "5df39cc393907ea78fddf461b494b4c5b1b5a2e4",
        "title": "Planning to Be Surprised: Optimal Bayesian Exploration in Dynamic Environments"
      },
      {
        "paperId": "33224ad0cdf6e2dc4893194dd587309c7887f0ba",
        "title": "Formal Theory of Creativity, Fun, and Intrinsic Motivation (1990\u20132010)"
      },
      {
        "paperId": "3ebc7ec2a9b89ebcc00328b11a1984d69a57f2a7",
        "title": "DAISY: An Efficient Dense Descriptor Applied to Wide-Baseline Stereo"
      },
      {
        "paperId": "cd5af41a81e7fc9588dc74f3831fb14daf2f8e2a",
        "title": "Semantic hashing"
      },
      {
        "paperId": "4282669947ca340de9f93a9a2a38007fe542195d",
        "title": "Near-Bayesian exploration in polynomial time"
      },
      {
        "paperId": "0cafe2903b097fc042782c359cb231ea34ef7ed3",
        "title": "Near-optimal Regret Bounds for Reinforcement Learning"
      },
      {
        "paperId": "237a1cf18ed83bb3ad852b34f443c6c1ff3336c1",
        "title": "An analysis of model-based Interval Estimation for Markov Decision Processes"
      },
      {
        "paperId": "e8077729ef723d71a0b7492f2f271ae413b5a4d5",
        "title": "What is Intrinsic Motivation? A Typology of Computational Approaches"
      },
      {
        "paperId": "e17529924798975856310a75cb3df3066ac7ccfa",
        "title": "Near-Optimal Hashing Algorithms for Approximate Nearest Neighbor in High Dimensions"
      },
      {
        "paperId": "63e2ca52df9c2ef5728e6326dcc74755ac3161ca",
        "title": "A theoretical analysis of Model-Based Interval Estimation"
      },
      {
        "paperId": "e8b12467bdc20bde976750b8a28decdb33246d1d",
        "title": "Histograms of oriented gradients for human detection"
      },
      {
        "paperId": "cd873347660c2af6a70d623a9fb265893e64c98d",
        "title": "An improved data stream summary: the count-min sketch and its applications"
      },
      {
        "paperId": "dc649486b881e672eea6546da48c46e1f98daf32",
        "title": "Near-Optimal Reinforcement Learning in Polynomial Time"
      },
      {
        "paperId": "42bb0ac384fb87933be67f63b98d90a45d2fe6e9",
        "title": "Similarity estimation techniques from rounding algorithms"
      },
      {
        "paperId": "56ebfeddc20fd9c17e64c4699c68eea6402176ac",
        "title": "Summary cache: a scalable wide-area web cache sharing protocol"
      },
      {
        "paperId": "f9f836d28f52ad260213d32224a6d227f8e8849a",
        "title": "Object recognition from local scale-invariant features"
      },
      {
        "paperId": "4c9e36a0dbdc3e7b20e38bd288a804c05c77e9fa",
        "title": "Asymptotically efficient adaptive allocation rules"
      },
      {
        "paperId": "f39a2c11983b21fd5054d5393614959bfbc4e50f",
        "title": "Space/time trade-offs in hash coding with allowable errors"
      },
      {
        "paperId": "0c2563caba6bcdb113817d560ce9492467e45873",
        "title": "Under review as a conference paper at ICLR 2020 many domain adaptation methods"
      },
      {
        "paperId": null,
        "title": "The tasks have the following state and action dimensions: CartPoleSwingup"
      },
      {
        "paperId": null,
        "title": "2016), following the sparse reward adaptation"
      },
      {
        "paperId": "c7dbf5ed7e9b63e104adb4e18bfbc98f5d6afdae",
        "title": "R-MAX - A General Polynomial Time Algorithm for Near-Optimal Reinforcement Learning"
      },
      {
        "paperId": "572379c1d56e7e19422ae38218ee228c61aefb2f",
        "title": "Asymptotically Efficient Adaptive Allocation Rules"
      }
    ],
    "cited_by": [
      {
        "paperId": "34b3cad03ec8d2ff83421dd0261a2ef2a24e2ea4",
        "title": "Learngene: Inheritable \u201cgenes\u201d in intelligent agents"
      },
      {
        "paperId": "d4cc1918e071be4148579a4984ee2a5ea682e31e",
        "title": "Learn the Ropes, Then Trust the Wins: Self-imitation with Progressive Exploration for Agentic Reinforcement Learning"
      },
      {
        "paperId": "b69f7136ed671c53da6588d29bec17407ed6e2e9",
        "title": "Leveraging Temporally Extended Behavior Sharing for Multi-task Reinforcement Learning"
      },
      {
        "paperId": "37fe2a997bf07a972473abd079d175335940e6bd",
        "title": "CDE: Curiosity-Driven Exploration for Efficient Reinforcement Learning in Large Language Models"
      },
      {
        "paperId": "c84c48399db70e99eadb5f081e91a9c4a29a2267",
        "title": "What Fundamental Structure in Reward Functions Enables Efficient Sparse-Reward Learning?"
      },
      {
        "paperId": "22b711eb36f408251070c31ae5107fa79375ef82",
        "title": "Uncertainty-driven Adaptive Exploration"
      },
      {
        "paperId": "905cb1672c1d685c171acd8a1ed6db59e671409a",
        "title": "BILE: An Effective Behavior-based Latent Exploration Scheme for Deep Reinforcement Learning"
      },
      {
        "paperId": "974784443bc46fdbd0a66becae859511b39887b2",
        "title": "Know When to Explore: Difficulty-Aware Certainty as a Guide for LLM Reinforcement Learning"
      },
      {
        "paperId": "d7ab15be4c5d6e0921407ac2d7754c7db4625771",
        "title": "Physical State Exploration for Reinforcement Learning from Scratch"
      },
      {
        "paperId": "edc017668408b4fc01501777f7bd29ddb70b13b9",
        "title": "Value Function Initialization for Knowledge Transfer and Jump-start in Deep Reinforcement Learning"
      },
      {
        "paperId": "f396247ebd0f4e9b487f352e9afb3b59ac0aa495",
        "title": "Exploitation Is All You Need... for Exploration"
      },
      {
        "paperId": "8bc4fd71b24982d68245450f59242faedb21b8ab",
        "title": "Is Exploration or Optimization the Problem for Deep Reinforcement Learning?"
      },
      {
        "paperId": "fe3dd3668cf1c17f18ee60e96988f58beab5d2ca",
        "title": "On Efficient Bayesian Exploration in Model-Based Reinforcement Learning"
      },
      {
        "paperId": "cd347ac9272358ec6b77115f22941d96cea51039",
        "title": "Zero-Incentive Dynamics: a look at reward sparsity through the lens of unrewarded subgoals"
      },
      {
        "paperId": "495e564fd62c9df25b605f29c50a8b05d79398c8",
        "title": "Exploring Large Action Sets with Hyperspherical Embeddings using von Mises-Fisher Sampling"
      },
      {
        "paperId": "392061e0d88393012c577f632b22e67536f65377",
        "title": "Data-Driven Exploration for a Class of Continuous-Time Indefinite Linear-Quadratic Reinforcement Learning Problems"
      },
      {
        "paperId": "c0041a02da6eca290b2b9bf44bae44dfb32878b3",
        "title": "Diverse Mini-Batch Selection in Reinforcement Learning for Efficient Chemical Exploration in de novo Drug Design"
      },
      {
        "paperId": "6d526e1e467df933014832c3f24866817ccff651",
        "title": "Learning Action Primitives in Manipulation Robots"
      },
      {
        "paperId": "845e7b768dd3b846be8e2aa5a001d57fcc635a90",
        "title": "Uncertainty Prioritized Experience Replay"
      },
      {
        "paperId": "5b576bebf7d18d8dc2ed90ef53cab0a8a283d1a6",
        "title": "Exploration by Random Reward Perturbation"
      },
      {
        "paperId": "388d3875fd7f159779620506f0bcb8a0c087295b",
        "title": "Scalable and Cost-Efficient de Novo Template-Based Molecular Generation"
      },
      {
        "paperId": "6e19a3ed0f582faa74d3909aaf74d8997100739f",
        "title": "Reinforcement Learning via Implicit Imitation Guidance"
      },
      {
        "paperId": "c82cd7dded7aacd4fa4598c49577ba7fcf946ec8",
        "title": "A Generative Physics-Informed Reinforcement Learning-Based Approach for Construction of Representative Drive Cycle"
      },
      {
        "paperId": "edafc73aa1a9eddf603c23af7efa7af6b7894332",
        "title": "Efficient Adversarially Guided Actor\u2013Critic"
      },
      {
        "paperId": "2109255b50868c453d397bc732fc44d8e83a164f",
        "title": "SCAR: Shapley Credit Assignment for More Efficient RLHF"
      },
      {
        "paperId": "760389d05f01f9acc7258b8ba00797d835f25044",
        "title": "DISCOVER: Automated Curricula for Sparse-Reward Reinforcement Learning"
      },
      {
        "paperId": "ec31a340ea5a2b2c7abad58ac9aae4e1df99e30d",
        "title": "Counter-Inferential Behavior in Natural and Artificial Cognitive Systems"
      },
      {
        "paperId": "536d6a3e9138647ca9935d40ce8fd0d6a0fc7344",
        "title": "Exploration by Random Distribution Distillation"
      },
      {
        "paperId": "675dce487447b58e2d41dff4b41f0dbaae54b051",
        "title": "Reward-Guided Subspace Fusion and Spotlight for Multi-Agent State Space Exploration"
      },
      {
        "paperId": "08941f8975a59bc21a89c9fca9ccf71d475a747b",
        "title": "Parameter Estimation using Reinforcement Learning Causal Curiosity: Limits and Challenges"
      },
      {
        "paperId": "87c184aa393900f90b24ff7c6c5bd8126ac57894",
        "title": "Credit Assignment and Efficient Exploration based on Influence Scope in Multi-agent Reinforcement Learning"
      },
      {
        "paperId": "052136a967f6dda0fcba463563c920fc81b1b1fa",
        "title": "Enhancing Cooperative Multi-Agent Reinforcement Learning with State Modelling and Adversarial Exploration"
      },
      {
        "paperId": "9f95fa0ed30c8c7d9b0b0b3d7d6471fa92b14d61",
        "title": "Temporal\u2013orbitofrontal pathway regulates choices across physical reward and visual novelty"
      },
      {
        "paperId": "14722ec8da3fb1b50a21be93fff4e1af3e4bdda3",
        "title": "An Exploration Method for Deep Reinforcement Learning Based on State Prioritized Replay"
      },
      {
        "paperId": "85317048752487e06ba31239eb5b4233c4337242",
        "title": "Bridging Deep Reinforcement Learning and Motion Planning for Model-Free Navigation in Cluttered Environments"
      },
      {
        "paperId": "26db19fe78c808fbed583016736dc40a7df9e20b",
        "title": "Exploration-Driven Generative Interactive Environments"
      },
      {
        "paperId": "c6a9edbfb61cf40cf9b3398491069e4fd7d66384",
        "title": "Unsupervised Representation Learning in Deep Reinforcement Learning: A Review"
      },
      {
        "paperId": "4f8c8bdccf1031e2fbdf0ae80323774fc44cf77c",
        "title": "Neighborhood-Curiosity-Based Exploration in Multiagent Reinforcement Learning"
      },
      {
        "paperId": "33193ebe3893181ad7febda86b3f880628d4159f",
        "title": "Curiosity-driven exploration based on hierarchical vision transformer for deep reinforcement learning with sparse rewards"
      },
      {
        "paperId": "284a15906f966fd9bea118e83e17993da45a67bd",
        "title": "World Model Agents with Change-Based Intrinsic Motivation"
      },
      {
        "paperId": "62a30950428d33031590d1af176cd22fdf335b38",
        "title": "Adventurer: Exploration with BiGAN for Deep Reinforcement Learning"
      },
      {
        "paperId": "5475444b8f7d48ca5643c3c36c8caf2cce52ed40",
        "title": "KEA: Keeping Exploration Alive by Proactively Coordinating Exploration Strategies"
      },
      {
        "paperId": "bc53729436473d7c62d850426f7956097ea69c10",
        "title": "SENSEI: Semantic Exploration Guided by Foundation Models to Learn Versatile World Models"
      },
      {
        "paperId": "c198ee0e0c2f9899bec53b21b0705e684ac9eeaa",
        "title": "Sim-to-Real Reinforcement Learning for Vision-Based Dexterous Manipulation on Humanoids"
      },
      {
        "paperId": "e97b58eb6e9303676d858f40f60198301cbdb56f",
        "title": "Think on Your Feet: Seamless Transition Between Human-Like Locomotion in Response to Changing Commands"
      },
      {
        "paperId": "1f327387a82b973518408d5f0c09843688661f2e",
        "title": "Model-Based Exploration in Monitored Markov Decision Processes"
      },
      {
        "paperId": "2f2e5f6feb8d65517afc10c0e93edfeab8f37f25",
        "title": "Evolving adaptive and interpretable decision trees for cooperative submarine search"
      },
      {
        "paperId": "1c4b4fb0db4d332bf3ff85f96b3c6accd281e8ef",
        "title": "Adaptive multi-model fusion learning for sparse-reward reinforcement learning"
      },
      {
        "paperId": "2b397ebc66a530078c160d99a84284a3df01f785",
        "title": "PIMAEX: Multi-Agent Exploration through Peer Incentivization"
      },
      {
        "paperId": "4cfc429120e90ea1997c6da2dbd7d082bdcb5915",
        "title": "\u03b2-DQN: Improving Deep Q-Learning By Evolving the Behavior"
      },
      {
        "paperId": "ea2f030235313983c283c137a9911624a20bca42",
        "title": "Safe Multiagent Coordination via Entropic Exploration"
      },
      {
        "paperId": "39b7a1cf4cee3e3da41f4278c819e7406144a933",
        "title": "Interactive Visual Analytics for Reward Function Setting of Reinforcement Learning: A Case Study of Soccer Games"
      },
      {
        "paperId": "2a8017583a38e36e99f0f805ad4bb9037029de49",
        "title": "Representational similarity modulates neural and behavioral signatures of novelty"
      },
      {
        "paperId": "85c48223a298d8b752853317a5cbdfd94f42de74",
        "title": "Effective Candidate Invariant Generation Using GPGPUs and Optimisations"
      },
      {
        "paperId": "b04162687d7c992b6acdbfe370204a2bc64ac393",
        "title": "Imitation from Diverse Behaviors: Wasserstein Quality Diversity Imitation Learning with Single-Step Archive Exploration"
      },
      {
        "paperId": "f743464f22136d5b845483c5fd1597cb5809ddad",
        "title": "Mr.Steve: Instruction-Following Agents in Minecraft with What-Where-When Memory"
      },
      {
        "paperId": "50bbe4f66e364743abe5b86adc8fd8adca9753c8",
        "title": "Deterministic Exploration via Stationary Bellman Error Maximization"
      },
      {
        "paperId": "d89ddb912b9680df66ba6a2ff2f8eb3716dc44b5",
        "title": "SPIRE: Synergistic Planning, Imitation, and Reinforcement Learning for Long-Horizon Manipulation"
      },
      {
        "paperId": "3775bd14050cd41b32e518d2f34a85125680b2dd",
        "title": "Leveraging Skills from Unlabeled Prior Data for Efficient Online Exploration"
      },
      {
        "paperId": "2b75372c8c024310fd377d074b0f427d98acb082",
        "title": "GUIDE: Real-Time Human-Shaped Agents"
      },
      {
        "paperId": "fbac8b1731f2329db4a9ad4cc085917aec09d3f4",
        "title": "Truncating Trajectories in Monte Carlo Policy Evaluation: an Adaptive Approach"
      },
      {
        "paperId": "be37ee6676879d08b78433c5c9d597e5c9b7be4d",
        "title": "Automated Rewards via LLM-Generated Progress Functions"
      },
      {
        "paperId": "331586853f0b2f45d66fa8bde7bdc5dde07725cf",
        "title": "ETGL-DDPG: A Deep Deterministic Policy Gradient Algorithm for Sparse Reward Continuous Control"
      },
      {
        "paperId": "b3fc201f3b149ba1ec0b491673722fafea654458",
        "title": "PreND: Enhancing Intrinsic Motivation in Reinforcement Learning through Pre-trained Network Distillation"
      },
      {
        "paperId": "35aa39bfb33c9966206fa6a782b17ff04bad7ece",
        "title": "Progressively Learning to Reach Remote Goals by Continuously Updating Boundary Goals"
      },
      {
        "paperId": "54d474451b469d9417af87d4b640555672fbb22f",
        "title": "Quasimetric Value Functions with Dense Rewards"
      },
      {
        "paperId": "c4dfc0fd5a55a9d5bbf2f0b0ecaed67bb6941d94",
        "title": "State-Novelty Guided Action Persistence in Deep Reinforcement Learning"
      },
      {
        "paperId": "195bf16bb03e9b0e3f5d28386383ed49a87f18e2",
        "title": "Hierarchical Reinforcement Learning-Based End-to-End Visual Servoing With Smooth Subgoals"
      },
      {
        "paperId": "9e38f3e1c92956f7ca8e4b2999e293239626f768",
        "title": "Improved Exploration With Demonstrations in Procedurally-Generated Environments"
      },
      {
        "paperId": "a7ad8ca36983ba08cf3166e4a6dffcd0a7cc4cf8",
        "title": "Mapless navigation via Hierarchical Reinforcement Learning with memory-decaying novelty"
      },
      {
        "paperId": "3e62d583c866615766e1607a742d85d1a135e143",
        "title": "Centralized Reward Agent for Knowledge Sharing and Transfer in Multi-Task Reinforcement Learning"
      },
      {
        "paperId": "46b6a615baf77306611f77ecf6dfe35d611eab59",
        "title": "Directed Exploration in Reinforcement Learning from Linear Temporal Logic"
      },
      {
        "paperId": "bc5a02469eda208c6df470da4840d2f6d00609da",
        "title": "A Single Goal is All You Need: Skills and Exploration Emerge from Contrastive RL without Rewards, Demonstrations, or Subgoals"
      },
      {
        "paperId": "91876fbf6d3da5ac4e16de5ac53a46e3a3f2cc50",
        "title": "Highly Efficient Self-Adaptive Reward Shaping for Reinforcement Learning"
      },
      {
        "paperId": "e344313490dc93bacf721c19f0b74ae921ac4285",
        "title": "Reward Models in Deep Reinforcement Learning: A Survey"
      },
      {
        "paperId": "15a4f40c9f98fa8c808c2208ebe49a24808c8242",
        "title": "Data Efficient Deep Reinforcement Learning With Action-Ranked Temporal Difference Learning"
      },
      {
        "paperId": "f5bbde644e1d7c22f3685dae72e19e7a4afde79f",
        "title": "Image-Based Deep Reinforcement Learning with Intrinsically Motivated Stimuli: On the Execution of Complex Robotic Tasks"
      },
      {
        "paperId": "efcd3b46b841571be2f2c22f8611a3d7064b81c5",
        "title": "Boosting Efficiency in Task-Agnostic Exploration through Causal Knowledge"
      },
      {
        "paperId": "0327596d1c83506035a2372eb649d5c16d9515be",
        "title": "Comprehensive Overview of Reward Engineering and Shaping in Advancing Reinforcement Learning Applications"
      },
      {
        "paperId": "22abfac098c7060f01b447235f8d6fb0c1c7d363",
        "title": "Inductive biases of neural network modularity in spatial navigation"
      },
      {
        "paperId": "ff5046d5e5e7d6501c604cc444b72a5ab18ba7a8",
        "title": "Variable-Agnostic Causal Exploration for Reinforcement Learning"
      },
      {
        "paperId": "ea5a1a2c7d531100a445d26706779240c8894f2e",
        "title": "Sparsity-based Safety Conservatism for Constrained Offline Reinforcement Learning"
      },
      {
        "paperId": "258b9e7e8fb8e68b3d51391620d5c4f56c14d57f",
        "title": "A Safe and Data-Efficient Model-Based Reinforcement Learning System for HVAC Control"
      },
      {
        "paperId": "e949e8c1ad3345190d12acc5d4aa59d46cd1a043",
        "title": "Preference-Guided Reinforcement Learning for Efficient Exploration"
      },
      {
        "paperId": "5ecf53ab083f72f10421225a7dde25eb51cb6b22",
        "title": "Can Learned Optimization Make Reinforcement Learning Less Difficult?"
      },
      {
        "paperId": "de1804fa279cd2fbfea5a5cfd7ac19443d59bc4a",
        "title": "Enhancing Car-Following Performance in Traffic Oscillations Using Expert Demonstration Reinforcement Learning"
      },
      {
        "paperId": "ce32e5dae73775c13b749a27fc53219912ab4b59",
        "title": "CMBE: Curiosity-driven Model-Based Exploration for Multi-Agent Reinforcement Learning in Sparse Reward Settings"
      },
      {
        "paperId": "ac4042056960db1f093b0e8c59dca1b0993c4391",
        "title": "PUZZLES: A Benchmark for Neural Algorithmic Reasoning"
      },
      {
        "paperId": "62555bd1938384ce9a00973b1a15899e71b72ddc",
        "title": "Safety through feedback in Constrained RL"
      },
      {
        "paperId": "a5cf63bac2089ed4ae547689255a03db997c763d",
        "title": "Curiosity-Driven Learning for Visual Control of Nonholonomic Mobile Robots"
      },
      {
        "paperId": "6bf550ce7f10a4347d448eb810a92e1a5cfffa4b",
        "title": "Beyond Optimism: Exploration With Partially Observable Rewards"
      },
      {
        "paperId": "94647af3dc1bdb344a09b24881dfb270306c0156",
        "title": "An Efficient Multi-UAV Policy Learning Method under Binary Rewards"
      },
      {
        "paperId": "29953443a020a1b117472f3a3d869b535c8c31c9",
        "title": "WoCoCo: Learning Whole-Body Humanoid Control with Sequential Contacts"
      },
      {
        "paperId": "af0851a60ac3b5f003e3fefad23a856ed5f91f99",
        "title": "LAGMA: LAtent Goal-guided Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "769d8fdf6520c52e8767ee6d54f6417cf6e7904e",
        "title": "RLeXplore: Accelerating Research in Intrinsically-Motivated Reinforcement Learning"
      },
      {
        "paperId": "2329c383c531030dc7eea3c4b8927b18f112cba7",
        "title": "Exclusively Penalized Q-learning for Offline Reinforcement Learning"
      },
      {
        "paperId": "ce04045785493342c950d0d6e9eea1d0a091289c",
        "title": "Deep reinforcement learning-based model predictive control of uncertain linear systems"
      },
      {
        "paperId": "ffcef91b1c25d2d740b335f9b4b3c942cd8d79de",
        "title": "Enhancing Q-Learning with Large Language Model Heuristics"
      },
      {
        "paperId": "b6408f0e498b82044f5de1779959520235f9b2da",
        "title": "Generative Active Learning for the Search of Small-molecule Protein Binders"
      },
      {
        "paperId": "63eb1afe96024c84bd4ba178842e615dae872475",
        "title": "MESA: Cooperative Meta-Exploration in Multi-Agent Learning through Exploiting State-Action Space Structure"
      },
      {
        "paperId": "81e883fd5f76f63a2b49f275ba6f1f785d9b26f2",
        "title": "An Energy Management Strategy Based on DDPG With Improved Exploration for Battery/Supercapacitor Hybrid Electric Vehicle"
      },
      {
        "paperId": "8e26cd15335a352216161f18b76c54cf53135a51",
        "title": "A configuration of multi-agent reinforcement learning integrating prior knowledge"
      },
      {
        "paperId": "4360cda7e6d17a330dbe9f1fe7792f60fef91788",
        "title": "Goal Decision-making in Active SLAM Using 2D Lidar Based on DRL Driven by Intrinsic Rewards"
      },
      {
        "paperId": "098c52a012cfda1b5ed4aef995efbbcf03dce25c",
        "title": "Goal Exploration via Adaptive Skill Distribution for Goal-Conditioned Reinforcement Learning"
      },
      {
        "paperId": "44ad89df417e3b796614f7e4eb4806c6c8318499",
        "title": "Curiosity-Driven Testing for Sequential Decision-Making Process"
      },
      {
        "paperId": "6023082e39e69e732140ee11bf871f66130f8303",
        "title": "Grid-Mapping Pseudo-Count Constraint for Offline Reinforcement Learning"
      },
      {
        "paperId": "e4885f2498ed24d258fd208e041181d226d6e90d",
        "title": "Dynamic Memory-Based Curiosity: A Bootstrap Approach for Exploration in Reinforcement Learning"
      },
      {
        "paperId": "9c49af687cd7b827e203bba837bc54bfefe037b0",
        "title": "VDSC: Enhancing Exploration Timing with Value Discrepancy and State Counts"
      },
      {
        "paperId": "bfbe7cd5a6535296b324c72920d6c9de035ef8b4",
        "title": "Efficient Episodic Memory Utilization of Cooperative Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "53db22a9d4ae77dd8218ba867184898adc84d1d1",
        "title": "Sample Efficient Offline-to-Online Reinforcement Learning"
      },
      {
        "paperId": "06df2005b1244940c711eb4819be41cc26e7eed7",
        "title": "ACE : Off-Policy Actor-Critic with Causality-Aware Entropy Regularization"
      },
      {
        "paperId": "92f771b0571fb2130f2c2dbb4e8d1a40d11d3fd7",
        "title": "Decentralized Lifelong Path Planning for Multiple Ackerman Car-Like Robots"
      },
      {
        "paperId": "5d95543ad4f9d8a9901302d133a91d8bd6d629fa",
        "title": "How does Your RL Agent Explore? An Optimal Transport Analysis of Occupancy Measure Trajectories"
      },
      {
        "paperId": "cd7050d0f1b9dc904e36f823063c0df1ac6f47a5",
        "title": "Just Cluster It: An Approach for Exploration in High-Dimensions using Clustering and Pre-Trained Representations"
      },
      {
        "paperId": "b261a0a152e59568c1291f2c2db8f21a7521d979",
        "title": "Settling Decentralized Multi-Agent Coordinated Exploration by Novelty Sharing"
      },
      {
        "paperId": "e9aff2323856f81522736c2cb27e49d37d337e2a",
        "title": "To the Max: Reinventing Reward in Reinforcement Learning"
      },
      {
        "paperId": "5799ba21b617852937ff29d5b0f7424395867412",
        "title": "Automated guided vehicle dispatching and routing integration via digital twin with deep reinforcement learning"
      },
      {
        "paperId": "a727f9ea6dfceaae6acdd01d51aca2a17caed016",
        "title": "Scheduled Curiosity-Deep Dyna-Q: Efficient Exploration for Dialog Policy Learning"
      },
      {
        "paperId": "5bd12e258b166a5747d2b9310a8b3eae973decc7",
        "title": "DittoGym: Learning to Control Soft Shape-Shifting Robots"
      },
      {
        "paperId": "87ddd7811eddfa67609f4ff8d10fb2f6ba42d94f",
        "title": "Exploration and Anti-Exploration with Distributional Random Network Distillation"
      },
      {
        "paperId": "faae9de3d314e8731b0505607298fd826e3de1a7",
        "title": "Beyond Sparse Rewards: Enhancing Reinforcement Learning with Language Model Critique in Text Generation"
      },
      {
        "paperId": "f49afbb0dbf654f4e311daa621db05a123faee58",
        "title": "Guiding Offline Reinforcement Learning Using a Safety Expert"
      },
      {
        "paperId": "696a035eddfb2860123863d5999b74b43133d7fd",
        "title": "Continuous Control With Swarm Intelligence Based Value Function Approximation"
      },
      {
        "paperId": "57fe51e14e4b3b98a16ac84d2a6b7401cb413587",
        "title": "Coverage-guided fuzzing for deep reinforcement learning systems"
      },
      {
        "paperId": "4a836363233398c0ac27daee942cb5533f467458",
        "title": "Pangu-Agent: A Fine-Tunable Generalist Agent with Structured Reasoning"
      },
      {
        "paperId": "1014bf18c16799577bf5899ad5b7a0e4972c99a5",
        "title": "Efficient Sparse-Reward Goal-Conditioned Reinforcement Learning with a High Replay Ratio and Regularization"
      },
      {
        "paperId": "74c2af7883f4bf044d4271f5d1dc32b4e2de3091",
        "title": "Entropy Maximization in High Dimensional Multiagent State Spaces"
      },
      {
        "paperId": "c3bcd40df07b78eaf6713de957aeffb799da9152",
        "title": "Regularity as Intrinsic Reward for Free Play"
      },
      {
        "paperId": "f62047a441c692c0d8d420b23f71041dc14f0c0b",
        "title": "An Improved Exploration Method for Cooperative Multi-UAV Policy Learning with Sparse Rewards"
      },
      {
        "paperId": "d9b25c8fc0ddd3c0f2f110c1fe4e173d9880a87f",
        "title": "CLUE: Safe Model-Based RL HVAC Control Using Epistemic Uncertainty Estimation"
      },
      {
        "paperId": "1727ff847aca881fe0c4e3255170028d772ff6b9",
        "title": "On-Policy Policy Gradient Reinforcement Learning Without On-Policy Sampling"
      },
      {
        "paperId": "6effc6f163bdb29bb6359cc7782f6f4d1575e221",
        "title": "General Policies, Subgoal Structure, and Planning Width"
      },
      {
        "paperId": "05603c65d813105eb136e867abfc412ea1735cf0",
        "title": "Accelerating Exploration with Unlabeled Prior Data"
      },
      {
        "paperId": "7d82f1021c3f385ec2fdd7b1a06ea71430e650e8",
        "title": "Hierarchical reinforcement learning with adaptive scheduling for robot control"
      },
      {
        "paperId": "4e98282f5f3f1a388b8d95380473d4ef4878266e",
        "title": "DrM: Mastering Visual Reinforcement Learning through Dormant Ratio Minimization"
      },
      {
        "paperId": "99f9198f9d7ae72d09c9ddc7a8451f119dfeab84",
        "title": "Improving Intrinsic Exploration by Creating Stationary Objectives"
      },
      {
        "paperId": "31ffa4a30d64b9d7436fb0141c511f61ece29401",
        "title": "Understanding when Dynamics-Invariant Data Augmentations Benefit Model-Free Reinforcement Learning Updates"
      },
      {
        "paperId": "694f07d6d84db62c14f9dc363d0dcf7eb5c9fe22",
        "title": "Neuro-Inspired Fragmentation and Recall to Overcome Catastrophic Forgetting in Curiosity"
      },
      {
        "paperId": "7b446eb9c7d3c707eb36c586035edcf2b2c88413",
        "title": "Provable Benefits of Multi-task RL under Non-Markovian Decision Making Processes"
      },
      {
        "paperId": "06ba616f2adfc08bc573529150c1c9131579417a",
        "title": "Reward Shaping for Happier Autonomous Cyber Security Agents"
      },
      {
        "paperId": "9b5dafe432446d9b627699aff65f4e81ac3fc914",
        "title": "METRA: Scalable Unsupervised RL with Metric-Aware Abstraction"
      },
      {
        "paperId": "8ed854f2329195009708bc560e8da4381fec0c39",
        "title": "Offline Retraining for Online RL: Decoupled Policy Learning to Mitigate Exploration Bias"
      },
      {
        "paperId": "0333acabeaac6a029fb374a1c27b79ea3f64f1ca",
        "title": "Generative Intrinsic Optimization: Intrisic Control with Model Learning"
      },
      {
        "paperId": "c35cd153d65b6a0f72f6fb398879344eb2150198",
        "title": "ELDEN: Exploration via Local Dependencies"
      },
      {
        "paperId": "b5f868fc76bd75874a1455eac3dc0a12f7a2ca15",
        "title": "Automatically Defining Game Action Spaces for Exploration Using Program Analysis"
      },
      {
        "paperId": "33b8457e15ceccd4821a006fbfab67a98267b19c",
        "title": "Privacy Preserving Demand Side Management Method via Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "f8c6d738617c10f9598037c20cee6ebffd855597",
        "title": "Controlling Continuous Relaxation for Combinatorial Optimization"
      },
      {
        "paperId": "49cfdc7ac43791564c1381965c2a084d0027af48",
        "title": "Draw on advantages and avoid disadvantages by making a multi-step prediction"
      },
      {
        "paperId": "fce53c0ede6151c3cd5b61841d6aafa67085047b",
        "title": "Go Beyond Imagination: Maximizing Episodic Reachability with World Models"
      },
      {
        "paperId": "b1f843c14c8fed4395d84d621d7be7c958280a1f",
        "title": "FoX: Formation-aware exploration in multi-agent reinforcement learning"
      },
      {
        "paperId": "fb0e167cfcb5bebd7aaeace128c40cbed47509f6",
        "title": "Never Explore Repeatedly in Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "7626b0431313de30d850d370714eaec251cea905",
        "title": "ACRE: Actor-Critic with Reward-Preserving Exploration"
      },
      {
        "paperId": "97622dbadf8bcb03ae2dc735be995f11490b97ef",
        "title": "Controlling Character Motions without Observable Driving Source"
      },
      {
        "paperId": "8e2451959120cb45d976849ab5dad9130d48eee9",
        "title": "Beyond Surprise: Improving Exploration Through Surprise Novelty"
      },
      {
        "paperId": "b955dd592fc3e597040805d665eca7427bb1fdbc",
        "title": "A shared novelty-seeking basis for creativity and curiosity"
      },
      {
        "paperId": "d922a1cca6708cc0d31953abd2ae9c0a3e97328c",
        "title": "AHEGC: Adaptive Hindsight Experience Replay With Goal-Amended Curiosity Module for Robot Control"
      },
      {
        "paperId": "de314e2ffbd005657d9bf31d0c031fc13804f6b9",
        "title": "Social Motivation for Modelling Other Agents under Partial Observability in Decentralised Training"
      },
      {
        "paperId": "53f8ce42ae198befe04259c966663705944bc05a",
        "title": "Adaptive Estimation Q-learning with Uncertainty and Familiarity"
      },
      {
        "paperId": "a3a1b88acf6e19cdf489e590bc91ae64ca889c2b",
        "title": "LJIR: Learning Joint-Action Intrinsic Reward in cooperative multi-agent reinforcement learning"
      },
      {
        "paperId": "232902398c8402c6dcd96e7e0920814ef8dfeb8a",
        "title": "Model-based Offline Reinforcement Learning with Count-based Conservatism"
      },
      {
        "paperId": "f520c9286301be1b98de357f88fe2c6b24239acb",
        "title": "Hindsight-DICE: Stable Credit Assignment for Deep Reinforcement Learning"
      },
      {
        "paperId": "f654c89b7ac36aacd9ca3c60b136070de553a93b",
        "title": "Goal-Conditioned Reinforcement Learning With Disentanglement-Based Reachability Planning"
      },
      {
        "paperId": "8ba93052c60a266b31e121fd06e8ce9cbd9b1bc0",
        "title": "The SocialAI School: Insights from Developmental Psychology Towards Artificial Socio-Cultural Agents"
      },
      {
        "paperId": "5e5504dbb8637946ce931137840f0a007d3ca6d4",
        "title": "Novelty Seeking Multiagent Evolutionary Reinforcement Learning"
      },
      {
        "paperId": "ad7d6d9ca1ac2f5ad6f35ea47dd5345860310a8d",
        "title": "Intrinsically motivated graph exploration using network theories of human curiosity"
      },
      {
        "paperId": "9ccec14734e236c0609464305e4fb2b9e27bf28f",
        "title": "HCS-R-HER: Hierarchical reinforcement learning based on cross subtasks rainbow hindsight experience replay"
      },
      {
        "paperId": "bd09f1477ff5ea1fe1b9ea57a0272978844ee6ad",
        "title": "Curious Replay for Model-based Adaptation"
      },
      {
        "paperId": "2832a3be1ae896d252ece850e7c4ca49edf49d12",
        "title": "RL3: Boosting Meta Reinforcement Learning via RL inside RL2"
      },
      {
        "paperId": "0a806876e419de58b650aacbf218e23f589327ad",
        "title": "CEM: Constrained Entropy Maximization for Task-Agnostic Safe Exploration"
      },
      {
        "paperId": "9c4ece695bfe0a9e99ee21323313882078b4ef4d",
        "title": "Subspace-Aware Exploration for Sparse-Reward Multi-Agent Tasks"
      },
      {
        "paperId": "c1844cda42b3732a5576d05bb6e007eb1db00919",
        "title": "Incremental Reinforcement Learning with Dual-Adaptive \u03b5-Greedy Exploration"
      },
      {
        "paperId": "2500ff9f01110467b2792f4da99e3181a5c1d4bd",
        "title": "CLUE: Calibrated Latent Guidance for Offline Reinforcement Learning"
      },
      {
        "paperId": "6a42f6362afa3a1a0936f7a6a8927d04a2285cc5",
        "title": "Balancing Exploration and Exploitation in Hierarchical Reinforcement Learning via Latent Landmark Graphs"
      },
      {
        "paperId": "3b52e4252b06380d2bf8129a5f38aa34b458d4f8",
        "title": "Pseudo Value Network Distillation for High-Performance Exploration"
      },
      {
        "paperId": "1c592b81a8661f21008b41d8391849ce4c886ccc",
        "title": "Mnemonic Dictionary Learning for Intrinsic Motivation in Reinforcement Learning"
      },
      {
        "paperId": "fb1ac8ef7ea13616f86fafcd3f7eeb06460c2afa",
        "title": "ENOTO: Improving Offline-to-Online Reinforcement Learning with Q-Ensembles"
      },
      {
        "paperId": "92d118852f6bf22f4698bec4115d6fd7f12ff84e",
        "title": "Multi-agent Exploration with Sub-state Entropy Estimation"
      },
      {
        "paperId": "d16feddfeca2617ca2127b7d134ceb78ce8a8b40",
        "title": "A Study of Global and Episodic Bonuses for Exploration in Contextual MDPs"
      },
      {
        "paperId": "ec24cba023e59a11253c60a5516df0562c62d62c",
        "title": "Flipping Coins to Estimate Pseudocounts for Exploration in Reinforcement Learning"
      },
      {
        "paperId": "54952feedd88ca743dc6b01cfbc707d9254bbdb3",
        "title": "Latent Exploration for Reinforcement Learning"
      },
      {
        "paperId": "b9e4ea7ff34a304cc0e7bfaa638eaa850391b676",
        "title": "Accelerating Reinforcement Learning with Value-Conditional State Entropy Exploration"
      },
      {
        "paperId": "c90aa0f206c6fd41c490c142f63f7ba046cae6b7",
        "title": "Provable and Practical: Efficient Exploration in Reinforcement Learning via Langevin Monte Carlo"
      },
      {
        "paperId": "8a84ee6294bc7f88d0343c8615a12f1c763209bd",
        "title": "Off-Policy RL Algorithms Can be Sample-Efficient for Continuous Control via Sample Multiple Reuse"
      },
      {
        "paperId": "8ef3f06353f65ca653b396b70d089f793bfc168b",
        "title": "End-to-End Meta-Bayesian Optimisation with Transformer Neural Processes"
      },
      {
        "paperId": "d065672768be81e9e0e9f6cbc3091ea51c6b75e7",
        "title": "Developmental Curiosity and Social Interaction in Virtual Agents"
      },
      {
        "paperId": "af9e69ec120e94b5542edf88b392ee3769c1379b",
        "title": "MIMEx: Intrinsic Rewards from Masked Input Modeling"
      },
      {
        "paperId": "987d908732fcbaad6b50dcc6793eed0d538e102b",
        "title": "Deep RL at Scale: Sorting Waste in Office Buildings with a Fleet of Mobile Manipulators"
      },
      {
        "paperId": "b2b4f8dba933b60c2ccb66c62c295a938dab2cbe",
        "title": "Unlocking the Power of Representations in Long-term Novelty-based Exploration"
      },
      {
        "paperId": "0cceb527d62abaf587268319575595eb6c93bc50",
        "title": "An Autonomous Non-monolithic Agent with Multi-mode Exploration based on Options Framework"
      },
      {
        "paperId": "63bf6fc80dc34750a1f377711768e3473b3f980e",
        "title": "Representations and Exploration for Deep Reinforcement Learning using Singular Value Decomposition"
      },
      {
        "paperId": "2f1fee5087d47e7a8c71763c22ac784c9565278c",
        "title": "Learning Achievement Structure for Structured Exploration in Domains with Sparse Reward"
      },
      {
        "paperId": "bd542a83925c68587e7009650625ee4c0878c066",
        "title": "DEIR: Efficient and Robust Exploration through Discriminative-Model-Based Episodic Intrinsic Rewards"
      },
      {
        "paperId": "06f7c730009a9cf5975350bceefa7987441be6bf",
        "title": "Aiding reinforcement learning for set point control"
      },
      {
        "paperId": "fbe1003ec391f6bcf4660f6ef81f1e6199849bfe",
        "title": "Provably Feedback-Efficient Reinforcement Learning via Active Reward Learning"
      },
      {
        "paperId": "253b41369d003952874c6a47a6038277b165cfa0",
        "title": "Affordances from Human Videos as a Versatile Representation for Robotics"
      },
      {
        "paperId": "3a7baf028efb106f2431965d79600725e0aab536",
        "title": "Signal Novelty Detection as an Intrinsic Reward for Robotics"
      },
      {
        "paperId": "3eb75f5f35a7963b46ad4c43c68082283c9c0489",
        "title": "Accelerating exploration and representation learning with offline pre-training"
      },
      {
        "paperId": "ea48c4ee9eab7da840ebaf32efc4d0e923fc72cd",
        "title": "Conditionally Optimistic Exploration for Cooperative Deep Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "4d0e2aa1142aa323df862ebf56513adf04fcb5ae",
        "title": "Learning robotic manipulation skills with multiple semantic goals by conservative curiosity-motivated exploration"
      },
      {
        "paperId": "f576d506be3c56d7deaaae71833ad917774d2289",
        "title": "Adaptive exploration network policy for effective exploration in reinforcement learning"
      },
      {
        "paperId": "027a402f21a6c76a82f315f41a4e0aba2b1d7a7a",
        "title": "Failure-aware Policy Learning for Self-assessable Robotics Tasks"
      },
      {
        "paperId": "4da7c38585ad044f62914a7411cb1878c7df7c4e",
        "title": "Self-supervised network distillation: An effective approach to exploration in sparse reward environments"
      },
      {
        "paperId": "4a50853984a126e2b77f7c3f216c0ef486a138ab",
        "title": "Efficient exploration via epistemic-risk-seeking policy optimization"
      },
      {
        "paperId": "f63adcba79ab09c2eed7d22174661be98a018bf4",
        "title": "ALAN: Autonomously Exploring Robotic Agents in the Real World"
      },
      {
        "paperId": "924501c0205218280cb0251c89bda88c5a142b3e",
        "title": "Investigating the role of model-based learning in exploration and transfer"
      },
      {
        "paperId": "26115e5d6f20c9e7560e1709f3c36847deffa148",
        "title": "A general Markov decision process formalism for action-state entropy-regularized reward maximization"
      },
      {
        "paperId": "71a8cf9781ac17b3c2ae89f0f2965a05fe1872c3",
        "title": "Improved Knowledge Distillation for Pre-trained Language Models via Knowledge Selection"
      },
      {
        "paperId": "2d9b75da46989bed3cffe8f51e050d674d5b5d15",
        "title": "Sample Efficient Deep Reinforcement Learning via Local Planning"
      },
      {
        "paperId": "528eaca08fac125c5c65c6d5d3f636d92ebd0517",
        "title": "Automatic Intrinsic Reward Shaping for Exploration in Deep Reinforcement Learning"
      },
      {
        "paperId": "621cb2bca92c12ba8381ca8512c432f5ba7dd815",
        "title": "Understanding the Complexity Gains of Single-Task RL with a Curriculum"
      },
      {
        "paperId": "efdedecdfb3c47164dc5ef3116ae09f4adb1120e",
        "title": "Symptoms are known by their companies: towards association guided disease diagnosis assistant"
      },
      {
        "paperId": "2c49cdfe6dc62fd3294b09d8bca44e43b300bbf6",
        "title": "TOCIM: An improved operant conditioning model with task-oriented curiosity"
      },
      {
        "paperId": "3d3b0704c61d47c7bafb70ae2670b2786b8e4d81",
        "title": "Efficient Exploration in Resource-Restricted Reinforcement Learning"
      },
      {
        "paperId": "34d1b6b3e73848caebe759de74d54753c9c2f3f5",
        "title": "PrefRec: Recommender Systems with Human Preferences for Reinforcing Long-term User Engagement"
      },
      {
        "paperId": "d3d2a45a524e47e7e829d5b57e3a1d5bf7e0def7",
        "title": "Curiosity-Driven and Victim-Aware Adversarial Policies"
      },
      {
        "paperId": "cbd7b03a3d031b39f7a8203d594c1da6be396947",
        "title": "Mixed Time-Frame Training for Reinforcement Learning"
      },
      {
        "paperId": "970928133696fbe66300d0f402e629a10ebf5537",
        "title": "Assessing Quality-Diversity Neuro-Evolution Algorithms Performance in Hard Exploration Problems"
      },
      {
        "paperId": "915f52bfbbb813bf50a1824282c0b490a6c30dd1",
        "title": "Curiosity in hindsight"
      },
      {
        "paperId": "b065c42b1f40368ff1dacb950c4817a4d18b3ed9",
        "title": "Foundation Models for Semantic Novelty in Reinforcement Learning"
      },
      {
        "paperId": "c90a33f1f0049d524e9b5b3174d35611fd9a8096",
        "title": "Pretraining in Deep Reinforcement Learning: A Survey"
      },
      {
        "paperId": "727769eb76b06c336ade115ba85fb6fc47488407",
        "title": "Curiosity-Driven Multi-Agent Exploration with Mixed Objectives"
      },
      {
        "paperId": "c9b4e6d73be55b358938667416f96e5b7ecb45b5",
        "title": "Building Heat Demand Prediction Based on Reinforcement Learning for Thermal Comfort Management"
      },
      {
        "paperId": "c4df1a5633615a5e23c16cc6f2d5998f6a2fc3de",
        "title": "Epistemic Monte Carlo Tree Search"
      },
      {
        "paperId": "1b9c5622b905a3da9733d2c306a87374101eb479",
        "title": "The Pump Scheduling Problem: A Real-World Scenario for Reinforcement Learning"
      },
      {
        "paperId": "203a1c0e5025489a52c030adbbc102a787685ee6",
        "title": "Unpacking Reward Shaping: Understanding the Benefits of Reward Engineering on Sample Complexity"
      },
      {
        "paperId": "144caddc0d09fa4d2a83056f61c2de4389132732",
        "title": "Symbol Guided Hindsight Priors for Reward Learning from Human Preferences"
      },
      {
        "paperId": "1f0c593cb5530661a538d7e857797dacd57cdd83",
        "title": "LECO: Learnable Episodic Count for Task-Specific Intrinsic Reward"
      },
      {
        "paperId": "0351a103915d2f10d1915b4892a988d9b15df406",
        "title": "Exploration via Elliptical Episodic Bonuses"
      },
      {
        "paperId": "d755158b5a95700729b460f739d74758e3e5dd3b",
        "title": "Towards Agent-Based Testing of 3D Games using Reinforcement Learning"
      },
      {
        "paperId": "292e46b54ba726bd4d2a23db035a5f565a221d09",
        "title": "Reward-Mixing MDPs with a Few Latent Contexts are Learnable"
      },
      {
        "paperId": "6bf1cb33ed31db18901c1d21a5de317a785d5a84",
        "title": "Query The Agent: Improving sample efficiency through epistemic uncertainty estimation"
      },
      {
        "paperId": "23514ced370ae993eb43453d13b00527536d220b",
        "title": "Learning Minimally-Violating Continuous Control for Infeasible Linear Temporal Logic Specifications"
      },
      {
        "paperId": "01575138a4c3b0993d875bda1e25a15508982866",
        "title": "Boosting Exploration in Actor-Critic Algorithms by Incentivizing Plausible Novel States"
      },
      {
        "paperId": "3877158d5c7774ac37f29d18bd603656bba41849",
        "title": "APD: Learning Diverse Behaviors for Reinforcement Learning Through Unsupervised Active Pre-Training"
      },
      {
        "paperId": "ef1b6bf0efc38748ddf612c510db74c0ba3c3044",
        "title": "Multi-AGV Scheduling based on Hierarchical Intrinsically Rewarded Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "c64a99ec780d5e1c6ba242a82e72e96eb0c3ef8a",
        "title": "An Information-Theoretic Perspective on Intrinsic Motivation in Reinforcement Learning: A Survey"
      },
      {
        "paperId": "ae412e9981d6bc9ffa22c543a2fca4d8312c2986",
        "title": "Rewarding Episodic Visitation Discrepancy for Exploration in Reinforcement Learning"
      },
      {
        "paperId": "79dbba6bfba5bf20bc564f895c2cc8c8bf6953b4",
        "title": "Optimistic Curiosity Exploration and Conservative Exploitation with Linear Reward Shaping"
      },
      {
        "paperId": "0c9b5412bcef781b001222a8952c104af84889f5",
        "title": "Self-supervised Sequential Information Bottleneck for Robust Exploration in Deep Reinforcement Learning"
      },
      {
        "paperId": "17f56366835fec4bbc4e63752711dad967e761b6",
        "title": "Go-Explore Complex 3-D Game Environments for Automated Reachability Testing"
      },
      {
        "paperId": "c28cca3538c949b223194a00e14c3575b323ef2f",
        "title": "Normality-Guided Distributional Reinforcement Learning for Continuous Control"
      },
      {
        "paperId": "f54fd65a4a8fa0b03290a5de4b0be79f545aced4",
        "title": "Unsupervised Representation Learning in Deep Reinforcement Learning: A Review"
      },
      {
        "paperId": "55174eacb397cadee19b3315a06e8b4df4c4cc0a",
        "title": "A Review of Uncertainty for Deep Reinforcement Learning"
      },
      {
        "paperId": "56df3ab84088a9f2b9363856671a5ed06ad1b432",
        "title": "Surrogate for Long-Term User Experience in Recommender Systems"
      },
      {
        "paperId": "8607533bd0e3fa2509cf9839a4bad874b6fb360c",
        "title": "Molecular Design Method Using a Reversible Tree Representation of Chemical Compounds and Deep Reinforcement Learning"
      },
      {
        "paperId": "9483ad6ef1365b51b3cf23a382ffb18ed7338dc7",
        "title": "The pursuit of happiness: A reinforcement learning perspective on habituation and comparisons"
      },
      {
        "paperId": "8d03f604071ed94f3165380c745c2fae5c613d87",
        "title": "Reinforcement learning with experience replay and adaptation of action dispersion"
      },
      {
        "paperId": "5cd0a8d1c420f165ed32fbed635817ddd58609d6",
        "title": "Annealed Training for Combinatorial Optimization on Graphs"
      },
      {
        "paperId": "ea782143b32771650651bc56bcfa24d793444d23",
        "title": "Deep Reinforcement Learning with Parametric Episodic Memory"
      },
      {
        "paperId": "5c0b56d9440ea70c79d1d0032d46c14e06f541f5",
        "title": "Reactive Exploration to Cope with Non-Stationarity in Lifelong Reinforcement Learning"
      },
      {
        "paperId": "e724c5cba05fbce16f73f5a409787615accec025",
        "title": "Fully probabilistic design of strategies with estimator"
      },
      {
        "paperId": "cede72a30e2c5833111357cc12b06eeb4676ebb5",
        "title": "GAN-based Intrinsic Exploration for Sample Efficient Reinforcement Learning"
      },
      {
        "paperId": "395d89ba308890901ad19905f7556cab0a1a6a27",
        "title": "Towards Understanding How Machines Can Learn Causal Overhypotheses"
      },
      {
        "paperId": "2fd346e7edf0ec9de793c96acb0a1e3f2f8a2718",
        "title": "BYOL-Explore: Exploration by Bootstrapped Prediction"
      },
      {
        "paperId": "3f66a70a181e9cd1cd8d0760c6c5e7a6f3ff2006",
        "title": "Action Noise in Off-Policy Deep Reinforcement Learning: Impact on Exploration and Performance"
      },
      {
        "paperId": "d360ef9f45e819889fca2b1a65e1b5e23a466ce9",
        "title": "Policy Gradient Algorithms with Monte Carlo Tree Learning for Non-Markov Decision Processes"
      },
      {
        "paperId": "c14df78cc488eaeb3e40e711c20057c63006a9f6",
        "title": "Graph Backup: Data Efficient Backup Exploiting Markovian Transitions"
      },
      {
        "paperId": "538f858732047960f5a390def882066b0c55fbe8",
        "title": "k-Means Maximum Entropy Exploration"
      },
      {
        "paperId": "215d62afc65ca93cda115d227e79be0716cc8667",
        "title": "Utility of doctrine with multi-agent RL for military engagements"
      },
      {
        "paperId": "6960168c7720053c5fd3c4b5ca3d5a4651f8e054",
        "title": "Off-Beat Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "2decff836d5a433fa917a1f9e37466a490c84abd",
        "title": "SFP: State-free Priors for Exploration in Off-Policy Reinforcement Learning"
      },
      {
        "paperId": "7146b289de310c95e3d0a6fded91642d4140b65e",
        "title": "Concurrent Credit Assignment for Data-efficient Reinforcement Learning"
      },
      {
        "paperId": "cc9f2fd320a279741403c4bfbeb91179803c428c",
        "title": "Reward Uncertainty for Exploration in Preference-based Reinforcement Learning"
      },
      {
        "paperId": "1dc27caf3c3615028e5a4de536edf6eaf1073b0c",
        "title": "An Evaluation Study of Intrinsic Motivation Techniques applied to Reinforcement Learning over Hard Exploration Environments"
      },
      {
        "paperId": "d39eaba62c0033004e4061e3d0d65c328684eb14",
        "title": "Complex behavior from intrinsic motivation to occupy future action-state path space"
      },
      {
        "paperId": "4ba973b38e448b2060bd6e2cbc0255d767ddaf98",
        "title": "ASE"
      },
      {
        "paperId": "baf2722c7a28912a6eb57ede95ae72509102c596",
        "title": "Sampling diversity driven exploration with state difference guidance"
      },
      {
        "paperId": "f60dd378987c05e54b577c857012b815744c2951",
        "title": "Learning Attentional and Gated Communication via Curiosity"
      },
      {
        "paperId": "715de83afbbe3a97dfbba6b3f62209233b434adf",
        "title": "Contributions of expected learning progress and perceptual novelty to curiosity-driven exploration"
      },
      {
        "paperId": "426e8449e900f6e16a77809b23cf3507b6f89917",
        "title": "Off-Policy Evaluation With Online Adaptation for Robot Exploration in Challenging Environments"
      },
      {
        "paperId": "12aeb6e6835e54a34a147b2070093ad775a42115",
        "title": "Habitat-Web: Learning Embodied Object-Search Strategies from Human Demonstrations at Scale"
      },
      {
        "paperId": "93874e16ff0f789040b7ae6789c6f7e6c981c9c6",
        "title": "Solving hard-exploration problems with counting and replay approach"
      },
      {
        "paperId": "e1609282cb724303f647eb0c99dc4e2bac65fffb",
        "title": "Curiosity Driven Self-supervised Tactile Exploration of Unknown Objects"
      },
      {
        "paperId": "1c35807e1a4c24e2013fa0a090cee9cc4716a5f5",
        "title": "Reinforcement Learning with Action-Free Pre-Training from Videos"
      },
      {
        "paperId": "daa0b9b5f9aba3b87b47429b2a2224eaf7f789b1",
        "title": "Exploration for Countering the Episodic Memory"
      },
      {
        "paperId": "356383070a9946bc967d6cff50d2e563d5599f33",
        "title": "Learning a World Model With Multitimescale Memory Augmentation"
      },
      {
        "paperId": "808dd538f0970668a48a611a5627e0babd36740b",
        "title": "Intrinsically-Motivated Reinforcement Learning: A Brief Introduction"
      },
      {
        "paperId": "4d54a4d53f82483f42abacc07cda7bacc6075876",
        "title": "Follow your Nose: Using General Value Functions for Directed Exploration in Reinforcement Learning"
      },
      {
        "paperId": "dd6d8476a0c3cbf3f1048654f0a0bd9dd290b1c3",
        "title": "A target-driven visual navigation method based on intrinsic motivation exploration and space topological cognition"
      },
      {
        "paperId": "47f28e1057eac8f7ec99ac8250e4bfed0eafd9e9",
        "title": "Unified curiosity-Driven learning with smoothed intrinsic reward estimation"
      },
      {
        "paperId": "a62c2b9cce3bd3df9ba1eea42aceeb070bd089b2",
        "title": "A Novel Reinforcement Learning Collision Avoidance Algorithm for USVs Based on Maneuvering Characteristics and COLREGs"
      },
      {
        "paperId": "da1e404ae194dcbcce198f5335f49f090403bbd7",
        "title": "Exploration in Deep Reinforcement Learning: A Survey"
      },
      {
        "paperId": "c68acbdb0fd6a69dfac9a31a4db281ac346eefe2",
        "title": "Exit Decisions Inspired by Reinforcement Learning"
      },
      {
        "paperId": "ce126d14a148210b12e69150f8b0905d01f76504",
        "title": "Collaborative training of heterogeneous reinforcement learning agents in environments with sparse rewards: what and when to share?"
      },
      {
        "paperId": "42d413feb8479b72da06f3d47d78fcb0d2f861f2",
        "title": "Using Deep Reinforcement Learning with Automatic Curriculum earning for Mapless Navigation in Intralogistics"
      },
      {
        "paperId": "0361ae63b3bd65e52ca495ac2f8ba0bab0bc325a",
        "title": "Multi-Agent Reinforcement Learning Based Fully Decentralized Dynamic Time Division Configuration for 5G and B5G Network"
      },
      {
        "paperId": "6d0adac188152fbaa45a88ba4da788926ed8144a",
        "title": "Reinforcement Learning in Practice: Opportunities and Challenges"
      },
      {
        "paperId": "026dc8d3cbb360bdd12d19c924bc633221c9b423",
        "title": "Learning Causal Overhypotheses through Exploration in Children and Computational Models"
      },
      {
        "paperId": "ba9b7f2998002529061532486226496883e86e30",
        "title": "Improving Intrinsic Exploration with Language Abstractions"
      },
      {
        "paperId": "0c529ba981b90b2d1536d342bf92a7451686164a",
        "title": "Learning Synthetic Environments and Reward Networks for Reinforcement Learning"
      },
      {
        "paperId": "9d270c56dd80a2f493e206781faf82a22351a6b2",
        "title": "Learning to Act with Affordance-Aware Multimodal Neural SLAM"
      },
      {
        "paperId": "76e86347243661adf0ef96271982961f3117af10",
        "title": "From Psychological Curiosity to Artificial Curiosity: Curiosity-Driven Learning in Artificial Intelligence Tasks"
      },
      {
        "paperId": "22cd19c961d997cb6a77b8751e92f203129d1038",
        "title": "SCC-rFMQ: a multiagent reinforcement learning method in cooperative Markov games with continuous actions"
      },
      {
        "paperId": "69951c75efb9ff615fb31eb4b6b42d4cd1ded58f",
        "title": "Reward Relabelling for combined Reinforcement and Imitation Learning on sparse-reward tasks"
      },
      {
        "paperId": "3b61bc41dff751edbead03aab5e4a1da1aafcc06",
        "title": "Multi-Stage Episodic Control for Strategic Exploration in Text Games"
      },
      {
        "paperId": "8e63e0ccd18fdfc1f7a73fc9e0d4bc6c36316016",
        "title": "Unsupervised Reinforcement Learning in Multiple Environments"
      },
      {
        "paperId": "8a9cae9d690433c1cf2211d5cb803d8face4e7c6",
        "title": "End-to-End Autonomous Exploration with Deep Reinforcement Learning and Intrinsic Motivation"
      },
      {
        "paperId": "4434d9ce81e5e4a0d1379dbc33f523405f64cd2f",
        "title": "A Benchmark for Low-Switching-Cost Reinforcement Learning"
      },
      {
        "paperId": "2bd2080efe63afd1fd0c17e04b80b79166ec5aa7",
        "title": "An Experimental Design Perspective on Model-Based Reinforcement Learning"
      },
      {
        "paperId": "faf8abf19a89fa0e40f3f50550816a3bdecb80f0",
        "title": "Information is Power: Intrinsic Control via Information Capture"
      },
      {
        "paperId": "87cfa078bbb74de8b14e59120246b5d385aa3f96",
        "title": "Sparse reward for reinforcement learning\u2010based continuous integration testing"
      },
      {
        "paperId": "d0825f0e1b808764f359dce8d4bed91d93f273da",
        "title": "On the Unreasonable Efficiency of State Space Clustering in Personalization Tasks"
      },
      {
        "paperId": "3f9f9f9fa1a0d8fb2d169aae2d81ed51ea0b0a1a",
        "title": "Estimating the Variance of Return Sequences for Exploration"
      },
      {
        "paperId": "956b91873de1d54b4be305b35a4ed00e08247046",
        "title": "Robust On-Policy Sampling for Data-Efficient Policy Evaluation in Reinforcement Learning"
      },
      {
        "paperId": "39d2e380967c91b447d47377840fa541e276b479",
        "title": "Interesting Object, Curious Agent: Learning Task-Agnostic Exploration"
      },
      {
        "paperId": "e227f71d7d84ed9a76278510a46f9b7db286ba92",
        "title": "Episodic Multi-agent Reinforcement Learning with Curiosity-Driven Exploration"
      },
      {
        "paperId": "07ae9fde025f71dc40f8b31c25b33469c9a5a5e8",
        "title": "Uncertainty-aware Low-Rank Q-Matrix Estimation for Deep Reinforcement Learning"
      },
      {
        "paperId": "895dbfa06a4441eec97f021ad8944c77e8ddae3f",
        "title": "Agent Spaces"
      },
      {
        "paperId": "e5fb483c877ba5e1c6cd87e8abef20de9260c65d",
        "title": "Learning to explore by reinforcement over high-level options"
      },
      {
        "paperId": "7bdd2b76fd0257dc929f6b2c04be83ef9d28f9ae",
        "title": "Automatic HMI Structure Exploration Via Curiosity-Based Reinforcement Learning"
      },
      {
        "paperId": "27bc680bf6a115cc3f28c4da462b6d25cf04cb09",
        "title": "Landmark-Guided Subgoal Generation in Hierarchical Reinforcement Learning"
      },
      {
        "paperId": "6f07784838ac8847c94e038ed5c288013c372dba",
        "title": "CIExplore: Curiosity and Influence-based Exploration in Multi-Agent Cooperative Scenarios with Sparse Rewards"
      },
      {
        "paperId": "c15168a044a2a97d31ee747f99b91be6911c0e72",
        "title": "Off-policy Reinforcement Learning with Optimistic Exploration and Distribution Correction"
      },
      {
        "paperId": "c4d91bca6066282da421671ab06ac1e156f19851",
        "title": "Anti-Concentrated Confidence Bonuses for Scalable Exploration"
      },
      {
        "paperId": "13dfb80b184a6568485fbfd11e5b24d51b0f503f",
        "title": "Hierarchical Skills for Efficient Exploration"
      },
      {
        "paperId": "b5bb16e1de8bce93ab78bf578090d1d64ebd25c2",
        "title": "Dynamic Bottleneck for Robust Self-Supervised Exploration"
      },
      {
        "paperId": "0382639a58733e95d4f093943455d58455676db0",
        "title": "Continuous Control with Action Quantization from Demonstrations"
      },
      {
        "paperId": "ccc1c5866b4479e81c257a8d2075a99430b7eb00",
        "title": "Provable RL with Exogenous Distractors via Multistep Inverse Dynamics"
      },
      {
        "paperId": "0a44b304b5902fff93f5e263f213543fd901f7f3",
        "title": "Curiosity-driven exploration: Diversity of mechanisms and functions"
      },
      {
        "paperId": "807f377de905eda62e4cd2f0797153a59296adbb",
        "title": "Partial Off-policy Learning: Balance Accuracy and Diversity for Human-Oriented Image Captioning"
      },
      {
        "paperId": "ac89d4156c66f792cacbd29600d4cf0cfead71f3",
        "title": "Deep Reinforcement Learning Versus Evolution Strategies: A Comparative Survey"
      },
      {
        "paperId": "9e8df3e4416c0aea3eec27f96e89184c1ef7f618",
        "title": "Exploratory State Representation Learning"
      },
      {
        "paperId": "718d9ecaa866109abbe4a9b97c522c6030b06929",
        "title": "Exploring More When It Needs in Deep Reinforcement Learning"
      },
      {
        "paperId": "c2ec144b633e2dcbf889da95c711b483803e8350",
        "title": "Hierarchical learning from human preferences and curiosity"
      },
      {
        "paperId": "145691e93cca32af2f2ecfef49b8b34520b9c462",
        "title": "Density-based Curriculum for Multi-goal Reinforcement Learning with Sparse Rewards"
      },
      {
        "paperId": "41872d5173d4ceaef1a5359dae8ad4198caf6b66",
        "title": "Knowledge is reward: Learning optimal exploration by predictive reward cashing"
      },
      {
        "paperId": "12075ea34f5fbe32ec5582786761ab34d401209b",
        "title": "Exploration in Deep Reinforcement Learning: From Single-Agent to Multiagent Domain"
      },
      {
        "paperId": "2f00238cff91020dd4ab9027d34504c61c84e1fd",
        "title": "Exploration in Recommender Systems"
      },
      {
        "paperId": "fb3ed1f977215983154b3a132ec233cb296c1238",
        "title": "Values of User Exploration in Recommender Systems"
      },
      {
        "paperId": "363ba1006cedaf56fa4b7a0073e847a18f6711ef",
        "title": "ADER: Adapting between Exploration and Robustness for Actor-Critic Methods"
      },
      {
        "paperId": "a8262348003a9d93ea7ceffc887729cf88dedf06",
        "title": "Eden: A Unified Environment Framework for Booming Reinforcement Learning Algorithms"
      },
      {
        "paperId": "8e9d44502f5b6de37d5a7181e169a2899a91f7b3",
        "title": "Self-Attention-Based Temporary Curiosity in Reinforcement Learning Exploration"
      },
      {
        "paperId": "bc18f1ec5209f8ebacb91eff42927a33664bc1a2",
        "title": "Influence-based Reinforcement Learning for Intrinsically-motivated Agents"
      },
      {
        "paperId": "44ff8ccb3a7a88465e991cfe0bb7cf8d2414da58",
        "title": "Actor\u2013Critic Reinforcement Learning and Application in Developing Computer-Vision-Based Interface Tracking"
      },
      {
        "paperId": "5079fb5db4953605753aeeefebe0edd979b030a0",
        "title": "Adaptive exploration policy for exploration\u2013exploitation tradeoff in continuous action control optimization"
      },
      {
        "paperId": "00eba1e415c798ef0479c369372850ab54d27620",
        "title": "Deep reinforcement learning-based safe interaction for industrial human-robot collaboration using intrinsic reward function"
      },
      {
        "paperId": "744f984568bbd0e2a5ce2bab1e6f2b4eae3e7e62",
        "title": "Online reinforcement learning for a continuous space system with experimental validation"
      },
      {
        "paperId": "a616614aeb18a06ee796fa0834b1ce56fe2c1e06",
        "title": "Playtesting: What is Beyond Personas"
      },
      {
        "paperId": "feb7f993c402e2663d20bbafa83c11e6db3dfe6b",
        "title": "Cooperative Exploration for Multi-Agent Deep Reinforcement Learning"
      },
      {
        "paperId": "1f601a78724e9e6c26e0a46333b88b3aae134410",
        "title": "Decoupled Reinforcement Learning to Stabilise Intrinsically-Motivated Exploration"
      },
      {
        "paperId": "a91de83f9ea98aab7f6f1c2036d67fa2e6efdfe2",
        "title": "Reinforcement Learning with Potential Functions Trained to Discriminate Good and Bad States"
      },
      {
        "paperId": "59a348666447d32e426ccd0954b689967c6bb538",
        "title": "Learning to Play Hard Exploration Games Using Graph-Guided Self-Navigation"
      },
      {
        "paperId": "6382bec2c2fd18d388483653409b1a18048521da",
        "title": "MURAL: Meta-Learning Uncertainty-Aware Rewards for Outcome-Driven Reinforcement Learning"
      },
      {
        "paperId": "9db6f170fcd0b6aa0e6f41caa07cb2202130b0b5",
        "title": "Novelty and MCTS"
      },
      {
        "paperId": "a27435387bfea3b3e2570bfd31f9cd7d40276e9c",
        "title": "SocialAI: Benchmarking Socio-Cognitive Abilities in Deep Reinforcement Learning Agents"
      },
      {
        "paperId": "6f0d0c10741dda1622d25104b09daa5e815373ec",
        "title": "Importance Sampling based Exploration in Q Learning"
      },
      {
        "paperId": "697edf4e095898cd1781dff50ddbe686ae22bafc",
        "title": "Curious Explorer: A Provable Exploration Strategy in Policy Learning"
      },
      {
        "paperId": "41d17c9a24acae59463e2186be1b7949fd772025",
        "title": "Exploration in policy optimization through multiple paths"
      },
      {
        "paperId": "42cc5f45d59a694813e990b29631be4e32bc6453",
        "title": "MADE: Exploration via Maximizing Deviation from Explored Regions"
      },
      {
        "paperId": "399806e861a2ef960a81b37b593c2176a728c399",
        "title": "Offline Reinforcement Learning as Anti-Exploration"
      },
      {
        "paperId": "45f573f302dc7e77cbc5d1a74ccbac3564bbebc8",
        "title": "PEBBLE: Feedback-Efficient Interactive Reinforcement Learning via Relabeling Experience and Unsupervised Pre-training"
      },
      {
        "paperId": "988dae20df8d69869aa41097a05d821446cff621",
        "title": "DisTop: Discovering a Topological Representation to Learn Diverse and Rewarding Skills"
      },
      {
        "paperId": "15d79a5e930fa88114bae59ffa106de5410965de",
        "title": "Sample-efficient deep reinforcement learning with directed associative graph"
      },
      {
        "paperId": "3a56dc25b3cba2cb438be8b5b95f4ff63f8c02ff",
        "title": "Active Hierarchical Exploration with Stable Subgoal Representation Learning"
      },
      {
        "paperId": "adcae35901c36325478a03b647e14222a53ea9fc",
        "title": "What Can I Do Here? Learning New Skills by Imagining Visual Affordances"
      },
      {
        "paperId": "e588d66d16623cc6d1ca007a179bb084ba1fb83b",
        "title": "Zero-shot Policy Learning with Spatial Temporal Reward Decomposition on Contingency-aware Observation"
      },
      {
        "paperId": "1e515c9c4aaca14194112dad3e7301bea2d61930",
        "title": "DIMSAN: Fast Exploration with the Synergy between Density-based Intrinsic Motivation and Self-adaptive Action Noise"
      },
      {
        "paperId": "0f4f6d1ee869ed496534d7eda51ac0f0110ce3fc",
        "title": "Don't Do What Doesn't Matter: Intrinsic Motivation with Action Usefulness"
      },
      {
        "paperId": "43c65031ca0dd8934b70374e4f7a326e146a3532",
        "title": "Improved Exploring Starts by Kernel Density Estimation-Based State-Space Coverage Acceleration in Reinforcement Learning"
      },
      {
        "paperId": "7e1b6ff0525e61c75c636982b400a3a9576b10e0",
        "title": "Task-Agnostic Exploration via Policy Gradient of a Non-Parametric State Entropy Estimate"
      },
      {
        "paperId": "6a9d4cbe2f1bcb8f81eb5eb91dd752e2970664ee",
        "title": "General Policies, Representations, and Planning Width"
      },
      {
        "paperId": "7c2bb6d3e9a154335d9af5c7298b587ac15945bd",
        "title": "Exploration via State influence Modeling"
      },
      {
        "paperId": "31d1cc6aa3a04bc524b40b089087c8ccffab35df",
        "title": "Sequential Generative Exploration Model for Partially Observable Reinforcement Learning"
      },
      {
        "paperId": "58b17c5a115c8440c530c6242a95076618107bf7",
        "title": "Non-decreasing Quantile Function Network with Efficient Exploration for Distributional Reinforcement Learning"
      },
      {
        "paperId": "6d2f554fbd24715c2268d64f90b53a5f19044774",
        "title": "Principled Exploration via Optimistic Bootstrapping and Backward Induction"
      },
      {
        "paperId": "4e9f9e1a69e1924aa9345d77510d43285900f465",
        "title": "Curious Representation Learning for Embodied Intelligence"
      },
      {
        "paperId": "491088fecea387f9d1315ba2e3be68dc01970924",
        "title": "A Survey of Brain-Inspired Intelligent Robots: Integration of Vision, Decision, Motion Control, and Musculoskeletal Systems"
      },
      {
        "paperId": "e25198f641ad53cd0a108a37598645bc7a536a74",
        "title": "SocialAI 0.1: Towards a Benchmark to Stimulate Research on Socio-Cognitive Abilities in Deep Reinforcement Learning Agents"
      },
      {
        "paperId": "26498b6bc4e80e8faf9e07aa661fb402ce73addb",
        "title": "Ask & Explore: Grounded Question Answering for Curiosity-Driven Exploration"
      },
      {
        "paperId": "6e069c026acd581e267630e43499a2f3908824c4",
        "title": "Rule-Based Reinforcement Learning for Efficient Robot Navigation With Space Reduction"
      },
      {
        "paperId": "8c2e0392ae194afa0173c26ec92fa6460707f241",
        "title": "Curiosity-Driven Exploration via Latent Bayesian Surprise"
      },
      {
        "paperId": "9efd4aefa4999d8ec6fc48e8097643fd9b3c494f",
        "title": "Behavior-Guided Actor-Critic: Improving Exploration via Learning Policy Behavior Representation for Deep Reinforcement Learning"
      },
      {
        "paperId": "ef159910831752335df38696a584a22a35c5345e",
        "title": "Multi-agent reinforcement learning with directed exploration and selective memory reuse"
      },
      {
        "paperId": "db5ec14be596e3c52cd2fa29473927cb70239973",
        "title": "Bayesian Distributional Policy Gradients"
      },
      {
        "paperId": "d2d3dd6188f98e5a9102bd50045df7f33b5f7104",
        "title": "Sample-efficient Reinforcement Learning Representation Learning with Curiosity Contrastive Forward Dynamics Model*"
      },
      {
        "paperId": "78d4285fbcb06fcf3289e099c19abae7e7ed95a3",
        "title": "Discovering Diverse Multi-Agent Strategic Behavior via Reward Randomization"
      },
      {
        "paperId": "da45e961f285fdba9aefb3f4d4270620044eccb3",
        "title": "Reinforcement Learning, Bit by Bit"
      },
      {
        "paperId": "fab4b767e3ae70e8e04e991e8d626fe6b3184213",
        "title": "Learning Navigation Policies for Mobile Robots in Deep Reinforcement Learning with Random Network Distillation"
      },
      {
        "paperId": "6c80e042dff64ad6fba6df856cd18e9cc6aa658a",
        "title": "Reinforcement Learning for Adaptive Mesh Refinement"
      },
      {
        "paperId": "0fae5a461e88461428df1762367aed22cc99a532",
        "title": "A Probabilistic Interpretation of Self-Paced Learning with Applications to Reinforcement Learning"
      },
      {
        "paperId": "c54d32c13298ee10abf6edb2ac8f18018519c8ce",
        "title": "No-Regret Reinforcement Learning with Heavy-Tailed Rewards"
      },
      {
        "paperId": "7fc04f8031598da6510bce4a7e5b4722305ca5b0",
        "title": "State Entropy Maximization with Random Encoders for Efficient Exploration"
      },
      {
        "paperId": "8a4a90a925d0376e740df508ac307613c647e5c3",
        "title": "DEUP: Direct Epistemic Uncertainty Prediction"
      },
      {
        "paperId": "ac6a1262023834d31febf503306b0d48154fa1cd",
        "title": "Policy Augmentation: An Exploration Strategy For Faster Convergence of Deep Reinforcement Learning Algorithms"
      },
      {
        "paperId": "1e299e8ff824224fcdf3cc9d5508cd24f66a6578",
        "title": "Modeling the Interaction between Agents in Cooperative Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "da7d5ef65577aafecc8764c4435685bac10e0a46",
        "title": "Sparse reward exploration via novelty search and emitters"
      },
      {
        "paperId": "5ca0dbb82b8f01c0e9f4b7fb6ea4f54582f15bfe",
        "title": "View-Action Representation Learning for Active First-Person Vision"
      },
      {
        "paperId": "1382b397451203df444f59ece2b9dbefa1d7fb51",
        "title": "Neural Discrete Abstraction of High-Dimensional Spaces: A Case Study In Reinforcement Learning"
      },
      {
        "paperId": "9e5fe2ba652774ba3b1127f626c192668a907132",
        "title": "Decoupled Exploration and Exploitation Policies for Sample-Efficient Reinforcement Learning"
      },
      {
        "paperId": "a188c3b58e71657ecfcddc37359aca51213e2187",
        "title": "Rank the Episodes: A Simple Approach for Exploration in Procedurally-Generated Environments"
      },
      {
        "paperId": "1c605fbece881c92df05f32565ea4e44b8f1a0b6",
        "title": "Vision-Based Autonomous Navigation Approach for a Tracked Robot Using Deep Reinforcement Learning"
      },
      {
        "paperId": "ee3f989a4c39836e92e801fa71266dc258049e00",
        "title": "MetaVIM: Meta Variationally Intrinsic Motivated Reinforcement Learning for Decentralized Traffic Signal Control"
      },
      {
        "paperId": "f4fd4a5a0901675e19bb4054d8861d2697194fd5",
        "title": "Deep Reinforcement Learning: A State-of-the-Art Walkthrough"
      },
      {
        "paperId": "463a75286a1246594a7cf8f6abfc8fe557bc7dde",
        "title": "Improved Sample Complexity for Incremental Autonomous Exploration in MDPs"
      },
      {
        "paperId": "ff3f7fbebe2159bb97eb79686eaf338ded8fc325",
        "title": "Self-Imitation Advantage Learning"
      },
      {
        "paperId": "18b1d9052cb82dc7c34827768c98aae09caeb649",
        "title": "Curiosity in exploring chemical spaces: intrinsic rewards for molecular reinforcement learning"
      },
      {
        "paperId": "85a965d01a9977714dafe8a6ccf6455318ee0938",
        "title": "General Policies, Serializations, and Planning Width"
      },
      {
        "paperId": "1760e16f7d6603c1f4981a3d7a9a188699ede2db",
        "title": "Policy Manifold Search for Improving Diversity-based Neuroevolution"
      },
      {
        "paperId": "376e0853411acb4e5732c587471d0a3910689b20",
        "title": "Adapting Behavior via Intrinsic Reward: A Survey and Empirical Study"
      },
      {
        "paperId": "bcd39372663bec903874a4b19582d7d4136916f0",
        "title": "Random curiosity-driven exploration in deep reinforcement learning"
      },
      {
        "paperId": "350904da231fa0c28e8a6b9358198366db9a2f9b",
        "title": "Revisiting Rainbow: Promoting more insightful and inclusive deep reinforcement learning research"
      },
      {
        "paperId": "3c9863eecca66c544ffc5f821d5703f9af179fd5",
        "title": "Model-based Reinforcement Learning for Continuous Control with Posterior Sampling"
      },
      {
        "paperId": "6b5adfa3cd172198c48997635d633579a6123eea",
        "title": "Leveraging the Variance of Return Sequences for Exploration Policy"
      },
      {
        "paperId": "b405764900fd2ec3979b16633056e0e6434973a8",
        "title": "Ridge Rider: Finding Diverse Solutions by Following Eigenvectors of the Hessian"
      },
      {
        "paperId": "b667641dc6acd7c0233503615942ea00ea9875f5",
        "title": "Continual Learning of Control Primitives: Skill Discovery via Reset-Games"
      },
      {
        "paperId": "a435dda070ce517c975b0c0b77a6048c57545f7c",
        "title": "Perturbation-based exploration methods in deep reinforcement learning"
      },
      {
        "paperId": "20473d88bb151fd94b05ea6ce3950113f4e9d10f",
        "title": "How do Offline Measures for Exploration in Reinforcement Learning behave?"
      },
      {
        "paperId": "2796d668f47fe53335438908876d29db08fae7ea",
        "title": "Exploration Strategy based on Validity of Actions in Deep Reinforcement Learning"
      },
      {
        "paperId": "0d1a96ce841ca438b295ba1f1c3803887b93d19e",
        "title": "TTR-Based Reward for Reinforcement Learning with Implicit Model Priors"
      },
      {
        "paperId": "151f167d781e073d29bc7b96b45aff6b81be3637",
        "title": "TAMPC: A Controller for Escaping Traps in Novel Environments"
      },
      {
        "paperId": "243467598d7db59aa7467763f5863a0f1ae0b6b4",
        "title": "Batch Exploration With Examples for Scalable Robotic Reinforcement Learning"
      },
      {
        "paperId": "c555556f277c1d72dc32dd0ba6f033f1afe52029",
        "title": "Optimising Stochastic Routing for Taxi Fleets with Model Enhanced Reinforcement Learning"
      },
      {
        "paperId": "2244e2af88d4efc911e22cf9dd9a954eb9398607",
        "title": "An empowerment-based solution to robotic manipulation tasks with sparse rewards"
      },
      {
        "paperId": "16e83f3f0f78ceb203746eeb88f1f5aae9ba3092",
        "title": "Deep reinforcement learning: a survey"
      },
      {
        "paperId": "72ec6eab1efd943fee33951eaf11945af5d37048",
        "title": "Inference-Based Posteriori Parameter Distribution Optimization"
      },
      {
        "paperId": "10228cfc5ccb33119262a83de5db8a58dd35d9a5",
        "title": "UneVEn: Universal Value Exploration for Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "b33b735352c78743707f66e525f7cca65ff207b0",
        "title": "Latent World Models For Intrinsically Motivated Exploration"
      },
      {
        "paperId": "0ca7c0d92a10359a2a7b8fd501a40c9ef768676d",
        "title": "Exploration in Approximate Hyper-State Space for Meta Reinforcement Learning"
      },
      {
        "paperId": "a98b9dbc8d84903d01944e7ba9197789764f3330",
        "title": "Exploration: from machines to humans"
      },
      {
        "paperId": "b1205a259d9a609c7d2c262da371387db57c5aa7",
        "title": "Reannealing of Decaying Exploration Based On Heuristic Measure in Deep Q-Network"
      },
      {
        "paperId": "d4d86f4a632d899ba5f511c1d0e7a7fbd44288c8",
        "title": "Novelty Search in representational space for sample efficient exploration"
      },
      {
        "paperId": "b5fc932d63a62ea5e6a77bb2ac703e66946ccfc4",
        "title": "SEMI: Self-supervised Exploration via Multisensory Incongruity"
      },
      {
        "paperId": "fab4008c13a365e1f3afa8d7a4c1aaede4849a8d",
        "title": "Revisiting Design Choices in Proximal Policy Optimization"
      },
      {
        "paperId": "a2b00e570440619bbe6a483df6b0da46b1aae665",
        "title": "Fast and slow curiosity for high-level exploration in reinforcement learning"
      },
      {
        "paperId": "ebbe4f4107c6b7190df5f340a468ef9012f2f15b",
        "title": "Evolutionary Reinforcement Learning via Cooperative Coevolutionary Negatively Correlated Search"
      },
      {
        "paperId": "654da778204b7f2a33aebbea99d31da7afd6bcc5",
        "title": "An effective maximum entropy exploration approach for deceptive game in reinforcement learning"
      },
      {
        "paperId": "389dc120b62005e60a3ead2e3bbf257537568ee3",
        "title": "On Advances in Deep Learning with Applications in Financial Market Modeling"
      },
      {
        "paperId": "c864b7a2bd0d82c1c091b9bae2a33f1ef927f6d2",
        "title": "REMAX: Relational Representation for Multi-Agent Exploration"
      },
      {
        "paperId": "6df5d79885bf1af65323a055684b2b92439a65c3",
        "title": "GRIMGEP: Learning Progress for Robust Goal Sampling in Visual Deep Reinforcement Learning"
      },
      {
        "paperId": "5f978b3829acad6ac8b3372d2fa8d38a45b96d3d",
        "title": "Noisy Agents: Self-supervised Exploration by Predicting Auditory Events"
      },
      {
        "paperId": "5347abe013ae752ed749a2a15dc56cba7866a3da",
        "title": "Understanding exploration in humans and machines by formalizing the function of curiosity"
      },
      {
        "paperId": "9ffc4fa1db8372d763f990a1f0a6985260d693b0",
        "title": "Explore and Explain: Self-supervised Navigation and Recounting"
      },
      {
        "paperId": "b1b0f9c33e4b3253a27e651d3ed47d29867d4df4",
        "title": "Learning Abstract Models for Strategic Exploration and Fast Reward Transfer"
      },
      {
        "paperId": "5c314a73207652d784044e82ff392cd8699edc28",
        "title": "Follow then Forage Exploration: Improving Asynchronous Advantage Actor Critic"
      },
      {
        "paperId": "5ab5d94f1a9ffeb076cad515854bbe0ea9c49db5",
        "title": "A Policy Gradient Method for Task-Agnostic Exploration"
      },
      {
        "paperId": "7c8dc7ee9c6910753cd9e20a4569db4731b0c41d",
        "title": "Guided Exploration with Proximal Policy Optimization using a Single Demonstration"
      },
      {
        "paperId": "48e15d9d875b8cb74a1261a61e2d64d225078f7b",
        "title": "See, Hear, Explore: Curiosity via Audio-Visual Association"
      },
      {
        "paperId": "72260c19441259404ed24003d9e27588fb3613ae",
        "title": "Maximum Entropy Gain Exploration for Long Horizon Multi-goal Reinforcement Learning"
      },
      {
        "paperId": "b0d65b8a41636def1c4a606d186eed41e03b7925",
        "title": "Regularly Updated Deterministic Policy Gradient Algorithm"
      },
      {
        "paperId": "a0f814398da24f602d43a6a268332a5d26b91464",
        "title": "Exploring Parameter Space with Structured Noise for Meta-Reinforcement Learning"
      },
      {
        "paperId": "45e11b96848fda789656913fc92515ccfb3cb319",
        "title": "Model-Based Robot Learning Control with Uncertainty Directed Exploration"
      },
      {
        "paperId": "a57f9e1a5a6331eabca988c4615c2fbbd34cf21d",
        "title": "Show me the Way: Intrinsic Motivation from Demonstrations"
      },
      {
        "paperId": "96fa6a040cad64170765016694d797e14212564f",
        "title": "Ecological Reinforcement Learning"
      },
      {
        "paperId": "f018493d8f64955f9fdd80a4b95465a327a771cd",
        "title": "Towards tractable optimism in model-based reinforcement learning"
      },
      {
        "paperId": "7d3699cf0e8a822cd8ae99b8eeed146930d24a0f",
        "title": "On Optimism in Model-Based Reinforcement Learning"
      },
      {
        "paperId": "0c39fbcc11e707901649c5bfc985a1989867283e",
        "title": "NROWAN-DQN: A Stable Noisy Network with Noise Reduction and Online Weight Adjustment for Exploration"
      },
      {
        "paperId": "3eebdbffbc05d6b2987010c4d594cfc11b9b5071",
        "title": "On Reward-Free Reinforcement Learning with Linear Function Approximation"
      },
      {
        "paperId": "034b2e3d957df5405783f2c5695a8c2bd87d6334",
        "title": "FLAMBE: Structural Complexity and Representation Learning of Low Rank MDPs"
      },
      {
        "paperId": "a011901fd788fc5ad452dd3d88f9f0970ea547a8",
        "title": "QD-RL: Efficient Mixing of Quality and Diversity in Reinforcement Learning"
      },
      {
        "paperId": "fca28a48e3a01b9ad4c269511d9d23ff22ccd051",
        "title": "Non-local Policy Optimization via Diversity-regularized Collaborative Exploration"
      },
      {
        "paperId": "669f07843065e471974080608a3cbee876d13cef",
        "title": "Adaptive Reward-Free Exploration"
      },
      {
        "paperId": "a13e1cb07cc35666b5a5c384c92416126f87fee2",
        "title": "Diversity Actor-Critic: Sample-Aware Entropy Regularization for Sample-Efficient Exploration"
      },
      {
        "paperId": "5de6d8b4436f6598f5bcba00bc07d864e962f1fb",
        "title": "Temporally-Extended {\\epsilon}-Greedy Exploration"
      },
      {
        "paperId": "822bdb6e8c39e272ebfee127666e032bd3aa0107",
        "title": "The NetHack Learning Environment"
      },
      {
        "paperId": "99bc5a19a5aa9f85e2c1ecdc9bf8add7b75dae45",
        "title": "Novel Policy Seeking with Constrained Optimization"
      },
      {
        "paperId": "40fd95d42c0f56e7cbae2838bd5ecb9e2e71a271",
        "title": "Experience Augmentation: Boosting and Accelerating Off-Policy Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "ba1ec8e5e8ab6c594fd6223853a1e5c787c38503",
        "title": "TOMA: Topological Map Abstraction for Reinforcement Learning"
      },
      {
        "paperId": "5b6b2ce6e809d8f54416a3d04e0ef474094be779",
        "title": "Exploring Exploration: Comparing Children with RL Agents in Unified Environments"
      },
      {
        "paperId": "0a38ed285c159e07cbf5edaf4b73ea4e045406d2",
        "title": "ACDER: Augmented Curiosity-Driven Experience Replay"
      },
      {
        "paperId": "e1a39a6614503546bbb72a8c75aaf0ae93a3ac01",
        "title": "Option Discovery using Deep Skill Chaining"
      },
      {
        "paperId": "2b2735cffb0d2321a456363880ff5671e80df4cb",
        "title": "On Bonus Based Exploration Methods In The Arcade Learning Environment"
      },
      {
        "paperId": "ae3b2768b0a3c73410bce0d2ae03feaf01f6f864",
        "title": "Dynamics-Aware Unsupervised Skill Discovery"
      },
      {
        "paperId": "616ac6772508e72084360158078058dfa8bda7e7",
        "title": "First return, then explore"
      },
      {
        "paperId": "de93c8aed64229571b03e40b36499d4f07ce875d",
        "title": "PBCS : Efficient Exploration and Exploitation Using a Synergy between Reinforcement Learning and Motion Planning"
      },
      {
        "paperId": "2e0cd27cd60acbbf2bc5f62d773df9c0eacacd6b",
        "title": "Self-Paced Deep Reinforcement Learning"
      },
      {
        "paperId": "c748dc8700684d90efe228043264ea5baac68c18",
        "title": "Flexible and Efficient Long-Range Planning Through Curious Exploration"
      },
      {
        "paperId": "a8cbef54001aa2077631827f10ca89a702e01a07",
        "title": "Generalize Robot Learning From Demonstration to Variant Scenarios With Evolutionary Policy Gradient"
      },
      {
        "paperId": "c0142146ff9cd2ff84001bd6eb48aca88318ff81",
        "title": "Improving Robot Dual-System Motor Learning with Intrinsically Motivated Meta-Control and Latent-Space Experience Imagination"
      },
      {
        "paperId": "d6c2107d265bf461ea08e8180b740d2eb745812d",
        "title": "Zero-Shot Learning of Text Adventure Games with Sentence-Level Semantics"
      },
      {
        "paperId": "9651e1987a39d100dc0f6696a2077199e5075ea2",
        "title": "Agent57: Outperforming the Atari Human Benchmark"
      },
      {
        "paperId": "afbc0140fd97899af5d977fdc2f6fad4dc4679bb",
        "title": "Provably Efficient Exploration for RL with Unsupervised Learning"
      },
      {
        "paperId": "d4e1ecb4f5adc0c2cbb7b17cd5679e76d90f4c63",
        "title": "Provably Efficient Exploration for Reinforcement Learning Using Unsupervised Learning"
      },
      {
        "paperId": "845aeb9dcf12efba0760c6eb2e2ac56ea27f3247",
        "title": "Option Discovery in the Absence of Rewards with Manifold Analysis"
      },
      {
        "paperId": "11236ae4f31b428b6313559fb99300643c172cf9",
        "title": "Meta-learning curiosity algorithms"
      },
      {
        "paperId": "6d5acdfa087409a2dd59c425cb6c68030f90b33b",
        "title": "Exploring Unknown States with Action Balance"
      },
      {
        "paperId": "1ba26d96f857883bda6b3b43c91bf6ffdc572053",
        "title": "Independent learning approaches: overcoming multi-agent learning pathologies in team-games"
      },
      {
        "paperId": "cab9ccf5b316c268e863d1206e90831413629d74",
        "title": "Efficient Multiagent Policy Optimization Based on Weighted Estimators in Stochastic Cooperative Environments"
      },
      {
        "paperId": "bebe8ffb0c357ac0c7eea2556f817b03ee22b570",
        "title": "RIDE: Rewarding Impact-Driven Exploration for Procedurally-Generated Environments"
      },
      {
        "paperId": "62c1bc6a8bffe09a1e1138ffd37a64988dc27d48",
        "title": "Optimistic Exploration even with a Pessimistic Initialisation"
      },
      {
        "paperId": "3134dc06e80bf955cc72985ef8b3e32d4bc6183b",
        "title": "Strategic Exploration in Reinforcement Learning - New Algorithms and Learning Guarantees"
      },
      {
        "paperId": "9db77e925015ad02efc9beeab233bedbfe04e4b7",
        "title": "Accelerating Reinforcement Learning with a Directional-Gaussian-Smoothing Evolution Strategy"
      },
      {
        "paperId": "3525ef618f14e331ccd01e2e871a4c24c4cc3486",
        "title": "Online Learning in Contextual Bandits using Gated Linear Networks"
      },
      {
        "paperId": "00133d41d5ecef1a9d046d2b92bb2a23a335cb7c",
        "title": "TempLe: Learning Template of Transitions for Sample Efficient Multi-task RL"
      },
      {
        "paperId": "39b2551f109fd0495abfca1ab0bb81311c1b3996",
        "title": "Explore, Discover and Learn: Unsupervised Discovery of State-Covering Skills"
      },
      {
        "paperId": "2d8a819980ba9413c8c5bea1a078ca8e634d3861",
        "title": "Vision-Language Navigation Policy Learning and Adaptation"
      },
      {
        "paperId": "14c88059503461afdab6d59c3e56914b6690eb02",
        "title": "An Exploration of Embodied Visual Exploration"
      },
      {
        "paperId": "2fb74185b99f69b488896e8efcb2730a57fbdf6f",
        "title": "Large-scale automated investigation of free-falling paper shapes via iterative physical experimentation"
      },
      {
        "paperId": "a2e43270a9b1421e452c2975e5163e2a216abeac",
        "title": "A Survey of Deep Reinforcement Learning in Video Games"
      },
      {
        "paperId": "8b559ea2c4abe9bbca4b5d2820dbb9cbeb37d510",
        "title": "Marginalized State Distribution Entropy Regularization in Policy Optimization"
      },
      {
        "paperId": "741ebf91225327bbd11e0e4fa08e329d45637b13",
        "title": "Optimism in Reinforcement Learning with Generalized Linear Function Approximation"
      },
      {
        "paperId": "140788dc3c2e3d716fc6d8b3ec9ab04bccf7001d",
        "title": "Long-Term Visitation Value for Deep Exploration in Sparse Reward Reinforcement Learning"
      },
      {
        "paperId": "ce9b93c52114e065a3f9175341fcc1df964d48e8",
        "title": "Evaluating task-agnostic exploration for fixed-batch learning of arbitrary future tasks"
      },
      {
        "paperId": "f89233364051f83997be5f0d34c3372dd6758f5e",
        "title": "Bayesian Curiosity for Efficient Exploration in Reinforcement Learning"
      },
      {
        "paperId": "9d01ca964043a52b265534e94abc37210a50cc5a",
        "title": "Kinematic State Abstraction and Provably Efficient Rich-Observation Reinforcement Learning"
      },
      {
        "paperId": "c9849e184301443d1761dda5603c6a17d4b2af19",
        "title": "Multi-Path Policy Optimization"
      },
      {
        "paperId": "0e54edd0c55c0cb63e719b501e41790c75a2c73a",
        "title": "Exploration-exploitation dilemma in Reinforcement Learning under various form of prior knowledge. (Impact des connaissances a priori sur le compromis exploration-exploitation en apprentissage par renforcement)"
      },
      {
        "paperId": "5b925493d2812a5077c4e135cd07bf6647ba59df",
        "title": "Efficient Exploration in Side-Scrolling Video Games with Trajectory Replay"
      },
      {
        "paperId": "e30fa08b1ef8f3e7a2394a4467dc0eddcff04681",
        "title": "Keeping Your Distance: Solving Sparse Reward Tasks Using Self-Balancing Shaped Rewards"
      },
      {
        "paperId": "542401c937e602ce22e1f5c781022aac2d64c023",
        "title": "Improved Exploration through Latent Trajectory Optimization in Deep Deterministic Policy Gradient"
      },
      {
        "paperId": "3110f8caaeddb0c5b0d95ae46e85cffab9f1847b",
        "title": "Dynamic Subgoal-based Exploration via Bayesian Optimization"
      },
      {
        "paperId": "1c3dafb77bcaf7fcfa3e34621d0d32176a95cf5b",
        "title": "Exploration via Sample-Efficient Subgoal Design"
      },
      {
        "paperId": "34c4621319b316cbca8fc1ff12739ca691fe9f95",
        "title": "Exploration via Cost-Aware Subgoal Design"
      },
      {
        "paperId": "9fea5a15df9fa4b1a488896cdb0e4db1f05f8ed1",
        "title": "Parallel exploration via negatively correlated search"
      },
      {
        "paperId": "838010e42249bceaa4a71ef15b81c50b1d8d6451",
        "title": "Generative adversarial exploration for reinforcement learning"
      },
      {
        "paperId": "46106fcd08223540d080f674b26770c1fa8a52ff",
        "title": "Influence-Based Multi-Agent Exploration"
      },
      {
        "paperId": "1534d860ab98b64d23ffa741d0ae52b6cadbf503",
        "title": "Benchmarking Batch Deep Reinforcement Learning Algorithms"
      },
      {
        "paperId": "74d17a438e268f972cdb60e18e74c4af90f52093",
        "title": "Autonomous Visual Navigation using Deep Reinforcement Learning: An Overview"
      },
      {
        "paperId": "2066b64c446968143e59f8e6d1637a80acfd4739",
        "title": "Scoring-Aggregating-Planning: Learning task-agnostic priors from interactions and sparse rewards for zero-shot generalization"
      },
      {
        "paperId": "6f6eb95b2bee897c3ba90cd6018d8545bed07abe",
        "title": "Self-Imitation Learning via Trajectory-Conditioned Policy for Hard-Exploration Tasks"
      },
      {
        "paperId": "0d6fcc24621f2f3be1785c4b864d018e25126f1f",
        "title": "Segregation dynamics with reinforcement learning and agent based modeling"
      },
      {
        "paperId": "33a2c0f3b9a0adc452a13d53f950c0c3c4abb11b",
        "title": "Emergent Tool Use From Multi-Agent Autocurricula"
      },
      {
        "paperId": "521538b3bc1fd64fb0d4dc71881edbeff03c642a",
        "title": "Attraction-Repulsion Actor-Critic for Continuous Control Reinforcement Learning"
      },
      {
        "paperId": "39c7ae0e161742a1184e7e8ebc2e65020fd97904",
        "title": "Model Based Planning with Energy Based Models"
      },
      {
        "paperId": "add0be0ce4228bf2e7fcacd333af03110a64713a",
        "title": "Imitation Learning from Pixel-Level Demonstrations by HashReward"
      },
      {
        "paperId": "9fd69b5eeee945c12ff8efde85a6fe8be34d784f",
        "title": "Expert-Level Atari Imitation Learning from Demonstrations Only"
      },
      {
        "paperId": "9d1c2998e7e0e873a67fb075ce3878f72f6182ee",
        "title": "AI Agents for Sequential Promotions: Combining Deep Reinforcement Learning and Dynamic Field Experimentation"
      },
      {
        "paperId": "c10a513140aa640385b3c60ad293b8d8554fc980",
        "title": "Proximal Parameter Distribution Optimization"
      },
      {
        "paperId": "0832a57ce3707cdcc88bc66d166bc5d6107e32e9",
        "title": "Curiosity-Driven Exploration Effectiveness on Various Environments"
      },
      {
        "paperId": "222baa4e9e7ce691fdfddbc826a70e027daed70d",
        "title": "Reinforcement Learning in Healthcare: A Survey"
      },
      {
        "paperId": "895735cace0de940aa647dbafc046b7f30316fe5",
        "title": "A survey on intrinsic motivation in reinforcement learning"
      },
      {
        "paperId": "8c78c9de81ef31a6550f07a65a37dfa38057a091",
        "title": "Learn How to Cook a New Recipe in a New House: Using Map Familiarization, Curriculum Learning, and Bandit Feedback to Learn Families of Text-Based Adventure Games"
      },
      {
        "paperId": "fa4277c83ae5c6c5b9fa5e58c9053e2d92cb8b0e",
        "title": "Benchmarking Bonus-Based Exploration Methods on the Arcade Learning Environment"
      },
      {
        "paperId": "d4c8f6c4a2b744fcfd82a7d7c8041d87d2b5c250",
        "title": "Curiosity-driven Reinforcement Learning for Diverse Visual Paragraph Generation"
      },
      {
        "paperId": "72799883b3378309e195a6bbd3c42fcaa88bd962",
        "title": "Efficient Intrinsically Motivated Robotic Grasping with Learning-Adaptive Imagination in Latent Space"
      },
      {
        "paperId": "df7a355aa5a069a3a0376e49026a064cc4a76561",
        "title": "Synchronous n-Step Method for Independent Q-Learning in Multi-Agent Deep Reinforcement Learning"
      },
      {
        "paperId": "972232a9f5406b046c4de2d9509759815c9fc36a",
        "title": "Modified PPO-RND Method for Solving Sparse Reward Problem in ViZDoom"
      },
      {
        "paperId": "b3e20a79d802baee478351d111e22e0e513ba376",
        "title": "Efficient Exploration with Self-Imitation Learning via Trajectory-Conditioned Policy"
      },
      {
        "paperId": "ef408defa807f0e364129796dc05b9b683344eb1",
        "title": "Potential-Based Advice for Stochastic Policy Learning *"
      },
      {
        "paperId": "74113c6764f0192fceedf4ae35d3bee26b26f377",
        "title": "Deep Reinforcement Learning via Past-Success Directed Exploration"
      },
      {
        "paperId": "969f6ef56d812fdb3afc7d2a84c59fdfc609dc15",
        "title": "Planning Approximate Exploration Trajectories for Model-Free Reinforcement Learning in Contact-Rich Manipulation"
      },
      {
        "paperId": "49a05c4f0505344317fcdbf08066aebfa1aadc3b",
        "title": "An Intrinsically-Motivated Approach for Learning Highly Exploring and Fast Mixing Policies"
      },
      {
        "paperId": "ffb3886a253ff927bcc46b78e00409893865a68e",
        "title": "Dynamics-Aware Unsupervised Discovery of Skills"
      },
      {
        "paperId": "94c731fd8982a24867e69e585d7887b8762e4736",
        "title": "MULEX: Disentangling Exploitation from Exploration in Deep RL"
      },
      {
        "paperId": "12e9394725a085a89114d2c981ce4a76ec6f37cf",
        "title": "Efficient and Scalable Exploration via Estimation-Error"
      },
      {
        "paperId": "7b22ed1e17afecb1d45e8862a71a6114da989ba2",
        "title": "Exploiting Action-Value Uncertainty to Drive Exploration in Reinforcement Learning"
      },
      {
        "paperId": "c6011b09783150bcda711c8d3176385875020a60",
        "title": "\u00c9tude de la motivation intrins\u00e8que en apprentissage par renforcement"
      },
      {
        "paperId": "bed33a92b759de8437ae51bceaf1f56f25e38df0",
        "title": "Optimistic Proximal Policy Optimization"
      },
      {
        "paperId": "8c07f69db199264c53accf45cb39fc8aa1ea632f",
        "title": "Reward Prediction Error as an Exploration Objective in Deep RL"
      },
      {
        "paperId": "8e07d0452bf09c0abb5a6df5b3315a9989fdf040",
        "title": "QXplore: Q-learning Exploration by Maximizing Temporal Difference Error"
      },
      {
        "paperId": "c6b8d35da220d274b561bc0affa12810990e720a",
        "title": "Adapting Behaviour via Intrinsic Reward: A Survey and Empirical Study"
      },
      {
        "paperId": "ee87cf824562db74b8555637652b22aa30589343",
        "title": "Directed Exploration for Reinforcement Learning"
      },
      {
        "paperId": "39e299cde9053346284b074d8ca58e083ad85cbd",
        "title": "Learning-Driven Exploration for Reinforcement Learning"
      },
      {
        "paperId": "cfec54f9208dcbb2c558aa7ff45f57be2533d046",
        "title": "Efficient Exploration via State Marginal Matching"
      },
      {
        "paperId": "5782b218017316542ec695cab39c0eca709e5733",
        "title": "Clustered Reinforcement Learning"
      },
      {
        "paperId": "d10d406a3bd115f8abb24a4d2c127caa502516b4",
        "title": "Exploration with Unreliable Intrinsic Reward in Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "af581dd2243ba89ba476f790cde876d1e1b6774b",
        "title": "Efficient Exploration in Reinforcement Learning through Time-Based Representations"
      },
      {
        "paperId": "51fe965c579a689d1dc466f2313227a07eb22126",
        "title": "Safety Augmented Value Estimation From Demonstrations (SAVED): Safe Deep Model-Based RL for Sparse Cost Robotic Tasks"
      },
      {
        "paperId": "4a417479df70d216e19c99d4a5408e41488d3621",
        "title": "Extending Deep Model Predictive Control with Safety Augmented Value Estimation from Demonstrations"
      },
      {
        "paperId": "c01dd6bc93ed01124079938cae19aba040273fc6",
        "title": "Coordinated Exploration via Intrinsic Rewards for Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "81768cdace11e14982d3aba9060059e1133d83fc",
        "title": "Learning Efficient and Effective Exploration Policies with Counterfactual Meta Policy"
      },
      {
        "paperId": "47231dec371ac92a5caabf64508ed5332cf7e4f8",
        "title": "Beyond Exponentially Discounted Sum: Automatic Learning of Return Function"
      },
      {
        "paperId": "8db16192c8abf006b90d22aab3e7fe3694640836",
        "title": "Hypothesis-Driven Skill Discovery for Hierarchical Deep Reinforcement Learning"
      },
      {
        "paperId": "801eb90352e603b1573a04a36804417d1c9496f0",
        "title": "Learning latent state representation for speeding up exploration"
      },
      {
        "paperId": "c60d789bf93dee52fc1e076c005cfb8385c84719",
        "title": "Curiosity-Bottleneck: Exploration By Distilling Task-Specific Novelty"
      },
      {
        "paperId": "6b36776e5c0473d82cbdd2c92cd97cca7925ae08",
        "title": "TibGM: A Transferable and Information-Based Graphical Model Approach for Reinforcement Learning"
      },
      {
        "paperId": "9d504ea4d1e52ae11ebe44e9b8003ec477e3b2a5",
        "title": "Combine PPO with NES to Improve Exploration"
      },
      {
        "paperId": "d6285ff3dcb15c1da84dcbc98141be10ef0e8dd1",
        "title": "Estimating Risk and Uncertainty in Deep Reinforcement Learning"
      },
      {
        "paperId": "ab5d0df373249bbfa0bb7476caedb3f34498b139",
        "title": "Reinforcement Learning without Ground-Truth State"
      },
      {
        "paperId": "bcfb3a3cec67aaaadfe563425e01ddf5ec4f5bfa",
        "title": "Combining Experience Replay with Exploration by Random Network Distillation"
      },
      {
        "paperId": "c3bd5dd10f555bc1622cd37598bdbdf5635c94fe",
        "title": "Leveraging exploration in off-policy algorithms via normalizing flows"
      },
      {
        "paperId": "ec9cf6922aae97b7c728693e78bd5cb812cc4e1e",
        "title": "Learning Novel Policies For Tasks"
      },
      {
        "paperId": "f6b218453170edcbb51e49dd44ba2f83af53ef92",
        "title": "Distributional Reinforcement Learning for Efficient Exploration"
      },
      {
        "paperId": "edcd3a5e8e0e6fd27f95c34348404ea449b6927d",
        "title": "Mega-Reward: Achieving Human-Level Play without Extrinsic Rewards"
      },
      {
        "paperId": "79fe495ce79ea200af7df54f8c8b4d4939d27be8",
        "title": "Exploration in the Face of Parametric and Intrinsic Uncertainties"
      },
      {
        "paperId": "b91f6a10375e7064ca57400ad265114b8365f0ed",
        "title": "Curious Meta-Controller: Adaptive Alternation between Model-Based and Model-Free Control in Deep Reinforcement Learning"
      },
      {
        "paperId": "f2b400335fda58d043c57289055db12ef488787e",
        "title": "Meta-learners' learning dynamics are unlike learners'"
      },
      {
        "paperId": "98c08d28a2319ce79b62b108dfed3acc0dd80983",
        "title": "Collaborative Evolutionary Reinforcement Learning"
      },
      {
        "paperId": "7abf2c5295bee7d18868e0c6bc961e8ea61c9074",
        "title": "Efficient Model-free Reinforcement Learning in Metric Spaces"
      },
      {
        "paperId": "e739403b08e22f4846e17fe8b2a1a9eb7b568d62",
        "title": "Learning Action Representations for Self-supervised Visual Exploration"
      },
      {
        "paperId": "b89f8e1b6eeb4639a34c8b287fdf0c7ea560be79",
        "title": "Curiosity-driven Reinforcement Learning for Dialogue Management"
      },
      {
        "paperId": "1b988fc987e1907afe3f3b1dd0e975defb37f4a4",
        "title": "Towards Combining On-Off-Policy Methods for Real-World Applications"
      },
      {
        "paperId": "fbe22105c3dd0fa3220e58b4a167129f1e548581",
        "title": "Generative Exploration and Exploitation"
      },
      {
        "paperId": "c35cf98ba89f7755f0c35acd7f4adf1c2b7482eb",
        "title": "Toward Deep Satisficing Reinforcement Learning"
      },
      {
        "paperId": "b57c24654d21b16067493e4e217f63281dcba086",
        "title": "TTR-Based Rewards for Reinforcement Learning with Implicit Model Priors"
      },
      {
        "paperId": "39905a5b13b41b47c3f6dfb48412f278c53f167f",
        "title": "Learning Gentle Object Manipulation with Curiosity-Driven Deep Reinforcement Learning"
      },
      {
        "paperId": "28348ca34d609bd52ff10705bf396cc1da3e85ab",
        "title": "Adaptive Variance for Changing Sparse-Reward Environments"
      },
      {
        "paperId": "7388826b5ee00efe17cb7f19a623d9b5e955ae70",
        "title": "Skew-Fit: State-Covering Self-Supervised Reinforcement Learning"
      },
      {
        "paperId": "b8c0071c74e04ea1598ed2a208cbc255656f50b0",
        "title": "A Meta-MDP Approach to Exploration for Lifelong Reinforcement Learning"
      },
      {
        "paperId": "ee248883b124a80bdbbc686e906145ccdeadc834",
        "title": "Competitive Experience Replay"
      },
      {
        "paperId": "bf7f1ada5feecc0992f71b39c1ebeccb19ae631b",
        "title": "InfoBot: Transfer and Exploration via the Information Bottleneck"
      },
      {
        "paperId": "c520bf47db3360ae3a52219771390a354ed8a91f",
        "title": "Go-Explore: a New Approach for Hard-Exploration Problems"
      },
      {
        "paperId": "72ecfb12938f044afdd2d9191c8e61386e2559bd",
        "title": "Never Forget: Balancing Exploration and Exploitation via Learning Optical Flow"
      },
      {
        "paperId": "06be94b4ab482d66368e65206a8aa0dd8a005b1b",
        "title": "A Short Survey on Probabilistic Reinforcement Learning"
      },
      {
        "paperId": "93ed1bfd12ddd23f7bf9410774cafac204189d56",
        "title": "Visual novelty, curiosity, and intrinsic reward in machine learning and the brain"
      },
      {
        "paperId": "f06b33e59ff17d891604fb5f0f81831819b9da64",
        "title": "Exploration Bonus for Regret Minimization in Undiscounted Discrete and Continuous Markov Decision Processes"
      },
      {
        "paperId": "5d4ebbaa1ef8feeac885e2869f45c0276c18834f",
        "title": "Learning Montezuma's Revenge from a Single Demonstration"
      },
      {
        "paperId": "adab275554eb745cdc21fd6c526ec55a5b2ef362",
        "title": "Provably Efficient Maximum Entropy Exploration"
      },
      {
        "paperId": "c66b8e508718f4b7f14829e5c2cde0add31d2693",
        "title": "Reinforced Cross-Modal Matching and Self-Supervised Imitation Learning for Vision-Language Navigation"
      },
      {
        "paperId": "54b0e71772f4c1f9e7c254faf51d3a02faef290b",
        "title": "Policy Optimization with Model-based Explorations"
      },
      {
        "paperId": "e93c016291d9f5da54f1c1cfc2575110fe5580b9",
        "title": "QUOTA: The Quantile Option Architecture for Reinforcement Learning"
      },
      {
        "paperId": "e227288aef97894bdf6ca0b07ad0b5f6108f6f52",
        "title": "Towards Governing Agent's Efficacy: Action-Conditional \u03b2-VAE for Deep Transparent Reinforcement Learning"
      },
      {
        "paperId": "f2ac2a3fd7b341f2b1be752b4dd46ed9abcf0751",
        "title": "Deep Reinforcement Learning"
      },
      {
        "paperId": "d69fa24c177d7861bc648b43f809daab245d4071",
        "title": "Empowerment-driven Exploration using Mutual Information Estimation"
      },
      {
        "paperId": "bf2a70ecfe3a0b701a16104cd8b880a41353dd2f",
        "title": "Scaling All-Goals Updates in Reinforcement Learning Using Convolutional Neural Networks"
      },
      {
        "paperId": "51ecd565f8a394b1907b3d37722435a573ce7b72",
        "title": "EMI: Exploration with Mutual Information"
      },
      {
        "paperId": "037b2b6ad206ba2ff657dc8dd31f2942c03d5466",
        "title": "Definition and Evaluation of Model-Free Coordination of Electrical Vehicle Charging With Reinforcement Learning"
      },
      {
        "paperId": "bf604ae3ddd5adec55554921b37f04035b7350a7",
        "title": "Contingency-Aware Exploration in Reinforcement Learning"
      },
      {
        "paperId": "a88d54c168a84ed8a04d2a32be0b5939586b5792",
        "title": "NADPEx: An on-policy temporally consistent exploration method for deep reinforcement learning"
      },
      {
        "paperId": "fdfeeb14bbde2ab31b18e56b92d362dcd1b14f71",
        "title": "Episodic Curiosity through Reachability"
      },
      {
        "paperId": "af03168ecb019c4ba427b8386b9c0bc3695e917e",
        "title": "The Laplacian in RL: Learning Representations with Efficient Approximations"
      },
      {
        "paperId": "4cb3fd057949624aa4f0bbe7a6dcc8777ff04758",
        "title": "Exploration by Random Network Distillation"
      },
      {
        "paperId": "a25b645c3d24f91164230a0ac5bb2d4ec88c1538",
        "title": "Information-Directed Exploration for Deep Reinforcement Learning"
      },
      {
        "paperId": "7aaa475a7de1eecba7089af5f4b0c76976f95d93",
        "title": "Interactive Agent Modeling by Learning to Probe"
      },
      {
        "paperId": "e784986bcb4698de5be6317e5871d93b5af1cfb7",
        "title": "Q-map: a Convolutional Approach for Goal-Oriented Reinforcement Learning"
      },
      {
        "paperId": "68c2bae04f08ffeb54c68afd436272ee459b1f79",
        "title": "EMI: Exploration with Mutual Information Maximizing State and Action Embeddings"
      },
      {
        "paperId": "607eee11d64c75d837bcf98f3ec1bcd0d5727d07",
        "title": "Hierarchical Deep Multiagent Reinforcement Learning with Temporal Abstraction"
      },
      {
        "paperId": "09806c3b252d1c26f69ce54214a6bd688f0e5f9c",
        "title": "Computational mechanisms of curiosity and goal-directed exploration"
      },
      {
        "paperId": "f335069ebdeda1f3d0572c3c30f5b3d09f3c650e",
        "title": "Reinforcement Learning under Threats"
      },
      {
        "paperId": "df9e166e7179843aa2de16af907485eacf5238da",
        "title": "Deep Reinforcement Learning by Parallelizing Reward and Punishment using the MaxPain Architecture"
      },
      {
        "paperId": "1f9988ed699e35745be6c8bd4d0780299f9faada",
        "title": "Reward-Based Exploration: Adaptive Control for Deep Reinforcement Learning"
      },
      {
        "paperId": "5ee84d7770793c94ae5b1174fe67494d6ead5f7b",
        "title": "Directed Exploration in PAC Model-Free Reinforcement Learning"
      },
      {
        "paperId": "5d7ffe235578dcbdce50f4447b8f436aef308f7e",
        "title": "Deep Reinforcement Learning with Risk-Seeking Exploration"
      },
      {
        "paperId": "ca14dce53be20d3d23d4f0db844a8389ab619db3",
        "title": "Large-Scale Study of Curiosity-Driven Learning"
      },
      {
        "paperId": "acc80763d7d9482e0ba67330380ff8ee721835dd",
        "title": "Memory Augmented Policy Optimization for Program Synthesis with Generalization"
      },
      {
        "paperId": "22039881e6b36bee905f7a9d35d8de7c5ac2ef76",
        "title": "Goal-oriented Trajectories for Efficient Exploration"
      },
      {
        "paperId": "787c563657c34041357b5a92354a69b05c435032",
        "title": "Curiosity Driven Exploration of Learned Disentangled Goal Spaces"
      },
      {
        "paperId": "2fc1cfc75d6ba80846d64fbec424f6c35682f5d8",
        "title": "Memory Augmented Policy Optimization for Program Synthesis and Semantic Parsing"
      },
      {
        "paperId": "c52dddfbb4a3af4cc5e72849fe965c62801539e7",
        "title": "Counting to Explore and Generalize in Text-based Games"
      },
      {
        "paperId": "3a5e2201b23232413b7cdef998aca7379be17b72",
        "title": "Computer Science & Information Technology"
      },
      {
        "paperId": "d397f4cf400f6ffcb1b8e3db27bb75966a0513cf",
        "title": "Self-Imitation Learning"
      },
      {
        "paperId": "e3abf4ee7448c051833981cc0e5347a7af5cb3f3",
        "title": "Between Progress and Potential Impact of AI: the Neglected Dimensions"
      },
      {
        "paperId": "5fd3ce235f5fcebd3d2807f710b060add527183b",
        "title": "Deep Curiosity Search: Intra-Life Exploration Improves Performance on Challenging Deep Reinforcement Learning Problems"
      },
      {
        "paperId": "8254d978af18a3675fafbef27988528cf99f367c",
        "title": "Designing Input Shapers Using Reinforcement Learning"
      },
      {
        "paperId": "90dbf33d452de57c5a4a55631d05df9509f721e9",
        "title": "Fast Exploration with Simplified Models and Approximately Optimistic Planning in Model Based Reinforcement Learning"
      },
      {
        "paperId": "a8bf5d45bd97972251ff2258b15cfefca1f196be",
        "title": "Strategic Object Oriented Reinforcement Learning"
      },
      {
        "paperId": "653bdfb3c35621ee04ee5d5253dc7e3a422d69e1",
        "title": "Learning Self-Imitating Diverse Policies"
      },
      {
        "paperId": "9dd606ce1442f8c0ae6764a0e52d049c2779e4c4",
        "title": "Evolutionary Reinforcement Learning"
      },
      {
        "paperId": "d5805a80b63ed0a605e5469e321a7e3c42eaf324",
        "title": "Evolution-Guided Policy Gradient in Reinforcement Learning"
      },
      {
        "paperId": "427b05d77d56a1047fcc12b66e4aa507a99bafa8",
        "title": "State Distribution-Aware Sampling for Deep Q-Learning"
      },
      {
        "paperId": "35271d36cb20bf8d716e79c9dd15d738d955a931",
        "title": "On Learning Intrinsic Rewards for Policy Gradient Methods"
      },
      {
        "paperId": "094a437e4caf4e2329bbc3410f64208d949c18a4",
        "title": "An Adaptive Clipping Approach for Proximal Policy Optimization"
      },
      {
        "paperId": "7f567f1e8972ff31a7ced59c329e7d75da645baf",
        "title": "Learning to Explore with Meta-Policy Gradient"
      },
      {
        "paperId": "1485c5ac93b3f01a5f3ef7824eb428c8bff39ba3",
        "title": "UCB EXPLORATION VIA Q-ENSEMBLES"
      },
      {
        "paperId": "b80991d12b41a5a68dc14dd87b692c0f903ceb9c",
        "title": "Some Considerations on Learning to Explore via Meta-Reinforcement Learning"
      },
      {
        "paperId": "330d56c3641cddd8c78440e768cd80795f23cab4",
        "title": "Unsupervised Learning of Goal Spaces for Intrinsically Motivated Goal Exploration"
      },
      {
        "paperId": "c9660db0c94edfb2b1f3ab4f08eb80acd83a1c07",
        "title": "Evolved Policy Gradients"
      },
      {
        "paperId": "3c3093f5f8e9601637612fcfb8f160f116fa30e4",
        "title": "Diversity-Driven Exploration Strategy for Deep Reinforcement Learning"
      },
      {
        "paperId": "2064020586d5832b55f80a7dffea1fd90a5d94dd",
        "title": "Improving Exploration in Evolution Strategies for Deep Reinforcement Learning via a Population of Novelty-Seeking Agents"
      },
      {
        "paperId": "c572fe4b80a21bff85c87844e02b6de6eaa603a5",
        "title": "Identifying Reusable Macros for Efficient Exploration via Policy Compression"
      },
      {
        "paperId": "7056e205e6829b3dc789ec35eac82f6e31923c2c",
        "title": "Exploration in Feature Space for Reinforcement Learning"
      },
      {
        "paperId": "2405c131d0807c7190b253b8e0524df20b8c633d",
        "title": "Shared Learning : Enhancing Reinforcement in $Q$-Ensembles"
      },
      {
        "paperId": "a51c3f0448db58c70033d829ccfb49ba8959f4f3",
        "title": "Deep Reinforcement Learning with Hidden Layers on Future States"
      },
      {
        "paperId": "498238a3bd5fd322fc3ce1572e33bbe3853a356f",
        "title": "Deep Reinforcement Learning: A Brief Survey"
      },
      {
        "paperId": "4de770c8569893f5757c056390ffe1ae8fb49b17",
        "title": "Lenient Multi-Agent Deep Reinforcement Learning"
      },
      {
        "paperId": "429ed4c9845d0abd1f8204e1d7705919559bc2a2",
        "title": "Hindsight Experience Replay"
      },
      {
        "paperId": "6b71505aadb185a5a671e0a0003a56f1aaaad9a8",
        "title": "Hashing Over Predicted Future Frames for Informed Exploration of Deep Reinforcement Learning"
      },
      {
        "paperId": "1fdf0319b4a1db62c611980351e5f4c2f08958cd",
        "title": "GEP-PG: Decoupling Exploration and Exploitation in Deep Reinforcement Learning Algorithms"
      },
      {
        "paperId": "0f810eb4777fd05317951ebaa7a3f5835ee84cf4",
        "title": "Count-Based Exploration in Feature Space for Reinforcement Learning"
      },
      {
        "paperId": "4599ffb361f137e95d9d950ed4f731e2e6f28081",
        "title": "On Optimistic versus Randomized Exploration in Reinforcement Learning"
      },
      {
        "paperId": "142497432fe179ddb6ffe600c64a837ec6179550",
        "title": "Parameter Space Noise for Exploration"
      },
      {
        "paperId": "b3aca06e27348ede37935c49ab66c093168051ea",
        "title": "UCB and InfoGain Exploration via $\\boldsymbol{Q}$-Ensembles"
      },
      {
        "paperId": "471f9742b4e32d8ee68f9ee493768ff0466a231d",
        "title": "Automatic Goal Generation for Reinforcement Learning Agents"
      },
      {
        "paperId": "225ab689f41cef1dc18237ef5dab059a49950abf",
        "title": "Curiosity-Driven Exploration by Self-Supervised Prediction"
      },
      {
        "paperId": "fc8294a1b8243b90f79abe6c2939f778a2d609be",
        "title": "A study of count-based exploration and bonus for reinforcement learning"
      },
      {
        "paperId": "a441728f9fd6af1946368240162a72c2028c8cb1",
        "title": "Deep Exploration via Randomized Value Functions"
      },
      {
        "paperId": "8499a250422a3c66357367c8d5fa504de5424c59",
        "title": "Intrinsic Motivation and Automatic Curricula via Asymmetric Self-Play"
      },
      {
        "paperId": "12f67fb182bc934fc95ce97acff553d83e2ca72e",
        "title": "Count-Based Exploration with Neural Density Models"
      },
      {
        "paperId": "4eb38b3460606a4042b04fc52d0044ab948b4a17",
        "title": "EX2: Exploration with Exemplar Models for Deep Reinforcement Learning"
      },
      {
        "paperId": "9f1e9e56d80146766bc2316efbc54d8b770a23df",
        "title": "Deep Reinforcement Learning: An Overview"
      },
      {
        "paperId": "6ecb8a743f92db6c6b8691ab8e8aebbb06fb1b48",
        "title": "Averaged-DQN: Variance Reduction and Stabilization for Deep Reinforcement Learning"
      },
      {
        "paperId": "dfcf99ef01d3429a7d890ef8bdf19293d84c6354",
        "title": "Towards Improving Exploration through Sibling Augmented GFlowNets"
      },
      {
        "paperId": "04632d9d03d403371fece40a123c50fe7c246380",
        "title": "Studying Exploration in RL: An Optimal Transport Analysis of Occupancy Measure Trajectories"
      },
      {
        "paperId": "618b5d75751c6448f6b9f065b22e4b2abd9ceac9",
        "title": "Learning generalizable agents via self-supervised exploration"
      },
      {
        "paperId": "c583ccd3db8e4ce5765a7a55e47bc4fd83b2f4cf",
        "title": "Continual Optimistic Initialization for Value-Based Reinforcement Learning"
      },
      {
        "paperId": "cd3b5a22ddd6d4f03602bc485c88c42f833cd273",
        "title": "Reward Shaping for Reinforcement Learning with An Assistant Reward Agent"
      },
      {
        "paperId": "f7734502f2d9d464b5bd2c62a6805ca492ea61c0",
        "title": "Enhancing Reinforcement Learning with Dense Rewards from Language Model Critic"
      },
      {
        "paperId": "c9240006dbe3be68b2e4a7b18ad40ea5f75bed9d",
        "title": "DRLC: Reinforcement Learning with Dense Rewards from LLM Critic"
      },
      {
        "paperId": "54cb726595cd8c03f59f49c9a97b76b4d3f932f2",
        "title": "Rethinking Exploration in Reinforcement Learning with Effective Metric-Based Exploration Bonus"
      },
      {
        "paperId": "4956e36809a74f2bd3b8018153bccbf19f7a552f",
        "title": "Deep Reinforcement Learning for Autonomous Cyber Operations: A Survey"
      },
      {
        "paperId": "81e78a7bdbd578a838cf0cc5f128b1f168500f03",
        "title": "Beyond Information Gain: An Empirical Benchmark for Low-Switching-Cost Reinforcement Learning"
      },
      {
        "paperId": "cad7438bc696e98a3a2a5981ef91333124f7bd50",
        "title": "Learning End-to-End Visual Servoing Using an Improved Soft Actor-Critic Approach With Centralized Novelty Measurement"
      },
      {
        "paperId": "de71c1065d05e284361c24417eba1c7a4817919d",
        "title": "Improving Description-based Q-learning"
      },
      {
        "paperId": "4a06ec48e4b413ab2563273981018df82faac32e",
        "title": "Pink Noise Is All You Need: Colored Noise Exploration in Deep Reinforcement Learning"
      },
      {
        "paperId": "20b6dc0642f68e156931067a192c06208d765c84",
        "title": "Reward-Mixing MDPs with Few Latent Contexts are Learnable"
      },
      {
        "paperId": "b451feeabf6a9ce867b7ec1b009cc638b542afcf",
        "title": "Deep Reinforcement Learning with Implicit Imitation for Lane-Free Autonomous Driving"
      },
      {
        "paperId": "6648f952ff303fa3393efa80266fe788ea685472",
        "title": "\u041e\u0431\u0437\u043e\u0440 \u0432\u044b\u043f\u0443\u043a\u043b\u043e\u0439 \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u0438 \u043c\u0430\u0440\u043a\u043e\u0432\u0441\u043a\u0438\u0445 \u043f\u0440\u043e\u0446\u0435\u0441\u0441\u043e\u0432 \u043f\u0440\u0438\u043d\u044f\u0442\u0438\u044f \u0440\u0435\u0448\u0435\u043d\u0438\u0439"
      },
      {
        "paperId": "8ae6c7cff8bb2d766f5f9d3585cd262032378b33",
        "title": "Ensemble-based Offline-to-Online Reinforcement Learning: From Pessimistic Learning to Optimistic Exploration"
      },
      {
        "paperId": "a8b134af3086ac771b232139b1d721c1c424279b",
        "title": "Noise-Immune Machine Learning and Autonomous Grid Control"
      },
      {
        "paperId": "db97417e3d20840243a8010ae19215a11b4a4c36",
        "title": "Two Heads are Better Than One: A Simple Exploration Framework for Efficient Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "b962554058f5f23edebe09d27fa6145d641467fd",
        "title": "Intrinsic Motivation via Surprise Memory"
      },
      {
        "paperId": "848b8458d36f0e976da8ad59dc7c073987c175e1",
        "title": "Exploration-Guided Reward Shaping for Reinforcement Learning under Sparse Rewards"
      },
      {
        "paperId": "a059870910339596c5c57969bc2397a7e268435e",
        "title": "Learning Embodied Agents with Scalably-Supervised Reinforcement Learning"
      },
      {
        "paperId": "8aa70b57229d15f3898ef796e3c398f1e047068e",
        "title": "Learning and Transferring Value Function for Robot Exploration in Subterranean Environments"
      },
      {
        "paperId": "e586087a99613f6d483d6622ff52a8166a8156ba",
        "title": "Anomaly Guided Policy Learning from Imperfect Demonstrations"
      },
      {
        "paperId": "ef777c461af99290142714acd87fd0530c295845",
        "title": "TempoRL: Temporal Priors for Exploration in Off-Policy Reinforcement Learning"
      },
      {
        "paperId": "4c4d41a977e6d8d82b6cc19ac28e50de3c814fd8",
        "title": "HyperDQN: A Randomized Exploration Method for Deep Reinforcement Learning"
      },
      {
        "paperId": "007017ca4b9760bbd72f7ea1a4357c4d52dfd205",
        "title": "Exploiting Reward Shifting in Value-Based Deep RL"
      },
      {
        "paperId": "b65118061650446bdd79087def417e6a4de7ecdf",
        "title": "PrefRec: Preference-based Recommender Systems for Reinforcing Long-term User Engagement"
      },
      {
        "paperId": "cfb55f2d65b84bd62942a19361a6af7af4b3b0ad",
        "title": "Learning Exploration Policies with View-based Intrinsic Rewards"
      },
      {
        "paperId": "fbacad118199d825d5aa8080ba5210ad4abbae95",
        "title": "N EURAL A LL -P AIRS S HORTEST P ATH FOR R EINFORCEMENT L EARNING"
      },
      {
        "paperId": "97e5f79f2833165c8fd382228912d2cd58cb897f",
        "title": "Colored Noise Exploration in Reinforcement Learning"
      },
      {
        "paperId": "60da2965da20122fb30921c2547806200427d1ea",
        "title": "Unsupervised Skill Discovery via Recurrent Skill Training"
      },
      {
        "paperId": "1558a92344ab38b5b650d703166c0c92bc565f55",
        "title": "U NDERSTANDING THE C OMPLEXITY"
      },
      {
        "paperId": "f927e2d99e4ebf6ed1d1d4d2efff67159bab305d",
        "title": "CuMARL: Curiosity-Based Learning in Multiagent Reinforcement Learning"
      },
      {
        "paperId": "06116b70e27d60da93329c2be14ab1101c61175f",
        "title": "Exploit Reward Shifting in Value-Based Deep-RL: Optimistic Curiosity-Based Exploration and Conservative Exploitation via Linear Reward Shaping"
      },
      {
        "paperId": "877327f92135c071be13ebaf8891a6f7ba720cfe",
        "title": "DevFly: Bio-Inspired Development of Binary Connections for Locality Preserving Sparse Codes"
      },
      {
        "paperId": "3d6ae7ada6368cd14b61ba89c64705ac7ede1f7a",
        "title": "Learning Search Guidance from Failures with Eliminable Edge Sets"
      },
      {
        "paperId": "901943f6e05b23fee6ca80f9973581c2792ba3af",
        "title": "Appendix: Curious Representation Learning for Embodied Intelligence"
      },
      {
        "paperId": "10e81e4a2614a80ff2f3ba058901550637bf47fa",
        "title": "Rethinking Exploration for Sample-Efficient Policy Learning"
      },
      {
        "paperId": "9ade87e97647c20b59894250c84fb4be85941449",
        "title": "Adapting Behaviour via Intrinsic Reward"
      },
      {
        "paperId": "426ea777dd8e9217a31d97f8b65583e0a86337e9",
        "title": "Self-Supervised Exploration via Latent Bayesian Surprise"
      },
      {
        "paperId": "eeb19f4f8fbcc82dc604f7f7f5a4ce83e7224fab",
        "title": "Intrinsic Motivated Multi-Agent Communication"
      },
      {
        "paperId": "b7512a6b69ce35af84c1bd44fffbb165a9e3a723",
        "title": "Exploration Methods in Sparse Reward Environments"
      },
      {
        "paperId": "beb9a281f46f103e23c03e84cab071526f11f233",
        "title": "m -Stage Epsilon-Greedy Exploration for Reinforcement Learning"
      },
      {
        "paperId": "e9094e2a33af929d67793d480fffa6836b1bfe07",
        "title": "Efficient Hierarchical Exploration with Stable Subgoal Representation Learning"
      },
      {
        "paperId": "ac96f9dbb1462339d87b58088488cdf91f9b4f25",
        "title": "R ELEVANT A CTIONS M ATTER : M OTIVATING A GENTS WITH A CTION U SEFULNESS"
      },
      {
        "paperId": "6b6b26a54a9d39b923bec6384d407e2ef374fc98",
        "title": "Theory and Application of Bonus-based Exploration in Reinforcement Learning"
      },
      {
        "paperId": "c2ebce87f856fada7ac735fdc9fe7549bfd5b583",
        "title": "Provable Rich Observation Reinforcement Learning with Combinatorial Latent States"
      },
      {
        "paperId": "e941b0898691455b85dbe12f551b1241b8cbf7ec",
        "title": "Deep Coherent Exploration for Continuous Control"
      },
      {
        "paperId": "3fc2f8f0c508d6bf7acc26ea51c81cb55aa71f96",
        "title": "Decoupling Exploration and Exploitation in Reinforcement Learning"
      },
      {
        "paperId": "556d3f6d15acc3b37d102ccda65c4ee8f9eeb738",
        "title": "Optimality Inductive Biases and Agnostic Guidelines for Offline Reinforcement Learning"
      },
      {
        "paperId": "2e5dc4b028e670d1b9ac0b4e15580cdb7197cb68",
        "title": "Exploration in Deep Reinforcement Learning: A Comprehensive Survey"
      },
      {
        "paperId": "7253e8722566cac5aea7dfc5f94c8091a9fcbb89",
        "title": "E\u0304valds Urta\u0304ns FUNCTION SHAPING IN DEEP LEARNING Summary of the Doctoral Thesis"
      },
      {
        "paperId": "c183ce00119c7f726bcb7e5ff3b7b5405284a232",
        "title": "Robust On-Policy Data Collection for Data-Efficient Policy Evaluation"
      },
      {
        "paperId": "02d1ff7e59fd8a66d3515493b5e63cee0ab8168f",
        "title": "Intrinsic Control of Variational Beliefs in Dynamic Partially-Observed Visual Environments"
      },
      {
        "paperId": "df72118cdf70e439753872a220c77835f4983b32",
        "title": "Density-Based Bonuses on Learned Representations for Reward-Free Exploration in Deep Reinforcement Learning"
      },
      {
        "paperId": "204e9c43583a08dda52b077cd161c0efda97735b",
        "title": "Broad Adversarial Training with Data Augmentation in the Output Space"
      },
      {
        "paperId": "adabcfc03fa110d40d6ab2a613a36dda9c12dfff",
        "title": "Exploration via Empowerment Gain: Combining Novelty, Surprise and Learning Progress"
      },
      {
        "paperId": "9e968f8186bc79e46756a081e272141f75e5a9b6",
        "title": "Benchmarking Sample Selection Strategies for Batch Reinforcement Learning"
      },
      {
        "paperId": "91329f5b8e60ca064b58649ddac3487b7100ed9b",
        "title": "Artificial Neural Networks and Machine Learning \u2013 ICANN 2020: 29th International Conference on Artificial Neural Networks, Bratislava, Slovakia, September 15\u201318, 2020, Proceedings, Part II"
      },
      {
        "paperId": "a28899f255c1ca35b6254adc1a0cd64fc20c2ce9",
        "title": "Memory Based Trajectory-conditioned Policies for Learning from Sparse Rewards"
      },
      {
        "paperId": "f26a89180b9bf2f58b8647ea30580fed0ee3ff28",
        "title": "Proximal Policy Optimization with Explicit Intrinsic Motivation"
      },
      {
        "paperId": "839ea5421bc5515f1465d49613972b64cce61302",
        "title": "Effective, interpretable algorithms for curiosity automatically discovered by evolutionary search"
      },
      {
        "paperId": "ede5822e48622274a7dccd91a2d9cba08f1cb5d7",
        "title": "Competitive and Cooperative Heterogeneous Deep Reinforcement Learning"
      },
      {
        "paperId": "8fb76a141d8e72eef7d6e220ea6556a6c5755756",
        "title": "Learning Intrinsic Rewards as a Bi-Level Optimization Problem"
      },
      {
        "paperId": "8ea506fc3e8e82d953f11d9fd82421f979eefd8e",
        "title": "Exact (Then Approximate) Dynamic Programming for Deep Reinforcement Learning"
      },
      {
        "paperId": "c5a58c44049977d8a8069f175af2d57e3fb81c49",
        "title": "Mitigating Forgetting in Online Continual Learning via Instance-Aware Parameterization"
      },
      {
        "paperId": "99846049e6836bbc2189703406f92d91e01b9557",
        "title": "N OISY A GENTS : S ELF - SUPERVISED E XPLORATION BY P REDICTING A UDITORY E VENTS"
      },
      {
        "paperId": "fd6d2cb65cf31fabbae2887566de3c3a26b37c85",
        "title": "Evaluation of Reinforcement Learning Methods for a Self-learning System"
      },
      {
        "paperId": "9725ade7582e9fb07b47e8cb5faf3e6084b4b252",
        "title": "Representation and General Value Functions"
      },
      {
        "paperId": "4e50d52f0b59561e3902b997ce8a48633351491e",
        "title": "An Empirical Study of Exploration Strategies for Model-Free Reinforcement Learning"
      },
      {
        "paperId": "d1923bd80314d8dea79f9643384870e33bb5663e",
        "title": "Residual Policy Learning"
      },
      {
        "paperId": "eb8a4ac9dcff99fa1aacff4ce85d987272b75b30",
        "title": "Explore, Discover and Learn: Unsupervised Discovery of State-Covering Skills"
      },
      {
        "paperId": "79166876b78ef894b63ac98eaf9bba6fbc09886b",
        "title": "Explore, Discover and Learn: Unsupervised Discovery of State-Covering Skills"
      },
      {
        "paperId": "bd09517f02e478f6cab47286aab1da29c3a51d77",
        "title": "Efficient Novelty Search Through Deep Reinforcement Learning"
      },
      {
        "paperId": "f715558b65fd4f3c6966505c237d9a622947010b",
        "title": "Sample Efficient Reinforcement Learning Method via High Efficient Episodic Memory"
      },
      {
        "paperId": "b4cbc58bbd0906bfe633097ec9ff7d50e78ca302",
        "title": "Directed curiosity-driven exploration in hard exploration, sparse reward environments"
      },
      {
        "paperId": "c0b1d43f03c4d959a0bf57ab03ff6ef83f849872",
        "title": "Intrinsically Motivated and Interactive Reinforcement Learning: a Developmental Approach. (Apprentissage par Renforcement Intrins\u00e8quement Motiv\u00e9 et Interactif: une approche d\u00e9veloppementale)"
      },
      {
        "paperId": "5654028d5193dbf8eaa5ab9ef6af21f6da265878",
        "title": "SKEW-FIT: STATE-COVERING SELF-SUPERVISED RE-"
      },
      {
        "paperId": "d3679e8d843ae29093ba3e071f44f303175859c1",
        "title": "TIONS WITH EFFICIENT APPROXIMATIONS"
      },
      {
        "paperId": "8e1c3013f1c86dbc76c8cff6001448bc22877aea",
        "title": "Leveraging learning techniques for agricultural robotics"
      },
      {
        "paperId": "664202881ce1181aa4193374a2bd192d018ca075",
        "title": "Exploration with Expert Policy Advice"
      },
      {
        "paperId": "e37260307362b869c025d170c5833453e3d1e4ea",
        "title": "An Empirical and Conceptual Categorization of Value-based Exploration Methods"
      },
      {
        "paperId": "e2adb8231150968a8a41216f662f852eeb1685d7",
        "title": "SEQUENCE-LEVEL INTRINSIC EXPLORATION MODEL"
      },
      {
        "paperId": "03805aad32c8b4c8eb16a68c8acceb4dda13dc83",
        "title": "Efficient Exploration using Value Bounds in Deep Reinforcement Learning Effiziente Exploration in Deep Reinforcement Learning mittels beschraenkter"
      },
      {
        "paperId": "6222c8b38521e38ebbe6ea4868d4aaafa619a335",
        "title": "Exploration Bonus for Regret Minimization in Discrete and Continuous Average Reward MDPs"
      },
      {
        "paperId": "fdbb79c7435765cde178d05bf591eb34c1b77c82",
        "title": "Explicit Planning for Efficient Exploration in Reinforcement Learning"
      },
      {
        "paperId": "72c22ba3ad1002f0bc88ee61d53815dae9f53ea7",
        "title": "Correlated Exploration in Deep Reinforcement Learning"
      },
      {
        "paperId": "11900ac111b5d50fc7a6d5c5f1d8f4fb0c009b22",
        "title": "Efficient and Robust Learning on Elaborated Gaits with Curriculum Learning"
      },
      {
        "paperId": "0f7f121d9f31757bd4e5acfa5a2ae1b45fb04fce",
        "title": "Autonomous Vehicle End-to-End Reinforcement Learning Model and the Effects of Image Segmentation on Model Quality By Grant"
      },
      {
        "paperId": "a1a536d92551b6690761e36f413f53e8ce923d33",
        "title": "Sense, Think, Grasp: A study on visual and tactile information processing for autonomous manipulation"
      },
      {
        "paperId": "c6c868f544bfd81c9ce62ce7c90124afecbc3fc5",
        "title": "Transition Space Distance Learning"
      },
      {
        "paperId": "daf93749171e3575cca5b584a4146b247692c86f",
        "title": "Learning and planning in videogames via task decomposition"
      },
      {
        "paperId": "2f2e3e219ac3a149673967d3a6ba231fb299905a",
        "title": "Learning to Generalize via Self-Supervised Prediction"
      },
      {
        "paperId": "0cd50250a858a43607b2c39a208302bbcb4a33db",
        "title": "Exploration strategies for reinforcement learning with function approximation"
      },
      {
        "paperId": "2bd506e55ffe397c768b4f816e3027f5b9b632e1",
        "title": "Hierarchy-Driven Exploration for Reinforcement Learning"
      },
      {
        "paperId": "7599db0e2562e2071055e8cd954e1372b707a293",
        "title": "PACE N OISE FOR E XPLORATION"
      },
      {
        "paperId": "cc12d9e836c7765500878e31ba57e40ca0023498",
        "title": "Minimal Exploration in Episodic Reinforcement Learning"
      },
      {
        "paperId": "77a31a4601444a3f7aeed15061b08684d0bea92b",
        "title": "Exploration-Exploitation Trade-off in Deep Reinforcement Learning"
      },
      {
        "paperId": "40df15ce1de6eb0366179f4726aa55a3e50141fc",
        "title": "Learning to Explore via Meta-Policy Gradient"
      },
      {
        "paperId": "7a4d7fdff84f8f177338fede4d1b443fd7be4442",
        "title": "Improving Asynchronous Advantage Actor Critic with a More Intelligent Exploration Strategy"
      },
      {
        "paperId": "98b74fa0345a646b23d96d83c8642f1141e33fbf",
        "title": "Achieving Gentle Manipulation with Deep Reinforcement Learning"
      },
      {
        "paperId": "7cba1fb6edb762c3db8d7b8ab169dbe9c12bb28b",
        "title": "The Importance of Sampling in Meta-Reinforcement Learning"
      },
      {
        "paperId": "8812a090ab59081c4361efa3978cec4c74324469",
        "title": "Computational Sensorimotor Learning"
      },
      {
        "paperId": "c5080b29db4a8bcadb06b64a2849ef27f1b65ca2",
        "title": "Deep Learning for Autonomous Control of Robot \u2019 s Flippers in Simulation"
      },
      {
        "paperId": "b5c37ac043c84d1fcf33ece2f0c3db2305a3487f",
        "title": "Deep Curiosity Networks"
      },
      {
        "paperId": "d0088ebdf25c7320e763ee1fec60ab2ea84f0e74",
        "title": "E 2 RL \u2013 Ef\ufb01cient Exploration in Reinforcement Learning"
      },
      {
        "paperId": "e610df55e71a666381e3c24cecaa3f49a39e29da",
        "title": "Elements of Intelligence: Memory, Communication and Intrinsic Motivation"
      },
      {
        "paperId": "d942c118fdc89d94f4383c5b0ca021313e7a1a6d",
        "title": "REINFORCEMENT LEARNING AGENTS"
      },
      {
        "paperId": "94e90a47500d083b5cf7d32e609a71de7a50eb2e",
        "title": "Optimizing web search engines with interactions"
      },
      {
        "paperId": "b9effa7f0d628ab825f07bee059a3997257348ce",
        "title": "Deep Reinforcement Learning with Skill Library : Exploring with Temporal Abstractions and coarse approximate Dynamics Models"
      },
      {
        "paperId": "a28e0a7f9ad66dec1e28150bd51954cffe3add42",
        "title": "Model-based reinforcement learning and navigation in animals and machines"
      },
      {
        "paperId": "cc43349d4fd3676ec299c9a9613da39f4392f995",
        "title": "Deep Reinforcement Learning for POMDPs"
      },
      {
        "paperId": "1e9617fed4fa147e07ae68776ef70aaf386b52c9",
        "title": "Efficient Deep Reinforcement Learning via Planning, Generalization, and Improved Exploration"
      },
      {
        "paperId": "70df66fad9f1cba284b968a4ef7f5aa1cf2431d7",
        "title": "Parameter Space Noise for Exploration in Deep Reinforcement Learning"
      },
      {
        "paperId": "e8dbee9765ac3c72b5142f194761a619d0498161",
        "title": "Improving Exploration in Deep Reinforcement Learning"
      },
      {
        "paperId": "4f8a6869d646b74b8c677c1777b9e20a3a309a59",
        "title": "Shared Learning in Ensemble Deep Q-Networks"
      },
      {
        "paperId": "96f8aedaf84d0f29abed4c6efc92231898b0860b",
        "title": "REINFORCEMENT LEARNING AGENTS"
      },
      {
        "paperId": "ec3611f59c6b1a5d4b6dc72b2dcced57b706150f",
        "title": "REINFORCEMENT LEARNING AGENTS"
      },
      {
        "paperId": "97c634c3b85ef4d78451cca6efb8811617f4ff2f",
        "title": "Learning Goal-Directed Behaviour"
      },
      {
        "paperId": "4f82f6375571c2704e279d3c301292b52f12aebc",
        "title": "Robotics and Autonomous Systems"
      },
      {
        "paperId": "ff48790aa23400ed42288af0db7036c6ce1305a9",
        "title": "Washington University Open Scholarship Washington University Open Scholarship"
      },
      {
        "paperId": "ddf3cf57ca65f520458ad0a34ebd0d1555ad042b",
        "title": "DVRSF: A Deep Reinforcement Learning Method Based on Dynamic Velocity Reward & Stagnation Fine"
      },
      {
        "paperId": "cc6f79f4dfa9d66dc8c1490ba32f8987b5e6a574",
        "title": "Enhancing Exploration in Actor-Critic Algorithms:An Approach to Incentivize Plausible Novel States"
      },
      {
        "paperId": "cbe88402f149550df8823d42a02cb50f770c4097",
        "title": "What can AI Learn from Human Exploration? Intrinsically-Motivated Humans and Agents in Open-World Exploration"
      },
      {
        "paperId": "26441083524556e9ab79ac9fed98f230f0041a0d",
        "title": "MACHINE LEARNING IN 5G URLLC LINK ADAPTATION"
      },
      {
        "paperId": "d90759d8495302b30e8b67bd259b79df0e61e28d",
        "title": "Digital Commons@Georgia Southern Digital Commons@Georgia Southern"
      },
      {
        "paperId": "85ef522f7b1e009b10a78d6a5d8a9c0dbbbbc5a0",
        "title": "Utilizing Priors to Guide Exploration"
      },
      {
        "paperId": "568082d6de3f5b13b3e1638c9cbe3bf7556cae94",
        "title": "\u03b4 2 - EXPLORATION FOR R EINFORCEMENT L EARNING"
      },
      {
        "paperId": "12a996a5d6238fcdc491c3272387644ed9e435e0",
        "title": "Selective Learning for Sample-Ef\ufb01cient Training in Multi-Agent Sparse Reward Tasks"
      },
      {
        "paperId": "2952cb3a6d07ba3108b9c119e34d817ce33b5452",
        "title": "vMF-exp: von Mises-Fisher Exploration of Large Action Sets with Hyperspherical Embeddings"
      },
      {
        "paperId": "f54394be656d3261c60eed5c72a6ab6fd295116f",
        "title": "Edinburgh Research Explorer Learning with sparse reward in a gap junction network inspired by the insect mushroom body"
      },
      {
        "paperId": "8c39be17f63e310c8052af533d519fa61761de6b",
        "title": "PBCS: E\ufb00icient Exploration and Exploitation Using a Synergy Between Reinforcement Learning and Motion Planning"
      },
      {
        "paperId": "8f3ff71fc8cebb04ea595e215774c8d25dbbaff3",
        "title": "MULTI-AGENT REINFORCEMENT LEARNING USING THE COLLECTIVE INTRINSIC MOTIVATION"
      },
      {
        "paperId": "d677ec4b495d6cbd2454944dbc1124570ce4085b",
        "title": "Continual Optimistic Initialization for Value-Based Reinforcement Learning"
      },
      {
        "paperId": "9329fb47f27a3cedacdba09a9b0d98e167860966",
        "title": "Intrinsically Motivated Social Play in Virtual Infants"
      },
      {
        "paperId": "04c4f2d76a7f3c3445c52f485c335730ddc73570",
        "title": "Does Novelty-Based Exploration Maximize Novelty?"
      },
      {
        "paperId": "9aab75e7aa0f9fff845d748300d6640aeb77b25f",
        "title": "Theory and Application of Bonus-based Exploration in Reinforcement Learning"
      }
    ],
    "score": 88.66666666666666
  },
  {
    "id": "45f573f302dc7e77cbc5d1a74ccbac3564bbebc8",
    "title": "PEBBLE: Feedback-Efficient Interactive Reinforcement Learning via Relabeling Experience and Unsupervised Pre-training",
    "authors": [
      "Kimin Lee",
      "Laura M. Smith",
      "P. Abbeel"
    ],
    "year": 2021,
    "citationCount": 316,
    "abstract": "Conveying complex objectives to reinforcement learning (RL) agents can often be difficult, involving meticulous design of reward functions that are sufficiently informative yet easy enough to provide. Human-in-the-loop RL methods allow practitioners to instead interactively teach agents through tailored feedback; however, such approaches have been challenging to scale since human feedback is very expensive. In this work, we aim to make this process more sample- and feedback-efficient. We present an off-policy, interactive RL algorithm that capitalizes on the strengths of both feedback and off-policy learning. Specifically, we learn a reward model by actively querying a teacher's preferences between two clips of behavior and use it to train an agent. To enable off-policy learning, we relabel all the agent's past experience when its reward model changes. We additionally show that pre-training our agents with unsupervised exploration substantially increases the mileage of its queries. We demonstrate that our approach is capable of learning tasks of higher complexity than previously considered by human-in-the-loop methods, including a variety of locomotion and robotic manipulation skills. We also show that our method is able to utilize real-time human feedback to effectively prevent reward exploitation and learn new behaviors that are difficult to specify with standard reward functions.",
    "url": "https://www.semanticscholar.org/paper/45f573f302dc7e77cbc5d1a74ccbac3564bbebc8",
    "pdf_url": "https://arxiv.org/pdf/2106.05091.pdf",
    "venue": "International Conference on Machine Learning",
    "publicationDate": "2021-06-09",
    "externalIds": {
      "DBLP": "conf/icml/LeeSA21",
      "ArXiv": "2106.05091",
      "CorpusId": 235377145
    },
    "references": [
      {
        "paperId": "0295df1b9d11e6e49b20119410fd755a4d7781af",
        "title": "Behavior From the Void: Unsupervised Active Pre-Training"
      },
      {
        "paperId": "7fc04f8031598da6510bce4a7e5b4722305ca5b0",
        "title": "State Entropy Maximization with Random Encoders for Efficient Exploration"
      },
      {
        "paperId": "2c656207b106bf2604c71607b2b53f7b87630851",
        "title": "Human Preference Scaling with Demonstrations For Deep Reinforcement Learning"
      },
      {
        "paperId": "8ba600c169f0d2422625822223976bce562eabe1",
        "title": "dm_control: Software and Tasks for Continuous Control"
      },
      {
        "paperId": "b6f028e8611417d813ed7939f2434e3fb8c1d641",
        "title": "Avoiding Side Effects in Complex Environments"
      },
      {
        "paperId": "061bfda5b75e03dcb15ee4e94e841c2aeeaeccbf",
        "title": "Active preference-based Gaussian process regression for reward learning and optimization"
      },
      {
        "paperId": "1803722f786b901a744bc363c0ebdc51902ceceb",
        "title": "Learning Agile Robotic Locomotion Skills by Imitating Animals"
      },
      {
        "paperId": "d5ea5bad3b5678fde06aa6e9e726ae5ab4f8135a",
        "title": "AVID: Learning Multi-Stage Tasks via Pixel-Level Translation of Human Videos"
      },
      {
        "paperId": "361c00b22e29d0816ca896513d2c165e26399821",
        "title": "Grandmaster level in StarCraft II using multi-agent reinforcement learning"
      },
      {
        "paperId": "0bc855f84668b35cb65618d996d09f6e434d28c9",
        "title": "Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning"
      },
      {
        "paperId": "320b227027030fc291de2896fc3c6da49d7614be",
        "title": "Solving Rubik's Cube with a Robot Hand"
      },
      {
        "paperId": "ffb3886a253ff927bcc46b78e00409893865a68e",
        "title": "Dynamics-Aware Unsupervised Discovery of Skills"
      },
      {
        "paperId": "cfec54f9208dcbb2c558aa7ff45f57be2533d046",
        "title": "Efficient Exploration via State Marginal Matching"
      },
      {
        "paperId": "00a5cdb5768fdfabfa9decad28271afde9880579",
        "title": "End-to-End Robotic Reinforcement Learning without Reward Engineering"
      },
      {
        "paperId": "2b46e8f2f2339e4dfbc8c3db33b7a6fb65ee62ad",
        "title": "Deep Reinforcement Learning from Policy-Dependent Human Feedback"
      },
      {
        "paperId": "d35f9c78fc6d656d530aac2ed9f2aae6137b9041",
        "title": "Preferences Implicit in the State of the World"
      },
      {
        "paperId": "adab275554eb745cdc21fd6c526ec55a5b2ef362",
        "title": "Provably Efficient Maximum Entropy Exploration"
      },
      {
        "paperId": "c6f913e4baa7f2c85363c0625c87003ad3b3a14c",
        "title": "Scalable agent alignment via reward modeling: a research direction"
      },
      {
        "paperId": "3e6cde685fdf321d7edf9319f7b07c01ff79c11a",
        "title": "Reward learning from human preferences and demonstrations in Atari"
      },
      {
        "paperId": "48692cce7e9e49bab6e012524fbe403edcb22b91",
        "title": "Batch Active Preference-Based Learning of Reward Functions"
      },
      {
        "paperId": "f0070a6e7345c3a703db308a55211b4bddae9c85",
        "title": "Few-Shot Goal Inference for Visuomotor Learning and Planning"
      },
      {
        "paperId": "4cb3fd057949624aa4f0bbe7a6dcc8777ff04758",
        "title": "Exploration by Random Network Distillation"
      },
      {
        "paperId": "ee9893ff2aa325ff3c9920f247436c514fd8b512",
        "title": "SOLAR: Deep Structured Representations for Model-Based Reinforcement Learning"
      },
      {
        "paperId": "d37a34c204a8beefcaef4dddddb7a90c16e973d4",
        "title": "Learning dexterous in-hand manipulation"
      },
      {
        "paperId": "5f8645a8474017f52e4d1d4b4a0ca95d8b39f66f",
        "title": "Variational Option Discovery Algorithms"
      },
      {
        "paperId": "eb37e7b76d26b75463df22b2a3aa32b6a765c672",
        "title": "QT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation"
      },
      {
        "paperId": "36f7f407bbad234929c69c0dd3bdcfcd80298c7c",
        "title": "Variational Inverse Control with Events: A General Framework for Data-Driven Reward Definition"
      },
      {
        "paperId": "5b01eaef54a653ba03ddd5a978690380fbc19bfc",
        "title": "Diversity is All You Need: Learning Skills without a Reward Function"
      },
      {
        "paperId": "811df72e210e20de99719539505da54762a11c6d",
        "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor"
      },
      {
        "paperId": "a9a3ed69c94a3e1c08ef1f833d9199f57736238b",
        "title": "DeepMind Control Suite"
      },
      {
        "paperId": "c27db32efa8137cbf654902f8f728f338e55cd1c",
        "title": "Mastering the game of Go without human knowledge"
      },
      {
        "paperId": "b864f89eaa91120e04e8c62eb0b36568ab4244a8",
        "title": "Deep Imitation Learning for Complex Manipulation Tasks from Virtual Reality Teleoperation"
      },
      {
        "paperId": "abcf11a9af3d83f85c5fbfffc5901d416ca7a73f",
        "title": "Deep TAMER: Interactive Agent Shaping in High-Dimensional State Spaces"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "e27467ca84c5c1f7239a6e643843c1b97e35671f",
        "title": "Active Preference-Based Learning of Reward Functions"
      },
      {
        "paperId": "5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd",
        "title": "Deep Reinforcement Learning from Human Preferences"
      },
      {
        "paperId": "225ab689f41cef1dc18237ef5dab059a49950abf",
        "title": "Curiosity-Driven Exploration by Self-Supervised Prediction"
      },
      {
        "paperId": "12f67fb182bc934fc95ce97acff553d83e2ca72e",
        "title": "Count-Based Exploration with Neural Density Models"
      },
      {
        "paperId": "630bf4c0b7e5abd276ec38468f490b7d6222f3a2",
        "title": "Interactive Learning from Policy-Dependent Human Feedback"
      },
      {
        "paperId": "0fcb2034e31a2bc2f12a2b1363d0d77baf445fdf",
        "title": "#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning"
      },
      {
        "paperId": "3deecaee4ec1a37de3cb10420eaabff067669e17",
        "title": "Stochastic Neural Networks for Hierarchical Reinforcement Learning"
      },
      {
        "paperId": "505808c55b2d96ad72f4b7bca04572655742b87d",
        "title": "Sim-to-Real Robot Learning from Pixels with Progressive Nets"
      },
      {
        "paperId": "c78b1d4af936a4dd6fe887c99c893b2512e60d23",
        "title": "Visual closed-loop control for pouring liquids"
      },
      {
        "paperId": "0334cf4857b465184f969c65f1ea0347d452ff18",
        "title": "Collective robot reinforcement learning with distributed asynchronous guided policy search"
      },
      {
        "paperId": "e86f71ca2948d17b003a5f068db1ecb2b77827f7",
        "title": "Concrete Problems in AI Safety"
      },
      {
        "paperId": "6e90fd78e8a3b98af3954aae5209703aa966603e",
        "title": "Unifying Count-Based Exploration and Intrinsic Motivation"
      },
      {
        "paperId": "317cd4522b1f4a6f889743578143bb8823623f8b",
        "title": "VIME: Variational Information Maximizing Exploration"
      },
      {
        "paperId": "494e2d5b40dcebde349f9872c7317e5003f9c5d2",
        "title": "Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection"
      },
      {
        "paperId": "f009a64f87841b79b1e2ea1748366270455ef9c0",
        "title": "Model-Free Preference-Based Reinforcement Learning"
      },
      {
        "paperId": "69e76e16740ed69f4dc55361a3d319ac2f1293dd",
        "title": "Asynchronous Methods for Deep Reinforcement Learning"
      },
      {
        "paperId": "f03b4ff1b4943691cec703b508c0a91f2d97a881",
        "title": "Supersizing self-supervision: Learning to grasp from 50K tries and 700 robot hours"
      },
      {
        "paperId": "449532187c94af3dd3aa55e16d2c50f7854d2199",
        "title": "Trust Region Policy Optimization"
      },
      {
        "paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8",
        "title": "Adam: A Method for Stochastic Optimization"
      },
      {
        "paperId": "05355b13af5e67cfb86a782de15e4491d84c98c9",
        "title": "Active Reward Learning"
      },
      {
        "paperId": "8101ec9a994551edfdc7c79ebc89ed939cd07eb3",
        "title": "Hierarchical Relative Entropy Policy Search"
      },
      {
        "paperId": "65438e0ba226c1f97bd8a36333ebc3297b1a32fd",
        "title": "Reinforcement learning in robotics: A survey"
      },
      {
        "paperId": "b3af2e367d7297775c71fa9a61b0b49fb888bc38",
        "title": "A Bayesian Approach for Policy Learning from Trajectory Preference Queries"
      },
      {
        "paperId": "781cfd4e09dc6d721844391f415518dfa2774f1d",
        "title": "APRIL: Active Preference-learning based Reinforcement Learning"
      },
      {
        "paperId": "bce01bd0f070a4aec6600ba78b031cb8a7e554e3",
        "title": "Trajectories and keyframes for kinesthetic teaching: A human-robot interaction perspective"
      },
      {
        "paperId": "ae76676f357816a4f34bff0b7fac3aee0a512d84",
        "title": "Monte Carlo Methods for Preference Learning"
      },
      {
        "paperId": "9f677d31731e1c95d686fc847a04841d7ebd331e",
        "title": "Online movement adaptation based on previous sensor experiences"
      },
      {
        "paperId": "cbbcf7b0ae2d7503836a5a1db42dd3104579972d",
        "title": "Preference-Based Policy Learning"
      },
      {
        "paperId": "c54174bd1a98b1ae1fb111b32950fce538f32007",
        "title": "Online human training of a myoelectric prosthesis controller via actor-critic reinforcement learning"
      },
      {
        "paperId": "27f0504bb0ddb10249aeee4b0216cdb6f2a51dd8",
        "title": "Robot motor skill coordination with EM-based Reinforcement Learning"
      },
      {
        "paperId": "33224ad0cdf6e2dc4893194dd587309c7887f0ba",
        "title": "Formal Theory of Creativity, Fun, and Intrinsic Motivation (1990\u20132010)"
      },
      {
        "paperId": "256c3bd45ab7452bb51721eb25d3367bb654225e",
        "title": "Interactively shaping agents via human reinforcement: the TAMER framework"
      },
      {
        "paperId": "f318fe2cab01619cef18624a7ba152212d5ebd6f",
        "title": "Learning collaborative manipulation tasks by demonstration using a haptic interface"
      },
      {
        "paperId": "4e5dfb0b1e54412e799eb0e86d552956cc3a5f54",
        "title": "A survey of robot learning from demonstration"
      },
      {
        "paperId": "413b32dc8855f9db4c95657a3de5ca6c1d793da0",
        "title": "Policy gradient reinforcement learning for fast quadrupedal locomotion"
      },
      {
        "paperId": "f65020fc3b1692d7989e099d6b6e698be5a50a93",
        "title": "Apprenticeship learning via inverse reinforcement learning"
      },
      {
        "paperId": "3adde98a328a48b5a49c23fa1ae6c3c89056b9f1",
        "title": "Nearest Neighbor Estimates of Entropy"
      },
      {
        "paperId": "dd6960d576da0d769ca30de6eb6607a66806211f",
        "title": "Algorithms for Inverse Reinforcement Learning"
      },
      {
        "paperId": "a9da09d1e63686706d64782e654d69f13fd292ad",
        "title": "Learning from Demonstration"
      },
      {
        "paperId": "7d47ee5f84103529f84297c98c21dadb4742e3ff",
        "title": "RANK ANALYSIS OF INCOMPLETE BLOCK DESIGNS THE METHOD OF PAIRED COMPARISONS"
      },
      {
        "paperId": "212ac4d35a15e96ad8ded97d1fe75816bbac9148",
        "title": "Unsupervised Pre-training and Preference-Based Learning via Relabeling Experience"
      },
      {
        "paperId": "3969731b77090aba72d71792ad5a4c0f6bf40b80",
        "title": "Foundations Of Statistics"
      },
      {
        "paperId": "544db7ccbd689d84e172d300ba6edccf8792560c",
        "title": "Preference-Based Reinforcement Learning: A Preliminary Survey"
      },
      {
        "paperId": "3306d0f3aa93267dd57c24b53e5142daf3b86d1b",
        "title": "Preference-learning based Inverse Reinforcement Learning for Dialog Control"
      },
      {
        "paperId": "2a65434d43ffa6554eaf14b728780919ad4f33eb",
        "title": "Modeling Purposeful Adaptive Behavior with the Principle of Maximum Causal Entropy"
      },
      {
        "paperId": "14ba9dccf06355d1c6478b843ccb8f56d7374409",
        "title": "Nonparametric entropy estimation. An overview"
      },
      {
        "paperId": "4d6a87d76ec0c0379f0afbf24f84bba848c6246e",
        "title": "Noname manuscript No. (will be inserted by the editor) Policy Search for Motor Primitives in Robotics"
      },
      {
        "paperId": "abafd744e85dbc3122c5463e12f9edcab6770a11",
        "title": "This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination. IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION 1 Intrinsic Motivation Systems for Autonomous Mental Development"
      }
    ],
    "cited_by": [
      {
        "paperId": "0d0d163691fe72ecb45c6ededbb19cfc0b52cc1d",
        "title": "How Well Can Preference Optimization Generalize Under Noisy Feedback?"
      },
      {
        "paperId": "03c62352af0a484dc151b380ae693f334c84b2ab",
        "title": "STAIR: Addressing Stage Misalignment through Temporal-Aligned Preference Reinforcement Learning"
      },
      {
        "paperId": "d625a9ce83cab9e314603fd46c78b59d05ec78fc",
        "title": "Clean First, Align Later: Benchmarking Preference Data Cleaning for Reliable LLM Alignment"
      },
      {
        "paperId": "30759df57a8b948bdd5881b87bdf0c1c99d30473",
        "title": "Preference-Guided Learning for Sparse-Reward Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "5f25bd70b974edf6a5babeaeb617303fca3dd1ee",
        "title": "Reward Evolution with Graph-of-Thoughts: A Bi-Level Language Model Framework for Reinforcement Learning"
      },
      {
        "paperId": "def0655b1d0d434442b3bbcb8475d4cc6e750718",
        "title": "How well can LLMs provide planning feedback in grounded environments?"
      },
      {
        "paperId": "1ab8fa5ee2fc77d9114d124d30017b7f8407a5c4",
        "title": "Active Query Selection for Crowd-Based Reinforcement Learning"
      },
      {
        "paperId": "9cd1302cb9cbfa9f6b3ede8a4f3b811de02b8414",
        "title": "Learning Real-World Acrobatic Flight from Human Preferences"
      },
      {
        "paperId": "8667c20f7736716a7991462e076dcc1ea6953759",
        "title": "Search-Based Credit Assignment for Offline Preference-Based Reinforcement Learning"
      },
      {
        "paperId": "3f7f25ea2af8d72693308b26f016e0e61eaf71f1",
        "title": "Pref-GUIDE: Continual Policy Learning from Real-Time Human Feedback via Preference-Based Learning"
      },
      {
        "paperId": "27a3089b808f442bcff957037166ac6c8204d7ee",
        "title": "HALO: Human Preference Aligned Offline Reward Learning for Robot Navigation"
      },
      {
        "paperId": "0aa91cbb6a43670e77c09c4c0353adfc89c64d7e",
        "title": "NPO: Learning Alignment and Meta-Alignment through Structured Human Feedback"
      },
      {
        "paperId": "3f2813b31d67831cc71f3c4de31b344001bc2db2",
        "title": "Preference-Based Multi-Objective Reinforcement Learning"
      },
      {
        "paperId": "aff2d0c577fdfbc85e568a8f949fa35afe10e86a",
        "title": "Aligning Humans and Robots via Reinforcement Learning from Implicit Human Feedback"
      },
      {
        "paperId": "e0daefa92aecf41d5191f1fb6560dd3db28982a6",
        "title": "CueLearner: Bootstrapping and local policy adaptation from relative feedback"
      },
      {
        "paperId": "2efc64d19778c87d6cc2e193cfeeb1229d964443",
        "title": "CRED: Counterfactual Reasoning and Environment Design for Active Preference Learning"
      },
      {
        "paperId": "b467036844e26c96ee94c466d771f1a5bf617204",
        "title": "A Technical Survey of Reinforcement Learning Techniques for Large Language Models"
      },
      {
        "paperId": "92d93ac08c2c5513ba5a0a95c4ab3d3bb8899b36",
        "title": "Residual Reward Models for Preference-based Reinforcement Learning"
      },
      {
        "paperId": "4eda07fa2536b3e9e3ff4d8b323c5c5b0f42075a",
        "title": "SENIOR: Efficient Query Selection and Preference-Guided Exploration in Preference-based Reinforcement Learning"
      },
      {
        "paperId": "a634ebb619673d032c433854578f5917aaa9673f",
        "title": "PB2: Preference Space Exploration via Population-Based Methods in Preference-Based Reinforcement Learning"
      },
      {
        "paperId": "8dccf45aa7b6d3e7ef8fe762751cb35c86778141",
        "title": "Enhancing Rating-Based Reinforcement Learning to Effectively Leverage Feedback from Large Vision-Language Models"
      },
      {
        "paperId": "a9d825e3009f777cb07a72d07f954bb358be8f8d",
        "title": "Similarity as Reward Alignment: Robust and Versatile Preference-based Reinforcement Learning"
      },
      {
        "paperId": "0f80c534cd3191142af9c1ab521b85263daf43fe",
        "title": "CLARIFY: Contrastive Preference Reinforcement Learning for Untangling Ambiguous Queries"
      },
      {
        "paperId": "cfe99165e00c47dce3f1350b7987595cb831d8cd",
        "title": "VIRAL: Vision-grounded Integration for Reward design And Learning"
      },
      {
        "paperId": "2edd56fd82749d0f0d987ee341d8caf95037b164",
        "title": "SafeDPO: A Simple Approach to Direct Preference Optimization with Enhanced Safety"
      },
      {
        "paperId": "8c376a7d9bad8122afaf4f841db39ae4c1bf2162",
        "title": "MisoDICE: Multi-Agent Imitation from Unlabeled Mixed-Quality Demonstrations"
      },
      {
        "paperId": "f14cdbeeaf122b627808d44b519fa5cad79cc7b0",
        "title": "Investigating Inverse Reinforcement Learning during Rapid Aiming Movement in Extended Reality and Human-Robot Interaction"
      },
      {
        "paperId": "85bbd08e5fcc53f003ce5209d08101eaaaa782e5",
        "title": "Personalization in Human-Robot Interaction Through Preference-Based Action Representation Learning"
      },
      {
        "paperId": "9fa625528073fee005cf2d0fd2b2e9699a1af926",
        "title": "ReWiND: Language-Guided Rewards Teach Robot Policies without New Demonstrations"
      },
      {
        "paperId": "206f07a961b480948bb3839a85f6897f3535e019",
        "title": "TREND: Tri-Teaching for Robust Preference-based Reinforcement Learning with Demonstrations"
      },
      {
        "paperId": "8c718716717a60c09c8bf33ee3b25f0b3288a5ef",
        "title": "Adaptive Confidence-aware Preference-based Reinforcement Learning with Noisy Feedback"
      },
      {
        "paperId": "f1173df25d83e5c6ff16e087f890993bccf355ed",
        "title": "Policy-labeled Preference Learning: Is Preference Enough for RLHF?"
      },
      {
        "paperId": "44bd8a38d97cda8cebeb71e0a6848868993069e2",
        "title": "PRISM: Projection-based Reward Integration for Scene-Aware Real-to-Sim-to-Real Transfer with Few Demonstrations"
      },
      {
        "paperId": "551a5e52bc72df8251a2f66fc9652faabfcacdb7",
        "title": "Flora: Sample-Efficient Preference-Based Rl Via Low-Rank Style Adaptation of Reward Functions"
      },
      {
        "paperId": "031dfd046a194e0b70ed0b02b9a2155a67e3f24b",
        "title": "The Structural Safety Generalization Problem"
      },
      {
        "paperId": "13dc13b76d34630cc341fccba4369cf962919996",
        "title": "A Comprehensive Survey of Reward Models: Taxonomy, Applications, Challenges, and Future"
      },
      {
        "paperId": "3990f2211e16fff86af9ad9f0c967b0fe37d3658",
        "title": "Reward Generation via Large Vision-Language Model in Offline Reinforcement Learning"
      },
      {
        "paperId": "08c2db784cd81063b2ed5d66d175051ccbfee77f",
        "title": "Latent Embedding Adaptation for Human Preference Alignment in Diffusion Planners"
      },
      {
        "paperId": "b625d9bf38268f245e29b9406c7ce182d66d577f",
        "title": "AED: Automatic Discovery of Effective and Diverse Vulnerabilities for Autonomous Driving Policy with Large Language Models"
      },
      {
        "paperId": "24d1b69bd6d2a5132170bc8e0d9596336e99cd32",
        "title": "On The Sample Complexity Bounds In Bilevel Reinforcement Learning"
      },
      {
        "paperId": "eafd292ee60cc833eee3a02c943e509f9ec46221",
        "title": "VARP: Reinforcement Learning from Vision-Language Model Feedback with Agent Regularized Preferences"
      },
      {
        "paperId": "3c9e60f9bd36ce0922f9bd95cc41dbe5f2af8ca7",
        "title": "PRISM: Preference Refinement via Implicit Scene Modeling for 3D Vision-Language Preference-Based Reinforcement Learning"
      },
      {
        "paperId": "58460941048b6f570313d6358c7b1dd6c19676a5",
        "title": "Adversarial Policy Optimization for Offline Preference-based Reinforcement Learning"
      },
      {
        "paperId": "b49cbccd88cacf9a7514c80d33cf5c42b80edc1a",
        "title": "Towards Autonomous Reinforcement Learning for Real-World Robotic Manipulation With Large Language Models"
      },
      {
        "paperId": "258a2b612e2248466032627261a3615cb949c7d1",
        "title": "Human Implicit Preference-Based Policy Fine-tuning for Multi-Agent Reinforcement Learning in USV Swarm"
      },
      {
        "paperId": "96814a0576c703b021b1a7e7ccc17c776bf89b29",
        "title": "M3HF: Multi-agent Reinforcement Learning from Multi-phase Human Feedback of Mixed Quality"
      },
      {
        "paperId": "ae843221017d8b8941d4ea11a883c52240f8ce07",
        "title": "PrefCLM: Enhancing Preference-Based Reinforcement Learning With Crowdsourced Large Language Models"
      },
      {
        "paperId": "72c15325e71730e2f454e162dc425690b039cb53",
        "title": "Reducing Reward Dependence in RL Through Adaptive Confidence Discounting"
      },
      {
        "paperId": "09fc9f8598c8e904496fb089ad0842da0225b984",
        "title": "RIZE: Regularized Imitation Learning via Distributional Reinforcement Learning"
      },
      {
        "paperId": "dd951242ebc94bf633eecc4994c64f46146a1413",
        "title": "Reward Shaping to Mitigate Reward Hacking in RLHF"
      },
      {
        "paperId": "95da947734f064da1be1f44b2903b0f3152ff214",
        "title": "Uncertainty Comes for Free: Human-in-the-Loop Policies with Diffusion Models"
      },
      {
        "paperId": "04166e6ad5162ab60b8cca145b4e8ed5fb1c0334",
        "title": "HPS: Hard Preference Sampling for Human Preference Alignment"
      },
      {
        "paperId": "d60baee72ad29b166d0a6e7e2810960ebb0c8ad4",
        "title": "A novel hybrid intelligent scheduling: integrating human feedback into reinforcement learning for adaptive preference objectives"
      },
      {
        "paperId": "1c3e59862cb82b78c6fb534fd2c5dc899216ca26",
        "title": "VLP: Vision-Language Preference Learning for Embodied Manipulation"
      },
      {
        "paperId": "84fc714f22535f82c3fbfa28f2883292d2a02167",
        "title": "ConRFT: A Reinforced Fine-tuning Method for VLA Models via Consistency Policy"
      },
      {
        "paperId": "020ce0b77f5d1ef6cea39fca302b132a14cab1a3",
        "title": "Efficiently Generating Expressive Quadruped Behaviors via Language-Guided Preference Learning"
      },
      {
        "paperId": "1d2644d650fa7e6ef771367e4764945890d2fac7",
        "title": "Speaking the Language of Teamwork: LLM-Guided Credit Assignment in Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "3f06ee4a359136f957818c4b62b5f4c8aea1daca",
        "title": "Towards Cost-Effective Reward Guided Text Generation"
      },
      {
        "paperId": "fb15fa7d5fbc2e36957057cc3a275252f5e3ce53",
        "title": "Robust Reward Alignment via Hypothesis Space Batch Cutting"
      },
      {
        "paperId": "49758204387580577d625ec1733acb782b05e342",
        "title": "Learning from Active Human Involvement through Proxy Value Propagation"
      },
      {
        "paperId": "78ed3c59818bf5d44cd6f75e23c811cd2f5d2cc0",
        "title": "Preference VLM: Leveraging VLMs for Scalable Preference-Based Reinforcement Learning"
      },
      {
        "paperId": "91da1331568c00c3eae6e599275f5d6148db52f3",
        "title": "Human-Aligned Skill Discovery: Balancing Behaviour Exploration and Alignment"
      },
      {
        "paperId": "e08c4d38cd851c38784a386832524cb7cb46f858",
        "title": "FDPP: Fine-Tune Diffusion Policy with Human Preference"
      },
      {
        "paperId": "1c3aaffa10f83cac66a78f7cb796cb64edac6030",
        "title": "Segmenting Text and Learning Their Rewards for Improved RLHF in Language Model"
      },
      {
        "paperId": "6ce659d5f52bc26e2f3aed6d867ce48c30f298fd",
        "title": "Contrastive Learning from Exploratory Actions: Leveraging Natural Interactions for Preference Elicitation"
      },
      {
        "paperId": "58a5a16fbad1eb32490f71c8b0b256c5c7c29860",
        "title": "Online Preference-based Reinforcement Learning with Self-augmented Feedback from Large Language Model"
      },
      {
        "paperId": "ebf4b3404d60a666cae1aa137db9da1a0296255a",
        "title": "RAT: Adversarial Attacks on Deep Reinforcement Agents for Targeted Behaviors"
      },
      {
        "paperId": "a581f4ea6caee47253c0813e081b5c89d7c3bd3a",
        "title": "A Human-Machine Reinforcement Learning Framework with Multi-dimensional Human Feedback Fusion"
      },
      {
        "paperId": "8c131d842b59a2a807a97cb39fb0f8a22fdb39a9",
        "title": "Mixing corrupted preferences for robust and feedback-efficient preference-based reinforcement learning"
      },
      {
        "paperId": "4749559e87d6a301a2ac61767d48ff4437fe4fd8",
        "title": "Real-World Offline Reinforcement Learning from Vision Language Model Feedback"
      },
      {
        "paperId": "47df61fb2a060ccab5407dde7f6ce237894ec14d",
        "title": "Offline reward shaping with scaling human preference feedback for deep reinforcement learning"
      },
      {
        "paperId": "cc5e7ef03adef088392c663a05ae06e003881c04",
        "title": "Preference-Based Multi-Objective Reinforcement Learning with Explicit Reward Modeling"
      },
      {
        "paperId": "35b9438bcdc218c766d5ca28ae464d4287291234",
        "title": "Direct Preference Optimization for Primitive-Enabled Hierarchical Reinforcement Learning"
      },
      {
        "paperId": "0111e51679678f81eafa0ce064b578f284eb8e59",
        "title": "Navigating Noisy Feedback: Enhancing Reinforcement Learning with Error-Prone Language Models"
      },
      {
        "paperId": "aeeaa6aa9b40454a0b0345197ab39a0758edc1a1",
        "title": "ICPL: Few-shot In-context Preference Learning via LLMs"
      },
      {
        "paperId": "80b75894562ac77a7074f164bbee0b18b861bf00",
        "title": "A Large Language Model-Driven Reward Design Framework via Dynamic Feedback for Reinforcement Learning"
      },
      {
        "paperId": "903d4ebbe907ff10df119596a14dd4e7eed2beb9",
        "title": "UNIQ: Offline Inverse Q-learning for Avoiding Undesirable Demonstrations"
      },
      {
        "paperId": "7d86d730cac7aa1350a6a74bb851cd8ba55ed87f",
        "title": "RL, but don't do anything I wouldn't do"
      },
      {
        "paperId": "123d8f72a57e7f956d383de500d5abf32380fd68",
        "title": "Reinforcement Learning From Imperfect Corrective Actions And Proxy Rewards"
      },
      {
        "paperId": "ed2f261706d1e81a767b23a1e52b89ed26d90b89",
        "title": "A survey of communicating robot learning during human-robot interaction"
      },
      {
        "paperId": "a57162ae7f03e875cf4e2ea130f696b58d967724",
        "title": "From Reward Shaping to Q-Shaping: Achieving Unbiased Learning with LLM-Guided Knowledge"
      },
      {
        "paperId": "cf72b887315edc26835f439eb1bfd01fa9e16563",
        "title": "Challenges and Future Directions of Data-Centric AI Alignment"
      },
      {
        "paperId": "0cac59b388a6dff38ba4b40391920d38c0431df3",
        "title": "Dynamic Policy Fusion for User Alignment Without Re-Interaction"
      },
      {
        "paperId": "f452e530f708e9c21cbabf527366fa6c5213d87c",
        "title": "CANDERE-COACH: Reinforcement Learning from Noisy Feedback"
      },
      {
        "paperId": "642a59982044d5e87cc479b5f529108878beca94",
        "title": "Reward Shaping for RL Based on Human Multi-Attribute Scaling Preferences Feedback"
      },
      {
        "paperId": "a3e894356ca9865b881cb53aa62548f1dbff82d8",
        "title": "Reward-Robust RLHF in LLMs"
      },
      {
        "paperId": "84bb47ae6e528ce1dc0e9e3353bf92a45fa120d2",
        "title": "Self-Evolutionary Large Language Models through Uncertainty-Enhanced Preference Optimization"
      },
      {
        "paperId": "781d3c7fe990303f56f5da39eba3253f152e3505",
        "title": "Multi-Type Preference Learning: Empowering Preference-Based Reinforcement Learning with Equal Preferences"
      },
      {
        "paperId": "272c30121d6ae571c53767958193f1af73cd734a",
        "title": "S-EPOA: Overcoming the Indistinguishability of Segments with Skill-Driven Preference-Based Reinforcement Learning"
      },
      {
        "paperId": "229140abb5f45178882080404edb8618b0f3158d",
        "title": "Metis: a python-based user interface to collect expert feedback for generative chemistry models"
      },
      {
        "paperId": "23053a3978c55582d0cf61998a8109b3eefc9875",
        "title": "Representation Alignment from Human Feedback for Cross-Embodiment Reward Learning from Mixed-Quality Demonstrations"
      },
      {
        "paperId": "8525434adbf25984e55c78063c71bcb958d364e4",
        "title": "Listwise Reward Estimation for Offline Preference-based Reinforcement Learning"
      },
      {
        "paperId": "21cdb3aced0e15d8581f3f58ea836b4f379a06fa",
        "title": "Can DPO Learn Diverse Human Values? A Theoretical Scaling Law"
      },
      {
        "paperId": "0327596d1c83506035a2372eb649d5c16d9515be",
        "title": "Comprehensive Overview of Reward Engineering and Shaping in Advancing Reinforcement Learning Applications"
      },
      {
        "paperId": "d6105f404d89b7d6267cf46b3a54efab9d0c7962",
        "title": "Curriculum-Guided Preference-based Reinforcement Learning: An Exploratory Approach"
      },
      {
        "paperId": "25908892225280f00bbef3fe555e2683ed50fd72",
        "title": "Continuous Control with Coarse-to-fine Reinforcement Learning"
      },
      {
        "paperId": "e949e8c1ad3345190d12acc5d4aa59d46cd1a043",
        "title": "Preference-Guided Reinforcement Learning for Efficient Exploration"
      },
      {
        "paperId": "59032a44318cf1f6be079f741d29f6df130294a5",
        "title": "Hybrid Reinforcement Learning based on Human Preference and Advice for Efficient Robot Skill Learning"
      },
      {
        "paperId": "2551adaf556fedab0e360ac36f26ce174bcaddee",
        "title": "Safe MPC Alignment with Human Directional Feedback"
      },
      {
        "paperId": "d1b4c920035c7e413aac3fd07b8d374b24b8f79f",
        "title": "Hindsight Preference Learning for Offline Preference-based Reinforcement Learning"
      },
      {
        "paperId": "62555bd1938384ce9a00973b1a15899e71b72ddc",
        "title": "Safety through feedback in Constrained RL"
      },
      {
        "paperId": "1675fbeff4aa1cccf6b47b7713814950039f8e2e",
        "title": "Preference Elicitation for Offline Reinforcement Learning"
      },
      {
        "paperId": "3a23956c5b97fb3cd7376113911c65abf9a3306f",
        "title": "MEReQ: Max-Ent Residual-Q Inverse RL for Sample-Efficient Alignment from Intervention"
      },
      {
        "paperId": "2257d0894da128e9c900df1237f43504c9c0e82e",
        "title": "Cascade Reward Sampling for Efficient Decoding-Time Alignment"
      },
      {
        "paperId": "c439ce66fe32f280b722bdf519ebdd04b17e53eb",
        "title": "Advancing Interactive Robot Learning: A User Interface Leveraging Mixed Reality and Dual Quaternions"
      },
      {
        "paperId": "af11d43d3b4e6f2d01f34feb149037cdbd05d9ee",
        "title": "SAIL: Self-Improving Efficient Online Alignment of Large Language Models"
      },
      {
        "paperId": "5dce7116746cc0f5b09f5b27d3ea42caa27a1120",
        "title": "DIPPER: Direct Preference Optimization to Accelerate Primitive-Enabled Hierarchical Reinforcement Learning"
      },
      {
        "paperId": "71da1f3cd9b9c48ae7c62d5452cc18045000b833",
        "title": "A Critical Look At Tokenwise Reward-Guided Text Generation"
      },
      {
        "paperId": "4ef4bb74612fedb4933de2b463a7f1b588193f43",
        "title": "Self-Consuming Generative Models with Curated Data Provably Optimize Human Preferences"
      },
      {
        "paperId": "b224881223d805a76493a4d44c37d815422e2ea4",
        "title": "Boosting Robustness in Preference-Based Reinforcement Learning with Dynamic Sparsity"
      },
      {
        "paperId": "a264dbba6c16a460998cd16e4f48d62a1eddd4d0",
        "title": "Spread Preference Annotation: Direct Preference Judgment for Efficient LLM Alignment"
      },
      {
        "paperId": "adabaf6fc304af494d6d5f37ec59ee640482801e",
        "title": "Aligning Agents like Large Language Models"
      },
      {
        "paperId": "cd6b87d6f746bd572a79c4f77cc60cf6a92fc015",
        "title": "Adaptive Preference Scaling for Reinforcement Learning with Human Feedback"
      },
      {
        "paperId": "a2494949a7061e8fe8343af44bc1edb00973726f",
        "title": "Strengthened Symbol Binding Makes Large Language Models Reliable Multiple-Choice Selectors"
      },
      {
        "paperId": "9179b3288e376b2036b358f78dd2e30181fe87b4",
        "title": "Do's and Don'ts: Learning Desirable Skills with Instruction Videos"
      },
      {
        "paperId": "257c27cfaf32387671d4f6bf204a50dcb9f26900",
        "title": "Bilevel reinforcement learning via the development of hyper-gradient without lower-level convexity"
      },
      {
        "paperId": "e96c41933f4b1f0e79449451395e6ed0e6644133",
        "title": "Preference Alignment with Flow Matching"
      },
      {
        "paperId": "8e4fb48e0882066cc44636fbd812607909e191a7",
        "title": "Efficient Preference-based Reinforcement Learning via Aligned Experience Estimation"
      },
      {
        "paperId": "6845ea4edeb241976a09f34795e4f5038424995f",
        "title": "Revision Matters: Generative Design Guided by Revision Edits"
      },
      {
        "paperId": "8f2363d60ee56bf7e7ad29a47c3ad64290361e85",
        "title": "A Preference-based Reinforcement Learning Approach Using Reward Exploration for Decision Making"
      },
      {
        "paperId": "d95664ecae2b9083cf211ab827a3dc28b8187018",
        "title": "Tell me why: Training preferences-based RL with human preferences and step-level explanations"
      },
      {
        "paperId": "945a899a93c03eb63be5e3197e318c077473cef9",
        "title": "DLPO: Diffusion Model Loss-Guided Reinforcement Learning for Fine-Tuning Text-to-Speech Diffusion Models"
      },
      {
        "paperId": "236428f46cc5315d8eadb1d7131a98039355d530",
        "title": "ELO-Rated Sequence Rewards: Advancing Reinforcement Learning Models"
      },
      {
        "paperId": "0a1ec333b79aa449ff9d2c2ce0913c625729e972",
        "title": "TRANSIC: Sim-to-Real Policy Transfer by Learning from Online Correction"
      },
      {
        "paperId": "206ca3c10095e12d0b893307fc58e33e147ad895",
        "title": "Leveraging Human Revisions for Improving Text-to-Layout Models"
      },
      {
        "paperId": "0c468fb51f94b55ff121bc4738b4ca2f7fcd7c61",
        "title": "POLITE: Preferences Combined with Highlights in Reinforcement Learning"
      },
      {
        "paperId": "5008574062623b3269ba9e39d37e319770101d6e",
        "title": "SEQUEL: Semi-Supervised Preference-based RL with Query Synthesis via Latent Interpolation"
      },
      {
        "paperId": "83331243fcaeab4d5c02145d62f3d42df2c2d651",
        "title": "Learning Reward for Robot Skills Using Large Language Models via Self-Alignment"
      },
      {
        "paperId": "6b3fb301ab6bcc97b278f7a059631445babcf051",
        "title": "Improving Instruction Following in Language Models through Proxy-Based Uncertainty Estimation"
      },
      {
        "paperId": "ffcef91b1c25d2d740b335f9b4b3c942cd8d79de",
        "title": "Enhancing Q-Learning with Large Language Model Heuristics"
      },
      {
        "paperId": "3e65e47eb6361213715b3098659ca75fbb172e4f",
        "title": "Leveraging Sub-Optimal Data for Human-in-the-Loop Reinforcement Learning"
      },
      {
        "paperId": "5b3a3f599674d7bba97f5120bd8e5737be757333",
        "title": "PIPER: Primitive-Informed Preference-based Hierarchical Reinforcement Learning via Hindsight Relabeling"
      },
      {
        "paperId": "80fd3d2d3b2f7e0cb41c935c6b7ac1b73823a60d",
        "title": "Position: Social Choice Should Guide AI Alignment in Dealing with Diverse Human Feedback"
      },
      {
        "paperId": "865bf47dedfec416221504708cd1f039050b0a2d",
        "title": "Impact of Preference Noise on the Alignment Performance of Generative Language Models"
      },
      {
        "paperId": "d38717a79a55cad8ed20d96ffe71136d0bbea9af",
        "title": "Hindsight PRIORs for Reward Learning from Human Preferences"
      },
      {
        "paperId": "3714bb3aac3f0429cd62e315da0ffb3f79655f98",
        "title": "Regularized Conditional Diffusion Model for Multi-Task Preference Alignment"
      },
      {
        "paperId": "781530d26700b79f21394d8c9f3ad71e5a672cbf",
        "title": "Reinforcement Learning with Human Feedback: A CartPole Case Study"
      },
      {
        "paperId": "279dbec2194f05ee0e9d49818eff5fbe5f61b2ee",
        "title": "Understanding the Learning Dynamics of Alignment with Human Feedback"
      },
      {
        "paperId": "006212f34af6237a3af5f9444af9bcb89d2c0dc5",
        "title": "Safe and Robust Reinforcement Learning: Principles and Practice"
      },
      {
        "paperId": "d9616c72ac9a98101867a58591ae21f49847f216",
        "title": "Can You Rely on Synthetic Labellers in Preference-Based Reinforcement Learning? It's Complicated"
      },
      {
        "paperId": "2b6f6977daf8525a101905d2b6d2a6d0aa59529b",
        "title": "Decoding Global Preferences: Temporal and Cooperative Dependency Modeling in Multi-Agent Preference-Based Reinforcement Learning"
      },
      {
        "paperId": "057bb1cf2e3450cd2a441f885a587bccbaddc7ba",
        "title": "Diffusion Model for Data-Driven Black-Box Optimization"
      },
      {
        "paperId": "922c881f7fb9b0d7b9162042200007d4be5c27f1",
        "title": "Visual Preference Inference: An Image Sequence-Based Preference Reasoning in Tabletop Object Manipulation"
      },
      {
        "paperId": "4c19c7a69072ba117624d8373d990062662b8c51",
        "title": "Online Policy Learning from Offline Preferences"
      },
      {
        "paperId": "a4065e70b25bdf20b4587f8d1bbaaa7e4eeb9390",
        "title": "Correlated Proxies: A New Definition and Improved Mitigation for Reward Hacking"
      },
      {
        "paperId": "bd738366a232005f84037a01d412dc26e4fc3757",
        "title": "Bayesian Constraint Inference from User Demonstrations Based on Margin-Respecting Preference Models"
      },
      {
        "paperId": "6a2895f994832459a3fb0e691dc90b9838fdcd93",
        "title": "Learning with Language-Guided State Abstractions"
      },
      {
        "paperId": "e7fc1b57dd032a8697902c01a9a715398b83a930",
        "title": "DecisionNCE: Embodied Multimodal Representations via Implicit Preference Learning"
      },
      {
        "paperId": "6ceca40e971add0ad0d3b304b79c157079838ba6",
        "title": "Sample-Efficient Preference-based Reinforcement Learning with Dynamics Aware Rewards"
      },
      {
        "paperId": "87f1b39c320e1fc71584a231855523167a5588ff",
        "title": "RIME: Robust Preference-based Reinforcement Learning with Noisy Preferences"
      },
      {
        "paperId": "208fdbf3ac095740a53230523db3828a52414da6",
        "title": "PREDILECT: Preferences Delineated with Zero-Shot Language-based Reasoning in Reinforcement Learning"
      },
      {
        "paperId": "9221567500f720ba43721f6967f1cd4ebfe86b17",
        "title": "MENTOR: Guiding Hierarchical Reinforcement Learning With Human Feedback and Dynamic Distance Constraint"
      },
      {
        "paperId": "6f52cc360e9c6b31e50446864fff40bfec71310a",
        "title": "Enhancing Robotic Manipulation with AI Feedback from Multimodal Large Language Models"
      },
      {
        "paperId": "2a7d3d3ded5511a184329ac404980e90f864d442",
        "title": "Active Preference Optimization for Sample Efficient RLHF"
      },
      {
        "paperId": "ede28224a1d6633826198d1702a5a737ea236575",
        "title": "Reinforcement Learning from Human Feedback with Active Queries"
      },
      {
        "paperId": "9e08d4f7348ff2afbe8c96e7564f43d326e67d00",
        "title": "A Dense Reward View on Aligning Text-to-Image Diffusion with Preference"
      },
      {
        "paperId": "73c38b406a9ec6d7a4d93969b003387e72e45ce1",
        "title": "Scalable Interactive Machine Learning for Future Command and Control"
      },
      {
        "paperId": "550006bea81e4ccb67743dd1b82a70b86b48d93a",
        "title": "RL-VLM-F: Reinforcement Learning from Vision Language Foundation Model Feedback"
      },
      {
        "paperId": "5f3c4145842792e5bc7de14f918fc381be00b06f",
        "title": "Uni-RLHF: Universal Platform and Benchmark Suite for Reinforcement Learning with Diverse Human Feedback"
      },
      {
        "paperId": "1491e2d5d8ba63132ff157e47e824af76c422450",
        "title": "ARGS: Alignment as Reward-Guided Search"
      },
      {
        "paperId": "cabfdb88348fc7707223aa12ae570444cfb7688e",
        "title": "Integrating Human Expertise in Continuous Spaces: A Novel Interactive Bayesian Optimization Framework with Preference Expected Improvement"
      },
      {
        "paperId": "05575a490607f0b33be58315613b66ce19233a7a",
        "title": "Robotic Test Tube Rearrangement Using Combined Reinforcement Learning and Motion Planning"
      },
      {
        "paperId": "4989bead096e388cc79dccb6f0ee73f0ebdfd504",
        "title": "Crowd-PrefRL: Preference-Based Reward Learning from Crowds"
      },
      {
        "paperId": "324786abbbc22ca1fba487709536ee682fe0af60",
        "title": "A Minimaximalist Approach to Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "55ffe0af98bcebc08aa6963c92525392decfd398",
        "title": "Two-Step Offline Preference-Based Reinforcement Learning with Constrained Actions"
      },
      {
        "paperId": "63c9f3ec3f1427d6b37c9b4ea4a262f500f6a231",
        "title": "Think Before You Duel: Understanding Complexities of Preference Learning under Constrained Resources"
      },
      {
        "paperId": "aa83690628456d1b197716f8d45f08f8f2d48ffc",
        "title": "Human-AI Collaboration in Real-World Complex Environment with Reinforcement Learning"
      },
      {
        "paperId": "3be4e453e29fb23dcb86d466ce0a68fc24a59b8c",
        "title": "Incorporating Human Flexibility through Reward Preferences in Human-AI Teaming"
      },
      {
        "paperId": "90ec1b95ab1579673f0a4b81dc5a76dfc955b2a5",
        "title": "Promptable Behaviors: Personalizing Multi-Objective Rewards from Human Preferences"
      },
      {
        "paperId": "4167ae976e4a057ba1d4f564a841266ac1ac5faa",
        "title": "A dynamical clipping approach with task feedback for Proximal Policy Optimization"
      },
      {
        "paperId": "f925755264deeae30a68d3c1a1d5faa9214e68f1",
        "title": "A Review of Communicating Robot Learning during Human-Robot Interaction"
      },
      {
        "paperId": "efd7ed166023cf38f721e4229238a1fd51076472",
        "title": "Feedback Decision Transformer: Offline Reinforcement Learning With Feedback"
      },
      {
        "paperId": "c2b6f7e403060ea5e610513acf710b742d2d2de5",
        "title": "Game Interactive Learning: A New Paradigm towards Intelligent Decision-Making"
      },
      {
        "paperId": "af30a9b7def3f6620044eab20585a3d54f6cc9f1",
        "title": "Agent-Aware Training for Agent-Agnostic Action Advising in Deep Reinforcement Learning"
      },
      {
        "paperId": "12cf8be0865649b5e878051182b69f51e67fb8d4",
        "title": "A density estimation perspective on learning from pairwise human preferences"
      },
      {
        "paperId": "ec97a1565dff9d2fab1ef489e47296bbef68b680",
        "title": "Using Human Feedback to Fine-tune Diffusion Models without Any Reward Model"
      },
      {
        "paperId": "45a119463a79e1f18d1234a96174b617a086388b",
        "title": "Accelerating Reinforcement Learning of Robotic Manipulations via Feedback from Large Language Models"
      },
      {
        "paperId": "e6a207308c3f898d2c97ddf771c0060c23b93fe5",
        "title": "Conditions on Preference Relations that Guarantee the Existence of Optimal Policies"
      },
      {
        "paperId": "33b813e34e71515c67b0e89f97b5ff8dfc506dc4",
        "title": "Autonomous Robotic Reinforcement Learning with Asynchronous Human Feedback"
      },
      {
        "paperId": "746eea132521f94d23818a18e57bcadd31710e43",
        "title": "Diversify & Conquer: Outcome-directed Curriculum RL via Out-of-Distribution Disagreement"
      },
      {
        "paperId": "a1eaec19705e7611561d0d9fba6b150c08ddef41",
        "title": "Socially Cognizant Robotics for a Technology Enhanced Society"
      },
      {
        "paperId": "81c95bf5b3aa6f858c55e7c9d0525566a849079c",
        "title": "Toward Human Perception-Centric Video Thumbnail Generation"
      },
      {
        "paperId": "1a5c6deab4fc087776c7f382f8df2649bb67319a",
        "title": "Active teacher selection for reinforcement learning from human feedback"
      },
      {
        "paperId": "462f0e65fa5ce0bc27241abba519f8a4bf40e879",
        "title": "Learning to Discern: Imitating Heterogeneous Human Demonstrations with Preference and Representation Learning"
      },
      {
        "paperId": "4dcc799bb5a47a54a69d9045c073d44f3c9ef225",
        "title": "Learning Reward for Physical Skills using Large Language Model"
      },
      {
        "paperId": "386cebdba39d2d5f2862a9ab43a8d807f3863dae",
        "title": "Contrastive Preference Learning: Learning from Human Feedback without RL"
      },
      {
        "paperId": "3395268b0944a25d388498370cc1be21bf644958",
        "title": "Sample Complexity of Preference-Based Nonparametric Off-Policy Evaluation with Deep Networks"
      },
      {
        "paperId": "a95ac430d8d5b13f164a120ccdfb370eaf2a7b19",
        "title": "What can knowledge graph alignment gain with Neuro-Symbolic learning approaches?"
      },
      {
        "paperId": "8c9e95f32982ca21a7e4ef9c986aaa934cb11293",
        "title": "RoboCLIP: One Demonstration is Enough to Learn Robot Policies"
      },
      {
        "paperId": "0de72952fa31b595f8dfd6f370dfe26463cf18f1",
        "title": "Diversity from Human Feedback"
      },
      {
        "paperId": "023d462ec6ff84cee0d0716a34d11efc7cde8534",
        "title": "Reward Model Ensembles Help Mitigate Overoptimization"
      },
      {
        "paperId": "07575d485efe74f9440a36d7b343ac51415ee63e",
        "title": "AlignDiff: Aligning Diverse Human Preferences via Behavior-Customisable Diffusion Model"
      },
      {
        "paperId": "e571f9b823dcb31580161a37342a73dd93ebbe52",
        "title": "Learning Optimal Advantage from Preferences and Mistaking it for Reward"
      },
      {
        "paperId": "3a0e65b8af17adfb0f5f6db2fc80ec908a776fbb",
        "title": "Automatic Pair Construction for Contrastive Post-training"
      },
      {
        "paperId": "287ff9a6899f8ff0690de3c6a7ce8d6da3efbb61",
        "title": "VARIQuery: VAE Segment-Based Active Learning for Query Selection in Preference-Based Reinforcement Learning"
      },
      {
        "paperId": "8b59b9fd9bf92c22947476133ac87e03d3dff491",
        "title": "TranDRL: A Transformer-Driven Deep Reinforcement Learning Enabled Prescriptive Maintenance Framework"
      },
      {
        "paperId": "ecb6a8003d4726cc1e9e9f3c2d8ff6c0627e4e95",
        "title": "Text2Reward: Reward Shaping with Language Models for Reinforcement Learning"
      },
      {
        "paperId": "0d9e19a5465e71196a35766ce3a6614b14d802cf",
        "title": "Natural and Robust Walking using Reinforcement Learning without Demonstrations in High-Dimensional Musculoskeletal Models"
      },
      {
        "paperId": "29288d4f7f7e2e0b18d88c6ce0681fd181a18c68",
        "title": "Proximal Policy Optimization Actual Combat: Manipulating Output Tokenizer Length"
      },
      {
        "paperId": "ad91dbea609bbfc499d8788a18171bf4744b0692",
        "title": "PARL: A Unified Framework for Policy Alignment in Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "8e0e795463a2007497c9c257f9de337c53f1c4b9",
        "title": "Rating-based Reinforcement Learning"
      },
      {
        "paperId": "6eb46737bf0ef916a7f906ec6a8da82a45ffb623",
        "title": "Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "99841e3d9e80ad90fea718704b20ebc2e7fc4f1b",
        "title": "DIP-RL: Demonstration-Inferred Preference Learning in Minecraft"
      },
      {
        "paperId": "37c0a8503522a22d21bb851fd80c0dbf9ee5a567",
        "title": "Breadcrumbs to the Goal: Goal-Conditioned Exploration from Human-in-the-Loop Feedback"
      },
      {
        "paperId": "edf7707dbdcb249cba3c4284fd0a8f2dead32488",
        "title": "FigCaps-HF: A Figure-to-Caption Generative Framework and Benchmark with Human Feedback"
      },
      {
        "paperId": "aa3d0904c2e1c74b5111f3950ace2a8267c86a2e",
        "title": "STRAPPER: Preference-based Reinforcement Learning via Self-training Augmentation and Peer Regularization"
      },
      {
        "paperId": "875730429d0e5d92b8ac4f81e95b3fcde9116ca2",
        "title": "Boosting Feedback Efficiency of Interactive Reinforcement Learning by Adaptive Learning from Scores"
      },
      {
        "paperId": "22c8858ae250144bdc87d5487dd341d8fe9903cd",
        "title": "Human-in-the-Loop Reinforcement Learning in Continuous-Action Space"
      },
      {
        "paperId": "df1c12b6bb0dea8b53c996ecce5a3a4dfe80eaaf",
        "title": "Censored Sampling of Diffusion Models Using 3 Minutes of Human Feedback"
      },
      {
        "paperId": "19db2d61f20a6c439cc79f28ef4c9e4bf26cd20e",
        "title": "Preference Ranking Optimization for Human Alignment"
      },
      {
        "paperId": "b636d830ffb96fc1f4536046bf0828252cacc2cb",
        "title": "Can Differentiable Decision Trees Enable Interpretable Reward Learning from Human Feedback?"
      },
      {
        "paperId": "5af0197ebf4f4ba809f66b00ea344551135f061d",
        "title": "Fairness in Preference-based Reinforcement Learning"
      },
      {
        "paperId": "d93e0c440ce323c5f94ffe555481b8daadaa3c0d",
        "title": "Prefer to Classify: Improving Text Classifiers via Auxiliary Preference Learning"
      },
      {
        "paperId": "3211678bf2f01c5ea28bc67399b956b6273478cb",
        "title": "PEARL: Zero-shot Cross-task Preference Alignment and Robust Reward Learning for Robotic Manipulation"
      },
      {
        "paperId": "acd0afc43871b626c5f2ec48a0c4e1453b94b670",
        "title": "Provable Reward-Agnostic Preference-Based Reinforcement Learning"
      },
      {
        "paperId": "cedfdde4b9d01530bf2932554561bb25623890e5",
        "title": "Reinforcement Learning with Human Feedback: Learning Dynamic Choices via Pessimism"
      },
      {
        "paperId": "f40a3e38b66867610d61a242f99f561c7ca895be",
        "title": "Aligning Human Preferences with Baseline Objectives in Reinforcement Learning"
      },
      {
        "paperId": "b7f74db45b05e79489541ad80c2332969a7d2e6f",
        "title": "Query-Policy Misalignment in Preference-Based Reinforcement Learning"
      },
      {
        "paperId": "9c8b88b4b80e3df523a9887105f4126c116d833d",
        "title": "Learning Interpretable Models of Aircraft Handling Behaviour by Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "d123a153790442bc7d2fe39baa7621b795b7f6d0",
        "title": "Beyond Reward: Offline Preference-guided Policy Optimization"
      },
      {
        "paperId": "a553bf27d801d09f667fe121c0ba9632257f364b",
        "title": "DPOK: Reinforcement Learning for Fine-tuning Text-to-Image Diffusion Models"
      },
      {
        "paperId": "4367911d9d28d83fafbcf6c908698dd981ddbe9e",
        "title": "Inverse Preference Learning: Preference-based RL without a Reward Function"
      },
      {
        "paperId": "c7585a32106d53a367238896201c77e87b55b04e",
        "title": "A Novel Route Planning Approach Based on Energy-Based Action Sample Reinforcement Learning"
      },
      {
        "paperId": "fbe1003ec391f6bcf4660f6ef81f1e6199849bfe",
        "title": "Provably Feedback-Efficient Reinforcement Learning via Active Reward Learning"
      },
      {
        "paperId": "924a5e5d238c26e728ecaa995c0fe7d83da5264a",
        "title": "LIMIT: Learning Interfaces to Maximize Information Transfer"
      },
      {
        "paperId": "1b2355c3c674b26a977768a91a164384ad51bbb1",
        "title": "ImageReward: Learning and Evaluating Human Preferences for Text-to-Image Generation"
      },
      {
        "paperId": "4cf3b0793973beb48364d2d47dd870e1fd8d1778",
        "title": "Towards Healthy AI: Large Language Models Need Therapists Too"
      },
      {
        "paperId": "4413611c196b58b40274102e98651f6b07eed489",
        "title": "Learning Rewards to Optimize Global Performance Metrics in Deep Reinforcement Learning"
      },
      {
        "paperId": "83f1e9ce01e801aaa2878a3ea79f779bbc32d200",
        "title": "Interactive Policy Shaping for Human-Robot Collaboration with Transparent Matrix Overlays"
      },
      {
        "paperId": "cf41ae462687f81ce95b27113c6a4f9c2751de42",
        "title": "Vision-Language Models as Success Detectors"
      },
      {
        "paperId": "28544cb4c409bde8239f03b595bb82219a35aecd",
        "title": "Controlled Diversity with Preference : Towards Learning a Diverse Set of Desired Skills"
      },
      {
        "paperId": "d7e36bcaabc37abb8af0fe5ef59d9df58f219c0e",
        "title": "How To Guide Your Learner: Imitation Learning with Active Adaptive Expert Involvement"
      },
      {
        "paperId": "c28af43206867cb5529164e3dd6d9ea8b7cfe7f2",
        "title": "Active Reward Learning from Multiple Teachers"
      },
      {
        "paperId": "c6478decdfff11ccbd085967c2f83aea11927a46",
        "title": "Preference Transformer: Modeling Human Preferences using Transformers for RL"
      },
      {
        "paperId": "1e4b6567ff66de6cdcbe58c863538241e2f62b45",
        "title": "Aligning Text-to-Image Models using Human Feedback"
      },
      {
        "paperId": "1a07669e66f9e03ec724035dc751b3ce279072a0",
        "title": "Data Driven Reward Initialization for Preference based Reinforcement Learning"
      },
      {
        "paperId": "d7c96b8b397ac5bb7fa19550d1f4a5ae465d9e12",
        "title": "Exploiting Unlabeled Data for Feedback Efficient Human Preference based Reinforcement Learning"
      },
      {
        "paperId": "f5a7b66d05322acae07a3b8196c64f877dec38b6",
        "title": "A State Augmentation based approach to Reinforcement Learning from Human Preferences"
      },
      {
        "paperId": "9001e28c3c1bf4a575d7765168e9c3baea2de0c8",
        "title": "Eliciting User Preferences for Personalized Multi-Objective Decision Making through Comparative Feedback"
      },
      {
        "paperId": "cb3125e4f63f3d058a2a39270ecb585e86c3d1ff",
        "title": "Chain of Hindsight Aligns Language Models with Feedback"
      },
      {
        "paperId": "e9d8a11d4987210477625ba09d9169c2c3e67136",
        "title": "Aligning Human and Robot Representations"
      },
      {
        "paperId": "4f0bfeadd39e64456d15d400fda8ecc2197c3265",
        "title": "Direct Preference-based Policy Optimization without Reward Modeling"
      },
      {
        "paperId": "535708777863633f7cc7fd6f4a716b1483c7100f",
        "title": "Reinforcement Learning from Diverse Human Preferences"
      },
      {
        "paperId": "68c7652c408937ed155bb4289decf7d416b381bc",
        "title": "Efficient Preference-Based Reinforcement Learning Using Learned Dynamics Models"
      },
      {
        "paperId": "f1e7332a76be8f38091193e6e929d0d653f4867c",
        "title": "On The Fragility of Learned Reward Functions"
      },
      {
        "paperId": "88ae720354fa4883be13e25e1cf92dcd4fed5f5d",
        "title": "Benchmarks and Algorithms for Offline Preference-Based Reward Learning"
      },
      {
        "paperId": "18d750263f1dfe43374e8791cefa580a511c2098",
        "title": "Few-Shot Preference Learning for Human-in-the-Loop RL"
      },
      {
        "paperId": "0fd6c747b48526ba4abc05b4ae9260f93718ce8f",
        "title": "Efficient Meta Reinforcement Learning for Preference-based Fast Adaptation"
      },
      {
        "paperId": "0bc9585faf634bf98124a3c69b1f171e1ab85cca",
        "title": "Robot Learning on the Job: Human-in-the-Loop Autonomy and Learning During Deployment"
      },
      {
        "paperId": "05a2c3bdd1eeaeef226f56be4545bfadc9eb68f5",
        "title": "Rewards Encoding Environment Dynamics Improves Preference-based Reinforcement Learning"
      },
      {
        "paperId": "0bec67b22a85cc4344bbbeb837f369afde091288",
        "title": "The Expertise Problem: Learning from Specialized Feedback"
      },
      {
        "paperId": "8e2daf1f06d2d75bff9519c86382bf50fff6ae0f",
        "title": "Relative Behavioral Attributes: Filling the Gap between Symbolic Goal Specification and Reward Learning from Human Preferences"
      },
      {
        "paperId": "33777d1de46f275aa0911954abc483c249eda9d1",
        "title": "Learning on the Job: Self-Rewarding Offline-to-Online Finetuning for Industrial Insertion of Novel Connectors from Vision"
      },
      {
        "paperId": "25e24f8fe75c2dab835b7c8f8c6e0e82ff6f2805",
        "title": "Towards customizable reinforcement learning agents: Enabling preference specification through online vocabulary expansion"
      },
      {
        "paperId": "5c926e61729fb37fd4f635bd42760aa20faf07ec",
        "title": "Task Decoupling in Preference-based Reinforcement Learning for Personalized Human-Robot Interaction"
      },
      {
        "paperId": "16bc62ca0672c7cdd0ef693b5ae0a73731af41a8",
        "title": "When to Ask for Help: Proactive Interventions in Autonomous Reinforcement Learning"
      },
      {
        "paperId": "144caddc0d09fa4d2a83056f61c2de4389132732",
        "title": "Symbol Guided Hindsight Priors for Reward Learning from Human Preferences"
      },
      {
        "paperId": "b0726dd009bae5f602a4e71ac5f9e8f53b6e385c",
        "title": "Advances in Preference-based Reinforcement Learning: A Review"
      },
      {
        "paperId": "d514092b1a7d7a1f9657297b2adda342057ae687",
        "title": "A Simulation System for Human-in-the-Loop Driving"
      },
      {
        "paperId": "096bd2bb8fb711d27043eba4a24d14a77f766935",
        "title": "Advice Conformance Verification by Reinforcement Learning agents for Human-in-the-Loop"
      },
      {
        "paperId": "ccc95a33440675d5d0947acef1e0b769c2b5c746",
        "title": "Reward Learning with Trees: Methods and Evaluation"
      },
      {
        "paperId": "0503229b89eb4611ac4a0d904541c7e2e3bf0ee2",
        "title": "Unified Learning from Demonstrations, Corrections, and Preferences during Physical Human\u2013Robot Interaction"
      },
      {
        "paperId": "9f9b61e429e85e37d6df0e3c478a074f7e6cb9fc",
        "title": "Models of human preference for learning reward functions"
      },
      {
        "paperId": "cc9f2fd320a279741403c4bfbeb91179803c428c",
        "title": "Reward Uncertainty for Exploration in Preference-based Reinforcement Learning"
      },
      {
        "paperId": "c9374c6435dc105e7142597fb9c0b5b36df3bf6f",
        "title": "Learning Dense Reward with Temporal Variant Self-Supervision"
      },
      {
        "paperId": "315acb4dece97c44e54e5ada981f68ed4e780d90",
        "title": "Digital Transformation in Smart Farm and Forest Operations Needs Human-Centered AI: Challenges and Future Directions"
      },
      {
        "paperId": "7cd79ad685a23e7a9acc1b7a25e15f739a6dd2a5",
        "title": "Teachable Reinforcement Learning via Advice Distillation"
      },
      {
        "paperId": "6d0adac188152fbaa45a88ba4da788926ed8144a",
        "title": "Reinforcement Learning in Practice: Opportunities and Challenges"
      },
      {
        "paperId": "668c2c23a39b35056db3908824564556d3522f14",
        "title": "Feedback-efficient Active Preference Learning for Socially Aware Robot Navigation"
      },
      {
        "paperId": "80110db20124242316a13795e964d09cd15a3802",
        "title": "Learning Long-Term Reward Redistribution via Randomized Return Decomposition"
      },
      {
        "paperId": "51965de80f86432d42749427db1e5bb0fa1e204c",
        "title": "B-Pref: Benchmarking Preference-Based Reinforcement Learning"
      },
      {
        "paperId": "7d541b023e6feb8aee059744b87e78c1d23cd160",
        "title": "A survey on deep learning and deep reinforcement learning in robotics with a tutorial on deep reinforcement learning"
      },
      {
        "paperId": "d5acaa2771c5db03c53b7cc656d77cb435f89201",
        "title": "A Framework for Learning to Request Rich and Contextually Useful Information from Humans"
      },
      {
        "paperId": "7981ed44d7c6c63990dca2ea3e29299dfd98ce87",
        "title": "Learning Latent Actions without Human Demonstrations"
      },
      {
        "paperId": "f70e82b8b7c792a3cdbf2c9bf2e7af06fd6a7269",
        "title": "Skill Preferences: Learning to Extract and Execute Robotic Skills from Human Feedback"
      },
      {
        "paperId": "a1d8cc2fdb7b0f262dc5b057f13bdcaea695842a",
        "title": "Offline Preference-Based Apprenticeship Learning"
      },
      {
        "paperId": "eb74aa6ace5316f1b3feb8e0a582a16fb077ba2c",
        "title": "Human engagement providing evaluative and informative advice for interactive reinforcement learning"
      },
      {
        "paperId": "364deb949f6ad1abee7f10079b48b61da9203579",
        "title": "Widening the Pipeline in Human-Guided Reinforcement Learning with Explanation and Context-Aware Data Augmentation"
      },
      {
        "paperId": "061bfda5b75e03dcb15ee4e94e841c2aeeaeccbf",
        "title": "Active preference-based Gaussian process regression for reward learning and optimization"
      },
      {
        "paperId": "d383d344818ee1d6ad58f7a1928cbc0b5f40a3a3",
        "title": "TEMPO: Timestep Explanations for Modeling Preferences in Online Preference-Based RL"
      },
      {
        "paperId": "9ecb46066c2e8934f8853183414562ee2d461afe",
        "title": "Towards human-like questioning: Knowledge base question generation with bias-corrected reinforcement learning from human feedback"
      },
      {
        "paperId": "72c0d8415f6f7c051f7ac65edcec4ec307c46375",
        "title": "Human-informed skill discovery: Controlled diversity with preference in reinforcement learning"
      },
      {
        "paperId": "ef9e1e3650c8d6f57ba25f417a32d57123d69dcf",
        "title": "Policy Evaluation for Reinforcement Learning from Human Feedback: A Sample Complexity Analysis"
      },
      {
        "paperId": "c3869d7109e11803b8d83cbe35d68224655c2983",
        "title": "S-EPOA: Overcoming the Indivisibility of Annotations with Skill-Driven Preference-Based Reinforcement Learning"
      },
      {
        "paperId": "f613110d1424bb4c3b0345f6b3fe869399ea0333",
        "title": "Large Learning Agents: Towards Continually Aligned Robots with Scale in RL"
      },
      {
        "paperId": "5a2c395ca5dc942dbb6ea7eea1e324b8c9e8534e",
        "title": "Social Choice for AI Alignment: Dealing with Diverse Human Feedback"
      },
      {
        "paperId": "945916c887b39eac59db25cbbf007f342ba076be",
        "title": "SCaR: Refining Skill Chaining for Long-Horizon Robotic Manipulation via Dual Regularization"
      },
      {
        "paperId": "35df81ea3e3799191b29ac661953cfeea4f353c0",
        "title": "A theoretical case-study of Scalable Oversight in Hierarchical Reinforcement Learning"
      },
      {
        "paperId": "8e3d4f0bf86baaf6db2f02b37cad82fdc862fdcb",
        "title": "REBEL: A Regularization-Based Solution for Reward Overoptimization in Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "8c2ed19db86875489a5f661ef68b6ba91ad15a94",
        "title": "Zero-shot Preference Learning for Offline RL via Optimal Transport"
      },
      {
        "paperId": "74e3b120e375e2fa98bbee0241f6ed3eeec75a3c",
        "title": "Adversarial Imitation Learning with Preferences"
      },
      {
        "paperId": "d239bb8e26e388f9053f51800239db1360ba0bf5",
        "title": "A Toolkit for Encouraging Safe Diversity in Skill Discovery"
      },
      {
        "paperId": "db20bd3bb82d1011ce704d440d8c2578f665e6e1",
        "title": "Aligning Robot and Human Representations"
      },
      {
        "paperId": "18a387c83289267c79ba4adf6af17c88b73d5180",
        "title": "Contextual Bandits and Imitation Learning via Preference-Based Active Queries"
      },
      {
        "paperId": "c52b30a60fda5f23cc0d2241c4e127f5191bbb2d",
        "title": "Text2Reward: Automated Dense Reward Function Generation for Reinforcement Learning"
      },
      {
        "paperId": "bcc91889adc389cbe25295961c07a3484225ee7b",
        "title": "How to Query Human Feedback Efficiently in RL?"
      },
      {
        "paperId": "b3206ab5761734c2135acbdc7863611667de8863",
        "title": "Preference learning for guiding the tree search in continuous POMDPs"
      },
      {
        "paperId": "4e086201a92a443244a15219514cb03bf9fc274d",
        "title": "Policy Optimization for Waste Crane Automation From Human Preferences"
      },
      {
        "paperId": "80537867280f854ceb55492121d59991eda9b95b",
        "title": "SafeDICE: Offline Safe Imitation Learning with Non-Preferred Demonstrations"
      },
      {
        "paperId": "836a463da0e813b6236adebef6b9a9cc9bdbe3f7",
        "title": "Contrastive Post-training Large Language Models on Data Curriculum"
      },
      {
        "paperId": "f6c2b952987fbea0add1a3f5ff6e416b6d63051b",
        "title": "Meta-Reward-Net: Implicitly Differentiable Reward Learning for Preference-based Reinforcement Learning"
      },
      {
        "paperId": "f2a2401a35b6b892d43642b31700e83e88b2ebb8",
        "title": "SURF: Semi-supervised Reward Learning with Data Augmentation for Feedback-efficient Preference-based Reinforcement Learning"
      },
      {
        "paperId": "a059870910339596c5c57969bc2397a7e268435e",
        "title": "Learning Embodied Agents with Scalably-Supervised Reinforcement Learning"
      },
      {
        "paperId": "8c8868d75f5fc7a055fdbc8610ab20b0a4304829",
        "title": "Deep Reinforcement Learning: Opportunities and Challenges"
      },
      {
        "paperId": "26928049100474c9366ec683e83353c57c3f28e3",
        "title": "Explicable Policy Search"
      },
      {
        "paperId": "de9cbcd12490b1a8232044cf486ee500d4edca3a",
        "title": "Tailoring Visual Object Representations to Human Requirements: A Case Study with a Recycling Robot"
      },
      {
        "paperId": "992a917bc523194f43e0fd38ba407d19561da058",
        "title": "Research Challenges in Coupling Artificial Intelligence and Network Management"
      },
      {
        "paperId": "0844bb51701e610c8df05af153bb243a089e3857",
        "title": "Exploiting Action Distances for Reward Learning from Human Preferences"
      },
      {
        "paperId": "00591c2a5a599fd7b0d56335fdf6088db7cd693d",
        "title": "BATTLE: T OWARDS B EHAVIOR - ORIENTED A DVER - SARIAL A TTACKS AGAINST D EEP R EINFORCEMENT L EARNING"
      },
      {
        "paperId": "2536f94c4dedcf9edd5cf9f854922590a6944204",
        "title": "Preference Proxies: Evaluating Large Language Models in capturing Human Preferences in Human-AI Tasks"
      },
      {
        "paperId": "1cbb8b5bba10c3fabf839b687894e23efaa4deaa",
        "title": "Toward Measuring the Effect of Robot Competency on Human Kinesthetic Feedback in Long-Term Task Learning"
      },
      {
        "paperId": "3a8da4edb5ad549fe01a0096a9e892c8f6658726",
        "title": "Reinforcement Learning from Human Feedback for Cyber-Physical Systems: On the Potential of Self-Supervised Pretraining"
      },
      {
        "paperId": "23114274b59700e485155fb5ec305935790e3f34",
        "title": "C ONTRASTIVE P OST - TRAINING"
      },
      {
        "paperId": "ccdd35ef5487e3ebb6aa11aa2fbbbfaaf2208a8f",
        "title": "Optimizing Reward Models with Proximal Policy Exploration in Preference-Based Reinforcement Learning"
      },
      {
        "paperId": "a248b0f89762f0427de4c60fae983af15aa2cabf",
        "title": "FDPP: Fine-tune Di\ufb00usion Policy with Human Preference"
      },
      {
        "paperId": "9af77bf52fb01d3dbbd6bac0b656da18309438ad",
        "title": "Interactive Terrain Affordance Learning via VAE Query Selection & Data Manipulation"
      }
    ],
    "score": 79.0
  },
  {
    "id": "f8d8e192979ab5d8d593e76a0c4e2c7581778732",
    "title": "Voronoi-Based Multi-Robot Autonomous Exploration in Unknown Environments via Deep Reinforcement Learning",
    "authors": [
      "Junyan Hu",
      "Hanlin Niu",
      "J. Carrasco",
      "B. Lennox",
      "F. Arvin"
    ],
    "year": 2020,
    "citationCount": 291,
    "abstract": "Autonomous exploration is an important application of multi-vehicle systems, where a team of networked robots are coordinated to explore an unknown environment collaboratively. This technique has earned significant research interest due to its usefulness in search and rescue, fault detection and monitoring, localization and mapping, etc. In this paper, a novel cooperative exploration strategy is proposed for multiple mobile robots, which reduces the overall task completion time and energy costs compared to conventional methods. To efficiently navigate the networked robots during the collaborative tasks, a hierarchical control architecture is designed which contains a high-level decision making layer and a low-level target tracking layer. The proposed cooperative exploration approach is developed using dynamic Voronoi partitions, which minimizes duplicated exploration areas by assigning different target locations to individual robots. To deal with sudden obstacles in the unknown environment, an integrated deep reinforcement learning based collision avoidance algorithm is then proposed, which enables the control policy to learn from human demonstration data and thus improve the learning speed and performance. Finally, simulation and experimental results are provided to demonstrate the effectiveness of the proposed scheme.",
    "url": "https://www.semanticscholar.org/paper/f8d8e192979ab5d8d593e76a0c4e2c7581778732",
    "pdf_url": "https://doi.org/10.1109/TVT.2020.3034800",
    "venue": "IEEE Transactions on Vehicular Technology",
    "publicationDate": "2020-10-29",
    "externalIds": {
      "DBLP": "journals/tvt/HuNCLA20",
      "MAG": "3095315965",
      "DOI": "10.1109/TVT.2020.3034800",
      "CorpusId": 228989788
    },
    "references": [
      {
        "paperId": "1db28045af0c1e2dcf7a2ec1e4bd1179e779049c",
        "title": "Occlusion-Based Coordination Protocol Design for Autonomous Robotic Shepherding Tasks"
      },
      {
        "paperId": "a3e4e79d89c2e73827f9f377e1e5420fd49b201a",
        "title": "Two\u2010layer distributed formation\u2010containment control strategy for linear swarm systems: Algorithm and experiments"
      },
      {
        "paperId": "7ec2b76ea930ac90bac33a670d122691a1b843e4",
        "title": "Reinforcement-Learning-Based Relay Mobility and Power Allocation for Underwater Sensor Networks Against Jamming"
      },
      {
        "paperId": "a492fc430a469ae36a3e2c73738059a8a2a2d19d",
        "title": "Prediction of Reward Functions for Deep Reinforcement Learning via Gaussian Process Regression"
      },
      {
        "paperId": "3d2a3b7880f8cff523546715e6d7dd2f43d24ccd",
        "title": "Distributed Adaptive Time-Varying Group Formation Tracking for Multiagent Systems With Multiple Leaders on Directed Graphs"
      },
      {
        "paperId": "4e373ea738a0a72616a7047e6b39989a734176c0",
        "title": "RACE: Reinforced Cooperative Autonomous Vehicle Collision Avoidance"
      },
      {
        "paperId": "6b859e32e23dbb5ac866837557f34f5c9ddd3a20",
        "title": "A Motion Planning and Tracking Framework for Autonomous Vehicles Based on Artificial Potential Field Elaborated Resistance Network Approach"
      },
      {
        "paperId": "d345e4f6b6188ab38809397d059fbdbad656fcb0",
        "title": "Cooperative Control of Heterogeneous Connected Vehicle Platoons: An Adaptive Leader-Following Approach"
      },
      {
        "paperId": "4c10ecc439973c3aa5420f6d384e71486715364d",
        "title": "Multi-UAV Formation Control Based on a Novel Back-Stepping Approach"
      },
      {
        "paperId": "10bb6a0c879940e5d55466b66972c19dcc374806",
        "title": "Bayesian Reinforcement Learning for Multi-Robot Decentralized Patrolling in Uncertain Environments"
      },
      {
        "paperId": "44ac9c36df912b250c98600182aa60d9cf985de1",
        "title": "Distributed Finite-Time Consensus Control for Heterogeneous Battery Energy Storage Systems in Droop-Controlled Microgrids"
      },
      {
        "paperId": "4a1358ca269cea6dc8b57f531ecd31524a82551f",
        "title": "Efficient Dynamic Object Search in Home Environment by Mobile Robot: A Priori Knowledge-Based Approach"
      },
      {
        "paperId": "8cee8a71b697b03726404ba465853a2a48788057",
        "title": "Deep Reinforcement Learning of Navigation in a Complex and Crowded Environment with a Limited Field of View"
      },
      {
        "paperId": "857066447d0031e757a0cbbf7aea394a148b8cea",
        "title": "Composite Platoon Trajectory Planning Strategy for Intersection Throughput Maximization"
      },
      {
        "paperId": "60da61a6a867d06194c6542dc2c176e5e6556c03",
        "title": "Mobile robot path planning using membrane evolutionary artificial potential field"
      },
      {
        "paperId": "002364ac0707b16c945fe7c129f09ff0ce45008c",
        "title": "Autonomous Robotic Exploration by Incremental Road Map Construction"
      },
      {
        "paperId": "86e3c8c79f1c56807a27bc6757ed3d2730bd272d",
        "title": "Efficient Autonomous Exploration Planning of Large-Scale 3-D Environments"
      },
      {
        "paperId": "ed6f2b10be3d87c7e7d416a506c35958286e590c",
        "title": "Communication-Efficient Planning and Mapping for Multi-Robot Exploration in Large Environments"
      },
      {
        "paperId": "3dc251c816f42e1101e8da43f9c8cac1c418e1d0",
        "title": "Autonomous Exploration and Mapping System Using Heterogeneous UAVs and UGVs in GPS-Denied Environments"
      },
      {
        "paperId": "aea13e94d9c75d8f31c141bfd4885648c0e0f8e8",
        "title": "Voronoi-Visibility Roadmap-based Path Planning Algorithm for Unmanned Surface Vehicles"
      },
      {
        "paperId": "cc13c12259d70d6ef09e38ce6cdfcf3d23db6d86",
        "title": "Deep Reinforcement Learning Robot for Search and Rescue Applications: Exploration in Unknown Cluttered Environments"
      },
      {
        "paperId": "12d8f42595f78f9b50bef817b77bee7f3947d1f1",
        "title": "Trajectory Design and Power Control for Multi-UAV Assisted Wireless Networks: A Machine Learning Approach"
      },
      {
        "paperId": "b472d6b1346450b8b3839cd52e0c41e8735d5fcc",
        "title": "An energy-efficient path planning algorithm for unmanned surface vehicles"
      },
      {
        "paperId": "ef63a895fe9ebb37999e4e258fa4e2762f048f47",
        "title": "Deep Reinforcement Learning Supervised Autonomous Exploration in Office Environments"
      },
      {
        "paperId": "413ac6b3f6ef8da79fb3765dadfb3cbb7a176b3b",
        "title": "Cooperative Exploration and Networking While Preserving Collision Avoidance"
      },
      {
        "paperId": "bca91c27d811588409a6ce78853c593f64215bda",
        "title": "Towards Optimally Decentralized Multi-Robot Collision Avoidance via Deep Reinforcement Learning"
      },
      {
        "paperId": "c311e948001d28bc2f5679c41127165c956f04e4",
        "title": "Collision Avoidance for Cooperative UAVs With Optimized Artificial Potential Field Algorithm"
      },
      {
        "paperId": "799c0e461332570ecde97e13266fecde8476efe3",
        "title": "Virtual-to-real deep reinforcement learning: Continuous control of mobile robots for mapless navigation"
      },
      {
        "paperId": "521bb4bc3a0e3e64922ab70eff3bf98d0f4090b5",
        "title": "A Two-Stage Optimized Next-View Planning Framework for 3-D Unknown Environment Exploration, and Structural Reconstruction"
      },
      {
        "paperId": "1a0d50fd4a3e52b25c0a662b687daeb8ea963b4b",
        "title": "Deep reinforcement learning with successor features for navigation across similar environments"
      },
      {
        "paperId": "024006d4c2a89f7acacc6e4438d156525b60a98f",
        "title": "Continuous control with deep reinforcement learning"
      },
      {
        "paperId": "7edde18a073b734a16c2f530cece4bb17906d86d",
        "title": "The Sensor-based Random Graph Method for Cooperative Robot Exploration"
      },
      {
        "paperId": "ce81a0b905dcfc206d5122dd93785be3d1f40bc7",
        "title": "Coordinated multi-robot exploration"
      },
      {
        "paperId": "df0295715f8b43a8cbaa19f74a3dceb06e610eaf",
        "title": "Design and use paradigms for Gazebo, an open-source multi-robot simulator"
      },
      {
        "paperId": "ec350223de4dbc7ddb1e8fd1d2fbeb2288a992e9",
        "title": "A Probabilistic Approach to Collaborative Multi-Robot Localization"
      },
      {
        "paperId": "752a9d8506a1e67687e29f845b13f465a705a63c",
        "title": "Planning and Control for Collision-Free Cooperative Aerial Transportation"
      },
      {
        "paperId": "600648d53c0ec87823a03904080a07e0c68ff871",
        "title": "Development of Collision Avoidance Algorithms for the C-Enduro USV"
      },
      {
        "paperId": null,
        "title": "collisionavoidance,deepreinforcementlearningofautonomousvehicles,andtele-operationofrobotic arm"
      },
      {
        "paperId": null,
        "title": "LeibnizUni-versittHannover,Hannover,Germany"
      },
      {
        "paperId": null,
        "title": "Barry Lennox (Senior Member, IEEE) is a Professor of applied control and nuclear engineering decommissioning and holds a Royal Academy of Engineering Chair in Emerging Technologies"
      },
      {
        "paperId": null,
        "title": "His research interests includeswarmroboticsandautonomoussystems"
      },
      {
        "paperId": null,
        "title": "Laboratory with the University Microelectronics"
      }
    ],
    "cited_by": [
      {
        "paperId": "9008e582b8ba3478ac84025f88cde14a21ee290f",
        "title": "LLM-MCoX: Large Language Model-based Multi-robot Coordinated Exploration and Search"
      },
      {
        "paperId": "b6f3942454362c0e61cd8f30e9fbb7a617623fac",
        "title": "GUIDE: A Diffusion-Based Autonomous Robot Exploration Framework Using Global Graph Inference"
      },
      {
        "paperId": "8013f1f866ede099598bfbcc4510a644868d6557",
        "title": "Subteaming and Adaptive Formation Control for Coordinated Multi-Robot Navigation"
      },
      {
        "paperId": "e850e195a0e398d942a405233f5628c6fb45cd83",
        "title": "Energy Efficient Multi Robot Package Delivery under Capacity-Constraints via Voronoi-Constrained Networks"
      },
      {
        "paperId": "341ebf2c8b5f3ea26a41411f3f5cfc700f7781d9",
        "title": "A Quantum-Inspired Hybrid Artificial Neural Network for Identifying the Dynamic Parameters of Mobile Car-Like Robots"
      },
      {
        "paperId": "2ee331bb658310042694033d628cb114bdeab340",
        "title": "DELIVER: A System for LLM-Guided Coordinated Multi-Robot Pickup and Delivery using Voronoi-Based Relay Planning"
      },
      {
        "paperId": "a555208cd9bad078efa92fd1a130ba7c3b4a550d",
        "title": "Sensing, Social, and Motion Intelligence in Embodied Navigation: A Comprehensive Survey"
      },
      {
        "paperId": "30df0ea048cde49140221a452af9a976bfbdd636",
        "title": "MoRoCo: Multi-operator-robot Coordination, Interaction and Exploration under Restricted Communication"
      },
      {
        "paperId": "0d2adbc14b7a5e69f0deefc73b71c640f2b9ff68",
        "title": "HAFE: Hierarchical Attention-based Frontier Exploration for Multi-Robot Mapping"
      },
      {
        "paperId": "dce30cc17f85cbc8a45a1fc79c810ba5a8dd32d8",
        "title": "Adaptive Predefined\u2010Time Formation Control for Multi\u2010Omnidirectional Mobile Robot Systems With Output Constraints"
      },
      {
        "paperId": "464888a476a1146cffda2778e6e674a89bbe5872",
        "title": "Omni-Explorer: A Rapid Autonomous Exploration Framework With FOV Expansion Mechanism"
      },
      {
        "paperId": "473be2a0901db539bf88d042d8028ab69b9dff44",
        "title": "Learning-Augmented Model-Based Multi-Robot Planning for Time-Critical Search and Inspection Under Uncertainty"
      },
      {
        "paperId": "a845cabf1bac427d0988106a94aa0d24c93855b0",
        "title": "Scalable and Adaptive Exploration Framework for Multi-UAV System"
      },
      {
        "paperId": "9535e4d3cfc721e92920fdabd0a72c01a94055cc",
        "title": "An Innovative Approach for Obstacle Avoidance and Path Planning of Mobile Robot Using Adaptive Deep Reinforcement Learning for Indoor Environment"
      },
      {
        "paperId": "1f9b59d18b51213ebdd6ce163adc9ae30cc7b7a2",
        "title": "Indoor cooperative exploration using robot swarms enhanced by human understanding"
      },
      {
        "paperId": "a3ea59cd6d11de3a0c45ea893fd2f0f2126748ed",
        "title": "A Solution Surface in Nine-Dimensional Space to Optimise Ground Vibration Effects Through Artificial Intelligence in Open-Pit Mine Blasting"
      },
      {
        "paperId": "9b2cdaae6ad01624ebc2041b2304bdf89e8e19fb",
        "title": "Vertex addition to a ball graph with application to reliability and area coverage in autonomous swarms"
      },
      {
        "paperId": "bb524e2381fb139ce84676fdfd2804a9d53488c6",
        "title": "FlexiRL: An Integrated Jamming Strategy Framework for Hybrid Mode and Parameter Decision-Making"
      },
      {
        "paperId": "b78709f7602cebcd5ce74767381c2d06fab2fb79",
        "title": "EA-OSPGB: Multiple robots dynamic online algorithm for solving full coverage path planning of multiple robots in unknown terrain environments"
      },
      {
        "paperId": "c4c2424a7455e403295b57dd2ee83224b0f570af",
        "title": "A* Algorithm Path Planning based on Geometric Optimization"
      },
      {
        "paperId": "cfc29a779be6ef935c1cdbd20452c591722954d6",
        "title": "Multi-Robot Full Coverage Algorithm Considering Charging Pile Constraints"
      },
      {
        "paperId": "f38814d36929bbc91010ac77f71bcbbed70a45e9",
        "title": "NavEX: A Multi-Agent Coverage in Non-Convex and Uneven Environments via Exemplar-Clustering"
      },
      {
        "paperId": "0166b626b056367769391f2f51f9fbbcbff961a7",
        "title": "ARMOR: Adaptive Meshing with Reinforcement Optimization for Real-time 3D Monitoring in Unexposed Scenes"
      },
      {
        "paperId": "128917430a43159f11d2e81503cc86d19ded17ac",
        "title": "Optimization of shunting operation plan in large freight train depot based on DQN algorithm"
      },
      {
        "paperId": "04ff0ac5a1550595bd936d2da83544add80bdcb4",
        "title": "Path-Planning Method Based on Reinforcement Learning for Cooperative Two-Crane Lift Considering Load Constraint"
      },
      {
        "paperId": "e28211948800f541217b1b1b3d77003603e476af",
        "title": "Hierarchical Reinforcement-Learning-Based Joint Allocation of Jamming Task and Power for Countering Networked Radar"
      },
      {
        "paperId": "1d2267827822b0da7c14e9058bec49d693123fb8",
        "title": "A Network Connectivity-Aware Reinforcement Learning Method for Task Exploration and Allocation"
      },
      {
        "paperId": "5dc39fc0f5c9db2812fc470ab6ac27cacba2bb9a",
        "title": "Voronoi-GRU-Based Multi-Robot Collaborative Exploration in Unknown Environments"
      },
      {
        "paperId": "62670e059c9e58fab14add10af425caa1c17b299",
        "title": "DART: Dual-level Autonomous Robotic Topology for Efficient Exploration in Unknown Environments"
      },
      {
        "paperId": "ea3a6c3324793336c7253811b7da98dd0826bc24",
        "title": "Unsupervised Multi-Clustering and Decision-Making Strategies for 4D-STEM Orientation Mapping"
      },
      {
        "paperId": "752bbed6c69164d2fd481483b492d63075fb340d",
        "title": "A high-effective swarm intelligence-based multi-robot cooperation method for target searching in unknown hazardous environments"
      },
      {
        "paperId": "550e68f9a184c213738489f698d359b982262d44",
        "title": "Semantic-Driven Informed Planning and 3D Reconstruction for the Quadrotor Unmanned Aerial Vehicle"
      },
      {
        "paperId": "a7b1787ebb3e67cd2657de1872d674250456645a",
        "title": "Hierarchical decision and control method for the human-exoskeleton collaborative packaging system based on deep reinforcement learning"
      },
      {
        "paperId": "74128804d80f6b543775ea39035f6e06ce2b5469",
        "title": "MARVEL: Multi-Agent Reinforcement Learning for Constrained Field-of-View Multi-Robot Exploration in Large-Scale Environments"
      },
      {
        "paperId": "2c955dfda671039dea41b36855629e0ff6636a76",
        "title": "A Context-Aware Framework for Sensor Placement Using PSO and Voronoi in Dynamic Scenarios"
      },
      {
        "paperId": "c6dafa0041e08c66d178aa74c5a9d7437a07cb47",
        "title": "FoxA1 knockdown promotes BMSC osteogenesis in part by activating the ERK1/2 signaling pathway and preventing ovariectomy-induced bone loss"
      },
      {
        "paperId": "4f20c9f6698ef1e6f8d72f8e2956513122ca833b",
        "title": "CMADRL: cross-modal attention based deep reinforcement learning for mobile robot\u2019s obstacle avoidance"
      },
      {
        "paperId": "505c656e7a28e80fe8429ae5fa76ca85e1453506",
        "title": "Deep Reinforcement Learning for Localisability\u2010Aware Mapless Navigation"
      },
      {
        "paperId": "9d1e401831dc5648de567093b161d2d216d7f863",
        "title": "Investigating the Impact of Communication-Induced Action Space on Exploration of Unknown Environments with Decentralized Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "8559bd3a1f1d3f75323a0e5298c56666e4e17ccd",
        "title": "Chatbot Adoption Framework for Real-Time Customer Care Support"
      },
      {
        "paperId": "e04c86d6399e11beb12e93c82a994b37d50f6176",
        "title": "A Distributed Multi-Robot Collaborative Hunting Method in Dynamic Cluttered Environments"
      },
      {
        "paperId": "95adfabe7dbc7b941aca9b193266e52397d8f7c1",
        "title": "Coordinating Search with Foundation Models and Multi-Agent Reinforcement Learning in Complex Environments"
      },
      {
        "paperId": "06dd33c995e8b655f41e7ec8a3d7aa7d99a8c6b9",
        "title": "Efficient and Resilient Multi-Robot Exploration in Complex and Unknown Indoor Environments"
      },
      {
        "paperId": "1a74203640ec8957c5b3555dc9915a4ff4a76f6e",
        "title": "Deep reinforcement learning path planning and task allocation for multi-robot collaboration"
      },
      {
        "paperId": "8f2eade5144d9bcf6048bd5885c6eda410a7053e",
        "title": "Deep learning in standard least-squares theory of linear models: Perspective, development and vision"
      },
      {
        "paperId": "bf9c9cbba7f80310b9a8324b8291a1ba10f6ae57",
        "title": "Applications of deep reinforcement learning in nuclear energy: A review"
      },
      {
        "paperId": "da047bcd92b6a0d4e075604a9427ee230ad02f8a",
        "title": "Z-Score Experience Replay in Off-Policy Deep Reinforcement Learning"
      },
      {
        "paperId": "33042c3bf2d147b30e5667a7b25b1ae0fc1118c5",
        "title": "Efficient deep reinforcement learning-based multirobot path planning"
      },
      {
        "paperId": "9fce416d5353250edf39ed611e1d203f923318c9",
        "title": "SPACE: 3D Spatial Co-operation and Exploration Framework for Robust Mapping and Coverage with Multi-Robot Systems"
      },
      {
        "paperId": "458d9ccb6583d3126536da458379378f9d809225",
        "title": "Multirobot unknown environment exploration and obstacle avoidance based on a Voronoi diagram and reinforcement learning"
      },
      {
        "paperId": "8b47b5901d1698483402f44196f7671efec14954",
        "title": "APF-CPP: An Artificial Potential Field Based Multi-Robot Online Coverage Path Planning Approach"
      },
      {
        "paperId": "bf140e0c4d02dfb425645b30640e48486e3b2d42",
        "title": "Exothermic reaction on non-Newtonian NEPCM over ellipses in a curved channel: Hybrid ISPH method and artificial intelligence"
      },
      {
        "paperId": "e4554583f671773f0227875c0a048198e69bbe8e",
        "title": "An Integrated Approach to Multi-Agent Scheduling with Bounded Objectives"
      },
      {
        "paperId": "5ed36cf6d266d42e04dab82ede73d869879932d2",
        "title": "Multi-Vehicle Exploration and Planning in Unknown Indoor Environment"
      },
      {
        "paperId": "c3725ddc4ee213001f8ca732047bfb6c4271f955",
        "title": "Low-Bandwidth Adaptive Human-Machine Collaboration Technology for Small Indoor Robots"
      },
      {
        "paperId": "bc1f1eb427a194a02b36e624a771edd61c200e58",
        "title": "Safe Autonomous Driving via Deep Reinforcement Learning and Model Predictive Control"
      },
      {
        "paperId": "b105781e729d83b9645f75cb3ded96c4639e0c95",
        "title": "Multi-Stage Data Collection and Path Planning for Multiple UAV-enabled Aerial REM Construction"
      },
      {
        "paperId": "fce91a68aba85e921e05b2d782b83916576ca589",
        "title": "Predicting low-velocity impact on the CNTs-reinforced plate using machine learning models"
      },
      {
        "paperId": "ba3bea4b15d5ef33579ce86da83b92ee1ae0b347",
        "title": "Using reinforcement learning to autonomously identify sources of error for agents in group missions"
      },
      {
        "paperId": "acd370650c2bd09fd8f67abc3e87217e25ad7e82",
        "title": "D-MARL: A Dynamic Communication-Based Action Space Enhancement for Multi Agent Reinforcement Learning Exploration of Large Scale Unknown Environments"
      },
      {
        "paperId": "f6a40dc7293a4485f45ddfc0d47e0c9f19296807",
        "title": "Decentralized Communication-Maintained Coordination for Multi-Robot Exploration: Achieving Connectivity and Adaptability"
      },
      {
        "paperId": "a7330f2233db87200c090aa2e94e8a36108e85bb",
        "title": "Online Unmanned Ground Vehicle Path Planning Based on Multi-Attribute Intelligent Reinforcement Learning for Mine Search and Rescue"
      },
      {
        "paperId": "f86e055671718bb12b91407ecda4858eaea5939a",
        "title": "Hypergraph-Based Coordinated Task Allocation and Socially-Aware Navigation for Multi-Robot Systems"
      },
      {
        "paperId": "8ac147c6bc742f9ee6b4145de1792a134feb505c",
        "title": "On-policy Actor-Critic Reinforcement Learning for Multi-UAV Exploration"
      },
      {
        "paperId": "981e28ceac80f44ef48ed110b1953ef6f5414166",
        "title": "Frontier Shepherding: A Bio-inspired Multi-robot Framework for Large-Scale Exploration"
      },
      {
        "paperId": "9cac52531fae72a2c6e956a471b7bb6642244ca4",
        "title": "NEUSIS: A Compositional Neuro-Symbolic Framework for Autonomous Perception, Reasoning, and Planning in Complex UAV Search Missions"
      },
      {
        "paperId": "6127c2ca7eb86072987061b9135d116e6f15f1c0",
        "title": "An Efficient Autonomous Exploration Framework for Unmanned Surface Vehicles in Unknown Waters"
      },
      {
        "paperId": "b002ab6d2c438e678e78b38e3db7e70d2699bbcc",
        "title": "Application of Robotic Exploration Principles to the Challenge of Cybersecurity Penetration Testing"
      },
      {
        "paperId": "b861ec6e9ced79bd6c5e9bc359942ed9250cf053",
        "title": "Fast and Communication-Efficient Multi-UAV Exploration Via Voronoi Partition on Dynamic Topological Graph"
      },
      {
        "paperId": "4807c089a9e8fdc760869bfa69576617eae52aeb",
        "title": "HMS-RRT: A novel hybrid multi-strategy rapidly-exploring random tree algorithm for multi-robot collaborative exploration in unknown environments"
      },
      {
        "paperId": "165de31bfca391faccb0eaf5a9c49a7005bf0c92",
        "title": "Combining spatial clustering and tour planning for efficient full area exploration"
      },
      {
        "paperId": "d04925c196e3d30b0744213f55745c3670401acf",
        "title": "Exploring Unknown Environments Via Distributed Multi-Robot Systems"
      },
      {
        "paperId": "c0174a54ce489a200426d52061a87556cb90d93a",
        "title": "Two-Stage Control Strategy Based on Motion Planning for Planar Prismatic\u2013Rotational Underactuated Robot"
      },
      {
        "paperId": "ac0504673d9434cf8d7f534b44392f369ad72d5c",
        "title": "Assessment of Published Papers on the Use of Machine Learning in Diagnosis and Treatment of Mastitis"
      },
      {
        "paperId": "e1272747d53f1cf0b00c14c4ad1faf4bf776f9e6",
        "title": "A Novel Frontier-Based Multi-Robot Cooperative Exploration Method"
      },
      {
        "paperId": "46ebbdb20595dca8dfaba7507afcc820030b62a5",
        "title": "Bayesian reinforcement learning for navigation planning in unknown environments"
      },
      {
        "paperId": "3262567b9b7f7ffcbccefd5e28d65ac3b3ddb134",
        "title": "A review of coordinated multi-robot exploration"
      },
      {
        "paperId": "5ecd8eb28e241cdf4c1c5c83721949e8b36f9b1a",
        "title": "Sim-to-Real Transfer of Deep Reinforcement Learning Agents for Online Coverage Path Planning"
      },
      {
        "paperId": "ec3c3e7c237dbec54ba22a06254904dcb66cdb63",
        "title": "Homotopic Path Set Planning for Robot Manipulation and Navigation"
      },
      {
        "paperId": "8d564a2f27323d1ff6457e7cb527b04be24fee29",
        "title": "Path Planning for Autonomous Mobile Robot Using Intelligent Algorithms"
      },
      {
        "paperId": "8ffaed7ab7f0d234b1573524592701a975a23cb2",
        "title": "Multi-Robot Environmental Coverage With a Two-Stage Coordination Strategy via Deep Reinforcement Learning"
      },
      {
        "paperId": "b42dcb7c86c743f35eb8bbf11b8d94addcb421a8",
        "title": "A Multiline Customized Bus Planning Method Based on Reinforcement Learning and Spatiotemporal Clustering Algorithm"
      },
      {
        "paperId": "d7e23f694cd86f8e256ac47b9e81040227e5c254",
        "title": "Reinforcement Learning-based Data-driven Control Design for Motion Control Systems"
      },
      {
        "paperId": "d2d2a2ea40f91de6431016736bb327e4070b630d",
        "title": "Cooperative Coverage Path Planning Using Q-Learning and Sarsa in Two Environments"
      },
      {
        "paperId": "e4c3ecbae9ae84b682950eeca5fc3bb8834c0140",
        "title": "Goal Driven Multi-Robot Navigation in Simulated Environments with Federated Deep Reinforcement Learning"
      },
      {
        "paperId": "a58e41d9e08391923b45ebc35c968743b0019b6f",
        "title": "Towards Robust and Accurate Cooperative State Estimation for Multiple Rigid Bodies"
      },
      {
        "paperId": "168391ad4e2c698e97b7b8bcb566435d07b3db92",
        "title": "Explainable AI-driven IoMT fusion: Unravelling techniques, opportunities, and challenges with Explainable AI in healthcare"
      },
      {
        "paperId": "3e8fc7425f17bcebe673d1a1ba3119c752d3eeae",
        "title": "A double\u2010layer crowd evacuation simulation method based on deep reinforcement learning"
      },
      {
        "paperId": "d244e82dff9446dd370ef13569b4b75e74ad352e",
        "title": "Riemannian Optimization for Active Mapping With Robot Teams"
      },
      {
        "paperId": "6257fcfd5cf2eacbae1bb8b97de57160ae864a12",
        "title": "Asymmetric Information Enhanced Mapping Framework for Multirobot Exploration based on Deep Reinforcement Learning"
      },
      {
        "paperId": "978b15c564d4d7e87137aae1e8e80f33caaa6a52",
        "title": "Uncrewed Vehicles in 6G Networks: A Unifying Treatment of Problems, Formulations, and Tools"
      },
      {
        "paperId": "202c5d2bad87c9e254c2e1ec7d9d7a737bf54008",
        "title": "Multi-Robot Exploration Employing Harmonic Map Transformations"
      },
      {
        "paperId": "b2a40f28541c584ce664e1ed0454f5af1c785cd2",
        "title": "Coordinated Multi-Robot Navigation with Formation Adaptation"
      },
      {
        "paperId": "ba057715753addb65186d023ee4d845b312659c7",
        "title": "L-NORM: Learning and Network Orchestration at the Edge for Robot Connectivity and Mobility in Factory Floor Environments"
      },
      {
        "paperId": "5a5ecc70c9117175b88ba0e697458b6c2c7bb3c0",
        "title": "Deep reinforcement learning algorithm based on multi-agent parallelism and its application in game environment"
      },
      {
        "paperId": "a6cb758741ea2e1b542c0577057a26b2f5c7da92",
        "title": "Trajectory Planning of Robotic Manipulator in Dynamic Environment Exploiting DRL"
      },
      {
        "paperId": "992f64ca7ea7b0dcc17abf3b27e9f576984953af",
        "title": "Damage Classification of a Three-Story Aluminum Building Model by Convolutional Neural Networks and the Effect of Scarce Accelerometers"
      },
      {
        "paperId": "9b9615790ae08a6f2992ba9fde2f0f37378b7ff1",
        "title": "A survey on autonomous environmental monitoring approaches: towards unifying active sensing and reinforcement learning"
      },
      {
        "paperId": "d317c765dcd75cb61becc711ac0e6b55bc8fe73c",
        "title": "Learning-Based Cloud Server Configuration for Energy Minimization Under Reliability Constraint"
      },
      {
        "paperId": "309e9279272aab01e7f92ae4b9c58fbe3a0df577",
        "title": "MFRLMO: Model-free reinforcement learning for multi-objective optimization of apache spark"
      },
      {
        "paperId": "c4df47a747473b5df802ff3a49d2005eb1d49411",
        "title": "Safe Distributed Control of Multi-Robot Systems With Communication Delays"
      },
      {
        "paperId": "52b77de5698c40819f87450622268b4d90c438ca",
        "title": "Integrating ISPH simulations with machine learning for thermal radiation and exothermic chemical reaction on heat and mass transfer in spline/triangle star annulus"
      },
      {
        "paperId": "540ec7e6679cfe1bb16f0c08194abcdccdb503b0",
        "title": "Swarm flocking using optimisation for a self-organised collective motion"
      },
      {
        "paperId": "9d9a4b030eac00b702e27355804e6b2ba3d03777",
        "title": "Current study on multi-robot collaborative vision SLAM"
      },
      {
        "paperId": "e1637115abaed3e7ec109a93ae203dad2da24b1d",
        "title": "Learn Once Plan Arbitrarily (LOPA): Attention-Enhanced Deep Reinforcement Learning Method for Global Path Planning"
      },
      {
        "paperId": "4b81bec262ddeb519f2ca59acadb58bc7e52ad89",
        "title": "An integrated Design Methodology for Swarm Robotics using Model-Based Systems Engineering and Robot Operating System"
      },
      {
        "paperId": "8c577c151d8c031c99af7ed9bf9d0d925eddbe8a",
        "title": "Performance Analysis of Cascade Tank System Using Deep Learning Controller"
      },
      {
        "paperId": "240c0a790c304e80617e969446df51bcefa688bf",
        "title": "MASP: Scalable GNN-based Planning for Multi-Agent Navigation"
      },
      {
        "paperId": "57c5b8c5ecaf096520110a3878f2a525943733fb",
        "title": "Design of a Low-Cost Wireless Charging Station Based on the Robotic Vision System"
      },
      {
        "paperId": "f1af0fb7dce06284b9fe04394d4ceac9a61161f0",
        "title": "Confidence-Guided Path Planning for Mobile Sensors"
      },
      {
        "paperId": "bd804892607f4ef656aeb03627fd7cb3dc41c03f",
        "title": "Reinforcement Learning-Based Wind Farm Control: Toward Large Farm Applications via Automatic Grouping and Transfer Learning"
      },
      {
        "paperId": "1749fa7ddc840dd174a23f0c4efd6c6eaf412d80",
        "title": "Coordinating Autonomous Vehicles in a Contested Environment"
      },
      {
        "paperId": "393797551f0499860ba72c56346383374853c3b0",
        "title": "Multi-Robot Autonomous Exploration in Unknown Environment: A Review"
      },
      {
        "paperId": "c36cefaa3c76012fe496bc240788aadf3aa7a22f",
        "title": "DeepFake detection against adversarial examples based on D-VAEGAN"
      },
      {
        "paperId": "d58a17e422497ab67f63f492d46b2a7c3870c818",
        "title": "Enhancing Autonomous Operations in Smart Objects and Devices through the Internet of Robotic Things"
      },
      {
        "paperId": "5d3bf101809b7782f6b89649f8f14c60a84be0c7",
        "title": "Active Neural Topological Mapping for Multi-Agent Exploration"
      },
      {
        "paperId": "58b18cf7409612e38c6576f30e52a94615e11a79",
        "title": "Robust Path-Following Control for Multiple Autonomous Vehicles Along an Implicit Elliptical Curve"
      },
      {
        "paperId": "41ef64629a91005068b2a6c9039d0132ff0fe809",
        "title": "Cooperative behavior of a heterogeneous robot team for planetary exploration using deep reinforcement learning"
      },
      {
        "paperId": "5490cde1501c4940056f96075aec876bc4dd456a",
        "title": "Advanced Power Converters and Learning in Diverse Robotic Innovation: A Review"
      },
      {
        "paperId": "96f7660cba68533bb3d6ce24aa7dc68b2f2f5616",
        "title": "Distributed Multirobot Path Planning Based on MRDWA-MADDPG"
      },
      {
        "paperId": "10b4faa7174eaad0027147bebe5e71b089642019",
        "title": "A comparative study of deep reinforcement learning based energy management strategy for hybrid electric vehicle"
      },
      {
        "paperId": "a619606f1793ce6a43f73dd842930e11b7c47006",
        "title": "Deep Q-Learning-Based Dynamic Management of a Robotic Cluster"
      },
      {
        "paperId": "b2f1a8085a87bf69ccb9da52368804eecffa5446",
        "title": "Unknown Building Exploration Simulator (UBES)"
      },
      {
        "paperId": "78ca39c315de9ad7e430b3f4be4a3a0e4b30d09c",
        "title": "Multipath Planning Acceleration Method With Double Deep R-Learning Based on a Genetic Algorithm"
      },
      {
        "paperId": "fae661684cc4acf222cbbdb2483c928d7e5a260b",
        "title": "Decentralised Multi-Robot Exploration Using Monte Carlo Tree Search"
      },
      {
        "paperId": "ba830e846c884e6c6ac4f3ea6c8786920e91ae18",
        "title": "Bio-inspired Decentralized Multi-robot Exploration in Unknown Environments"
      },
      {
        "paperId": "5618da22e1c0faa35b67e568f2568ae6b568faae",
        "title": "Off-policy Q-learning-based Output Feedback Fault-tolerant Tracking Control of Industrial Processes"
      },
      {
        "paperId": "41c613b2f17169e9c7ae7c80cfb98b981f7445a8",
        "title": "A Deep Reinforcement Learning Approach to Optimal Morphologies Generation in Reconfigurable Tiling Robots"
      },
      {
        "paperId": "c5933d3a9e3c816a14fd668b1093f47b07cd678d",
        "title": "Learning to Recharge: UAV Coverage Path Planning through Deep Reinforcement Learning"
      },
      {
        "paperId": "bc982dae2e76c4be25ec588e058ac93e45a720ba",
        "title": "Deep Reinforcement Learning for Smart Grid Operations: Algorithms, Applications, and Prospects"
      },
      {
        "paperId": "940af5fc9445cf896f6c5816756b37b8e06684a8",
        "title": "Obstacle Avoidance for Trackless Rubber-Tired Vehicle Based on Risk-Grid Particle Swarm Optimization in Confined Space of Deep Well"
      },
      {
        "paperId": "d5d5f1551c2a2e3ecdc793ac976b1647f0e87109",
        "title": "Analysis of Machine Learning Classification Approaches for Predicting Students\u2019 Programming Aptitude"
      },
      {
        "paperId": "af545ecc023c43d3c71c3f6c7a7ba6967fc05942",
        "title": "Learning team-based navigation: a review of deep reinforcement learning techniques for multi-agent pathfinding"
      },
      {
        "paperId": "e4698fedb1e5d131d79c2be9f77a395d4e359a18",
        "title": "Dynamic Trajectory Planning for Ships in Dense Environment Using Collision Grid with Deep\u00a0 Reinforcement Learning"
      },
      {
        "paperId": "a35d19bf652abe3a4c994e33c732e2f861600283",
        "title": "Multi-Robot Multi-Room Exploration With Geometric Cue Extraction and Circular Decomposition"
      },
      {
        "paperId": "84ae61cbffa8c8c2757f12b1cdc07862e60aad9b",
        "title": "Fast and secure distributed multi-agent coverage control with an application to infrastructure inspection and reconstruction"
      },
      {
        "paperId": "422f4c62fe447cdea64b25f7b35290da3a304960",
        "title": "A SwarmBased Flocking Control Algorithm for Exploration and Coverage of Unknown Environments"
      },
      {
        "paperId": "f7cf0b33a6d85437576ab7f260ffbee745320bd3",
        "title": "Classification of drainage crossings on high-resolution digital elevation models: A deep learning approach"
      },
      {
        "paperId": "768ebe432274b5f721fadcdbcbff484e90f95301",
        "title": "Communication-Efficient Multi-Robot Exploration Using Coverage-Biased Distributed Q-Learning"
      },
      {
        "paperId": "ca934fb063dc16f89712de7bd1834e60f5f3299e",
        "title": "MUI-TARE: Cooperative Multi-Agent Exploration With Unknown Initial Position"
      },
      {
        "paperId": "b33890f9cd32e04eee7051e4cc49050995052a0d",
        "title": "Reinforcement Learning With Model-Based Assistance for Shape Control in Sendzimir Rolling Mills"
      },
      {
        "paperId": "330a0efe6863cc2d5e7431d1656bd48c0b04dc38",
        "title": "Learning Coverage Paths in Unknown Environments with Deep Reinforcement Learning"
      },
      {
        "paperId": "1d9cf988c47c9293637adf9abc0d40007682164c",
        "title": "On the Applicability of Reinforced Learning in the Route Selection Task of an Unmanned Vehicle"
      },
      {
        "paperId": "c0061f2124428f21848098ebb631ff9d5dda0a1e",
        "title": "SEAL: Simultaneous Exploration and Localization for Multi-Robot Systems"
      },
      {
        "paperId": "3b72cb8c5e1cdff75fb04b44b8615c2f671266b9",
        "title": "Research progress on deep learning in magnetic resonance imaging\u2013based diagnosis and treatment of prostate cancer: a review on the current status and perspectives"
      },
      {
        "paperId": "46bb0be51dbd7de785484e45964167cd7865a0fb",
        "title": "Modeling Crossing Behaviors of E-Bikes at Intersection With Deep Maximum Entropy Inverse Reinforcement Learning Using Drone-Based Video Data"
      },
      {
        "paperId": "9a43401359e73bcfd49a9505b95eeedb83db7203",
        "title": "Finite-Time Predictive Consensus for Discrete-Time Heterogeneous Multi-Agent Systems Over Switching Digraphs"
      },
      {
        "paperId": "979428313afe747b0b996c8d63c73699a616b52c",
        "title": "A Novel Statistically-Aided Learning Framework for Precise Localization of UAVs"
      },
      {
        "paperId": "ad2c1f3c606cd55f6dfbf4742829031edba0404b",
        "title": "PreOBP_ML: Machine Learning Algorithms for Prediction of Optical Biosensor Parameters"
      },
      {
        "paperId": "c22473db445de5cc874f15b4e616192d770d17c2",
        "title": "Lighthouses and Global Graph Stabilization: Active SLAM for Low-compute, Narrow-FoV Robots"
      },
      {
        "paperId": "670fca18089f52f90bcfb6f96832baadeea7d83c",
        "title": "Decentralized Multi-UAV Cooperative Exploration Using Dynamic Centroid-Based Area Partition"
      },
      {
        "paperId": "bd4cb77a0cee2db31296531365114dd67fff15b5",
        "title": "Epistemic planning for multi-robot systems in communication-restricted environments"
      },
      {
        "paperId": "e81b97ff8367d331fbd6ac44be85c61f8a8d12cf",
        "title": "Application of deep neural network in cost estimation of hydropower projects"
      },
      {
        "paperId": "dc7d5574998f7e04d0cabf62b7bb2a980c43c869",
        "title": "Autonomous Collision Avoidance of Unmanned Surface Vehicles Based on Improved A-Star and Dynamic Window Approach Algorithms"
      },
      {
        "paperId": "9e11c850056a181b4ef7ea4268dea23df9cc7e30",
        "title": "Efficient Autonomous Exploration and Mapping in Unknown Environments"
      },
      {
        "paperId": "e3c8d41b630c7f659ffa5ed5cd775697ae28afff",
        "title": "Deep-Reinforcement-Learning-Based Object Transportation Using Task Space Decomposition"
      },
      {
        "paperId": "224ea5f97074e88a6bdd76c9c5e52ff96c0f0469",
        "title": "Vehicle Trajectory Prediction via Urban Network Modeling"
      },
      {
        "paperId": "5dd28612deeb24cabc80f30b5819b277a2218f7e",
        "title": "On the role and opportunities in teamwork design for advanced multi-robot search systems"
      },
      {
        "paperId": "b8de7a5c724834ecec9cdd55c00dd6edced29373",
        "title": "MLOps Spanning Whole Machine Learning Life Cycle: A Survey"
      },
      {
        "paperId": "396b2edb73c02e8536c7ea5b916b0422a2f1fc0d",
        "title": "A Metaverse-Based Teaching Building Evacuation Training System With Deep Reinforcement Learning"
      },
      {
        "paperId": "7f58a8d7e87b191052b4936c2f55e8d2299feded",
        "title": "FedDSR: Daily Schedule Recommendation in a Federated Deep Reinforcement Learning Framework"
      },
      {
        "paperId": "92c590df020b448c06c8eacb17cbf9c869dd88d8",
        "title": "Multi-Agent Deep Reinforcement Learning for Multi-Robot Applications: A Survey"
      },
      {
        "paperId": "91b270f3bc5b9de4402ac58b231b2292baf49e5c",
        "title": "Comprehensive machine and deep learning analysis of sensor-based human activity recognition"
      },
      {
        "paperId": "09c626053fd10ef95ba363205987d960911e9279",
        "title": "Graph-based Simultaneous Coverage and Exploration Planning for Fast Multi-robot Search"
      },
      {
        "paperId": "4e186f7162d702044bf864b792b5e893bfce59bc",
        "title": "A heuristic maintenance scheduling framework for a military aircraft fleet under limited maintenance capacities"
      },
      {
        "paperId": "983351176a9fe577e27756022b9483e0ecf987e8",
        "title": "Adaptive Clustering Quasi-Line Search Path Planning Algorithm Based On Sampling"
      },
      {
        "paperId": "2f3c3638a065d22525781bc9365aaaca07353e55",
        "title": "Asynchronous Multi-Agent Reinforcement Learning for Efficient Real-Time Multi-Robot Cooperative Exploration"
      },
      {
        "paperId": "b09ad4366022e1c51dc5ee881d0b1021edb49a30",
        "title": "Quantum Multiagent Actor\u2013Critic Neural Networks for Internet-Connected Multirobot Coordination in Smart Factory Management"
      },
      {
        "paperId": "04aaf2a2ab1b38f9a98cc8f19211bd345c59d476",
        "title": "Machine Learning for Emergency Management: A Survey and Future Outlook"
      },
      {
        "paperId": "2d5c46fe6d075a4fca0060c2f07de074d627760a",
        "title": "DTTrans: PV Power Forecasting Using Delaunay Triangulation and TransGRU"
      },
      {
        "paperId": "198d1c732ee9b1f97838da5b101945e69a9048de",
        "title": "Prolonging Robot Lifespan Using Fatigue Balancing with Reinforcement Learning"
      },
      {
        "paperId": "860585b7006d8e236d783c72ddc85cda09add922",
        "title": "1D Convolutional Neural Network to Detect Ventricular Fibrillation"
      },
      {
        "paperId": "f1408101eba8ba1ab9ade221d5e0493256e6105b",
        "title": "A Benchmark of Planning-based Exploration Methods in Photo-Realistic 3D Simulator"
      },
      {
        "paperId": "272a834959a5f6e3f719c3d6eb38bc369e90b497",
        "title": "LDL: A Defense for Label-Based Membership Inference Attacks"
      },
      {
        "paperId": "6db0f79053b53bcfd76d2dde263f9c976639af80",
        "title": "Simultaneous localization and formation using angle-only measurements in 2D"
      },
      {
        "paperId": "24e4596103e2468d91402de5bd9dc033238a7a05",
        "title": "Multi-Robot Exploration in Unknown Environments via Multi-Agent Deep Reinforcement Learning"
      },
      {
        "paperId": "31d1c41424214067c649e75ade984cbe27918a68",
        "title": "An In-Depth Analysis of Cooperative Multi-Robot Hierarchical Reinforcement Learning"
      },
      {
        "paperId": "5cc260203d562c2cbd3338a4892881764d1905e6",
        "title": "Approche locale pour l'exploration autonome d'environnements inconnus par une flottille de robots"
      },
      {
        "paperId": "b5b2fcc2902fda61346c3005668120058e563b59",
        "title": "Towards Coordinated Multi-Robot Exploration under Bandwidth-constrained Conditions"
      },
      {
        "paperId": "9382f16a9ae72a5bcf5b14d349e62a1a52f2cc7d",
        "title": "Scene Recognition for Urban Search and Rescue using Global Description and Semi-Supervised Labelling"
      },
      {
        "paperId": "58de4b7b33b6af242ef937e632bd9c87826e2d00",
        "title": "Learning Task-Oriented Channel Allocation for Multi-Agent Communication"
      },
      {
        "paperId": "4fd22ce265c899d20ebb4df1b2a2140c2c62891a",
        "title": "Sensing and Detection of Traffic Signs Using CNNs: An Assessment on Their Performance"
      },
      {
        "paperId": "e94556220514426a2577b3826646d985592d918c",
        "title": "Fixed-Time Coverage Control of Mobile Robot Networks Considering the Time Cost Metric"
      },
      {
        "paperId": "fd1bb0fb743fe491482f71470bd86c48fbb7908a",
        "title": "ReSemble: Reinforced Ensemble Framework for Data Prefetching"
      },
      {
        "paperId": "5d058b35c1b75e80845b8c8f88d5457c1d65c8eb",
        "title": "Federated Learning and Meta Learning: Approaches, Applications, and Directions"
      },
      {
        "paperId": "42e80df13f963d3d84bb9a235bb45e00ef87e60d",
        "title": "Federated and Meta learning over Non-Wireless and Wireless Networks: A Tutorial"
      },
      {
        "paperId": "5fc6f29fe6bfe763b2d288ac38d44de48dc48bf3",
        "title": "Asynchronous Collaborative Autoscanning with Mode Switching for Multi-Robot Scene Reconstruction"
      },
      {
        "paperId": "479a06fd6815420abdecf2ddb1cf15cb5c2109ea",
        "title": "Generalization in Deep Reinforcement Learning for Robotic Navigation by Reward Shaping"
      },
      {
        "paperId": "e9ddf2f76a4c3dacd04e7265cab6700f8aa48a6f",
        "title": "Resilient Consensus via Voronoi Communication Graphs"
      },
      {
        "paperId": "c9dd76529bd356a212c19a402d886738c9bd1d7f",
        "title": "A Novel Knowledge-Based Genetic Algorithm for Robot Path Planning in Complex Environments"
      },
      {
        "paperId": "2f5868e6786825396a97a1dff741f96129849e80",
        "title": "Controlling Fleets of Autonomous Mobile Robots with Reinforcement Learning: A Brief Survey"
      },
      {
        "paperId": "6b58db0bc8e3db70460226609ca1235920ae6bd3",
        "title": "LEADs: A Swarm Behavioral Decision-making System for Different Tasks"
      },
      {
        "paperId": "58d065bf0910c8e7d58825889d4a68c95fc0b641",
        "title": "Application of Massive Parallel Computation Based Q-Learning in System Control"
      },
      {
        "paperId": "93904b2cabb61a339bc92f9c34b2405e44019c75",
        "title": "Autonomous Multiple Robots Exploration using Evolutionary Optimization Algorithm"
      },
      {
        "paperId": "1474e7b7055e68ec10facca37ee78da6ea55d606",
        "title": "Distributed Neural Networks Training for Robotic Manipulation With Consensus Algorithm"
      },
      {
        "paperId": "49cc565483139ac43b7fc64d6dc370eacb816423",
        "title": "A Survey on Active Simultaneous Localization and Mapping: State of the Art and New Frontiers"
      },
      {
        "paperId": "604c55787bd3f988b676d02d32063581ae031589",
        "title": "Efficient Privacy-Preserving Federated Learning With Unreliable Users"
      },
      {
        "paperId": "b5498c96af1373438ed562aae2ffc951b5dc2cdb",
        "title": "Fully Adaptive Framework: Neural Computerized Adaptive Testing for Online Education"
      },
      {
        "paperId": "d3414f399ab2d38e0a35534a37792e1d82bee4be",
        "title": "A Risk Based Approach for Privacy Compliant Machine Learning Lifecycle"
      },
      {
        "paperId": "c041ed530e64377b42e28713879b9b4430f814f1",
        "title": "An Applicable Machine Learning Model Based on Preoperative Examinations Predicts Histology, Stage, and Grade for Endometrial Cancer"
      },
      {
        "paperId": "9af28bc346e1f6aaac8f35e6c51f1ae740002d9d",
        "title": "Clinical analysis and artificial intelligence survival prediction of serous ovarian cancer based on preoperative circulating leukocytes"
      },
      {
        "paperId": "b3e67b2e955949c23ec81662701e7a0a676f3317",
        "title": "A Fully Controllable Agent in the Path Planning using Goal-Conditioned Reinforcement Learning"
      },
      {
        "paperId": "9c5f056c4e7986064722bb522e46e3546be8da51",
        "title": "A Review of Safe Reinforcement Learning: Methods, Theory and Applications"
      },
      {
        "paperId": "fe351c053f9155f40cac7c85fe803af9249edabf",
        "title": "A Survey of Personalized and Incentive Mechanisms for Federated Learning"
      },
      {
        "paperId": "656ecbbe447e0c40252c793e5d7da2f5f7fc86c6",
        "title": "The Design of Sports Games under the Internet of Things Fitness by Deep Reinforcement Learning"
      },
      {
        "paperId": "46e6ab62d7dd90b7c14281e4b5590731c71c6b0a",
        "title": "Collaborative Target Search With a Visual Drone Swarm: An Adaptive Curriculum Embedded Multistage Reinforcement Learning Approach"
      },
      {
        "paperId": "d3ad8e336a29358733e999d6d0a070fd68f48e24",
        "title": "GraphWare: A graph\u2010based middleware enabling multi\u2010robot cooperation"
      },
      {
        "paperId": "69b253d572a77cb5de1048f73b994732310ddd10",
        "title": "A survey on vision guided robotic systems with intelligent control strategies for autonomous tasks"
      },
      {
        "paperId": "a054cefc710d140316496b3d0aaca503eaea6d88",
        "title": "Will artificial intelligence assume a role in anatomy education?"
      },
      {
        "paperId": "b0d7069f662002df0c0b72cb460717db39f24a60",
        "title": "Learning to act: a Reinforcement Learning approach to recommend the best next activities"
      },
      {
        "paperId": "b01375828e4703424427be1800a7bc4ab618c32a",
        "title": "Collaborative multi-agents in dynamic industrial internet of things using deep reinforcement learning"
      },
      {
        "paperId": "d3efd343cb1fb232ed2e78b4c8c2a3d65426404a",
        "title": "Towards accurate and reliable resolution of structural variants for clinical diagnosis"
      },
      {
        "paperId": "c65d20a5a86c96e5cce97684a8d0a79dcb71ccd7",
        "title": "Author Correction: Towards accurate and reliable resolution of structural variants for clinical diagnosis"
      },
      {
        "paperId": "525b5a871a4a952c90cd578ee5972b3c3b3b7be7",
        "title": "Dynamic multi-constrained assembly sequence planning of large space structures considering structural vibration"
      },
      {
        "paperId": "839bf43e96401a45ae4ab50adca79a909fae2f2f",
        "title": "Fault-tolerant cooperative navigation of networked UAV swarms for forest fire monitoring"
      },
      {
        "paperId": "8e93c526b67c98f4dfb52ffbfed97878b047a1dc",
        "title": "A Soar-Based Space Exploration Algorithm for Mobile Robots"
      },
      {
        "paperId": "10d1bf978fa78ad8e1b40a8658db9292b930eb5c",
        "title": "Review of multiple unmanned surface vessels collaborative search and hunting based on swarm intelligence"
      },
      {
        "paperId": "4c7dd6a4d7d5a24b6236ff8f6ba93cde1dbc7922",
        "title": "Bio-Inspired Collision Avoidance in Swarm Systems via Deep Reinforcement Learning"
      },
      {
        "paperId": "280f0b87bedfe9577630d5551e6acb2a33d8958a",
        "title": "Autonomous Drone Swarm Navigation and Multitarget Tracking With Island Policy-Based Optimization Framework"
      },
      {
        "paperId": "d26f1916d8da0016c52d2d6aa250850db2153f5b",
        "title": "Federated Reinforcement Learning for Collective Navigation of Robotic Swarms"
      },
      {
        "paperId": "d17d8c11fb22d498df8542f40569d00bab920dfd",
        "title": "Intelligent robot motion trajectory planning based on machine vision"
      },
      {
        "paperId": "99ae4e176f0856c99e8e39fbb90c610f11a56020",
        "title": "Machine Learning Techniques for Increasing Efficiency of the Robot\u2019s Sensor and Control Information Processing"
      },
      {
        "paperId": "48d32cb454755f292d2f7d11e42c0ea934bc4b9e",
        "title": "Robot-assisted movement training system based on PID control"
      },
      {
        "paperId": "a820b48248909a2ec1abe4d8c230f4afdb98c380",
        "title": "Robot vision system based on information visualization algorithm"
      },
      {
        "paperId": "e39d2aac5823b243661ac9842929ec0b7a1c1fcf",
        "title": "Robot motion planning based on information graphic design"
      },
      {
        "paperId": "988ef31ee262e4e979955362554c67e589101a0d",
        "title": "Medical Image Denoising Techniques: A Review"
      },
      {
        "paperId": "64d1a0abe6436bb203381c13abef4f8577386aa5",
        "title": "Machine learning concept in de-spiking process for nuclear resonant vibrational spectra - automation using no external parameter."
      },
      {
        "paperId": "8793470632bc367c0fcd866d5b0a0cf1c4550704",
        "title": "Towards the Achievement of Path Planning with Multi-robot Systems in Dynamic Environments"
      },
      {
        "paperId": "ed6a0bb6d435ccd5df778904b6fca9b3771a630b",
        "title": "Machine Learning and Urban Drainage Systems: State-of-the-Art Review"
      },
      {
        "paperId": "bbbd9985431af85192f561b2a5c5e5b5c8ddc775",
        "title": "Monitoring and Adapting the Physical State of a Camera for Autonomous Vehicles"
      },
      {
        "paperId": "cc948778d33ce5d453317e2d6f24692add1eb7f7",
        "title": "Semantic OcTree Mapping and Shannon Mutual Information Computation for Robot Exploration"
      },
      {
        "paperId": "22a2a756888e1114f66c80241d013a377c04ba58",
        "title": "A Reinforcement Learning Based Dirt-Exploration for Cleaning-Auditing Robot"
      },
      {
        "paperId": "a23707d62e7298040920a27132b4a3eb058ac552",
        "title": "Target location detection of mobile robots based on R-FCN deep convolutional neural network"
      },
      {
        "paperId": "fe609b4a2ab1ee99c50602f7ac1a2df8496356b5",
        "title": "Reinforcement Learning for Mobile Robotics Exploration: A Survey"
      },
      {
        "paperId": "b6e57fb46f0efe522cde5a986a1036a36f099a2d",
        "title": "Machine Learning in the analysis of lethality and evolution of infection by the SARS-CoV-2 virus (COVID-19) in workers of the Mexico City Metro"
      },
      {
        "paperId": "813f6e34feb3dc0346b6392d061af12ff186ba7e",
        "title": "Learning Efficient Multi-Agent Cooperative Visual Exploration"
      },
      {
        "paperId": "ac89d4156c66f792cacbd29600d4cf0cfead71f3",
        "title": "Deep Reinforcement Learning Versus Evolution Strategies: A Comparative Survey"
      },
      {
        "paperId": "a66f247271239005adac1419c948ef5d41032574",
        "title": "Meta Reinforcement Learning Based Sensor Scanning in 3D Uncertain Environments for Heterogeneous Multi-Robot Systems"
      },
      {
        "paperId": "c83f48fbc96f5f5cfb1f7ea779d65f049e6eeb4e",
        "title": "Dependability Analysis of Deep Reinforcement Learning based Robotics and Autonomous Systems through Probabilistic Model Checking"
      },
      {
        "paperId": "eb5b1f2511da4492e5e08dcadd428bc916dc13f7",
        "title": "Industry 4.0 with intelligent manufacturing 5G mobile robot based on genetic algorithm"
      },
      {
        "paperId": "ca284589378ecd4b74b5d5468dc134b48d5495d2",
        "title": "Safe Deep Reinforcement Learning for Multi-Agent Systems with Continuous Action Spaces"
      },
      {
        "paperId": "4aad4792dff146f4cc13940baf8c35120ef02301",
        "title": "MACHINE LEARNING: AN OVERVIEW"
      },
      {
        "paperId": "e74b3d2dcede69bb5e35826a7343356a35e4122e",
        "title": "Group Coordinated Control of Networked Mobile Robots With Applications to Object Transportation"
      },
      {
        "paperId": "620d95759aa59c04f2c49c59929d0e3eee3a4313",
        "title": "From Networked Robotics to Cloud and Big Data Supercharged Robotics: A Survey and Analysis"
      },
      {
        "paperId": "f868caa11cf9b9c941fa236e2c2b77fb58f32855",
        "title": "Self-Organised Collision-Free Flocking Mechanism in Heterogeneous Robot Swarms"
      },
      {
        "paperId": "3b608c04bf1949ab696f16cfe91776e02e1bb938",
        "title": "Decentralized Trajectory Optimization for Multi-Agent Ergodic Exploration"
      },
      {
        "paperId": "909ad00cefef06f2111a7a237657189d7247ea8f",
        "title": "A Deep Reinforcement Learning Algorithm Suitable for Autonomous Vehicles: Double Bootstrapped Soft-Actor\u2013Critic-Discrete"
      },
      {
        "paperId": "b4227b659bbe598e72a371818ef74bea4984cb46",
        "title": "Multi-target Coverage with Connectivity Maintenance using Knowledge-incorporated Policy Framework"
      },
      {
        "paperId": "40a0d55b6de6749bf2ec3313d8ed6ca5c0ddd7c6",
        "title": "Deep Reinforcement Learning Versus Evolution Strategies: A Comparative Survey"
      },
      {
        "paperId": "a5a9ee20611f367f383dbd1f2968a4884cf1cf72",
        "title": "A Review of Deep Learning-Based Contactless Heart Rate Measurement Methods"
      },
      {
        "paperId": "c60629a2a5e85f422b6b74444dd5a3cdfe1518f3",
        "title": "Q-Learning traffic-distributing approach to managing multiple-destination traffic"
      },
      {
        "paperId": "362e7339bc419feab6170dd6fecd168771d8e0b0",
        "title": "A Decentralized Cluster Formation Containment Framework for Multirobot Systems"
      },
      {
        "paperId": "4a0fe8e7afb9e79556bed20c9616db44bfd9da03",
        "title": "Multi-roller Trajectory Planning under the Dynamic Obstacle in Pavement Construction"
      },
      {
        "paperId": "bcffc6ec91c25af4a195e19f7f426cbf27cdff5a",
        "title": "Robust Formation Coordination of Robot Swarms With Nonlinear Dynamics and Unknown Disturbances: Design and Experiments"
      },
      {
        "paperId": "1143f6edefc75257c34469aa37f73118c474d0fd",
        "title": "SDP-Based Robust Formation-Containment Coordination of Swarm Robotic Systems with Input Saturation"
      },
      {
        "paperId": "98d45634f718a090698084d736b8e2092a5544e3",
        "title": "Universal Artificial Pheromone Framework with Deep Reinforcement Learning for Robotic Systems"
      },
      {
        "paperId": "718ddfd0032cd8ad4b439dcea8886e4858935f84",
        "title": "\u201cMulti-Agent\u201d Screening Improves the Efficiency of Directed Enzyme Evolution"
      },
      {
        "paperId": "5d25aaf23057ec209bbe20ab2ddb1a4d8d7c7032",
        "title": "Intelligent Exploration Approaches Based on Utility Functions Optimization for Multi-Agent Environment Applications"
      },
      {
        "paperId": "78bcd86b005f6163ce720909274c02373ae2d3f7",
        "title": "Communication Efficient Parallel Reinforcement Learning"
      },
      {
        "paperId": "f5ddea84891b50f9daffd7bc4f6e0354c936a3ed",
        "title": "Self-Organised Swarm Flocking with Deep Reinforcement Learning"
      },
      {
        "paperId": "ae9ca6546bd43802e8ae3905076fbfd832834f34",
        "title": "Cross-Modal Contrastive Learning of Representations for Navigation Using Lightweight, Low-Cost Millimeter Wave Radar for Adverse Environmental Conditions"
      },
      {
        "paperId": "e17c3251aba246112395b44901d51e2c2645cc2b",
        "title": "Looking Good by Doing Good: CEO Attractiveness and Corporate Philanthropy"
      },
      {
        "paperId": "d03de978b6b843bf20418afe20ee8a3f089b7ba7",
        "title": "Development of Tributary Mapping System Using Multiple Unmanned Aerial Vehicles With LiDAR"
      },
      {
        "paperId": "d297a8d5dd1db2e0a50d48073ddddeac582c62a3",
        "title": "Multi-Robot Collaborative Exploration on Communication-Constrained Environments"
      },
      {
        "paperId": "af1ba3918a828dd0aeff46536d41649f7e10b7ef",
        "title": "AAGE: Air-Assisted Ground Robotic Autonomous Exploration in Large-Scale Unknown Environments"
      },
      {
        "paperId": "ee0a26f22e5f41ab42e2468b1937293129a1a6f8",
        "title": "Efficient Multi-Agent Exploration in Area Coverage Under Spatial and Resource Constraints"
      },
      {
        "paperId": "8fdd0f751f3b0ca79d8c39df42d7783cdee9a793",
        "title": "Omniveyor: An Assembled Logistics Sorting System Powered by Reinforcement Learning"
      },
      {
        "paperId": "baf67d0c873c51d9c46b777463b9d1c14d2d226a",
        "title": "Reinforcement Learning-Based Dynamic Coverage Control of Multi-Rotor UAVs With Safety Priority"
      },
      {
        "paperId": "4edbb8965ef17a9342c6285d6a14351d42d96b5b",
        "title": "Motion Coordination of Swarm Robots for Mobile Target Search"
      },
      {
        "paperId": "0927139e20bef8207edbd1a686a8f87e36973251",
        "title": "Adaptive Centroidal Voronoi Tessellation With Agent Dropout and Reinsertion for Multi-Agent Non-Convex Area Coverage"
      },
      {
        "paperId": "996dbd07c0a4ee74ccd47cc8b0824343ac7df9c2",
        "title": "Sweeping-Based Multi-Robot Exploration in an Unknown Environment Using Webots"
      },
      {
        "paperId": "a4dcc57ecd1f943a1150d375b5050951642a5f73",
        "title": "Frontier Shepherding: A Bio-Mimetic Multi-robot Framework for Large-Scale Exploration"
      },
      {
        "paperId": "4cb93d2b87d32245915e556db174a0cca04c96c6",
        "title": "Optimized Mission Planning for Heterogeneous Uncrewed Vehicle Teams"
      },
      {
        "paperId": "0a784ab730cbe85224842811a0f7327631404c36",
        "title": "Intercommunication in Multi Robotic Cleaning System with Efficient Route Planning Through Map Decomposition"
      },
      {
        "paperId": "22b8f760584c65e3abb807e7e14524365577e112",
        "title": "A Controllable Agent by Subgoals in Path Planning Using Goal-Conditioned Reinforcement Learning"
      },
      {
        "paperId": "6ff75074eb888ddfa61afa26f164536211dcc5b8",
        "title": "A MDPs-Based Dynamic Path Planning in Unknown Environments for Hopping Locomotion"
      },
      {
        "paperId": "beb0d80fbb0de59e0a521f69205a2099ce1a6487",
        "title": "Distributed Adaptive Formation Control for Uncertain Point Mass Agents With Mixed Dimensional Space"
      },
      {
        "paperId": "17540bf6288093812037a32bd8d5629e37b47969",
        "title": "Multi-Robot Multi-Room Exploration with Geometric Cue Extraction and Spherical Decomposition"
      },
      {
        "paperId": "d4188960e61bef670a76d8bb09c2d4e5fae10bab",
        "title": "DEGREE PROJECT Parliament proceeding classification via Machine Learning algorithms: A case of Greek parliament proceedings"
      },
      {
        "paperId": "1f3ae673bb7f58c0d32fd2bdbdc3bf8d26c9dbf9",
        "title": "A Survey on Localization for Autonomous Vehicles"
      },
      {
        "paperId": "c6294f307d818dab1d35f2096087bc12bc4fef28",
        "title": "MASP: Scalable GNN-based Planning for Multi-Agent Navigation"
      },
      {
        "paperId": "4322a403be991242d1ffb12e55fe314ea1032b12",
        "title": "On the Generalization of Deep Reinforcement Learning Methods in the Problem of Local Navigation"
      },
      {
        "paperId": "dc3c40efdca00742f4528629023ee58b4c3f2f19",
        "title": "Analysis and processing of infant cry for diagnosis purposes"
      },
      {
        "paperId": "a9367a6e8ccc0759edba62b897b75177aff9e535",
        "title": "Improved Sampling Strategy for Representative Set Construction"
      },
      {
        "paperId": "c9e602b5f1df92388e0b6755e7c851f2ec8d7495",
        "title": "Cooperative Multi-Robot Hierarchical Reinforcement Learning"
      },
      {
        "paperId": null,
        "title": "Supplementary Materials of Learning Efficient Multi-Agent Cooperative Visual Exploration"
      },
      {
        "paperId": "e92372e1aa33601434d6cc373c00e7b8dd53661d",
        "title": "Preemptive Anomaly Prediction in IoT Components (short paper)"
      },
      {
        "paperId": "9b05d1c3de246d4e41c33fec5bbd8071f201a2b1",
        "title": "Collaborative Coverage for a Network of Vacuum Cleaner Robots"
      },
      {
        "paperId": "236cd506eb168f0cd8c61c05be312c9b04c6a5cb",
        "title": "Training and Development Programmes as Predictors of students\u2019 Academic Performance in Senior Secondary Schools in Osun State, Nigeria"
      },
      {
        "paperId": "da002c1059707a542772b14bb2b4558658fe53af",
        "title": "A Comprehensive Review of Coverage Path Planning in Robotics Using Classical and Heuristic Algorithms"
      },
      {
        "paperId": "3a1ee1d91095620feb01dc66aacf09cbeb351529",
        "title": "with a Swarm of Heterogeneous Surface Robots"
      }
    ],
    "score": 58.2
  },
  {
    "id": "2470fcf0f89082de874ac9133ccb3a8667dd89a8",
    "title": "Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models",
    "authors": [
      "Bradly C. Stadie",
      "S. Levine",
      "P. Abbeel"
    ],
    "year": 2015,
    "citationCount": 512,
    "abstract": "Achieving efficient and scalable exploration in complex domains poses a major challenge in reinforcement learning. While Bayesian and PAC-MDP approaches to the exploration problem offer strong formal guarantees, they are often impractical in higher dimensions due to their reliance on enumerating the state-action space. Hence, exploration in complex domains is often performed with simple epsilon-greedy methods. In this paper, we consider the challenging Atari games domain, which requires processing raw pixel inputs and delayed rewards. We evaluate several more sophisticated exploration strategies, including Thompson sampling and Boltzman exploration, and propose a new exploration method based on assigning exploration bonuses from a concurrently learned model of the system dynamics. By parameterizing our learned model with a neural network, we are able to develop a scalable and efficient approach to exploration bonuses that can be applied to tasks with complex, high-dimensional state spaces. In the Atari domain, our method provides the most consistent improvement across a range of games that pose a major challenge for prior methods. In addition to raw game-scores, we also develop an AUC-100 metric for the Atari Learning domain to evaluate the impact of exploration on this benchmark.",
    "url": "https://www.semanticscholar.org/paper/2470fcf0f89082de874ac9133ccb3a8667dd89a8",
    "pdf_url": "https://arxiv.org/pdf/1507.00814.pdf",
    "venue": "arXiv.org",
    "publicationDate": "2015-07-03",
    "externalIds": {
      "DBLP": "journals/corr/StadieLA15",
      "ArXiv": "1507.00814",
      "MAG": "779494576",
      "CorpusId": 10296902
    },
    "references": [
      {
        "paperId": "340f48901f72278f6bf78a04ee5b01df208cc508",
        "title": "Human-level control through deep reinforcement learning"
      },
      {
        "paperId": "b6cc21b30912bdaecd9f178d700a4c545b1d0838",
        "title": "Deep Learning for Real-Time Atari Game Play Using Offline Monte-Carlo Tree Search Planning"
      },
      {
        "paperId": "6bdb186ec4726e00a8051119636d4df3b94043b5",
        "title": "Caffe: Convolutional Architecture for Fast Feature Embedding"
      },
      {
        "paperId": "d9eeb1277a4cea35a2c1a147dfcd4b41b83ecacd",
        "title": "PAC Optimal Exploration in Continuous Space Markov Decision Processes"
      },
      {
        "paperId": "fe859f7e498ad093721c795b70d2e8380f980ce3",
        "title": "Exploration in Model-based Reinforcement Learning by Empirically Estimating Learning Progress"
      },
      {
        "paperId": "f82e4ff4f003581330338aaae71f60316e58dd26",
        "title": "The Arcade Learning Environment: An Evaluation Platform for General Agents"
      },
      {
        "paperId": "eec4783610ca8405fa2ff134aeaa410a4a3f14fd",
        "title": "Near-Optimal BRL using Optimistic Local Transitions"
      },
      {
        "paperId": "918e8781344e7eee60a4f6bf883c858205a917d0",
        "title": "Efficient Bayes-Adaptive Reinforcement Learning using Sample-Based Search"
      },
      {
        "paperId": "7ee8a69a13ed60f99d39111637abd2db948a22bb",
        "title": "Nonparametric Bayesian Policy Priors for Reinforcement Learning"
      },
      {
        "paperId": "1925db1d8b59fcec2293d8adbf8d92ff444a4c32",
        "title": "Variance-Based Rewards for Approximate Bayesian Reinforcement Learning"
      },
      {
        "paperId": "4282669947ca340de9f93a9a2a38007fe542195d",
        "title": "Near-Bayesian exploration in polynomial time"
      },
      {
        "paperId": "94524df841bb7aaa4cbe73ca2a31bac56146f9f3",
        "title": "Proceedings of the 26th Annual International Conference on Machine Learning"
      },
      {
        "paperId": "237a1cf18ed83bb3ad852b34f443c6c1ff3336c1",
        "title": "An analysis of model-based Interval Estimation for Markov Decision Processes"
      },
      {
        "paperId": "19bb0dce99466077e9bc5a2ad4941607fc28b40c",
        "title": "Manifold Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples"
      },
      {
        "paperId": "6bddcc4ff63e80fe576d379776284ede0be0a80c",
        "title": "Exploration in Metric State Spaces"
      },
      {
        "paperId": "e41ba5dc12c79a64dfa905c0328f95976252ffe0",
        "title": "The Elements of Statistical Learning"
      },
      {
        "paperId": "dc649486b881e672eea6546da48c46e1f98daf32",
        "title": "Near-Optimal Reinforcement Learning in Polynomial Time"
      },
      {
        "paperId": "a7cabf8bbf69510fa15595064fe21fe4b2e69b16",
        "title": "Practical Reinforcement Learning in Continuous Spaces"
      },
      {
        "paperId": "e3678df4a8c183fc50d59e6277284078785600e6",
        "title": "Efficient Reinforcement Learning in Factored MDPs"
      },
      {
        "paperId": "cfe859be0a5339e4f02a4b04c72b92f9ffb3ae90",
        "title": "Dropout as a Bayesian Approximation : Insights and Applications"
      },
      {
        "paperId": "a0c5d8025a9dc5621ce4d2ebe21ccdd0745760a8",
        "title": "Exploration in relational domains for model-based reinforcement learning"
      },
      {
        "paperId": "2a2d55d893a5b1073de061ecb3f5422fce19daee",
        "title": "Managing Uncertainty within Value Function Approximation in Reinforcement Learning"
      },
      {
        "paperId": "02552a8b40f3a82a5353f596264db71d899a9b4a",
        "title": "Supporting Online Material for Reducing the Dimensionality of Data with Neural Networks"
      },
      {
        "paperId": "31b46fdddbe4fc1265615a5c128063858f999d41",
        "title": "Optimal Arti\ufb01cial Curiosity, Creativity, Music, and the Fine Arts"
      },
      {
        "paperId": "c7dbf5ed7e9b63e104adb4e18bfbc98f5d6afdae",
        "title": "R-MAX - A General Polynomial Time Algorithm for Near-Optimal Reinforcement Learning"
      },
      {
        "paperId": "97efafdb4a3942ab3efba53ded7413199f79c054",
        "title": "Reinforcement Learning: An Introduction"
      },
      {
        "paperId": null,
        "title": "Exploration Strategies for Model-based Learning in Multi-agent Systems Autonomous Agents and Multi-Agent Systems"
      },
      {
        "paperId": null,
        "title": "Jordan , Pieter Abbeel"
      },
      {
        "paperId": null,
        "title": "Theano : A CPU and GPU Math Expression Compiler"
      },
      {
        "paperId": null,
        "title": "Trust Region Policy Optimization Arxiv preprint 1502"
      }
    ],
    "cited_by": [
      {
        "paperId": "543f87b15b888cb0a2381c5b36141256ad5ec84e",
        "title": "LLM-Guided Task- and Affordance-Level Exploration in Reinforcement Learning"
      },
      {
        "paperId": "8e4974efb1f3656c92ba5b717018347eb3263377",
        "title": "GeoExplorer: Active Geo-localization with Curiosity-Driven Exploration"
      },
      {
        "paperId": "a98587acde8e1df366d7c4964b5a08ed979aefb2",
        "title": "Behavioral Exploration: Learning to Explore via In-Context Adaptation"
      },
      {
        "paperId": "fe3dd3668cf1c17f18ee60e96988f58beab5d2ca",
        "title": "On Efficient Bayesian Exploration in Model-Based Reinforcement Learning"
      },
      {
        "paperId": "845e7b768dd3b846be8e2aa5a001d57fcc635a90",
        "title": "Uncertainty Prioritized Experience Replay"
      },
      {
        "paperId": "6cf69bada16e23b9ab912b1d15afabdbefcd4e50",
        "title": "Diffusion-Based Generative System Surrogates for Scalable Learning-Driven Optimization in Virtual Playgrounds"
      },
      {
        "paperId": "25dc47cb7c1ccb47a946fd7c445c635bf3e35ca0",
        "title": "Of Mice and Machines: A Comparison of Learning Between Real World Mice and RL Agents"
      },
      {
        "paperId": "536d6a3e9138647ca9935d40ce8fd0d6a0fc7344",
        "title": "Exploration by Random Distribution Distillation"
      },
      {
        "paperId": "08941f8975a59bc21a89c9fca9ccf71d475a747b",
        "title": "Parameter Estimation using Reinforcement Learning Causal Curiosity: Limits and Challenges"
      },
      {
        "paperId": "993c8df81acf8aad54f80f13c02e3c64717739e1",
        "title": "Fast Adaptation with Behavioral Foundation Models"
      },
      {
        "paperId": "823311266b7e638ce2fd2afa7547afe25f6a7999",
        "title": "\u201cImproving Sample Efficiency and Exploration in Upside-Down Reinforcement Learning\u201d"
      },
      {
        "paperId": "5475444b8f7d48ca5643c3c36c8caf2cce52ed40",
        "title": "KEA: Keeping Exploration Alive by Proactively Coordinating Exploration Strategies"
      },
      {
        "paperId": "5fafd699ee39d0b7990245965db4893d003406e7",
        "title": "Temporal Difference Flows"
      },
      {
        "paperId": "0b32d9781745580d9247b5418ee56c2ec5674aa9",
        "title": "InDRiVE: Intrinsic Disagreement based Reinforcement for Vehicle Exploration through Curiosity Driven Generalized World Model"
      },
      {
        "paperId": "2fc0ac5012517b5ab0093120e1ef24d24a887f31",
        "title": "GAILPG: Multiagent Policy Gradient With Generative Adversarial Imitation Learning"
      },
      {
        "paperId": "c198ee0e0c2f9899bec53b21b0705e684ac9eeaa",
        "title": "Sim-to-Real Reinforcement Learning for Vision-Based Dexterous Manipulation on Humanoids"
      },
      {
        "paperId": "8c1ad54195cbc44eb8fe96aae175900d6eead2fd",
        "title": "Multi-Objective Reinforcement Learning for Critical Scenario Generation of Autonomous Vehicles"
      },
      {
        "paperId": "ebac41944cf445f0dab93ea3a45d69f81508cc46",
        "title": "DIAL: Distribution-Informed Adaptive Learning of Multi-Task Constraints for Safety-Critical Systems"
      },
      {
        "paperId": "53d5d595b263f54c9a5c4d51e298413c450abb79",
        "title": "Curiosity-Driven Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "2b397ebc66a530078c160d99a84284a3df01f785",
        "title": "PIMAEX: Multi-Agent Exploration through Peer Incentivization"
      },
      {
        "paperId": "e4fef8d5864c5468100ca167639ef3fa374c0442",
        "title": "MaxInfoRL: Boosting exploration in reinforcement learning through information gain maximization"
      },
      {
        "paperId": "19d55190d783052ba25fa564e5a5b8acb355e451",
        "title": "Pythia: An Edge-First Agent for State Prediction in High-Dimensional Environments"
      },
      {
        "paperId": "74abb1a62b70e7f9019c7209ea4e65b72867a574",
        "title": "Online Intrinsic Rewards for Decision Making Agents from Large Language Model Feedback"
      },
      {
        "paperId": "d89ddb912b9680df66ba6a2ff2f8eb3716dc44b5",
        "title": "SPIRE: Synergistic Planning, Imitation, and Reinforcement Learning for Long-Horizon Manipulation"
      },
      {
        "paperId": "3775bd14050cd41b32e518d2f34a85125680b2dd",
        "title": "Leveraging Skills from Unlabeled Prior Data for Efficient Online Exploration"
      },
      {
        "paperId": "aaa152a502a12ba70aabbc3a0c94390c75d25bf0",
        "title": "Optimal Robotic Assembly Sequence Planning (ORASP): A Sequential Decision-Making Approach"
      },
      {
        "paperId": "6efb032415f6e62d8d5de7a1571f35901e1c6f97",
        "title": "Overcoming Slow Decision Frequencies in Continuous Control: Model-Based Sequence Reinforcement Learning for Model-Free Control"
      },
      {
        "paperId": "a57162ae7f03e875cf4e2ea130f696b58d967724",
        "title": "From Reward Shaping to Q-Shaping: Achieving Unbiased Learning with LLM-Guided Knowledge"
      },
      {
        "paperId": "c4dfc0fd5a55a9d5bbf2f0b0ecaed67bb6941d94",
        "title": "State-Novelty Guided Action Persistence in Deep Reinforcement Learning"
      },
      {
        "paperId": "195bf16bb03e9b0e3f5d28386383ed49a87f18e2",
        "title": "Hierarchical Reinforcement Learning-Based End-to-End Visual Servoing With Smooth Subgoals"
      },
      {
        "paperId": "bc5a02469eda208c6df470da4840d2f6d00609da",
        "title": "A Single Goal is All You Need: Skills and Exploration Emerge from Contrastive RL without Rewards, Demonstrations, or Subgoals"
      },
      {
        "paperId": "f5bbde644e1d7c22f3685dae72e19e7a4afde79f",
        "title": "Image-Based Deep Reinforcement Learning with Intrinsically Motivated Stimuli: On the Execution of Complex Robotic Tasks"
      },
      {
        "paperId": "5ecf53ab083f72f10421225a7dde25eb51cb6b22",
        "title": "Can Learned Optimization Make Reinforcement Learning Less Difficult?"
      },
      {
        "paperId": "fa70bc2fda70a1741129fef5774151943122524f",
        "title": "Improving Reinforcement Learning Exploration by Autoencoders"
      },
      {
        "paperId": "ce32e5dae73775c13b749a27fc53219912ab4b59",
        "title": "CMBE: Curiosity-driven Model-Based Exploration for Multi-Agent Reinforcement Learning in Sparse Reward Settings"
      },
      {
        "paperId": "6bf550ce7f10a4347d448eb810a92e1a5cfffa4b",
        "title": "Beyond Optimism: Exploration With Partially Observable Rewards"
      },
      {
        "paperId": "9445a7fcf495ba13169533d591cefaddadaf6f43",
        "title": "World Models with Hints of Large Language Models for Goal Achieving"
      },
      {
        "paperId": "af0851a60ac3b5f003e3fefad23a856ed5f91f99",
        "title": "LAGMA: LAtent Goal-guided Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "769d8fdf6520c52e8767ee6d54f6417cf6e7904e",
        "title": "RLeXplore: Accelerating Research in Intrinsically-Motivated Reinforcement Learning"
      },
      {
        "paperId": "ffcef91b1c25d2d740b335f9b4b3c942cd8d79de",
        "title": "Enhancing Q-Learning with Large Language Model Heuristics"
      },
      {
        "paperId": "d8b10ccccd1fd7295ce86e0c214925b37f6a22de",
        "title": "Continuously evolving rewards in an open-ended environment"
      },
      {
        "paperId": "bfdb99adacfc21fb38cd50482e437ed533ba28ce",
        "title": "Curiosity model policy optimization for robotic manipulator tracking control with input saturation in uncertain environment"
      },
      {
        "paperId": "5059dd54b9add9e0a27af4e5f85a4683eaf7b99f",
        "title": "Nuclear Norm Maximization-Based Curiosity-Driven Reinforcement Learning"
      },
      {
        "paperId": "098c52a012cfda1b5ed4aef995efbbcf03dce25c",
        "title": "Goal Exploration via Adaptive Skill Distribution for Goal-Conditioned Reinforcement Learning"
      },
      {
        "paperId": "bce604b670336ddf07b61f8586015be9fe76219e",
        "title": "Improved ICM-DDQN Obstacle Avoidance Method for Unmanned Underwater Vehicle"
      },
      {
        "paperId": "34a1b39ded76621764814fe339cfbc69d9d7158b",
        "title": "Mol-AIR: Molecular Reinforcement Learning with Adaptive Intrinsic Rewards for Goal-Directed Molecular Generation"
      },
      {
        "paperId": "143c403d4a2f44b694b13a5d56b5cd70fc589cd6",
        "title": "Double Buffers CEM-TD3: More Efficient Evolution and Richer Exploration"
      },
      {
        "paperId": "52c7bcf519e56b1dcd24730bb13f1f91573de554",
        "title": "MalBoT-DRL: Malware Botnet Detection Using Deep Reinforcement Learning in IoT Networks"
      },
      {
        "paperId": "bfbe7cd5a6535296b324c72920d6c9de035ef8b4",
        "title": "Efficient Episodic Memory Utilization of Cooperative Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "620a6b42aa63695e7e5587a1398e598926b0f202",
        "title": "Virtual Network Embedding Based on Hierarchical Cooperative Multiagent Reinforcement Learning"
      },
      {
        "paperId": "a2436e924b1ecf4e22d5269033b0bbabce70295d",
        "title": "Automated Gameplay Testing and Validation With Curiosity-Conditioned Proximal Trajectories"
      },
      {
        "paperId": "839b6e963039b1e3382e861f6bd9b95417707562",
        "title": "Monitored Markov Decision Processes"
      },
      {
        "paperId": "87ddd7811eddfa67609f4ff8d10fb2f6ba42d94f",
        "title": "Exploration and Anti-Exploration with Distributional Random Network Distillation"
      },
      {
        "paperId": "5cd9d01459fb90d5799d077898003e5ade202f41",
        "title": "Adaptive trajectory-constrained exploration strategy for deep reinforcement learning"
      },
      {
        "paperId": "c5f9b27a1252d6819ee52ffc328e7116f7a9a311",
        "title": "DCIR: Dynamic Consistency Intrinsic Reward for Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "74c2af7883f4bf044d4271f5d1dc32b4e2de3091",
        "title": "Entropy Maximization in High Dimensional Multiagent State Spaces"
      },
      {
        "paperId": "06cb176ff32fbf498f0df93016a46dbbb333d533",
        "title": "Goal-conditioned Offline Planning from Curious Exploration"
      },
      {
        "paperId": "f62047a441c692c0d8d420b23f71041dc14f0c0b",
        "title": "An Improved Exploration Method for Cooperative Multi-UAV Policy Learning with Sparse Rewards"
      },
      {
        "paperId": "da483afe840f58fb5a58ed3184406401cc8a6158",
        "title": "Multi-timescale reinforcement learning in the brain"
      },
      {
        "paperId": "05603c65d813105eb136e867abfc412ea1735cf0",
        "title": "Accelerating Exploration with Unlabeled Prior Data"
      },
      {
        "paperId": "01e5ed1eef3bb498c635d104f62f4f0befa2fe1c",
        "title": "Model-based exploration strategy to accelerate deterministic strategy algorithm training"
      },
      {
        "paperId": "a162c3b95d50bc2e4e89884061464699a97a94da",
        "title": "Variational Curriculum Reinforcement Learning for Unsupervised Discovery of Skills"
      },
      {
        "paperId": "0f6206fc97b23c63fe6b9eb61e823ce3f433e137",
        "title": "Optimal Robotic Assembly Sequence Planning: A Sequential Decision-Making Approach"
      },
      {
        "paperId": "23acd2d5ac7e50db0646833e0a73d023c3a7d493",
        "title": "Sample-Efficient Imitation Learning via Reconstructed Intrinsic Reward"
      },
      {
        "paperId": "c35cd153d65b6a0f72f6fb398879344eb2150198",
        "title": "ELDEN: Exploration via Local Dependencies"
      },
      {
        "paperId": "70bea8a9ce5949eef691809d20fa6f880cee31cf",
        "title": "LESSON: Learning to Integrate Exploration Strategies for Reinforcement Learning via an Option Framework"
      },
      {
        "paperId": "24a3a6cb5966ce99f725c5965ca73b5e2515ed7a",
        "title": "Learning to Terminate in Object Navigation"
      },
      {
        "paperId": "cd6d2eca8c03ae9bc75c5e8d55e36be1dc088893",
        "title": "Subwords as Skills: Tokenization for Sparse-Reward Reinforcement Learning"
      },
      {
        "paperId": "0bf353d07ca6642a3e7c888eabd49273a6573e2b",
        "title": "VDFD: Multi-Agent Value Decomposition Framework with Disentangled World Model"
      },
      {
        "paperId": "fce53c0ede6151c3cd5b61841d6aafa67085047b",
        "title": "Go Beyond Imagination: Maximizing Episodic Reachability with World Models"
      },
      {
        "paperId": "8e2451959120cb45d976849ab5dad9130d48eee9",
        "title": "Beyond Surprise: Improving Exploration Through Surprise Novelty"
      },
      {
        "paperId": "4bb427b603474dd862aab68357a472ba8af86f5d",
        "title": "Efficient Q-Learning over Visit Frequency Maps for Multi-Agent Exploration of Unknown Environments"
      },
      {
        "paperId": "ad7d6d9ca1ac2f5ad6f35ea47dd5345860310a8d",
        "title": "Intrinsically motivated graph exploration using network theories of human curiosity"
      },
      {
        "paperId": "193af00290b5ee975226e5d8c9fbbdca2c9eeb36",
        "title": "Active sensing with predictive coding and uncertainty minimization"
      },
      {
        "paperId": "9ccec14734e236c0609464305e4fb2b9e27bf28f",
        "title": "HCS-R-HER: Hierarchical reinforcement learning based on cross subtasks rainbow hindsight experience replay"
      },
      {
        "paperId": "bd09f1477ff5ea1fe1b9ea57a0272978844ee6ad",
        "title": "Curious Replay for Model-based Adaptation"
      },
      {
        "paperId": "6b8e98792e4af57687939156c07b99cd12187f89",
        "title": "Maximum State Entropy Exploration using Predecessor and Successor Representations"
      },
      {
        "paperId": "0a806876e419de58b650aacbf218e23f589327ad",
        "title": "CEM: Constrained Entropy Maximization for Task-Agnostic Safe Exploration"
      },
      {
        "paperId": "e66d55565f6cd6336f92be01b4efc7e6a1eb2381",
        "title": "Optimistic Active Exploration of Dynamical Systems"
      },
      {
        "paperId": "9b3b2829cd0ea4737615ad2f29074a47acdca0ff",
        "title": "ALP: Action-Aware Embodied Learning for Perception"
      },
      {
        "paperId": "d16feddfeca2617ca2127b7d134ceb78ce8a8b40",
        "title": "A Study of Global and Episodic Bonuses for Exploration in Contextual MDPs"
      },
      {
        "paperId": "ec24cba023e59a11253c60a5516df0562c62d62c",
        "title": "Flipping Coins to Estimate Pseudocounts for Exploration in Reinforcement Learning"
      },
      {
        "paperId": "aa5cc1f2cec1881ba8f9fce3dad310df9fe1ff87",
        "title": "Efficient Dialog Policy Learning With Hindsight, User Modeling, and Adaptation"
      },
      {
        "paperId": "b9e4ea7ff34a304cc0e7bfaa638eaa850391b676",
        "title": "Accelerating Reinforcement Learning with Value-Conditional State Entropy Exploration"
      },
      {
        "paperId": "1311944e1ac408fd4a7829b254f25a6560b66e07",
        "title": "Maximize to Explore: One Objective Function Fusing Estimation, Planning, and Exploration"
      },
      {
        "paperId": "b28802b664f3eb1d51ecb7892d8f75ec180f380f",
        "title": "Measuring and Modeling Physical Intrinsic Motivation"
      },
      {
        "paperId": "af9e69ec120e94b5542edf88b392ee3769c1379b",
        "title": "MIMEx: Intrinsic Rewards from Masked Input Modeling"
      },
      {
        "paperId": "2f1fee5087d47e7a8c71763c22ac784c9565278c",
        "title": "Learning Achievement Structure for Structured Exploration in Domains with Sparse Reward"
      },
      {
        "paperId": "a9ff9a7aaedcca29c93087a24f0b87a6a3a24862",
        "title": "An Overview of Environmental Features that Impact Deep Reinforcement Learning in Sparse-Reward Domains"
      },
      {
        "paperId": "4d0e2aa1142aa323df862ebf56513adf04fcb5ae",
        "title": "Learning robotic manipulation skills with multiple semantic goals by conservative curiosity-motivated exploration"
      },
      {
        "paperId": "66fb5b32220041ab54c39afa38ea199b2cfb1c66",
        "title": "Human-Inspired Framework to Accelerate Reinforcement Learning"
      },
      {
        "paperId": "1c3548e622a958c527eaeaf32a338ca8bdf24efa",
        "title": "CLR-GAM: Contrastive Point Cloud Learning with Guided Augmentation and Feature Mapping"
      },
      {
        "paperId": "4da7c38585ad044f62914a7411cb1878c7df7c4e",
        "title": "Self-supervised network distillation: An effective approach to exploration in sparse reward environments"
      },
      {
        "paperId": "4a50853984a126e2b77f7c3f216c0ef486a138ab",
        "title": "Efficient exploration via epistemic-risk-seeking policy optimization"
      },
      {
        "paperId": "e713bf4f0d17ad654e7f3f6bf1f03cbe5d450a91",
        "title": "Systemic kappa opioid receptor antagonism accelerates reinforcement learning via augmentation of novelty processing in male mice"
      },
      {
        "paperId": "528eaca08fac125c5c65c6d5d3f636d92ebd0517",
        "title": "Automatic Intrinsic Reward Shaping for Exploration in Deep Reinforcement Learning"
      },
      {
        "paperId": "c109f1c22a531087ce2415711658e2949432a056",
        "title": "Story Shaping: Teaching Agents Human-like Behavior with Stories"
      },
      {
        "paperId": "9aaf82233b7258b6540b45fa72f53f07093b97a3",
        "title": "Multi-Agent Interplay in a Competitive Survival Environment"
      },
      {
        "paperId": "74d8118572b8cbfca792a3f178d431ac5fe499ef",
        "title": "Learning Configurations of Operating Environment of Autonomous Vehicles to Maximize their Collisions"
      },
      {
        "paperId": "a7d52c02213d5de2aa79134a5dd9b24c5619155c",
        "title": "Strangeness-driven Exploration in Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "621cb2bca92c12ba8381ca8512c432f5ba7dd815",
        "title": "Understanding the Complexity Gains of Single-Task RL with a Curriculum"
      },
      {
        "paperId": "ac6e815478bc8dcb4c49fa258e5e725b8a56c9ae",
        "title": "An Optimistic Approach to the Temporal Difference Error in Off-Policy Actor-Critic Algorithms"
      },
      {
        "paperId": "7595d53e249d9e6e43049b81ae02d347ac08b836",
        "title": "Deep Reinforcement Learning With Part-Aware Exploration Bonus in Video Games"
      },
      {
        "paperId": "cbd7b03a3d031b39f7a8203d594c1da6be396947",
        "title": "Mixed Time-Frame Training for Reinforcement Learning"
      },
      {
        "paperId": "8225ac1bec34a645d45d903642a8af59b3367cc2",
        "title": "Tackling Visual Control via Multi-View Exploration Maximization"
      },
      {
        "paperId": "2960b767b3b763948b15e597d7247048b5b97690",
        "title": "Modified Annealed Adversarial Bonus for Adversarially Guided Actor-Critic"
      },
      {
        "paperId": "915f52bfbbb813bf50a1824282c0b490a6c30dd1",
        "title": "Curiosity in hindsight"
      },
      {
        "paperId": "b065c42b1f40368ff1dacb950c4817a4d18b3ed9",
        "title": "Foundation Models for Semantic Novelty in Reinforcement Learning"
      },
      {
        "paperId": "c90a33f1f0049d524e9b5b3174d35611fd9a8096",
        "title": "Pretraining in Deep Reinforcement Learning: A Survey"
      },
      {
        "paperId": "203a1c0e5025489a52c030adbbc102a787685ee6",
        "title": "Unpacking Reward Shaping: Understanding the Benefits of Reward Engineering on Sample Complexity"
      },
      {
        "paperId": "551633338103610cb8a60e6df004ef6fd3841d17",
        "title": "Reinforcement Learning with Automated Auxiliary Loss Search"
      },
      {
        "paperId": "0351a103915d2f10d1915b4892a988d9b15df406",
        "title": "Exploration via Elliptical Episodic Bonuses"
      },
      {
        "paperId": "8d4f1b12c4332170f0bf0991635e085faef852b9",
        "title": "ELIGN: Expectation Alignment as a Multi-Agent Intrinsic Reward"
      },
      {
        "paperId": "01575138a4c3b0993d875bda1e25a15508982866",
        "title": "Boosting Exploration in Actor-Critic Algorithms by Incentivizing Plausible Novel States"
      },
      {
        "paperId": "9ec5100caef004f14e6daa6766c0ad18fe2b21c3",
        "title": "Active inference, preference learning and adaptive behaviour"
      },
      {
        "paperId": "85257a741bb7ee51c2954f63bf55188f61dbf47e",
        "title": "Delayed Geometric Discounts: An Alternative Criterion for Reinforcement Learning"
      },
      {
        "paperId": "c64a99ec780d5e1c6ba242a82e72e96eb0c3ef8a",
        "title": "An Information-Theoretic Perspective on Intrinsic Motivation in Reinforcement Learning: A Survey"
      },
      {
        "paperId": "ae412e9981d6bc9ffa22c543a2fca4d8312c2986",
        "title": "Rewarding Episodic Visitation Discrepancy for Exploration in Reinforcement Learning"
      },
      {
        "paperId": "54ae0a1c394167badd331babffb92908cbd41c81",
        "title": "Ask Before You Act: Generalising to Novel Environments by Asking Questions"
      },
      {
        "paperId": "89ae946f74e75d1e7a9ca5d9d44c0c610e70d41b",
        "title": "Self-Supervised Exploration via Temporal Inconsistency in Reinforcement Learning"
      },
      {
        "paperId": "eaeb49dcfd7f29329214e16484445492fff48305",
        "title": "Intrinsic-Motivated Sensor Management: Exploring with Physical Surprise"
      },
      {
        "paperId": "8d03f604071ed94f3165380c745c2fae5c613d87",
        "title": "Reinforcement learning with experience replay and adaptation of action dispersion"
      },
      {
        "paperId": "df2499f13dd98d4ce7a46888834084798c9930e4",
        "title": "Guaranteed Discovery of Control-Endogenous Latent States with Multi-Step Inverse Models"
      },
      {
        "paperId": "5c0b56d9440ea70c79d1d0032d46c14e06f541f5",
        "title": "Reactive Exploration to Cope with Non-Stationarity in Lifelong Reinforcement Learning"
      },
      {
        "paperId": "f4839f5657b2c9a5d5c599229f55d6f7aa2c5b68",
        "title": "Deep Reinforcement Learning with Swin Transformers"
      },
      {
        "paperId": "cede72a30e2c5833111357cc12b06eeb4676ebb5",
        "title": "GAN-based Intrinsic Exploration for Sample Efficient Reinforcement Learning"
      },
      {
        "paperId": "60306257fbac5e4fb8b94552b67094e09157f049",
        "title": "Optimistic Initialization for Exploration in Continuous Control"
      },
      {
        "paperId": "c5a63ef816ac556d78f02f478dff8796e64f2d85",
        "title": "An approach to solving optimal control problems of nonlinear systems by introducing detail-reward mechanism in deep reinforcement learning."
      },
      {
        "paperId": "027526624220bc90efb648cdbb90f0a4829f66a3",
        "title": "SEREN: Knowing When to Explore and When to Exploit"
      },
      {
        "paperId": "215d62afc65ca93cda115d227e79be0716cc8667",
        "title": "Utility of doctrine with multi-agent RL for military engagements"
      },
      {
        "paperId": "cc9f2fd320a279741403c4bfbeb91179803c428c",
        "title": "Reward Uncertainty for Exploration in Preference-based Reinforcement Learning"
      },
      {
        "paperId": "ee5b628d998cf4d532f0607e4e2568246f42027b",
        "title": "Nuclear Norm Maximization Based Curiosity-Driven Learning"
      },
      {
        "paperId": "0b8ef454e35c9fb6c3c6050d2d79e295ff2a7ac7",
        "title": "Count-Based Exploration via Embedded State Space for Deep Reinforcement Learning"
      },
      {
        "paperId": "4ba973b38e448b2060bd6e2cbc0255d767ddaf98",
        "title": "ASE"
      },
      {
        "paperId": "f60dd378987c05e54b577c857012b815744c2951",
        "title": "Learning Attentional and Gated Communication via Curiosity"
      },
      {
        "paperId": "426e8449e900f6e16a77809b23cf3507b6f89917",
        "title": "Off-Policy Evaluation With Online Adaptation for Robot Exploration in Challenging Environments"
      },
      {
        "paperId": "12aeb6e6835e54a34a147b2070093ad775a42115",
        "title": "Habitat-Web: Learning Embodied Object-Search Strategies from Human Demonstrations at Scale"
      },
      {
        "paperId": "daa0b9b5f9aba3b87b47429b2a2224eaf7f789b1",
        "title": "Exploration for Countering the Episodic Memory"
      },
      {
        "paperId": "cca11c388bfedf34f21a36b0d48c283b26ba1168",
        "title": "R\u00e9nyi State Entropy Maximization for Exploration Acceleration in Reinforcement Learning"
      },
      {
        "paperId": "549bfdfd9fa718331076810f0d5817adcd79fe69",
        "title": "AutoDIME: Automatic Design of Interesting Multi-Agent Environments"
      },
      {
        "paperId": "808dd538f0970668a48a611a5627e0babd36740b",
        "title": "Intrinsically-Motivated Reinforcement Learning: A Brief Introduction"
      },
      {
        "paperId": "47f28e1057eac8f7ec99ac8250e4bfed0eafd9e9",
        "title": "Unified curiosity-Driven learning with smoothed intrinsic reward estimation"
      },
      {
        "paperId": "866b5a0c31f850b37dbaf8acd6db8cfb7dfb2198",
        "title": "Continual Auxiliary Task Learning"
      },
      {
        "paperId": "ba9b7f2998002529061532486226496883e86e30",
        "title": "Improving Intrinsic Exploration with Language Abstractions"
      },
      {
        "paperId": "6d8f5366a04ed1955bb62759369c938f05da7f27",
        "title": "Open-Ended Reinforcement Learning with Neural Reward Functions"
      },
      {
        "paperId": "2fbd69f02691a464366d7e7b88a1c84204615396",
        "title": "Multi-robot Object Finding Using Thompson Sampling"
      },
      {
        "paperId": "76e86347243661adf0ef96271982961f3117af10",
        "title": "From Psychological Curiosity to Artificial Curiosity: Curiosity-Driven Learning in Artificial Intelligence Tasks"
      },
      {
        "paperId": "545493813b7f9cc1ce9511edd4fda798a31c3aa6",
        "title": "Model-Value Inconsistency as a Signal for Epistemic Uncertainty"
      },
      {
        "paperId": "faf8abf19a89fa0e40f3f50550816a3bdecb80f0",
        "title": "Information is Power: Intrinsic Control via Information Capture"
      },
      {
        "paperId": "39d2e380967c91b447d47377840fa541e276b479",
        "title": "Interesting Object, Curious Agent: Learning Task-Agnostic Exploration"
      },
      {
        "paperId": "e227f71d7d84ed9a76278510a46f9b7db286ba92",
        "title": "Episodic Multi-agent Reinforcement Learning with Curiosity-Driven Exploration"
      },
      {
        "paperId": "895dbfa06a4441eec97f021ad8944c77e8ddae3f",
        "title": "Agent Spaces"
      },
      {
        "paperId": "1534db651327ca7e57bbc9e7ba5d6c4926c5c9bd",
        "title": "Towards Robust Bisimulation Metric Learning"
      },
      {
        "paperId": "6f07784838ac8847c94e038ed5c288013c372dba",
        "title": "CIExplore: Curiosity and Influence-based Exploration in Multi-Agent Cooperative Scenarios with Sparse Rewards"
      },
      {
        "paperId": "c15168a044a2a97d31ee747f99b91be6911c0e72",
        "title": "Off-policy Reinforcement Learning with Optimistic Exploration and Distribution Correction"
      },
      {
        "paperId": "c4d91bca6066282da421671ab06ac1e156f19851",
        "title": "Anti-Concentrated Confidence Bonuses for Scalable Exploration"
      },
      {
        "paperId": "5b8392ca789edfad543c3f27ca6221b6cb434713",
        "title": "Near-Optimal Reward-Free Exploration for Linear Mixture MDPs with Plug-in Solver"
      },
      {
        "paperId": "5a8b4d43cdf73ef7218dfa5cb305064a4f55b8b5",
        "title": "Mismatched No More: Joint Model-Policy Optimization for Model-Based RL"
      },
      {
        "paperId": "718d9ecaa866109abbe4a9b97c522c6030b06929",
        "title": "Exploring More When It Needs in Deep Reinforcement Learning"
      },
      {
        "paperId": "145691e93cca32af2f2ecfef49b8b34520b9c462",
        "title": "Density-based Curriculum for Multi-goal Reinforcement Learning with Sparse Rewards"
      },
      {
        "paperId": "41872d5173d4ceaef1a5359dae8ad4198caf6b66",
        "title": "Knowledge is reward: Learning optimal exploration by predictive reward cashing"
      },
      {
        "paperId": "12075ea34f5fbe32ec5582786761ab34d401209b",
        "title": "Exploration in Deep Reinforcement Learning: From Single-Agent to Multiagent Domain"
      },
      {
        "paperId": "2f00238cff91020dd4ab9027d34504c61c84e1fd",
        "title": "Exploration in Recommender Systems"
      },
      {
        "paperId": "fb3ed1f977215983154b3a132ec233cb296c1238",
        "title": "Values of User Exploration in Recommender Systems"
      },
      {
        "paperId": "3be84e24b144541a8cd9030526ef2b8ef2cbfe54",
        "title": "A Survey of Exploration Methods in Reinforcement Learning"
      },
      {
        "paperId": "8e9d44502f5b6de37d5a7181e169a2899a91f7b3",
        "title": "Self-Attention-Based Temporary Curiosity in Reinforcement Learning Exploration"
      },
      {
        "paperId": "bc18f1ec5209f8ebacb91eff42927a33664bc1a2",
        "title": "Influence-based Reinforcement Learning for Intrinsically-motivated Agents"
      },
      {
        "paperId": "59d129cb6021e2f1ca1aeda68d185aab59c190dc",
        "title": "Exploration via Distributional Reinforcement Learning with Epistemic and Aleatoric Uncertainty Estimation*"
      },
      {
        "paperId": "694cac5ed2db350922d404cea214ab5f38a1e3ee",
        "title": "Goal-driven active learning"
      },
      {
        "paperId": "350347808e5011999b6f90deb996465a2e7674e7",
        "title": "Human-Level Reinforcement Learning through Theory-Based Modeling, Exploration, and Planning"
      },
      {
        "paperId": "feb7f993c402e2663d20bbafa83c11e6db3dfe6b",
        "title": "Cooperative Exploration for Multi-Agent Deep Reinforcement Learning"
      },
      {
        "paperId": "a5c238ebcda0ce7a875832157b4d9a37ad5dd0a0",
        "title": "Multimodal Reward Shaping for Efficient Exploration in Reinforcement Learning"
      },
      {
        "paperId": "7c8ce0557cf09ebb2353456cec0fc269d11bc193",
        "title": "Remembering the Past to See the Future."
      },
      {
        "paperId": "6382bec2c2fd18d388483653409b1a18048521da",
        "title": "MURAL: Meta-Learning Uncertainty-Aware Rewards for Outcome-Driven Reinforcement Learning"
      },
      {
        "paperId": "7aa03760057a55a52c0c56d5bd408edf3891ffa8",
        "title": "UAV-Assisted Online Machine Learning Over Multi-Tiered Networks: A Hierarchical Nested Personalized Federated Learning Approach"
      },
      {
        "paperId": "adc4df3c3c5bfaecb5510cf97da0d441938f5ab9",
        "title": "Exploration and preference satisfaction trade-off in reward-free learning"
      },
      {
        "paperId": "a7f844de5b1335b6d4f754f6370f30ce36294fc7",
        "title": "Online reinforcement learning with sparse rewards through an active inference capsule"
      },
      {
        "paperId": "adcae35901c36325478a03b647e14222a53ea9fc",
        "title": "What Can I Do Here? Learning New Skills by Imagining Visual Affordances"
      },
      {
        "paperId": "b666facd47d8878bf6a1e481c9e39dd40a017b1c",
        "title": "Learning from Demonstration without Demonstrations"
      },
      {
        "paperId": "7a62d9d1f5617559fd446596cc286eeefd39b959",
        "title": "Sample Efficient Reinforcement Learning via Model-Ensemble Exploration and Exploitation"
      },
      {
        "paperId": "1e515c9c4aaca14194112dad3e7301bea2d61930",
        "title": "DIMSAN: Fast Exploration with the Synergy between Density-based Intrinsic Motivation and Self-adaptive Action Noise"
      },
      {
        "paperId": "0f4f6d1ee869ed496534d7eda51ac0f0110ce3fc",
        "title": "Don't Do What Doesn't Matter: Intrinsic Motivation with Action Usefulness"
      },
      {
        "paperId": "31d1cc6aa3a04bc524b40b089087c8ccffab35df",
        "title": "Sequential Generative Exploration Model for Partially Observable Reinforcement Learning"
      },
      {
        "paperId": "a8e8c037136750a2e989e60c9a56dad082fe1269",
        "title": "PP-PG: Combining Parameter Perturbation with Policy Gradient Methods for Effective and Efficient Explorations in Deep Reinforcement Learning"
      },
      {
        "paperId": "16486b905177860b5b68b7f4da9e499562c674b0",
        "title": "RECON: Rapid Exploration for Open-World Navigation with Latent Goal Models"
      },
      {
        "paperId": "76907582d260103c90058d3fc4b9f9969fb75922",
        "title": "Touch-based Curiosity for Sparse-Reward Tasks"
      },
      {
        "paperId": "ef159910831752335df38696a584a22a35c5345e",
        "title": "Multi-agent reinforcement learning with directed exploration and selective memory reuse"
      },
      {
        "paperId": "7c1ed780d2dd7d780c5ff6371d4d14c36c357e3c",
        "title": "Learning to Shape Rewards Using a Game of Two Partners"
      },
      {
        "paperId": "fab4b767e3ae70e8e04e991e8d626fe6b3184213",
        "title": "Learning Navigation Policies for Mobile Robots in Deep Reinforcement Learning with Random Network Distillation"
      },
      {
        "paperId": "b4d08620b6cd42d60795fc3104fcd9a3f060bbe4",
        "title": "CDDPG: A Deep-Reinforcement-Learning-Based Approach for Electric Vehicle Charging Control"
      },
      {
        "paperId": "7fc04f8031598da6510bce4a7e5b4722305ca5b0",
        "title": "State Entropy Maximization with Random Encoders for Efficient Exploration"
      },
      {
        "paperId": "1adbeb95eae6bad58425cdb40565e627032f72aa",
        "title": "State-Aware Variational Thompson Sampling for Deep Q-Networks"
      },
      {
        "paperId": "5ca0dbb82b8f01c0e9f4b7fb6ea4f54582f15bfe",
        "title": "View-Action Representation Learning for Active First-Person Vision"
      },
      {
        "paperId": "9e5fe2ba652774ba3b1127f626c192668a907132",
        "title": "Decoupled Exploration and Exploitation Policies for Sample-Efficient Reinforcement Learning"
      },
      {
        "paperId": "a188c3b58e71657ecfcddc37359aca51213e2187",
        "title": "Rank the Episodes: A Simple Approach for Exploration in Procedurally-Generated Environments"
      },
      {
        "paperId": "18b1d9052cb82dc7c34827768c98aae09caeb649",
        "title": "Curiosity in exploring chemical spaces: intrinsic rewards for molecular reinforcement learning"
      },
      {
        "paperId": "9345e5f5631455b3a8a7dc86a42cb22ce22084e8",
        "title": "BeBold: Exploration Beyond the Boundary of Explored Regions"
      },
      {
        "paperId": "376e0853411acb4e5732c587471d0a3910689b20",
        "title": "Adapting Behavior via Intrinsic Reward: A Survey and Empirical Study"
      },
      {
        "paperId": "eb97a79d822593baf46bb6d058de7e4ab1ac986f",
        "title": "Resistive Crossbars as Approximate Hardware Building Blocks for Machine Learning: Opportunities and Challenges"
      },
      {
        "paperId": "a435dda070ce517c975b0c0b77a6048c57545f7c",
        "title": "Perturbation-based exploration methods in deep reinforcement learning"
      },
      {
        "paperId": "ff7cb0662b0c644d302631361f81589708a505c4",
        "title": "DeepFoldit - A Deep Reinforcement Learning Neural Network Folding Proteins"
      },
      {
        "paperId": "3a8e812ec6e9b99577e0c282b71a12960a2dc545",
        "title": "Learning to Plan Optimistically: Uncertainty-Guided Deep Exploration via Latent Model Ensembles"
      },
      {
        "paperId": "b33b735352c78743707f66e525f7cca65ff207b0",
        "title": "Latent World Models For Intrinsically Motivated Exploration"
      },
      {
        "paperId": "0ca7c0d92a10359a2a7b8fd501a40c9ef768676d",
        "title": "Exploration in Approximate Hyper-State Space for Meta Reinforcement Learning"
      },
      {
        "paperId": "b1205a259d9a609c7d2c262da371387db57c5aa7",
        "title": "Reannealing of Decaying Exploration Based On Heuristic Measure in Deep Q-Network"
      },
      {
        "paperId": "d4d86f4a632d899ba5f511c1d0e7a7fbd44288c8",
        "title": "Novelty Search in representational space for sample efficient exploration"
      },
      {
        "paperId": "b91a18f0f6e86a347bbca9d3a7be4770eeaef3f0",
        "title": "Regret Bounds and Reinforcement Learning Exploration of EXP-based Algorithms"
      },
      {
        "paperId": "53681a2cc0b2f0ef8df73df76e79017cfa399a74",
        "title": "Energy-based Surprise Minimization for Multi-Agent Value Factorization"
      },
      {
        "paperId": "a2b00e570440619bbe6a483df6b0da46b1aae665",
        "title": "Fast and slow curiosity for high-level exploration in reinforcement learning"
      },
      {
        "paperId": "389dc120b62005e60a3ead2e3bbf257537568ee3",
        "title": "On Advances in Deep Learning with Applications in Financial Market Modeling"
      },
      {
        "paperId": "5f978b3829acad6ac8b3372d2fa8d38a45b96d3d",
        "title": "Noisy Agents: Self-supervised Exploration by Predicting Auditory Events"
      },
      {
        "paperId": "5dd561e231a1f9ce3c802add041dd3a601164270",
        "title": "Active World Model Learning with Progress Curiosity"
      },
      {
        "paperId": "6fb1b9628e9fc14a8c387657e50203b0fcdad8c6",
        "title": "Active World Model Learning in Agent-rich Environments with Progress Curiosity"
      },
      {
        "paperId": "5c314a73207652d784044e82ff392cd8699edc28",
        "title": "Follow then Forage Exploration: Improving Asynchronous Advantage Actor Critic"
      },
      {
        "paperId": "48e15d9d875b8cb74a1261a61e2d64d225078f7b",
        "title": "See, Hear, Explore: Curiosity via Audio-Visual Association"
      },
      {
        "paperId": "fd6ed4267c9ddc17cd03a493b6505e6b5f41179d",
        "title": "Directed Exploration Via Learnable Probability Distribution For Random Action Selection"
      },
      {
        "paperId": "8307a2915f56f48d07158fa335cb49b3959aa738",
        "title": "Novelty-Guided Reinforcement Learning via Encoded Behaviors"
      },
      {
        "paperId": "1c6435cb353271f3cb87b27ccc6df5b727d55f26",
        "title": "Model-based Reinforcement Learning: A Survey"
      },
      {
        "paperId": "8f11224f513c1ac7c8dd98b69b0291de3d069488",
        "title": "A Framework for Reinforcement Learning and Planning"
      },
      {
        "paperId": "df8a0bed6d685fee9c5ad418f4834e9537878de2",
        "title": "Learning with AMIGo: Adversarially Motivated Intrinsic Goals"
      },
      {
        "paperId": "1d4288c47a7802575d2a0d231d1283a4f225a85b",
        "title": "MetaCURE: Meta Reinforcement Learning with Empowerment-Driven Exploration"
      },
      {
        "paperId": "7013e9f2874e11749fdd0be2b9cce0d08a0c6d1a",
        "title": "Learn to Effectively Explore in Context-Based Meta-RL"
      },
      {
        "paperId": "fca28a48e3a01b9ad4c269511d9d23ff22ccd051",
        "title": "Non-local Policy Optimization via Diversity-regularized Collaborative Exploration"
      },
      {
        "paperId": "b6f5b2529c4b4baf4f599acd8843753a2132f4e1",
        "title": "Learning to Plan via Deep Optimistic Value Exploration"
      },
      {
        "paperId": "7965da1ff0296dac975454ecd90ac378f428c771",
        "title": "Unbiased Deep Reinforcement Learning: A General Training Framework for Existing and Future Algorithms"
      },
      {
        "paperId": "22e9a394cff574d5988642c28bcbdf7dcfe0be3b",
        "title": "Ensemble Wrapper Subsampling for Deep Modulation Classification"
      },
      {
        "paperId": "afe14fc36b09388f5cc2d50ea677d2c6c39b66fa",
        "title": "Adaptive Dialog Policy Learning with Hindsight and User Modeling"
      },
      {
        "paperId": "ae3b2768b0a3c73410bce0d2ae03feaf01f6f864",
        "title": "Dynamics-Aware Unsupervised Skill Discovery"
      },
      {
        "paperId": "616ac6772508e72084360158078058dfa8bda7e7",
        "title": "First return, then explore"
      },
      {
        "paperId": "de93c8aed64229571b03e40b36499d4f07ce875d",
        "title": "PBCS : Efficient Exploration and Exploitation Using a Synergy between Reinforcement Learning and Motion Planning"
      },
      {
        "paperId": "6874b196102f1de902bef1679ec26337bde6891a",
        "title": "Policy Gradient From Demonstration and Curiosity"
      },
      {
        "paperId": "3d622cae9fe9e01b246fb18e70c34fe9fd780c97",
        "title": "AutoEG: Automated Experience Grafting for Off-Policy Deep Reinforcement Learning"
      },
      {
        "paperId": "f52870cb67875888440de7097eb849dd3557dfe0",
        "title": "Curiosity-Driven Variational Autoencoder for Deep Q Network"
      },
      {
        "paperId": "cd2df53e6831460a984774e2c38bbe204f8045c5",
        "title": "Balancing Exploration and Exploitation in Self-imitation Learning"
      },
      {
        "paperId": "15c2194160de38b3800e09a7d98bc94c289c6286",
        "title": "Intrinsic Exploration as Multi-Objective RL"
      },
      {
        "paperId": "08277cd43a97bb8f389dfd12544e3fdf55f29408",
        "title": "Deep Reinforcement Learning with Weighted Q-Learning"
      },
      {
        "paperId": "8e9a4bfa14c10627a84300057c048511af0e19f5",
        "title": "An Adversarial Objective for Scalable Exploration"
      },
      {
        "paperId": "1ba26d96f857883bda6b3b43c91bf6ffdc572053",
        "title": "Independent learning approaches: overcoming multi-agent learning pathologies in team-games"
      },
      {
        "paperId": "bebe8ffb0c357ac0c7eea2556f817b03ee22b570",
        "title": "RIDE: Rewarding Impact-Driven Exploration for Procedurally-Generated Environments"
      },
      {
        "paperId": "7a1c3d2f064ea708eeddcd80c8bf1f643d4382bb",
        "title": "Disentangling Controllable Object Through Video Prediction Improves Visual Reinforcement Learning"
      },
      {
        "paperId": "086159600bede14e00f96043c733d4f3b45855aa",
        "title": "Never Give Up: Learning Directed Exploration Strategies"
      },
      {
        "paperId": "4dc50d9c88d4d4b802363785424bd58f7dcff458",
        "title": "A Comparative Study of Model-Free Reinforcement Learning Approaches"
      },
      {
        "paperId": "4dc5be46f267e8fa0213c12d374dab5e46377d28",
        "title": "Intrinsic Motivation for Encouraging Synergistic Behavior"
      },
      {
        "paperId": "8b04f4d0a435e1e614a501284fecde3d56769d00",
        "title": "MIME: Mutual Information Minimisation Exploration"
      },
      {
        "paperId": "67a586e80d9f29519d6218fa03355579f0d88230",
        "title": "PANTHER: A Programmable Architecture for Neural Network Training Harnessing Energy-Efficient ReRAM"
      },
      {
        "paperId": "c1aacd34b21c3a27a2d80ceb150e7676551591fa",
        "title": "Integrating Perception and Optimization for Dexterous Grasping and Manipulation"
      },
      {
        "paperId": "140788dc3c2e3d716fc6d8b3ec9ab04bccf7001d",
        "title": "Long-Term Visitation Value for Deep Exploration in Sparse Reward Reinforcement Learning"
      },
      {
        "paperId": "a951a731ef3ab06f3961a6ffabf49249fae32cae",
        "title": "Scaling Active Inference"
      },
      {
        "paperId": "542401c937e602ce22e1f5c781022aac2d64c023",
        "title": "Improved Exploration through Latent Trajectory Optimization in Deep Deterministic Policy Gradient"
      },
      {
        "paperId": "f58a77a92b795241943b7ff1740dfcc58039589c",
        "title": "TendencyRL: Multi-stage Discriminative Hints for Efficient Goal-Oriented Reverse Curriculum Learning"
      },
      {
        "paperId": "43ba8929e9ba09082b19800d36b793fe67000cb3",
        "title": "VASE: Variational Assorted Surprise Exploration for Reinforcement Learning"
      },
      {
        "paperId": "3110f8caaeddb0c5b0d95ae46e85cffab9f1847b",
        "title": "Dynamic Subgoal-based Exploration via Bayesian Optimization"
      },
      {
        "paperId": "1c3dafb77bcaf7fcfa3e34621d0d32176a95cf5b",
        "title": "Exploration via Sample-Efficient Subgoal Design"
      },
      {
        "paperId": "34c4621319b316cbca8fc1ff12739ca691fe9f95",
        "title": "Exploration via Cost-Aware Subgoal Design"
      },
      {
        "paperId": "9e8f14582200c121a2787a2b5eb5b8ba8c799427",
        "title": "Autonomous exploration for navigating in non-stationary CMPs"
      },
      {
        "paperId": "081ef50ed2b05e92bd8d61e753a7af751fcbecff",
        "title": "Skill-based curiosity for intrinsically motivated reinforcement learning"
      },
      {
        "paperId": "acd23b7f6f024ab41e9b8b9620bbf5c188739025",
        "title": "Receding Horizon Curiosity"
      },
      {
        "paperId": "2f9d13cbb9bc01b3261f8ff6afbace923eefa341",
        "title": "Reinforcement Learning with Probabilistically Complete Exploration"
      },
      {
        "paperId": "2066b64c446968143e59f8e6d1637a80acfd4739",
        "title": "Scoring-Aggregating-Planning: Learning task-agnostic priors from interactions and sparse rewards for zero-shot generalization"
      },
      {
        "paperId": "d85cc7cd2e424f598c447c72473078957e55b74b",
        "title": "Skew-Explore: Learn faster in continuous spaces with sparse rewards"
      },
      {
        "paperId": "e2689bca55c930c30bd627d551cf629a24f37aca",
        "title": "DS-VIC: Unsupervised Discovery of Decision States for Transfer in RL"
      },
      {
        "paperId": "6f6eb95b2bee897c3ba90cd6018d8545bed07abe",
        "title": "Self-Imitation Learning via Trajectory-Conditioned Policy for Hard-Exploration Tasks"
      },
      {
        "paperId": "33a2c0f3b9a0adc452a13d53f950c0c3c4abb11b",
        "title": "Emergent Tool Use From Multi-Agent Autocurricula"
      },
      {
        "paperId": "7cde4f7cc7f21fc9ba16bb0411725273b6a86b2c",
        "title": "ISL: Optimal Policy Learning With Optimal Exploration-Exploitation Trade-Off"
      },
      {
        "paperId": "f0480fe6421eb05f9ec7054736ac6ea15b477d61",
        "title": "ISL: A novel approach for deep exploration"
      },
      {
        "paperId": "6417d1bfc8f7dc8df7b04e50f8eb9188784b776a",
        "title": "Learning action-oriented models through active inference"
      },
      {
        "paperId": "45ac74350d21387c42ff92e90cd088bf310e1542",
        "title": "Guided goal generation for hindsight multi-goal reinforcement learning"
      },
      {
        "paperId": "222baa4e9e7ce691fdfddbc826a70e027daed70d",
        "title": "Reinforcement Learning in Healthcare: A Survey"
      },
      {
        "paperId": "895735cace0de940aa647dbafc046b7f30316fe5",
        "title": "A survey on intrinsic motivation in reinforcement learning"
      },
      {
        "paperId": "fa4277c83ae5c6c5b9fa5e58c9053e2d92cb8b0e",
        "title": "Benchmarking Bonus-Based Exploration Methods on the Arcade Learning Environment"
      },
      {
        "paperId": "1d47564c053571175e81cc3fe537da5d60cd8aee",
        "title": "A Categorization of Reinforcement Learning Exploration Techniques which Facilitates Combination of Different Methods"
      },
      {
        "paperId": "72799883b3378309e195a6bbd3c42fcaa88bd962",
        "title": "Efficient Intrinsically Motivated Robotic Grasping with Learning-Adaptive Imagination in Latent Space"
      },
      {
        "paperId": "b3e20a79d802baee478351d111e22e0e513ba376",
        "title": "Efficient Exploration with Self-Imitation Learning via Trajectory-Conditioned Policy"
      },
      {
        "paperId": "855112d5c051616353f11180b9801c0ee09f2fa5",
        "title": "Deriving Subgoals Autonomously to Accelerate Learning in Sparse Reward Domains"
      },
      {
        "paperId": "22005795db3a0e85c9091a855427a39b5f8bb33a",
        "title": "Neural Embedding for Physical Manipulations"
      },
      {
        "paperId": "49a05c4f0505344317fcdbf08066aebfa1aadc3b",
        "title": "An Intrinsically-Motivated Approach for Learning Highly Exploring and Fast Mixing Policies"
      },
      {
        "paperId": "def8207fb6457f3c2656bac9f2bbf954de1d7de1",
        "title": "Deep Active Inference as Variational Policy Gradients"
      },
      {
        "paperId": "26e17b1a079708ef2985929a287b209367aafb73",
        "title": "Clustering subspace generalization to obtain faster reinforcement learning"
      },
      {
        "paperId": "ffb3886a253ff927bcc46b78e00409893865a68e",
        "title": "Dynamics-Aware Unsupervised Discovery of Skills"
      },
      {
        "paperId": "12e9394725a085a89114d2c981ce4a76ec6f37cf",
        "title": "Efficient and Scalable Exploration via Estimation-Error"
      },
      {
        "paperId": "c6011b09783150bcda711c8d3176385875020a60",
        "title": "\u00c9tude de la motivation intrins\u00e8que en apprentissage par renforcement"
      },
      {
        "paperId": "8c07f69db199264c53accf45cb39fc8aa1ea632f",
        "title": "Reward Prediction Error as an Exploration Objective in Deep RL"
      },
      {
        "paperId": "8e07d0452bf09c0abb5a6df5b3315a9989fdf040",
        "title": "QXplore: Q-learning Exploration by Maximizing Temporal Difference Error"
      },
      {
        "paperId": "c6b8d35da220d274b561bc0affa12810990e720a",
        "title": "Adapting Behaviour via Intrinsic Reward: A Survey and Empirical Study"
      },
      {
        "paperId": "39e299cde9053346284b074d8ca58e083ad85cbd",
        "title": "Learning-Driven Exploration for Reinforcement Learning"
      },
      {
        "paperId": "cfec54f9208dcbb2c558aa7ff45f57be2533d046",
        "title": "Efficient Exploration via State Marginal Matching"
      },
      {
        "paperId": "5782b218017316542ec695cab39c0eca709e5733",
        "title": "Clustered Reinforcement Learning"
      },
      {
        "paperId": "bb4c48e06dcf16dad1a9caca9da3ef1c077feaa5",
        "title": "Sequence Modeling of Temporal Credit Assignment for Episodic Reinforcement Learning"
      },
      {
        "paperId": "81768cdace11e14982d3aba9060059e1133d83fc",
        "title": "Learning Efficient and Effective Exploration Policies with Counterfactual Meta Policy"
      },
      {
        "paperId": "47231dec371ac92a5caabf64508ed5332cf7e4f8",
        "title": "Beyond Exponentially Discounted Sum: Automatic Learning of Return Function"
      },
      {
        "paperId": "801eb90352e603b1573a04a36804417d1c9496f0",
        "title": "Learning latent state representation for speeding up exploration"
      },
      {
        "paperId": "c42037adbfaa3b8727aa2d08531a1b244f9bbb84",
        "title": "Flow-based Intrinsic Curiosity Module"
      },
      {
        "paperId": "c60d789bf93dee52fc1e076c005cfb8385c84719",
        "title": "Curiosity-Bottleneck: Exploration By Distilling Task-Specific Novelty"
      },
      {
        "paperId": "0efe0f9c0cbc79a6e651af89ed15cc25e850ffc1",
        "title": "Exploration via Flow-Based Intrinsic Rewards"
      },
      {
        "paperId": "d369d15aacb9a0fce14ac5ac060489f9007aabdd",
        "title": "Path Planning via an Improved DQN-Based Learning Policy"
      },
      {
        "paperId": "d6285ff3dcb15c1da84dcbc98141be10ef0e8dd1",
        "title": "Estimating Risk and Uncertainty in Deep Reinforcement Learning"
      },
      {
        "paperId": "b91f6a10375e7064ca57400ad265114b8365f0ed",
        "title": "Curious Meta-Controller: Adaptive Alternation between Model-Based and Model-Free Control in Deep Reinforcement Learning"
      },
      {
        "paperId": "5eeb867f38b4b901f0e0081baa2bea8f118d0f01",
        "title": "An exploratory rollout policy for imagination-augmented agents"
      },
      {
        "paperId": "b89f8e1b6eeb4639a34c8b287fdf0c7ea560be79",
        "title": "Curiosity-driven Reinforcement Learning for Dialogue Management"
      },
      {
        "paperId": "e739403b08e22f4846e17fe8b2a1a9eb7b568d62",
        "title": "Learning Action Representations for Self-supervised Visual Exploration"
      },
      {
        "paperId": "13a0023f890af535dd3b7488fde9a4bde2f55bc8",
        "title": "Learning Good Representation via Continuous Attention"
      },
      {
        "paperId": "39905a5b13b41b47c3f6dfb48412f278c53f167f",
        "title": "Learning Gentle Object Manipulation with Curiosity-Driven Deep Reinforcement Learning"
      },
      {
        "paperId": "8ed7de1196c2ef42d608d6081c5964c1856837c3",
        "title": "Probabilistic Policy Reuse for Safe Reinforcement Learning"
      },
      {
        "paperId": "7388826b5ee00efe17cb7f19a623d9b5e955ae70",
        "title": "Skew-Fit: State-Covering Self-Supervised Reinforcement Learning"
      },
      {
        "paperId": "48182d7620a9278d7e9cd880a961fa14d22a0281",
        "title": "Learning Exploration Policies for Navigation"
      },
      {
        "paperId": "c372396db5db3ea5d1acc1255e79791c51dfc959",
        "title": "Learning Dynamics Model in Reinforcement Learning by Incorporating the Long Term Future"
      },
      {
        "paperId": "16ca034b434c15e1da6eef50a67264137e1380bd",
        "title": "World Discovery Models"
      },
      {
        "paperId": "ee248883b124a80bdbbc686e906145ccdeadc834",
        "title": "Competitive Experience Replay"
      },
      {
        "paperId": "62392a49bb2be0ea1b23dc85ea744c95dd4ea180",
        "title": "Bayesian Optimisation for Planning And Reinforcement Learning"
      },
      {
        "paperId": "c520bf47db3360ae3a52219771390a354ed8a91f",
        "title": "Go-Explore: a New Approach for Hard-Exploration Problems"
      },
      {
        "paperId": "72ecfb12938f044afdd2d9191c8e61386e2559bd",
        "title": "Never Forget: Balancing Exploration and Exploitation via Learning Optical Flow"
      },
      {
        "paperId": "10d7c9b42899106904e552e6ec74ca4b457e876c",
        "title": "Amplifying the Imitation Effect for Reinforcement Learning of UCAV's Mission Execution"
      },
      {
        "paperId": "1076bd3b56411d4b0291a0333c1ae5348353180e",
        "title": "Exploration Conscious Reinforcement Learning Revisited"
      },
      {
        "paperId": "d1fc7377f408b23fc755cd55d232aca93645da82",
        "title": "Revisiting Exploration-Conscious Reinforcement Learning"
      },
      {
        "paperId": "4d21334ea3564c89586e1ba176e11382bdd3d394",
        "title": "Learning to Learn How to Learn: Self-Adaptive Visual Navigation Using Meta-Learning"
      },
      {
        "paperId": "ab1c6b333aa773274ff13f3cd98a171e165bb7d0",
        "title": "Perspective: Uniform switching of artificial synapses for large-scale neuromorphic arrays"
      },
      {
        "paperId": "4b61c25a86083c20730c9b12737ac6ac4178c364",
        "title": "An Introduction to Deep Reinforcement Learning"
      },
      {
        "paperId": "557a51eade0e02b4e3261658f3806cac4fc87f03",
        "title": "\u0418\u043d\u0442\u0435\u043b\u043b\u0435\u043a\u0442\u0443\u0430\u043b\u044c\u043d\u0430\u044f \u043c\u043e\u0431\u0438\u043b\u044c\u043d\u0430\u044f \u0441\u0438\u0441\u0442\u0435\u043c\u0430 \u043d\u0430\u0431\u043b\u044e\u0434\u0435\u043d\u0438\u044f \u0437\u0430 \u0441\u0440\u0435\u0434\u043e\u0439 \u0432 \u0443\u0441\u043b\u043e\u0432\u0438\u044f\u0445 \u043f\u043e\u043c\u0435\u0449\u0435\u043d\u0438\u0439"
      },
      {
        "paperId": "c7b80441bc6c0235d386a5318abf8786bd133634",
        "title": "Deep intrinsically motivated continuous actor-critic for efficient robotic visuomotor skill learning"
      },
      {
        "paperId": "233a1b9d9311e12e7319717d40487b201d5c4548",
        "title": "Bayesian RL for Goal-Only Rewards"
      },
      {
        "paperId": "d69fa24c177d7861bc648b43f809daab245d4071",
        "title": "Empowerment-driven Exploration using Mutual Information Estimation"
      },
      {
        "paperId": "bf2a70ecfe3a0b701a16104cd8b880a41353dd2f",
        "title": "Scaling All-Goals Updates in Reinforcement Learning Using Convolutional Neural Networks"
      },
      {
        "paperId": "51ecd565f8a394b1907b3d37722435a573ce7b72",
        "title": "EMI: Exploration with Mutual Information"
      },
      {
        "paperId": "fdfeeb14bbde2ab31b18e56b92d362dcd1b14f71",
        "title": "Episodic Curiosity through Reachability"
      },
      {
        "paperId": "4cb3fd057949624aa4f0bbe7a6dcc8777ff04758",
        "title": "Exploration by Random Network Distillation"
      },
      {
        "paperId": "a25b645c3d24f91164230a0ac5bb2d4ec88c1538",
        "title": "Information-Directed Exploration for Deep Reinforcement Learning"
      },
      {
        "paperId": "6a9013a8cdd84e423223f76a903028011c84c4ab",
        "title": "Plan Online, Learn Offline: Efficient Learning and Exploration via Model-Based Control"
      },
      {
        "paperId": "c30a6192b6cbb665aefe837230da1bcfbbf42153",
        "title": "Curiosity-Driven Experience Prioritization via Density Estimation"
      },
      {
        "paperId": "e784986bcb4698de5be6317e5871d93b5af1cfb7",
        "title": "Q-map: a Convolutional Approach for Goal-Oriented Reinforcement Learning"
      },
      {
        "paperId": "68c2bae04f08ffeb54c68afd436272ee459b1f79",
        "title": "EMI: Exploration with Mutual Information Maximizing State and Action Embeddings"
      },
      {
        "paperId": "0eaa2616bf65233eb3a5ea63262913015914dce7",
        "title": "Model-Based Regularization for Deep Reinforcement Learning with Transcoder Networks"
      },
      {
        "paperId": "a2c145dbb30c942f136b2525a0132bf72d51b0c2",
        "title": "Model-Based Stabilisation of Deep Reinforcement Learning"
      },
      {
        "paperId": "ca14dce53be20d3d23d4f0db844a8389ab619db3",
        "title": "Large-Scale Study of Curiosity-Driven Learning"
      },
      {
        "paperId": "9f67b3edc67a35c884bd532a5e73fa3a7f3660d8",
        "title": "Count-Based Exploration with the Successor Representation"
      },
      {
        "paperId": "5f8645a8474017f52e4d1d4b4a0ca95d8b39f66f",
        "title": "Variational Option Discovery Algorithms"
      },
      {
        "paperId": "563dac292e449d5b602719845bcda3787c075c15",
        "title": "Variational Bayesian Reinforcement Learning with Regret Bounds"
      },
      {
        "paperId": "83040001210751239553269727b9ea53e152af71",
        "title": "Building Machines that Learn and Think Like People"
      },
      {
        "paperId": "22039881e6b36bee905f7a9d35d8de7c5ac2ef76",
        "title": "Goal-oriented Trajectories for Efficient Exploration"
      },
      {
        "paperId": "73abd0733e0125c123e6a0b23472772a9c343e5f",
        "title": "Region Growing Curriculum Generation for Reinforcement Learning"
      },
      {
        "paperId": "214799f90dcf59c22fccec2e68c20cf9bf6ab37a",
        "title": "Revisiting the Arcade Learning Environment: Evaluation Protocols and Open Problems for General Agents (Extended Abstract)"
      },
      {
        "paperId": "0d03470691ccc9be612c3e3a842a9a7b50f78087",
        "title": "Adversarial Active Exploration for Inverse Dynamics Model Learning"
      },
      {
        "paperId": "3a5e2201b23232413b7cdef998aca7379be17b72",
        "title": "Computer Science & Information Technology"
      },
      {
        "paperId": "d397f4cf400f6ffcb1b8e3db27bb75966a0513cf",
        "title": "Self-Imitation Learning"
      },
      {
        "paperId": "b93317f61c6ed99542da9d1d691ded9732c16c1c",
        "title": "Unsupervised Meta-Learning for Reinforcement Learning"
      },
      {
        "paperId": "0f710daa7bbba3350169f0bbb5d24f8db3e5199e",
        "title": "Self-Consistent Trajectory Autoencoder: Hierarchical Reinforcement Learning with Trajectory Embeddings"
      },
      {
        "paperId": "0046675571f67bda5ab8f2cfed9d79fc8b2b7035",
        "title": "DAQN: Deep Auto-encoder and Q-Network"
      },
      {
        "paperId": "6879ecec797cd0f1319b54a963f91abb5f7325de",
        "title": "Randomized Value Functions via Multiplicative Normalizing Flows"
      },
      {
        "paperId": "653bdfb3c35621ee04ee5d5253dc7e3a422d69e1",
        "title": "Learning Self-Imitating Diverse Policies"
      },
      {
        "paperId": "06a6f890f1ae3215b3ecf972858950915a3843a9",
        "title": "Searching for Rewards Like a Child Means Less Generalization and More Directed Exploration"
      },
      {
        "paperId": "9ed96ed9df3f24f65261096ea76a9b22a0fc2a64",
        "title": "Residential scene classification for gridded population sampling in developing countries using deep convolutional neural networks on satellite imagery"
      },
      {
        "paperId": "92027f3111b968cde0d2d15e2c7ca087b161d53d",
        "title": "Review of Intrinsic Motivation in Simulation-based Game Testing"
      },
      {
        "paperId": "12cd422b0e5377d05a48103797d7bbe63b3867a0",
        "title": "Reinforcement Learning for Determining Spread Dynamics of Spatially Spreading Processes with Emphasis on Forest Fires"
      },
      {
        "paperId": "35271d36cb20bf8d716e79c9dd15d738d955a931",
        "title": "On Learning Intrinsic Rewards for Policy Gradient Methods"
      },
      {
        "paperId": "0a1a05f4b14e4a90534fe7113e0787e0693984d7",
        "title": "Information Maximizing Exploration with a Latent Dynamics Model"
      },
      {
        "paperId": "7f567f1e8972ff31a7ced59c329e7d75da645baf",
        "title": "Learning to Explore with Meta-Policy Gradient"
      },
      {
        "paperId": "68c108795deef06fa929d1f6e96b75dbf7ce8531",
        "title": "Meta-Reinforcement Learning of Structured Exploration Strategies"
      },
      {
        "paperId": "b80991d12b41a5a68dc14dd87b692c0f903ceb9c",
        "title": "Some Considerations on Learning to Explore via Meta-Reinforcement Learning"
      },
      {
        "paperId": "3c3093f5f8e9601637612fcfb8f160f116fa30e4",
        "title": "Diversity-Driven Exploration Strategy for Deep Reinforcement Learning"
      },
      {
        "paperId": "2064020586d5832b55f80a7dffea1fd90a5d94dd",
        "title": "Improving Exploration in Evolution Strategies for Deep Reinforcement Learning via a Population of Novelty-Seeking Agents"
      },
      {
        "paperId": "a73ed8431fe1a41acefcae36f513fc891a6fb38d",
        "title": "Self-Augmenting Strategy for Reinforcement Learning"
      },
      {
        "paperId": "6f40b8d2e73ec0bcc4d12ec2c7c8387d53c7727b",
        "title": "Efficient exploration with Double Uncertain Value Networks"
      },
      {
        "paperId": "3ed43560278063489e5c9da084694a0c8cf03853",
        "title": "Vertical Ensemble Co-Training for Text Classification"
      },
      {
        "paperId": "0ab3f7ecbdc5a33565a234215604a6ca9d155a33",
        "title": "Rainbow: Combining Improvements in Deep Reinforcement Learning"
      },
      {
        "paperId": "7056e205e6829b3dc789ec35eac82f6e31923c2c",
        "title": "Exploration in Feature Space for Reinforcement Learning"
      },
      {
        "paperId": "3b290ffa1f4f8226e326f00984acecdfbe9e28bf",
        "title": "Revisiting the Arcade Learning Environment: Evaluation Protocols and Open Problems for General Agents"
      },
      {
        "paperId": "2405c131d0807c7190b253b8e0524df20b8c633d",
        "title": "Shared Learning : Enhancing Reinforcement in $Q$-Ensembles"
      },
      {
        "paperId": "c0cb3236721495432f5a80baaa8f886f7d52001f",
        "title": "Curiosity-driven exploration enhances motor skills of continuous actor-critic learner"
      },
      {
        "paperId": "323ec49b7d3619d146bc9263e680ced7cab94d9c",
        "title": "Deep deformable Q-Network: an extension of deep Q-Network"
      },
      {
        "paperId": "498238a3bd5fd322fc3ce1572e33bbe3853a356f",
        "title": "Deep Reinforcement Learning: A Brief Survey"
      },
      {
        "paperId": "cf020b27d06efb28f3e5db264aceeec1f397817b",
        "title": "Value Prediction Network"
      },
      {
        "paperId": "6b71505aadb185a5a671e0a0003a56f1aaaad9a8",
        "title": "Hashing Over Predicted Future Frames for Informed Exploration of Deep Reinforcement Learning"
      },
      {
        "paperId": "fb9b0a6e88ca6e3cef9fc6ba060b27c5303da258",
        "title": "Teacher\u2013Student Curriculum Learning"
      },
      {
        "paperId": "0f810eb4777fd05317951ebaa7a3f5835ee84cf4",
        "title": "Count-Based Exploration in Feature Space for Reinforcement Learning"
      },
      {
        "paperId": "142497432fe179ddb6ffe600c64a837ec6179550",
        "title": "Parameter Space Noise for Exploration"
      },
      {
        "paperId": "225ab689f41cef1dc18237ef5dab059a49950abf",
        "title": "Curiosity-Driven Exploration by Self-Supervised Prediction"
      },
      {
        "paperId": "0e76e8d623882945ef8891a02fc01b3675fd1222",
        "title": "Learning Multimodal Transition Dynamics for Model-Based Reinforcement Learning"
      },
      {
        "paperId": "44d793a263624e26b80c5c72fc150fe3998a5f6c",
        "title": "Bayesian learning for data-efficient control"
      },
      {
        "paperId": "5349a76597082ee5917feb2e4fd027fa02926c4f",
        "title": "On Improving Deep Reinforcement Learning for POMDPs"
      },
      {
        "paperId": "66bf51dfb04c0c6b957fb079b5b47611887c273d",
        "title": "Micro-Objective Learning : Accelerating Deep Reinforcement Learning through the Discovery of Continuous Subgoals"
      },
      {
        "paperId": "d6757aedcb53142bc439ec64bfd0b056d99b1881",
        "title": "Surprise-Based Intrinsic Motivation for Deep Reinforcement Learning"
      },
      {
        "paperId": "4eb38b3460606a4042b04fc52d0044ab948b4a17",
        "title": "EX2: Exploration with Exemplar Models for Deep Reinforcement Learning"
      },
      {
        "paperId": "96a067e188f1c89db9faea1fea2314a15ae51bbc",
        "title": "Bridging the Gap Between Value and Policy Based Reinforcement Learning"
      },
      {
        "paperId": "f152be084cc22a5ffd0838956b1b4d3515d38db7",
        "title": "Soft Label Memorization-Generalization for Natural Language Inference"
      },
      {
        "paperId": "2844495557b5eba17227d83324cfe614d52e7afd",
        "title": "Improving Machine Learning Ability with Fine-Tuning"
      },
      {
        "paperId": "d4f30336c52749a5c1304bdb09cb1004afb6cbb3",
        "title": "Towards a common implementation of reinforcement learning for multiple robotic tasks"
      },
      {
        "paperId": "4645a414ee8fcacdf59239d6e7f57df7cf0afb0d",
        "title": "An Analysis of Machine Learning Intelligence"
      },
      {
        "paperId": "9f1e9e56d80146766bc2316efbc54d8b770a23df",
        "title": "Deep Reinforcement Learning: An Overview"
      },
      {
        "paperId": "da7a2974d58c1102c0ab516f1514e58574c1dbc6",
        "title": "Reinforcement Learning via Recurrent Convolutional Neural Networks"
      },
      {
        "paperId": "a89138c99436f0c9e81c3b2c139898ec1b8d52cc",
        "title": "Cooperation and communication in multiagent deep reinforcement learning"
      },
      {
        "paperId": "0fcb2034e31a2bc2f12a2b1363d0d77baf445fdf",
        "title": "#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning"
      },
      {
        "paperId": "9caf76734a611286513b478ce10557bda477e8ce",
        "title": "Improving Policy Gradient by Exploring Under-appreciated Rewards"
      },
      {
        "paperId": "25e64758698a20cc242b9ece87cb91b3050e3b2d",
        "title": "Multi-Objective Deep Reinforcement Learning"
      },
      {
        "paperId": "f826381aea632791b6007e427a9587c11b239b6a",
        "title": "Efficient Exploration for Dialog Policy Learning with Deep BBQ Networks \\& Replay Buffer Spiking"
      },
      {
        "paperId": "10b85d9ee5c3188d6bddb1cd674d9c6f9566afd7",
        "title": "Efficient Dialogue Policy Learning with BBQ-Networks"
      },
      {
        "paperId": "885fe11ed7ab81c8609ccddb3e10f62577c04ab9",
        "title": "BBQ-Networks: Efficient Exploration in Deep Reinforcement Learning for Task-Oriented Dialogue Systems"
      },
      {
        "paperId": "bc69708aeaae562ab1406ca7dd0e50c1ec247635",
        "title": "Terrain-adaptive locomotion skills using deep reinforcement learning"
      },
      {
        "paperId": "10a4992ece5baea79326a8878a6244eeacbc6af5",
        "title": "Deep Successor Reinforcement Learning"
      },
      {
        "paperId": "6e90fd78e8a3b98af3954aae5209703aa966603e",
        "title": "Unifying Count-Based Exploration and Intrinsic Motivation"
      },
      {
        "paperId": "fb3c6456708b0e143f545d77dc8ec804eb947395",
        "title": "Curiosity-driven Exploration in Deep Reinforcement Learning via Bayesian Neural Networks"
      },
      {
        "paperId": "5129a9cbb6de3c6579f6a7d974394d392ac29829",
        "title": "Control of Memory, Active Perception, and Action in Minecraft"
      },
      {
        "paperId": "317cd4522b1f4a6f889743578143bb8823623f8b",
        "title": "VIME: Variational Information Maximizing Exploration"
      },
      {
        "paperId": "8cbafc53a3991758bf668883e53cfdf66d179e7b",
        "title": "Deep Learning for Reward Design to Improve Monte Carlo Tree Search in ATARI Games"
      },
      {
        "paperId": "d37620e6f8fe678a43e12930743281cd8cca6a66",
        "title": "Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation"
      },
      {
        "paperId": "3bbf2ee642ed311e500017def1f54df453a935c1",
        "title": "Dialog-based Language Learning"
      },
      {
        "paperId": "dfdf2dbffd191134c84d67cbc0ec164f1c45cf01",
        "title": "Eight open questions in the computational modeling of higher sensory cortex"
      },
      {
        "paperId": "5f0625c30014c12f333eb518268647673d18f9f1",
        "title": "What can the brain teach us about building artificial intelligence?"
      },
      {
        "paperId": "2b3ec72786da1853fc379e9494a73bcce88f47b8",
        "title": "Exploratory Gradient Boosting for Reinforcement Learning in Complex Domains"
      },
      {
        "paperId": "94c4ba7246f781632aa68ca5b1acff0fdbb2d92f",
        "title": "Using goal-driven deep learning models to understand sensory cortex"
      },
      {
        "paperId": "4b63e34276aa98d5345efa7fe09bb06d8a9d8f52",
        "title": "Deep Exploration via Bootstrapped DQN"
      },
      {
        "paperId": "45b47d87736cf45924abe9bc4761ca0894de54cd",
        "title": "Learning to Communicate to Solve Riddles with Deep Distributed Recurrent Q-Networks"
      },
      {
        "paperId": "4c05d7caa357148f0bbd61720bdd35f0bc05eb81",
        "title": "Dueling Network Architectures for Deep Reinforcement Learning"
      },
      {
        "paperId": "c6170fa90d3b2efede5a2e1660cb23e1c824f2ca",
        "title": "Prioritized Experience Replay"
      },
      {
        "paperId": "49da57cf2900cd50d64a6d63d45e1bccd454fcbb",
        "title": "Deep Reinforcement Learning in Parameterized Action Space"
      },
      {
        "paperId": "07bc6fdbff0eba74dea992fdc2dbb89f511b06d6",
        "title": "Boosting Policy Learning in Reinforcement Learning via Adaptive Intrinsic Reward Regulation"
      },
      {
        "paperId": "c583ccd3db8e4ce5765a7a55e47bc4fd83b2f4cf",
        "title": "Continual Optimistic Initialization for Value-Based Reinforcement Learning"
      },
      {
        "paperId": "6e7ebdeffb7fd5ce8bbf0d27a7f551b531a75008",
        "title": "Finding and Navigating to Humans in Complex Environments for Assistive Tasks"
      },
      {
        "paperId": "0527e2a4599e4c6d082dff97547ef843af3a68d7",
        "title": "Deep Learning in the Fast Lane: A Survey on Advanced Intrusion Detection Systems for Intelligent Vehicle Networks"
      },
      {
        "paperId": "cad7438bc696e98a3a2a5981ef91333124f7bd50",
        "title": "Learning End-to-End Visual Servoing Using an Improved Soft Actor-Critic Approach With Centralized Novelty Measurement"
      },
      {
        "paperId": "9c9a527ad885e664a31f0b70ec5f838d0c6ebfa6",
        "title": "TA-Explore: Teacher-Assisted Exploration for Facilitating Fast Reinforcement Learning"
      },
      {
        "paperId": "4956e36809a74f2bd3b8018153bccbf19f7a552f",
        "title": "Deep Reinforcement Learning for Autonomous Cyber Operations: A Survey"
      },
      {
        "paperId": "4d1e15f03bc4bd07533ec1b20741134d2efb0608",
        "title": "One Objective to Rule Them All: A Maximization Objective Fusing Estimation and Planning for Exploration"
      },
      {
        "paperId": "b962554058f5f23edebe09d27fa6145d641467fd",
        "title": "Intrinsic Motivation via Surprise Memory"
      },
      {
        "paperId": "a059870910339596c5c57969bc2397a7e268435e",
        "title": "Learning Embodied Agents with Scalably-Supervised Reinforcement Learning"
      },
      {
        "paperId": "8aa70b57229d15f3898ef796e3c398f1e047068e",
        "title": "Learning and Transferring Value Function for Robot Exploration in Subterranean Environments"
      },
      {
        "paperId": "4c4d41a977e6d8d82b6cc19ac28e50de3c814fd8",
        "title": "HyperDQN: A Randomized Exploration Method for Deep Reinforcement Learning"
      },
      {
        "paperId": "3a2bd0e263d5d0fe810952e5431ceeb4de939e03",
        "title": "Mitigating Targeting Bias in Content Recommendation with Causal Bandits"
      },
      {
        "paperId": "cfb55f2d65b84bd62942a19361a6af7af4b3b0ad",
        "title": "Learning Exploration Policies with View-based Intrinsic Rewards"
      },
      {
        "paperId": "60da2965da20122fb30921c2547806200427d1ea",
        "title": "Unsupervised Skill Discovery via Recurrent Skill Training"
      },
      {
        "paperId": "848b8458d36f0e976da8ad59dc7c073987c175e1",
        "title": "Exploration-Guided Reward Shaping for Reinforcement Learning under Sparse Rewards"
      },
      {
        "paperId": "1558a92344ab38b5b650d703166c0c92bc565f55",
        "title": "U NDERSTANDING THE C OMPLEXITY"
      },
      {
        "paperId": "fa7b186382cfc93454830b5d947a4ac1a9453cff",
        "title": "Model-augmented Prioritized Experience Replay"
      },
      {
        "paperId": "dbafc8a6306e3819d7a133ea284d7270b7bd5096",
        "title": "Surprise Minimizing Multi-Agent Learning with Energy-based Models"
      },
      {
        "paperId": "10e81e4a2614a80ff2f3ba058901550637bf47fa",
        "title": "Rethinking Exploration for Sample-Efficient Policy Learning"
      },
      {
        "paperId": "9ade87e97647c20b59894250c84fb4be85941449",
        "title": "Adapting Behaviour via Intrinsic Reward"
      },
      {
        "paperId": "933bbf523f77e3384bde51b1a3a06ac6d6197d9d",
        "title": "Learning to Shape Rewards using a Game of Switching Controls"
      },
      {
        "paperId": "eeb19f4f8fbcc82dc604f7f7f5a4ce83e7224fab",
        "title": "Intrinsic Motivated Multi-Agent Communication"
      },
      {
        "paperId": "b7512a6b69ce35af84c1bd44fffbb165a9e3a723",
        "title": "Exploration Methods in Sparse Reward Environments"
      },
      {
        "paperId": "ac96f9dbb1462339d87b58088488cdf91f9b4f25",
        "title": "R ELEVANT A CTIONS M ATTER : M OTIVATING A GENTS WITH A CTION U SEFULNESS"
      },
      {
        "paperId": "6b6b26a54a9d39b923bec6384d407e2ef374fc98",
        "title": "Theory and Application of Bonus-based Exploration in Reinforcement Learning"
      },
      {
        "paperId": "2e5dc4b028e670d1b9ac0b4e15580cdb7197cb68",
        "title": "Exploration in Deep Reinforcement Learning: A Comprehensive Survey"
      },
      {
        "paperId": "d12e383d3058fa5edcb5591723a7247a8837d687",
        "title": "Hybrid Adversarial Imitation Learning"
      },
      {
        "paperId": "5a55921a9d83049fb0a2c138c706095c1028770d",
        "title": "Gaussian Approximation for Bias Reduction in Q-Learning"
      },
      {
        "paperId": "8d15f17ea8f807efe8801d236b7218b6659ac1d9",
        "title": "NovelD: A Simple yet Effective Exploration Criterion"
      },
      {
        "paperId": "02d1ff7e59fd8a66d3515493b5e63cee0ab8168f",
        "title": "Intrinsic Control of Variational Beliefs in Dynamic Partially-Observed Visual Environments"
      },
      {
        "paperId": "adabcfc03fa110d40d6ab2a613a36dda9c12dfff",
        "title": "Exploration via Empowerment Gain: Combining Novelty, Surprise and Learning Progress"
      },
      {
        "paperId": "c3980c4ce42ebcd493514180897829096c189337",
        "title": "Advances in Knowledge Discovery and Data Mining: 24th Pacific-Asia Conference, PAKDD 2020, Singapore, May 11\u201314, 2020, Proceedings, Part II"
      },
      {
        "paperId": "f26a89180b9bf2f58b8647ea30580fed0ee3ff28",
        "title": "Proximal Policy Optimization with Explicit Intrinsic Motivation"
      },
      {
        "paperId": "f25a69a981384a71f16d3312784e80a3a6369538",
        "title": "DCRAC: Deep Conditioned Recurrent Actor-Critic for Multi-Objective Partially Observable Environments"
      },
      {
        "paperId": "8fb76a141d8e72eef7d6e220ea6556a6c5755756",
        "title": "Learning Intrinsic Rewards as a Bi-Level Optimization Problem"
      },
      {
        "paperId": "07b3f20eb119f167e9e9da9dd8dd6066a337136f",
        "title": "T OWARDS MODELING THE DEVELOPMENTAL VARIABIL - ITY OF HUMAN ATTENTION"
      },
      {
        "paperId": "91329f5b8e60ca064b58649ddac3487b7100ed9b",
        "title": "Artificial Neural Networks and Machine Learning \u2013 ICANN 2020: 29th International Conference on Artificial Neural Networks, Bratislava, Slovakia, September 15\u201318, 2020, Proceedings, Part II"
      },
      {
        "paperId": "10a700d749b11018e97de03ce9c11da6313a930c",
        "title": "Efficient Exploration by Novelty-Pursuit"
      },
      {
        "paperId": "d2862c998d3ab6d24ee224f062427de026dd9ee9",
        "title": "Autonomous Control of Combat Unmanned Aerial Vehicles to Evade Surface-to-Air Missiles Using Deep Reinforcement Learning"
      },
      {
        "paperId": "99846049e6836bbc2189703406f92d91e01b9557",
        "title": "N OISY A GENTS : S ELF - SUPERVISED E XPLORATION BY P REDICTING A UDITORY E VENTS"
      },
      {
        "paperId": "787e775ad8dd53c6ee2843f888d093b248aa9f7d",
        "title": "On Cooperation in Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "4e50d52f0b59561e3902b997ce8a48633351491e",
        "title": "An Empirical Study of Exploration Strategies for Model-Free Reinforcement Learning"
      },
      {
        "paperId": "e04fafb57e40098da2f9d66bbbe0d1e5ee9db531",
        "title": "Generating Novel Policies and Rewards for Continuous Control Tasks with Curiosity Driven Exploration"
      },
      {
        "paperId": "2f2e3e219ac3a149673967d3a6ba231fb299905a",
        "title": "Learning to Generalize via Self-Supervised Prediction"
      },
      {
        "paperId": "8e0eb0eec72f60e259fbbe12d009c7a56ad6c552",
        "title": "ICLR 2019 PPO Egocentric Fine Map Egocentric Coarse Map RGB Image Compute Reward ! \""
      },
      {
        "paperId": "dc9c9592955f46cf333055ecba1e8b8bcc8bdeff",
        "title": "Deep Reinforcement Learning with Prior Knowledge"
      },
      {
        "paperId": "5654028d5193dbf8eaa5ab9ef6af21f6da265878",
        "title": "SKEW-FIT: STATE-COVERING SELF-SUPERVISED RE-"
      },
      {
        "paperId": "e37260307362b869c025d170c5833453e3d1e4ea",
        "title": "An Empirical and Conceptual Categorization of Value-based Exploration Methods"
      },
      {
        "paperId": "532a5086a58746046ac3de191b5adcd86cadb703",
        "title": "Auto-Perceptive Reinforcement Learning (APRiL)"
      },
      {
        "paperId": "e2adb8231150968a8a41216f662f852eeb1685d7",
        "title": "SEQUENCE-LEVEL INTRINSIC EXPLORATION MODEL"
      },
      {
        "paperId": "d295db274b50cdfffa5ca47fbffd8fb26ad7178e",
        "title": "Modeling the visual world: reconstruction and neural episodic representation"
      },
      {
        "paperId": "74559aaf76f2896ffd1e9ec9127405006dd4f8dc",
        "title": "Autonomous Sensing with Scientific Machine Learning for Monitoring Greenhouse Gas Emissions"
      },
      {
        "paperId": "0cf2f0a27e06695a0c433ee9e61d91ebf82c8df2",
        "title": "Approximate Bayesian Reinforcement Learning for System Identification Approximatives Bayesianisches Versta\u0308rkendes Lernen zur Systemidentifikation"
      },
      {
        "paperId": "56c145b5a263a4243a68899088e94fdd8adc3ef0",
        "title": "Enhanced Probabilistic Inference Algorithm Using Probabilistic Neural Networks for Learning Control"
      },
      {
        "paperId": "3bcc491b58acf5bfef0135c668be683cd3444e37",
        "title": "FLOW-BASED INTRINSIC REWARDS"
      },
      {
        "paperId": "72c22ba3ad1002f0bc88ee61d53815dae9f53ea7",
        "title": "Correlated Exploration in Deep Reinforcement Learning"
      },
      {
        "paperId": "a1a536d92551b6690761e36f413f53e8ce923d33",
        "title": "Sense, Think, Grasp: A study on visual and tactile information processing for autonomous manipulation"
      },
      {
        "paperId": "9bd98d3553cb348acdb307722f98e6a4b08b8a89",
        "title": "Winter 12-6-2018 Model-based reinforcement learning : A survey"
      },
      {
        "paperId": "b9859bcf207a1f72353911d0ecc7ace160552360",
        "title": "Transferable Environment Model With Disentangled Dynamics"
      },
      {
        "paperId": "daf93749171e3575cca5b584a4146b247692c86f",
        "title": "Learning and planning in videogames via task decomposition"
      },
      {
        "paperId": "7599db0e2562e2071055e8cd954e1372b707a293",
        "title": "PACE N OISE FOR E XPLORATION"
      },
      {
        "paperId": "77a31a4601444a3f7aeed15061b08684d0bea92b",
        "title": "Exploration-Exploitation Trade-off in Deep Reinforcement Learning"
      },
      {
        "paperId": "40df15ce1de6eb0366179f4726aa55a3e50141fc",
        "title": "Learning to Explore via Meta-Policy Gradient"
      },
      {
        "paperId": "7a4d7fdff84f8f177338fede4d1b443fd7be4442",
        "title": "Improving Asynchronous Advantage Actor Critic with a More Intelligent Exploration Strategy"
      },
      {
        "paperId": "cbf9482140f9e022b0f30581a11bf67526437382",
        "title": "Epitaxial SiGe synapses for neuromorphic arrays"
      },
      {
        "paperId": "98b74fa0345a646b23d96d83c8642f1141e33fbf",
        "title": "Achieving Gentle Manipulation with Deep Reinforcement Learning"
      },
      {
        "paperId": "7cba1fb6edb762c3db8d7b8ab169dbe9c12bb28b",
        "title": "The Importance of Sampling in Meta-Reinforcement Learning"
      },
      {
        "paperId": "8812a090ab59081c4361efa3978cec4c74324469",
        "title": "Computational Sensorimotor Learning"
      },
      {
        "paperId": "0cd50250a858a43607b2c39a208302bbcb4a33db",
        "title": "Exploration strategies for reinforcement learning with function approximation"
      },
      {
        "paperId": "b5c37ac043c84d1fcf33ece2f0c3db2305a3487f",
        "title": "Deep Curiosity Networks"
      },
      {
        "paperId": "5a6b2b9bc3b51ff187826fc2dc21a967e04125ed",
        "title": "Model-based reinforcement learning: A survey"
      },
      {
        "paperId": "aaff69d5535ae91faf9e34d77453220a8633dafd",
        "title": "State of the Art on : Generative models for reinforcement learning"
      },
      {
        "paperId": "b9effa7f0d628ab825f07bee059a3997257348ce",
        "title": "Deep Reinforcement Learning with Skill Library : Exploring with Temporal Abstractions and coarse approximate Dynamics Models"
      },
      {
        "paperId": "643ed38525fa63f35f2456e88f2a68a14cff1068",
        "title": "Exploration and Exploitation in Reinforcement Learning"
      },
      {
        "paperId": "1e9617fed4fa147e07ae68776ef70aaf386b52c9",
        "title": "Efficient Deep Reinforcement Learning via Planning, Generalization, and Improved Exploration"
      },
      {
        "paperId": "f9e7c1f933953c4e71db851dacca2a5e93e8433d",
        "title": "Approaches to Spatial Reasoning in Reinforcement Learning"
      },
      {
        "paperId": "9a46505e124d6470e82eb3cc38803bbf0209d512",
        "title": "Under review as a conference paper at ICLR 2017 # Exploration : A Study of Count-Based Exploration for Deep Reinforcement Learning"
      },
      {
        "paperId": "d1f58798db460996501f224fff6cceada08f59f9",
        "title": "Transferrable Representations for Visual Recognition"
      },
      {
        "paperId": "4f8a6869d646b74b8c677c1777b9e20a3a309a59",
        "title": "Shared Learning in Ensemble Deep Q-Networks"
      },
      {
        "paperId": "05347fea3713c5e52ac7de903105545ae66ab44e",
        "title": "Multi-agent Deep Reinforcement Learning for Task Allocation in Dynamic Environment"
      },
      {
        "paperId": "70df66fad9f1cba284b968a4ef7f5aa1cf2431d7",
        "title": "Parameter Space Noise for Exploration in Deep Reinforcement Learning"
      },
      {
        "paperId": "726551dabd70f4f284dca0b0adf388196d7ee9e2",
        "title": "Di\ufb00erentiable Neural Planners with Temporally Extended Actions"
      },
      {
        "paperId": "6d7a36eeb9b5dd4276de9753c997fc6f5ba99259",
        "title": "Human Learning in Atari"
      },
      {
        "paperId": "d9e4736bdfe5a841f12274202c1c9d124922f063",
        "title": "Deep Learning and Reward Design for Reinforcement Learning"
      },
      {
        "paperId": "29c1b38c3129c60ea6b8737f0d7a461ca3feee4b",
        "title": "Deep Reinforcement Learning with Temporal Abstraction and Intrinsic Motivation"
      },
      {
        "paperId": "817152e291ea2d1696d3f6e18970f51d390f2182",
        "title": "Deep Reinforcement Learning for Continuous Control"
      },
      {
        "paperId": "3e59b3e1e3ef65f9574a0fe30f18ba7a815ea0af",
        "title": "Ef\ufb01cient Exploration for Dialogue Policy Learning with BBQ Networks & Replay Buffer Spiking"
      },
      {
        "paperId": "b26cb2d152690c49c8c81a52beefa789ea543aa7",
        "title": "Generating Music by Fine-Tuning Recurrent Neural Networks with Reinforcement Learning"
      },
      {
        "paperId": "3c623c08329e129e784a5d03f7606ec8feba3a28",
        "title": "Uncertainty in Deep Learning"
      },
      {
        "paperId": "dfc5bfa5874acd04724c9a0c837fadff7bc51269",
        "title": "The importance of experience replay database composition in deep reinforcement learning"
      },
      {
        "paperId": "cc6f79f4dfa9d66dc8c1490ba32f8987b5e6a574",
        "title": "Enhancing Exploration in Actor-Critic Algorithms:An Approach to Incentivize Plausible Novel States"
      },
      {
        "paperId": "123f54f81d8ac2b7079c26e6d99eb22282f36a00",
        "title": "Supplementary Information for 2 Socially Situated Arti\ufb01cial Intelligence Enables Learning from Human Interaction"
      },
      {
        "paperId": "abc06cb12e31b416a7e5709e0e5fe3c0056f918c",
        "title": "Multi-Reward Learning and Sparse Rewards"
      },
      {
        "paperId": "8c39be17f63e310c8052af533d519fa61761de6b",
        "title": "PBCS: E\ufb00icient Exploration and Exploitation Using a Synergy Between Reinforcement Learning and Motion Planning"
      },
      {
        "paperId": "9a0f7481e36b6651c47b7bc39e6271ef0a7c3bf5",
        "title": "Prioritizing Compression Explains Human Perceptual Preferences"
      },
      {
        "paperId": "d677ec4b495d6cbd2454944dbc1124570ce4085b",
        "title": "Continual Optimistic Initialization for Value-Based Reinforcement Learning"
      },
      {
        "paperId": "3d6ffac2498b79d348a6acd94bb621ec8890dcb4",
        "title": "Bayesian Deep Reinforcement Learning \u2014 Tools and Methods \u2014"
      },
      {
        "paperId": "9aab75e7aa0f9fff845d748300d6640aeb77b25f",
        "title": "Theory and Application of Bonus-based Exploration in Reinforcement Learning"
      }
    ],
    "score": 51.2
  },
  {
    "id": "68c108795deef06fa929d1f6e96b75dbf7ce8531",
    "title": "Meta-Reinforcement Learning of Structured Exploration Strategies",
    "authors": [
      "Abhishek Gupta",
      "Russell Mendonca",
      "Yuxuan Liu",
      "P. Abbeel",
      "S. Levine"
    ],
    "year": 2018,
    "citationCount": 357,
    "abstract": "Exploration is a fundamental challenge in reinforcement learning (RL). Many of the current exploration methods for deep RL use task-agnostic objectives, such as information gain or bonuses based on state visitation. However, many practical applications of RL involve learning more than a single task, and prior tasks can be used to inform how exploration should be performed in new tasks. In this work, we explore how prior tasks can inform an agent about how to explore effectively in new situations. We introduce a novel gradient-based fast adaptation algorithm -- model agnostic exploration with structured noise (MAESN) -- to learn exploration strategies from prior experience. The prior experience is used both to initialize a policy and to acquire a latent exploration space that can inject structured stochasticity into a policy, producing exploration strategies that are informed by prior knowledge and are more effective than random action-space noise. We show that MAESN is more effective at learning exploration strategies when compared to prior meta-RL methods, RL without learned exploration strategies, and task-agnostic exploration methods. We evaluate our method on a variety of simulated tasks: locomotion with a wheeled robot, locomotion with a quadrupedal walker, and object manipulation.",
    "url": "https://www.semanticscholar.org/paper/68c108795deef06fa929d1f6e96b75dbf7ce8531",
    "pdf_url": "https://arxiv.org/pdf/1802.07245.pdf",
    "venue": "Neural Information Processing Systems",
    "publicationDate": "2018-02-20",
    "externalIds": {
      "DBLP": "conf/nips/GuptaMLAL18",
      "MAG": "2963176272",
      "ArXiv": "1802.07245",
      "CorpusId": 3418899
    },
    "references": [
      {
        "paperId": "565af8f2ef461b1d7368f3e9899e0f576e4f0a24",
        "title": "Learning an Embedding Space for Transferable Robot Skills"
      },
      {
        "paperId": "b80991d12b41a5a68dc14dd87b692c0f903ceb9c",
        "title": "Some Considerations on Learning to Explore via Meta-Reinforcement Learning"
      },
      {
        "paperId": "2bdebf2fb0f5c21907fcaae6d87c7ba5811e778a",
        "title": "Meta-Learning and Universality: Deep Representations and Gradient Descent can Approximate any Learning Algorithm"
      },
      {
        "paperId": "e010ba3ff5744604cdbfe44a733e2a98649ee907",
        "title": "Learning Complex Dexterous Manipulation with Deep Reinforcement Learning and Demonstrations"
      },
      {
        "paperId": "482c0cbfffa77154e3c879c497f50b605297d5bc",
        "title": "One-Shot Visual Imitation Learning via Meta-Learning"
      },
      {
        "paperId": "d33ad6a25264ba1747d8c93f6621c7f90a7ec601",
        "title": "Meta-SGD: Learning to Learn Quickly for Few Shot Learning"
      },
      {
        "paperId": "00357a417ce470a78f7a84d18ae2604330455d2a",
        "title": "Meta-Learning with Temporal Convolutions"
      },
      {
        "paperId": "3fab4d28be5ff73f42c8b307117aa8d680345ad9",
        "title": "Noisy Networks for Exploration"
      },
      {
        "paperId": "a74d52915f3c9f45d6c66a29f6d17d27d2c3d745",
        "title": "Learning to Learn: Meta-Critic Networks for Sample Efficient Learning"
      },
      {
        "paperId": "142497432fe179ddb6ffe600c64a837ec6179550",
        "title": "Parameter Space Noise for Exploration"
      },
      {
        "paperId": "c269858a7bb34e8350f2442ccf37797856ae9bca",
        "title": "Prototypical Networks for Few-shot Learning"
      },
      {
        "paperId": "c889d6f98e6d79b89c3a6adf8a921f88fa6ba518",
        "title": "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks"
      },
      {
        "paperId": "470d11b8ca4586c930adbbfc3f60bff08f2a0161",
        "title": "Meta Networks"
      },
      {
        "paperId": "9172cd6c253edf7c3a1568e03577db20648ad0c4",
        "title": "Reinforcement Learning with Deep Energy-Based Policies"
      },
      {
        "paperId": "282a380fb5ac26d99667224cef8c630f6882704f",
        "title": "Learning to reinforcement learn"
      },
      {
        "paperId": "3deecaee4ec1a37de3cb10420eaabff067669e17",
        "title": "Stochastic Neural Networks for Hierarchical Reinforcement Learning"
      },
      {
        "paperId": "29c887794eed2ca9462638ff853e6fe1ab91d5d8",
        "title": "Optimization as a Model for Few-Shot Learning"
      },
      {
        "paperId": "954b01151ff13aef416d27adc60cd9a076753b1a",
        "title": "RL$^2$: Fast Reinforcement Learning via Slow Reinforcement Learning"
      },
      {
        "paperId": "3904315e2eca50d0086e4b7273f7fd707c652230",
        "title": "Meta-Learning with Memory-Augmented Neural Networks"
      },
      {
        "paperId": "71683e224ab91617950956b5005ed0439a733a71",
        "title": "Learning to learn by gradient descent by gradient descent"
      },
      {
        "paperId": "be1bb4e4aa1fcf70281b4bd24d8cd31c04864bb6",
        "title": "Matching Networks for One Shot Learning"
      },
      {
        "paperId": "6e90fd78e8a3b98af3954aae5209703aa966603e",
        "title": "Unifying Count-Based Exploration and Intrinsic Motivation"
      },
      {
        "paperId": "317cd4522b1f4a6f889743578143bb8823623f8b",
        "title": "VIME: Variational Information Maximizing Exploration"
      },
      {
        "paperId": "1464776f20e2bccb6182f183b5ff2e15b0ae5e56",
        "title": "Benchmarking Deep Reinforcement Learning for Continuous Control"
      },
      {
        "paperId": "4b63e34276aa98d5345efa7fe09bb06d8a9d8f52",
        "title": "Deep Exploration via Bootstrapped DQN"
      },
      {
        "paperId": "024006d4c2a89f7acacc6e4438d156525b60a98f",
        "title": "Continuous control with deep reinforcement learning"
      },
      {
        "paperId": "2470fcf0f89082de874ac9133ccb3a8667dd89a8",
        "title": "Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models"
      },
      {
        "paperId": "b6b8a1b80891c96c28cc6340267b58186157e536",
        "title": "End-to-End Training of Deep Visuomotor Policies"
      },
      {
        "paperId": "340f48901f72278f6bf78a04ee5b01df208cc508",
        "title": "Human-level control through deep reinforcement learning"
      },
      {
        "paperId": "449532187c94af3dd3aa55e16d2c50f7854d2199",
        "title": "Trust Region Policy Optimization"
      },
      {
        "paperId": "fe859f7e498ad093721c795b70d2e8380f980ce3",
        "title": "Exploration in Model-based Reinforcement Learning by Empirically Estimating Learning Progress"
      },
      {
        "paperId": "ab867c140d2947511979c87e7ae580d9d3f0aeab",
        "title": "An Empirical Evaluation of Thompson Sampling"
      },
      {
        "paperId": "237a1cf18ed83bb3ad852b34f443c6c1ff3336c1",
        "title": "An analysis of model-based Interval Estimation for Markov Decision Processes"
      },
      {
        "paperId": "64a9fea05ff78283f05f396f2ce68e8ee7ccbf2c",
        "title": "Learning omnidirectional path following using dimensionality reduction"
      },
      {
        "paperId": "8570302f7b63e8fcf87030f556b065fd8c260021",
        "title": "Linearly-solvable Markov decision problems"
      },
      {
        "paperId": "12d6fde053e2c7174a76fe1bbdb97dd039a3b662",
        "title": "Intrinsically Motivated Reinforcement Learning"
      },
      {
        "paperId": "dc649486b881e672eea6546da48c46e1f98daf32",
        "title": "Near-Optimal Reinforcement Learning in Polynomial Time"
      },
      {
        "paperId": "e526a65b9ef5afb6639fd3a062f4045d24448232",
        "title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning"
      },
      {
        "paperId": "44b6da0cd36c0fa2f7d3485c6dc0b6d2fbe379bb",
        "title": "Learning how to learn"
      },
      {
        "paperId": null,
        "title": "Reward Functions While training all tasks we used dense reward functions to enable meta-training as described in Section 5 of the paper"
      },
      {
        "paperId": null,
        "title": "Our policies were all feedforward policies of 2 layers, with a hundred units each and ReLU nonlinearities. We performed meta-training for a single step of adaptation"
      },
      {
        "paperId": "f216444d4f2959b4520c61d20003fa30a199670a",
        "title": "Siamese Neural Networks for One-Shot Image Recognition"
      },
      {
        "paperId": "8784f905f4f9fb6fa4a3cc9b0faa5b5479c687ec",
        "title": "On the Optimization of a Synaptic Learning Rule"
      },
      {
        "paperId": "4c915c1eecb217c123a36dc6d3ce52d12c742614",
        "title": "Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning"
      },
      {
        "paperId": "c7dbf5ed7e9b63e104adb4e18bfbc98f5d6afdae",
        "title": "R-MAX - A General Polynomial Time Algorithm for Near-Optimal Reinforcement Learning"
      },
      {
        "paperId": "bdaec1b3eb9a8f7a2b296be009a148c35236f3ce",
        "title": "Evolutionary principles in self-referential learning, or on learning how to learn: The meta-meta-. hook"
      },
      {
        "paperId": "a6bcf0e9b5034c4c9cbea839baadd69a42b05cc1",
        "title": "Learning curves for stochastic gradient descent in linear feedforward networks"
      }
    ],
    "cited_by": [
      {
        "paperId": "62ad04f000834aa66cc19a7a81983d5f7a8227ff",
        "title": "Scenario-Free Autonomous Driving With Multi-Task Offline-to-Online Reinforcement Learning"
      },
      {
        "paperId": "e5af57591d1bcfdb9b9cbd81dc49b2e6753995cc",
        "title": "Robot Trains Robot: Automatic Real-World Policy Adaptation and Learning for Humanoids"
      },
      {
        "paperId": "86abe4d4ffb239fc73e8eedadf197a6e2576a210",
        "title": "Large Model Empowered Embodied AI: A Survey on Decision-Making and Embodied Learning"
      },
      {
        "paperId": "dc99b68951c899bb62fc46ff2a135fd1bd0b5541",
        "title": "Toward Adaptive and Coordinated Transportation Systems: A Multi-Personality Multi-Agent Meta-Reinforcement Learning Framework"
      },
      {
        "paperId": "3e72fffc559b040122230defd142d3231ccb2fa3",
        "title": "Meta-ETI: Meta-Reinforcement Learning With Explicit Task Inference for AAV-IoT Coverage"
      },
      {
        "paperId": "846f52edcdee8534b9bd2a97e13357526f439987",
        "title": "Learning Task Belief Similarity with Latent Dynamics for Meta-Reinforcement Learning"
      },
      {
        "paperId": "b61a64b24ddaf5fb83f8c0cd0c48dd9c4e96d47f",
        "title": "Self-Adapting Language Models"
      },
      {
        "paperId": "ff76e9e076f4aa1b6fb9b1826297553973a54dc4",
        "title": "e3: Learning to Explore Enables Extrapolation of Test-Time Compute for LLMs"
      },
      {
        "paperId": "3d75feab06a7e629c87720577cf6a96e69b92d69",
        "title": "SeqMvRL: A Sequential Fusion Framework for Multi-view Representation Learning"
      },
      {
        "paperId": "46fc4220d573cf98470c792e908bab9d07713630",
        "title": "MCA-Bench: A Multimodal Benchmark for Evaluating CAPTCHA Robustness Against VLM-based Attacks"
      },
      {
        "paperId": "548e1788c6cf36c9a4334f3a964f6df98942d515",
        "title": "Bayesian Meta-Reinforcement Learning with Laplace Variational Recurrent Networks"
      },
      {
        "paperId": "e4ba644f7a50bb6ea59be876ccf9c313fdd0539e",
        "title": "Your Offline Policy is Not Trustworthy: Bilevel Reinforcement Learning for Sequential Portfolio Optimization"
      },
      {
        "paperId": "4046e8f4ac46a533f67aa9432609c150aba5faff",
        "title": "Real-Time Verification of Embodied Reasoning for Generative Skill Acquisition"
      },
      {
        "paperId": "db7f60ab5da3c84a47f43bb13b5a2a9e96dcd90d",
        "title": "Enerdast: a Free Energy Based Mechanism for Strategy Tuning in Sparse Reward Scenarios"
      },
      {
        "paperId": "39fad7204f487586e8261f4c5212c5e19faf51a7",
        "title": "Fast and Robust: Task Sampling with Posterior and Diversity Synergies for Adaptive Decision-Makers in Randomized Environments"
      },
      {
        "paperId": "8c41da5ef4a5a54becc8c1e45a96d155c22b48c3",
        "title": "Proxy-Anchor and EVT-Driven Continual Learning Method for Generalized Category Discovery"
      },
      {
        "paperId": "428062f1f55560adf3ab20bb2c518e19b53e38fb",
        "title": "A GAN enhanced meta-deep reinforcement learning approach for DCN routing optimization"
      },
      {
        "paperId": "55287d882523480b9e1bb09729f6f556a1abd79d",
        "title": "Learning Nonlinear Activation Functions in RL Through Evolutionary Computation"
      },
      {
        "paperId": "9f7bb45a82f361a878df70d5389ab44c109a2e95",
        "title": "Managing and Controlling Innovation in the 21st Century Using Artificial Intelligence"
      },
      {
        "paperId": "eac5e8965f20425cbe6e9ef689792cccc1ccbcbd",
        "title": "Optimizing Test-Time Compute via Meta Reinforcement Fine-Tuning"
      },
      {
        "paperId": "2e99f62aae6f722b11dbd49c140a6046a0861689",
        "title": "Learning Policy Committees for Effective Personalization in MDPs with Diverse Tasks"
      },
      {
        "paperId": "981063181df8255e41f153907458fa657e6ba647",
        "title": "Global\u2013Local Decomposition of Contextual Representations in Meta-Reinforcement Learning"
      },
      {
        "paperId": "cf73b9b5d16f2b90e2805acce73c38e76011e0d0",
        "title": "TIMRL: A Novel Meta-Reinforcement Learning Framework for Non-Stationary and Multi-Task Environments"
      },
      {
        "paperId": "ea50f0eddb0558041e10055855c14cf68d32bc0c",
        "title": "State of Art of Decision Making Methods Based on Reinforcement Learning"
      },
      {
        "paperId": "0ec0bb4545779144c8b69de133c1373b7299a202",
        "title": "Knowledge-Reinforced Cross-Domain Recommendation"
      },
      {
        "paperId": "dd2f739ddeab7f8f5e84d6a355c63c104052a387",
        "title": "The Development and Future Challenges of the Multi-armed Bandit Algorithm"
      },
      {
        "paperId": "5c6f2a5aa37d74e5ef7c1e77c0cf331e955eef6d",
        "title": "Neuromodulated Meta-Learning"
      },
      {
        "paperId": "3da26d9d73644bb43c93727fe58190fb053817b8",
        "title": "SCORE: Simple Contrastive Representation and Reset-Ensemble for offline meta-reinforcement learning"
      },
      {
        "paperId": "acd370650c2bd09fd8f67abc3e87217e25ad7e82",
        "title": "D-MARL: A Dynamic Communication-Based Action Space Enhancement for Multi Agent Reinforcement Learning Exploration of Large Scale Unknown Environments"
      },
      {
        "paperId": "77b71b16ceed3fb0ce657a0290113540e81dc07b",
        "title": "C-MORL: Multi-Objective Reinforcement Learning through Efficient Discovery of Pareto Front"
      },
      {
        "paperId": "8dcdbef8a0694baeed0dd61408c0961ace783157",
        "title": "Adaptive meta-reinforcement learning for AUVs 3D guidance and control under unknown ocean currents"
      },
      {
        "paperId": "1e86f26758c0823b85412d23b781ef8c9415790a",
        "title": "Unsupervised Meta-Testing With Conditional Neural Processes for Hybrid Meta-Reinforcement Learning"
      },
      {
        "paperId": "890cf60419b22089665ca440bdc1d35d12e6785b",
        "title": "From efficiency to equity: A multi-user paradigm in mobile route optimization"
      },
      {
        "paperId": "0171da7048b4616224044dbbc29556bcae40e987",
        "title": "Lifelong Reinforcement Learning via Neuromodulation"
      },
      {
        "paperId": "42345d95ef489bc0d510a7c5fddbe198a3a6a164",
        "title": "Certifiably Robust Policies for Uncertain Parametric Environments"
      },
      {
        "paperId": "83e4b1c286facab374e432d621106b7ee5f4976b",
        "title": "Black box meta-learning intrinsic rewards for sparse-reward environments"
      },
      {
        "paperId": "d91e3068b8b30868d5eb171d06fbe4fefa27c136",
        "title": "Constrained Meta Agnostic Reinforcement Learning"
      },
      {
        "paperId": "15a666fcd1557394d922b7ff2cabdbd77b914936",
        "title": "Pretraining Decision Transformers with Reward Prediction for In-Context Multi-task Structured Bandit Learning"
      },
      {
        "paperId": "812dc119865060ef169facd70f7b9151ce7b3583",
        "title": "Perturbing the Gradient for Alleviating Meta Overfitting"
      },
      {
        "paperId": "015562270ff19914fec984aa2a653808ffc95150",
        "title": "Composable Interaction Primitives: A Structured Policy Class for Efficiently Learning Sustained-Contact Manipulation Skills"
      },
      {
        "paperId": "63eb1afe96024c84bd4ba178842e615dae872475",
        "title": "MESA: Cooperative Meta-Exploration in Multi-Agent Learning through Exploiting State-Action Space Structure"
      },
      {
        "paperId": "ed8246ebe105c0b52441fb5e25efbc3260354605",
        "title": "Sequential Decision Making with Expert Demonstrations under Unobserved Heterogeneity"
      },
      {
        "paperId": "5a43a8b91973777bb78a19b0ed7f0a3011c4da19",
        "title": "Learning-based methods for adaptive informative path planning"
      },
      {
        "paperId": "6198c820f648c790622ae7ec24658acd86a2ee76",
        "title": "Supervised Meta-Reinforcement Learning With Trajectory Optimization for Manipulation Tasks"
      },
      {
        "paperId": "1bf0f04a8cb9f6c39dddff4ed206591b617b2912",
        "title": "MetaRLEC: Meta-Reinforcement Learning for Discovery of Brain Effective Connectivity"
      },
      {
        "paperId": "5301ef92d5d9d5f2ae0771db8e5b1ea66ea96788",
        "title": "Deep Reinforcement Learning for Modelling Protein Complexes"
      },
      {
        "paperId": "53db22a9d4ae77dd8218ba867184898adc84d1d1",
        "title": "Sample Efficient Offline-to-Online Reinforcement Learning"
      },
      {
        "paperId": "0a30481695a069ed664bda52d8a33f979d9a1854",
        "title": "Meta Reinforcement Learning for Multi-Task Offloading in Vehicular Edge Computing"
      },
      {
        "paperId": "fe10bf13aeb8728a955f1f8fd312ce77773b59ec",
        "title": "MetaABR: A Meta-Learning Approach on Adaptative Bitrate Selection for Video Streaming"
      },
      {
        "paperId": "8d6d320dbe9bdd6a1ea844faaf3dc0f5fff2543b",
        "title": "Contrastive Learning-Based Bayes-Adaptive Meta-Reinforcement Learning for Active Pantograph Control in High-Speed Railways"
      },
      {
        "paperId": "e4b072842394531f2822164690d32e1fa11d8c81",
        "title": "Learning an Actionable Discrete Diffusion Policy via Large-Scale Actionless Video Pre-Training"
      },
      {
        "paperId": "41ecff85ec0c8082fcd4efcf31fc5ec3911dce88",
        "title": "Large Language Models as Agents in Two-Player Games"
      },
      {
        "paperId": "4a836363233398c0ac27daee942cb5533f467458",
        "title": "Pangu-Agent: A Fine-Tunable Generalist Agent with Structured Reasoning"
      },
      {
        "paperId": "cc23112117688618d01ddf12176f95c2f5c1311d",
        "title": "Scheduling Real-Time Wireless Traffic: A Network-Aided Offline Reinforcement Learning Approach"
      },
      {
        "paperId": "65a46213c60ed5e61cea2698f4f0ab1460cec684",
        "title": "On Task-Relevant Loss Functions in Meta-Reinforcement Learning and Online LQR"
      },
      {
        "paperId": "7a8e3a68bc33c1d775a060e435324571b4ff3c03",
        "title": "Object Localization Algorithm Based on Meta-Reinforcement Learning"
      },
      {
        "paperId": "d3367dc9a7a1d7ae70a06eadc02b2430f4529f7c",
        "title": "Large Language Models for Robotics: A Survey"
      },
      {
        "paperId": "8651cccfd739fe406e4292a0fcaa2f0e645c7ead",
        "title": "Dream to Adapt: Meta Reinforcement Learning by Latent Context Imagination and MDP Imagination"
      },
      {
        "paperId": "ebed5dcd605f2e89279322d4167229cf7f081f2f",
        "title": "Meta-Learning Strategies through Value Maximization in Neural Networks"
      },
      {
        "paperId": "736dbbb1b7aea6a126bc62e32be43018a04976f0",
        "title": "Transformers as Decision Makers: Provable In-Context Reinforcement Learning via Supervised Pretraining"
      },
      {
        "paperId": "a815c3209e7baff4466dbf6e129129511f842b7e",
        "title": "Making Scalable Meta Learning Practical"
      },
      {
        "paperId": "fcf4696ee6883245ab5a062d8b775a0f492f5f19",
        "title": "Amortized Network Intervention to Steer the Excitatory Point Processes"
      },
      {
        "paperId": "578c3a5e9afdb8d68e65b5796484460393062dcb",
        "title": "Subtask-masked curriculum learning for reinforcement learning with application to UAV maneuver decision-making"
      },
      {
        "paperId": "8bffc877582e2943e580e49d882760c973cba8a4",
        "title": "Digital Twin and Meta RL Empowered Fast-Adaptation of Joint User Scheduling and Task Offloading for Mobile Industrial IoT"
      },
      {
        "paperId": "5cca3c338cf676073c5ab8b5dbd5acded90cb676",
        "title": "Adapting Energy Management Strategies for Hybrid Electric Vehicles in Dynamic Driving Cycles Through Recurrent Policy"
      },
      {
        "paperId": "c2d42a891800b2f5ee810716f9f3b265eae12e93",
        "title": "AdaptNet: Policy Adaptation for Physics-Based Character Control"
      },
      {
        "paperId": "fa7bccd45859dc7492eac2b79946e1f8f506b5ff",
        "title": "In Search of Dispersed Memories: Generative Diffusion Models Are Associative Memory Networks"
      },
      {
        "paperId": "8432b4b8a33ab65f6e596d9b3168e9dd8b385f28",
        "title": "Diagnosing and exploiting the computational demands of videos games for deep reinforcement learning"
      },
      {
        "paperId": "8f640507a01ae4f394dad324385aa855d0293250",
        "title": "Toward Discretization-Consistent Closure Schemes for Large Eddy Simulation Using Reinforcement Learning"
      },
      {
        "paperId": "12d66d607bc6f541209d5d7a59b5cb68d14c22ba",
        "title": "A Novel Meta Control Framework for Robot Arm Reaching with Changeable Configuration"
      },
      {
        "paperId": "b61447a9ed4df67f2e3f60416f9dfc8ec3cb299a",
        "title": "Is Meta-Learning the Right Approach for the Cold-Start Problem in Recommender Systems?"
      },
      {
        "paperId": "e78e6f1d2a71a43cbab18297503c90ae9580d881",
        "title": "A Survey of Algorithmic Methods for Competency Self-Assessments in Human-Autonomy Teaming"
      },
      {
        "paperId": "4b4221f6d3637fce7524620dd5107b9cd414c2f9",
        "title": "Multi-Agent Chronological Planning with Model-Agnostic Meta Reinforcement Learning"
      },
      {
        "paperId": "ea909a6f35e89e82a09f8f6f1f41d251af807798",
        "title": "An Efficient Meta-Reinforcement Learning Approach for Circuit Linearity Calibration via Style Injection"
      },
      {
        "paperId": "d293a75f529d7b1abb161480a95122c9a3ed6376",
        "title": "Causal Reinforcement Learning: A Survey"
      },
      {
        "paperId": "7f7c7e093e875a28c1d1ddce3d6d7f4c23c0c96b",
        "title": "Few-shot and meta-learning methods for image understanding: a survey"
      },
      {
        "paperId": "2832a3be1ae896d252ece850e7c4ca49edf49d12",
        "title": "RL3: Boosting Meta Reinforcement Learning via RL inside RL2"
      },
      {
        "paperId": "5bac7d00035bc1e246a34f9ee3152b290f97bb92",
        "title": "Supervised Pretraining Can Learn In-Context Reinforcement Learning"
      },
      {
        "paperId": "24a010a777ea5072bf91e800a1677c0a02e91fca",
        "title": "Meta-Learning Based Runtime Adaptation for Industrial Wireless Sensor-Actuator Networks"
      },
      {
        "paperId": "1d33ddee7291eae1f0e4b1ac1de8aa631dad2b3b",
        "title": "Acceleration in Policy Optimization"
      },
      {
        "paperId": "095dbb6c4a4bec36b0c973bd67bd94df49134dad",
        "title": "Meta Generative Flow Networks with Personalization for Task-Specific Adaptation"
      },
      {
        "paperId": "fb5e2eca7ec1537d438f652b1079aab04d6b4221",
        "title": "Reinforcement Learning Methods for Computation Offloading: A Systematic Review"
      },
      {
        "paperId": "c9f8d503f911bd5f025dba953a08277a27ff4c7a",
        "title": "Fast Context Adaptation in Cost-Aware Continual Learning"
      },
      {
        "paperId": "7e81c841a5784e98f800c200445f4e1063e50d0b",
        "title": "Learning Embeddings for Sequential Tasks Using Population of Agents"
      },
      {
        "paperId": "0c2d80cd47961d463e1255fb8a17f4f89030bfbe",
        "title": "Meta-Scheduling Framework With Cooperative Learning Toward Beyond 5G"
      },
      {
        "paperId": "196604d1f9497830c0cf1aabea8011b719a7bae1",
        "title": "Rethinking the hippocampal cognitive map as a meta-learning computational module"
      },
      {
        "paperId": "110d8bd601fd808125bf461abc58f68c9ea455b2",
        "title": "Rapid Adaptation for Active Pantograph Control in High-Speed Railway via Deep Meta Reinforcement Learning"
      },
      {
        "paperId": "eabfc1846af8f09d6c65e45cb04f83fa397f6799",
        "title": "Meta-Reinforcement Learning Based on Self-Supervised Task Representation Learning"
      },
      {
        "paperId": "cecd400cb0bdd14aa4b2568015b197d6862ede63",
        "title": "Moving Forward by Moving Backward: Embedding Action Impact over Action Semantics"
      },
      {
        "paperId": "171cbc5cf9a814e2598862b944c976a43c0918d2",
        "title": "Efficient hyperparameters optimization through model-based reinforcement learning with experience exploiting and meta-learning"
      },
      {
        "paperId": "43208ea3fe44739df80c3b879cc39fa615e4df08",
        "title": "SPOT: Sequential Predictive Modeling of Clinical Trial Outcome with Meta-Learning"
      },
      {
        "paperId": "19b878a8a1e1ac88cb15f6b31ddf113cee8c94e8",
        "title": "Towards Retrieval-Based Neural Code Summarization: A Meta-Learning Approach"
      },
      {
        "paperId": "7f437f4af59ff994d97482ee1c12aaeb4b310e85",
        "title": "Implicit Posteriori Parameter Distribution Optimization in Reinforcement Learning"
      },
      {
        "paperId": "74dd51db773ea883d9804d1845345a46ab908ccd",
        "title": "A Task-Agnostic Regularizer for Diverse Subpolicy Discovery in Hierarchical Reinforcement Learning"
      },
      {
        "paperId": "ae92c828d9709583d64e2d4bc813e89d2be4b737",
        "title": "Meta attention for Off-Policy Actor-Critic"
      },
      {
        "paperId": "9212ba44efe8890f9825b7d91adb775ef13772e5",
        "title": "A Comprehensive Review and a Taxonomy of Edge Machine Learning: Requirements, Paradigms, and Techniques"
      },
      {
        "paperId": "06ad7a1b42b4a6a395ac01091e5cafeea23918ab",
        "title": "Meta-Reinforcement Learning via Exploratory Task Clustering"
      },
      {
        "paperId": "e89d9a5c97f69c7c6f06440ed0bbbc56152042e5",
        "title": "Train Hard, Fight Easy: Robust Meta Reinforcement Learning"
      },
      {
        "paperId": "5291241f4bcc077844694181ce8722e91cb41e3b",
        "title": "Advanced Reinforcement Learning and Its Connections with Brain Neuroscience"
      },
      {
        "paperId": "4a78b5a53bec06aeaecb3fdf5951a476884a8645",
        "title": "Concept Discovery for Fast Adapatation"
      },
      {
        "paperId": "009f39969ec181ac8b8548193a37d29cbafb1079",
        "title": "Variational Information Bottleneck Regularized Deep Reinforcement Learning for Efficient Robotic Skill Adaptation"
      },
      {
        "paperId": "07641a0b4f052e5c5baf6bbc74d00a95390cf94c",
        "title": "Prioritized Hindsight with Dual Buffer for Meta-Reinforcement Learning"
      },
      {
        "paperId": "f19dfc360088922cf1d423c538662aae8d542c28",
        "title": "Is Conditional Generative Modeling all you need for Decision-Making?"
      },
      {
        "paperId": "70cdd8ab50416cd789891a221aa48ebb027c1eac",
        "title": "Prototypical context-aware dynamics generalization for high-dimensional model-based reinforcement learning"
      },
      {
        "paperId": "d1cbf800e1ff687b1e39c23239766d7a35844687",
        "title": "Implicit Training of Energy Model for Structure Prediction"
      },
      {
        "paperId": "d5d794b9657eb15b2bb73872a43c82b0ee325b71",
        "title": "Giving Feedback on Interactive Student Programs with Meta-Exploration"
      },
      {
        "paperId": "95beb8362abfa056adbcc30a7900238d5037fc11",
        "title": "Build generally reusable agent-environment interaction models"
      },
      {
        "paperId": "23d9e8319f58451a20fac7fc1316c4e664d66eb1",
        "title": "Knowing the Past to Predict the Future: Reinforcement Virtual Learning"
      },
      {
        "paperId": "0928fb7f908ec70412644414601cb956c15bc134",
        "title": "Meta Reinforcement Learning with Hebbian Learning"
      },
      {
        "paperId": "20e72dea8b732b8327057e77826fbbe467aa3b4e",
        "title": "Nonlinear Model Predictive Control with Cost Function Scheduling for a Wheeled Mobile Robot"
      },
      {
        "paperId": "714c5f81e849bf55a68e8dafa18c6f2293a89155",
        "title": "Bayesian optimization with unknown constraints in graphical skill models for compliant manipulation tasks using an industrial robot"
      },
      {
        "paperId": "200fe438d93ee79b3cf68e66977068afe843c7ef",
        "title": "The Role of Exploration for Task Transfer in Reinforcement Learning"
      },
      {
        "paperId": "3bea3772ba6f650a63b7d10042c2e3d3edd0b3ca",
        "title": "Decomposed Mutual Information Optimization for Generalized Context in Meta-Reinforcement Learning"
      },
      {
        "paperId": "afa519d9f4104dd35764d7ff2187f4995084ab7c",
        "title": "Winner Takes It All: Training Performant RL Populations for Combinatorial Optimization"
      },
      {
        "paperId": "60b88aab5b8c712dedeff7b5283004a09e01ccc6",
        "title": "Distributionally Adaptive Meta Reinforcement Learning"
      },
      {
        "paperId": "83f1343500c9f0df62da0d61736738d8d7a9bba0",
        "title": "Zero-Shot Policy Transfer with Disentangled Task Representation of Meta-Reinforcement Learning"
      },
      {
        "paperId": "4e48d349574cac944fc854ee2b4ae598dd150246",
        "title": "Meta Reinforcement Learning for Optimal Design of Legged Robots"
      },
      {
        "paperId": "15b1151a6c95e9ec00f419c90c712b13fd021e1c",
        "title": "Deep Reinforcement Learning: A Survey"
      },
      {
        "paperId": "6bc761321b836c8eef8602ceb2357cb28807b564",
        "title": "Enhanced Meta Reinforcement Learning using Demonstrations in Sparse Reward Environments"
      },
      {
        "paperId": "a5643269e2697cbb2ea70b01a03aaf582dd58931",
        "title": "Progressive Meta-Learning With Curriculum"
      },
      {
        "paperId": "caafa684d204fde798f919f575ee00cc91a2a438",
        "title": "MIRA: Model-Based Imagined Rollouts Augmentation for Non-Stationarity in Multi-Agent Systems"
      },
      {
        "paperId": "18995e1270a0799e61b80249379ed8a8d093c6dd",
        "title": "Versatile Control of Fluid-directed Solid Objects Using Multi-task Reinforcement Learning"
      },
      {
        "paperId": "daccd2ebe2d37ea2b7b9ec735f58f42a3f5011c7",
        "title": "Meta Reinforcement Learning with Successor Feature Based Context"
      },
      {
        "paperId": "88f48480ffb3c36683a63b0f6d5932b40bfbef64",
        "title": "Celebrating Robustness in Efficient Off-Policy Meta-Reinforcement Learning"
      },
      {
        "paperId": "31eba23839649c21c3e462a7568b6b72041d4b5c",
        "title": "Meta-Reinforcement Learning in Non-Stationary and Dynamic Environments"
      },
      {
        "paperId": "42bce7bd068f36f3091ac5469dac55ae8a0ae721",
        "title": "Provable Generalization of Overparameterized Meta-learning Trained with SGD"
      },
      {
        "paperId": "90cd70aef56e83c8888004fc738e0f5f3f68c620",
        "title": "Transformers are Meta-Reinforcement Learners"
      },
      {
        "paperId": "a66e581eca1e6531298563f39d8b4ccdcf9489e2",
        "title": "Fast Inference and Transfer of Compositional Task Structures for Few-shot Task Generalization"
      },
      {
        "paperId": "90bef1be595079cea11ea076f1f19cb97b4e1a00",
        "title": "Towards designing a generic and comprehensive deep reinforcement learning framework"
      },
      {
        "paperId": "23bac2542b145bf2fcd17d7fa0a02ae03d0a45f7",
        "title": "Skill-based Meta-Reinforcement Learning"
      },
      {
        "paperId": "07c3b4c087a6a13b1e463d0e0adbbb4546919d99",
        "title": "Few-Shot Forecasting of Time-Series with Heterogeneous Channels"
      },
      {
        "paperId": "041ebfb3a36b6c0bbc81c5ee35ce3c56af099670",
        "title": "Model Based Meta Learning of Critics for Policy Gradients"
      },
      {
        "paperId": "29186936422c199599fd0a24e6f209376775fce9",
        "title": "The Sandbox Environment for Generalizable Agent Research (SEGAR)"
      },
      {
        "paperId": "5e296b739f26490d10f57479c5a7750bf0e34dbc",
        "title": "What Matters For Meta-Learning Vision Regression Tasks?"
      },
      {
        "paperId": "1bb82660573bbaf01572041da16842ee2398ae39",
        "title": "Learning Robust Real-Time Cultural Transmission without Human Data"
      },
      {
        "paperId": "b8b475c09c9bce3d420978d70657b40b1e70724b",
        "title": "Meta-Reinforcement Learning with Self-Modifying Networks"
      },
      {
        "paperId": "1b63ba2b0d21d33b5bd198c0c8fb299cb177875f",
        "title": "System-Agnostic Meta-Learning for MDP-based Dynamic Scheduling via Descriptive Policy"
      },
      {
        "paperId": "0180ce1963fee47a5f6bc3aa5325ae7559ae21ec",
        "title": "A Theoretical Understanding of Gradient Bias in Meta-Reinforcement Learning"
      },
      {
        "paperId": "5dfb50074b7b592af70e37bcb3f6037f2fdc3666",
        "title": "Continuous self-adaptive optimization to learn multi-task multi-agent"
      },
      {
        "paperId": "8e63e0ccd18fdfc1f7a73fc9e0d4bc6c36316016",
        "title": "Unsupervised Reinforcement Learning in Multiple Environments"
      },
      {
        "paperId": "b64b3880198289fca95e54a001da3dd336502d7a",
        "title": "CoMPS: Continual Meta Policy Search"
      },
      {
        "paperId": "9ce34bbb54a0fafc7a47c69476c983f846387559",
        "title": "Hindsight Task Relabelling: Experience Replay for Sparse Reward Meta-RL"
      },
      {
        "paperId": "e09abb4e3287f4d36ec1af910f3dfcc022da001b",
        "title": "Reinforcement Learning for Few-Shot Text Generation Adaptation"
      },
      {
        "paperId": "84a32b3a9cfb78969c21afd69df9788142a27031",
        "title": "Exploration With Task Information for Meta Reinforcement Learning"
      },
      {
        "paperId": "4e869a444a2edd2e390fe8a9f7f83d2c7930b2fd",
        "title": "Robust Dynamic Bus Control: a Distributional Multi-Agent Reinforcement Learning Approach"
      },
      {
        "paperId": "b25cccde97f1f38e574c12d9e97136040b561887",
        "title": "Quantum-enhanced reinforcement learning for control: a preliminary study"
      },
      {
        "paperId": "a23d4f3bb038c02cc78cd143b05d8ee316589d76",
        "title": "Context Meta-Reinforcement Learning via Neuromodulation"
      },
      {
        "paperId": "00e9ab769b04ee1cf3a908e09c790742c2947299",
        "title": "One Step at a Time: Pros and Cons of Multi-Step Meta-Gradient Reinforcement Learning"
      },
      {
        "paperId": "aef6678f9da757baf381f5c367a87c13339c04af",
        "title": "GalilAI: Out-of-Task Distribution Detection using Causal Active Experimentation for Safe Transfer RL"
      },
      {
        "paperId": "08581fecef8cbad6e614758400a618f57fa4c7a4",
        "title": "Curriculum-Based Meta-learning"
      },
      {
        "paperId": "2057862ad9c9fb9909cb7414e26903a893013666",
        "title": "Boosting Lightweight Single Image Super-resolution via Joint-distillation"
      },
      {
        "paperId": "65e36b8fc38819944528d232368d8669feb3b01a",
        "title": "Wasserstein Unsupervised Reinforcement Learning"
      },
      {
        "paperId": "f40784dad6fdf29b4b23302c97f8a954ea6a2d57",
        "title": "ROA: A Rapid Learning Scheme for In-Situ Memristor Networks"
      },
      {
        "paperId": "987efaeadb88248e7e54c8789b3cc028c571e135",
        "title": "REIN-2: Giving Birth to Prepared Reinforcement Learning Agents Using Reinforcement Learning Agents"
      },
      {
        "paperId": "357de3483ba31c57484f481ae312cf0f822268dc",
        "title": "Offline Meta-Reinforcement Learning for Industrial Insertion"
      },
      {
        "paperId": "487f77e967453d8cea506aee3b83a1847e9de082",
        "title": "A multi-robot path-planning algorithm for autonomous navigation using meta-reinforcement learning based on transfer learning"
      },
      {
        "paperId": "ac89d4156c66f792cacbd29600d4cf0cfead71f3",
        "title": "Deep Reinforcement Learning Versus Evolution Strategies: A Comparative Survey"
      },
      {
        "paperId": "4ab9a90bc1ce86359b69d9cf8d005e7de89985f7",
        "title": "Hindsight Foresight Relabeling for Meta-Reinforcement Learning"
      },
      {
        "paperId": "41872d5173d4ceaef1a5359dae8ad4198caf6b66",
        "title": "Knowledge is reward: Learning optimal exploration by predictive reward cashing"
      },
      {
        "paperId": "3be84e24b144541a8cd9030526ef2b8ef2cbfe54",
        "title": "A Survey of Exploration Methods in Reinforcement Learning"
      },
      {
        "paperId": "7c742e1c8f28c949631d5d863019470844478912",
        "title": "Meta-Reinforcement Learning With Dynamic Adaptiveness Distillation"
      },
      {
        "paperId": "a8f5adc6ff6db4909deb477f873281d97be7b858",
        "title": "Improved Robustness and Safety for Pre-Adaptation of Meta Reinforcement Learning with Prior Regularization"
      },
      {
        "paperId": "7637672671a94235535ac1176c1dd12ba15be7da",
        "title": "A Novel Exploration-Exploitation-Based Adaptive Law for Intelligent Model-Free Control Approaches"
      },
      {
        "paperId": "517f6fe6dd05a891d837fd2a9fd81aa7954c56fd",
        "title": "Deep Reinforcement Learning for Demand-Driven Services in Logistics and Transportation Systems: A Survey"
      },
      {
        "paperId": "ce94ed54e166753a660f161af8d5e9c93d872a12",
        "title": "Meta-Reinforcement Learning in Nonstationary and Nonparametric Environments"
      },
      {
        "paperId": "61f371768cdc093828f432660e22f7a17f22e2af",
        "title": "Offline Meta-Reinforcement Learning with Online Self-Supervision"
      },
      {
        "paperId": "53a7c3b3fb3dc8cd59f12054301bdf9018a4f58e",
        "title": "Leveraging Task Modularity in Reinforcement Learning for Adaptable Industry 4.0 Automation"
      },
      {
        "paperId": "fc83e3bb81ef461f9848fae00885dbea66e76fbc",
        "title": "Policy gradient assisted MAP-Elites"
      },
      {
        "paperId": "27cc8023a9671f30cf0264d4c9cc8120594b2008",
        "title": "Meta-Adaptive Nonlinear Control: Theory and Algorithms"
      },
      {
        "paperId": "d8783108eabae0f5133165267aca025370130879",
        "title": "Quickest change detection with unknown parameters: Constant complexity and near optimality"
      },
      {
        "paperId": "a1a9a02deb172cfe0ec0c3c4fb70b32ae5d09ab4",
        "title": "Improving Generalization in Meta-RL with Imaginary Tasks from Latent Dynamics Mixture"
      },
      {
        "paperId": "40a0d55b6de6749bf2ec3313d8ed6ca5c0ddd7c6",
        "title": "Deep Reinforcement Learning Versus Evolution Strategies: A Comparative Survey"
      },
      {
        "paperId": "459c9101792d16a9780282e83d2119761c4f2668",
        "title": "DACBench: A Benchmark Library for Dynamic Algorithm Configuration"
      },
      {
        "paperId": "f402e1d43bb06b966709b52574151e25271ffe68",
        "title": "Meta Reinforcement Learning for Heuristic Planing"
      },
      {
        "paperId": "b3d6f462a5f183ef5f856a79311bdbf738ff00d2",
        "title": "A Meta Reinforcement Learning-based Approach for Self-Adaptive System"
      },
      {
        "paperId": "ca51a42cd413e8a552ab217f507e355c0e0c59b1",
        "title": "Context-Based Soft Actor Critic for Environments with Non-stationary Dynamics"
      },
      {
        "paperId": "449c4f166f2856029687c6e9ba2e8b7de119a33c",
        "title": "Quick Learner Automated Vehicle Adapting its Roadmanship to Varying Traffic Cultures with Meta Reinforcement Learning"
      },
      {
        "paperId": "3e85d208b1b927fdb69ecf8336c70995818aaebd",
        "title": "MT-Opt: Continuous Multi-Task Robotic Reinforcement Learning at Scale"
      },
      {
        "paperId": "9c5caaf7d014d1b1d176258af15bb763d7f62d01",
        "title": "Towards Enabling Meta-Learning from Target Models"
      },
      {
        "paperId": "001d890509ff45f731f643f2cb9e55bb27ab58c6",
        "title": "Multi goals and multi scenes visual mapless navigation in indoor using meta-learning and scene priors"
      },
      {
        "paperId": "dff6ceff1c6ad4b05c923a71b2ccba35118893f7",
        "title": "Population-Based Evolution Optimizes a Meta-Learning Objective"
      },
      {
        "paperId": "44f5f9dbe7ba6d3132d0f9482c7783d72548a307",
        "title": "Bayesian Meta-Learning for Few-Shot Policy Adaptation Across Robotic Platforms"
      },
      {
        "paperId": "29f05531de4426cf0b88bace56b47bf0ac9ce0a2",
        "title": "Deep reinforcement learning in medical imaging: A literature review"
      },
      {
        "paperId": "4f0462f76c4a6000d89638bfdcc7b7d0a7016c07",
        "title": "Meta-Learning Dynamics Forecasting Using Task Inference"
      },
      {
        "paperId": "aedd968f09752786785d2de91151d22a7dc36117",
        "title": "Model-based Meta Reinforcement Learning using Graph Structured Surrogate Models"
      },
      {
        "paperId": "b556f9dfc19158d8a0def0eb71805da8ca0163a8",
        "title": "A Fuzzy Curiosity-Driven Mechanism for Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "ba1817d10bdbd7d96fe0196092f8b6ef0436cc3a",
        "title": "Transfer Reinforcement Learning Across Homotopy Classes"
      },
      {
        "paperId": "eca9e42f081767eadf6a5ebed738b82bde2a17a6",
        "title": "Alchemy: A benchmark and analysis toolkit for meta-reinforcement learning agents"
      },
      {
        "paperId": "3c518ea2a625235da3ef23d12f8da7049e79fcf9",
        "title": "Linear Representation Meta-Reinforcement Learning for Instant Adaptation"
      },
      {
        "paperId": "ee3f989a4c39836e92e801fa71266dc258049e00",
        "title": "MetaVIM: Meta Variationally Intrinsic Motivated Reinforcement Learning for Decentralized Traffic Signal Control"
      },
      {
        "paperId": "f7d1da54ad9d6a91e7c453e6f0ae62bd610131aa",
        "title": "Artificial intelligence and innovation management: A review, framework, and research agenda\u2730"
      },
      {
        "paperId": "3b0b15dcef21d2ac69ba85300c3dbd95f1182f29",
        "title": "Locally Persistent Exploration in Continuous Control Tasks with Sparse Rewards"
      },
      {
        "paperId": "5b69894df2cbd04f297e88d460d512026c84e6d8",
        "title": "Investigating Active Learning and Meta-Learning for Iterative Peptide Design"
      },
      {
        "paperId": "673101bbda8cbe92f53c58b152ada85a5ade68eb",
        "title": "Efficient Hyperparameters optimization Through Model-based Reinforcement Learning and Meta-Learning"
      },
      {
        "paperId": "2ad5471662f9fd22da3a8dea23c8b9d3ac46b1bc",
        "title": "Adaptable Automation with Modular Deep Reinforcement Learning and Policy Transfer"
      },
      {
        "paperId": "0a4b550ec609a54a27f1b47fc3a228fbee040fb3",
        "title": "Distilling a Hierarchical Policy for Planning and Control via Representation and Reinforcement Learning"
      },
      {
        "paperId": "3d951a374fcb009607d42fa178bce9254a65a48b",
        "title": "Information-theoretic Task Selection for Meta-Reinforcement Learning"
      },
      {
        "paperId": "7fad5ce5ef04b84d3cee1ab79f16532b93c8aad5",
        "title": "BSE-MAML: Model Agnostic Meta-Reinforcement Learning via Bayesian Structured Exploration"
      },
      {
        "paperId": "e4a46c64aafbef0406e9cfa90dd9c43e3e07598c",
        "title": "One Solution is Not All You Need: Few-Shot Extrapolation via Structured MaxEnt RL"
      },
      {
        "paperId": "acf24ff124d9359d0404ed77967d292fc2e0a342",
        "title": "MELD: Meta-Reinforcement Learning from Images via Latent State Models"
      },
      {
        "paperId": "94bcbb759907f63d3529ce3af839d644d5fee09d",
        "title": "Few-Shot Model-Based Adaptation in Noisy Conditions"
      },
      {
        "paperId": "16e83f3f0f78ceb203746eeb88f1f5aae9ba3092",
        "title": "Deep reinforcement learning: a survey"
      },
      {
        "paperId": "332c44793b70776b9b966128c52e694222b1ab73",
        "title": "A survey of deep meta-learning"
      },
      {
        "paperId": "975e33347fd841ded9ebcd77d928b243ea786a8c",
        "title": "Regularization Matters in Policy Optimization - An Empirical Study on Continuous Control"
      },
      {
        "paperId": "8ccb580c25d3205a67d99c038233437c5957b466",
        "title": "FORK: A FORward-looKing Actor for Model-Free Reinforcement Learning"
      },
      {
        "paperId": "0ca7c0d92a10359a2a7b8fd501a40c9ef768676d",
        "title": "Exploration in Approximate Hyper-State Space for Meta Reinforcement Learning"
      },
      {
        "paperId": "77be65bb396cb6309b6d03023c5f71203b3a39ea",
        "title": "Towards Effective Context for Meta-Reinforcement Learning: an Approach based on Contrastive Learning"
      },
      {
        "paperId": "62463a7864ff43b234f35c94151e6b05912e8dde",
        "title": "Meta Reinforcement Learning with Generative Adversarial Reward from Expert Knowledge"
      },
      {
        "paperId": "d728e7bd49ade263c8170a64f6f5ef57a69349f6",
        "title": "GeneraLight: Improving Environment Generalization of Traffic Signal Control via Meta Reinforcement Learning"
      },
      {
        "paperId": "53681a2cc0b2f0ef8df73df76e79017cfa399a74",
        "title": "Energy-based Surprise Minimization for Multi-Agent Value Factorization"
      },
      {
        "paperId": "8695df3746b1e175fa1dab489bad4545843a5ab1",
        "title": "Importance Weighted Policy Learning and Adaption"
      },
      {
        "paperId": "0922fc9d7aa9610896890f4d417ca7e72e417cdf",
        "title": "Importance Weighted Policy Learning and Adaptation"
      },
      {
        "paperId": "62e25f3c24c1ff6a0f0ccceea9a7700d108dd5a9",
        "title": "Detecting and adapting to crisis pattern with context based Deep Reinforcement Learning"
      },
      {
        "paperId": "31b54cd8eb2d9f76a0c028e56ec2c649f2675c55",
        "title": "Towards human-centered artificial intelligence : learning from demonstration and meta-learning for model adaptation"
      },
      {
        "paperId": "cd4f26d784c48788425d1f74bcd71ac70bbe9f71",
        "title": "Meta Reinforcement Learning-Based Lane Change Strategy for Autonomous Vehicles"
      },
      {
        "paperId": "f751e163dcef8e078e6ba60308d8690f69811e35",
        "title": "DMRO: A Deep Meta Reinforcement Learning-Based Task Offloading Framework for Edge-Cloud Computing"
      },
      {
        "paperId": "a60b2bbd511beb7123ff10a5928ad38d9be5952e",
        "title": "Scaling simulation-to-real transfer by learning a latent space of robot skills"
      },
      {
        "paperId": "17908c7db26985704c00dd4521932f25c43dbe17",
        "title": "Offline Meta-Reinforcement Learning with Advantage Weighting"
      },
      {
        "paperId": "2a9336f92bcdc650c5257ec0cc1b4cd272f5ed1a",
        "title": "Decoupling Exploration and Exploitation for Meta-Reinforcement Learning without Sacrifices"
      },
      {
        "paperId": "dffd9a1a1da4457c5c24d8de6cfaca9f63c39a6b",
        "title": "Explore then Execute: Adapting without Rewards via Factorized Meta-Reinforcement Learning"
      },
      {
        "paperId": "9342fce9c5a69f545a778ca7e885ba9d63af928f",
        "title": "Offline Meta Reinforcement Learning"
      },
      {
        "paperId": "769c5e812f0c3c7393b5fae215bd731694667ba2",
        "title": "Offline Meta Learning of Exploration"
      },
      {
        "paperId": "d6306470c05f0f77af48d8255221a3e9b49b86f0",
        "title": "IntelligentPooling: practical Thompson sampling for mHealth"
      },
      {
        "paperId": "9af2b207750268981d20b4c491c7eb96a80a170c",
        "title": "Hypersolvers: Toward Fast Continuous-Depth Models"
      },
      {
        "paperId": "4650f9265eef5e5fd05835860c06b814d0f6db34",
        "title": "Lifelong Policy Gradient Learning of Factored Policies for Faster Training Without Forgetting"
      },
      {
        "paperId": "adbf2bb1938d5ccbcc92049c2afe8bf55e64caeb",
        "title": "Indoor Navigation for Mobile Agents: A Multimodal Vision Fusion Model"
      },
      {
        "paperId": "48843f326ac4821afa04db53561a1568c76f9558",
        "title": "Latent Context Based Soft Actor-Critic"
      },
      {
        "paperId": "a0f814398da24f602d43a6a268332a5d26b91464",
        "title": "Exploring Parameter Space with Structured Noise for Meta-Reinforcement Learning"
      },
      {
        "paperId": "e50d01508896743817a03bd73d869d843e619535",
        "title": "A Study on the Implementation Method of Artificial Intelligence Shipboard Combat System"
      },
      {
        "paperId": "f1b5f17f836bf96d144dc2dd9b669440159abc84",
        "title": "On the Global Optimality of Model-Agnostic Meta-Learning"
      },
      {
        "paperId": "4c81e2999cb6a9642fba026613327753235ac0d1",
        "title": "Opponent Modelling with Local Information Variational Autoencoders"
      },
      {
        "paperId": "2a830450f65dc41bc196777356701bed52d91e34",
        "title": "Local Information Opponent Modelling Using Variational Autoencoders."
      },
      {
        "paperId": "1d4288c47a7802575d2a0d231d1283a4f225a85b",
        "title": "MetaCURE: Meta Reinforcement Learning with Empowerment-Driven Exploration"
      },
      {
        "paperId": "7013e9f2874e11749fdd0be2b9cce0d08a0c6d1a",
        "title": "Learn to Effectively Explore in Context-Based Meta-RL"
      },
      {
        "paperId": "3984c51b039bc18b2494cceb18a9eeb37c6c4714",
        "title": "A Brief Look at Generalization in Visual Meta-Reinforcement Learning"
      },
      {
        "paperId": "41a5c7271635f66aa1fa3497e28d6791db27b73b",
        "title": "Meta-Reinforcement Learning Robust to Distributional Shift via Model Identification and Experience Relabeling"
      },
      {
        "paperId": "1b4a1672929438410d9b6bdc23daccd34656bd95",
        "title": "Lifelong Learning of Factored Policies via Policy Gradients"
      },
      {
        "paperId": "14a44d4555bae15c01b1ed48fb3e1d75185573e9",
        "title": "Meta-Model-Based Meta-Policy Optimization"
      },
      {
        "paperId": "fac54bfee9d6efd82bef3f62c953f8eaef5fc426",
        "title": "A Survey of Reinforcement Learning Algorithms for Dynamically Varying Environments"
      },
      {
        "paperId": "596579e8e72efdc3cdcc6f7faa84545c2b9b7423",
        "title": "Learning Adaptive Exploration Strategies in Dynamic Environments Through Informed Policy Regularization"
      },
      {
        "paperId": "7b201e42e32430d951458916810a7dbf1e946a6d",
        "title": "Regularizing Meta-Learning via Gradient Dropout"
      },
      {
        "paperId": "020bb2ba5f3923858cd6882ba5c5a44ea8041ab6",
        "title": "Meta-Learning in Neural Networks: A Survey"
      },
      {
        "paperId": "f5b0226b810222d83ccb9433813e0367339e8025",
        "title": "Learning for Scale-Arbitrary Super-Resolution from Scale-Specific Networks"
      },
      {
        "paperId": "e7905a3943675e73d4d685e1af2da02bc628229b",
        "title": "When Autonomous Systems Meet Accuracy and Transferability through AI: A Survey"
      },
      {
        "paperId": "94c2da0ec49fbdef663befba9231b27c5c589c69",
        "title": "Meta-level learning for the effective reduction of model search space"
      },
      {
        "paperId": "613f00378c2add3d7cb2a217f9a8043083944754",
        "title": "Online Meta-Critic Learning for Off-Policy Actor-Critic Methods"
      },
      {
        "paperId": "11236ae4f31b428b6313559fb99300643c172cf9",
        "title": "Meta-learning curiosity algorithms"
      },
      {
        "paperId": "23175fc9e17afa07317f6d13f72892b8b99162ea",
        "title": "Learning Context-aware Task Reasoning for Efficient Meta Reinforcement Learning"
      },
      {
        "paperId": "33a9513d6bf2c90ec81f98cee3518565b142d028",
        "title": "Rapidly Adaptable Legged Robots via Evolutionary Meta-Learning"
      },
      {
        "paperId": "a9fdf3f6a85cdb278d8b6a7e82bd8d4512aac9d0",
        "title": "Curriculum in Gradient-Based Meta-Reinforcement Learning"
      },
      {
        "paperId": "4b55252b949ab3d02e7e95579d0eea3a0eb15631",
        "title": "On the Convergence Theory of Debiased Model-Agnostic Meta-Reinforcement Learning"
      },
      {
        "paperId": "ecb0bbee797410b8e8b3272421ffbd8ab4f6da71",
        "title": "Provably Convergent Policy Gradient Methods for Model-Agnostic Meta-Reinforcement Learning"
      },
      {
        "paperId": "923651bf821c799aa07f1dbf47989f6663864e79",
        "title": "HMRL: Hyper-Meta Learning for Sparse Reward Reinforcement Learning Problem"
      },
      {
        "paperId": "d569777654c81b66eedc31987e62ad2de5d9564b",
        "title": "Hyper-Meta Reinforcement Learning with Sparse Reward"
      },
      {
        "paperId": "1733a43e44bcbdd6cb0fc99b1d7dbcff0ae450e0",
        "title": "Bayesian Residual Policy Optimization: : Scalable Bayesian Reinforcement Learning with Clairvoyant Experts"
      },
      {
        "paperId": "0c62a64c6e758d4cd2d968ca39d41f0f6579c14f",
        "title": "Effective Diversity in Population-Based Reinforcement Learning"
      },
      {
        "paperId": "749f4d58ea2cc98562ac589c642c5beb53dfe77f",
        "title": "Fast Adaptation to Super-Resolution Networks via Meta-Learning"
      },
      {
        "paperId": "4f07310856b3ebbe17f021ce142a7bb3631ff559",
        "title": "Meta Reinforcement Learning with Autonomous Inference of Subtask Dependencies"
      },
      {
        "paperId": "4813be9599249bc024aad1d674c8d1da3479d532",
        "title": "Unsupervised Curricula for Visual Meta-Reinforcement Learning"
      },
      {
        "paperId": "bc68acfe0043bed525eeacfe17506d5399c3fbe1",
        "title": "How Does an Approximate Model Help in Reinforcement Learning"
      },
      {
        "paperId": "d411c4a5720962a4db11557a02cbff9dde84f9d0",
        "title": "Iterative Peptide Modeling With Active Learning And Meta-Learning"
      },
      {
        "paperId": "34075ea242d6d76e360fb9fe7998ba859151dbca",
        "title": "MAME : Model-Agnostic Meta-Exploration"
      },
      {
        "paperId": "3110f8caaeddb0c5b0d95ae46e85cffab9f1847b",
        "title": "Dynamic Subgoal-based Exploration via Bayesian Optimization"
      },
      {
        "paperId": "1c3dafb77bcaf7fcfa3e34621d0d32176a95cf5b",
        "title": "Exploration via Sample-Efficient Subgoal Design"
      },
      {
        "paperId": "34c4621319b316cbca8fc1ff12739ca691fe9f95",
        "title": "Exploration via Cost-Aware Subgoal Design"
      },
      {
        "paperId": "361e953f792a585496834ee14216b94d0ce9ae74",
        "title": "VariBAD: A Very Good Method for Bayes-Adaptive Deep RL via Meta-Learning"
      },
      {
        "paperId": "7e12836510909097f592cbec8c2f646f1b3790e5",
        "title": "OffWorld Gym: open-access physical robotics environment for real-world reinforcement learning benchmark and research"
      },
      {
        "paperId": "46106fcd08223540d080f674b26770c1fa8a52ff",
        "title": "Influence-Based Multi-Agent Exploration"
      },
      {
        "paperId": "6fcfc9094813ad45f03d4138ef1b90dc6d76d946",
        "title": "Learning Transferable Graph Exploration"
      },
      {
        "paperId": "886f9e4e78b13db6b965388f7fe044ea8c5c0f61",
        "title": "Variational Autoencoders for Opponent Modeling in Multi-Agent Systems"
      },
      {
        "paperId": "bb797edf7556c57fe255d29a6c0647d6815f02b9",
        "title": "Regularization Matters in Policy Optimization"
      },
      {
        "paperId": "da47685ad651f764bf749e4ff38bd88b22b3d1fa",
        "title": "Pre-training as Batch Meta Reinforcement Learning with tiMe"
      },
      {
        "paperId": "cfdd4f45074a40c4ba44fc3723dfc66abd5e10cc",
        "title": "Meta Reinforcement Learning from observational data."
      },
      {
        "paperId": "521538b3bc1fd64fb0d4dc71881edbeff03c642a",
        "title": "Attraction-Repulsion Actor-Critic for Continuous Control Reinforcement Learning"
      },
      {
        "paperId": "d8e996fa4be2a967d0c7ca323019321bdb3c7424",
        "title": "The Tools Challenge: Rapid Trial-and-Error Learning in Physical Problem Solving"
      },
      {
        "paperId": "f33ae3a6f47ff3897a7ff12c6a0bacec2223d6d6",
        "title": "A Review of Robot Learning for Manipulation: Challenges, Representations, and Algorithms"
      },
      {
        "paperId": "2aac2436314fa56baf5ebc8aba3785eea59c6e5f",
        "title": "ExTra: Transfer-guided Exploration"
      },
      {
        "paperId": "4127183a8c37efa2c07fa13f1b4179a52c04709c",
        "title": "Disentangled Skill Embeddings for Reinforcement Learning"
      },
      {
        "paperId": "339e2610de8487ccb54af05cb59b63854d25f02d",
        "title": "Meta Learning via Learned Loss"
      },
      {
        "paperId": "cfec54f9208dcbb2c558aa7ff45f57be2533d046",
        "title": "Efficient Exploration via State Marginal Matching"
      },
      {
        "paperId": "ce2512f76d879b02ad36a84dd74cedceed8a767c",
        "title": "Knowledge reuse for deep reinforcement learning."
      },
      {
        "paperId": "390c7c230c223498c281a204006c5fc141759460",
        "title": "Transfer Learning by Modeling a Distribution over Policies"
      },
      {
        "paperId": "00753de4e5553de8a569e951f531bd683d8dcb16",
        "title": "Watch, Try, Learn: Meta-Learning from Demonstrations and Reward"
      },
      {
        "paperId": "1a074c546e54155809492a80310d5799013c4d85",
        "title": "Lifelong Learning with a Changing Action Set"
      },
      {
        "paperId": "30e7f96c987c0f8a7cc64ce8a0599a7d31870c05",
        "title": "Reinforcement Learning Experience Reuse with Policy Residual Representation"
      },
      {
        "paperId": "81768cdace11e14982d3aba9060059e1133d83fc",
        "title": "Learning Efficient and Effective Exploration Policies with Counterfactual Meta Policy"
      },
      {
        "paperId": "801eb90352e603b1573a04a36804417d1c9496f0",
        "title": "Learning latent state representation for speeding up exploration"
      },
      {
        "paperId": "7bd95a62fd6320730cbb24a0e4fafac97d840652",
        "title": "Meta Reinforcement Learning with Task Embedding and Shared Policy"
      },
      {
        "paperId": "9a3c9a0ac460c7891d03d56146f2d566c7e0fb08",
        "title": "Meta reinforcement learning as task inference"
      },
      {
        "paperId": "f2b400335fda58d043c57289055db12ef488787e",
        "title": "Meta-learners' learning dynamics are unlike learners'"
      },
      {
        "paperId": "a853a3dc5e48d33948af0695cdfaea389c22aab3",
        "title": "Intelligent Pooling in Thompson Sampling for Rapid Personalization in Mobile Health"
      },
      {
        "paperId": "dd1b41e52d7aa7ca6f55ca43b7852d41833da599",
        "title": "Deep Knowledge Based Agent: Learning to do tasks by self-thinking about imaginary worlds"
      },
      {
        "paperId": "d05353adb12e9f74504ef3cb4229ec7b4dcfe1a4",
        "title": "Multitask Soft Option Learning"
      },
      {
        "paperId": "65b072f590433f2c09aa2a8d566bcaa9c3b5b9a0",
        "title": "Guided Meta-Policy Search"
      },
      {
        "paperId": "4625628163a2ee0e6cd320cd7a14b4ccded2a631",
        "title": "Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables"
      },
      {
        "paperId": "7388826b5ee00efe17cb7f19a623d9b5e955ae70",
        "title": "Skew-Fit: State-Covering Self-Supervised Reinforcement Learning"
      },
      {
        "paperId": "fb180f75b43f008115da66d3cce5f8cbf01b2cbc",
        "title": "Concurrent Meta Reinforcement Learning"
      },
      {
        "paperId": "a936732daa9e7d6c170d7f4b6386e2cf74ea3e0c",
        "title": "NoRML: No-Reward Meta Learning"
      },
      {
        "paperId": "84b888b66d598f4fd5909c524a1818e1004a3254",
        "title": "Learning to Generalize from Sparse and Underspecified Rewards"
      },
      {
        "paperId": "bf7f1ada5feecc0992f71b39c1ebeccb19ae631b",
        "title": "InfoBot: Transfer and Exploration via the Information Bottleneck"
      },
      {
        "paperId": "30ae05d17cc6946ca972ea56eb5b02dc2a401826",
        "title": "Meta-Learning for Contextual Bandit Exploration"
      },
      {
        "paperId": "4d21334ea3564c89586e1ba176e11382bdd3d394",
        "title": "Learning to Learn How to Learn: Self-Adaptive Visual Navigation Using Meta-Learning"
      },
      {
        "paperId": "f2ac2a3fd7b341f2b1be752b4dd46ed9abcf0751",
        "title": "Deep Reinforcement Learning"
      },
      {
        "paperId": "2f240424ca0761d0549252dacfbbeece14bb3cb6",
        "title": "Fast Context Adaptation via Meta-Learning"
      },
      {
        "paperId": "c07237c0d776436f79021eee73f938b46aec2ce6",
        "title": "Bayesian Transfer Reinforcement Learning with Prior Knowledge Rules"
      },
      {
        "paperId": "d9e08e872c69e5acfed2a93ec6f6d623cfd0c680",
        "title": "CEM-RL: Combining evolutionary and gradient-based methods for policy search"
      },
      {
        "paperId": "c456941a270cb2040eed4abfb39150508caf920c",
        "title": "ProMP: Proximal Meta-Policy Search"
      },
      {
        "paperId": "6e1ace38ef6b73595ab200bf7a1338f715c9e553",
        "title": "CAML: Fast Context Adaptation via Meta-Learning"
      },
      {
        "paperId": "3996f47c119223fb547905784124a0a2aa09c9a4",
        "title": "Scaling simulation-to-real transfer by learning composable robot skills"
      },
      {
        "paperId": "1766648967f6206a944a4bd18bbbd92a74c164bd",
        "title": "Automatically Composing Representation Transformations as a Means for Generalization"
      },
      {
        "paperId": "2e7b6e73398af01bc975e9bf9374ee5f255252b3",
        "title": "VFunc: a Deep Generative Model for Functions"
      },
      {
        "paperId": "b93317f61c6ed99542da9d1d691ded9732c16c1c",
        "title": "Unsupervised Meta-Learning for Reinforcement Learning"
      },
      {
        "paperId": "6ecc4b1ab05f3ec12484a0ea36abfd6271c5c5ba",
        "title": "Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review"
      },
      {
        "paperId": "80eec7deeffd76a42db351409d85e4a9165542b0",
        "title": "Long short-term memory and Learning-to-learn in networks of spiking neurons"
      },
      {
        "paperId": "42a8d7b6818ddd2864874407b3611b414f69de68",
        "title": "Autonomous Landing of the Quadrotor on the Mobile Platform via Meta Reinforcement Learning"
      },
      {
        "paperId": "508dd24dc7e5a5bd170444372801c7a2af2898f9",
        "title": "Meta Reinforcement Learning of Locomotion Policy for Quadruped Robots With Motor Stuck"
      },
      {
        "paperId": "1f0c107dbbc28aa376c394de9c1fd991a1df03ec",
        "title": "A Metareinforcement-Learning-Based Hyperspectral Image Classification With a Small Sample Set"
      },
      {
        "paperId": "2b2b9ecaf6d3b21aef5284931648937fc13facc8",
        "title": "Amortized Network Intervention to Steer the Excitatory Point Processes"
      },
      {
        "paperId": "914fe22dcbbb82c973aff31370c8f0d762661838",
        "title": "Distributional Meta-Gradient Reinforcement Learning"
      },
      {
        "paperId": "f103ea52dd91ab9f0d8d2fe85e6a75b403f10d2e",
        "title": "Implicit Training of Inference Network Models for Structured Prediction"
      },
      {
        "paperId": "e0bfd4f8f9cf89f59e4b1561e37b776eec2128f3",
        "title": "Reinforcement Learning Methods for Computing Offloading: A Systematic Review"
      },
      {
        "paperId": "01404a50a3c0c065da3391c60dda49a0cab36251",
        "title": "A Review and a Taxonomy of Edge Machine Learning: Requirements, Paradigms, and Techniques"
      },
      {
        "paperId": "a75bf7d612a2db4431458bdb61d44c2e6a9316df",
        "title": "MetaEx-GAN: Meta Exploration to Improve Natural Language Generation via Generative Adversarial Networks"
      },
      {
        "paperId": "ba95a186d33098d3ebd9c414af992904928d2a78",
        "title": "Hypothesis Network Planned Exploration for Rapid Meta-Reinforcement Learning Adaptation"
      },
      {
        "paperId": "acfcf5b58f912cf0ddcbc83cbf2e8242934eb83e",
        "title": "Statistical Learning under Heterogenous Distribution Shift"
      },
      {
        "paperId": "9bc63d773bc04cca984ad6b9f80949b64e56e536",
        "title": "Parameterizing Non-Parametric Meta-Reinforcement Learning Tasks via Subtask Decomposition"
      },
      {
        "paperId": "51dff7c43e1cd6fb0661cfb1e1c3a48115046b24",
        "title": "Doubly Robust Augmented Transfer for Meta-Reinforcement Learning"
      },
      {
        "paperId": "a059870910339596c5c57969bc2397a7e268435e",
        "title": "Learning Embodied Agents with Scalably-Supervised Reinforcement Learning"
      },
      {
        "paperId": "0b3a9633d7111453d51f3cbd794db285bbdb5d7b",
        "title": "A Discourse on MetODS: Meta-Optimized Dynamical Synapses for Meta-Reinforcement Learning"
      },
      {
        "paperId": "56112084f07e90f71ed2f4d03c793d661e855f64",
        "title": "A Bayesian Nonparametric Approach to Multi-Task Learning for Contextual Bandits in Mobile Health"
      },
      {
        "paperId": "7814709d014d107960bb6c25fd980c9dfb650051",
        "title": "Efficient Variance Reduction for Meta-learning"
      },
      {
        "paperId": "dbafc8a6306e3819d7a133ea284d7270b7bd5096",
        "title": "Surprise Minimizing Multi-Agent Learning with Energy-based Models"
      },
      {
        "paperId": "a0df9d4a1ed47d68eac8dd17cae8f38fc64fd5b9",
        "title": "Meta-Reinforcement Learning in Broad and Non-Parametric Environments"
      },
      {
        "paperId": "f3a5e8a9fa420ae0528c92727164eed81461c7c3",
        "title": "Prior Is All You Need to Improve the Robustness and Safety for the First Time Deployment of Meta RL"
      },
      {
        "paperId": "c7629a4d7e1c87fd1ea73850bcb800538fd0aa4b",
        "title": "VariBAD: Variational Bayes-Adaptive Deep RL via Meta-Learning"
      },
      {
        "paperId": "d27edce419e1c731c0aeb9e1842d1e022b2cc6ab",
        "title": "Offline Meta Reinforcement Learning - Identifiability Challenges and Effective Data Collection Strategies"
      },
      {
        "paperId": "c92ee32fe2d77bb6b8d4ce78c54ff3b838e0cb35",
        "title": "Settling the Bias and Variance of Meta-Gradient Estimation for Meta-Reinforcement Learning"
      },
      {
        "paperId": "1bb43790c5d1e5c347a17caaa2e6ddfc98deae3a",
        "title": "Meta-learning from sparse recovery"
      },
      {
        "paperId": "48c21bf65ca2583d56c49bccbc2742a595eb3bfe",
        "title": "Crossing the Reality Gap: a Survey on Sim-to-Real Transferability of Robot Controllers in Reinforcement Learning"
      },
      {
        "paperId": "a3e4c634d35eaee493f37a2129e47756b4f9f9aa",
        "title": "Demystifying Reproducibility in Meta- and Multi-Task Reinforcement Learning"
      },
      {
        "paperId": "839ea5421bc5515f1465d49613972b64cce61302",
        "title": "Effective, interpretable algorithms for curiosity automatically discovered by evolutionary search"
      },
      {
        "paperId": "26b6477ef683ef55c453d65fbcbf4e51e0d77b07",
        "title": "Learning to Play: Reinforcement Learning and Games"
      },
      {
        "paperId": "602940b9267e59fe252d6bdf9f8c216fd2237ed1",
        "title": "Q UICKEST CHANGE DETECTION FOR MULTI - TASK PROBLEMS UNDER UNKNOWN PARAMETERS"
      },
      {
        "paperId": "5a3b64e38054bb9372a70cf2abd0234d72e1035b",
        "title": "Promoting Stochasticity for Expressive Policies via a Simple and Efficient Regularization Method"
      },
      {
        "paperId": "cdcc0c98e19e0e952946366bc202800bef2ac676",
        "title": "A MODULAR REINFORCEMENT LEARNING METHOD FOR ADAPTABLE ROBOTIC ARMS"
      },
      {
        "paperId": "5654028d5193dbf8eaa5ab9ef6af21f6da265878",
        "title": "SKEW-FIT: STATE-COVERING SELF-SUPERVISED RE-"
      },
      {
        "paperId": "86c1cc0244651d813fab04ff1825fc8335a47c5a",
        "title": "Meta Exploration for Model-Agnostic Reinforcement Learning"
      },
      {
        "paperId": "41121dd73ce28ebe31ebf9c9a402cf575665ea32",
        "title": "Removing the i\u2019s from i.i.d : Testing generalization on hard datasets"
      },
      {
        "paperId": "38512b6062d161d3a72a1ee9a4926cfea76adfbd",
        "title": "DROPGRAD: GRADIENT DROPOUT REGULARIZATION"
      },
      {
        "paperId": "ff92730e0ad9d09597d0b57ed1402d0bf421a7ea",
        "title": "Improving Model Robustness via Automatically Incorporating Self-supervision Tasks"
      },
      {
        "paperId": "cd273494738c897aa7abfac145ccf0ecf099ab0a",
        "title": "Decoupled Meta-Learning with Structured Latents"
      },
      {
        "paperId": "c0b1d43f03c4d959a0bf57ab03ff6ef83f849872",
        "title": "Intrinsically Motivated and Interactive Reinforcement Learning: a Developmental Approach. (Apprentissage par Renforcement Intrins\u00e8quement Motiv\u00e9 et Interactif: une approche d\u00e9veloppementale)"
      },
      {
        "paperId": "a1a536d92551b6690761e36f413f53e8ce923d33",
        "title": "Sense, Think, Grasp: A study on visual and tactile information processing for autonomous manipulation"
      },
      {
        "paperId": "1645e14ce790b349e300355dba5032273baa6308",
        "title": "CHARACTERIZING AGENT BEHAVIOR UNDER META REINFORCEMENT LEARNING WITH GRIDWORLD"
      },
      {
        "paperId": "3efb7330d2ff9435f35296dd896b5b9aaf0d3b06",
        "title": "Control Adaptation via Meta-Learning Dynamics"
      },
      {
        "paperId": "510c6eaceb3e637f28da4a6f0923578346d9ae2a",
        "title": "Adaptive Reinforcement Learning with LLM-augmented Reward Functions"
      },
      {
        "paperId": "8732c4ab0c47c4f7b0c05bafbbb3311e53aa32ba",
        "title": "Robotics and Autonomous Systems"
      }
    ],
    "score": 51.0
  },
  {
    "id": "431dc05ac25510de6264084434254cca877f9ab3",
    "title": "Recovery RL: Safe Reinforcement Learning With Learned Recovery Zones",
    "authors": [
      "Brijen Thananjeyan",
      "A. Balakrishna",
      "Suraj Nair",
      "Michael Luo",
      "K. Srinivasan",
      "M. Hwang",
      "Joseph E. Gonzalez",
      "Julian Ibarz",
      "Chelsea Finn",
      "Ken Goldberg"
    ],
    "year": 2020,
    "citationCount": 247,
    "abstract": "Safety remains a central obstacle preventing widespread use of RL in the real world: learning new tasks in uncertain environments requires extensive exploration, but safety requires limiting exploration. We propose Recovery RL, an algorithm which navigates this tradeoff by (1) leveraging offline data to learn about constraint violating zones before policy learning and (2) separating the goals of improving task performance and constraint satisfaction across two policies: a task policy that only optimizes the task reward and a recovery policy that guides the agent to safety when constraint violation is likely. We evaluate Recovery RL on 6 simulation domains, including two contact-rich manipulation tasks and an image-based navigation task, and an image-based obstacle avoidance task on a physical robot. We compare Recovery RL to 5 prior safe RL methods which jointly optimize for task performance and safety via constrained optimization or reward shaping and find that Recovery RL outperforms the next best prior method across all domains. Results suggest that Recovery RL trades off constraint violations and task successes 2\u201320 times more efficiently in simulation domains and 3 times more efficiently in physical experiments. See https://tinyurl.com/rl-recovery for videos and supplementary material.",
    "url": "https://www.semanticscholar.org/paper/431dc05ac25510de6264084434254cca877f9ab3",
    "pdf_url": "https://arxiv.org/pdf/2010.15920.pdf",
    "venue": "IEEE Robotics and Automation Letters",
    "publicationDate": "2020-10-29",
    "externalIds": {
      "DBLP": "journals/corr/abs-2010-15920",
      "ArXiv": "2010.15920",
      "MAG": "3096918561",
      "DOI": "10.1109/LRA.2021.3070252",
      "CorpusId": 226221775
    },
    "references": [
      {
        "paperId": "96055d058984b15a9b83024bb2e07292ee7559f5",
        "title": "Learning to be Safe: Deep RL with a Safety Critic"
      },
      {
        "paperId": "2b6909d9302389f0fd981acdb99a9542efcbf168",
        "title": "Goal-Aware Prediction: Learning to Model What Matters"
      },
      {
        "paperId": "f6fa899c3c9605f00066fe189f6a1698d09fd916",
        "title": "Efficiently Calibrating Cable-Driven Surgical Robots With RGBD Sensing, Temporal Windowing, and Linear and Recurrent Neural Network Compensation"
      },
      {
        "paperId": "022ed6daae861295fe7d48fffd7cffb5da91d9ec",
        "title": "ABC-LMPC: Safe Sample-Based Learning MPC for Stochastic Nonlinear Dynamical Systems with Adjustable Boundary Conditions"
      },
      {
        "paperId": "46712dd04ab67286efb47a8c072360d0c25946a6",
        "title": "Worst Cases Policy Gradients"
      },
      {
        "paperId": "b094fc7057f77a6bf66ad07f6c68be148ca2b442",
        "title": "Robust Model Predictive Shielding for Safe Reinforcement Learning with Stochastic Dynamics"
      },
      {
        "paperId": "7a450675968d31b8363e21fb5d5b72474c128076",
        "title": "Deep Dynamics Models for Learning Dexterous Manipulation"
      },
      {
        "paperId": "51fe965c579a689d1dc466f2313227a07eb22126",
        "title": "Safety Augmented Value Estimation From Demonstrations (SAVED): Safe Deep Model-Based RL for Sparse Cost Robotic Tasks"
      },
      {
        "paperId": "331638e2105fd2ed70ec9e700255af1bc962f1be",
        "title": "Bridging Hamilton-Jacobi Safety Analysis and Reinforcement Learning"
      },
      {
        "paperId": "3fa50569925cfecc66fed5ec616682ecf3794ad7",
        "title": "Lyapunov-based Safe Policy Optimization for Continuous Control"
      },
      {
        "paperId": "12c0751b4f51ed833172a713b7e32390032ead93",
        "title": "Soft Actor-Critic Algorithms and Applications"
      },
      {
        "paperId": "54cd5a5ddd286442fa94da7ec344a7e76b9a6ccd",
        "title": "Visual Foresight: Model-Based Deep Reinforcement Learning for Vision-Based Robotic Control"
      },
      {
        "paperId": "eb37e7b76d26b75463df22b2a3aa32b6a765c672",
        "title": "QT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation"
      },
      {
        "paperId": "56136aa0b2c347cbcf3d50821f310c4253155026",
        "title": "Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models"
      },
      {
        "paperId": "cb7c479a36520da1caeeec67db10772351a390c6",
        "title": "Reward Constrained Policy Optimization"
      },
      {
        "paperId": "65fb1b37c41902793ac65db3532a6e51631a9aff",
        "title": "A Lyapunov-based Approach to Safe Reinforcement Learning"
      },
      {
        "paperId": "811df72e210e20de99719539505da54762a11c6d",
        "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor"
      },
      {
        "paperId": "ec4df801c640169e18d8da1bdc65a0b6fc2d3d94",
        "title": "Leave no Trace: Learning to Reset for Safe and Autonomous Reinforcement Learning"
      },
      {
        "paperId": "2d0fc3b6c8bca95f290c05ff5c64235e4c06fad4",
        "title": "Hamilton-Jacobi reachability: A brief overview and recent advances"
      },
      {
        "paperId": "e4b4066e46fd160cb84e246b0895a4cd674d8ce4",
        "title": "Safe Reinforcement Learning via Shielding"
      },
      {
        "paperId": "5712edce918859a9b22471dd0ed78ef4ea1fc724",
        "title": "Safe Visual Navigation via Deep Learning and Novelty Detection"
      },
      {
        "paperId": "7a4193d0b042643a8bb9ec262ed7f9d509bdb12e",
        "title": "Constrained Policy Optimization"
      },
      {
        "paperId": "88880d88073a99107bbc009c9f4a4197562e1e44",
        "title": "Safe Model-based Reinforcement Learning with Stability Guarantees"
      },
      {
        "paperId": "171bfa2abddd30ad177cd620c86b7f8fa64964d1",
        "title": "A General Safety Framework for Learning-Based Control in Uncertain Robotic Systems"
      },
      {
        "paperId": "e15369457639ffa1c8ef98860830514c444237c2",
        "title": "Learning Model Predictive Control for Iterative Tasks. A Data-Driven Control Framework"
      },
      {
        "paperId": "a9beebe284b2c70895d4f51fe14fc50eda41fc60",
        "title": "Safe Exploration in Finite Markov Decision Processes with Gaussian Processes"
      },
      {
        "paperId": "024006d4c2a89f7acacc6e4438d156525b60a98f",
        "title": "Continuous control with deep reinforcement learning"
      },
      {
        "paperId": "546f0c3922f87302b506b5fba4402004bfd6744d",
        "title": "Learning compound multi-step controllers under unknown dynamics"
      },
      {
        "paperId": "014da00e522ee5a5391a615d855287fb2d4a8f54",
        "title": "An open-source research kit for the da Vinci\u00ae Surgical System"
      },
      {
        "paperId": "54900e8f7ba2f9b94bf1604ec4be0361558bea52",
        "title": "Policy Gradients Beyond Expectations: Conditional Value-at-Risk"
      },
      {
        "paperId": "f0c4b7568f378e652645232e66a1dab4c5b5293f",
        "title": "Risk-Sensitive Reinforcement Learning"
      },
      {
        "paperId": "7655b41aa31e89b0381e89de79fc82b1d483182d",
        "title": "Guaranteed Safe Online Learning via Reachability: tracking a ground target using a quadrotor"
      },
      {
        "paperId": "69a2d3bfbb7cbaa53df983483421ce86b87d80e1",
        "title": "Risk-Sensitive Reinforcement Learning Applied to Control under Constraints"
      },
      {
        "paperId": "3cc2608fd77b9b65f5bd378e8797b2ab1b8acde7",
        "title": "Constrained Markov Decision Processes"
      },
      {
        "paperId": null,
        "title": "Efficiently calibrating cabledriven surgical robots with rgbd sensing"
      },
      {
        "paperId": null,
        "title": "Pytorch implementation of soft actor critic"
      },
      {
        "paperId": "4d0f6a6ffcd6ab04732ff76420fd9f8a7bb649c3",
        "title": "Benchmarking Safe Exploration in Deep Reinforcement Learning"
      },
      {
        "paperId": null,
        "title": "and U"
      },
      {
        "paperId": "f5d5a699770808228a2de1b5f99e76ce4bd2b9ee",
        "title": "Consideration of Risk in Reinforcement Learning"
      },
      {
        "paperId": null,
        "title": ". PREPRINT VERSION"
      }
    ],
    "cited_by": [
      {
        "paperId": "2ec7d7a5caa49c58443c0206b2c870b4dd9d5a6e",
        "title": "SPiDR: A Simple Approach for Zero-Shot Safety in Sim-to-Real Transfer"
      },
      {
        "paperId": "a84ad57caea4d15879bafc4e17dbe20f0ed38354",
        "title": "Robust and Resilient Soft Robotic Object Insertion with Compliance-Enabled Contact Formation and Failure Recovery"
      },
      {
        "paperId": "ffa62fff9d1431b5ecc53d9c8f94ffb71fa4f944",
        "title": "MSMAR-RL: Multi-Step Masked-Attention Recovery Reinforcement Learning for Safe Maneuver Decision in High-Speed Pursuit-Evasion Game"
      },
      {
        "paperId": "4b7843a00d2f7bc8bf79f31adf560bf6af27afbc",
        "title": "Reinforcement Learning-based Control via Y-wise Affine Neural Networks (YANNs)"
      },
      {
        "paperId": "50f0c65126cfc65b80bb04575ac64df54bbcafdb",
        "title": "Group Confident Policy Optimization"
      },
      {
        "paperId": "07f1886a757db272fa69471dc086fef06a76739b",
        "title": "\u201cFeariosity\u201d-Guided Reinforcement Learning for Safe and Efficient Autonomous End-to-End Navigation"
      },
      {
        "paperId": "5b40f45bc89b6d1fcf9d6daca992610793630b12",
        "title": "Failure Forecasting Boosts Robustness of Sim2Real Rhythmic Insertion Policies"
      },
      {
        "paperId": "5f148c77abebd10d7a74f56088b378af4be6bd08",
        "title": "Safe Reinforcement Learning with Constraints: A Survey"
      },
      {
        "paperId": "67532596a21e1d84b36a036a5a3e7dc6ec52dd63",
        "title": "A practical reinforcement learning control design for nonlinear systems with input and output constraints"
      },
      {
        "paperId": "5ab028de2dcd2b6aa8ab1245451e4a0caa8e3554",
        "title": "A Probabilistic Mechanism for Safe Reinforcement Learning"
      },
      {
        "paperId": "b31186f984a1808c3cd41146c6d1bdf2dbc9814f",
        "title": "SafeMimic: Towards Safe and Autonomous Human-to-Robot Imitation for Mobile Manipulation"
      },
      {
        "paperId": "315b1af3bdb7b94a9db7483db518f50b2f4c88f9",
        "title": "Research on Dynamic Monitoring and Fault Recovery Mechanism for Container Orchestration Platforms"
      },
      {
        "paperId": "b3f17385dc095298b4e63d0ea5af23c4864b2a24",
        "title": "Verification-Guided Falsification for Safe RL via Explainable Abstraction and Risk-Aware Exploration"
      },
      {
        "paperId": "cae97e770f6e07effec238ba02983c11a87bd92a",
        "title": "Integrating Reinforcement Learning and Virtual Fixtures for Safer Automatic Robotic Surgery"
      },
      {
        "paperId": "7eb59aa3eb96e66d714243919582eae5cc773ecd",
        "title": "Safety Probability Estimation in Dimension Reduction Space for Model-Free Safe Reinforcement Learning of Robotics"
      },
      {
        "paperId": "1cfcb419dc9d88563fa8dec3a141940b7f02809e",
        "title": "Accelerated Learning with Linear Temporal Logic using Differentiable Simulation"
      },
      {
        "paperId": "fa9cc69ac7acdf876120472bb2a66e45f5d1fa4c",
        "title": "The Application of Deep Network and Reinforcement Learning for Art Design in Graphical User Interface Wireframe Generation"
      },
      {
        "paperId": "ae7a54eed5b97b0c034c5f34f372de057052ffee",
        "title": "Parallel Multistep Evaluation With Efficient Data Utilization for Safe Neural Critic Control and Its Application to Orbital Maneuver Systems"
      },
      {
        "paperId": "2e7f982d56720ce15915196069c90f28379b41a5",
        "title": "Runtime Safety through Adaptive Shielding: From Hidden Parameter Inference to Provable Guarantees"
      },
      {
        "paperId": "ce8cbc83e0b77d1576d23accbacf467f6f97e009",
        "title": "Model-Free Safety Filter for Soft Robots: A Q-Learning Approach"
      },
      {
        "paperId": "02033405c3af3e591450d6738a288af91343b373",
        "title": "Robust Planning for Autonomous Driving via Mixed Adversarial Diffusion Predictions"
      },
      {
        "paperId": "8268adc75edd365b3c836ac8a8619e3476906112",
        "title": "Feasibility-Aware Pessimistic Estimation: Toward Long-Horizon Safety in Offline RL"
      },
      {
        "paperId": "fad94bf31f04e693180e6228e5351fa973fb0f2c",
        "title": "Barrier Function Overrides For Non-Convex Fixed Wing Flight Control and Self-Driving Cars"
      },
      {
        "paperId": "7b5290473fd3a040a6bee5edb391eb1a99e55d95",
        "title": "Skill-based Safe Reinforcement Learning with Risk Planning"
      },
      {
        "paperId": "459c0fa2605be2c3e4febc7f95b6178d9760809a",
        "title": "Uncertainty-aware Latent Safety Filters for Avoiding Out-of-Distribution Failures"
      },
      {
        "paperId": "ef932f5f34a67462f7d448bb30b1fbd5eaf2f3f2",
        "title": "Deep Reinforcement Learning for Automated Insulin Delivery Systems: Algorithms, Applications, and Prospects"
      },
      {
        "paperId": "1b11fe1ea5376e869bd41b53aed08ee5efc3070a",
        "title": "Reinforcement Learning for Fail-Operational Systems with Disentangled Dual-Skill Variables"
      },
      {
        "paperId": "f8cabb9734a7ce9733efbb4c2ac0276de4488901",
        "title": "Dependable Policy Improvement for Intelligent Agents in New Environments"
      },
      {
        "paperId": "32b8b629cde2b2792e0834655d034ac5ca207c01",
        "title": "Bresa: Bio-inspired Reflexive Safe Reinforcement Learning for Contact-Rich Robotic Tasks"
      },
      {
        "paperId": "7fc4284b11bb5e24315b0eeff8834d044e6f4b96",
        "title": "Efficient Action-Constrained Reinforcement Learning via Acceptance-Rejection Method and Augmented MDPs"
      },
      {
        "paperId": "4c1b7cf0550130a2aca6215b759cb09c76b19978",
        "title": "SafeVLA: Towards Safety Alignment of Vision-Language-Action Model via Constrained Learning"
      },
      {
        "paperId": "78602131a1901b340f93680e541904977522de97",
        "title": "Passivity-Centric Safe Reinforcement Learning for Contact-Rich Robotic Tasks"
      },
      {
        "paperId": "de082f150b5e7b8db15d047a154eae76465f4b35",
        "title": "Multi-robot hierarchical safe reinforcement learning autonomous decision-making strategy based on uniformly ultimate boundedness constraints"
      },
      {
        "paperId": "cbc0ba27982d42acb1cce9115fc7b4f0fabcff1a",
        "title": "Modified Reinforcement Learning With Pid Control for Obstacle Avoidance in Autonomous Robots"
      },
      {
        "paperId": "8fdbec698c8d19e8df4f871db997fee55ee32b0d",
        "title": "Leveraging Constraint Violation Signals For Action-Constrained Reinforcement Learning"
      },
      {
        "paperId": "73b6391b4ef112d385dccf1b639be4c693e4723a",
        "title": "Generalizing Safety Beyond Collision-Avoidance via Latent-Space Reachability Analysis"
      },
      {
        "paperId": "846384c882bb29e580a1951b21771f4c1ae9123d",
        "title": "A safe-enhanced fully closed-loop artificial pancreas controller based on deep reinforcement learning"
      },
      {
        "paperId": "a1627f6dd425933b4f1c0e172227ae389ca6b1b3",
        "title": "Safety\u2010Critical Optimal Control of Discrete\u2010Time Non\u2010Linear Systems via Policy Iteration\u2010Based Q\u2010Learning"
      },
      {
        "paperId": "7c83a2598be6c1ad3e1de2d1d0fdec6985b34744",
        "title": "Back to Base: Towards Hands-Off Learning via Safe Resets with Reach-Avoid Safety Filters"
      },
      {
        "paperId": "16046af49186c0b403987b54b0d0922592329ccb",
        "title": "Autonomous Driving via Knowledge-Enhanced Safe Reinforcement Learning"
      },
      {
        "paperId": "ea2f030235313983c283c137a9911624a20bca42",
        "title": "Safe Multiagent Coordination via Entropic Exploration"
      },
      {
        "paperId": "94ab4e97bb7ba62e2b7e45c6c450b24af980db08",
        "title": "A Dynamic Safety Shield for Safe and Efficient Reinforcement Learning of Navigation Tasks"
      },
      {
        "paperId": "23d000275c535fa4a5941a178abe8d944f37e040",
        "title": "GRAM: Generalization in Deep RL with a Robust Adaptation Module"
      },
      {
        "paperId": "11d89b4578abfb7b34e7378db61412040b28ecb1",
        "title": "A Survey on Recent Advancements in Autonomous Driving Using Deep Reinforcement Learning: Applications, Challenges, and Solutions"
      },
      {
        "paperId": "f6641a894e349a40c20f81ea99d66bfb39c19839",
        "title": "Safe Reinforcement Learning of Robot Trajectories in the Presence of Moving Obstacles"
      },
      {
        "paperId": "95b4f289c3bc9083f113bb6d5daea27da151b2b9",
        "title": "Primal-Dual Multi-Agent Trust Region Policy Optimization for Safe Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "388af6b0785671e2e2e055cc56be42a8f7f6a8f4",
        "title": "Predicting Future Actions of Reinforcement Learning Agents"
      },
      {
        "paperId": "4c590291dbbefa850d56864639a8fa4d2ada7b8b",
        "title": "RecoveryChaining: Learning Local Recovery Policies for Robust Manipulation"
      },
      {
        "paperId": "dcd89dd8b926ffcbe22fc5bbb06ce03aa2fdd93e",
        "title": "Repairing Neural Networks for Safety in Robotic Systems using Predictive Models"
      },
      {
        "paperId": "0d6810090d2ae12cd20530f8f883d635dcf4b5c5",
        "title": "ActSafe: Active Exploration with Safety Constraints for Reinforcement Learning"
      },
      {
        "paperId": "96d57f72fba28f12472ba8cafa517f5efc2d96b6",
        "title": "A Retention-Centric Framework for Continual Learning with Guaranteed Model Developmental Safety"
      },
      {
        "paperId": "44b377577bb89bdf6ffc5d967fa12882df378c7c",
        "title": "Absolute State-wise Constrained Policy Optimization: High-Probability State-wise Constraints Satisfaction"
      },
      {
        "paperId": "65066761220dd8dbc74a54d241325ce11291086f",
        "title": "Enhancing Autonomous Lane-Changing Safety: Deep Reinforcement Learning via Pre-Exploration in Parallel Imaginary Environments"
      },
      {
        "paperId": "4d333aecca0acfc1d023c93ca0b4b379c852796c",
        "title": "NavRL: Learning Safe Flight in Dynamic Environments"
      },
      {
        "paperId": "3ec4c31e7d582cec6c2c15be4e9d7aa7f20a9fda",
        "title": "SafeRPlan: Safe deep reinforcement learning for intraoperative planning of pedicle screw placement"
      },
      {
        "paperId": "648320026e0ca0a88a4b7255bd0c463a6b7dae59",
        "title": "Critic as Lyapunov function (CALF): a model-free, stability-ensuring agent"
      },
      {
        "paperId": "b2d827c286e32dbf0739e8c796b119b1074809b4",
        "title": "Meta SAC-Lag: Towards Deployable Safe Reinforcement Learning via MetaGradient-based Hyperparameter Tuning"
      },
      {
        "paperId": "d377908df88d5a7fbce1d7a5eac9e60e0313be66",
        "title": "A Safe Exploration Strategy for Model-Free Task Adaptation in Safety-Constrained Grid Environments"
      },
      {
        "paperId": "1e95b30d5eb765c0f2dc07b19998a0a6f1adfe39",
        "title": "Anomalous State Sequence Modeling to Enhance Safety in Reinforcement Learning"
      },
      {
        "paperId": "0a13b36f2dbd124b9004cdd57e5264d178f9a1db",
        "title": "Learning of Energy Primitives for Electrified Aircraft"
      },
      {
        "paperId": "256350be05a1762bd5f7ef843e9ce0004074d753",
        "title": "Task-Oriented Reinforcement Learning with Interest State Representation"
      },
      {
        "paperId": "e1f2d6b7b1c3e69e3781dcd5693548d2e69fca55",
        "title": "FOSP: Fine-tuning Offline Safe Policy through World Models"
      },
      {
        "paperId": "22059156abe3c2a80ee5f1acea8f52c2e3536e86",
        "title": "Learn From Safe Experience: Safe Reinforcement Learning for Task Automation of Surgical Robot"
      },
      {
        "paperId": "cbace136a148b9174b47ea3236cf473c3a30a4a6",
        "title": "Robust Barrier-Certified Safe Learning-based Adaptive Control for Multi-Agent Systems in Presence of Uncertain Environments"
      },
      {
        "paperId": "b2e560236682f0decc37ba5efdc35c231e38887b",
        "title": "Imperative Learning: A Self-supervised Neural-Symbolic Learning Framework for Robot Autonomy"
      },
      {
        "paperId": "fa4e16615abc6d25d82da0c9c867a3e0a66bdc5a",
        "title": "To Err is Robotic: Rapid Value-Based Trial-and-Error during Deployment"
      },
      {
        "paperId": "ef0cd988e78c8f84bef6225a59aeee973d4ebc82",
        "title": "CIMRL: Combining IMitation and Reinforcement Learning for Safe Autonomous Driving"
      },
      {
        "paperId": "c8c6c810d4fce999f47898c6b1cefe4b20313925",
        "title": "Safety Alignment Should Be Made More Than Just a Few Tokens Deep"
      },
      {
        "paperId": "6ffcf0e29dee9b8d1d3312687af88c2bd3b657b9",
        "title": "GenSafe: A Generalizable Safety Enhancer for Safe Reinforcement Learning Algorithms Based on Reduced Order Markov Decision Process Model"
      },
      {
        "paperId": "39c8b1932ff48a856ac3132a2b09da234f403f2d",
        "title": "Learning-based legged locomotion; state of the art and future perspectives"
      },
      {
        "paperId": "0345be96b3e07ddbc973259c71fe23b5716aa095",
        "title": "SRL-VIC: A Variable Stiffness-Based Safe Reinforcement Learning for Contact-Rich Robotic Tasks"
      },
      {
        "paperId": "1bf1bf7a9bede65aa6653461bab60baf72dd4098",
        "title": "Learning to Recover from Plan Execution Errors during Robot Manipulation: A Neuro-symbolic Approach"
      },
      {
        "paperId": "78aa1431cb5a239a31a96497b8928d0e83230646",
        "title": "Safe Reinforcement Learning in Black-Box Environments via Adaptive Shielding"
      },
      {
        "paperId": "7f136c5c8ac18bffd045d7f21fa5de5cf939da15",
        "title": "Diagnosing and Predicting Autonomous Vehicle Operational Safety Using Multiple Simulation Modalities and a Virtual Environment"
      },
      {
        "paperId": "94cd9c6b5116ff36a1b02dedf4cb9c573bd382db",
        "title": "A Hierarchical Framework for Robot Safety using Whole-body Tactile Sensors"
      },
      {
        "paperId": "9c0bac23e5e0954b767d2d167bbff4081002c484",
        "title": "Safe Reinforcement Learning with Learned Non-Markovian Safety Constraints"
      },
      {
        "paperId": "ca275d9ef374dbbf81a6f6175cac2311526308c3",
        "title": "IntervenGen: Interventional Data Generation for Robust and Data-Efficient Robot Imitation Learning"
      },
      {
        "paperId": "878e00d8bd65cb3d4582222d0e07568d925e0e57",
        "title": "Robotic Systems Design in Endovascular Treatment"
      },
      {
        "paperId": "c29795142a96d980ab3e629cb723a6f6af22ad7e",
        "title": "Safe Reinforcement Learning-Based Motion Planning for Functional Mobile Robots Suffering Uncontrollable Mobile Robots"
      },
      {
        "paperId": "94dd63ef455f89019a51c254d83b2effdeab3eb1",
        "title": "Gameplay Filters: Robust Zero-Shot Safety through Adversarial Imagination"
      },
      {
        "paperId": "18883e42afc7d288cd218db59878fe2d9b3ed679",
        "title": "Model-based safe reinforcement learning for nonlinear systems under uncertainty with constraints tightening approach"
      },
      {
        "paperId": "5463d2c8fc5c6144e288117234d8fd176ec5d1c8",
        "title": "Synthesizing Control Barrier Functions With Feasible Region Iteration for Safe Reinforcement Learning"
      },
      {
        "paperId": "7eab8816a914b7a8b275382c8d0b8d6a1b3841f0",
        "title": "Long and Short-Term Constraints Driven Safe Reinforcement Learning for Autonomous Driving"
      },
      {
        "paperId": "1458fcc3fe2bf7ecdc1c51e29709cf17d0ac269f",
        "title": "Physics-informed RL for Maximal Safety Probability Estimation"
      },
      {
        "paperId": "f0209a08dd7f48ea7d8c63f29fc575b4c74bf032",
        "title": "Learning Hierarchical Control Systems for Autonomous Systems with Energy Constraints"
      },
      {
        "paperId": "2018cc857bf07b1201d8dd7f55cab9f0ae025192",
        "title": "Enhancing Safety in Learning from Demonstration Algorithms via Control Barrier Function Shielding"
      },
      {
        "paperId": "bf80cd626a592dac8e2314104167d9a2e418ca9c",
        "title": "Offline Goal-Conditioned Reinforcement Learning for Safety-Critical Tasks with Recovery Policy"
      },
      {
        "paperId": "e9e1b57dfc99969871372adb7cadf3fa154026ee",
        "title": "Deep reinforcement learning based energy management strategies for electrified vehicles: Recent advances and perspectives"
      },
      {
        "paperId": "51073cb61b40bb1ba2ceb8d25f7afd263a01f905",
        "title": "Pre-Grasp Approaching on Mobile Robots: A Pre-Active Layered Approach"
      },
      {
        "paperId": "d1f30df851e2e2d883988f2a4910889002e9e45c",
        "title": "Uniformly Safe RL with Objective Suppression for Multi-Constraint Safety-Critical Applications"
      },
      {
        "paperId": "e63f0dc1963db342f8e87be1b565f4273404db5b",
        "title": "Safety Optimized Reinforcement Learning via Multi-Objective Policy Optimization"
      },
      {
        "paperId": "290c4f09e376395dd61e11a8431c5b56cc3a874d",
        "title": "Safety Filters for Black-Box Dynamical Systems by Learning Discriminating Hyperplanes"
      },
      {
        "paperId": "3a69da89418bfcd675ab7b5e6225b23b240cf9d1",
        "title": "Shielded Planning Guided Data-Efficient and Safe Reinforcement Learning"
      },
      {
        "paperId": "77d3e323ceb51681033bc484fe70832e1fa328eb",
        "title": "Reinforcement Learning with Ensemble Model Predictive Safety Certification"
      },
      {
        "paperId": "1f356f5271c60794dc3b7448954b4bdc14ed4cac",
        "title": "A Survey of Constraint Formulations in Safe Reinforcement Learning"
      },
      {
        "paperId": "c59812a09d67800d7cb893bc25a328cb24b8aa16",
        "title": "Agile But Safe: Learning Collision-Free High-Speed Legged Locomotion"
      },
      {
        "paperId": "a2246e09ab0b56ba91322747addad030eae56c50",
        "title": "Safe Offline Reinforcement Learning with Feasibility-Guided Diffusion Model"
      },
      {
        "paperId": "ff41838ee635b902e3f8c316b4df19dcec7745d5",
        "title": "Gradient Shaping for Multi-Constraint Safe Reinforcement Learning"
      },
      {
        "paperId": "c0584bcf6bd89ea9871f053250e8faad7570745c",
        "title": "Autonomous Driving Based on Approximate Safe Action"
      },
      {
        "paperId": "55ee6525865f42155158a36985474f5f8567e3d9",
        "title": "FT-Net: Learning Failure Recovery and Fault-Tolerant Locomotion for Quadruped Robots"
      },
      {
        "paperId": "95a8ed69372548b825778bd073fb1951843b74f7",
        "title": "How to ensure a safe control strategy? Towards a SRL for urban transit autonomous operation"
      },
      {
        "paperId": "c4f7b34c5087844f26c05ab0a30051260292a221",
        "title": "A Method to Improve the Performance of Reinforcement Learning Based on the Y Operator for a Class of Stochastic Differential Equation-Based Child-Mother Systems"
      },
      {
        "paperId": "1de31db47dec81ed82d9df429c46ba2bc45bfeba",
        "title": "Learning Safety Critics via a Non-Contractive Binary Bellman Operator"
      },
      {
        "paperId": "5eaac0008a2fc75994089fb3fdb8872213da8dbc",
        "title": "Safe reinforcement learning for autonomous vehicles to make lane-change decisions: Constraint based on Incomplete Information Game Theory"
      },
      {
        "paperId": "c96645997967df130d425932f06f3cfe89962d0f",
        "title": "Model-Based Runtime Monitoring with Interactive Imitation Learning"
      },
      {
        "paperId": "2eaa070b0b1579ee2181497178a5a865da22e1af",
        "title": "Reduced Policy Optimization for Continuous Control with Hard Constraints"
      },
      {
        "paperId": "90297b6d751be3b515b7a3545fb510b9dcb6a985",
        "title": "Robot Learning Incorporating Human Interventions in the Real World for Autonomous Surgical Endoscopic Camera Control"
      },
      {
        "paperId": "1b2a4e8da37e192eef240c46a7239440ce3a28ed",
        "title": "A safe reinforcement learning approach for autonomous navigation of mobile robots in dynamic environments"
      },
      {
        "paperId": "292c65e005d0b1b3d38c69783a4fc62f183bcd88",
        "title": "Learning a Better Control Barrier Function Under Uncertain Dynamics"
      },
      {
        "paperId": "6fdf3348b684376e9ddfe8ffbeaac48eeb601720",
        "title": "Fear-Neuro-Inspired Reinforcement Learning for Safe Autonomous Driving"
      },
      {
        "paperId": "c03fe4316a7e0e589cc9e91cf133eaa2b4620362",
        "title": "Safe Reinforcement Learning via Hierarchical Adaptive Chance-Constraint Safeguards"
      },
      {
        "paperId": "b9dff780b3cbe81adca8f063a1fdcc699db0f79f",
        "title": "Constraint-Conditioned Policy Optimization for Versatile Safe Reinforcement Learning"
      },
      {
        "paperId": "d3d7fa13b3f9b533647ac84fc2e446ef98211b08",
        "title": "Safe Exploration in Reinforcement Learning: A Generalized Formulation and Algorithms"
      },
      {
        "paperId": "10370e6e9393a9927aed8b9ab37d0b032ebac17a",
        "title": "A safe reinforcement learning-based charging strategy for electric vehicles in residential microgrid"
      },
      {
        "paperId": "334a004ee7de23fb3a6ca5507fd0adf85b91d548",
        "title": "Risk-Sensitive Mobile Robot Navigation in Crowded Environment via Offline Reinforcement Learning"
      },
      {
        "paperId": "73ee1392b2cf8a8ed5eb0d93c15ffbb6e61ffeab",
        "title": "Learning to Recover for Safe Reinforcement Learning"
      },
      {
        "paperId": "c3207f4155fc4ac26e38e940255a2ecc2180a948",
        "title": "Guided Online Distillation: Promoting Safe Reinforcement Learning by Offline Demonstration"
      },
      {
        "paperId": "b063eb34e666030ff8ae84a9688bb3dc6ab38572",
        "title": "Closing the Loop on Runtime Monitors with Fallback-Safe MPC"
      },
      {
        "paperId": "2368d84435085af4cb680bcacf68a8c2fce721dc",
        "title": "The Safety Filter: A Unified View of Safety-Critical Control in Autonomous Systems"
      },
      {
        "paperId": "2de6cb834f04a0e8f2db229649290de22304b770",
        "title": "A Distributed and Privacy-Aware High-Throughput Transaction Scheduling Approach for Scaling Blockchain"
      },
      {
        "paperId": "984beef1e3c012bd84645486242f0a61a74aca61",
        "title": "Learn With Imagination: Safe Set Guided State-wise Constrained Policy Optimization"
      },
      {
        "paperId": "1950323025b083ab11a54cc2f72875cb752cedaf",
        "title": "A deep reinforcement learning\u2010based maintenance optimization for vacuum packaging machines considering product quality degradation"
      },
      {
        "paperId": "0fe76519cdd0d09e40b0eb532eccf42d6ad532b8",
        "title": "Safe Data-Driven Model Predictive Control of Systems With Complex Dynamics"
      },
      {
        "paperId": "31d481a2d4b1a13aee09b592ccdfeb4a4f6e3a01",
        "title": "SeRO: Self-Supervised Reinforcement Learning for Recovery from Out-of-Distribution Situations"
      },
      {
        "paperId": "013157c0f88471694de0e8cb1804328a83a38273",
        "title": "Adversarial Behavior Exclusion for Safe Reinforcement Learning"
      },
      {
        "paperId": "8ca9a74503c240b2746e351995ee0415657f1cd0",
        "title": "Reinforcement Learning by Guided Safe Exploration"
      },
      {
        "paperId": "8f65c327891cd28790b23f139843834828cf6724",
        "title": "Safe Reinforcement Learning for Strategic Bidding of Virtual Power Plants in Day-Ahead Markets"
      },
      {
        "paperId": "07f59677ee85bb7bf07b9f6dc665963c78ed7233",
        "title": "Safe batch constrained deep reinforcement learning with generative adversarial network"
      },
      {
        "paperId": "9eed36fb9b9bbb97579953d5e303dc0cf6c70a58",
        "title": "Safe Reinforcement Learning With Dead-Ends Avoidance and Recovery"
      },
      {
        "paperId": "e25a311f855ace4ef2510cbf80f17426723787fc",
        "title": "Safe Online Integral Reinforcement Learning for Control Systems via Controller Decomposition"
      },
      {
        "paperId": "03d24a1ba3c289da414995e17147ced9da05eeee",
        "title": "Safe Explainable Agents for Autonomous Navigation using Evolving Behavior Trees"
      },
      {
        "paperId": "7fc69210214a55076b6109d6628120c96cdf825e",
        "title": "Demonstration-free Autonomous Reinforcement Learning via Implicit and Bidirectional Curriculum"
      },
      {
        "paperId": "f1d63e7035d2a8b6fb4d0c007d9ef0e1143696ee",
        "title": "Reinforcement Learning for Safe Robot Control using Control Lyapunov Barrier Functions"
      },
      {
        "paperId": "bc76637b1dc78b9825817c7148f0d06704e5cf33",
        "title": "Real-Time Coordinated Operation of Power and Autonomous Electric Ride-Hailing Systems"
      },
      {
        "paperId": "af579fc5949f6547f6557cf0a9d63c0271c591bb",
        "title": "Enhancing Deep Reinforcement Learning with Executable Specifications"
      },
      {
        "paperId": "c6f2e15980449192f4173bebdbc33558f5c97549",
        "title": "Towards Safe and Efficient Reinforcement Learning for Surgical Robots Using Real-Time Human Supervision and Demonstration"
      },
      {
        "paperId": "176886f70478884410f81c3a2223d129a0a9d9a1",
        "title": "Safe Reinforcement Learning via a Model-Free Safety Certifier"
      },
      {
        "paperId": "ceb90e2f14495d4d524ebf49e71279c79b530a1e",
        "title": "Model-based Dynamic Shielding for Safe and Efficient Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "6241c70a0b196ca302b9346d4743bd309f28d38d",
        "title": "Safe Reinforcement Learning for High-speed Autonomous Racing"
      },
      {
        "paperId": "e9abbbf1e64cd972fb2e8bbc1ffe983c8cdc640e",
        "title": "A Survey of Demonstration Learning"
      },
      {
        "paperId": "b169f2ce55e14584d2db6f64eeb5ad2702d39d40",
        "title": "Artificial intelligence foundation and pre-trained models: Fundamentals, applications, opportunities, and social impacts"
      },
      {
        "paperId": "1d872bdcf8fd5bb8b033ece24eb31cba5d9476c9",
        "title": "Adaptive Aggregation for Safety-Critical Control"
      },
      {
        "paperId": "23b1f60fbf01e4ec9f49b23d5f7d1cdc189d6ded",
        "title": "State-wise Safe Reinforcement Learning: A Survey"
      },
      {
        "paperId": "29c218fa2bf6e2f01c6bd8f3e4db95fcc7b47fb4",
        "title": "Optimal Transport Perturbations for Safe Reinforcement Learning with Robustness Guarantees"
      },
      {
        "paperId": "2260cf7df47436c682545cc75de67ae689ad645b",
        "title": "Trust Region-Based Safe Distributional Reinforcement Learning for Multiple Constraints"
      },
      {
        "paperId": "c2dbb66df7639f5daa0f8010c69951dcf7032a7c",
        "title": "Safe Reinforcement Learning for an Energy-Efficient Driver Assistance System"
      },
      {
        "paperId": "35a127391f4d67479857c9bad29d79682782eb9d",
        "title": "Safety Filtering for Reinforcement Learning-based Adaptive Cruise Control"
      },
      {
        "paperId": "f44682bbd7d27dc0b4c708437b54806c61ee1a6b",
        "title": "Risk-Sensitive Policy with Distributional Reinforcement Learning"
      },
      {
        "paperId": "b58ef62a34bdbcd357824c9e58d89cbbcc0e2375",
        "title": "Evaluating Model-free Reinforcement Learning toward Safety-critical Tasks"
      },
      {
        "paperId": "90a17da7eebfa88d1aa9e8d9f4fd7e4f0085721b",
        "title": "Safe reinforcement learning for dynamical systems using barrier certificates"
      },
      {
        "paperId": "f6d6ac35038ee3e3b82160e5a27bbb7b72c050bf",
        "title": "ISAACS: Iterative Soft Adversarial Actor-Critic for Safety"
      },
      {
        "paperId": "66bffa8b9ca59acdb70b6260d5e34332befd7424",
        "title": "Safe reinforcement learning for affine nonlinear systems with state constraints and input saturation using control barrier functions"
      },
      {
        "paperId": "54f2331f75f8330203ef2402efa3102de6bcab99",
        "title": "Safety Guided Policy Optimization"
      },
      {
        "paperId": "b46d843c08e707ad3d6fe31c7154ca0cb541f230",
        "title": "Robust Sim2Real Transfer with the da Vinci Research Kit: A Study On Camera, Lighting, and Physics Domain Randomization"
      },
      {
        "paperId": "8faf8a1f9f00eafbd4f4e4bab2b82332a6b84c3f",
        "title": "SafeTAC: Safe Tsallis Actor-Critic Reinforcement Learning for Safer Exploration"
      },
      {
        "paperId": "4d157fda990f939e7e7df8334b692b4bcf8195c6",
        "title": "Safe Policy Improvement in Constrained Markov Decision Processes"
      },
      {
        "paperId": "16bc62ca0672c7cdd0ef693b5ae0a73731af41a8",
        "title": "When to Ask for Help: Proactive Interventions in Autonomous Reinforcement Learning"
      },
      {
        "paperId": "69bdc99655204190697067c3da5296e544e6865d",
        "title": "Safe Model-Based Reinforcement Learning With an Uncertainty-Aware Reachability Certificate"
      },
      {
        "paperId": "a1e7be62296af87f5a5786c313d22fa9af143bcd",
        "title": "Monte Carlo Augmented Actor-Critic for Sparse Reward Deep Reinforcement Learning from Suboptimal Demonstrations"
      },
      {
        "paperId": "25425e299101b13ec2872417a14f961f4f8aa18e",
        "title": "VIMA: General Robot Manipulation with Multimodal Prompts"
      },
      {
        "paperId": "8e6a4da891d1eb1a05b2ecc7b5ebb74675c545e1",
        "title": "Safe Exploration Method for Reinforcement Learning under Existence of Disturbance"
      },
      {
        "paperId": "7b47b49f0aa0da4468c91c7821d1ee7873a8e84f",
        "title": "Safe Reinforcement Learning of Dynamic High-Dimensional Robotic Tasks: Navigation, Manipulation, Interaction"
      },
      {
        "paperId": "f449269a16a6d447fe8073b5e4b941cc724ea833",
        "title": "Efficient Recovery Learning using Model Predictive Meta-Reasoning"
      },
      {
        "paperId": "308a801dafad411f092a2b0303d5802c3c75169b",
        "title": "Trustworthy Reinforcement Learning Against Intrinsic Vulnerabilities: Robustness, Safety, and Generalizability"
      },
      {
        "paperId": "36363f225a0c3db468695578fdc16c4d49a473c3",
        "title": "Safe Reinforcement Learning with Contrastive Risk Prediction"
      },
      {
        "paperId": "f99e04b5d027dfb4e3386d626c16dcd3a18571d6",
        "title": "Cooperative Guidance of Multiple Missiles: A Hybrid Coevolutionary Approach"
      },
      {
        "paperId": "4a9018b08ced98a84998348bd55d5be26af5947f",
        "title": "Reinforcement Learning of Space Robotic Manipulation with Multiple Safety Constraints"
      },
      {
        "paperId": "7cf3140e8bd8640bca79e7941e3e1bfc53172f33",
        "title": "Back to the Manifold: Recovering from Out-of-Distribution States"
      },
      {
        "paperId": "6ffa21166c99b1156612ba831ef0e5f7755ee6b3",
        "title": "Efficient Off-Policy Safe Reinforcement Learning Using Trust Region Conditional Value At Risk"
      },
      {
        "paperId": "40c2d7e241b3b27d440bfcabf9ebfe7114e7f540",
        "title": "Fleet-DAgger: Interactive Robot Fleet Learning with Scalable Human Supervision"
      },
      {
        "paperId": "b75ff79617904f769c5f61cea3693e021757c600",
        "title": "Exploring Safer Behaviors for Deep Reinforcement Learning"
      },
      {
        "paperId": "f06854a6533a1425c5e73d7deba73f5b21b3f2f3",
        "title": "SafeRL-Kit: Evaluating Efficient Reinforcement Learning Methods for Safe Autonomous Driving"
      },
      {
        "paperId": "274f0cee47c8a5bbc38c32de6787adee6a08f69e",
        "title": "On the Robustness of Safe Reinforcement Learning under Observational Perturbations"
      },
      {
        "paperId": "7310a959dae090f520a63982c933f348124f1dcd",
        "title": "Reachability Constrained Reinforcement Learning"
      },
      {
        "paperId": "b1fb5611b7e0e92f1f9d2f269e6e7ce8b4d1cedb",
        "title": "Provably Safe Reinforcement Learning: Conceptual Analysis, Survey, and Benchmarking"
      },
      {
        "paperId": "443303964d619cd6501d0e912a55a9459d884156",
        "title": "Learning a Better Control Barrier Function"
      },
      {
        "paperId": "81bbf1bccd16e145ae3933e829aa41d2c8f7faa8",
        "title": "SAAC: Safe Reinforcement Learning as an Adversarial Game of Actor-Critics"
      },
      {
        "paperId": "92522c9985213b58dd1a62531116982609acd2c2",
        "title": "Automating Reinforcement Learning With Example-Based Resets"
      },
      {
        "paperId": "7b4d4e974ff96e04823bc96ed13a4f679fcb03fe",
        "title": "TRC: Trust Region Conditional Value at Risk for Safe Reinforcement Learning"
      },
      {
        "paperId": "1475d58c46fc5f04a80cd3215ca068292663daf5",
        "title": "Offline reinforcement learning application in robotic manipulation with a COG method case"
      },
      {
        "paperId": "466db1c53d8d06b5795b14ec52e8bf94df3eed6a",
        "title": "Doubly Pessimistic Algorithms for Strictly Safe Off-Policy Optimization"
      },
      {
        "paperId": "a90b9c22950e03f4f38ed42d7ca2422ca565cc73",
        "title": "Safe Reinforcement Learning for Legged Locomotion"
      },
      {
        "paperId": "1015c935649824664c206fabe546f13995d156aa",
        "title": "Fail-Safe Adversarial Generative Imitation Learning"
      },
      {
        "paperId": "4d64294ba319555885227ce5101e78b13d91781d",
        "title": "L2C2: Locally Lipschitz Continuous Constraint towards Stable and Smooth Reinforcement Learning"
      },
      {
        "paperId": "d8da5d28807fe5f8af2e6942c67738bd9f12a1ec",
        "title": "Safe Reinforcement Learning by Imagining the Near Future"
      },
      {
        "paperId": "1947bf44da309a3fc7bc50af0a6a09553d345c76",
        "title": "Failure Prediction with Statistical Guarantees for Vision-Based Robot Control"
      },
      {
        "paperId": "f28a97e857b7857291665c98ba7ba414d64da9c4",
        "title": "SAFER: Data-Efficient and Safe Reinforcement Learning via Skill Acquisition"
      },
      {
        "paperId": "a85d12930bbbc031427a89301aa7e20b5121705a",
        "title": "Joint Differentiable Optimization and Verification for Certified Reinforcement Learning"
      },
      {
        "paperId": "4b89f78987e5d0dba67a4f533945a838a5428ed9",
        "title": "Constrained Variational Policy Optimization for Safe Reinforcement Learning"
      },
      {
        "paperId": "0ba7f2e592dda172bc3d07f88cdcf2a57830deec",
        "title": "Towards Safe Reinforcement Learning with a Safety Editor Policy"
      },
      {
        "paperId": "4d9c9d5e5afc3313bfbcf5e69e49ec32f4ec49d4",
        "title": "Sim-to-Lab-to-Real: Safe Reinforcement Learning with Shielding and Generalization Guarantees"
      },
      {
        "paperId": "d54b59e0648ce3c768f9978f6a0a86456253248b",
        "title": "Do Androids Dream of Electric Fences? Safety-Aware Reinforcement Learning with Latent Shielding"
      },
      {
        "paperId": "4b3f44cd0e32b3896cd1bf8a0e07793abe52357c",
        "title": "Model-Based Safe Reinforcement Learning With Time-Varying Constraints: Applications to Intelligent Vehicles"
      },
      {
        "paperId": "87381420a1231cd7faacfec4e46b554e232cfb6a",
        "title": "An Online Data-Driven Emergency-Response Method for Autonomous Agents in Unforeseen Situations"
      },
      {
        "paperId": "d2f4b49930d3976b29e11886c90a3b824c7d996b",
        "title": "MESA: Offline Meta-RL for Safe Adaptation and Fault Tolerance"
      },
      {
        "paperId": "a88f35662af6a2bfe5c1e361b11010b2928c354d",
        "title": "Safe Autonomous Racing via Approximate Reachability on Ego-vision"
      },
      {
        "paperId": "cab7d01a2ab666b485e1e505996654f2615d94a6",
        "title": "Auditing Robot Learning for Safety and Compliance during Deployment"
      },
      {
        "paperId": "22e586f420b596e398072b07b1d4a08c2a067c04",
        "title": "Safe Reinforcement Learning Using Robust Control Barrier Functions"
      },
      {
        "paperId": "c77f42bb6c3f12a6110a56f08ee9b14259a5b66e",
        "title": "Improving Safety in Deep Reinforcement Learning using Unsupervised Action Planning"
      },
      {
        "paperId": "f2420a34c266645025da028de4eecad42eef99f8",
        "title": "ThriftyDAgger: Budget-Aware Novelty and Risk Gating for Interactive Imitation Learning"
      },
      {
        "paperId": "d6e783bce3b8e3ad082c2757235c34cb86c4e653",
        "title": "Safe Learning in Robotics: From Learning-Based Control to Safe Reinforcement Learning"
      },
      {
        "paperId": "36d55c909e8c1be84f3a4f2631e3303ef5392fb0",
        "title": "Learning Barrier Certificates: Towards Safe Reinforcement Learning with Zero Training-time Violations"
      },
      {
        "paperId": "b63b38cd26a35d38be531aa849cf391f2208bd05",
        "title": "Safe and efficient imitation learning by clarification of experienced latent space"
      },
      {
        "paperId": "95e078afe47c574da17c14d52614b0ccfceb1eb4",
        "title": "Autonomous Reinforcement Learning via Subgoal Curricula"
      },
      {
        "paperId": "c46f1353b342b3dffb75e289b51ae2a2f8b98d34",
        "title": "LS3: Latent Space Safe Sets for Long-Horizon Visuomotor Control of Sparse Reward Iterative Tasks"
      },
      {
        "paperId": "4529960c17ab63a94412eaec6cada1028220100e",
        "title": "SimNet: Enabling Robust Unknown Object Manipulation from Pure Synthetic Data via Stereo"
      },
      {
        "paperId": "b9cceaae9b1f452842b9b0f6b01793e71ffc868c",
        "title": "Untangling Dense Non-Planar Knots by Learning Manipulation Features and Recovery Policies"
      },
      {
        "paperId": "bbbf6228d7471854bb152e4062c7086481ba280b",
        "title": "Safe Reinforcement Learning Using Advantage-Based Intervention"
      },
      {
        "paperId": "a9a0b47df7d254055710a4dc4bcbee2c8cf19eab",
        "title": "Policy Gradient Bayesian Robust Optimization for Imitation Learning"
      },
      {
        "paperId": "0533c7ae44cec0b5c5c186fa4ea9781df0d286f0",
        "title": "Abstraction-Guided Policy Recovery from Expert Demonstrations"
      },
      {
        "paperId": "1e066cabcd8a03a2262955274f457bcf1dcd7a25",
        "title": "Accelerating Surgical Robotics Research: A Review of 10 Years With the da Vinci Research Kit"
      },
      {
        "paperId": "7d937358c6c7c2ce29720aab7c4ed2c630e9730d",
        "title": "LazyDAgger: Reducing Context Switching in Interactive Imitation Learning"
      },
      {
        "paperId": "b284afe9a7363b898661c9b3cfb7f015b158cc63",
        "title": "Maximum Entropy RL (Provably) Solves Some Robust RL Problems"
      },
      {
        "paperId": "1543d2d35cb8d8a597abc0635967f4f4facc33a2",
        "title": "VisuoSpatial Foresight for physical sequential fabric manipulation"
      },
      {
        "paperId": "400811ee31020a3f002551476dac25973e13035e",
        "title": "How to train your robot with deep reinforcement learning: lessons we have learned"
      },
      {
        "paperId": "0053e5d294cf983a3c0a9b84dfd6477d291a7089",
        "title": "Intermittent Visual Servoing: Efficiently Learning Policies Robust to Instrument Changes for High-precision Surgical Manipulation"
      },
      {
        "paperId": "5dcad60d3ae2271e209badeb1755b06423415e27",
        "title": "Safe Reinforcement Learning with Natural Language Constraints"
      },
      {
        "paperId": "0d1654fb6446b86f5b88b28aa01c0d5e1a37d0e7",
        "title": "Accelerating Safe Reinforcement Learning with Constraint-mismatched Baseline Policies"
      },
      {
        "paperId": "e7964a084a68a7ab7bd0d0ffa9bc223dd5a5956c",
        "title": "A Low-Collision and Efficient Grasping Method for Manipulator Based on Safe Reinforcement Learning"
      },
      {
        "paperId": "3bee3eb0209c0d1d0c0ac5e6737a8fc642f34268",
        "title": "SafeVLA: Towards Safety Alignment of Vision-Language-Action Model via Safe Reinforcement Learning"
      },
      {
        "paperId": "2e38d00bf8f589b004fb6c9484458e831fea6fc3",
        "title": "Towards Passive Safe Reinforcement Learning: A Comparative Study on Contact-rich Robotic Manipulation"
      },
      {
        "paperId": "6144e241badaba568e644dbc28fa69d8a4c4518d",
        "title": "Gameplay Filters: Safe Robot Walking through Adversarial Imagination"
      },
      {
        "paperId": "68c2489182356842ff742e10dfaf76d6b6431300",
        "title": "Safe and Efficient Robot Learning by Biasing Exploration Towards Expert Demonstrations"
      },
      {
        "paperId": "5b06056345034e1559ef8680190cdccc79a2196d",
        "title": "VIMA: Robot Manipulation with Multimodal Prompts"
      },
      {
        "paperId": "3fa26b8afb7a642dc7a91617db1fcd4f0e35df85",
        "title": "An Actor-Critic Framework for Online Control With Environment Stability Guarantee"
      },
      {
        "paperId": "a87f30f52f8e2ac3f1ce06d8dbdcf374ae7aa873",
        "title": "Provably Safe Reinforcement Learning: A Theoretical and Experimental Comparison"
      },
      {
        "paperId": "b496066c799d6a83dd98637396581165f0219e2b",
        "title": "Reinforcement learning with guarantees: a review"
      },
      {
        "paperId": "977092d7ecff4ad027beab2df5bb216a598fc49c",
        "title": "Efficiently Learning Recoveries from Failures Under Partial Observability"
      },
      {
        "paperId": "7cfc146f4cacf6aa91d952e302fe375d1e3e5540",
        "title": "Reinforcement Learning With Safety and Stability Guarantees During Exploration For Linear Systems"
      },
      {
        "paperId": "bc5fb401ab616b1039232dcf12d81c52b1328813",
        "title": "Explainability for Safety in Reinforcement Learning"
      },
      {
        "paperId": "f6f7d8d09d817ae2272864d1eb8bd5a3e384f111",
        "title": "A Rule-based Shield: Accumulating Safety Rules from Catastrophic Action Effects"
      },
      {
        "paperId": "d1017bc28f6809d271a49572d7e1fc57dccb0ab4",
        "title": "LS3: Latent Space Safe Sets for Long-Horizon Visuomotor Control of Iterative Tasks"
      },
      {
        "paperId": "0a2a83070c83d879bc3dd54df1b96f92af973b47",
        "title": "Accelerating Surgical Robotics Research: Reviewing 10 Years of Research with the dVRK"
      },
      {
        "paperId": "933a780b3648ebb7d1ee9294795f7f46ee15d1ab",
        "title": "Robotic Untangling and Disentangling of Cables via Learned Manipulation and Recovery Strategies"
      },
      {
        "paperId": "9cad1e028240a13aaf2326698d20b2700abd7c14",
        "title": "Accelerated Policy Evaluation: Learning Adversarial Environments with Adaptive Importance Sampling"
      },
      {
        "paperId": "5584789fa0574b0499aa06ee1f29f50bfd25a038",
        "title": "LS 3 : Latent Space Safe Sets for Long-Horizon Visuomotor Control of Iterative Tasks"
      },
      {
        "paperId": "e80e766294ccf4a6274d9689dda185b245e36619",
        "title": "Lane Keeping Algorithm for Autonomous Driving via Safe Reinforcement Learning"
      },
      {
        "paperId": "8b3fb7ec4d54dd1dd8e775bf98491bb3e1f71eab",
        "title": "Safe Model-Based Reinforcement Learning Using Robust Control Barrier Functions"
      },
      {
        "paperId": "2206aff7df89f443285e135e8c1186867ea67f63",
        "title": "Safety-aware Policy Optimisation for Autonomous Racing"
      },
      {
        "paperId": "2d45bfcb21a2f55c4edc2dc3d12e2cd8e0b317aa",
        "title": "A Blood Glucose Control Framework Based on Reinforcement Learning With Safety and Interpretability: In Silico Validation"
      },
      {
        "paperId": "13f2f1627f6078a7ba202b1d4b9953f4e95c7584",
        "title": "Intermittent Visual Servoing: Effciently Learning Policies Robust to Instrument Changes for High- precision Surgical Manipulation"
      },
      {
        "paperId": "dd92bf9e00ae89e94cdd1f69e32a6f253737f8e9",
        "title": "Persistent Reinforcement Learning via Subgoal Curricula"
      },
      {
        "paperId": "46b76890e848b467c57b417813431f24ed37047c",
        "title": "Pre-grasp approaching on mobile robots a pre-active layered approach"
      },
      {
        "paperId": "23e9beb5141833767c7a0af3ce64d6aff1905f58",
        "title": "Safe Reinforcement Learning: A Survey"
      },
      {
        "paperId": "353b1c3b731cfdfe4a56e5c3810ac8f73aac8a3a",
        "title": "Evaluating Reinforcement Learning Strategies in the Card Game Gin Rummy: A Comparative Study of DQN, CDQN, and Reward Shaping Approaches"
      },
      {
        "paperId": "a04a70dcded840addf9cd0cab6ae45c599a31920",
        "title": "Crystallization Process Design by Model-Free Deep Reinforcement Learning"
      },
      {
        "paperId": "a27f7884b558aaff152058f42649da11dc9d5ad4",
        "title": "Context Aware Policy Adaptation: Towards Robust Safe Reinforcement Learning"
      }
    ],
    "score": 49.400000000000006
  },
  {
    "id": "2c23085488337c4c1b5673b8d0f4ac95bda73529",
    "title": "Uncertainty Weighted Actor-Critic for Offline Reinforcement Learning",
    "authors": [
      "Yue Wu",
      "Shuangfei Zhai",
      "Nitish Srivastava",
      "J. Susskind",
      "Jian Zhang",
      "R. Salakhutdinov",
      "Hanlin Goh"
    ],
    "year": 2021,
    "citationCount": 196,
    "abstract": "Offline Reinforcement Learning promises to learn effective policies from previously-collected, static datasets without the need for exploration. However, existing Q-learning and actor-critic based off-policy RL algorithms fail when bootstrapping from out-of-distribution (OOD) actions or states. We hypothesize that a key missing ingredient from the existing methods is a proper treatment of uncertainty in the offline setting. We propose Uncertainty Weighted Actor-Critic (UWAC), an algorithm that detects OOD state-action pairs and down-weights their contribution in the training objectives accordingly. Implementation-wise, we adopt a practical and effective dropout-based uncertainty estimation method that introduces very little overhead over existing RL algorithms. Empirically, we observe that UWAC substantially improves model stability during training. In addition, UWAC out-performs existing offline RL methods on a variety of competitive tasks, and achieves significant performance gains over the state-of-the-art baseline on datasets with sparse demonstrations collected from human experts.",
    "url": "https://www.semanticscholar.org/paper/2c23085488337c4c1b5673b8d0f4ac95bda73529",
    "pdf_url": "https://arxiv.org/pdf/2105.08140.pdf",
    "venue": "International Conference on Machine Learning",
    "publicationDate": "2021-05-17",
    "externalIds": {
      "DBLP": "journals/corr/abs-2105-08140",
      "ArXiv": "2105.08140",
      "CorpusId": 234763307
    },
    "references": [
      {
        "paperId": "0e684465067afbfc002fd167ea250b03f9ba01c2",
        "title": "Reinforcement Learning with Uncertainty Estimation for Tactical Decision-Making in Intersections"
      },
      {
        "paperId": "0272b14dd471fe7b81df703af1b71d7600b77215",
        "title": "Accelerating Online Reinforcement Learning with Offline Datasets"
      },
      {
        "paperId": "28db20a81eec74a50204686c3cf796c42a020d2e",
        "title": "Conservative Q-Learning for Offline Reinforcement Learning"
      },
      {
        "paperId": "dea0f1c5949f8d898b9b6ff68226a781558e413c",
        "title": "MOPO: Model-based Offline Policy Optimization"
      },
      {
        "paperId": "309c2c5ee60e725244da09180f913cd8d4b8d4e9",
        "title": "MOReL : Model-Based Offline Reinforcement Learning"
      },
      {
        "paperId": "a326d9f2d2d351001fece788165dbcbb524da2e4",
        "title": "D4RL: Datasets for Deep Data-Driven Reinforcement Learning"
      },
      {
        "paperId": "ad14227e4f51276892ffc37aa43fd8750bb5eba8",
        "title": "Advantage-Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning"
      },
      {
        "paperId": "9be492858863c8c7c24be1ecb75724de5086bd8e",
        "title": "Behavior Regularized Offline Reinforcement Learning"
      },
      {
        "paperId": "4012d4ab621f3f5f04b0f91849a60c6eaabe64b4",
        "title": "An Optimistic Perspective on Offline Reinforcement Learning"
      },
      {
        "paperId": "57daffd65a5d73a439903f3e50950c21c9eba687",
        "title": "Way Off-Policy Batch Deep Reinforcement Learning of Implicit Human Preferences in Dialog"
      },
      {
        "paperId": "bdc47f8a781c99578b76e27708afac3691b6b1ec",
        "title": "Practical Deep Learning with Bayesian Principles"
      },
      {
        "paperId": "82b4b03a4659d6e04bd7cbf51d6e08fde1348dbd",
        "title": "Stabilizing Off-Policy Q-Learning via Bootstrapping Error Reduction"
      },
      {
        "paperId": "d6285ff3dcb15c1da84dcbc98141be10ef0e8dd1",
        "title": "Estimating Risk and Uncertainty in Deep Reinforcement Learning"
      },
      {
        "paperId": "5285cb8faada5de8a92a47622950f6cfd476ac1d",
        "title": "Off-Policy Deep Reinforcement Learning without Exploration"
      },
      {
        "paperId": "07d55ff5b313c4c382d80700f5039c89a9124ebb",
        "title": "Variational Bayesian dropout: pitfalls and fixes"
      },
      {
        "paperId": "eb37e7b76d26b75463df22b2a3aa32b6a765c672",
        "title": "QT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation"
      },
      {
        "paperId": "f802802b3af5a22b79ac65d033ba3cbee33da91b",
        "title": "Randomized Prior Functions for Deep Reinforcement Learning"
      },
      {
        "paperId": "6879ecec797cd0f1319b54a963f91abb5f7325de",
        "title": "Randomized Value Functions via Multiplicative Normalizing Flows"
      },
      {
        "paperId": "4debb99c0c63bfaa97dd433bc2828e4dac81c48b",
        "title": "Addressing Function Approximation Error in Actor-Critic Methods"
      },
      {
        "paperId": "80196cdfcd0c6ce2953bf65a7f019971e2026386",
        "title": "IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures"
      },
      {
        "paperId": "811df72e210e20de99719539505da54762a11c6d",
        "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor"
      },
      {
        "paperId": "7eb0db941fbf19857631ec1c907f2888ea308779",
        "title": "Safe Policy Improvement with Baseline Bootstrapping"
      },
      {
        "paperId": "ecd000837136585b52f869adaa57003a0b78537b",
        "title": "Variational Deep Q Network"
      },
      {
        "paperId": "fe3e91e40a950c6b6601b8f0a641884774d949ae",
        "title": "Distributional Reinforcement Learning with Quantile Regression"
      },
      {
        "paperId": "c27db32efa8137cbf654902f8f728f338e55cd1c",
        "title": "Mastering the game of Go without human knowledge"
      },
      {
        "paperId": "e010ba3ff5744604cdbfe44a733e2a98649ee907",
        "title": "Learning Complex Dexterous Manipulation with Deep Reinforcement Learning and Demonstrations"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "3fab4d28be5ff73f42c8b307117aa8d680345ad9",
        "title": "Noisy Networks for Exploration"
      },
      {
        "paperId": "802168a81571dde28f5ddb94d84677bc007afa7b",
        "title": "Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles"
      },
      {
        "paperId": "6ecb8a743f92db6c6b8691ab8e8aebbb06fb1b48",
        "title": "Averaged-DQN: Variance Reduction and Stabilization for Deep Reinforcement Learning"
      },
      {
        "paperId": "885fe11ed7ab81c8609ccddb3e10f62577c04ab9",
        "title": "BBQ-Networks: Efficient Exploration in Deep Reinforcement Learning for Task-Oriented Dialogue Systems"
      },
      {
        "paperId": "dc3e905bfb27d21675ee1720413e007b014b37d3",
        "title": "Safe and Efficient Off-Policy Reinforcement Learning"
      },
      {
        "paperId": "4b63e34276aa98d5345efa7fe09bb06d8a9d8f52",
        "title": "Deep Exploration via Bootstrapped DQN"
      },
      {
        "paperId": "4a026fd65af4ba3575e64174de56fee093fa3330",
        "title": "Taming the Noise in Reinforcement Learning via Soft Updates"
      },
      {
        "paperId": "0c1f9ca23f4f09ecfc44bcc8ca1c2736624f4652",
        "title": "A Theoretically Grounded Application of Dropout in Recurrent Neural Networks"
      },
      {
        "paperId": "4443e2e5bfd112562ecef578f772cee3c692f19f",
        "title": "Variational Dropout and the Local Reparameterization Trick"
      },
      {
        "paperId": "f35de4f9b1a7c4d3fa96a0d2ab1bf8937671f6b6",
        "title": "Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning"
      },
      {
        "paperId": "2319a491378867c7049b3da055c5df60e1671158",
        "title": "Playing Atari with Deep Reinforcement Learning"
      },
      {
        "paperId": "225f78ae8a44723c136646044fd5c5d7f1d3d15a",
        "title": "A Kernel Two-Sample Test"
      },
      {
        "paperId": "d64705e93df52a219a92e7ca993af1fab28dc8de",
        "title": "Error Propagation for Approximate Policy and Value Iteration"
      },
      {
        "paperId": "565c478742e4c000a598d44a2d1e957a704a5717",
        "title": "Error Bounds for Approximate Policy Iteration"
      },
      {
        "paperId": "3a7c4f696b2b846358ada1b331989bc75fcfc28f",
        "title": "Disentangling Sources of Uncertainty for Active Exploration"
      },
      {
        "paperId": "3c623c08329e129e784a5d03f7606ec8feba3a28",
        "title": "Uncertainty in Deep Learning"
      },
      {
        "paperId": "a6ee4ae5344033fee613898841e2b9894bbfe4b7",
        "title": "Approximate modified policy iteration and its application to the game of Tetris"
      },
      {
        "paperId": "34f25a8704614163c4095b3ee2fc969b60de4698",
        "title": "Dropout: a simple way to prevent neural networks from overfitting"
      },
      {
        "paperId": "5c65d095600d6c647426fa3bc45031b208882d5f",
        "title": "Batch Reinforcement Learning"
      },
      {
        "paperId": "b225a9eb169a3530289bf834d3b6e785947959ee",
        "title": "Neuro-Dynamic Programming"
      },
      {
        "paperId": "97efafdb4a3942ab3efba53ded7413199f79c054",
        "title": "Reinforcement Learning: An Introduction"
      },
      {
        "paperId": null,
        "title": "Calculate variance of the y ( s, a ) through variance of T stochastic samples from Q \u03b8 (cid:48) 1 , Q \u03b8 (cid:48) 2"
      },
      {
        "paperId": null,
        "title": "Perform one step of SGD to minimize L ( Q \u03b8 1 , 2 ) = \u03b2 V ar [ y ( s,a )] ( Q \u03b8 1 , 2 ( s, a ) \u2212 ( r + \u03b3y ( s, a ))) 2"
      }
    ],
    "cited_by": [
      {
        "paperId": "ba1f70ea8347a229dd083d92c442b9eb3d2f89b6",
        "title": "Uncertainty-Based Smooth Policy Regularisation for Reinforcement Learning with Few Demonstrations"
      },
      {
        "paperId": "2cfe7a7e76c49b6230550d944af80daeec49b46d",
        "title": "An Uncertainty-Weighted Decision Transformer for Navigation in Dense, Complex Driving Scenarios"
      },
      {
        "paperId": "4558776c0e34bfa6d3533cbb9ad3ad16a473f54a",
        "title": "Mildly Conservative Regularized Evaluation for Offline Reinforcement Learning"
      },
      {
        "paperId": "4d3bb9dc28d043a7bb74862c581841f34e2e1f37",
        "title": "Enhancing Generalization of Offline RL in Data-Limited Settings With Heuristic Rules"
      },
      {
        "paperId": "7fbb7673d57c8b772c26bc3bdf93763f94dd1d70",
        "title": "Pseudo-distribution elite critics: Enhancing accuracy in reinforcement learning value estimation."
      },
      {
        "paperId": "a08de2520fbb7f9414e402b1bb9cae86e1078078",
        "title": "Pessimistic policy iteration with bounded uncertainty"
      },
      {
        "paperId": "de2e28de06b77e444577eca7f6af5c2d354d79be",
        "title": "ADDQ: Adaptive Distributional Double Q-Learning"
      },
      {
        "paperId": "46e57e25876b7743883c6a80a657e7c8b3eafbb3",
        "title": "CAWR: Corruption-Averse Advantage-Weighted Regression for Robust Policy Optimization"
      },
      {
        "paperId": "f7160d864f630dc15a80c0e11787253845c65a38",
        "title": "Imagination-Limited Q-Learning for Offline Reinforcement Learning"
      },
      {
        "paperId": "40203b4b646c8d063c381656d191ebb249068457",
        "title": "Combining Bayesian Inference and Reinforcement Learning for Agent Decision Making: A Review"
      },
      {
        "paperId": "eedb94105a930996f7e49b4c1592d642f901271b",
        "title": "Uncertainty Quantification for Machine Learning in Healthcare: A Survey"
      },
      {
        "paperId": "3bf82e9d0c47ebf75e1c6be7f731903719ddafc1",
        "title": "Learning Conservative Neural Control Barrier Functions from Offline Data"
      },
      {
        "paperId": "1ea9c6268f2fcc2b2498e11c197fe182c066ebef",
        "title": "Provable Zero-Shot Generalization in Offline Reinforcement Learning"
      },
      {
        "paperId": "cd5e1d9b81fae68403c910ec532bdc0f2dd5f15a",
        "title": "Guaranteeing Out-Of-Distribution Detection in Deep RL via Transition Estimation"
      },
      {
        "paperId": "e1f707f6f78024bf18ab812b3899f068f0f6bf10",
        "title": "Enhancing Offline Model-Based RL via Active Model Selection: A Bayesian Optimization Perspective"
      },
      {
        "paperId": "f0096167c046fbe15e8dbe75356f7d876c04f600",
        "title": "Ameliorating Learning Stability by Regularizing Soft Actor-Critic"
      },
      {
        "paperId": "6de3ca31ca92d394bce2c31ca0d54961dc4f738d",
        "title": "Robust Offline Actor-Critic with On-Policy Regularized Policy Evaluation"
      },
      {
        "paperId": "52381b5af6a1bdbdd42e4ac6403052eda1f4c1d3",
        "title": "Soft Actor-Critic-based Distributed Routing Scheme for Edge Computing Integrated with Dynamic IoT Networks"
      },
      {
        "paperId": "28030b8f202e4ae2246e4fae5cfbc9c5aec8f00c",
        "title": "Mitigating Relative Over-Generalization in Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "90c2af458f55defe03c911d76f4e07ebee3a22d5",
        "title": "Hypercube Policy Regularization Framework for Offline Reinforcement Learning"
      },
      {
        "paperId": "2b0d7cf925cb749df5f6078d57b786f82b8994d9",
        "title": "Uncertainty-based Offline Variational Bayesian Reinforcement Learning for Robustness under Diverse Data Corruptions"
      },
      {
        "paperId": "f545f23405c753f83e3185b99b08111c193ce52a",
        "title": "A Non-Monolithic Policy Approach of Offline-to-Online Reinforcement Learning"
      },
      {
        "paperId": "df6c20aee8d31bbfd024d2051e728990bd73f2c1",
        "title": "Adaptive Policy Regularization for Offline-to-Online Reinforcement Learning in HVAC Control"
      },
      {
        "paperId": "b8c799591619258c35c0ef8627573fc770031070",
        "title": "Q-Distribution guided Q-learning for offline reinforcement learning: Uncertainty penalized Q-value via consistency model"
      },
      {
        "paperId": "94e04d0e1ad8d9a6b82e8068df71df9036123d19",
        "title": "Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration"
      },
      {
        "paperId": "67d19b3e36d430140c755ff8fc4bb4d240ac97be",
        "title": "Grounded Answers for Multi-agent Decision-making Problem through Generative World Model"
      },
      {
        "paperId": "7685acc107ab46fac007913c284d955b15755065",
        "title": "Real-time Network Intrusion Detection via Importance Sampled Decision Transformers"
      },
      {
        "paperId": "086a4d45a9deef5a10ee4febcd4c92c95a6305de",
        "title": "Improving Offline Reinforcement Learning With in-Sample Advantage Regularization for Robot Manipulation"
      },
      {
        "paperId": "7d98a1533bb65d57dcabe946836184346c50efb0",
        "title": "De-Pessimism Offline Reinforcement Learning via Value Compensation"
      },
      {
        "paperId": "a6bbafa8c56b3b2732eb37dbce52a3de0b998bf3",
        "title": "Hokoff: Real Game Dataset from Honor of Kings and its Offline Reinforcement Learning Benchmarks"
      },
      {
        "paperId": "5a1af758d1ba24e06895fbea2f788af92fc76697",
        "title": "SelfBC: Self Behavior Cloning for Offline Reinforcement Learning"
      },
      {
        "paperId": "1bcf0239dbb4c49f08419933a6434c3bb98d8af3",
        "title": "Adaptive pessimism via target Q-value for offline reinforcement learning"
      },
      {
        "paperId": "efc4e5b17ac376b7080c12a1a3c3f6e122b6b140",
        "title": "Data-Efficient Uncertainty-Guided Model-Based Reinforcement Learning with Unscented Kalman Bayesian Neural Networks"
      },
      {
        "paperId": "9455a8e8dff9533e21c4fee4d8a2686a8991692a",
        "title": "Enhancing Offline Reinforcement Learning via Dynamics-Aware Mixup"
      },
      {
        "paperId": "26c1e857c64036767abcd4ca422d60545779ee4a",
        "title": "Efficient Offline Reinforcement Learning: The Critic is Critical"
      },
      {
        "paperId": "8af0861cb38a5a335d69eebefc8d1917222be960",
        "title": "A Rate-Distortion View of Uncertainty Quantification"
      },
      {
        "paperId": "5971402c4cee5863b25c99405e886b673ade223c",
        "title": "Is Value Learning Really the Main Bottleneck in Offline RL?"
      },
      {
        "paperId": "12a19c9d18244a0300f6ec273651f16b2a4293b4",
        "title": "DiffPoGAN: Diffusion Policies with Generative Adversarial Networks for Offline Reinforcement Learning"
      },
      {
        "paperId": "808a929594785545d3e01c3315f935e71231d558",
        "title": "Integrating Domain Knowledge for handling Limited Data in Offline RL"
      },
      {
        "paperId": "252566c6953f50070540ac3504fbbb3b9be8aae4",
        "title": "CDSA: Conservative Denoising Score-based Algorithm for Offline Reinforcement Learning"
      },
      {
        "paperId": "29d00675fb8a5919229a4a9ac3b71680187da441",
        "title": "Augmenting Offline RL with Unlabeled Data"
      },
      {
        "paperId": "dd6f90c6d9045fe5fffa99e9daeb236dbc30b874",
        "title": "UDQL: Bridging The Gap between MSE Loss and The Optimal Value Function in Offline Reinforcement Learning"
      },
      {
        "paperId": "007f0aa79ac482bceab208e17fdfeaf18e4a4f29",
        "title": "Mamba as Decision Maker: Exploring Multi-scale Sequence Modeling in Offline Reinforcement Learning"
      },
      {
        "paperId": "55891594e3b4815d16774e89acac5590543a12de",
        "title": "GTA: Generative Trajectory Augmentation with Guidance for Offline Reinforcement Learning"
      },
      {
        "paperId": "2329c383c531030dc7eea3c4b8927b18f112cba7",
        "title": "Exclusively Penalized Q-learning for Offline Reinforcement Learning"
      },
      {
        "paperId": "288716016924bbfe396147b2cc723ca7687382ad",
        "title": "Actor-Critic With Synthesis Loss for Solving Approximation Biases"
      },
      {
        "paperId": "2e3e34391f5bebfce5eb215c8df29c2065d9e335",
        "title": "Reward-free offline reinforcement learning: Optimizing behavior policy via action exploration"
      },
      {
        "paperId": "9afbd6712708ccffbf74006c36080131728bbc33",
        "title": "USN: A Robust Imitation Learning Method against Diverse Action Noise"
      },
      {
        "paperId": "22711187b1b7d22e41ece0cdc15998b8ec53a297",
        "title": "Knowledgeable Agents by Offline Reinforcement Learning from Large Language Model Rollouts"
      },
      {
        "paperId": "9d59e3a3952ae3b1ba8cdb11454fbac242f3ff9d",
        "title": "Offline Reinforcement Learning with Policy Guidance and Uncertainty Estimation"
      },
      {
        "paperId": "08dfb362e96cdd99a3b4b3919fad30b4752a4a00",
        "title": "Diverse Randomized Value Functions: A Provably Pessimistic Approach for Offline Reinforcement Learning"
      },
      {
        "paperId": "6023082e39e69e732140ee11bf871f66130f8303",
        "title": "Grid-Mapping Pseudo-Count Constraint for Offline Reinforcement Learning"
      },
      {
        "paperId": "ca8fa01a225be159c8430609933764228140d8db",
        "title": "Effective Offline Robot Learning With Structured Task Graph"
      },
      {
        "paperId": "ca6eee8ac3f980885aa6ea232b8c83bab1fd37ea",
        "title": "An Implicit Trust Region Approach to Behavior Regularized Offline Reinforcement Learning"
      },
      {
        "paperId": "b44b0e8222021b51783c62b0bce071ef6fb3f12c",
        "title": "OCEAN-MBRL: Offline Conservative Exploration for Model-Based Offline Reinforcement Learning"
      },
      {
        "paperId": "5e9e298b1ca33dd7ebffdba6bbd4095cb9a7483f",
        "title": "Multi-Fidelity Reinforcement Learning for Time-Optimal Quadrotor Re-planning"
      },
      {
        "paperId": "da59acaf11b3e351866d595afc5dbba177743bbd",
        "title": "MORE-3S:Multimodal-based Offline Reinforcement Learning with Shared Semantic Spaces"
      },
      {
        "paperId": "f406a1ccb5159750a4aa2ed467edbb1330a7bcf5",
        "title": "Efficient Offline Reinforcement Learning With Relaxed Conservatism"
      },
      {
        "paperId": "3c1fba46a4272d160df80178799c41b065f88296",
        "title": "Adaptive Q-Aid for Conditional Supervised Learning in Offline Reinforcement Learning"
      },
      {
        "paperId": "e0817781261f075fca19aa5eac370b689eb90c43",
        "title": "SLIM: Skill Learning with Multiple Critics"
      },
      {
        "paperId": "5216f1ffc6e65a6a8f8b9d99572670a5c9ab0e0d",
        "title": "Optimistic Model Rollouts for Pessimistic Offline Policy Optimization"
      },
      {
        "paperId": "f49afbb0dbf654f4e311daa621db05a123faee58",
        "title": "Guiding Offline Reinforcement Learning Using a Safety Expert"
      },
      {
        "paperId": "85e3fbeb7b90ac71a2912870273c4928c457bdb1",
        "title": "Pessimistic value iteration for multi-task data sharing in Offline Reinforcement Learning"
      },
      {
        "paperId": "95d2952cff544d3504f9434d9f4ba52e3a8ca710",
        "title": "Possibilities of reinforcement learning for nuclear power plants: Evidence on current applications and beyond"
      },
      {
        "paperId": "50aa1554c4e8b230784d7903e31116022984894c",
        "title": "CUDC: A Curiosity-Driven Unsupervised Data Collection Method with Adaptive Temporal Distances for Offline Reinforcement Learning"
      },
      {
        "paperId": "44a9d8b0314d34aff91ccff9207d38eed37216ed",
        "title": "Iterative Preference Learning from Human Feedback: Bridging Theory and Practice for RLHF under KL-constraint"
      },
      {
        "paperId": "956c47ae72723519414aebf5278ba9c727257350",
        "title": "Active Reinforcement Learning for Robust Building Control"
      },
      {
        "paperId": "bfafe4b8272432c281b1775caab4adcab174ddf9",
        "title": "Large Vehicle Scheduling Based on Uncertainty Weighting Harmonic Twin-Critic Network"
      },
      {
        "paperId": "310a2b6fdb2e8a2984912ca5c476617dedb5426b",
        "title": "Self-Supervised Imitation for Offline Reinforcement Learning With Hindsight Relabeling"
      },
      {
        "paperId": "ee844f841fa9d93f63819c5dc63c314681534a5a",
        "title": "DCAC: Reducing Unnecessary Conservatism in Offline-to-online Reinforcement Learning"
      },
      {
        "paperId": "64170ce0f75998a1175a2f081238c80539391d29",
        "title": "Improving Offline-to-Online Reinforcement Learning with Q Conditioned State Entropy Exploration"
      },
      {
        "paperId": "4bcbd9affdf015815886ce3d9dc87cfabd8047b3",
        "title": "Uncertainty-aware hierarchical reinforcement learning for long-horizon tasks"
      },
      {
        "paperId": "023d462ec6ff84cee0d0716a34d11efc7cde8534",
        "title": "Reward Model Ensembles Help Mitigate Overoptimization"
      },
      {
        "paperId": "3e3fe6b52405f4235b07ca5962cf5608fe900bd7",
        "title": "Conservative network for offline reinforcement learning"
      },
      {
        "paperId": "82ff15c65a7ab93f1c43cc7f383ba8fd9415923d",
        "title": "Uncertainty-Aware Decision Transformer for Stochastic Driving Environments"
      },
      {
        "paperId": "be934e18660a816d3e9089fc314ffda7e2b4a6b7",
        "title": "Zero-Shot Reinforcement Learning from Low Quality Data"
      },
      {
        "paperId": "341975e953eae14932b708d7f80e378d11013a57",
        "title": "Boosting Offline Reinforcement Learning for Autonomous Driving with Hierarchical Latent Skills"
      },
      {
        "paperId": "0b7a6431a91337dafb9cc9a8d605869a86488c28",
        "title": "Counterfactual Conservative Q Learning for Offline Multi-agent Reinforcement Learning"
      },
      {
        "paperId": "89798c02604d4e007a85ea337d173c85dd9aa1e3",
        "title": "An improved deep reinforcement learning-based scheduling approach for dynamic task scheduling in cloud manufacturing"
      },
      {
        "paperId": "8b7b46a53677d60b1e3fec890ab64a82b43db761",
        "title": "Mild Policy Evaluation for Offline Actor\u2013Critic"
      },
      {
        "paperId": "4bd6c98a009f4826979bed5fbf40ff54926cad14",
        "title": "Statistically Efficient Variance Reduction with Double Policy Estimation for Off-Policy Evaluation in Sequence-Modeled Reinforcement Learning"
      },
      {
        "paperId": "5b19d4f3312370048407ee931f34f980ed653680",
        "title": "Adaptive Reward Shifting Based on Behavior Proximity for Offline Reinforcement Learning"
      },
      {
        "paperId": "31d481a2d4b1a13aee09b592ccdfeb4a4f6e3a01",
        "title": "SeRO: Self-Supervised Reinforcement Learning for Recovery from Out-of-Distribution Situations"
      },
      {
        "paperId": "207f9a6a39251fa4f79046e78cd806bd50103094",
        "title": "Integrating Offline Reinforcement Learning with Transformers for Sequential Recommendation"
      },
      {
        "paperId": "e6a54f56883e009f09259d98010981d8cf0b80f3",
        "title": "Offline Multi-Agent Reinforcement Learning with Implicit Global-to-Local Value Regularization"
      },
      {
        "paperId": "b07117b0f1e671d5557921290f83f3f2a0416b62",
        "title": "Bayesian inference for data-efficient, explainable, and safe robotic motion planning: A review"
      },
      {
        "paperId": "8f9ea5ed79c7996be594b08a0fc8d5471ecdf0f5",
        "title": "Offline Reinforcement Learning with Imbalanced Datasets"
      },
      {
        "paperId": "07f59677ee85bb7bf07b9f6dc665963c78ed7233",
        "title": "Safe batch constrained deep reinforcement learning with generative adversarial network"
      },
      {
        "paperId": "32a31c899c145d9502ba3f5ef9fc70671a45da76",
        "title": "DARL: Distance-Aware Uncertainty Estimation for Offline Reinforcement Learning"
      },
      {
        "paperId": "535c385767b188d8112f2748ce5433b744b18361",
        "title": "Design from Policies: Conservative Test-Time Adaptation for Offline Policy Optimization"
      },
      {
        "paperId": "53ff1c1f81c62a7799ec90e170e9b7ea6b775a60",
        "title": "Model-Based Offline Weighted Policy Optimization (Student Abstract)"
      },
      {
        "paperId": "2ab82ae37b7d27e79f59e624fa9419682c7cf8b8",
        "title": "Online Tuning for Offline Decentralized Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "c7439db4be4e9034472c852ca3a04e5a5d283b37",
        "title": "Beyond OOD State Actions: Supported Cross-Domain Offline Reinforcement Learning"
      },
      {
        "paperId": "a0d87da6f822789da3b7c2c832594d1a542848ee",
        "title": "Uncertainty-Aware Data Augmentation for Offline Reinforcement Learning"
      },
      {
        "paperId": "6ff3fb272e56249ce8a32180147a1e370a83fc2d",
        "title": "Offline Reinforcement Learning with Uncertainty Critic Regularization Based on Density Estimation"
      },
      {
        "paperId": "33d92b568e79176ab607a3b9d991c686d8300765",
        "title": "Offline Multi-Agent Reinforcement Learning with Coupled Value Factorization"
      },
      {
        "paperId": "2807f9c666335946113fb11dccadf36f8d78b772",
        "title": "A Simple Unified Uncertainty-Guided Framework for Offline-to-Online Reinforcement Learning"
      },
      {
        "paperId": "fd3f34a37e3963d3c28b6d389ce4448ff9fd0c33",
        "title": "A Unified Uncertainty-Aware Exploration: Combining Epistemic and Aleatory Uncertainty"
      },
      {
        "paperId": "6de56cb7f5f0f18041cbc0e8feb994e496d8ab69",
        "title": "Uncertainty-Guided Active Reinforcement Learning with Bayesian Neural Networks"
      },
      {
        "paperId": "2398cea6d38816bc9de41880f7cbbb4ee768ec37",
        "title": "Metric-agnostic Ranking Optimization"
      },
      {
        "paperId": "bfd9d1b5d0b4af68d393ee4865aaa2eeebd97ae6",
        "title": "Uncertainty-Driven Trajectory Truncation for Data Augmentation in Offline Reinforcement Learning"
      },
      {
        "paperId": "18d82f2a4aa1e2c1c4b447876c95b8f7e717e1a1",
        "title": "Offline RL with No OOD Actions: In-Sample Learning via Implicit Value Regularization"
      },
      {
        "paperId": "ea30bcd9c2b2d0dbaf63e808c59733b8e4d104a4",
        "title": "Uncertainty-Aware Instance Reweighting for Off-Policy Learning"
      },
      {
        "paperId": "86b361f3ea60c35182500dc9782a5f900d9c36bb",
        "title": "Double Actors and Uncertainty-Weighted Critics for Offline Reinforcement Learning"
      },
      {
        "paperId": "488fb301e6a18df78bf3e3cb6a79a8a776296ab2",
        "title": "The In-Sample Softmax for Offline Reinforcement Learning"
      },
      {
        "paperId": "c293b63151e9e02f7694938cabb7bbae442ae4d2",
        "title": "The Provable Benefits of Unsupervised Data Sharing for Offline Reinforcement Learning"
      },
      {
        "paperId": "7d200b868cb92657a68ac64c112a2cd0a4045f87",
        "title": "VIPeR: Provably Efficient Algorithm for Offline RL with Neural Function Approximation"
      },
      {
        "paperId": "e9f1d5f5c366e2f18a31ec34175b9eb6795ae82b",
        "title": "Learning to Forecast Aleatoric and Epistemic Uncertainties over Long Horizon Trajectories"
      },
      {
        "paperId": "c6e7f5097e1073f164d4c580a0b2652b882cc34e",
        "title": "Deep Offline Reinforcement Learning for Real-world Treatment Optimization Applications"
      },
      {
        "paperId": "8c4a2558851522b0344c7d605aac7d2a36b740aa",
        "title": "Conservative State Value Estimation for Offline Reinforcement Learning"
      },
      {
        "paperId": "041db591ce899cc86eb62c51693eb8594dc34e80",
        "title": "Multi-armed bandit problem with online clustering as side information"
      },
      {
        "paperId": "4f0bfeadd39e64456d15d400fda8ecc2197c3265",
        "title": "Direct Preference-based Policy Optimization without Reward Modeling"
      },
      {
        "paperId": "a4abac32084083654ead9af3fda3e4f1338ff3b4",
        "title": "Constrained Policy Optimization with Explicit Behavior Density For Offline Reinforcement Learning"
      },
      {
        "paperId": "b1cdd73c39fbe7b4e9b758389554efc51d121447",
        "title": "Realistic Actor-Critic: A framework for balance between value overestimation and underestimation"
      },
      {
        "paperId": "203ce4bd674b127df54824047e94051fccb239be",
        "title": "Offline Robot Reinforcement Learning with Uncertainty-Guided Human Expert Sampling"
      },
      {
        "paperId": "34861a134bc2f515bf937e488b5b0b46f94fe64c",
        "title": "State-Aware Proximal Pessimistic Algorithms for Offline Reinforcement Learning"
      },
      {
        "paperId": "630afad39e7d523cee62f9a1f8254e4e3d6ac0be",
        "title": "Domain Generalization for Robust Model-Based Offline Reinforcement Learning"
      },
      {
        "paperId": "2b485e20ead0389911db1d269656a322d7049e73",
        "title": "Offline Reinforcement Learning with Adaptive Behavior Regularization"
      },
      {
        "paperId": "0fd1a48877f62d8fa5e4c9adc8189e84b507a349",
        "title": "Contextual Transformer for Offline Meta Reinforcement Learning"
      },
      {
        "paperId": "2bc3eeb2adc8deb8f13cce27ceeced302695a1fd",
        "title": "Monotonic Quantile Network for Worst-Case Offline Reinforcement Learning"
      },
      {
        "paperId": "be0288847c13f3ef286524899bb95709eb1213e8",
        "title": "TD3 with Reverse KL Regularizer for Offline Reinforcement Learning from Mixed Datasets"
      },
      {
        "paperId": "c4df1a5633615a5e23c16cc6f2d5998f6a2fc3de",
        "title": "Epistemic Monte Carlo Tree Search"
      },
      {
        "paperId": "168a5a913352dc1376401cfb758f99637256470f",
        "title": "Robust Offline Reinforcement Learning with Gradient Penalty and Constraint Relaxation"
      },
      {
        "paperId": "60380ee913d20e722368245f23e0d4baf52e139a",
        "title": "A Policy-Guided Imitation Approach for Offline Reinforcement Learning"
      },
      {
        "paperId": "73bfe561d9ed870e4564dd82671a4b8d68af7840",
        "title": "State Advantage Weighting for Offline RL"
      },
      {
        "paperId": "cb0396b055e03d68eaca98f53ce56f2de69aeab6",
        "title": "How to Enable Uncertainty Estimation in Proximal Policy Optimization"
      },
      {
        "paperId": "550d9aadc6322adaccef9e1fe7dbc1b623962685",
        "title": "Offline Reinforcement Learning with Differentiable Function Approximation is Provably Efficient"
      },
      {
        "paperId": "5019eb2f88872e6d30ad4933c2f03af60e954c50",
        "title": "Exploring Policy Diversity in Parallel Actor-Critic Learning"
      },
      {
        "paperId": "e6886596ea1174cb1f88676849d0b3dd607865f2",
        "title": "DCE: Offline Reinforcement Learning With Double Conservative Estimates"
      },
      {
        "paperId": "1bbf1c6e56c26d829e6fc8032168e3b1ad1d7c10",
        "title": "Distributionally Robust Offline Reinforcement Learning with Linear Function Approximation"
      },
      {
        "paperId": "d5aa0efc065ac3073404aebbcc3a2ca3eb63db4b",
        "title": "Cost-based or Learning-based? A Hybrid Query Optimizer for Query Plan Selection"
      },
      {
        "paperId": "55174eacb397cadee19b3315a06e8b4df4c4cc0a",
        "title": "A Review of Uncertainty for Deep Reinforcement Learning"
      },
      {
        "paperId": "124eeb634a14b135b1789cf35e41903b8642a900",
        "title": "Multi-Task Fusion via Reinforcement Learning for Long-Term User Satisfaction in Recommender Systems"
      },
      {
        "paperId": "e61619690983a443209fe5252f06e60443d692a9",
        "title": "A Survey of Learning on Small Data: Generalization, Optimization, and Challenge"
      },
      {
        "paperId": "5b93eb7af42d546c9d2d7ac249fee7a2d238df32",
        "title": "Offline Reinforcement Learning at Multiple Frequencies"
      },
      {
        "paperId": "57899d8ecaec316ce3a3dc46b4c6fe80037664a0",
        "title": "Double Check Your State Before Trusting It: Confidence-Aware Bidirectional Offline Model-Based Imagination"
      },
      {
        "paperId": "ebc87ed7e0586f5b31fd7351f8b5b5db32a74505",
        "title": "Regularizing a Model-based Policy Stationary Distribution to Stabilize Offline Reinforcement Learning"
      },
      {
        "paperId": "7e045a7fe78a6c0de5511980f292c42d1055f396",
        "title": "Mildly Conservative Q-Learning for Offline Reinforcement Learning"
      },
      {
        "paperId": "514a1e7af5884a8e1c6dae488fa618787b7a69f7",
        "title": "On the Role of Discount Factor in Offline Reinforcement Learning"
      },
      {
        "paperId": "08f8fdb2ac880bb8cc4f4e529f36772e2067dbbd",
        "title": "When Data Geometry Meets Deep Function: Generalizing Offline Reinforcement Learning"
      },
      {
        "paperId": "714b7c61d81687d093fd8b7d6d6737d582a4c9b7",
        "title": "Towards Applicable Reinforcement Learning: Improving the Generalization and Sample Efficiency with Policy Ensemble"
      },
      {
        "paperId": "23abe79046d7f7b430d5d21b6a93598d0aa1b9c2",
        "title": "Pessimistic Bootstrapping for Uncertainty-Driven Offline Reinforcement Learning"
      },
      {
        "paperId": "3c29bf1f84ce12a71616e7319b8e358386cb0883",
        "title": "A Behavior Regularized Implicit Policy for Offline Reinforcement Learning"
      },
      {
        "paperId": "7ed1566a286068f39effa67ae5c7489dbea06414",
        "title": "VRL3: A Data-Driven Framework for Visual Deep Reinforcement Learning"
      },
      {
        "paperId": "6f987340adf4f0b90cf900d3e219341cf46248d5",
        "title": "Sample Efficient Deep Reinforcement Learning via Uncertainty Estimation"
      },
      {
        "paperId": "63afc8d1a187d2f2faf603a51d3987db89574308",
        "title": "RvS: What is Essential for Offline RL via Supervised Learning?"
      },
      {
        "paperId": "ca5c8d732e7f6046af1b23b549e08460a1893ade",
        "title": "Plan Better Amid Conservatism: Offline Multi-Agent Reinforcement Learning with Actor Rectification"
      },
      {
        "paperId": "17609f260d7deb836702029521c7f120c61ad6a9",
        "title": "The Difficulty of Passive Learning in Deep Reinforcement Learning"
      },
      {
        "paperId": "d8fbe83ea0863722ad445f8f6bb7c4d4d9793071",
        "title": "False Correlation Reduction for Offline Reinforcement Learning"
      },
      {
        "paperId": "b7ba291af983b5e31f3adfa9a0ceb7a7b3114c7f",
        "title": "Offline Reinforcement Learning with Value-based Episodic Memory"
      },
      {
        "paperId": "5363112778f8e71a69ca717611550bbeaabc0be2",
        "title": "Balancing Value Underestimation and Overestimationwith Realistic Actor-Critic"
      },
      {
        "paperId": "46277ce0f40fd8565a59e46af7400342434f796f",
        "title": "Value Penalized Q-Learning for Recommender Systems"
      },
      {
        "paperId": "0407889beacd6add97aa7b13f4a98724acc9cbef",
        "title": "Offline Reinforcement Learning with Soft Behavior Regularization"
      },
      {
        "paperId": "57343996ae98043a4343634d867afdbecc89b999",
        "title": "Offline RL With Resource Constrained Online Deployment"
      },
      {
        "paperId": "9560080a2c32682bd1c1a9850a54ca6163f1956e",
        "title": "Uncertainty-Based Offline Reinforcement Learning with Diversified Q-Ensemble"
      },
      {
        "paperId": "7974c3d631ab7e2d8e05ce6ad69cadf06667f15c",
        "title": "MEPG: A Minimalist Ensemble Policy Gradient Framework for Deep Reinforcement Learning"
      },
      {
        "paperId": "1e00e8760472dab0fe1632a04a037d52d227d3a6",
        "title": "Provable Benefits of Actor-Critic Methods for Offline Reinforcement Learning"
      },
      {
        "paperId": "01961d9cedaef8f08ed822012e5083eaeed70128",
        "title": "Offline Decentralized Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "245682e8b3fa76f4a3e2991b5497577af95cbb3f",
        "title": "COMBO: Conservative Offline Model-Based Policy Optimization"
      },
      {
        "paperId": "16b03f654b259eaeb744b185191490a953af3d5f",
        "title": "Learning of Long-Horizon Sparse-Reward Robotic Manipulator Tasks With Base Controllers"
      },
      {
        "paperId": "1433c709492dc79b0c58a5090f543d374990a000",
        "title": "Cross-Domain Offline Policy Adaptation with Optimal Transport and Dataset Constraint"
      },
      {
        "paperId": "b90dbe327bde8ce4d4055231d08fc64de318e01c",
        "title": "ContraDiff: Planning Towards High Return States via Contrastive Learning"
      },
      {
        "paperId": "5b9ed6108e8655f3dc1569759f9f8ed3fd028f97",
        "title": "Cross-Domain Generalization with Reverse Dynamics Models in Offline Model-Based Reinforcement Learning"
      },
      {
        "paperId": "2995fdd2073562effa7bcfed17c2928527f575c2",
        "title": "Neural Stochastic Differential Equations for Uncertainty-Aware Offline RL"
      },
      {
        "paperId": "941e31df751884e8319d275ecaa5697407001347",
        "title": "Uncertainty-Aware Rank-One MIMO Q Network Framework for Accelerated Offline Reinforcement Learning"
      },
      {
        "paperId": "9950fe19120674047c2c3358753e328744347976",
        "title": "Offline Reinforcement Learning via Tsallis Regularization"
      },
      {
        "paperId": "60601c268a5397efdbf70ab2795495c83633dea0",
        "title": "SpOiLer: Offline reinforcement learning using scaled penalties"
      },
      {
        "paperId": "1184d7f464eb21847a0bc0216e53fe6501b1da1b",
        "title": "Implicit and Explicit Policy Constraints for Offline Reinforcement Learning"
      },
      {
        "paperId": "3ed097d7351c32bb971a00d981281b5a9c28facd",
        "title": "Uncertainty-driven Trajectory Truncation for Model-based Offline Reinforcement Learning"
      },
      {
        "paperId": "6be34c9df17ce93d45566f41e54f26316d9e43bb",
        "title": "Uncertainty-Aware Off-Policy Learning"
      },
      {
        "paperId": "5864161371cfb3e4f1f8c55692f74779a0df61c6",
        "title": "Spurious Correlation Reduction for Offline Reinforcement Learning"
      },
      {
        "paperId": "a8dac0d0837ac4800f4462a121c59a98a05531ee",
        "title": "Provably Efficient Neural Offline Reinforcement Learning via Perturbed Rewards"
      },
      {
        "paperId": "d8f9bcd397bc008f2f50cbce48e7bd5d6f9b7b55",
        "title": "Learning to Coordinate from Offline Datasets with Uncoordinated Behavior Policies"
      },
      {
        "paperId": "b1d94794b356dc9ff3f5334035c90688485dcdcb",
        "title": "Curriculum Offline Reinforcement Learning"
      },
      {
        "paperId": "f01b7d88743a0833bc101b8b7e13730dbf49de2b",
        "title": "Conformal Prediction for Uncertainty-Aware Planning with Diffusion Dynamics Model"
      },
      {
        "paperId": "04ca100936f7c304b8986fe93c97987b59152dca",
        "title": "Supported Value Regularization for Offline Reinforcement Learning"
      },
      {
        "paperId": "ceac3aaba97b40e9ace78dfad0331f28efb36e02",
        "title": "Designing an offline reinforcement learning objective from scratch"
      },
      {
        "paperId": "9b8444f3fba46f861740808fc20bf90ae791b478",
        "title": "Domain Generalisation for Robust Model-Based Offline Reinforcement Learning"
      },
      {
        "paperId": "1e650cb12aaad371a49b0c8c4514e1b988a5178c",
        "title": "Pareto Policy Pool for Model-based Offline Reinforcement Learning"
      },
      {
        "paperId": "34880fe679b1c2c0fbf130ebba5251ca2981af25",
        "title": "A Regularized Implicit Policy for Offline Reinforcement Learning"
      },
      {
        "paperId": "4ff099b6fa07017bb065669028957c9ff36041ab",
        "title": "Distance-Sensitive Offline Reinforcement Learning"
      },
      {
        "paperId": "bc06ddb3a55ee829989fe9638b3bf0c811a3dbd6",
        "title": "A Survey of Learning on Small Data"
      },
      {
        "paperId": "6e33c58326ad8e6f0b9719fe398771878383d558",
        "title": "D OMAIN G ENERALIZATION FOR R OBUST M ODEL -B ASED O FFLINE RL"
      },
      {
        "paperId": "99dff0f1591050f09fd471692b9b62a9b744508c",
        "title": "LAPO: Latent-Variable Advantage-Weighted Policy Optimization for Offline Reinforcement Learning"
      },
      {
        "paperId": "9e968f8186bc79e46756a081e272141f75e5a9b6",
        "title": "Benchmarking Sample Selection Strategies for Batch Reinforcement Learning"
      },
      {
        "paperId": "bdd86c092ef55484a56966be48809c1becf38d0f",
        "title": "SCORE: Spurious COrrelation REduction for Offline Reinforcement Learning"
      },
      {
        "paperId": "d9d6d8f082b333f690d166321a235a3a8be5b07c",
        "title": "Uncertainty-Based Of\ufb02ine Reinforcement Learning with Diversi\ufb01ed Q-Ensemble"
      },
      {
        "paperId": "6bd8ceca5c38521ee928bed68e07a6c277632d13",
        "title": "E NTROPY -R EGULARIZED M ODEL -B ASED O FFLINE R EINFORCEMENT L EARNING"
      },
      {
        "paperId": "f73d2945db52f419feba1ece3180fb8832797705",
        "title": "P ESSIMISTIC P OLICY I TERATION FOR O FFLINE R EIN - FORCEMENT L EARNING"
      },
      {
        "paperId": "ed3a6c456b7447a45886097694ef41c946082bbd",
        "title": "Improving Generalization in Of\ufb02ine Reinforcement Learning via Adversarial Data Splitting"
      },
      {
        "paperId": "900a5e6fac28c8f3180a5c80499634da92832bda",
        "title": "O\ufb04ine Reinforcement Learning without Regularization and Pessimism"
      },
      {
        "paperId": "476433f07b95d3e095a1c31ccf6be8d3f9e10ead",
        "title": "A Closer Look at Of\ufb02ine RL Agents"
      },
      {
        "paperId": "cce859de2e5b9a6a254b652cb0c2a34d1224b6b5",
        "title": "A UTOMATIC F INE -T UNED O FFLINE - TO -O NLINE R EINFORCEMENT L EARNING VIA I NCREASED S IMPLE M OVING A VERAGE Q-VALUE"
      },
      {
        "paperId": "149daa710da4f7eeed40dab51ff1b27494dd924a",
        "title": "Safe and Efficient Offline Reinforcement Learning: The Critic is Critical"
      },
      {
        "paperId": "1d40ee2edc215644b738eca9ad931cf5c1c5749d",
        "title": "KALM: Knowledgeable Agent by Offline Reinforcement Learning from Large Language Model Rollouts"
      },
      {
        "paperId": "f440cb7a24317ffa4016466d5206c914c7207e62",
        "title": "Double Policy Estimation for Importance Sampling in Sequence Modeling Based Reinforcement Learning"
      }
    ],
    "score": 49.0
  },
  {
    "id": "2064020586d5832b55f80a7dffea1fd90a5d94dd",
    "title": "Improving Exploration in Evolution Strategies for Deep Reinforcement Learning via a Population of Novelty-Seeking Agents",
    "authors": [
      "Edoardo Conti",
      "Vashisht Madhavan",
      "F. Such",
      "J. Lehman",
      "Kenneth O. Stanley",
      "J. Clune"
    ],
    "year": 2017,
    "citationCount": 351,
    "abstract": "Evolution strategies (ES) are a family of black-box optimization algorithms able to train deep neural networks roughly as well as Q-learning and policy gradient methods on challenging deep reinforcement learning (RL) problems, but are much faster (e.g. hours vs. days) because they parallelize better. However, many RL problems require directed exploration because they have reward functions that are sparse or deceptive (i.e. contain local optima), and it is unknown how to encourage such exploration with ES. Here we show that algorithms that have been invented to promote directed exploration in small-scale evolved neural networks via populations of exploring agents, specifically novelty search (NS) and quality diversity (QD) algorithms, can be hybridized with ES to improve its performance on sparse or deceptive deep RL tasks, while retaining scalability. Our experiments confirm that the resultant new algorithms, NS-ES and two QD algorithms, NSR-ES and NSRA-ES, avoid local optima encountered by ES to achieve higher performance on Atari and simulated robots learning to walk around a deceptive trap. This paper thus introduces a family of fast, scalable algorithms for reinforcement learning that are capable of directed exploration. It also adds this new family of exploration algorithms to the RL toolbox and raises the interesting possibility that analogous algorithms with multiple simultaneous paths of exploration might also combine well with existing RL algorithms outside ES.",
    "url": "https://www.semanticscholar.org/paper/2064020586d5832b55f80a7dffea1fd90a5d94dd",
    "pdf_url": "https://arxiv.org/pdf/1712.06560.pdf",
    "venue": "Neural Information Processing Systems",
    "publicationDate": "2017-12-18",
    "externalIds": {
      "MAG": "2963790038",
      "DBLP": "conf/nips/ContiMSLSC18",
      "ArXiv": "1712.06560",
      "CorpusId": 24994970
    },
    "references": [
      {
        "paperId": "819bcae49054e00cef3c0972d48b4e40a525f4d9",
        "title": "Deep Neuroevolution: Genetic Algorithms Are a Competitive Alternative for Training Deep Neural Networks for Reinforcement Learning"
      },
      {
        "paperId": "af10f3c1c0859aa620623f760c8a29e78f177f7f",
        "title": "Population Based Training of Neural Networks"
      },
      {
        "paperId": "3fab4d28be5ff73f42c8b307117aa8d680345ad9",
        "title": "Noisy Networks for Exploration"
      },
      {
        "paperId": "142497432fe179ddb6ffe600c64a837ec6179550",
        "title": "Parameter Space Noise for Exploration"
      },
      {
        "paperId": "7ddca18a41ec7058425fc21562db94ceb6c1c7c1",
        "title": "Diffusion-based neuromodulation can eliminate catastrophic forgetting in simple neural networks"
      },
      {
        "paperId": "225ab689f41cef1dc18237ef5dab059a49950abf",
        "title": "Curiosity-Driven Exploration by Self-Supervised Prediction"
      },
      {
        "paperId": "3167b590e47b08828555938d3126fde1bb3c038e",
        "title": "Stein Variational Policy Gradient"
      },
      {
        "paperId": "4ee802a58d32aa049d549d06be440ac947b53987",
        "title": "Evolution Strategies as a Scalable Alternative to Reinforcement Learning"
      },
      {
        "paperId": "12f67fb182bc934fc95ce97acff553d83e2ca72e",
        "title": "Count-Based Exploration with Neural Density Models"
      },
      {
        "paperId": "0359739027d44f2baa0ae7e99fca8e3400b8181f",
        "title": "Evolving Deep Neural Networks"
      },
      {
        "paperId": "2e55ba6c97ce5eb55abd959909403fe8da7e9fe9",
        "title": "Overcoming catastrophic forgetting in neural networks"
      },
      {
        "paperId": "0fcb2034e31a2bc2f12a2b1363d0d77baf445fdf",
        "title": "#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning"
      },
      {
        "paperId": "250b20c201df2dd3fb425a95f9be61ed8b80afda",
        "title": "Curiosity Search: Producing Generalists by Encouraging Individuals to Continually Explore and Acquire Skills throughout Their Lifetime"
      },
      {
        "paperId": "6c34b839fc0a90cbb5ec9e00ed797f8d1270a75d",
        "title": "Does Aligning Phenotypic and Genotypic Modularity Improve the Evolution of Neural Networks?"
      },
      {
        "paperId": "237d50e1188a22c9648021b8372ade1544c1c602",
        "title": "Learning Behavior Characterizations for Novelty Search"
      },
      {
        "paperId": "eaa518ad767dae7875f7f8fbc9eca6e05f9d11d0",
        "title": "Quality Diversity: A New Frontier for Evolutionary Computation"
      },
      {
        "paperId": "0936352b78a52bc5d2b5e3f04233efc56664af51",
        "title": "Conditional Image Generation with PixelCNN Decoders"
      },
      {
        "paperId": "571b0750085ae3d939525e62af510ee2cee9d5ea",
        "title": "Improved Techniques for Training GANs"
      },
      {
        "paperId": "6e90fd78e8a3b98af3954aae5209703aa966603e",
        "title": "Unifying Count-Based Exploration and Intrinsic Motivation"
      },
      {
        "paperId": "2b10281297ee001a9f3f4ea1aa9bea6b638c27df",
        "title": "OpenAI Gym"
      },
      {
        "paperId": "07925910d45761d96269fc3bdfdc21b1d20d84ad",
        "title": "Deep Learning without Poor Local Minima"
      },
      {
        "paperId": "317cd4522b1f4a6f889743578143bb8823623f8b",
        "title": "VIME: Variational Information Maximizing Exploration"
      },
      {
        "paperId": "69e76e16740ed69f4dc55361a3d319ac2f1293dd",
        "title": "Asynchronous Methods for Deep Reinforcement Learning"
      },
      {
        "paperId": "1c4927af526d5c28f7c2cfa492ece192d80a61d4",
        "title": "Policy Distillation"
      },
      {
        "paperId": "6e487937856c3e70d4c5062548ce4533754a27c6",
        "title": "Confronting the Challenge of Quality Diversity"
      },
      {
        "paperId": "2470fcf0f89082de874ac9133ccb3a8667dd89a8",
        "title": "Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models"
      },
      {
        "paperId": "45373921f06a6efebefa6189d2dd80362ab0836e",
        "title": "Illuminating search spaces by mapping elites"
      },
      {
        "paperId": "340f48901f72278f6bf78a04ee5b01df208cc508",
        "title": "Human-level control through deep reinforcement learning"
      },
      {
        "paperId": "449532187c94af3dd3aa55e16d2c50f7854d2199",
        "title": "Trust Region Policy Optimization"
      },
      {
        "paperId": "995c5f5e62614fcb4d2796ad2faab969da51713e",
        "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"
      },
      {
        "paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8",
        "title": "Adam: A Method for Stochastic Optimization"
      },
      {
        "paperId": "1a632fb89b6b05dc16fbc026d86e390e22ca6ac3",
        "title": "Robots that can adapt like animals"
      },
      {
        "paperId": "1aef73d4225e6190c830e71ba9f20872cbf19aab",
        "title": "Novelty search creates robots with general skills for exploration"
      },
      {
        "paperId": "df207ae851616a3d2cf9126cf3f7327353136872",
        "title": "Systematic Derivation of Behaviour Characterisations in Evolutionary Robotics"
      },
      {
        "paperId": "981ce6b655cc06416ff6bf7fac8c6c2076fd7fac",
        "title": "Identifying and attacking the saddle point problem in high-dimensional non-convex optimization"
      },
      {
        "paperId": "2319a491378867c7049b3da055c5df60e1671158",
        "title": "Playing Atari with Deep Reinforcement Learning"
      },
      {
        "paperId": "1e6fb805131943525f5c7126299611865b76e061",
        "title": "Behavioral repertoire learning in robotics"
      },
      {
        "paperId": "f82e4ff4f003581330338aaae71f60316e58dd26",
        "title": "The Arcade Learning Environment: An Evaluation Platform for General Agents"
      },
      {
        "paperId": "25b8211adafb5a1863558992adc2a693fb082eb4",
        "title": "Evolving a diversity of virtual creatures through novelty search and local competition"
      },
      {
        "paperId": "88d40cd5d8a937d7ce7f3ca0fb95fa281d9500a6",
        "title": "Novelty-based restarts for evolution strategies"
      },
      {
        "paperId": "0de77eceda6308618132204b28755ac1e63648c5",
        "title": "Abandoning Objectives: Evolution Through the Search for Novelty Alone"
      },
      {
        "paperId": "ab45bf760d53b61bdd333306d4901f31507320d6",
        "title": "When Novelty Is Not Enough"
      },
      {
        "paperId": "33224ad0cdf6e2dc4893194dd587309c7887f0ba",
        "title": "Formal Theory of Creativity, Fun, and Intrinsic Motivation (1990\u20132010)"
      },
      {
        "paperId": "36086ff255207cc1adb818c4d0cd62287d437d38",
        "title": "Deep auto-encoder neural networks in reinforcement learning"
      },
      {
        "paperId": "2a25f57a5ac627e5f7a2e4502c24bfa51cfdc1cf",
        "title": "2010 Special Issue: Parameter-exploring policy gradients"
      },
      {
        "paperId": "dd5cf95a7af93d2733120d177c593989b19b98fe",
        "title": "Natural Evolution Strategies"
      },
      {
        "paperId": "e8077729ef723d71a0b7492f2f271ae413b5a4d5",
        "title": "What is Intrinsic Motivation? A Typology of Computational Approaches"
      },
      {
        "paperId": "26afab5607f4bfaf2fb9f786e4ed4f2d93c88e84",
        "title": "Reducing the Time Complexity of the Derandomized Evolution Strategy with Covariance Matrix Adaptation (CMA-ES)"
      },
      {
        "paperId": "2722b9e5ab8da95f03e578bb65879c452c105385",
        "title": "Catastrophic forgetting in connectionist networks"
      },
      {
        "paperId": "ef4f5a50837a7c1b3e87b9300ffc7ba00d461a0f",
        "title": "AUTO-ENCODING VARIATIONAL BAYES"
      },
      {
        "paperId": null,
        "title": "2017), ES scales well with the amount of computation available. Specifically, as more CPUs are used, training times reduce almost linearly, whereas DQN and A3C"
      },
      {
        "paperId": null,
        "title": "2017), the network architecture"
      },
      {
        "paperId": "387ab57cf27e7b8b10e734cd88da5b1c11edc6c7",
        "title": "Super Mario Bros."
      },
      {
        "paperId": "1325bbd04a3e5a7e8137cf2edf9cbca7fc6fd55d",
        "title": "Novelty Search and the Problem with Objectives"
      },
      {
        "paperId": "414af1e96a84fcdb799a12757e35b241b1d0d388",
        "title": "Game-independent AI agents for playing Atari 2600 console games"
      },
      {
        "paperId": "4c915c1eecb217c123a36dc6d3ce52d12c742614",
        "title": "Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning"
      },
      {
        "paperId": "97efafdb4a3942ab3efba53ded7413199f79c054",
        "title": "Reinforcement Learning: An Introduction"
      },
      {
        "paperId": "cb131ac2c7ea8b16295a0ed970239247f71b00f0",
        "title": "Deceptiveness and Genetic Algorithm Dynamics"
      },
      {
        "paperId": null,
        "title": "Evolutionsstrategien. In Simulationsmethoden in der Medizin und Biologie, pages 83\u2013114"
      },
      {
        "paperId": null,
        "title": "Soros , and Kenneth O . Stanley"
      }
    ],
    "cited_by": [
      {
        "paperId": "21b81883c1f4c789fec33d239ef173ea51b73918",
        "title": "Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning"
      },
      {
        "paperId": "9c9eb42629b24feb04fdf778584af0094603f815",
        "title": "Boosting Exploration in Reinforcement Learning for Sparse Reward Tasks"
      },
      {
        "paperId": "5b117e8f678a966e4cfa6ceedb515a923cc0331f",
        "title": "Robust Dynamic Material Handling via Adaptive Constrained Evolutionary Reinforcement Learning"
      },
      {
        "paperId": "eb22a58bd21cd490c99cd2f76da5433a5dcbb1c0",
        "title": "HeurAgenix: Leveraging LLMs for Solving Complex Combinatorial Optimization Challenges"
      },
      {
        "paperId": "2794a8bd7b2a49eeb5d9909bf8c39ece6ca23b1b",
        "title": "Provable Reinforcement Learning from Human Feedback with an Unknown Link Function"
      },
      {
        "paperId": "34d8817c85fd7f351e75a6bcbdd851a559877300",
        "title": "Trajectory First: A Curriculum for Discovering Diverse Policies"
      },
      {
        "paperId": "caf20774ab23a550995ca13638c9b4e2704311dc",
        "title": "ZO-APFPG: A Provably Zeroth-Order Adaptive Personalized Federated Policy Gradient Algorithm for Reinforcement Learning with Environment Heterogeneity"
      },
      {
        "paperId": "4a0be5039b2d462fedafec282ac19dce5746dad8",
        "title": "Diversity-Aware Policy Optimization for Large Language Model Reasoning"
      },
      {
        "paperId": "309bd4508e9f454dd74ca3075c05ec82d2872990",
        "title": "ComPO: Preference Alignment via Comparison Oracles"
      },
      {
        "paperId": "f9dc23dd57aa7b3bcde7194d484a559f68a92bff",
        "title": "Evolutionary Policy Optimization"
      },
      {
        "paperId": "d1f3ee24307e1fa04d000959dbef8601d90a8c86",
        "title": "Quality Diversity for Variational Quantum Circuit Optimization"
      },
      {
        "paperId": "3f241b46a94f5bdb08abcea4f862e6c4688a49ad",
        "title": "Beyond Possessive Agency: TikTok, YouTube, and the Inadequacies of GDPR, OSA, DSA, and AIA"
      },
      {
        "paperId": "08c2db784cd81063b2ed5d66d175051ccbfee77f",
        "title": "Latent Embedding Adaptation for Human Preference Alignment in Diffusion Planners"
      },
      {
        "paperId": "63a83544427e4ce5ef7d32cfc070c4d992bef124",
        "title": "CubeAgent: Efficient query-based video adversarial examples generation through deep reinforcement learning"
      },
      {
        "paperId": "410f3513063ff60cee9cf37bfe80a35991624cb5",
        "title": "Utilizing Novelty-based Evolution Strategies to Train Transformers in Reinforcement Learning"
      },
      {
        "paperId": "436758c4f4ee1306f3c3d85e0c56a65664fc45e3",
        "title": "Fuzzy logic applied to tunning mutation size in evolutionary algorithms"
      },
      {
        "paperId": "5b4c54594509a3c3071acf2abe9ed699b46972ad",
        "title": "Pareto Set Learning for Multi-Objective Reinforcement Learning"
      },
      {
        "paperId": "5689ae64aa5595a2d54f60b2a45980c844fe9310",
        "title": "Proposing Hierarchical Goal-Conditioned Policy Planning in Multi-Goal Reinforcement Learning"
      },
      {
        "paperId": "be7df72b6848eaa83035f8a21971bdd484d7dd63",
        "title": "Interlocking-free Selective Rationalization Through Genetic-based Learning"
      },
      {
        "paperId": "f40de37f18aa1ea521295bf350b8509a246d23e0",
        "title": "Acceleration for Deep Reinforcement Learning using Parallel and Distributed Computing: A Survey"
      },
      {
        "paperId": "11b46778501fdd825f6385f7159accc87d44c2b2",
        "title": "Real-Time Policy Optimization for UAV Swarms Based on Evolution Strategies"
      },
      {
        "paperId": "1ad56245d9a8e302152766ab45258ab896dbbc8b",
        "title": "Sharpness-Aware Black-Box Optimization"
      },
      {
        "paperId": "85c961d5b3fea95b48f94c0461782e887a8b3b0f",
        "title": "Zeroth-Order Policy Gradient for Reinforcement Learning from Human Feedback without Reward Inference"
      },
      {
        "paperId": "197d763381fc4885205237f9cf01e63792a6e075",
        "title": "Exploration-Driven Reinforcement Learning for Avionic System Fault Detection (Experience Paper)"
      },
      {
        "paperId": "4c2bf778ad14aaddbb66a02a2ab2735f7e75f93a",
        "title": "Diversifying Policies With Non-Markov Dispersion to Expand the Solution Space"
      },
      {
        "paperId": "bc5a02469eda208c6df470da4840d2f6d00609da",
        "title": "A Single Goal is All You Need: Skills and Exploration Emerge from Contrastive RL without Rewards, Demonstrations, or Subgoals"
      },
      {
        "paperId": "cbef0337ff213119ac4bb53897ef49b8e2a752ce",
        "title": "Summary of \"Curiosity creates Diversity in Policy Search\""
      },
      {
        "paperId": "9c96bcd722ffee253d64d3c1014ae10a9a5ec8cf",
        "title": "Informed Diversity Search for Learning in Asymmetric Multiagent Systems"
      },
      {
        "paperId": "0126490c9934b4f369030fca280237e06be1c03e",
        "title": "Covariance matrix adaptation evolution strategy based on correlated evolution paths with application to reinforcement learning"
      },
      {
        "paperId": "1820e81b187284112d645f38b4bfe6c86b72154e",
        "title": "A hybrid evolution strategies-simulated annealing algorithm for job shop scheduling problems"
      },
      {
        "paperId": "c7ed31463edfb640902e1104a18746be8959a689",
        "title": "Multi-Agent Reinforcement Learning with Asymmetric Representation Assisted by Multi-Objective Evolutionary Algorithms"
      },
      {
        "paperId": "1e09172e1bbc88b5454620ce87db75413ea9afe1",
        "title": "Mimicry and the Emergence of Cooperative Communication"
      },
      {
        "paperId": "db5d6598901c5c85ada57996c8164f93b8fd8458",
        "title": "Quality with Just Enough Diversity in Evolutionary Policy Search"
      },
      {
        "paperId": "d8b10ccccd1fd7295ce86e0c214925b37f6a22de",
        "title": "Continuously evolving rewards in an open-ended environment"
      },
      {
        "paperId": "160b6bf8fc4604aac8f0d05c7d9f2cd96b04e26e",
        "title": "Large Language Models as In-context AI Generators for Quality-Diversity"
      },
      {
        "paperId": "e3683278f61f5ad5b9d3c79126ade80e90dac987",
        "title": "Generalized Population-Based Training for Hyperparameter Optimization in Reinforcement Learning"
      },
      {
        "paperId": "0a46734079af14a423c477a5324c6c500fa94dcc",
        "title": "Extremum-Seeking Action Selection for Accelerating Policy Optimization"
      },
      {
        "paperId": "8357670aac3c98a71b454ab5bca89558f265369d",
        "title": "Two-Stage Evolutionary Reinforcement Learning for Enhancing Exploration and Exploitation"
      },
      {
        "paperId": "ed168bc1cf9f2dfa2a07618fda89204276e63d2f",
        "title": "Anomaly detection in time-series data using evolutionary neural architecture search with non-differentiable functions"
      },
      {
        "paperId": "a519a5477ba9696aefb2aaad5bf547ec422f3763",
        "title": "Quality-Diversity Algorithms Can Provably Be Helpful for Optimization"
      },
      {
        "paperId": "f468afbac76e79f7b2fb137339715e505018be73",
        "title": "Density Descent for Diversity Optimization"
      },
      {
        "paperId": "cf68c4559fc42755b21306c336ccf2b280d8f8eb",
        "title": "Shared Autonomy Locomotion Synthesis With a Virtual Powered Prosthetic Ankle"
      },
      {
        "paperId": "4eaeff753902cdb6b153bcd0f6de5efc70bc4b7c",
        "title": "Cost Explosion for Efficient Reinforcement Learning Optimisation of Quantum Circuits"
      },
      {
        "paperId": "c47785aa96672fdb1d2da3515a5c9a0cf8997e69",
        "title": "Quality Diversity through Human Feedback"
      },
      {
        "paperId": "0de72952fa31b595f8dfd6f370dfe26463cf18f1",
        "title": "Diversity from Human Feedback"
      },
      {
        "paperId": "9907b6596008845a832f441a16ab482a1622994b",
        "title": "Reward tampering and evolutionary computation: a study of concrete AI-safety problems using evolutionary algorithms"
      },
      {
        "paperId": "7b7eed33615f61175eeac693f62fbcf5ba35de31",
        "title": "Quality Diversity under Sparse Reward and Sparse Interaction: Application to Grasping in Robotics"
      },
      {
        "paperId": "9bd9db1af9c004c3b13b9e185b22791f21f3bb78",
        "title": "BiERL: A Meta Evolutionary Reinforcement Learning Framework via Bilevel Optimization"
      },
      {
        "paperId": "357c871f33b5fe11be8f4efc2008eb6c9207e6bb",
        "title": "A survey on Evolutionary Reinforcement Learning algorithms"
      },
      {
        "paperId": "53f8ce42ae198befe04259c966663705944bc05a",
        "title": "Adaptive Estimation Q-learning with Uncertainty and Familiarity"
      },
      {
        "paperId": "584ca135b61482fd89247113da87d784f738dbfa",
        "title": "Foundational Models Defining a New Era in Vision: A Survey and Outlook"
      },
      {
        "paperId": "07c75f0a7dd52e9c70e9159dd4e8780eef37c671",
        "title": "Robust Driving Policy Learning with Guided Meta Reinforcement Learning"
      },
      {
        "paperId": "99759171d799d1894eb5893beedff3e961de98e3",
        "title": "Overcoming Deceptive Rewards with Quality-Diversity"
      },
      {
        "paperId": "48e985318859866c9174f6312680a1515eac4abf",
        "title": "Stable and Sample-Efficient Policy Search for Continuous Control via Hybridizing Phenotypic Evolutionary Algorithm with the Double Actors Regularized Critics"
      },
      {
        "paperId": "bd6ff3e59b605cb3a6be4a3ee37a7eabb3e1c2bc",
        "title": "Evolution strategies-based optimized graph reinforcement learning for solving dynamic job shop scheduling problem"
      },
      {
        "paperId": "b9e883e168d5b25ee23fc361e3a5c51401c76123",
        "title": "Evolutionary Strategy Guided Reinforcement Learning via MultiBuffer Communication"
      },
      {
        "paperId": "e95fa69159e0007397994d17b5f176182096de3b",
        "title": "Multi-Dimensional Resource Allocation in Distributed Data Centers Using Deep Reinforcement Learning"
      },
      {
        "paperId": "517393c7b3a0730fc73000b9b575fa5756ed589c",
        "title": "Evolving Connectivity for Recurrent Spiking Neural Networks"
      },
      {
        "paperId": "f197bf0fc2f228483f6af3285000d54d8d97f9eb",
        "title": "Voyager: An Open-Ended Embodied Agent with Large Language Models"
      },
      {
        "paperId": "04dd447d9f174bcd0659fe80de7f3400365035ec",
        "title": "Proximal Policy Gradient Arborescence for Quality Diversity Reinforcement Learning"
      },
      {
        "paperId": "6bd6c5a2bbeaa5fe028ad5ebc3fe203a06269f8d",
        "title": "Games for Artificial Intelligence Research: A Review and Perspectives"
      },
      {
        "paperId": "a1d990d92bf199525aea68b84a56c62eafc7f27b",
        "title": "Byzantine-Resilient Learning Beyond Gradients: Distributing Evolutionary Search"
      },
      {
        "paperId": "f811132d3a737ee1c71b52706f0cd78b8904f056",
        "title": "Learning Diverse Policies with Soft Self-Generated Guidance"
      },
      {
        "paperId": "0426c6de6df07bb371e66d5dbfa0e58f9aa98f6b",
        "title": "The optimized gate recurrent unit based on improved evolutionary algorithm to predict stock market returns"
      },
      {
        "paperId": "c6fecb90fb67d2a14e7c7113d57d07dc8120252d",
        "title": "Enhancing MAP-Elites with Multiple Parallel Evolution Strategies"
      },
      {
        "paperId": "8b7d33373f018535adb24bdc80b312549c2f488f",
        "title": "Evolving Populations of Diverse RL Agents with MAP-Elites"
      },
      {
        "paperId": "4bb9f478cb464049100abafc77b6491b6e5b731a",
        "title": "Evolutionary Reinforcement Learning: A Survey"
      },
      {
        "paperId": "809268c9397a22c1b6034eba115786a3547cd9c8",
        "title": "pyribs: A Bare-Bones Python Library for Quality Diversity Optimization"
      },
      {
        "paperId": "135e90ca09b6d483b86ddf2fb9f901348f0e0d11",
        "title": "Policy Dispersion in Non-Markovian Environment"
      },
      {
        "paperId": "53d661d536965daf4ff6dcfd3b7e42ffa9061d78",
        "title": "Diverse Policy Optimization for Structured Action Space"
      },
      {
        "paperId": "59634cb5db3074b51661a74456338ca2375298bb",
        "title": "Curiosity Creates Diversity in Policy Search"
      },
      {
        "paperId": "20f4c75e9d2bd0e6c31a374b2224f175f115eb59",
        "title": "Winning the CityLearn Challenge: Adaptive Optimization with Evolutionary Search under Trajectory-based Guidance"
      },
      {
        "paperId": "72a3203ffa726e532f94e93e0ce2fcfb43765d5c",
        "title": "APR-ES: Adaptive Penalty-Reward Based Evolution Strategy for Deep Reinforcement Learning"
      },
      {
        "paperId": "a2d6e9091d3474392a6e2e9b91b0b657afaa4bf0",
        "title": "Reinforced Genetic Algorithm for Structure-based Drug Design"
      },
      {
        "paperId": "970928133696fbe66300d0f402e629a10ebf5537",
        "title": "Assessing Quality-Diversity Neuro-Evolution Algorithms Performance in Hard Exploration Problems"
      },
      {
        "paperId": "e03d6414dd5a3e7fcac7fe273089ca6e5ad848dd",
        "title": "Efficient Exploration using Model-Based Quality-Diversity with Gradients"
      },
      {
        "paperId": "9e8b29d025cd2718ff61b363ce1c1f422d612303",
        "title": "Learning General World Models in a Handful of Reward-Free Deployments"
      },
      {
        "paperId": "d71c13d5d9cdcce68a396a46f6d978b308537dc6",
        "title": "Weibull-Open-World (WOW) Multi-Type Novelty Detection in CartPole3D"
      },
      {
        "paperId": "38df075cbaa327f2d34d6371b4e4b0f77ea7d6d7",
        "title": "A novelty-search-based evolutionary reinforcement learning algorithm for continuous optimization problems"
      },
      {
        "paperId": "52f8e8c9c6278ec4fa50b299ae242d3c45595293",
        "title": "Efficient circuit implementation for coined quantum walks on binary trees and application to reinforcement learning"
      },
      {
        "paperId": "c0a1e468f8d7222f133f8cd2800855c867c7665a",
        "title": "Training Diverse High-Dimensional Controllers by Scaling Covariance Matrix Adaptation MAP-Annealing"
      },
      {
        "paperId": "2cd4233bf08e45760645f1e79be0acbd4262163e",
        "title": "Neuroevolution is a Competitive Alternative to Reinforcement Learning for Skill Discovery"
      },
      {
        "paperId": "603158382a7577aaf365c8a21a421de4ac351e9f",
        "title": "Adaptive Evolution Strategies for Stochastic Zeroth-Order Optimization"
      },
      {
        "paperId": "c64a99ec780d5e1c6ba242a82e72e96eb0c3ef8a",
        "title": "An Information-Theoretic Perspective on Intrinsic Motivation in Reinforcement Learning: A Survey"
      },
      {
        "paperId": "21650e029a725f955e06594876ec19793bb4247e",
        "title": "A scalable species-based genetic algorithm for reinforcement learning problems"
      },
      {
        "paperId": "0c9b5412bcef781b001222a8952c104af84889f5",
        "title": "Self-supervised Sequential Information Bottleneck for Robust Exploration in Deep Reinforcement Learning"
      },
      {
        "paperId": "694c7550e2f034a4f9bfc33bddd704503d859dc9",
        "title": "Survey on Evolutionary Deep Learning: Principles, Algorithms, Applications, and Open Issues"
      },
      {
        "paperId": "59714a5c3af2778fdf888760a20ca87b5e7f6358",
        "title": "Heterogeneous Multi-agent Zero-Shot Coordination by Coevolution"
      },
      {
        "paperId": "9483ad6ef1365b51b3cf23a382ffb18ed7338dc7",
        "title": "The pursuit of happiness: A reinforcement learning perspective on habituation and comparisons"
      },
      {
        "paperId": "75175ec0a794875a1b089f6de6ed57e04f352168",
        "title": "Towards Run-time Efficient Hierarchical Reinforcement Learning"
      },
      {
        "paperId": "07c007f516c0bc09f9e4b72091d9ea1bc48d1b7d",
        "title": "Fiber laser development enabled by machine learning: review and prospect"
      },
      {
        "paperId": "5118c3cd2be04e60b5ce35e8fe18bd7683f5237f",
        "title": "Scalable evolutionary hierarchical reinforcement learning"
      },
      {
        "paperId": "649e713a760143f49c20ed4019fbf5434f89b596",
        "title": "Safety-informed mutations for evolutionary deep reinforcement learning"
      },
      {
        "paperId": "3826f86bae9ab38277ec9bec0b46a57bd001e375",
        "title": "Dynamics-aware novelty search with behavior repulsion"
      },
      {
        "paperId": "b75ff79617904f769c5f61cea3693e021757c600",
        "title": "Exploring Safer Behaviors for Deep Reinforcement Learning"
      },
      {
        "paperId": "d42d33c0024ed1a3a96babd514807a172c595df9",
        "title": "Open-Ended Learning Strategies for Learning Complex Locomotion Skills"
      },
      {
        "paperId": "a6543436d2413ee0f1dbd8bf745b0c4798b0c0fa",
        "title": "Evolutionary neural networks for deep learning: a review"
      },
      {
        "paperId": "f515a23a8ad4bd184b681aa2c776f0604dc3b46b",
        "title": "Deep Surrogate Assisted Generation of Environments"
      },
      {
        "paperId": "87e229b8090525ab1b2344f18e0a69f5ed79e134",
        "title": "The Elements of Flexibility for Task-Performing Systems"
      },
      {
        "paperId": "3212c957a11a46d059e981826372914045f5cc4a",
        "title": "Reinforcement Learning for Branch-and-Bound Optimisation using Retrospective Trajectories"
      },
      {
        "paperId": "94747f3dc831ec30ad96da811a2aa6a690facf74",
        "title": "Promoting Quality and Diversity in Population-based Reinforcement Learning via Hierarchical Trajectory Space Exploration"
      },
      {
        "paperId": "dd44f0a6fd81bf1b884918712188f786cdcade26",
        "title": "Covariance Matrix Adaptation MAP-Annealing"
      },
      {
        "paperId": "a07c0f92b9ac1ee695a99888a37cf9a8ff528533",
        "title": "TTOpt: A Maximum Volume Quantized Tensor Train-based Optimization and its Application to Reinforcement Learning"
      },
      {
        "paperId": "8d2feca35eeff419aefb7d63ab6b51b2241df8c9",
        "title": "Adaptive flow-induced vibration control using distributed sensor/actuator networks with gated recurrent units and genetic algorithms"
      },
      {
        "paperId": "d0e783eeb54893c7a283ccbd044647f8f390b16e",
        "title": "Combining Evolution and Deep Reinforcement Learning for Policy Search: A Survey"
      },
      {
        "paperId": "650e9b20e3962a0e6116f0535a643f43b1dd24c5",
        "title": "Multitask Neuroevolution for Reinforcement Learning With Long and Short Episodes"
      },
      {
        "paperId": "e2f1030ea866914d6767331d1350c3f6963aa195",
        "title": "On-the-fly Strategy Adaptation for ad-hoc Agent Coordination"
      },
      {
        "paperId": "da1e404ae194dcbcce198f5335f49f090403bbd7",
        "title": "Exploration in Deep Reinforcement Learning: A Survey"
      },
      {
        "paperId": "47f28e1057eac8f7ec99ac8250e4bfed0eafd9e9",
        "title": "Unified curiosity-Driven learning with smoothed intrinsic reward estimation"
      },
      {
        "paperId": "f90aad04d28c27e1083e3cf4a2f40c78ddea7ea2",
        "title": "Adaptive evolution strategy with ensemble of mutations for Reinforcement Learning"
      },
      {
        "paperId": "4666e94b48e1513ea61b40f84915a2e01ac06d2a",
        "title": "Gradient-free Multi-domain Optimization for Autonomous Systems"
      },
      {
        "paperId": "5a19a50c5b194dee7cc0fcf61b6a8617fe8e33bf",
        "title": "A Globally Convergent Evolutionary Strategy for Stochastic Constrained Optimization with Applications to Reinforcement Learning"
      },
      {
        "paperId": "5ccf50aba13287580004378817a066bc657acfae",
        "title": "Illuminating the Space of Dungeon Maps, Locked-door Missions and Enemy Placement Through MAP-Elites"
      },
      {
        "paperId": "7d06aa77f40117fbd10daaf124b3b2d911d0e40c",
        "title": "How to Fill the Optimum Set? Population Gradient Descent with Harmless Diversity"
      },
      {
        "paperId": "5a42c0d8f6fa4539493f4bb978d7b22ac9bbb89d",
        "title": "Evolving Neural Networks with Optimal Balance between Information Flow and Connections Cost"
      },
      {
        "paperId": "a24c3e689c351cd5baf420609b11a7ebc1cbc0a3",
        "title": "Approximating gradients for differentiable quality diversity in reinforcement learning"
      },
      {
        "paperId": "fa21a215468e881820266d1df362340987bc3fa8",
        "title": "Diversify and Disambiguate: Learning From Underspecified Data"
      },
      {
        "paperId": "e5c5b15ff7b523ef7160968331dc5bfe057b62fe",
        "title": "Accelerated Quality-Diversity through Massive Parallelism"
      },
      {
        "paperId": "e46811df6731df51fb5c209dea7b835d0cd734da",
        "title": "Deep surrogate assisted MAP-elites for automated hearthstone deckbuilding"
      },
      {
        "paperId": "21e6f813ac6ddaaf0d1d11b32a86afbb9ac27806",
        "title": "Enhanced DQN Framework for Selecting Actions and Updating Replay Memory Considering Massive Non-Executable Actions"
      },
      {
        "paperId": "ca5c8d732e7f6046af1b23b549e08460a1893ade",
        "title": "Plan Better Amid Conservatism: Offline Multi-Agent Reinforcement Learning with Actor Rectification"
      },
      {
        "paperId": "895dbfa06a4441eec97f021ad8944c77e8ddae3f",
        "title": "Agent Spaces"
      },
      {
        "paperId": "05f6f3dc22f23ca39d4e09d22dcfd79032b548ae",
        "title": "Grammar-based cooperative learning for evolving collective behaviours in multi-agent systems"
      },
      {
        "paperId": "b6d149124307ed0ab765e6e1a663d635b08d3f5d",
        "title": "Effects of Different Optimization Formulations in Evolutionary Reinforcement Learning on Diverse Behavior Generation"
      },
      {
        "paperId": "8ae24702cd58751a2d981438c8d450018812809c",
        "title": "Guiding Evolutionary Strategies by Differentiable Robot Simulators"
      },
      {
        "paperId": "ac89d4156c66f792cacbd29600d4cf0cfead71f3",
        "title": "Deep Reinforcement Learning Versus Evolution Strategies: A Comparative Survey"
      },
      {
        "paperId": "9f12cc414d9ad7896deab6f428c0b7e8cd926a02",
        "title": "Evolutionary Self-Replication as a Mechanism for Producing Artificial Intelligence"
      },
      {
        "paperId": "67576622cac5ecc27aeef3160743b3e63a2936b6",
        "title": "Illuminating diverse neural cellular automata for level generation"
      },
      {
        "paperId": "bcf5ec24d943da818e6b083fbacccea11aa8285d",
        "title": "Variational quantum reinforcement learning via evolutionary optimization"
      },
      {
        "paperId": "3be84e24b144541a8cd9030526ef2b8ef2cbfe54",
        "title": "A Survey of Exploration Methods in Reinforcement Learning"
      },
      {
        "paperId": "a734a83f531a416b7b90ca011d9e74c3844792a9",
        "title": "Expressive Cognitive Architecture for a Curious Social Robot"
      },
      {
        "paperId": "fc83e3bb81ef461f9848fae00885dbea66e76fbc",
        "title": "Policy gradient assisted MAP-Elites"
      },
      {
        "paperId": "74f46f22c0fc072f5fb331ab07f51d77f5881bf3",
        "title": "Towards Automatic Actor-Critic Solutions to Continuous Control"
      },
      {
        "paperId": "a8c695d9755a3b514677fef3aa08c08336237560",
        "title": "Differentiable Quality Diversity"
      },
      {
        "paperId": "032731295fb9434a82b31fdb5e50309a2cbfef3d",
        "title": "Discovering Diverse Nearly Optimal Policies withSuccessor Features"
      },
      {
        "paperId": "4dcaa2a7ec73554ef74b7e1b8614972209f183c4",
        "title": "A unified view of likelihood ratio and reparameterization gradients"
      },
      {
        "paperId": "40a0d55b6de6749bf2ec3313d8ed6ca5c0ddd7c6",
        "title": "Deep Reinforcement Learning Versus Evolution Strategies: A Comparative Survey"
      },
      {
        "paperId": "7d00cb3f3e0ee559c713591eefd1a14ae5f60a11",
        "title": "Remaining useful life prediction of turbofan engine with GA optimized hybrid neural network"
      },
      {
        "paperId": "dc6d4a4a6f8f30072381f7c00028eeaaa628bfa9",
        "title": "A Survey on Evolutionary Construction of Deep Neural Networks"
      },
      {
        "paperId": "b1cbe4d5400d6ae593c7de69ec14f6d0a3a85159",
        "title": "Discovering diverse athletic jumping strategies"
      },
      {
        "paperId": "d76b24ec2224091797615e1a2127efcae3a5a506",
        "title": "Neuroevolutive Control of Industrial Processes Through Mapping Elites"
      },
      {
        "paperId": "f532f6fcb1bd0e296ba0c9991fdb9c450d77fc68",
        "title": "Modeling hesitancy in airport choice: A comparison of discrete choice and machine learning methods"
      },
      {
        "paperId": "eb8d1812804ce4dc4ec6588ce05428aee03fb714",
        "title": "Productive fitness in diversity-aware evolutionary algorithms"
      },
      {
        "paperId": "a6d9938a000137ae0953e8ec029ea82f5fb4e39e",
        "title": "MAGMA: An Optimization Framework for Mapping Multiple DNNs on Multiple Accelerator Cores"
      },
      {
        "paperId": "d0290bda376e812353963ac447df4c4889c1dd65",
        "title": "Playing Atari with few neurons"
      },
      {
        "paperId": "de1ef9d97b6d4654b1d8d09741313f0e89850403",
        "title": "Evolution of Situated and Abstract Communication in Leader Selection and Borderline Identification Swarm Robotics Problems"
      },
      {
        "paperId": "0f6b569af25f46404aced510f047acd18aab08dd",
        "title": "A global optima search field division method for evolutionary algorithms"
      },
      {
        "paperId": "9efd4aefa4999d8ec6fc48e8097643fd9b3c494f",
        "title": "Behavior-Guided Actor-Critic: Improving Exploration via Learning Policy Behavior Representation for Deep Reinforcement Learning"
      },
      {
        "paperId": "be0018474abd2aa9a9267eb39a9e18c02a715498",
        "title": "Quality Evolvability ES: Evolving Individuals With a Distribution of Well Performing and Diverse Offspring"
      },
      {
        "paperId": "4263d650b32bf84c55c3a4cf1ef2dfb4f6e8a108",
        "title": "Hierarchical Semantic Risk Minimization for Large-Scale Classification"
      },
      {
        "paperId": "712304e3f06a8690ca59ea56a394108090ae884d",
        "title": "Policy search with rare significant events: Choosing the right partner to cooperate with"
      },
      {
        "paperId": "5a7db3a24bcc8f17901fe142142a67586559d652",
        "title": "Model-free Policy Learning with Reward Gradients"
      },
      {
        "paperId": "d7291fa1ad88b68e7758efdd1221c5e9ce3c36fa",
        "title": "Direct-Search for a Class of Stochastic Min-Max Problems"
      },
      {
        "paperId": "976f3774b91f9926ac96a44d7d055b8a9ddeedf9",
        "title": "Derivative-free reinforcement learning: a review"
      },
      {
        "paperId": "da7d5ef65577aafecc8764c4435685bac10e0a46",
        "title": "Sparse reward exploration via novelty search and emitters"
      },
      {
        "paperId": "453e1220ee8d876b54cb14f10b1ea6814b574b4a",
        "title": "Enhanced Reinforcement Learning Method Combining One-Hot Encoding-Based Vectors for CNN-Based Alternative High-Level Decisions"
      },
      {
        "paperId": "c281f850cbf765dd7a437f74c9445798e8dbf55a",
        "title": "ES-ENAS: Efficient Evolutionary Optimization for Large Hybrid Search Spaces"
      },
      {
        "paperId": "f0f8f9a52b8130550fa2faeebe8d1595ef9c45fc",
        "title": "Intrinsically Motivated Exploration of Learned Goal Spaces"
      },
      {
        "paperId": "7e077a7009e6af1537a3198f1f761dd6f432d552",
        "title": "General Characterization of Agents by States they Visit"
      },
      {
        "paperId": "01fafe541bd49e8cb8f48c6a462aeb6ae99a60a5",
        "title": "Policy Supervectors: General Characterization of Agents by their Behaviour"
      },
      {
        "paperId": "3e0ced72d6390b08293fddfeafad1964e234e704",
        "title": "Evolutionary training and abstraction yields algorithmic generalization of neural computers"
      },
      {
        "paperId": "b405764900fd2ec3979b16633056e0e6434973a8",
        "title": "Ridge Rider: Finding Diverse Solutions by Following Eigenvectors of the Hessian"
      },
      {
        "paperId": "2004861a5cd5a5a51483785b7be7dd53c0e111ea",
        "title": "Reward Conditioned Neural Movement Primitives for Population-Based Variational Policy Optimization"
      },
      {
        "paperId": "2be22d8a3f39c7bcd1c7639b849e640a0003c831",
        "title": "Harnessing Distribution Ratio Estimators for Learning Agents with Quality and Diversity"
      },
      {
        "paperId": "d1eab8b92c366624ac5f9b0d0e5a193db7bdb661",
        "title": "WECC Composite Load Model Parameter Identification Using Evolutionary Deep Reinforcement Learning"
      },
      {
        "paperId": "6e1ee4042e627e17128ff38adc550c305e539a85",
        "title": "Efficient Wasserstein Natural Gradients for Reinforcement Learning"
      },
      {
        "paperId": "37151639a0f6a59a4d21821c10520459a1c20c91",
        "title": "Instance Weighted Incremental Evolution Strategies for Reinforcement Learning in Dynamic Environments"
      },
      {
        "paperId": "3d5aa58a32a459b433837140f471404113176749",
        "title": "Multimodal Safety-Critical Scenarios Generation for Decision-Making Algorithms Evaluation"
      },
      {
        "paperId": "31494e140814df76667d94a7149787b58497f7f5",
        "title": "Pareto Multi-task Deep Learning"
      },
      {
        "paperId": "cb97aa4880627c5fec9552039469b78a21e5d078",
        "title": "An Evaluation of a Bootstrapped Neuro-Evolution Approach"
      },
      {
        "paperId": "ebbe4f4107c6b7190df5f340a468ef9012f2f15b",
        "title": "Evolutionary Reinforcement Learning via Cooperative Coevolutionary Negatively Correlated Search"
      },
      {
        "paperId": "5e12a40ab9fb3296d1ea0a76ef1c32512a0c0ccd",
        "title": "Lights and Shadows in Evolutionary Deep Learning: Taxonomy, Critical Methodological Analysis, Cases of Study, Learned Lessons, Recommendations and Challenges"
      },
      {
        "paperId": "5347abe013ae752ed749a2a15dc56cba7866a3da",
        "title": "Understanding exploration in humans and machines by formalizing the function of curiosity"
      },
      {
        "paperId": "fdb0c61f2a72e0778a1968d5806e0f98642bd78f",
        "title": "Accelerating Evolutionary Design Exploration with Predictive and Generative Models. (Acc\u00e9l\u00e9rer l'exploration de la conception par algorithme \u00e9volutionniste gr\u00e2ce \u00e0 des mod\u00e8les pr\u00e9dictifs et g\u00e9n\u00e9ratifs)"
      },
      {
        "paperId": "d3985681bfd4b912a64c463e4ff7d52a937cb34d",
        "title": "CoNES: Convex Natural Evolutionary Strategies"
      },
      {
        "paperId": "dc4699ef6addcec3452ba8ce870d237675c672ec",
        "title": "Sample and time efficient policy learning with CMA-ES and Bayesian Optimisation"
      },
      {
        "paperId": "e18c07146c47eca16584ad2aabd54f2bb2ddebf8",
        "title": "On Pros and Cons of Evolving Topologies with Novelty Search"
      },
      {
        "paperId": "e423c07b36936ddce137bce009b318f2c2741be5",
        "title": "Adaptive Procedural Task Generation for Hard-Exploration Problems"
      },
      {
        "paperId": "8307a2915f56f48d07158fa335cb49b3959aa738",
        "title": "Novelty-Guided Reinforcement Learning via Encoded Behaviors"
      },
      {
        "paperId": "553b9e25ada18c8f2d2ab16000e12143c94bb41e",
        "title": "One-Shot Neural Architecture Search via Novelty Driven Sampling"
      },
      {
        "paperId": "d6db51d704eaa33ff8f69e17d7c5d0a66653bf92",
        "title": "EV charging bidding by multi-DQN reinforcement learning in electricity auction market"
      },
      {
        "paperId": "fd6ed4267c9ddc17cd03a493b6505e6b5f41179d",
        "title": "Directed Exploration Via Learnable Probability Distribution For Random Action Selection"
      },
      {
        "paperId": "ad154aa2b6442d724d1093752143392334548e34",
        "title": "Self-Guided Evolution Strategies with Historical Estimated Gradients"
      },
      {
        "paperId": "030b276c519351e72abfced55a166f7ca451b696",
        "title": "Playing Atari with Six Neurons (Extended Abstract)"
      },
      {
        "paperId": "7eb188b3094f8905eb19aa3f3fdc1c15ee195e04",
        "title": "Toward Data\u2010Driven Generation and Evaluation of Model Structure for Integrated Representations of Human Behavior in Water Resources Systems"
      },
      {
        "paperId": "69035fbf8707c1b0e1937f80f487702137c14499",
        "title": "Diversity policy gradient for sample efficient quality-diversity optimization"
      },
      {
        "paperId": "a011901fd788fc5ad452dd3d88f9f0970ea547a8",
        "title": "QD-RL: Efficient Mixing of Quality and Diversity in Reinforcement Learning"
      },
      {
        "paperId": "fca28a48e3a01b9ad4c269511d9d23ff22ccd051",
        "title": "Non-local Policy Optimization via Diversity-regularized Collaborative Exploration"
      },
      {
        "paperId": "0b18c555d6360668451fe113d83deaefb15a792f",
        "title": "Language-Conditioned Goal Generation: a New Approach to Language Grounding for RL"
      },
      {
        "paperId": "8090f2a78481795e3556a93122ed79aa014f2a95",
        "title": "Exploration by Maximizing Renyi Entropy for Reward-Free RL Framework"
      },
      {
        "paperId": "3f673101c2cac3b47639056e2988e018546c3c90",
        "title": "Zeroth-Order Supervised Policy Improvement"
      },
      {
        "paperId": "a6f08a137526b89e18141c3a82f5a8f6fadf3902",
        "title": "Exploration by Maximizing R\u00e9nyi Entropy for Zero-Shot Meta RL"
      },
      {
        "paperId": "8db8a2659cba1f6fdcdebe13bdccbae82be8394b",
        "title": "Learning Algorithmic Solutions to Symbolic Planning Tasks with a Neural Computer Architecture."
      },
      {
        "paperId": "456668d42e2cc428feac6621697c850daa5ef950",
        "title": "Gradient Monitored Reinforcement Learning"
      },
      {
        "paperId": "25706a1b0819a957c3cdabda087f948d7f6bb986",
        "title": "Should artificial agents ask for help in human-robot collaborative problem-solving?"
      },
      {
        "paperId": "99bc5a19a5aa9f85e2c1ecdc9bf8add7b75dae45",
        "title": "Novel Policy Seeking with Constrained Optimization"
      },
      {
        "paperId": "616ac6772508e72084360158078058dfa8bda7e7",
        "title": "First return, then explore"
      },
      {
        "paperId": "36e739a71e408e19d1c37ea4873370f293bfceb2",
        "title": "Evolutionary Stochastic Policy Distillation"
      },
      {
        "paperId": "a8cbef54001aa2077631827f10ca89a702e01a07",
        "title": "Generalize Robot Learning From Demonstration to Variant Scenarios With Evolutionary Policy Gradient"
      },
      {
        "paperId": "af370bc5ddae5ab5b4ab7f3f33ddc8f50377ee6e",
        "title": "Adversarial Evaluation of Autonomous Vehicles in Lane-Change Scenarios"
      },
      {
        "paperId": "3cc1f55462838cc36342b02d96d55ed0bbfc8d3f",
        "title": "Understanding features on evolutionary policy optimizations: feature learning difference between gradient-based and evolutionary policy optimizations"
      },
      {
        "paperId": "ad0ebb70381d76967807abe76377fb977968f129",
        "title": "Fiber: A Platform for Efficient Development and Distributed Training for Reinforcement Learning and Population-Based Methods"
      },
      {
        "paperId": "707eb919d3daa087e63d48930c8630b06c43d24f",
        "title": "Evolutionary Population Curriculum for Scaling Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "9b9ac0169a8d7c4325fbdf0a1660d1b2ad17a80e",
        "title": "Learning feature spaces for regression with genetic programming"
      },
      {
        "paperId": "26f559fd35000a14a5a081827a46a6a6429b5469",
        "title": "Scaling MAP-Elites to deep neuroevolution"
      },
      {
        "paperId": "9aaa11190026766bab8a5b6b7be9f50efb4d24a2",
        "title": "AutoPhase: Juggling HLS Phase Orderings in Random Forests with Deep Reinforcement Learning"
      },
      {
        "paperId": "1a736c856a5871bc67d0b91c59da1e5b9c181f67",
        "title": "ConQUR: Mitigating Delusional Bias in Deep Q-learning"
      },
      {
        "paperId": "3a3b455921724841ca3c0edada24738b560fc637",
        "title": "Simultaneously Evolving Deep Reinforcement Learning Models using Multifactorial optimization"
      },
      {
        "paperId": "9db77e925015ad02efc9beeab233bedbfe04e4b7",
        "title": "Accelerating Reinforcement Learning with a Directional-Gaussian-Smoothing Evolution Strategy"
      },
      {
        "paperId": "39b2551f109fd0495abfca1ab0bb81311c1b3996",
        "title": "Explore, Discover and Learn: Unsupervised Discovery of State-Covering Skills"
      },
      {
        "paperId": "0c62a64c6e758d4cd2d968ca39d41f0f6579c14f",
        "title": "Effective Diversity in Population-Based Reinforcement Learning"
      },
      {
        "paperId": "c8d224bb179c749edcea764e690c2cdaa65b9e93",
        "title": "Periodic Intra-Ensemble Knowledge Distillation for Reinforcement Learning"
      },
      {
        "paperId": "b55de6a119991437d58072baa5240edc8895adcd",
        "title": "Exploration Based Language Learning for Text-Based Games"
      },
      {
        "paperId": "3da43bd9e6d691600e97cd04322660d324d623c0",
        "title": "Population-Guided Parallel Policy Search for Reinforcement Learning"
      },
      {
        "paperId": "3b09ff457604a35e4591ce598bb843376ad848f2",
        "title": "Effective Mutation and Recombination for Evolving Convolutional Networks"
      },
      {
        "paperId": "ecf4fe70d75d52a046699b21ece67a2522ef461c",
        "title": "State-of-the-Art Reinforcement Learning Algorithms"
      },
      {
        "paperId": "d639c183e7518962f3c7f7b78aface6018a7c439",
        "title": "Covariance matrix adaptation for the rapid illumination of behavior space"
      },
      {
        "paperId": "7306f9a6bbc961becdb9b106fbb8e9297742b8de",
        "title": "The Evolution of Reinforcement Learning *"
      },
      {
        "paperId": "43ac3e4d78d59a584cd4e0ea924ce673d424997f",
        "title": "Advances in Neuroevolution through Augmenting Topologies \u2013 A Case Study"
      },
      {
        "paperId": "a5a757260edadb6cf35aa9a3b706fb8b9638df2c",
        "title": "Learning Neural Search Policies for Classical Planning"
      },
      {
        "paperId": "5b925493d2812a5077c4e135cd07bf6647ba59df",
        "title": "Efficient Exploration in Side-Scrolling Video Games with Trajectory Replay"
      },
      {
        "paperId": "2a04670a69e06223aa4eb3356e279f9ab236e08b",
        "title": "Diversifying experiences in multi agent reinforcement learning"
      },
      {
        "paperId": "92ef2853b6007880b3a058e84042fb8cffea3ff8",
        "title": "Mirror Natural Evolution Strategies"
      },
      {
        "paperId": "9fea5a15df9fa4b1a488896cdb0e4db1f05f8ed1",
        "title": "Parallel exploration via negatively correlated search"
      },
      {
        "paperId": "e73b8790d223d7ec907435d632a4366ba80389ae",
        "title": "A unified view of likelihood ratio and reparameterization gradients and an optimal importance sampling scheme"
      },
      {
        "paperId": "9a25fd83dfbb31d93a807298f4aa05d8531e43d9",
        "title": "Evolutionary Learning in Decision Making for Tactical Lane Changing"
      },
      {
        "paperId": "159ac7c47b99bf9d6ff95fe4051ce41a1966dfa4",
        "title": "Stabilizing Off-Policy Reinforcement Learning with Conservative Policy Gradients"
      },
      {
        "paperId": "2ac4ab4047eecbb512aab0f93ea7727fa0f8e736",
        "title": "Learning Algorithmic Solutions to Symbolic Planning Tasks with a Neural Computer"
      },
      {
        "paperId": "5ce1303133d3702431eb7f291744dc92f47e22cb",
        "title": "Behavior-Guided Reinforcement Learning"
      },
      {
        "paperId": "b286725663f96638afe18fa384bad21faff2a185",
        "title": "Evo-NAS: Evolutionary-Neural Hybrid Agent for Architecture Search"
      },
      {
        "paperId": "f99a79ed2163bb479130913929ec8d8a453f7ab9",
        "title": "Comparative Study of NeuroEvolution Algorithms in Reinforcement Learning for Self-Driving Cars"
      },
      {
        "paperId": "6f5c5435549379b78095baa107ff66a92610d1d3",
        "title": "Improved Salp Swarm Algorithm with Space Transformation Search for Training Neural Network"
      },
      {
        "paperId": "521538b3bc1fd64fb0d4dc71881edbeff03c642a",
        "title": "Attraction-Repulsion Actor-Critic for Continuous Control Reinforcement Learning"
      },
      {
        "paperId": "0b10fc8dbecb1ff7c75cd7944f64417f804e9613",
        "title": "Discovering Differential Features: Adversarial Learning for Information Credibility Evaluation"
      },
      {
        "paperId": "2286fc66803fc158e8d95ce19ae99236c811d1f2",
        "title": "Hyper-parameter Optimisation by Restrained Stochastic Hill Climbing"
      },
      {
        "paperId": "6d3d88edd9d8cb0463292ec8fdb14952e27388ce",
        "title": "Multi-task Learning by Pareto Optimality"
      },
      {
        "paperId": "b3e9c1fa15bd08afc7bef3e58c0ac2527f409ec1",
        "title": "Intrinsically Motivated Discovery of Diverse Patterns in Self-Organizing Systems"
      },
      {
        "paperId": "d00873f71a79132ea9b6215580c68e8c065a8467",
        "title": "Learning Representations and Agents for Information Retrieval"
      },
      {
        "paperId": "0dbf2d2c05e1955fbcf3461def336224cc68ce1f",
        "title": "An Evolution Strategy with Progressive Episode Lengths for Playing Games"
      },
      {
        "paperId": "93c07b1aaaaa617709024f3ee2654c91efde8260",
        "title": "Efficient Novelty-Driven Neural Architecture Search"
      },
      {
        "paperId": "9c588cb5879a47f4f35201d112aad2070eca2cc4",
        "title": "Accelerating Reinforcement Learning through GPU Atari Emulation"
      },
      {
        "paperId": "6557f9e7832dd127ab3ea2bcd0d1a6b924d4efc2",
        "title": "Trust Region Evolution Strategies"
      },
      {
        "paperId": "ff46111a636aee36f87f3343ecf5ddecddadc2c3",
        "title": "Evolvability ES: scalable and direct optimization of evolvability"
      },
      {
        "paperId": "ac6449fc71c8768e654a5cdd5f3b3fa230fbb523",
        "title": "POET: open-ended coevolution of environments and their optimized solutions"
      },
      {
        "paperId": "195c930e5b52d8072aadf9def7111ecf97e610b2",
        "title": "Learning a Behavioral Repertoire from Demonstrations"
      },
      {
        "paperId": "59965408cdf673065bb40d6c8b71cb6046a30851",
        "title": "Neuroevolution of Humanoids that Walk Further and Faster with Robust Gaits"
      },
      {
        "paperId": "1067d14c0f3766277cb6a482c5ced515889df37e",
        "title": "Learning to Score Behaviors for Guided Policy Optimization"
      },
      {
        "paperId": "a9e7d06c12505ce70cfd630bce0b78db92754f19",
        "title": "Wasserstein Reinforcement Learning"
      },
      {
        "paperId": "540b2a6afd515f74035ca4c83fc4b5dcbcb7a823",
        "title": "Autonomous Goal Exploration using Learned Goal Spaces for Visuomotor Skill Acquisition in Robots"
      },
      {
        "paperId": "2ed89af84aacaadd0d3abed2517a08adf2262793",
        "title": "Projections for Approximate Policy Iteration Algorithms"
      },
      {
        "paperId": "9a9675cc1d8759150b8ab4623c23a0f37d695781",
        "title": "Enhancing Multi-model Inference with Natural Selection"
      },
      {
        "paperId": "42525a5143c6a87d3ab466684dfa471dc43a5bd0",
        "title": "AI-GAs: AI-generating algorithms, an alternate paradigm for producing general artificial intelligence"
      },
      {
        "paperId": "c3bd5dd10f555bc1622cd37598bdbdf5635c94fe",
        "title": "Leveraging exploration in off-policy algorithms via normalizing flows"
      },
      {
        "paperId": "ec9cf6922aae97b7c728693e78bd5cb812cc4e1e",
        "title": "Learning Novel Policies For Tasks"
      },
      {
        "paperId": "4b07c865db232020318c1ba6bb6f8c3ffbb27baf",
        "title": "ES-CTC: A Deep Neuroevolution Model for Cooperative Intelligent Freeway Traffic Control"
      },
      {
        "paperId": "1f5087be94a5776df61253dc9fed83acc041295c",
        "title": "REVISITING SYSTEMS AND APPLICATIONS OF ARTIFICIAL NEURAL NETWORKS IN CONSTRUCTION ENGINEERING AND MANAGEMENT"
      },
      {
        "paperId": "8fb04d52334871b831724231570fddb40fbde9d3",
        "title": "Multi-agent query reformulation: Challenges and the role of diversity"
      },
      {
        "paperId": "0b0ff82b0f4f436919b7768f2d1c979308cdb9a0",
        "title": "Adaptive Sample-Efficient Blackbox Optimization via ES-active Subspaces"
      },
      {
        "paperId": "c2f28b786611b67f19dfb91660ce980157e5b3cc",
        "title": "From Complexity to Simplicity: Adaptive ES-Active Subspaces for Blackbox Optimization"
      },
      {
        "paperId": "bd2a9f6926312fb8b28f064812b009621266adcc",
        "title": "Guiding Neuroevolution with Structural Objectives"
      },
      {
        "paperId": "c520bf47db3360ae3a52219771390a354ed8a91f",
        "title": "Go-Explore: a New Approach for Hard-Exploration Problems"
      },
      {
        "paperId": "c48ca266c1e16f9adc5fb7770afd95a0feec8753",
        "title": "Paired Open-Ended Trailblazer (POET): Endlessly Generating Increasingly Complex and Diverse Learning Environments and Their Solutions"
      },
      {
        "paperId": "47105fa6795c58bf45c6428cdd8d2cc15278b20f",
        "title": "Recurrent Control Nets for Deep Reinforcement Learning"
      },
      {
        "paperId": "36cd89332d305d01605d6d08cd8452c8a752138a",
        "title": "Designing neural networks through neuroevolution"
      },
      {
        "paperId": "55a3165c66d40ae7a5f36fbca48fa2b320f908be",
        "title": "Neural Architecture Search Over a Graph Search Space"
      },
      {
        "paperId": "a01aa16ba8f8b9541577e49f54f701df5bfb6105",
        "title": "An Atari Model Zoo for Analyzing, Visualizing, and Comparing Deep Reinforcement Learning Agents"
      },
      {
        "paperId": "cf4a4d18da4aa335cd18e095bdfff21290afebb0",
        "title": "Malthusian Reinforcement Learning"
      },
      {
        "paperId": "a2c36c333d99d4baf46091f0f669139151de29de",
        "title": "PNS: Population-Guided Novelty Search for Reinforcement Learning in Hard Exploration Environments"
      },
      {
        "paperId": "1bd1f81ea6c940bafc4999a99a7616288ba006bd",
        "title": "Evolving intrinsic motivations for altruistic behavior"
      },
      {
        "paperId": "1181d692fbfba96f5c7fe91c5db421aeb48b10b4",
        "title": "Importance Weighted Evolution Strategies"
      },
      {
        "paperId": "f601f077a43056ee66d52e2a78063c81b321e3bc",
        "title": "EA-LSTM: Evolutionary Attention-based LSTM for Time Series Prediction"
      },
      {
        "paperId": "d717ccc5a9fb1c92001bcd6a8485b6722b268522",
        "title": "Transfer Learning versus Multiagent Learning regarding Distributed Decision-Making in Highway Traffic"
      },
      {
        "paperId": "f2ac2a3fd7b341f2b1be752b4dd46ed9abcf0751",
        "title": "Deep Reinforcement Learning"
      },
      {
        "paperId": "1fee1523881ab6f9cdfe4f91c227c39068f87ce5",
        "title": "Numerical optimization using stochastic evolutionary algorithms : application to seismic tomography inverse problems"
      },
      {
        "paperId": "d9e08e872c69e5acfed2a93ec6f6d623cfd0c680",
        "title": "CEM-RL: Combining evolutionary and gradient-based methods for policy search"
      },
      {
        "paperId": "fdfeeb14bbde2ab31b18e56b92d362dcd1b14f71",
        "title": "Episodic Curiosity through Reachability"
      },
      {
        "paperId": "0817ceff5c8819fa3d79bb3489a28547f1e52b1a",
        "title": "Learning to Coordinate Multiple Reinforcement Learning Agents for Diverse Query Reformulation"
      },
      {
        "paperId": "3ff97679a77f8d6f95566345610083a4c3bf4c6f",
        "title": "Evolutionary-Neural Hybrid Agents for Architecture Search"
      },
      {
        "paperId": "3b59124f04e94662243ce34f9e3f23c4c86c864b",
        "title": "Importance mixing: Improving sample reuse in evolutionary policy search methods"
      },
      {
        "paperId": "337315f3d53590bb1b066edd0f4d19fdba30d514",
        "title": "The Evolution of Training Parameters for Spiking Neural Networks with Hebbian Learning"
      },
      {
        "paperId": "787c563657c34041357b5a92354a69b05c435032",
        "title": "Curiosity Driven Exploration of Learned Disentangled Goal Spaces"
      },
      {
        "paperId": "a7dc6bb1ffb17fa9428dc12303a9d762c959fd00",
        "title": "Learning concise representations for regression by evolving networks of trees"
      },
      {
        "paperId": "7455a96b433c38ce5944af4e7a4d4849cf8426c8",
        "title": "Evolving indirectly encoded convolutional neural networks to play tetris with low-level features"
      },
      {
        "paperId": "d05b204cd96f1aaf2fba8861f29bba77dec17c74",
        "title": "Swarm and Evolutionary Computation"
      },
      {
        "paperId": "6201fdf649a54892cce3786dc88ad0d074661243",
        "title": "Playing Atari with Six Neurons"
      },
      {
        "paperId": "5fd3ce235f5fcebd3d2807f710b060add527183b",
        "title": "Deep Curiosity Search: Intra-Life Exploration Improves Performance on Challenging Deep Reinforcement Learning Problems"
      },
      {
        "paperId": "39e401fc872b0f74f4e7f4491fbd646be7ee0cbe",
        "title": "Being curious about the answers to questions: novelty search with learned attention"
      },
      {
        "paperId": "653bdfb3c35621ee04ee5d5253dc7e3a422d69e1",
        "title": "Learning Self-Imitating Diverse Policies"
      },
      {
        "paperId": "9dd606ce1442f8c0ae6764a0e52d049c2779e4c4",
        "title": "Evolutionary Reinforcement Learning"
      },
      {
        "paperId": "e47493b0657ff1396d550a10a975eb42d1e3badb",
        "title": "VINE: an open source interactive data visualization tool for neuroevolution"
      },
      {
        "paperId": "d5805a80b63ed0a605e5469e321a7e3c42eaf324",
        "title": "Evolution-Guided Policy Gradient in Reinforcement Learning"
      },
      {
        "paperId": "fc188954c8392a0f60f0917f942c0f49533d1eb8",
        "title": "Data-efficient neuroevolution with kernel-based surrogate models"
      },
      {
        "paperId": "3bc41f52011ea12a8be48207362b36582950d164",
        "title": "Discovering the elite hypervolume by leveraging interspecies correlation"
      },
      {
        "paperId": "dfb2b26f15466bf3ec34fbd72a22bb9d6ecd42f4",
        "title": "Policy Search in Continuous Action Domains: an Overview"
      },
      {
        "paperId": "8a18517e8d9aab5be3891fe860937003c496550d",
        "title": "Model-Based Stochastic Search for Large Scale Optimization of Multi-Agent UAV Swarms"
      },
      {
        "paperId": "b6cb37e9177f7ace642b23cf4d4210f1fd9d3394",
        "title": "Transfer Learning with Neural AutoML"
      },
      {
        "paperId": "19af387c0ee32929b54ff45656129a721a84c192",
        "title": "Back to Basics: Benchmarking Canonical Evolution Strategies for Playing Atari"
      },
      {
        "paperId": "a6a16d202b0c904ce0cd5b7d208e3d2bc6a42eda",
        "title": "Structured Control Nets for Deep Reinforcement Learning"
      },
      {
        "paperId": "eed8cae46eb28311e88f6fc41f788528ca2d0f00",
        "title": "State Representation Learning for Control: An Overview"
      },
      {
        "paperId": "3c3093f5f8e9601637612fcfb8f160f116fa30e4",
        "title": "Diversity-Driven Exploration Strategy for Deep Reinforcement Learning"
      },
      {
        "paperId": "81a28e2f46115648beb4a2700b2b9a9fca476fbc",
        "title": "ES is more than just a traditional finite-difference approximator"
      },
      {
        "paperId": "819bcae49054e00cef3c0972d48b4e40a525f4d9",
        "title": "Deep Neuroevolution: Genetic Algorithms Are a Competitive Alternative for Training Deep Neural Networks for Reinforcement Learning"
      },
      {
        "paperId": "1f2bc5d57ccbf5a04e7fea87f1f4db464f533ca8",
        "title": "Deep Learning for Video Game Playing"
      },
      {
        "paperId": "1fdf0319b4a1db62c611980351e5f4c2f08958cd",
        "title": "GEP-PG: Decoupling Exploration and Exploitation in Deep Reinforcement Learning Algorithms"
      },
      {
        "paperId": "40eb96507465afeb353dd222ef1a5163a7077664",
        "title": "Gradient Monitored Reinforcement Learning for Jamming Attack Detection in FANETs"
      },
      {
        "paperId": "8fdf267a5f959c42ff81cc29784c9fadcd552c2c",
        "title": "Adaptive Evolutionary Reinforcement Learning Algorithm with Early Termination Strategy"
      },
      {
        "paperId": "d9b45079fb7d7b1031837197fe784756b465c011",
        "title": "Sample-Efficient Quality-Diversity by Cooperative Coevolution"
      },
      {
        "paperId": "560a936cc95d6e08b425d29d6f315b71b13cbd11",
        "title": "Objective-Informed Diversity for Multi-Objective Multiagent Coordination"
      },
      {
        "paperId": "016a9b8cc59b92f5226b2a756f9b575a24f4ea24",
        "title": "Adaptive Noise-based Evolutionary Reinforcement Learning With Maximum Entropy"
      },
      {
        "paperId": "35af40853f48030c5050c1c3191f6e227ceb264b",
        "title": "Diversify and Disambiguate: Out-of-Distribution Robustness via Disagreement"
      },
      {
        "paperId": "98abf6219d648a18292686670668aef7177dc64a",
        "title": "Quality-Similar Diversity via Population Based Reinforcement Learning"
      },
      {
        "paperId": "8a26326c1e9deeb1d9c7b2fe12419c7cfddcdbb5",
        "title": "Game-based Platforms for Artificial Intelligence Research"
      },
      {
        "paperId": "fed0701afdfa6896057f7d04bd30ab1328eff110",
        "title": "Evolutionary Diversity Optimization with Clustering-based Selection for Reinforcement Learning"
      },
      {
        "paperId": "954a161c70418bd6cc313c599eed5c21fa3abffd",
        "title": "Zeroth-Order Actor-Critic"
      },
      {
        "paperId": "78f7b8958eadf4822454bff1958187dac21d6913",
        "title": "Lean Evolutionary Reinforcement Learning by Multitasking with Importance Sampling"
      },
      {
        "paperId": "3875db09dd17fd845741ef4a1653a4703a87512d",
        "title": "I NTRODUCING C OORDINATION IN C ONCURRENT R EIN - FORCEMENT L EARNING"
      },
      {
        "paperId": "06f936df764ea2d0973575be7cce270b31cca544",
        "title": "DSA-ME: D EEP S URROGATE A SSISTED MAP-E LITES"
      },
      {
        "paperId": "cd243f3f20ca0005481f86153b21000c1946e099",
        "title": "Stein Variational Goal Generation For Reinforcement Learning in Hard Exploration Problems"
      },
      {
        "paperId": "6c6cfdaea4d7dd986f95f84acba6ad7d72fff655",
        "title": "A Memory-Related Multi-Task Method Based on Task-Agnostic Exploration"
      },
      {
        "paperId": "d649e6f9234595a42eed80c1eaf08752d7d5ddb8",
        "title": "Scaling Covariance Matrix Adaptation MAP-Annealing to High-Dimensional Controllers"
      },
      {
        "paperId": "bf75d10543875f0fa203b1fd3dd30f7afeac5902",
        "title": "Application of the Hierarchic Memetic Strategy HMS in Neuroevolution"
      },
      {
        "paperId": "90a484a4ca288d7c69a25e999f4158e1a23fa485",
        "title": "ES-ENAS: Blackbox Optimization over Hybrid Spaces via Combinatorial and Continuous Evolution"
      },
      {
        "paperId": "637cb5b797697930d7d1577031134801af99f83d",
        "title": "Coverage as a Principle for Discovering Transferable Behavior in Reinforcement Learning"
      },
      {
        "paperId": "21e2b662d29a8f4e133cf0323ab94394af53f5da",
        "title": "Neural AutoML with Convolutional Networks for Diabetic Retinopathy Diagnosis"
      },
      {
        "paperId": "38588676ec34d8da60e00334a41d664fe952198b",
        "title": "Multiagent Deep Reinforcement Learning: Challenges and Directions Towards Human-Like Approaches"
      },
      {
        "paperId": "ca77fb5411bb49bf625e33f8d7c937d319aecb05",
        "title": "ES-ENAS: BLACKBOX OPTIMIZATION"
      },
      {
        "paperId": "41cb02be82cd8170979c742a233d68f01a02c17c",
        "title": "A uni\ufb01ed view of likelihood ratio and reparameterization gradients:"
      },
      {
        "paperId": "bd09517f02e478f6cab47286aab1da29c3a51d77",
        "title": "Efficient Novelty Search Through Deep Reinforcement Learning"
      },
      {
        "paperId": "a4b6b99a9ce7120aac9aadcab7d60843e58d5f09",
        "title": "Improving Learning to Branch via Reinforcement Learning"
      },
      {
        "paperId": "8a710039389844a70cf4f23a7119500aa900afb3",
        "title": "Deep Hybrid Neural Network and Improved Differential Neuroevolution for Chaotic Time Series Prediction"
      },
      {
        "paperId": "60d933023312ac6f060d5071b2c19c613eacccbf",
        "title": "Development of the heuristic method of evolutionary strategies for reinforcement learning problems solving"
      },
      {
        "paperId": "4db734163d6cbdbd75bd40a40b73cb7cd2946e3e",
        "title": "S AMPLE EFFICIENT Q UALITY D IVERSITY FOR NEURAL CONTINUOUS CONTROL"
      },
      {
        "paperId": "ac3a09c613c5e98f2b3863ca71ba64309a3b6f5d",
        "title": "OLICY G RADIENT WITH P OPULATION - BASED S EARCH (PGPS)"
      },
      {
        "paperId": "eb8a4ac9dcff99fa1aacff4ce85d987272b75b30",
        "title": "Explore, Discover and Learn: Unsupervised Discovery of State-Covering Skills"
      },
      {
        "paperId": "79166876b78ef894b63ac98eaf9bba6fbc09886b",
        "title": "Explore, Discover and Learn: Unsupervised Discovery of State-Covering Skills"
      },
      {
        "paperId": "271fd2f0db9e55950649517227ff82ee375d777d",
        "title": "INTERIOR POLICY DIFFERENTIATION"
      },
      {
        "paperId": "30cf5e4952326f39a936a516ee7afa88450161ef",
        "title": "Opleiding Informatica Psychology-Inspired Memory Sampling in a Deep Q Network"
      },
      {
        "paperId": "07888aa3f15acf421bbe55ac304df95e09928174",
        "title": "Training Neural Networks Through the Integration of Evolution and Gradient Descent"
      },
      {
        "paperId": "72c22ba3ad1002f0bc88ee61d53815dae9f53ea7",
        "title": "Correlated Exploration in Deep Reinforcement Learning"
      },
      {
        "paperId": "770d79c89ca27de2efd143b16a81fe98497820a5",
        "title": "Swarm-inspired Reinforcement Learning via Collaborative Inter-agent Knowledge Distillation"
      },
      {
        "paperId": "728ca14eb85e6dd2d36fe3b300ff043f68fc77af",
        "title": "J un 2 01 8 State Representation Learning for Control : An Overview"
      },
      {
        "paperId": "68b3af56038b361cb5ff4e9036d9946eb2d50642",
        "title": "THE PURDUE UNIVERSITY GRADUATE SCHOOL STATEMENT OF DISSERTATION APPROVAL"
      },
      {
        "paperId": "1b93209238542728082641bac37ceeb7b9a4e8c4",
        "title": "Policy Search in Continuous Action Domains: an Overview"
      },
      {
        "paperId": "c0f0791ec1357e634f2d597fe6faf34e5757609d",
        "title": "Maze Navigation using Neural Networks Evolved with Novelty Search and Differential Evolution"
      },
      {
        "paperId": "d793637525e4ad8902927d18b9ea7b9145e7262d",
        "title": "EVOLUTIONARY-NEURAL HYBRID AGENTS"
      },
      {
        "paperId": "4966cf1e6fa52216b919ef232ba370d7d5f49b9c",
        "title": "New York University at TREC 2018 Complex Answer Retrieval Track"
      },
      {
        "paperId": "5a952c08128c71f39c4508c069778fc1f51f0e19",
        "title": "Autonomous vehicle control: Exploring driver modelling conventions by implementation of neuroevolutionary and knowledge-based algorithms"
      },
      {
        "paperId": "a74fde9c64d5d340e0b984dce82530849b015464",
        "title": "ICAS 2018 Proceedings"
      },
      {
        "paperId": "b18978bd8438b399ebcc68611e69e2e9ede87d83",
        "title": "A Study of the Impact of Evolutionary Strategies on Performance of Reinforcement Learning Autonomous Agents"
      },
      {
        "paperId": "c19114115e9f4fa4e6bc5c924fb0e58cf9ce41b0",
        "title": "Mitigating Against a Succession of Hidden Failure Accelerants Involved in an Insider Threat Sequential Topology Attack on a Smart Grid Devising a Defensive Paradigm via a Bespoke Convolutional Adversarial Neural Network Module and Particle Swarm Optimization-based Enhanced Reinforcement Learning Com"
      },
      {
        "paperId": "abc06cb12e31b416a7e5709e0e5fe3c0056f918c",
        "title": "Multi-Reward Learning and Sparse Rewards"
      }
    ],
    "score": 43.875
  },
  {
    "id": "1c35807e1a4c24e2013fa0a090cee9cc4716a5f5",
    "title": "Reinforcement Learning with Action-Free Pre-Training from Videos",
    "authors": [
      "Younggyo Seo",
      "Kimin Lee",
      "Stephen James",
      "P. Abbeel"
    ],
    "year": 2022,
    "citationCount": 129,
    "abstract": "Recent unsupervised pre-training methods have shown to be effective on language and vision domains by learning useful representations for multiple downstream tasks. In this paper, we investigate if such unsupervised pre-training methods can also be effective for vision-based reinforcement learning (RL). To this end, we introduce a framework that learns representations useful for understanding the dynamics via generative pre-training on videos. Our framework consists of two phases: we pre-train an action-free latent video prediction model, and then utilize the pre-trained representations for efficiently learning action-conditional world models on unseen environments. To incorporate additional action inputs during fine-tuning, we introduce a new architecture that stacks an action-conditional latent prediction model on top of the pre-trained action-free prediction model. Moreover, for better exploration, we propose a video-based intrinsic bonus that leverages pre-trained representations. We demonstrate that our framework significantly improves both final performances and sample-efficiency of vision-based RL in a variety of manipulation and locomotion tasks. Code is available at https://github.com/younggyoseo/apv.",
    "url": "https://www.semanticscholar.org/paper/1c35807e1a4c24e2013fa0a090cee9cc4716a5f5",
    "pdf_url": "https://arxiv.org/pdf/2203.13880.pdf",
    "venue": "International Conference on Machine Learning",
    "publicationDate": "2022-03-25",
    "externalIds": {
      "DBLP": "journals/corr/abs-2203-13880",
      "ArXiv": "2203.13880",
      "DOI": "10.48550/arXiv.2203.13880",
      "CorpusId": 247762941
    },
    "references": [
      {
        "paperId": "7241e08610d2b8a567dabb3dc6f4ad53437af7b1",
        "title": "HARP: Autoregressive Latent Video Prediction with High-Fidelity Image Generator"
      },
      {
        "paperId": "1e8c1cf415dcacc3f621697fa28666e618122f6e",
        "title": "Patch-based Object-centric Transformers for Efficient Video Generation"
      },
      {
        "paperId": "523acd658742fb9c978e3f7638c09d7ce78af719",
        "title": "Masked Visual Pre-training for Motor Control"
      },
      {
        "paperId": "2fec20377bc947ec1df003b4aedcb4d7f25ac934",
        "title": "TransDreamer: Reinforcement Learning with Transformer World Models"
      },
      {
        "paperId": "3ba64ff17dbde936353660b3fcb8eab48f086e9e",
        "title": "Mask-based Latent Reconstruction for Reinforcement Learning"
      },
      {
        "paperId": "6351ebb4a3287f5f3e1273464b3b91e5df5a16d7",
        "title": "Masked Autoencoders Are Scalable Vision Learners"
      },
      {
        "paperId": "9317736e9bb1b25c9d8e7325b7b364b7fbae0f3f",
        "title": "URLB: Unsupervised Reinforcement Learning Benchmark"
      },
      {
        "paperId": "558ca2e8c7eb56edd77a52b084e6cc24dffe5bcd",
        "title": "Deep Reinforcement Learning at the Edge of the Statistical Precipice"
      },
      {
        "paperId": "e06c005e98281af455c454ce2478285f6f3afeca",
        "title": "Mastering Visual Continuous Control: Improved Data-Augmented Reinforcement Learning"
      },
      {
        "paperId": "aa5e7fffefb7f53fb6f7e937b5976584d001c906",
        "title": "FitVid: Overfitting in Pixel-Level Video Prediction"
      },
      {
        "paperId": "685ccd3d1ce5afc0dbf06d3f34624dc069f3e044",
        "title": "Model-Based Reinforcement Learning via Latent-Space Collocation"
      },
      {
        "paperId": "0f1382cb004b4834cc3ca7824a61d0d6b86a5763",
        "title": "Pretraining Representations for Data-Efficient Reinforcement Learning"
      },
      {
        "paperId": "d54112833080d52be2d4cd43676f6a751470b90b",
        "title": "PlayVirtual: Augmenting Cycle-Consistent Virtual Trajectories for Reinforcement Learning"
      },
      {
        "paperId": "61ff6872fa07788acdbe85b1319554a5fb9ce0db",
        "title": "XIRL: Cross-embodiment Inverse Reinforcement Learning"
      },
      {
        "paperId": "02f316857c1d20649b18e5fec3e92dda8ef1d0a0",
        "title": "DriveGAN: Towards a Controllable High-Quality Neural Simulation"
      },
      {
        "paperId": "c64025f83864ec9c40e2970a24314b6b84d4c753",
        "title": "GODIVA: Generating Open-DomaIn Videos from nAtural Descriptions"
      },
      {
        "paperId": "2d9ae4c167510ed78803735fc57ea67c3cc55a35",
        "title": "VideoGPT: Video Generation using VQ-VAE and Transformers"
      },
      {
        "paperId": "3e85d208b1b927fdb69ecf8336c70995818aaebd",
        "title": "MT-Opt: Continuous Multi-Task Robotic Reinforcement Learning at Scale"
      },
      {
        "paperId": "b3efeb1a8e44b4ba4476d3e7bf83cf0cb9682291",
        "title": "Learning Generalizable Robotic Reward Functions from \"In-The-Wild\" Human Videos"
      },
      {
        "paperId": "0295df1b9d11e6e49b20119410fd755a4d7781af",
        "title": "Behavior From the Void: Unsupervised Active Pre-Training"
      },
      {
        "paperId": "a50f7bcf8a998f3de11bc085b0f4dea32be19783",
        "title": "Reinforcement Learning with Prototypical Representations"
      },
      {
        "paperId": "7fc04f8031598da6510bce4a7e5b4722305ca5b0",
        "title": "State Entropy Maximization with Random Encoders for Efficient Exploration"
      },
      {
        "paperId": "5c9815f7d907b79b088cdf49665be9b8dc89e5e7",
        "title": "A Framework for Efficient Robotic Manipulation"
      },
      {
        "paperId": "7d99f5dc92678f61576427f92adba9bef43dfd65",
        "title": "Reinforcement Learning with Videos: Combining Offline Observations with Interaction"
      },
      {
        "paperId": "b44bb1762640ed72091fd5f5fdc20719a6dc24af",
        "title": "Mastering Atari with Discrete World Models"
      },
      {
        "paperId": "17985b57240bfaea02a6098a7a34e71e780180eb",
        "title": "Decoupling Representation Learning from Reinforcement Learning"
      },
      {
        "paperId": "7c4356ec0dca6e6df0af7a882e2cd1571c8bf3dc",
        "title": "Data-Efficient Reinforcement Learning with Self-Predictive Representations"
      },
      {
        "paperId": "8ba600c169f0d2422625822223976bce562eabe1",
        "title": "dm_control: Software and Tasks for Continuous Control"
      },
      {
        "paperId": "518b827e340c26582b5093401283a4f5cff605b9",
        "title": "Learning Invariant Representations for Reinforcement Learning without Reconstruction"
      },
      {
        "paperId": "065ed97f8296036fa059e95523ed11d97d49dd48",
        "title": "Semantic Visual Navigation by Watching YouTube Videos"
      },
      {
        "paperId": "27a7df880e9c4ccd87cb88cccb131e2b4687567f",
        "title": "Deep Reinforcement and InfoMax Learning"
      },
      {
        "paperId": "28e4fa2cb562816542e90d264f4b69f341eccf38",
        "title": "Learning to Simulate Dynamic Environments With GameGAN"
      },
      {
        "paperId": "3a71c306eb6232658c9e5fd48aed1ef3befe5fbe",
        "title": "Planning to Explore via Self-Supervised World Models"
      },
      {
        "paperId": "744139d65c3bf6da6a6acd384a32d94a06f44f62",
        "title": "Reinforcement Learning with Augmented Data"
      },
      {
        "paperId": "9efb64f20ab1f157ca9f4050d4aaacf6c3f9b2b2",
        "title": "CURL: Contrastive Unsupervised Representations for Reinforcement Learning"
      },
      {
        "paperId": "7891b414b353a88b96a4a0e14ccfbea8b16c8d39",
        "title": "Transformation-based Adversarial Video Prediction on Large-Scale Data"
      },
      {
        "paperId": "95e96506dd17ef781f4a31dfb36b73bd53b9e499",
        "title": "Stochastic Latent Residual Video Prediction"
      },
      {
        "paperId": "43f2ad297941db230c089ba353efc3f281ab678c",
        "title": "5\u5206\u3067\u5206\u304b\u308b!? \u6709\u540d\u8ad6\u6587\u30ca\u30ca\u30e1\u8aad\u307f\uff1aJacob Devlin et al. : BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding"
      },
      {
        "paperId": "7af72a461ed7cda180e7eab878efd5f35d79bbf4",
        "title": "A Simple Framework for Contrastive Learning of Visual Representations"
      },
      {
        "paperId": "b19729b27a1b4c24b52f87308c907653300afa7f",
        "title": "Dota 2 with Large Scale Deep Reinforcement Learning"
      },
      {
        "paperId": "0cc956565c7d249d4197eeb1dbab6523c648b2c9",
        "title": "Dream to Control: Learning Behaviors by Latent Imagination"
      },
      {
        "paperId": "124ec2c95bc0fe417be2bb0d0701f88583d6a16a",
        "title": "Learning Predictive Models From Observation and Interaction"
      },
      {
        "paperId": "3a427c79721676a473a57c0ba112c904d9cefb7c",
        "title": "Scalable methods for computing state similarity in deterministic Markov Decision Processes"
      },
      {
        "paperId": "add2f205338d70e10ce5e686df4a690e2851bdfc",
        "title": "Momentum Contrast for Unsupervised Visual Representation Learning"
      },
      {
        "paperId": "aec380c44646a7e467cd9f6d78cba301f877734c",
        "title": "High Fidelity Video Prediction with Large Stochastic Recurrent Neural Networks"
      },
      {
        "paperId": "361c00b22e29d0816ca896513d2c165e26399821",
        "title": "Grandmaster level in StarCraft II using multi-agent reinforcement learning"
      },
      {
        "paperId": "0bc855f84668b35cb65618d996d09f6e434d28c9",
        "title": "Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning"
      },
      {
        "paperId": "3e519d85cdcefdd1d2ad89829d6ad445695d8c58",
        "title": "RoboNet: Large-Scale Multi-Robot Learning"
      },
      {
        "paperId": "320b227027030fc291de2896fc3c6da49d7614be",
        "title": "Solving Rubik's Cube with a Robot Hand"
      },
      {
        "paperId": "a2fdfda785b3a2a0178d174daa515377c531f222",
        "title": "RLBench: The Robot Learning Benchmark & Learning Environment"
      },
      {
        "paperId": "88dd6594c9ddd4c4bb7f9b407b162e283907f4f3",
        "title": "Improving Sample Efficiency in Model-Free Reinforcement Learning from Images"
      },
      {
        "paperId": "12c37cb419121cdb43f2c6620303932f43e2e1b7",
        "title": "Adversarial Video Generation on Complex Datasets"
      },
      {
        "paperId": "e0c6abdbdecf04ffac65c440da77fb9d66bb474c",
        "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding"
      },
      {
        "paperId": "2e73ba5c098e5207aaecf159b6e6620b3f2ea56e",
        "title": "Unsupervised State Representation Learning in Atari"
      },
      {
        "paperId": "cfec54f9208dcbb2c558aa7ff45f57be2533d046",
        "title": "Efficient Exploration via State Marginal Matching"
      },
      {
        "paperId": "e763fdc9ae56826ff799163ea035b29bffd8ea6f",
        "title": "Scaling Autoregressive Video Models"
      },
      {
        "paperId": "f373bccc69bec811fa93b27d59a560b9e4ed0946",
        "title": "Predicting Future Frames Using Retrospective Cycle GAN"
      },
      {
        "paperId": "188dac491f04c56e1eb7d7b33ac6aa0b87303232",
        "title": "DeepMDP: Learning Continuous Latent Space Models for Representation Learning"
      },
      {
        "paperId": "87a40a2d506fec06d67dc8f14ce2c708a789a2b4",
        "title": "Self-Supervised Exploration via Disagreement"
      },
      {
        "paperId": "1fd4694e7c2d9c872a427d50e81b5475056de6bc",
        "title": "Model-Based Reinforcement Learning for Atari"
      },
      {
        "paperId": "adab275554eb745cdc21fd6c526ec55a5b2ef362",
        "title": "Provably Efficient Maximum Entropy Exploration"
      },
      {
        "paperId": "fea3e63c97c7292dc6fbcb3ffe7131eb54053986",
        "title": "Learning Latent Dynamics for Planning from Pixels"
      },
      {
        "paperId": "b6cad0b0ffd5d7b5ec1db60173c194ae2a9ec947",
        "title": "FUTUREGAN: ANTICIPATING THE FUTURE FRAMES OF VIDEO SEQUENCES USING SPATIO-TEMPORAL 3D CONVOLUTIONS IN PROGRESSIVELY GROWING GANS"
      },
      {
        "paperId": "4cb3fd057949624aa4f0bbe7a6dcc8777ff04758",
        "title": "Exploration by Random Network Distillation"
      },
      {
        "paperId": "ee9893ff2aa325ff3c9920f247436c514fd8b512",
        "title": "SOLAR: Deep Structured Representations for Model-Based Reinforcement Learning"
      },
      {
        "paperId": "93adca9ce6f4a0fab9ea027c90b4df828cfa10d7",
        "title": "Learning Actionable Representations from Visual Observations"
      },
      {
        "paperId": "1f1f8330cddf1f4bf7bd73478223e5c02b69a1ff",
        "title": "Generative Adversarial Imitation from Observation"
      },
      {
        "paperId": "b227f3e4c0dc96e5ac5426b85485a70f2175a205",
        "title": "Representation Learning with Contrastive Predictive Coding"
      },
      {
        "paperId": "5ec868ebe59918f94140bb2889b9027c55c09b65",
        "title": "Video Prediction with Appearance and Motion Conditions"
      },
      {
        "paperId": "705bbc4dcd475f9230863771da6596e1f677a92d",
        "title": "Playing hard exploration games by watching YouTube"
      },
      {
        "paperId": "395ea8a62d84c8dd85a8dadfc3043cf2228e38c5",
        "title": "Imitating Latent Policies from Observation"
      },
      {
        "paperId": "9f67271a1edea3bf80c64c2d54e2a0a57612a567",
        "title": "Stochastic Adversarial Video Prediction"
      },
      {
        "paperId": "de3b9eb697feed3d097e3f671afe395f48c1ab76",
        "title": "Stochastic Video Generation with a Learned Prior"
      },
      {
        "paperId": "aab368284210c1bb917ec2d31b84588e3d2d7eb4",
        "title": "Unsupervised Representation Learning by Predicting Image Rotations"
      },
      {
        "paperId": "811df72e210e20de99719539505da54762a11c6d",
        "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor"
      },
      {
        "paperId": "59d86da5c5936e7a236678bf5eaaa7753c226fb1",
        "title": "Stochastic Variational Video Prediction"
      },
      {
        "paperId": "c27db32efa8137cbf654902f8f728f338e55cd1c",
        "title": "Mastering the game of Go without human knowledge"
      },
      {
        "paperId": "77fa0239b9b074e7b62ca3798b8abf6fa3823f80",
        "title": "Imitation from Observation: Learning to Imitate Behaviors from Raw Video via Context Translation"
      },
      {
        "paperId": "b68811a9b5cafe4795a11c1048541750068b7ad0",
        "title": "The \u201cSomething Something\u201d Video Database for Learning and Evaluating Visual Common Sense"
      },
      {
        "paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
        "title": "Attention is All you Need"
      },
      {
        "paperId": "225ab689f41cef1dc18237ef5dab059a49950abf",
        "title": "Curiosity-Driven Exploration by Self-Supervised Prediction"
      },
      {
        "paperId": "2adae2da173b9dd720c8bcac0250a90a7f1ec697",
        "title": "Time-Contrastive Networks: Self-Supervised Learning from Video"
      },
      {
        "paperId": "32ceb28e45a445df4d89df281bb0e3ab5aab1a2a",
        "title": "Domain randomization for transferring deep neural networks from simulation to the real world"
      },
      {
        "paperId": "2d1b8f60f2724efd6c9344870fb60e8525157d70",
        "title": "Parallel Multiscale Autoregressive Density Estimation"
      },
      {
        "paperId": "12f67fb182bc934fc95ce97acff553d83e2ca72e",
        "title": "Count-Based Exploration with Neural Density Models"
      },
      {
        "paperId": "0fcb2034e31a2bc2f12a2b1363d0d77baf445fdf",
        "title": "#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning"
      },
      {
        "paperId": "d7bd6e3addd8bc8e2e154048300eea15f030ed33",
        "title": "Reinforcement Learning with Unsupervised Auxiliary Tasks"
      },
      {
        "paperId": "b01871c114b122340209562972ff515b86b16ccf",
        "title": "Video Pixel Networks"
      },
      {
        "paperId": "ee091ccf24c4f053c5c3dfbefe4a7975ed3447c1",
        "title": "Generating Videos with Scene Dynamics"
      },
      {
        "paperId": "6e90fd78e8a3b98af3954aae5209703aa966603e",
        "title": "Unifying Count-Based Exploration and Intrinsic Motivation"
      },
      {
        "paperId": "4954fa180728932959997a4768411ff9136aac81",
        "title": "TensorFlow: A system for large-scale machine learning"
      },
      {
        "paperId": "ad367b44f3434b9ba6b46b41ab083210f6827a9f",
        "title": "Deep Predictive Coding Networks for Video Prediction and Unsupervised Learning"
      },
      {
        "paperId": "f110cfdbe9ded7a384bcf5c0d56e536bd275a7eb",
        "title": "Unsupervised Learning for Physical Interaction through Video Prediction"
      },
      {
        "paperId": "317cd4522b1f4a6f889743578143bb8823623f8b",
        "title": "VIME: Variational Information Maximizing Exploration"
      },
      {
        "paperId": "2ec8f7e0257a07d3914322b36072d1bbcd58a1e0",
        "title": "Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles"
      },
      {
        "paperId": "c8f41160130980c1ffead5a812cf2b3c6b03049f",
        "title": "Deep spatial autoencoders for visuomotor learning"
      },
      {
        "paperId": "e4257bc131c36504a04382290cbc27ca8bb27813",
        "title": "Action-Conditional Video Prediction using Deep Networks in Atari Games"
      },
      {
        "paperId": "d316c82c12cf4c45f9e85211ef3d1fa62497bff8",
        "title": "High-Dimensional Continuous Control Using Generalized Advantage Estimation"
      },
      {
        "paperId": "b6b8a1b80891c96c28cc6340267b58186157e536",
        "title": "End-to-End Training of Deep Visuomotor Policies"
      },
      {
        "paperId": "340f48901f72278f6bf78a04ee5b01df208cc508",
        "title": "Human-level control through deep reinforcement learning"
      },
      {
        "paperId": "829510ad6f975c939d589eeb01a3cf6fc6c8ce4d",
        "title": "Unsupervised Learning of Video Representations using LSTMs"
      },
      {
        "paperId": "355f98e4827a1b6ad3f29d07ea2bcf9ad078295c",
        "title": "Video (language) modeling: a baseline for generative models of natural videos"
      },
      {
        "paperId": "544b80ef13bb0d4288c6a1f50c504c03d3a14d37",
        "title": "Modeling Deep Temporal Dependencies with Recurrent \"Grammar Cells\""
      },
      {
        "paperId": "f37e1b62a767a307c046404ca96bc140b3e68cb5",
        "title": "GloVe: Global Vectors for Word Representation"
      },
      {
        "paperId": "5f5dc5b9a2ba710937e2c413b37b053cd673df02",
        "title": "Auto-Encoding Variational Bayes"
      },
      {
        "paperId": "62c76ca0b2790c34e85ba1cce09d47be317c7235",
        "title": "Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation"
      },
      {
        "paperId": "f6b51c8753a871dc94ff32152c00c01e94f90f09",
        "title": "Efficient Estimation of Word Representations in Vector Space"
      },
      {
        "paperId": "3c58166098c07f2efe30651446a0f4f19b9b7ce9",
        "title": "Random projection in dimensionality reduction: applications to image and text data"
      },
      {
        "paperId": "f2a2401a35b6b892d43642b31700e83e88b2ebb8",
        "title": "SURF: Semi-supervised Reward Learning with Data Augmentation for Feedback-efficient Preference-based Reinforcement Learning"
      },
      {
        "paperId": null,
        "title": "2016a), which would be useful for various applications, e.g., control with world models (Hafner et al., 2019; Kaiser et al., 2019; Rybkin et al., 2021), and simulator development"
      },
      {
        "paperId": null,
        "title": "Yarats et al., 2021b). Our framework differs in that we explicitly consider a sequence of future model states for computing the intrinsic bonus, while previous works only consider a single state"
      },
      {
        "paperId": null,
        "title": "with Action-Free Pre-Training from Videos a robot videos"
      },
      {
        "paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035",
        "title": "Improving Language Understanding by Generative Pre-Training"
      },
      {
        "paperId": "c68796f833a7151f0a63d1d1608dc902b4fdc9b6",
        "title": "GENERATIVE ADVERSARIAL NETS"
      },
      {
        "paperId": "1c200fd0057b32e1c67084999d2912f3dec6e1cd",
        "title": "DARLA: Improving Zero-Shot Transfer in Reinforcement Learning"
      },
      {
        "paperId": null,
        "title": "Extended Related Work Video prediction. A line of works close to our work is video prediction methods, that aims to predict the future frames conditioned on images"
      },
      {
        "paperId": "2a65434d43ffa6554eaf14b728780919ad4f33eb",
        "title": "Modeling Purposeful Adaptive Behavior with the Principle of Maximum Causal Entropy"
      },
      {
        "paperId": "1c46943103bd7b7a2c7be86859995a4144d1938b",
        "title": "Visualizing Data using t-SNE"
      },
      {
        "paperId": "97efafdb4a3942ab3efba53ded7413199f79c054",
        "title": "Reinforcement Learning: An Introduction"
      }
    ],
    "cited_by": [
      {
        "paperId": "eb2a5bc28b87955ad0fa8266a29565be7b7b7431",
        "title": "MultiModal Action Conditioned Video Generation"
      },
      {
        "paperId": "27332c540b16f454e4f37a043b4ce521433cbeb1",
        "title": "SAMPO:Scale-wise Autoregression with Motion PrOmpt for generative world models"
      },
      {
        "paperId": "fc04c23e088d2339c3fe9bb7d4839fc4c2a2c2a8",
        "title": "DisPIM: Distilling PreTrained Image Models for Generalizable Visuo-Motor Control"
      },
      {
        "paperId": "6aa422169a5667d664248304d50a914a68429fa6",
        "title": "Ag2x2: Robust Agent-Agnostic Visual Representations for Zero-Shot Bimanual Manipulation"
      },
      {
        "paperId": "9c2c1a3e1e9d0b3b14c3a8975ca1d09f6c438b23",
        "title": "ManiGaussian++: General Robotic Bimanual Manipulation with Hierarchical Gaussian World Model"
      },
      {
        "paperId": "bfaefd0879622ac96619ed236d9ad11abfcc7fcf",
        "title": "Flattening Hierarchies with Policy Bootstrapping"
      },
      {
        "paperId": "9da58a29f476f560dda373976e820a369fba9568",
        "title": "Temporal Distance-aware Transition Augmentation for Offline Model-based Reinforcement Learning"
      },
      {
        "paperId": "ad258b6e72e9c70418dd7c58026d4d0cf66a5fca",
        "title": "Video-Enhanced Offline Reinforcement Learning: A Model-Based Approach"
      },
      {
        "paperId": "bc6016c22b3ccdc838adaf0963d04bdc4ec654b4",
        "title": "Controlling Large Language Model with Latent Actions"
      },
      {
        "paperId": "17b69015d4724f0b0120786b59da9cebed17748a",
        "title": "Offline Action-Free Learning of Ex-BMDPs by Comparing Diverse Datasets"
      },
      {
        "paperId": "1190c204564459b46171e2e179816deb40320769",
        "title": "AdaWorld: Learning Adaptable World Models with Latent Actions"
      },
      {
        "paperId": "423c6f1180ed65500daabe6f5998300d1bae2293",
        "title": "LuciBot: Automated Robot Policy Learning from Generated Videos"
      },
      {
        "paperId": "0aefae99e10281f7f64e7100b8da572ba4351d82",
        "title": "Disentangled World Models: Learning to Transfer Semantic Knowledge from Distracting Videos for Reinforcement Learning"
      },
      {
        "paperId": "91af851f674aaa6064827527a3f40c733e51f742",
        "title": "Generative Artificial Intelligence in Robotic Manipulation: A Survey"
      },
      {
        "paperId": "696188ba6b12122ba8b154dbeac7917097c013d6",
        "title": "Efficient Reinforcement Learning by Guiding Generalist World Models with Non-Curated Data"
      },
      {
        "paperId": "157ae9fe598f1f5c0ae60d286bae2712bf360d93",
        "title": "Trajectory World Models for Heterogeneous Environments"
      },
      {
        "paperId": "6393a8e8f8ac3e85a8ea986fdf7241bcb5d5dd47",
        "title": "Towards General-Purpose Model-Free Reinforcement Learning"
      },
      {
        "paperId": "efe02465fea19b48cdf610d1c6206b520d4cf305",
        "title": "GLAM: Global-Local Variation Awareness in Mamba-based World Model"
      },
      {
        "paperId": "ecb06833bb161a0eedfac6056583bebe407c4238",
        "title": "Bullet-Screen-Emoji Attack With Temporal Difference Noise for Video Action Recognition"
      },
      {
        "paperId": "6f04d0d52bca0f9647d78dcbcae597f6437c0efa",
        "title": "Sample-Efficient Unsupervised Policy Cloning from Ensemble Self-Supervised Labeled Videos"
      },
      {
        "paperId": "43e70db1d549deccd0392edb991989dcd3317443",
        "title": "Reinforcement Learning from Wild Animal Videos"
      },
      {
        "paperId": "edd6ab58eebeb551130a127ea34b265b27d22da6",
        "title": "Tra-MoE: Learning Trajectory Prediction Model from Multiple Domains for Adaptive Policy Conditioning"
      },
      {
        "paperId": "d3f4c938b5a31f82402ec838fc3f2458860f6f4b",
        "title": "The Surprising Ineffectiveness of Pre-Trained Visual Representations for Model-Based Reinforcement Learning"
      },
      {
        "paperId": "1786e5d90ef797a9f71a5fa53eeda9fc8d5a426d",
        "title": "Grounding Video Models to Actions through Goal Conditioned Exploration"
      },
      {
        "paperId": "88b82a07abd62247a6fe2ca98382b2b86d174b4a",
        "title": "Pre-trained Visual Dynamics Representations for Efficient Policy Learning"
      },
      {
        "paperId": "b718711020853736dd1020e2101aeae6a3a963fa",
        "title": "On-Robot Reinforcement Learning with Goal-Contrastive Rewards"
      },
      {
        "paperId": "d84d3c0bf595f4523afe95c585a10b7b70484c3f",
        "title": "Introducing an Adapter into the Pre-trained Encoder for Visual Reinforcement Learning"
      },
      {
        "paperId": "c5c525d6bedc7f39e887182584e2a032589ea9cc",
        "title": "DrivingDojo Dataset: Advancing Interactive and Knowledge-Enriched Driving World Model"
      },
      {
        "paperId": "8791e284a34aec22bd729648fcaee1b031da4e37",
        "title": "Imitation Learning with Limited Actions via Diffusion Planners and Deep Koopman Controllers"
      },
      {
        "paperId": "0e3b0a91d5068b4a6aea144d1dde1ddff3845654",
        "title": "Diffusion Imitation from Observation"
      },
      {
        "paperId": "0082995dd6c58da9566db9e0eb11b16e0b3d49ed",
        "title": "Open-World Reinforcement Learning over Long Short-Term Imagination"
      },
      {
        "paperId": "c70bb9721ae75d650832bfe609c412165332fd5a",
        "title": "AVID: Adapting Video Diffusion Models to World Models"
      },
      {
        "paperId": "cf3b71fe73e7b61ab807a92be7156c3075b2482f",
        "title": "DynaMo: In-Domain Dynamics Pretraining for Visuo-Motor Control"
      },
      {
        "paperId": "da99c68367f3892231e219abd01f46a4210a9a56",
        "title": "Unsupervised-to-Online Reinforcement Learning"
      },
      {
        "paperId": "29ed78ea73d88dbc2c7f89368bd9e8db7882913a",
        "title": "NOLO: Navigate Only Look Once"
      },
      {
        "paperId": "25908892225280f00bbef3fe555e2683ed50fd72",
        "title": "Continuous Control with Coarse-to-fine Reinforcement Learning"
      },
      {
        "paperId": "c18768fc623a57b54a5cccbc7a5b1c0805ecf36b",
        "title": "Hybrid Reinforcement Learning from Offline Observation Alone"
      },
      {
        "paperId": "3dda11162315b56aed083467b6aa3988981f5d75",
        "title": "ASIMO: Agent-centric scene representation in multi-object manipulation"
      },
      {
        "paperId": "7f1b7dff5330f57eb9587fd0c7f0699acb12d5a5",
        "title": "Investigating Pre-Training Objectives for Generalization in Vision-Based Reinforcement Learning"
      },
      {
        "paperId": "3decf96941423923b4da076f915cb9f3bce18448",
        "title": "BWArea Model: Learning World Model, Inverse Dynamics, and Policy for Controllable Language Generation"
      },
      {
        "paperId": "2667a5c50bdb924b46d2e195c92f1047870c1b41",
        "title": "iVideoGPT: Interactive VideoGPTs are Scalable World Models"
      },
      {
        "paperId": "8c708714e8bb186a1fb374c8cb402642c56c9311",
        "title": "Masked Visual-Tactile Pre-training for Robot Manipulation"
      },
      {
        "paperId": "d6c5f48d5e7ea58f02e94546f5e75ddde563c49d",
        "title": "Robotic Offline RL from Internet Videos via Value-Function Learning"
      },
      {
        "paperId": "a8ad39fc162c238b5c126a2d350d00dd7ab1ba87",
        "title": "Overcoming Knowledge Barriers: Online Imitation Learning from Visual Observation with Pretrained World Models"
      },
      {
        "paperId": "607bb336a1577aba9d4b9c1c8bdb7d60ddd03749",
        "title": "Automated construction of cognitive maps with visual predictive coding"
      },
      {
        "paperId": "c44471e846846bde281779405a3b5c132fd60b00",
        "title": "Survey on Large Language Model-Enhanced Reinforcement Learning: Concept, Taxonomy, and Methods"
      },
      {
        "paperId": "1df731e254be9c0122a1055629a2df561b4ad5c0",
        "title": "ManiGaussian: Dynamic Gaussian Splatting for Multi-task Robotic Manipulation"
      },
      {
        "paperId": "baad600fc95596b37297a10ababdea7c721a5daa",
        "title": "Generalising Multi-Agent Cooperation through Task-Agnostic Communication"
      },
      {
        "paperId": "e947ee36decc18e316c0282487df2fb1425b6f64",
        "title": "Spatiotemporal Predictive Pre-training for Robotic Motor Control"
      },
      {
        "paperId": "e7fc1b57dd032a8697902c01a9a715398b83a930",
        "title": "DecisionNCE: Embodied Multimodal Representations via Implicit Preference Learning"
      },
      {
        "paperId": "c4e31cad004cb6eda60dd3e042f9b7d60ddee6ea",
        "title": "Foundation Policies with Hilbert Representations"
      },
      {
        "paperId": "e4b072842394531f2822164690d32e1fa11d8c81",
        "title": "Learning an Actionable Discrete Diffusion Policy via Large-Scale Actionless Video Pre-Training"
      },
      {
        "paperId": "96910d8365709393ea33630e200f6229fe947178",
        "title": "The Essential Role of Causality in Foundation World Models for Embodied AI"
      },
      {
        "paperId": "40c52f7a1df09143f7cc4d686e3d78e9e0f77f21",
        "title": "Fine-tuning Reinforcement Learning Models is Secretly a Forgetting Mitigation Problem"
      },
      {
        "paperId": "6a504c1a94139791854bc9a68ee1d3ebb925e642",
        "title": "Any-point Trajectory Modeling for Policy Learning"
      },
      {
        "paperId": "8367a694ac6a3f003e79c955364b7e8d2cb223de",
        "title": "Prospective Role of Foundation Models in Advancing Autonomous Vehicles"
      },
      {
        "paperId": "7e04fd1ba5f35ed6f6270553ca37c417dfa945cb",
        "title": "Action Inference by Maximising Evidence: Zero-Shot Imitation from Observation with World Models"
      },
      {
        "paperId": "6a656190e574de769b8d255732d2dc843c41a9ea",
        "title": "Tracking Object Positions in Reinforcement Learning: A Metric for Keypoint Detection (extended version)"
      },
      {
        "paperId": "9179cbfd829a3828d0d3d40d0d16f05909bef8f1",
        "title": "Part-Guided 3D RL for Sim2Real Articulated Object Manipulation"
      },
      {
        "paperId": "2892b19b1ac3c3e7d1bb95ceb39b2d4273b2939b",
        "title": "Learning to Discover Skills through Guidance"
      },
      {
        "paperId": "d6bd400073090b88ea535a6166ca9c164b8015b7",
        "title": "Zero-Shot Robotic Manipulation with Pretrained Image-Editing Diffusion Models"
      },
      {
        "paperId": "c3d14e7a319ab764297a60112ce74af201762a73",
        "title": "Learning Interactive Real-World Simulators"
      },
      {
        "paperId": "364efc2b66200171efd133e2a8cddd7b8fccb5f9",
        "title": "Human-oriented Representation Learning for Robotic Manipulation"
      },
      {
        "paperId": "f218a95e583841c6f668e360a90739d7e4f4610b",
        "title": "HarmonyDream: Task Harmonization Inside World Models"
      },
      {
        "paperId": "b15769e20f2a07a0f53a4a4b877e38e9c57eb96e",
        "title": "Robotic Offline RL from Internet Videos via Value-Function Pre-Training"
      },
      {
        "paperId": "b435bd3fd2c855a755061a5ace42fb444d4f81a7",
        "title": "Guide Your Agent with Adaptive Multimodal Rewards"
      },
      {
        "paperId": "b65cd9857c72c917eec19a0514cad9885ee7620a",
        "title": "Transformer Decoder-Based Enhanced Exploration Method to Alleviate Initial Exploration Problems in Reinforcement Learning"
      },
      {
        "paperId": "ff0e870f4a53e99da65762dbaf4aa426c4e29860",
        "title": "Automated mapping of virtual environments with visual predictive coding"
      },
      {
        "paperId": "f18587247e4769ad0efd96a0286b012d856ba214",
        "title": "HIQL: Offline Goal-Conditioned RL with Latent States as Actions"
      },
      {
        "paperId": "8502a401690c7d048cd516eb102105b1fc5d56d1",
        "title": "Is Imitation All You Need? Generalized Decision-Making with Dual-Phase Training"
      },
      {
        "paperId": "93480449b43a801c47f34fd89a312d58fa1c42a2",
        "title": "SeMAIL: Eliminating Distractors in Visual Imitation via Separated Models"
      },
      {
        "paperId": "eb989c7ad4d9b0c5b8b32d0ae4a92b14b09b4fe8",
        "title": "PLASTIC: Improving Input and Label Plasticity for Sample Efficient Reinforcement Learning"
      },
      {
        "paperId": "765a59b5267616a24069946afefc8c05bda6460a",
        "title": "LAGOON: Language-Guided Motion Control"
      },
      {
        "paperId": "1c73d712dd614d2eebf06f55229348f8e8b46b47",
        "title": "On the Importance of Feature Decorrelation for Unsupervised Representation Learning in Reinforcement Learning"
      },
      {
        "paperId": "6a9ff71938196854e15e9e013dcc8b40d3dc51f3",
        "title": "Model-Based Reinforcement Learning with Multi-task Offline Pretraining"
      },
      {
        "paperId": "b9e4ea7ff34a304cc0e7bfaa638eaa850391b676",
        "title": "Accelerating Reinforcement Learning with Value-Conditional State Entropy Exploration"
      },
      {
        "paperId": "9314fa28ccb7308367efe704241944baf66534f4",
        "title": "Pre-training Contextualized World Models with In-the-wild Videos for Reinforcement Learning"
      },
      {
        "paperId": "64a98f9c40ef559117324e6d92417e8f5173b694",
        "title": "Inverse Dynamics Pretraining Learns Good Representations for Multitask Imitation"
      },
      {
        "paperId": "05a06f734b594840606906084657197fd447a4ed",
        "title": "Making Offline RL Online: Collaborative World Models for Offline Visual Reinforcement Learning"
      },
      {
        "paperId": "6eef081e4a2322ae9f96632032da6cbe147b9f66",
        "title": "Learning Video-Conditioned Policies for Unseen Manipulation Tasks"
      },
      {
        "paperId": "abec4dcbff2c6be5578ee5bf6c96347e7901b6a0",
        "title": "Reinforcement Learning from Passive Data via Latent Intentions"
      },
      {
        "paperId": "5d5024fae7223a0f8a8b86e1fdf5c5f4b7a9d9c6",
        "title": "UniDexGrasp++: Improving Dexterous Grasping Policy Learning via Geometry-aware Curriculum and Iterative Generalist-Specialist Learning"
      },
      {
        "paperId": "e8d67eaaf2a495fb5b2b1f813028c1784263ebc1",
        "title": "PartManip: Learning Cross-Category Generalizable Part Manipulation Policy from Point Cloud Observations"
      },
      {
        "paperId": "18190e8a77bb5ae4678aa466a9a805ec1910f44e",
        "title": "Task Aware Dreamer for Task Generalization in Reinforcement Learning"
      },
      {
        "paperId": "5d9617dd1e0cd7bd9888c2f1c793a8b450d151df",
        "title": "Grasping Student: semi-supervised learning for robotic manipulation"
      },
      {
        "paperId": "2ebd5df74980a37370b0bcdf16deff958289c041",
        "title": "Foundation Models for Decision Making: Problems, Methods, and Opportunities"
      },
      {
        "paperId": "1b6c1822cc273bc452eb0b3dfbe14de53dd10681",
        "title": "RePreM: Representation Pre-training with Masked Model for Reinforcement Learning"
      },
      {
        "paperId": "9e13ffe77bc1ae88dc9e89ddf6fdaa12f15c165f",
        "title": "Data-Driven Robotic Manipulation of Cloth-like Deformable Objects: The Present, Challenges and Future Prospects"
      },
      {
        "paperId": "d1177de3e533a40257c44e5ed39ba24eefcc8090",
        "title": "Multi-View Masked World Models for Visual Robotic Manipulation"
      },
      {
        "paperId": "7f270c9b098727675c9d8b893e362b561d61f27e",
        "title": "Policy Expansion for Bridging Offline-to-Online Reinforcement Learning"
      },
      {
        "paperId": "abba9a6f99d877fdd1b8412ddfcc26fdac6163dc",
        "title": "SMART: Self-supervised Multi-task pretrAining with contRol Transformers"
      },
      {
        "paperId": "638b5c76d96e32f54475a8327a9c68e0167156a9",
        "title": "A Survey on Transformers in Reinforcement Learning"
      },
      {
        "paperId": "0672e63cb2fb7a754e6549598440d3b0c66a002d",
        "title": "Pre-Trained Image Encoder for Generalizable Visual Reinforcement Learning"
      },
      {
        "paperId": "713ceff7a052d2270340f474ec2a987bd1c61117",
        "title": "Policy Adaptation from Foundation Model Feedback"
      },
      {
        "paperId": "72abe936b6d3603c69dbda2c36eccf78c04923b9",
        "title": "Multi-Environment Pretraining Enables Transfer to Action Limited Datasets"
      },
      {
        "paperId": "0703c5c7f737574d708babf48cdc876271415802",
        "title": "Masked Autoencoding for Scalable and Generalizable Decision Making"
      },
      {
        "paperId": "c90a33f1f0049d524e9b5b3174d35611fd9a8096",
        "title": "Pretraining in Deep Reinforcement Learning: A Survey"
      },
      {
        "paperId": "4535ea7eda92b5363e0faa8dedc33b3873aca74e",
        "title": "On the Feasibility of Cross-Task Transfer with Model-Based Reinforcement Learning"
      },
      {
        "paperId": "8cb359a03b499319c05eb7d36726341579dbe56f",
        "title": "EUCLID: Towards Efficient Unsupervised Reinforcement Learning with Multi-choice Dynamics Model"
      },
      {
        "paperId": "7241e08610d2b8a567dabb3dc6f4ad53437af7b1",
        "title": "HARP: Autoregressive Latent Video Prediction with High-Fidelity Image Generator"
      },
      {
        "paperId": "235303a8bc1e4892efd525a38ead657422d8a519",
        "title": "Transformers are Sample Efficient World Models"
      },
      {
        "paperId": "b62f6f765f033c1f023c4a424a20571564e61d97",
        "title": "Light-weight probing of unsupervised representations for Reinforcement Learning"
      },
      {
        "paperId": "31d629bb161d8199e18b6f2ed7e4ecbda10b6797",
        "title": "Masked World Models for Visual Control"
      },
      {
        "paperId": "0e02a6ed0b87634769b78d252b6d1aee468d2941",
        "title": "Reinforcement Learning with Neural Radiance Fields"
      },
      {
        "paperId": "c18bed90f9781e0750ffc634143015919886725b",
        "title": "POLTER: Policy Trajectory Ensemble Regularization for Unsupervised Reinforcement Learning"
      },
      {
        "paperId": "c9bdc9ad2c3cf3230ba9aac7b5783ab411f0d204",
        "title": "R3M: A Universal Visual Representation for Robot Manipulation"
      },
      {
        "paperId": "7ed1566a286068f39effa67ae5c7489dbea06414",
        "title": "VRL3: A Data-Driven Framework for Visual Deep Reinforcement Learning"
      },
      {
        "paperId": "f5ef05e654d4555704f180ad7436f6afb72fb343",
        "title": "PreLAR: World Model Pre-training with Learnable Action Representation"
      },
      {
        "paperId": "0a176ff2fcf08e852903a5bfa158e53a67c0b71a",
        "title": "Overcoming Knowledge Barriers: Online Imitation Learning from Observation with Pretrained World Models"
      },
      {
        "paperId": "c9c8336f31bf3debc6e30fb3ce7fa947c489980c",
        "title": "Decentralized Deepfake Task Management Algorithm Based on Blockchain and Edge Computing"
      },
      {
        "paperId": "60713a58c50aaa7965bb83afc729b6a44d20d58c",
        "title": "Pre-Training Goal-based Models for Sample-Efficient Reinforcement Learning"
      },
      {
        "paperId": "3c66bdfcddbcde02854582a8f97ca8d302924ad8",
        "title": "Vid2Act: Activate Offline Videos for Visual RL"
      },
      {
        "paperId": "04f1feccbea7d93037bd5e507d3f8e199b9acb09",
        "title": "O N T HE R OLE OF F ORGETTING IN F INE -T UNING R E - INFORCEMENT L EARNING M ODELS"
      },
      {
        "paperId": "bd2755fd96e20183f642628f818d7d161d3d0f0b",
        "title": "Hierarchical Diffusion for Offline Decision Making"
      },
      {
        "paperId": "34eaa5026f66adcd7ad77a320a0869639c5de0bd",
        "title": "Reward Informed Dreamer for Task Generalization in Reinforcement Learning"
      },
      {
        "paperId": "48f5b8bf81a860b46e69e5482f8ae7ecf8909a0d",
        "title": "On the Feasibility of Cross-Task Transfer with Model-Based Reinforcement Learning"
      },
      {
        "paperId": "d3381404950dc163f91787f0f4bbbd81c445246d",
        "title": "Become a Proficient Player with Limited Data through Watching Pure Videos"
      },
      {
        "paperId": "10c045a0e1e9b68a3e8a6cc2adb66bec96659984",
        "title": "Learning About Progress From Experts"
      },
      {
        "paperId": "6f016a780fd4ab40f12c2d223ecb0f080bfb2c00",
        "title": "Harmony World Models: Boosting Sample Efficiency for Model-based Reinforcement Learning"
      },
      {
        "paperId": "12012baeb768bf25c727b9f8c6bf85f836066f15",
        "title": "Collaborative World Models: An Online-Offline Transfer RL Approach"
      },
      {
        "paperId": "3dc0fc9eebd226994f3b9f3d78fc0eb8b54f155d",
        "title": "What Makes Representation Learning from Videos Hard for Control?"
      },
      {
        "paperId": "18d69b50ecf56a16906ce57d058a9394b36d9009",
        "title": "W HAT M AKES C ERTAIN P RE -T RAINED V ISUAL R EP - RESENTATIONS B ETTER FOR R OBOTIC L EARNING ?"
      },
      {
        "paperId": "bafbb3c535d9ee0fbffaad266f732a3892f53b4e",
        "title": "Deep reinforcement learning for real-world quadrupedal locomotion: a comprehensive review"
      },
      {
        "paperId": "a7dab897bbf1b174e7b1defb0fc8bd2366391f46",
        "title": "Self-Play and Self-Describe: Policy Adaptation with Vision-Language Foundation Models"
      },
      {
        "paperId": "f4e26372e5f3d220c2d5176b6a2a654dcc0123d8",
        "title": "I NVESTIGATING MULTI - TASK PRETRAINING AND GEN - ERALIZATION IN REINFORCEMENT LEARNING"
      },
      {
        "paperId": "1e5a80cceb581252dd30a1178f851b4302d483ed",
        "title": "Rank2Reward: Learning Robot Reward Functions from Passive Video"
      },
      {
        "paperId": "bb47c37a38818c7f5b401b7e967e9b7ecb4fce5e",
        "title": "A Universal World Model Learned from Large Scale and Diverse Videos"
      },
      {
        "paperId": "e2dcc9edc867afa96a9be965b8cf9fa04f30fbe3",
        "title": "S TRESS -T ESTING O FFLINE R EWARD -F REE R EINFORCE - MENT L EARNING : A C ASE FOR P LANNING WITH L ATENT D YNAMICS M ODELS"
      },
      {
        "paperId": "d03f3da50d23cdf4326e0c6033b8819ae0ca75ff",
        "title": "Point Track Prediction Models Enable Imitation from Action-less Datasets"
      }
    ],
    "score": 43.0
  },
  {
    "id": "f3a63a840185fa8c5e0db4bbe12afa7d3de7d029",
    "title": "Jump-Start Reinforcement Learning",
    "authors": [
      "Ikechukwu Uchendu",
      "Ted Xiao",
      "Yao Lu",
      "Banghua Zhu",
      "Mengyuan Yan",
      "J. Sim\u00f3n",
      "Matthew Bennice",
      "Chuyuan Fu",
      "Cong Ma",
      "Jiantao Jiao",
      "S. Levine",
      "Karol Hausman"
    ],
    "year": 2022,
    "citationCount": 127,
    "abstract": "Reinforcement learning (RL) provides a theoretical framework for continuously improving an agent's behavior via trial and error. However, efficiently learning policies from scratch can be very difficult, particularly for tasks with exploration challenges. In such settings, it might be desirable to initialize RL with an existing policy, offline data, or demonstrations. However, naively performing such initialization in RL often works poorly, especially for value-based methods. In this paper, we present a meta algorithm that can use offline data, demonstrations, or a pre-existing policy to initialize an RL policy, and is compatible with any RL approach. In particular, we propose Jump-Start Reinforcement Learning (JSRL), an algorithm that employs two policies to solve tasks: a guide-policy, and an exploration-policy. By using the guide-policy to form a curriculum of starting states for the exploration-policy, we are able to efficiently improve performance on a set of simulated robotic tasks. We show via experiments that JSRL is able to significantly outperform existing imitation and reinforcement learning algorithms, particularly in the small-data regime. In addition, we provide an upper bound on the sample complexity of JSRL and show that with the help of a guide-policy, one can improve the sample complexity for non-optimism exploration methods from exponential in horizon to polynomial.",
    "url": "https://www.semanticscholar.org/paper/f3a63a840185fa8c5e0db4bbe12afa7d3de7d029",
    "pdf_url": "https://arxiv.org/pdf/2204.02372.pdf",
    "venue": "International Conference on Machine Learning",
    "publicationDate": "2022-04-05",
    "externalIds": {
      "ArXiv": "2204.02372",
      "DBLP": "journals/corr/abs-2204-02372",
      "DOI": "10.48550/arXiv.2204.02372",
      "CorpusId": 247957953
    },
    "references": [
      {
        "paperId": "eb92a453cf982126fa2125d4c8915352a52af54d",
        "title": "Online Decision Transformer"
      },
      {
        "paperId": "62272403114c67a85e6fde9e428334d89e143485",
        "title": "AW-Opt: Learning Robotic Skills with Imitation and Reinforcement at Scale"
      },
      {
        "paperId": "348a855fe01f3f4273bf0ecf851ca688686dbfcc",
        "title": "Offline Reinforcement Learning with Implicit Q-Learning"
      },
      {
        "paperId": "d769ca62d90adc7e7869849a421426bdc54a32fb",
        "title": "Policy Finetuning: Bridging Sample-Efficient Offline and Online Reinforcement Learning"
      },
      {
        "paperId": "f864d4d2267abba15eb43db54f58286aef78292b",
        "title": "Offline Reinforcement Learning as One Big Sequence Modeling Problem"
      },
      {
        "paperId": "c1ad5f9b32d80f1c65d67894e5b8c2fdf0ae4500",
        "title": "Decision Transformer: Reinforcement Learning via Sequence Modeling"
      },
      {
        "paperId": "0bcc734246586966596b1aa22efac2565224ebee",
        "title": "Bridging Offline Reinforcement Learning and Imitation Learning: A Tale of Pessimism"
      },
      {
        "paperId": "82da3373cdf63393c004cf8b87e5af86398bccd3",
        "title": "BATCH POLICY LEARNING IN AVERAGE REWARD MARKOV DECISION PROCESSES."
      },
      {
        "paperId": "5da1b4e1ddc612970530e5bb29470fe43bfcf2d6",
        "title": "PC-PG: Policy Cover Directed Exploration for Provable Policy Gradient Learning"
      },
      {
        "paperId": "6bf9b58b8f418fb2922762550fb78bb22c83c0f8",
        "title": "Variational Policy Gradient Method for Reinforcement Learning with General Utilities"
      },
      {
        "paperId": "0272b14dd471fe7b81df703af1b71d7600b77215",
        "title": "Accelerating Online Reinforcement Learning with Offline Datasets"
      },
      {
        "paperId": "28db20a81eec74a50204686c3cf796c42a020d2e",
        "title": "Conservative Q-Learning for Offline Reinforcement Learning"
      },
      {
        "paperId": "616ac6772508e72084360158078058dfa8bda7e7",
        "title": "First return, then explore"
      },
      {
        "paperId": "8046dbd4ebc3fda0fcc43d9110c0d2d940052980",
        "title": "Almost Optimal Model-Free Reinforcement Learning via Reference-Advantage Decomposition"
      },
      {
        "paperId": "a326d9f2d2d351001fece788165dbcbb524da2e4",
        "title": "D4RL: Datasets for Deep Data-Driven Reinforcement Learning"
      },
      {
        "paperId": "07f90ffc986803e5b87c6e31774ca804668cfc19",
        "title": "Bypassing the Monster: A Faster and Simpler Optimal Algorithm for Contextual Bandits under Realizability"
      },
      {
        "paperId": "5c5f76af16e9fc7f5b65df390b0d4cc6d64e6f20",
        "title": "Learning Near Optimal Policies with Low Inherent Bellman Error"
      },
      {
        "paperId": "ad14227e4f51276892ffc37aa43fd8750bb5eba8",
        "title": "Advantage-Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning"
      },
      {
        "paperId": "893a16bfc2e14c4eec45470f76083632470fc41c",
        "title": "Neural Policy Gradient Methods: Global Optimality and Rates of Convergence"
      },
      {
        "paperId": "f7f8f05eb2798272fc3a61443d45f2aa47e65135",
        "title": "On the Theory of Policy Gradient Methods: Optimality, Approximation, and Distribution Shift"
      },
      {
        "paperId": "d423fa6cd0f4d088941d9fe4bebd834d0137c9b9",
        "title": "Provably Efficient Reinforcement Learning with Linear Function Approximation"
      },
      {
        "paperId": "63a863dacc78d42b0c971649abc1e35a5575f7f7",
        "title": "On Value Functions and the Agent-Environment Boundary"
      },
      {
        "paperId": "b32ffe8f86bf0c36d85f8274ff0b6e2c9690a133",
        "title": "Information-Theoretic Considerations in Batch Reinforcement Learning"
      },
      {
        "paperId": "95c126448c7218adface63db44a63260fb4030aa",
        "title": "Contextual Bandits with Continuous Actions: Smoothing, Zooming, and Adapting"
      },
      {
        "paperId": "c520bf47db3360ae3a52219771390a354ed8a91f",
        "title": "Go-Explore: a New Approach for Hard-Exploration Problems"
      },
      {
        "paperId": "5d4ebbaa1ef8feeac885e2869f45c0276c18834f",
        "title": "Learning Montezuma's Revenge from a Single Demonstration"
      },
      {
        "paperId": "9b4d48eca82e8a3907b0e941a7ebd6976c95d6ae",
        "title": "Backplay: \"Man muss immer umkehren\""
      },
      {
        "paperId": "03cc81e98942bdafd994af7a1d1e62a68ff8b682",
        "title": "Is Q-learning Provably Efficient?"
      },
      {
        "paperId": "eb37e7b76d26b75463df22b2a3aa32b6a765c672",
        "title": "QT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation"
      },
      {
        "paperId": "fded244c5d244fe02b62eada842bd466ab9853fa",
        "title": "BaRC: Backward Reachability Curriculum for Robotic Reinforcement Learning"
      },
      {
        "paperId": "993db634039c441876bfe09e22ea0429edbe1f16",
        "title": "Solving the Rubik's Cube Without Human Knowledge"
      },
      {
        "paperId": "c28ec2a40a2c77e20d64cf1c85dc931106df8e83",
        "title": "Overcoming Exploration in Reinforcement Learning with Demonstrations"
      },
      {
        "paperId": "e010ba3ff5744604cdbfe44a733e2a98649ee907",
        "title": "Learning Complex Dexterous Manipulation with Deep Reinforcement Learning and Demonstrations"
      },
      {
        "paperId": "daea16e16370c9d141ce6bd1b42ee3e5287bf275",
        "title": "Learning Unknown Markov Decision Processes: A Thompson Sampling Approach"
      },
      {
        "paperId": "1bead9000a719cb258bac7320228055aee650d2c",
        "title": "Leveraging Demonstrations for Deep Reinforcement Learning on Robotics Problems with Sparse Rewards"
      },
      {
        "paperId": "9862caed8ee93321c78b0196e0b7eef516b545ba",
        "title": "Reverse Curriculum Generation for Reinforcement Learning"
      },
      {
        "paperId": "2d0c876919ff95ca68d38a7963ed14e94b5129d8",
        "title": "Contextual Decision Processes with low Bellman rank are PAC-Learnable"
      },
      {
        "paperId": "6fab0b3b321988cedd0a017c1dad997f6d4da930",
        "title": "Playing Atari Games with Deep Reinforcement Learning and Human Checkpoint Replay"
      },
      {
        "paperId": "8783688bfe249bd1cab13146a76ba50fe88128c7",
        "title": "Model-based Reinforcement Learning and the Eluder Dimension"
      },
      {
        "paperId": "d746a1f64daae2d3fb91de8ffe08e9e5668cdc38",
        "title": "Approximate Policy Iteration Schemes: A Comparison"
      },
      {
        "paperId": "2319a491378867c7049b3da055c5df60e1671158",
        "title": "Playing Atari with Deep Reinforcement Learning"
      },
      {
        "paperId": "c8d7fe6495e020c879e222afa1d794cbec58ed29",
        "title": "Contextual Bandits with Linear Payoff Functions"
      },
      {
        "paperId": "70e10a5459c6f1aaf346ee4f2dcc837151fbe75c",
        "title": "Efficient Reductions for Imitation Learning"
      },
      {
        "paperId": "9a5cdfaf0968a908f167bc2c9c073cb5b56f08b3",
        "title": "The Epoch-Greedy algorithm for contextual multi-armed bandits"
      },
      {
        "paperId": "d925fca2d6859c43e4c91a53fca96528de0c28dc",
        "title": "Policy Search by Dynamic Programming"
      },
      {
        "paperId": "d00595481313734057f8c5205cee3e79a11ebe10",
        "title": "Effective reinforcement learning for mobile robots"
      },
      {
        "paperId": "523b4ce1c2a1336962444abc1dec215756c2f3e6",
        "title": "Approximately Optimal Approximate Reinforcement Learning"
      },
      {
        "paperId": "a9da09d1e63686706d64782e654d69f13fd292ad",
        "title": "Learning from Demonstration"
      },
      {
        "paperId": "f90bb9b59b64ded0c98b909deb949acc1d9c71df",
        "title": "Complexity Analysis of Real-Time Reinforcement Learning"
      },
      {
        "paperId": "9cd8193a66cf53143cbba6ccb0c7b9c2ebf2452b",
        "title": "Self-improving reactive agents based on reinforcement learning, planning and teaching"
      },
      {
        "paperId": null,
        "title": "2020), this assumption on offline regression oracle implies our Assumption on regret bound in Assumption A.2"
      },
      {
        "paperId": "3114f22f2db023e93bfe6ffb6197e533beb31e76",
        "title": "Neural Trust Region/Proximal Policy Optimization Attains Globally Optimal Policy"
      },
      {
        "paperId": null,
        "title": "Deepmimic: Example-guided deep reinforcement learning of physics-based character skills"
      },
      {
        "paperId": "c5f8d083ea2a49d921f4f74d16ba6923a504dd08",
        "title": "Imitation and Reinforcement Learning for Motor Primitives with Perceptual Coupling"
      },
      {
        "paperId": "af49e9dd262e1c8e902dd31b5f035f93402c6cfe",
        "title": "Learning decisions: robustness, uncertainty, and approximation"
      },
      {
        "paperId": "4d98ce60f4f8ed822503b8d13b0605f8c5d74ca7",
        "title": "In Advances in Neural Information Processing Systems"
      },
      {
        "paperId": null,
        "title": "Here S can be relaxed to the maximum state size that \u03c0 g visits among all steps"
      }
    ],
    "cited_by": [
      {
        "paperId": "75a85fa71377ac3ad2ee514cc9750df19a4d2ba5",
        "title": "Solving Robotics Tasks with Prior Demonstration via Exploration-Efficient Deep Reinforcement Learning"
      },
      {
        "paperId": "0aab03cbdd5f3971af9ade1b00dbb311ae8de202",
        "title": "State Revisit and Re-explore: Bridging Sim-to-Real Gaps in Offline-and-Online Reinforcement Learning with An Imperfect Simulator"
      },
      {
        "paperId": "edc017668408b4fc01501777f7bd29ddb70b13b9",
        "title": "Value Function Initialization for Knowledge Transfer and Jump-start in Deep Reinforcement Learning"
      },
      {
        "paperId": "956c03803fca72e647a19f26fef50d0177505e07",
        "title": "Learning to See and Act: Task-Aware View Planning for Robotic Manipulation"
      },
      {
        "paperId": "36d5adfdd38dca61b7034bc2aca0f9724659f9d2",
        "title": "Distributed Hierarchical Reinforcement Learning for Path Planning of Long-Horizon Micro-Assembly Tasks"
      },
      {
        "paperId": "02be602aabe91e283fd129935ff60a459ac0f89f",
        "title": "SCRIPT: A Scalable Continual Reinforcement Learning Framework for Autonomous Penetration Testing"
      },
      {
        "paperId": "f4ae9c12411d8744cf7ebcde1c6fc97e98a48604",
        "title": "Deep Reinforcement Learning for Robotic Grasping: Insights into Learning from Raw Visual Data"
      },
      {
        "paperId": "8bede6b5fe990c815b1695a02862c6e012b95038",
        "title": "Penalizing Infeasible Actions and Reward Scaling in Reinforcement Learning with Offline Data"
      },
      {
        "paperId": "a98587acde8e1df366d7c4964b5a08ed979aefb2",
        "title": "Behavioral Exploration: Learning to Explore via In-Context Adaptation"
      },
      {
        "paperId": "096d4a024a818d497291ba0371ef4d924b52b892",
        "title": "Accelerated Online Reinforcement Learning using Auxiliary Start State Distributions"
      },
      {
        "paperId": "d76a0939456508ca0008e26482c12d862ed0c48e",
        "title": "Reliable Routing and Scheduling in Time Sensitive Networks Based on Reinforcement Learning"
      },
      {
        "paperId": "b828d3d65d078a03f8c6e69ce2caeefd91acc8eb",
        "title": "Jump-Start Reinforcement Learning with Self-Evolving Priors for Extreme Monopedal Locomotion"
      },
      {
        "paperId": "86c0a312cc432615dada12f056881270e1661c11",
        "title": "Energy-Based Transfer for Reinforcement Learning"
      },
      {
        "paperId": "34d8817c85fd7f351e75a6bcbdd851a559877300",
        "title": "Trajectory First: A Curriculum for Discovering Diverse Policies"
      },
      {
        "paperId": "73cbc44842bdfe8c746f5ea11fc6d0948e949f49",
        "title": "Bayesian curriculum generation in sparse reward reinforcement learning environments"
      },
      {
        "paperId": "e484e4186ba7e0c6e5b85b2dd35719849031771b",
        "title": "EcoRouteQ: A Reinforcement Learning Framework for Green Route Recommendations"
      },
      {
        "paperId": "c3f2c71e6aa8987b10f8d614ffbc1331b37b68bf",
        "title": "Learning to Trust Bellman Updates: Selective State-Adaptive Regularization for Offline RL"
      },
      {
        "paperId": "4a3e88d203564e547f5fb3f3d816a0b381492eae",
        "title": "VLA-RL: Towards Masterful and General Robotic Manipulation with Scalable Reinforcement Learning"
      },
      {
        "paperId": "9fa625528073fee005cf2d0fd2b2e9699a1af926",
        "title": "ReWiND: Language-Guided Rewards Teach Robot Policies without New Demonstrations"
      },
      {
        "paperId": "8208bdf61e934062c726dc6d7158d3f4137afa11",
        "title": "A Policy-Guided Reinforcement Learning Method for Encirclement Control in Multiobstacle Environment"
      },
      {
        "paperId": "5a83b1754fc4acb4cbafed806ef432656ff62cd2",
        "title": "Fine-Tuning without Performance Degradation"
      },
      {
        "paperId": "d704151119ac7dfdf4902ee39978edf002fbabeb",
        "title": "Symbolic State Seeding Improves Coverage of Reinforcement Learning"
      },
      {
        "paperId": "c8d8513bcde9c9225baa0eef14ecdd399cb96d0a",
        "title": "A Survey of Reinforcement Learning-Based Motion Planning for Autonomous Driving: Lessons Learned from a Driving Task Perspective"
      },
      {
        "paperId": "c7ceb19e3497294bbe57ce634f9c9aa94ed9c54e",
        "title": "From Task Distributions to Expected Paths Lengths Distributions: Value Function Initialization in Sparse Reward Environments for Lifelong Reinforcement Learning"
      },
      {
        "paperId": "1db4673e6e9fa3b79021e5fceaed78d83f88e71c",
        "title": "RoboTron-Nav: A Unified Framework for Embodied Navigation Integrating Perception, Planning, and Prediction"
      },
      {
        "paperId": "ff94f04357e145de58cec24d651d039b7ee3bc34",
        "title": "Dexterous Grasping with Real-World Robotic Reinforcement Learning"
      },
      {
        "paperId": "45439384b065b56f5b56140a1b40d1ca14d6ec31",
        "title": "A2Perf: Real-World Autonomous Agents Benchmark"
      },
      {
        "paperId": "6623a117f1b1b5c11ebc06d7be93c6368eae7417",
        "title": "A Preference-Based Online Reinforcement Learning With Embedded Communication Failure Solutions in Smart Grid"
      },
      {
        "paperId": "696188ba6b12122ba8b154dbeac7917097c013d6",
        "title": "Efficient Reinforcement Learning by Guiding Generalist World Models with Non-Curated Data"
      },
      {
        "paperId": "e1f707f6f78024bf18ab812b3899f068f0f6bf10",
        "title": "Enhancing Offline Model-Based RL via Active Model Selection: A Bayesian Optimization Perspective"
      },
      {
        "paperId": "74cde73aeaa438d9d57d586057e39585d2bae3cf",
        "title": "Risk-Conscious Mutations in Jump-Start Reinforcement Learning for Autonomous Racing Policy"
      },
      {
        "paperId": "cb52ed456c02472f7de5df58645fde7c2e8e8613",
        "title": "Optimistic Critic Reconstruction and Constrained Fine-Tuning for General Offline-to-Online RL"
      },
      {
        "paperId": "466b7a6e2b12330b6bb23f4af64732b4e77ca794",
        "title": "Policy Decorator: Model-Agnostic Online Refinement for Large Policy Model"
      },
      {
        "paperId": "2a7fa520acae21e33f646a4716c7ca680f8f60f4",
        "title": "QoS-Aware Energy-Efficient Multi-UAV Offloading Ratio and Trajectory Control Algorithm in Mobile-Edge Computing"
      },
      {
        "paperId": "c2c0e1ec3e006ebd6533aae98131128dc1358f0d",
        "title": "Efficient Online Reinforcement Learning Fine-Tuning Need Not Retain Offline Data"
      },
      {
        "paperId": "aeb1f4ab2160135fc4ff32e8cae20d23e31234ac",
        "title": "Marvel: Accelerating Safe Online Reinforcement Learning with Finetuned Offline Policy"
      },
      {
        "paperId": "1ad829a48cbc69fc3b76eb8ceb4cf6a3ae77b44b",
        "title": "Towards Practical Deep Schedulers for Allocating Cellular Radio Resources"
      },
      {
        "paperId": "221b6b8c72ff611ff0567a5ed1bb06e6c7aabcbc",
        "title": "A Harmonized Approach: Beyond-the-Limit Control for Autonomous Vehicles Balancing Performance and Safety in Unpredictable Environments"
      },
      {
        "paperId": "06b8e37e8f43b7ad636b58688ac0e19c05e97883",
        "title": "Reinforcement Learning Gradients as Vitamin for Online Finetuning Decision Transformers"
      },
      {
        "paperId": "d81b4df5f88306286d1867197b98dbf99cf3f7bc",
        "title": "Stepping Out of the Shadows: Reinforcement Learning in Shadow Mode"
      },
      {
        "paperId": "3775bd14050cd41b32e518d2f34a85125680b2dd",
        "title": "Leveraging Skills from Unlabeled Prior Data for Efficient Online Exploration"
      },
      {
        "paperId": "40ccf8bac2903c013c3232bda40c5be8cf7e30ae",
        "title": "Reinforcement Learning for Control of Non-Markovian Cellular Population Dynamics"
      },
      {
        "paperId": "861190af506eeba158343fe3f5b3b42248f79a1a",
        "title": "Learning placement order for constructive floorplanning"
      },
      {
        "paperId": "f2144e875ca6270cd3fc4f4e177c2c07a338cb62",
        "title": "FLaRe: Achieving Masterful and Adaptive Robot Policies with Large-Scale Reinforcement Learning Fine-Tuning"
      },
      {
        "paperId": "d5af71a15801e4d8610b6f3ebc2f3e616da7ef45",
        "title": "Robotic Hand-Eye Coordination Fusion"
      },
      {
        "paperId": "843d8906ab712b14fb4590404c939d3319aa98e6",
        "title": "Goal-Reaching Policy Learning from Non-Expert Observations via Effective Subgoal Guidance"
      },
      {
        "paperId": "c5d0b2fe5c6b468c8fe5ef90ea8418df07d5ad90",
        "title": "Whole-Body Control Through Narrow Gaps from Pixels to Action"
      },
      {
        "paperId": "ae66bf807131f32fe045d5d40dfb774e3d596273",
        "title": "Reference RL: Reinforcement learning with reference mechanism and its application in traffic signal control"
      },
      {
        "paperId": "f9516ccd48fb7d8fa1098b7cb5e4d3583508aed3",
        "title": "Improving Multi-agent Reinforcement Learning with Stable Prefix Policy"
      },
      {
        "paperId": "f3a3df0d0debdc272dbdbae9f0b06ec6813ed615",
        "title": "Efficient and Stable Offline-to-online Reinforcement Learning via Continual Policy Revitalization"
      },
      {
        "paperId": "d8f75adfc48528833a6fc5fa0dcc6e856016a243",
        "title": "From Imitation to Refinement - Residual Rl for Precise Assembly"
      },
      {
        "paperId": "e3ec6fa351268f017709b7ab869c21ad7711559c",
        "title": "Rocket Landing Control with Random Annealing Jump Start Reinforcement Learning"
      },
      {
        "paperId": "05e9d9a26dfd92fc76a91451e98affe1415134df",
        "title": "GuideLight: \"Industrial Solution\" Guidance for More Practical Traffic Signal Control Agents"
      },
      {
        "paperId": "543b4fe08cfff003a602629d027e7775627e1f15",
        "title": "A Benchmark Environment for Offline Reinforcement Learning in Racing Games"
      },
      {
        "paperId": "b8613422f34a792325a8e7425524de31536ce48e",
        "title": "Residual-MPPI: Online Policy Customization for Continuous Control"
      },
      {
        "paperId": "29938d5e818910d3c4af9cb8c84c6c1b84aa2f38",
        "title": "Planning Using Schr\u00f6dinger Bridge Diffusion Models"
      },
      {
        "paperId": "c18768fc623a57b54a5cccbc7a5b1c0805ecf36b",
        "title": "Hybrid Reinforcement Learning from Offline Observation Alone"
      },
      {
        "paperId": "77654aeb80095f332e40f5aab6553784f5e52487",
        "title": "Physics-guided actor-critic reinforcement learning for swimming in turbulence"
      },
      {
        "paperId": "1a4238a1dd6068f59b3daa8923a6612a2f98cd12",
        "title": "DEER: A Delay-Resilient Framework for Reinforcement Learning with Variable Delays"
      },
      {
        "paperId": "427ea6bbe46c1bb0999a3b8a26b8aa57a489cc2e",
        "title": "Multi-Agent Transfer Learning via Temporal Contrastive Learning"
      },
      {
        "paperId": "761509e64bf8eb462b615e5fbcea866289ccc8af",
        "title": "FuRL: Visual-Language Models as Fuzzy Rewards for Reinforcement Learning"
      },
      {
        "paperId": "e1ba406f477df54d747a77b40781b07f837ff33c",
        "title": "Offline-Boosted Actor-Critic: Adaptively Blending Optimal Historical Behaviors in Deep Off-Policy RL"
      },
      {
        "paperId": "56354d90733c5ad450d6073dcd4568db041a5d01",
        "title": "On-Demand Model and Client Deployment in Federated Learning With Deep Reinforcement Learning"
      },
      {
        "paperId": "8532475afc69e55782bfb2da6fbe00c8537fcc08",
        "title": "Ensemble Successor Representations for Task Generalization in Offline-to-Online Reinforcement Learning"
      },
      {
        "paperId": "0fecc3a349cf4bc0f430cb8b9a69e8540ee10905",
        "title": "Reverse Forward Curriculum Learning for Extreme Sample and Demonstration Efficiency in Reinforcement Learning"
      },
      {
        "paperId": "a8cf219c83b66995f5fa535078e4a9e3f3e68a01",
        "title": "RICE: Breaking Through the Training Bottlenecks of Reinforcement Learning with Explanation"
      },
      {
        "paperId": "6a472311da2e19f44e7999acc8d8aac4fd59b5a2",
        "title": "Distilling Privileged Information for Dubins Traveling Salesman Problems with Neighborhoods"
      },
      {
        "paperId": "0b3db8cb73853bb3663cdc63ea8fc180f25e3757",
        "title": "CDA-MBPO:Corrected Data Aggregation for Model-Based Policy Optimization"
      },
      {
        "paperId": "be4d96d4be590f1b3af60953caf3f19b641db70c",
        "title": "Offline Reinforcement Learning Based on Next State Supervision"
      },
      {
        "paperId": "f036c481cb86c96228c66e2e3e9778ba4730435c",
        "title": "Dataset Reset Policy Optimization for RLHF"
      },
      {
        "paperId": "033b96503d85082445fc724b91d5ab252934418f",
        "title": "Demonstration Guided Multi-Objective Reinforcement Learning"
      },
      {
        "paperId": "a4065e70b25bdf20b4587f8d1bbaaa7e4eeb9390",
        "title": "Correlated Proxies: A New Definition and Improved Mitigation for Reward Hacking"
      },
      {
        "paperId": "6a8d04f7f47954f72944db4b37652352857e5ef8",
        "title": "Snapshot Reinforcement Learning: Leveraging Prior Trajectories for Efficiency"
      },
      {
        "paperId": "87f1b39c320e1fc71584a231855523167a5588ff",
        "title": "RIME: Robust Preference-based Reinforcement Learning with Noisy Preferences"
      },
      {
        "paperId": "1d759e71640b235d7252af45d7ce3ca4f4be65d2",
        "title": "Enhancing Reinforcement Learning Agents with Local Guides"
      },
      {
        "paperId": "d1b60b99e9702924b27c6b05fbd3eff948b347b1",
        "title": "A Competition Winning Deep Reinforcement Learning Agent in microRTS"
      },
      {
        "paperId": "ac0a728f40b843b43392f94d383f749c5c2afe1a",
        "title": "Closure Discovery for Coarse-Grained Partial Differential Equations Using Grid-based Reinforcement Learning"
      },
      {
        "paperId": "9e0da24b96eb4f3f77ccdfc485a4d7d7d21a4585",
        "title": "Large Language Model as a Policy Teacher for Training Reinforcement Learning Agents"
      },
      {
        "paperId": "05603c65d813105eb136e867abfc412ea1735cf0",
        "title": "Accelerating Exploration with Unlabeled Prior Data"
      },
      {
        "paperId": "6980a76b7e1df3e84893e90bfcae9aff1f8f94e6",
        "title": "Learning Realistic Traffic Agents in Closed-loop"
      },
      {
        "paperId": "9d552f44fe2dc219038a4a3874b7fc67e917c0fd",
        "title": "CQM: Curriculum Reinforcement Learning with a Quantized World Model"
      },
      {
        "paperId": "60bbf59000ee42a30c03a50e151c45aae2c283fc",
        "title": "PolyTask: Learning Unified Policies through Behavior Distillation"
      },
      {
        "paperId": "0dd4696e8b075ffdc12325dc828cf36752ca7447",
        "title": "Human-Robot Gym: Benchmarking Reinforcement Learning in Human-Robot Collaboration"
      },
      {
        "paperId": "c3207f4155fc4ac26e38e940255a2ecc2180a948",
        "title": "Guided Online Distillation: Promoting Safe Reinforcement Learning by Offline Demonstration"
      },
      {
        "paperId": "90bfabd63d9e8823570ff77213e97d1d3232865a",
        "title": "Learning to Collaborate by Grouping: a Consensus-oriented Strategy for Multi-agent Reinforcement Learning"
      },
      {
        "paperId": "f3e1417a3552679e53dab907a61dca458f47f3bd",
        "title": "Warm-Start Actor-Critic: From Approximation Error to Sub-optimality Gap"
      },
      {
        "paperId": "6ff3fb272e56249ce8a32180147a1e370a83fc2d",
        "title": "Offline Reinforcement Learning with Uncertainty Critic Regularization Based on Density Estimation"
      },
      {
        "paperId": "436787c5977afd75a818402ddb69743d21d1dbba",
        "title": "Residual Q-Learning: Offline and Online Policy Customization without Value"
      },
      {
        "paperId": "09bc7b16e793369f673368eccc7cd7e9e0467a0b",
        "title": "On the Value of Myopic Behavior in Policy Reuse"
      },
      {
        "paperId": "2bd9c93edb1ac68ea8d45f6252209a1682bec757",
        "title": "PROTO: Iterative Policy Regularized Offline-to-Online Reinforcement Learning"
      },
      {
        "paperId": "05d759990fc286c9938d24f0ec4dea6716a311e5",
        "title": "Knowledge Transfer from Teachers to Learners in Growing-Batch Reinforcement Learning"
      },
      {
        "paperId": "b2961394ecb6051ffe1847fcc9e5eb7685d3ec6d",
        "title": "Inverse Reinforcement Learning without Reinforcement Learning"
      },
      {
        "paperId": "96a19cd83080d70e10f3e6fd5af327a1263f20d6",
        "title": "Teach a Robot to FISH: Versatile Imitation from One Minute of Demonstrations"
      },
      {
        "paperId": "672ec9fa4ddb5b6bfc46c61c5b2f4bdfa1aa8ed9",
        "title": "Dual RL: Unification and New Methods for Reinforcement and Imitation Learning"
      },
      {
        "paperId": "7f270c9b098727675c9d8b893e362b561d61f27e",
        "title": "Policy Expansion for Bridging Offline-to-Online Reinforcement Learning"
      },
      {
        "paperId": "af34181ae916e01c72513f915f984a1d2c7febab",
        "title": "Augmented Behavioral Annotation Tools, with Application to Multimodal Datasets and Models: A Systematic Review"
      },
      {
        "paperId": "18ef9c5a2d728703dc8e576e4b07a0c5c82df77d",
        "title": "PIRLNav: Pretraining with Imitation and RL Finetuning for OBJECTNAV"
      },
      {
        "paperId": "6279a0556a80aa70dc2064ebc3c5f4e630a0dbc4",
        "title": "Vehicle Extreme Control based on Offline Reinforcement Leaning"
      },
      {
        "paperId": "55ebeeee3885b722fd73f13cf3cb9a51e9f5cc63",
        "title": "Accelerate online reinforcement learning for building HVAC control with heterogeneous expert guidances"
      },
      {
        "paperId": "bd2ff852e86d16df09376f2dfdc934c533bb04a2",
        "title": "Residual Skill Policies: Learning an Adaptable Skill-based Action Space for Reinforcement Learning for Robotics"
      },
      {
        "paperId": "fc4c08a07637634fe156da49a866d3a5c4afe4a6",
        "title": "LEAGUE: Guided Skill Learning and Abstraction for Long-Horizon Manipulation"
      },
      {
        "paperId": "2d33f309f7e92c75434a2bb16f70d6ec65ab7d2a",
        "title": "Hybrid RL: Using Both Offline and Online Data Can Make RL Efficient"
      },
      {
        "paperId": "104213c5ead6ed90419aa230ee2f73ce8f793a5a",
        "title": "Some Supervision Required: Incorporating Oracle Policies in Reinforcement Learning via Epistemic Uncertainty Metrics"
      },
      {
        "paperId": "2d5edc038a177bed2f58e5374f38a411d8538f0b",
        "title": "Watch and Match: Supercharging Imitation with Regularized Optimal Transport"
      },
      {
        "paperId": "a94aaf192fc1d46d697e4d7eb3e999021ec88b46",
        "title": "Phasic Self-Imitative Reduction for Sparse-Reward Goal-Conditioned Reinforcement Learning"
      },
      {
        "paperId": "5e86a1e80cd7a84a5ff316f59345f00c402bddb5",
        "title": "Reincarnating Reinforcement Learning: Reusing Prior Computation to Accelerate Progress"
      },
      {
        "paperId": "adc0c5769274e1c242dece4cad6b2e7990eb622f",
        "title": "JUNO: Jump-Start Reinforcement Learning-based Node Selection for UWB Indoor Localization"
      },
      {
        "paperId": "784c1e1b02ac95b31189ac12d12a4fa909634052",
        "title": "Combining imitation and deep reinforcement learning to human-level performance on a virtual foraging task"
      },
      {
        "paperId": "7ed1566a286068f39effa67ae5c7489dbea06414",
        "title": "VRL3: A Data-Driven Framework for Visual Deep Reinforcement Learning"
      },
      {
        "paperId": "ec95e665bfb3b73144ce8717e1e033fee167fddb",
        "title": "Never Worse, Mostly Better: Stable Policy Improvement in Deep Reinforcement Learning"
      },
      {
        "paperId": "fc2413e670f81d79cd49601674225d976d89b516",
        "title": "Autonomous PID Tuning: Two-Phase Reinforcement Learning Through Adversarial Imitation Learning Under Imperfect Demonstrations"
      },
      {
        "paperId": "543243cab16c978e3732f77eab5aeb2dac5cf294",
        "title": "Adaptive Drift Control of Autonomous Electric Vehicles After Brake System Failures"
      },
      {
        "paperId": "8762a37a33b7a64ce876a79ea888441feb49384f",
        "title": "Closure Discovery for Coarse-Grained Partial Differential Equations using Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "af7e54b921dd5318e805b729b222cd565cdff84d",
        "title": "Contextual Online Imitation Learning (COIL): Using Guide Policies in Reinforcement Learning"
      },
      {
        "paperId": "878e2c80a9247b6106b479bf8d74c02427947176",
        "title": "Preventing Reward Hacking with Occupancy Measure Regularization"
      },
      {
        "paperId": "90050ce8752719200760e33dc07bb812c38d33bb",
        "title": "Imitation from Arbitrary Experience: A Dual Unification of Reinforcement and Imitation Learning Methods"
      },
      {
        "paperId": "70611c87aba567ff0587cf63b409ffc98cbc63e0",
        "title": "SC-MAIRL: Semi-Centralized Multi-Agent Imitation Reinforcement Learning"
      },
      {
        "paperId": "ab83df0afa15fb5fcce8addc634314ea3a187055",
        "title": "Reduce, Reuse, Recycle: Selective Reincarnation in Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "452452f203866be2ede5416fb598618a6419d4b9",
        "title": "InfODist: Online distillation with Informative rewards improves generalization in Curriculum Learning"
      },
      {
        "paperId": "33c1087b86025c7bc919f2fda817a491c0c350ab",
        "title": "Beyond Tabula Rasa: Reincarnating Reinforcement Learning"
      },
      {
        "paperId": "cba897aa98ffa8ec2caca3d0266c9443189dd07a",
        "title": "Offline Multi-Agent Reinforcement Learning with Knowledge Distillation"
      },
      {
        "paperId": "0f8296ca42d21daa709944ea041ec93552919a47",
        "title": "Will it Blend? Learning to Coexecute Subskills"
      },
      {
        "paperId": "268c95bf45e6f176af1bc95199d11ecccbaeb156",
        "title": "Contextual Online Imitation Learning"
      },
      {
        "paperId": "fc06240770e06e9fdc1332268a8266ee2c2f1d86",
        "title": "Transfer Learning via Temporal Contrastive Learning"
      },
      {
        "paperId": "471930540ec0b085b7ca6fc2330eba89b0d43e22",
        "title": "Learning Realistic Traf\ufb01c Agents in Closed-loop"
      },
      {
        "paperId": "23e9beb5141833767c7a0af3ce64d6aff1905f58",
        "title": "Safe Reinforcement Learning: A Survey"
      },
      {
        "paperId": "04a5fd29c8fde3a9698d5a33d35034b9f1f745da",
        "title": "Reinforcement Learning for Optimal Control of Adaptive Cell Populations"
      }
    ],
    "score": 42.33333333333333
  },
  {
    "id": "52a6657cf1cb3cc847695a386bd65b5eea34bc13",
    "title": "Deep Reinforcement Learning-Based Automatic Exploration for Navigation in Unknown Environment",
    "authors": [
      "Haoran Li",
      "Qichao Zhang",
      "Dongbin Zhao"
    ],
    "year": 2020,
    "citationCount": 206,
    "abstract": "This paper investigates the automatic exploration problem under the unknown environment, which is the key point of applying the robotic system to some social tasks. The solution to this problem via stacking decision rules is impossible to cover various environments and sensor properties. Learning-based control methods are adaptive for these scenarios. However, these methods are damaged by low learning efficiency and awkward transferability from simulation to reality. In this paper, we construct a general exploration framework via decomposing the exploration process into the decision, planning, and mapping modules, which increases the modularity of the robotic system. Based on this framework, we propose a deep reinforcement learning-based decision algorithm that uses a deep neural network to learning exploration strategy from the partial map. The results show that this proposed algorithm has better learning efficiency and adaptability for unknown environments. In addition, we conduct the experiments on the physical robot, and the results suggest that the learned policy can be well transferred from simulation to the real robot.",
    "url": "https://www.semanticscholar.org/paper/52a6657cf1cb3cc847695a386bd65b5eea34bc13",
    "pdf_url": "https://arxiv.org/pdf/2007.11808.pdf",
    "venue": "IEEE Transactions on Neural Networks and Learning Systems",
    "publicationDate": "2020-06-01",
    "externalIds": {
      "DBLP": "journals/corr/abs-2007-11808",
      "MAG": "2966477753",
      "ArXiv": "2007.11808",
      "DOI": "10.1109/TNNLS.2019.2927869",
      "CorpusId": 199519158,
      "PubMed": "31398138"
    },
    "references": [
      {
        "paperId": "96f2f4792e19de7d24d9fe49b514700b668ff1db",
        "title": "Reinforcement Learning and Deep Learning based Lateral Control for Autonomous Driving"
      },
      {
        "paperId": "b0a9cd290251cf66b6057ffa390efc502c66f5e3",
        "title": "Efficient Online Learning Algorithms Based on LSTM Neural Networks"
      },
      {
        "paperId": "399dbf5ba3d00f37b8476bef04ad0dee90f85102",
        "title": "Optimal and Autonomous Control Using Reinforcement Learning: A Survey"
      },
      {
        "paperId": "535d184eadf47fa17ce4073b6e2f180783e85300",
        "title": "Curiosity-driven Exploration for Mapless Navigation with Deep Reinforcement Learning"
      },
      {
        "paperId": "537ec9c862600a9e8ac8e0a2f4aa89f2cdeef867",
        "title": "Deep Reinforcement Learning With Visual Attention for Vehicle Classification"
      },
      {
        "paperId": "1c9eb7a0a96cceb93c83827bba1c17d33d7240cc",
        "title": "Toward autonomous mapping and exploration for mobile robots through deep supervised learning"
      },
      {
        "paperId": "5f6256d0d4ffc6021b99e08b0f1d9f1089ab9679",
        "title": "Kinodynamic trajectory optimization and control for car-like robots"
      },
      {
        "paperId": "e23b92fcac8a1a4ff7cbf31dc35b3821b2e78500",
        "title": "Multi-task learning for dangerous object detection in autonomous driving"
      },
      {
        "paperId": "225ab689f41cef1dc18237ef5dab059a49950abf",
        "title": "Curiosity-Driven Exploration by Self-Supervised Prediction"
      },
      {
        "paperId": "e900f8a10c1db2d8a85467a759501df9700199e2",
        "title": "Model-Free Optimal Control Based Intelligent Cruise Control with Hardware-in-the-Loop Demonstration [Research Frontier]"
      },
      {
        "paperId": "f7017d021b11cf2f130812e13e22532cafac45d6",
        "title": "Event-Triggered Optimal Control for Partially Unknown Constrained-Input Systems via Adaptive Dynamic Programming"
      },
      {
        "paperId": "7f3c55c3ea86bef29c0f9c8719bb4b326e9e20fc",
        "title": "Iterative Adaptive Dynamic Programming for Solving Unknown Nonlinear Zero-Sum Game Based on Online Data"
      },
      {
        "paperId": "3ee01ec27e4e66e089b72a9989724be611c2ad90",
        "title": "Neural Map: Structured Memory for Deep Reinforcement Learning"
      },
      {
        "paperId": "8a8f5ec1d76ab3308434b8fccc2b5e67907226ac",
        "title": "Mobile robots exploration through cnn-based reinforcement learning"
      },
      {
        "paperId": "d35b05f440b5ba00d9429139edef7182bf9f7ce7",
        "title": "Learning to Navigate in Complex Environments"
      },
      {
        "paperId": "76dceabf5f6d3dcedc89746046b42009375978bc",
        "title": "Decentralized non-communicating multiagent collision avoidance with deep reinforcement learning"
      },
      {
        "paperId": "f2bc398896cb3f16b0c090703c6ce821fdd460f2",
        "title": "Experience Replay for Optimal Control of Nonzero-Sum Game Systems With Unknown Dynamics"
      },
      {
        "paperId": "846aedd869a00c09b40f1f1f35673cb22bc87490",
        "title": "Mastering the game of Go with deep neural networks and tree search"
      },
      {
        "paperId": "4c05d7caa357148f0bbd61720bdd35f0bc05eb81",
        "title": "Dueling Network Architectures for Deep Reinforcement Learning"
      },
      {
        "paperId": "024006d4c2a89f7acacc6e4438d156525b60a98f",
        "title": "Continuous control with deep reinforcement learning"
      },
      {
        "paperId": "28e323447be3e878b2ae3a1b380f921f8b07028a",
        "title": "Hilbert maps: Scalable continuous occupancy mapping with stochastic gradient descent"
      },
      {
        "paperId": "d6b93b766dff2066cebbc897c9ec7fbc44848ad7",
        "title": "DeepDriving: Learning Affordance for Direct Perception in Autonomous Driving"
      },
      {
        "paperId": "340f48901f72278f6bf78a04ee5b01df208cc508",
        "title": "Human-level control through deep reinforcement learning"
      },
      {
        "paperId": "abd1c342495432171beb7ca8fd9551ef13cbd0ff",
        "title": "ImageNet classification with deep convolutional neural networks"
      },
      {
        "paperId": "426a3b8fb728cfc9281a2a54011e84137edcbd79",
        "title": "A comparison of path planning strategies for autonomous exploration and mapping of unknown environments"
      },
      {
        "paperId": "24d1afc81644877f6fc34a5a15d7a41e03a4e522",
        "title": "G2o: A general framework for graph optimization"
      },
      {
        "paperId": "115cd605202a03f9ef93375b736b814cc62e8304",
        "title": "Path Planning for Virtual Human Motion Using Improved A* Star Algorithm"
      },
      {
        "paperId": "bcadeadb4656ef2d4e3d8c0abf251196764c73a8",
        "title": "Information-Based Compact Pose SLAM"
      },
      {
        "paperId": "50d385523ccf32ae41a4c31556129f2e10110d27",
        "title": "A Tutorial on Graph-Based SLAM"
      },
      {
        "paperId": "f07e40a14e4f7e6d32bbaad32fe47cf61713e637",
        "title": "Real-time correlative scan matching"
      },
      {
        "paperId": "cd9de9fe5d8d9796dc29c92e94068da3d873d945",
        "title": "The Player/Stage Project: Tools for Multi-Robot and Distributed Sensor Systems"
      },
      {
        "paperId": "423567b410da7e8619c5ab361f88fa6ec58d82d0",
        "title": "Information based adaptive robotic exploration"
      },
      {
        "paperId": "cc6a857746f74176ec55ffa8f83b4d6cef940add",
        "title": "Navigation Strategies for Exploring Indoor Environments"
      },
      {
        "paperId": "6410aacd3f02f12bf78a8b3e5e664d263cee2183",
        "title": "FastSLAM: a factored solution to the simultaneous localization and mapping problem"
      },
      {
        "paperId": "a1875055e9c526cbdc7abb161959d76d14b58266",
        "title": "A frontier-based approach for autonomous exploration"
      },
      {
        "paperId": "39f553011f636065fc371c90859efa4534e121c0",
        "title": "Event-Based Robust Control for Uncertain Nonlinear Systems Using Adaptive Dynamic Programming"
      },
      {
        "paperId": "4f8d648c52edf74e41b0996128aa536e13cc7e82",
        "title": "Deep Learning"
      },
      {
        "paperId": "97efafdb4a3942ab3efba53ded7413199f79c054",
        "title": "Reinforcement Learning: An Introduction"
      },
      {
        "paperId": "b1b800aacf850a18e6504c693b4c5d33b8b3ac32",
        "title": "State of the Art\u2014A Survey of Partially Observable Markov Decision Processes: Theory, Models, and Algorithms"
      },
      {
        "paperId": "736f5e20a8e05875eccb855c02beb15cb6995608",
        "title": "Ef\ufb01cient Online Learning Algorithms Based on LSTM Neural Networks"
      }
    ],
    "cited_by": [
      {
        "paperId": "7026fe231d1ae563c711c274949f0440a068dadf",
        "title": "Method for Mobile Robot Navigation Using Reinforcement Learning in an Unfamiliar and Complicated Environment"
      },
      {
        "paperId": "0fcdc53b3a3c9f4289e7d9913ba1bdbf99ff4402",
        "title": "CogniPlan: Uncertainty-Guided Path Planning with Conditional Generative Layout Prediction"
      },
      {
        "paperId": "d16cb50f5fb62fdb162f36e2fc12d9c278e3e973",
        "title": "A Planar Array Synthesis Method Based on Deep Learning and Radiation Pattern Superposition Method"
      },
      {
        "paperId": "599d092e5aadf58160cf18a87539bff8e743178f",
        "title": "Development and Validation of a Fully Autonomous Navigation Framework for Autonomous Underwater Vehicles: Improvements in Path Planning and Algorithm Optimization"
      },
      {
        "paperId": "53d0112b068ab92083de56eee1706d7f94fc6de2",
        "title": "Integrated decision-control for social robot autonomous navigation considering nonlinear dynamics model"
      },
      {
        "paperId": "ce1603c3474306b85e81b5430882355e9815b7ee",
        "title": "Indoor navigation for mobile robots based on deep reinforcement learning with convolutional neural network"
      },
      {
        "paperId": "ddbbfc3bb5870c40071475b7b7b946ce82f7d498",
        "title": "NeuronsGym: A Hybrid Framework and Benchmark for Robot Navigation With Sim2Real Policy Learning"
      },
      {
        "paperId": "efc028b1a397eeb46eb6b945faea536d068b752a",
        "title": "TeViR: Text-to-Video Reward with Diffusion Models for Efficient Reinforcement Learning"
      },
      {
        "paperId": "7e1732065b7232d71826aaf92f1f3d0d308f8d46",
        "title": "Adaptive Exploration Deep Q-Network for AGV Path Planning in Dynamic Environments Using 3D Convolutional Neural Networks"
      },
      {
        "paperId": "81a5ef40de4cb64267a3a408675d078efe4d4b63",
        "title": "Balancing State Exploration and Skill Diversity in Unsupervised Skill Discovery"
      },
      {
        "paperId": "62670e059c9e58fab14add10af425caa1c17b299",
        "title": "DART: Dual-level Autonomous Robotic Topology for Efficient Exploration in Unknown Environments"
      },
      {
        "paperId": "cc38d12a1ca8a7f5bcffde78679925c861452747",
        "title": "MapExRL: Human-Inspired Indoor Exploration with Predicted Environment Context and Reinforcement Learning"
      },
      {
        "paperId": "550e68f9a184c213738489f698d359b982262d44",
        "title": "Semantic-Driven Informed Planning and 3D Reconstruction for the Quadrotor Unmanned Aerial Vehicle"
      },
      {
        "paperId": "7e3757cd4e5d65fa5d0aca686bffe1fb6004ccf1",
        "title": "Improved DDPG based on enhancing decision evaluation for path planning in high-density environments"
      },
      {
        "paperId": "74128804d80f6b543775ea39035f6e06ce2b5469",
        "title": "MARVEL: Multi-Agent Reinforcement Learning for Constrained Field-of-View Multi-Robot Exploration in Large-Scale Environments"
      },
      {
        "paperId": "75c97aba86eecf9688d4fa96090bde2edb58710a",
        "title": "Salience-Invariant Consistent Policy Learning for Generalization in Visual Reinforcement Learning"
      },
      {
        "paperId": "65ba6ba15eae5234c87b73574da9dd3fd0872f1c",
        "title": "Optimal Navigation in Microfluidics via the Optimization of a Discrete Loss"
      },
      {
        "paperId": "efa903f3ae81a0fe1144d36d3807da079a33a932",
        "title": "Harnessing machine learning algorithms for the prediction and optimization of various properties of polylactic acid in biomedical use: a comprehensive review"
      },
      {
        "paperId": "81b3b63cc30c90eeedb7b7567f6d387a5af7fd5d",
        "title": "Navigation of autonomous mobile robots in dynamic unknown environments based on dueling double deep q networks"
      },
      {
        "paperId": "13e9950341a26816c01bc0c6a051d698641c7c82",
        "title": "Quantum-Inspired Reinforcement Learning for Quantum Control"
      },
      {
        "paperId": "6f04d0d52bca0f9647d78dcbcae597f6437c0efa",
        "title": "Sample-Efficient Unsupervised Policy Cloning from Ensemble Self-Supervised Labeled Videos"
      },
      {
        "paperId": "1cd2c34771ece5c7044043263ed00bec9555f27c",
        "title": "Recent progress, challenges and future prospects of applied deep reinforcement learning : A practical perspective in path planning"
      },
      {
        "paperId": "3fbcd1d40b4eeb080aa59a4d94278054d5ec8f58",
        "title": "Dream to Drive With Predictive Individual World Model"
      },
      {
        "paperId": "cd61dfade264a6e0110cc088ec190999c4087fe0",
        "title": "Multilayer Decision-Making Framework for Adversarial Game Scenarios"
      },
      {
        "paperId": "a4aa251db85ff03430e292fa752204ff3ebf5bf4",
        "title": "Refining Large Language Model Query Optimization: An Adaptive Semantic Approach"
      },
      {
        "paperId": "71099b1b600ab28ff1cb99f460588ed1d00d8763",
        "title": "Semantic-Driven Autonomous Visual Navigation for Unmanned Aerial Vehicles"
      },
      {
        "paperId": "05251327ea1edd35ebf60d605eec66330ac65287",
        "title": "Multi-Agent Exploration With Similarity Score Map and Topological Memory"
      },
      {
        "paperId": "b8591fefc74fb63cf19363547b077d7edef43899",
        "title": "Efficient 3D Exploration with Distributed Multi-UAV Teams: Integrating Frontier-Based and Next-Best-View Planning"
      },
      {
        "paperId": "ef3ebba8c442c48403fe0284648b4b69e9d9b87c",
        "title": "DARE: Diffusion Policy for Autonomous Robot Exploration"
      },
      {
        "paperId": "efdc2ba255b5d0ab258f74669fbb22fe5182d7d1",
        "title": "Practical Probabilistic Model-Based Reinforcement Learning by Integrating Dropout Uncertainty and Trajectory Sampling"
      },
      {
        "paperId": "dcbddf55802b6f94644cc2e056c540d6b0dc88f9",
        "title": "BoxMap: Efficient Structural Mapping and Navigation"
      },
      {
        "paperId": "6260c0fd2cfb7907ad0b9e269a740f85163fb21c",
        "title": "Cooperative and Asynchronous Transformer-based Mission Planning for Heterogeneous Teams of Mobile Robots"
      },
      {
        "paperId": "e56014dbd87edc013d13bbedd91f6a833069329e",
        "title": "Dynamic event-triggered time-varying formation control for heterogeneous unmanned swarm systems with scaling attacks"
      },
      {
        "paperId": "aff3fbe1a8115f9e2f76da9b0f3b120bed2429fd",
        "title": "Intelligent mobile robot navigation in unknown and complex environment using reinforcement learning technique"
      },
      {
        "paperId": "fd0c04712a597404ae12ae31e07a6f893a8c854d",
        "title": "LiDAR-Based End-to-End Active SLAM Using Deep Reinforcement Learning in Large-Scale Environments"
      },
      {
        "paperId": "09a01f6a420040fdc3dff751df0fb75524e8ac7b",
        "title": "A novel autonomous exploration algorithm via LiDAR/IMU SLAM and hierarchical subsystem for mobile robot in unknown indoor environments"
      },
      {
        "paperId": "851c588dba8c7e241aca5c036779640bb7646c3f",
        "title": "Generalizing Consistency Policy to Visual RL with Prioritized Proximal Experience Regularization"
      },
      {
        "paperId": "784f72c8e388e2ba34b6527c8714f36687e1f00b",
        "title": "The Role of Environments and Sensing Strategies in Unmanned Aerial Vehicle Crowdsensing"
      },
      {
        "paperId": "82128f9bf11ffde6bd9e6b079d2af5716c7ea5da",
        "title": "DDPG-MPC: A Safe and Efficient Hybrid Path Planning Algorithm for Robots"
      },
      {
        "paperId": "03bb46fd5858394009cf26547b4b9ba9e7ecc91f",
        "title": "Visuomotor Navigation for Embodied Robots With Spatial Memory and Semantic Reasoning Cognition"
      },
      {
        "paperId": "6127c2ca7eb86072987061b9135d116e6f15f1c0",
        "title": "An Efficient Autonomous Exploration Framework for Unmanned Surface Vehicles in Unknown Waters"
      },
      {
        "paperId": "691817e12b5fcbce065bbe3f7d5c7a4cef401dc0",
        "title": "Enhancing Reinforcement Learning via Transformer-Based State Predictive Representations"
      },
      {
        "paperId": "dc52360929ff884da10b4ccd87d632863deb6212",
        "title": "Stabilizing Diffusion Model for Robotic Control With Dynamic Programming and Transition Feasibility"
      },
      {
        "paperId": "51f362f8a1badea426ea0655d752e6bd99db15fe",
        "title": "Deep Reinforcement Learning-Based Off-Road Path Planning via Low-Dimensional Simulation"
      },
      {
        "paperId": "61c680bfb9517f670e93debb2d9d599ad4ba2d69",
        "title": "A Survey on Reinforcement Learning Applications in SLAM"
      },
      {
        "paperId": "ea1ba7d4d9158609e54390a0dbe95a7f8739d14d",
        "title": "Multi-Robot Cooperative Navigation in Dynamic Environments using Deep Reinforcement Learning in ROS"
      },
      {
        "paperId": "d201369d356a08d901ae30dc15c63379d76d04c1",
        "title": "HDPlanner: Advancing Autonomous Deployments in Unknown Environments Through Hierarchical Decision Networks"
      },
      {
        "paperId": "d03c704f60055665307fc765bb1e0d6af73da0d9",
        "title": "A deep residual reinforcement learning algorithm based on Soft Actor-Critic for autonomous navigation"
      },
      {
        "paperId": "540e036e73157e2f68f316746c26cb5d9d9c1353",
        "title": "Mobile robot sequential decision making using a deep reinforcement learning hyper-heuristic approach"
      },
      {
        "paperId": "755819773bdcb1fa85adf8c492c97456ab7dd7f9",
        "title": "Transformer-Based Reinforcement Learning for Multi-Robot Autonomous Exploration"
      },
      {
        "paperId": "3bc8687878663e301e2db7513f650bd97d216be7",
        "title": "MAT: Morphological Adaptive Transformer for Universal Morphology Policy Learning"
      },
      {
        "paperId": "46ebbdb20595dca8dfaba7507afcc820030b62a5",
        "title": "Bayesian reinforcement learning for navigation planning in unknown environments"
      },
      {
        "paperId": "9670017de23a58fb5084728c1758889317dbb13a",
        "title": "High-quality Synthetic Data is Efficient for Model-based Offline Reinforcement Learning"
      },
      {
        "paperId": "b1160d0a13f1a6091a92a4f98ff80bbbe38771a0",
        "title": "RASLS: Reinforcement Learning Active SLAM Approach with Layout Semantic"
      },
      {
        "paperId": "7f6b5768e0a37d6b4fe4c08e0df9609ae91df6d8",
        "title": "Designing Reliable Navigation Behaviors for Autonomous Agents in Partially Observable Grid-world Environments"
      },
      {
        "paperId": "ec75dbd839c663253877af826b4392cda56ee183",
        "title": "Dynamic Environment-driven Autonomous Drone Path Planning via Deep Reinforcement Learning"
      },
      {
        "paperId": "406adf33fde52803747ecdf7d3a55c3873f52ae2",
        "title": "Learning Hierarchical Graph-Based Policy for Goal-Reaching in Unknown Environments"
      },
      {
        "paperId": "ad36987b3b66b44bd8f595c470ec80c2dc5fe32b",
        "title": "Efficient Navigation of a Robotic Fish Swimming Across the Vortical Flow Field"
      },
      {
        "paperId": "b6af09722063318e9b890760aedbe2672c3489a2",
        "title": "Learning Future Representation with Synthetic Observations for Sample-efficient Reinforcement Learning"
      },
      {
        "paperId": "27b409b664cb90ea15292c5548aad9ed957e2c97",
        "title": "Reinforcement Learning with Neighborhood Search and Self-learned Rules"
      },
      {
        "paperId": "18d2b5273c1f44f00f4d85b34c7f3994efbfdcbd",
        "title": "Robot Autonomous Exploration System Base on Arm-Chassis Collaboration"
      },
      {
        "paperId": "526c7cfba432f84a6259bc78b9f023377bae5d64",
        "title": "Incremental Reinforcement Learning via Performance Evaluation and Policy Perturbation"
      },
      {
        "paperId": "22179c374eb0a7dc0b539e367fbb1fc31e69fa07",
        "title": "Deep-learning based autonomous-exploration for UAV navigation"
      },
      {
        "paperId": "4360cda7e6d17a330dbe9f1fe7792f60fef91788",
        "title": "Goal Decision-making in Active SLAM Using 2D Lidar Based on DRL Driven by Intrinsic Rewards"
      },
      {
        "paperId": "2a16fa07fd48693e2ff2d050c6de03cfe5f87bf4",
        "title": "Boosting On-Policy Actor\u2013Critic With Shallow Updates in Critic"
      },
      {
        "paperId": "ce03059378825fca55df88d2b74d2224e78d09ea",
        "title": "Path Planning Method for Manipulators Based on Improved Twin Delayed Deep Deterministic Policy Gradient and RRT*"
      },
      {
        "paperId": "1ecb2c6d341974aa060e3881e558e73906844cf3",
        "title": "Consensus-based Deep Reinforcement Learning for Mobile Robot Mapless Navigation"
      },
      {
        "paperId": "91c83328d5907c715e7cf2f9f51d07a0271f391d",
        "title": "Deep Reinforcement Learning-Based Large-Scale Robot Exploration"
      },
      {
        "paperId": "7e923ec5808a6c82e92ce00e335ef25811e5f0e2",
        "title": "Advancing Object Goal Navigation Through LLM-enhanced Object Affinities Transfer"
      },
      {
        "paperId": "daee3362e3b174984d128a56e61f475e0077d5f2",
        "title": "EFP: Efficient Frontier-Based Autonomous UAV Exploration Strategy for Unknown Environments"
      },
      {
        "paperId": "19e40ab3a16876306fe21d725ca4597007f35aca",
        "title": "Deep Reinforcement Learning With Reward Design for Quantum Control"
      },
      {
        "paperId": "ef837a79c37009f2287ff82a3b00b3fda226b7de",
        "title": "Aerodynamic force reduction of rectangular cylinder using deep reinforcement learning-controlled multiple jets"
      },
      {
        "paperId": "5b191f509bb6491b6022f26d6824859a679e8268",
        "title": "Approximate Policy Iteration With Deep Minimax Average Bellman Error Minimization"
      },
      {
        "paperId": "b9232e9fc02b9eea96cb6a4804cdfce77dcfe56f",
        "title": "Autonomous navigation of mobile robots in unknown environments using off-policy reinforcement learning with curriculum learning"
      },
      {
        "paperId": "8bce871080807853f7ffa4ca3db58aaf57083042",
        "title": "Planning-Inspired Hierarchical Trajectory Prediction via Lateral-Longitudinal Decomposition for Autonomous Driving"
      },
      {
        "paperId": "493ed9b156ceefe31a79f9aa585eec98797c7d7a",
        "title": "Deep Reinforcement Learning-Based Multirestricted Dynamic-Request Transportation Framework"
      },
      {
        "paperId": "0179ee951f2b01794743aaad49fcb88ae25d2959",
        "title": "A Comparison Study between Traditional and Deep-Reinforcement-Learning-Based Algorithms for Indoor Autonomous Navigation in Dynamic Scenarios"
      },
      {
        "paperId": "310a2b6fdb2e8a2984912ca5c476617dedb5426b",
        "title": "Self-Supervised Imitation for Offline Reinforcement Learning With Hindsight Relabeling"
      },
      {
        "paperId": "bf1b830a7b5e3276f6d6edf17bcb7cea81cc2bde",
        "title": "Robust Multi-Agent Communication With Graph Information Bottleneck Optimization"
      },
      {
        "paperId": "1749fa7ddc840dd174a23f0c4efd6c6eaf412d80",
        "title": "Coordinating Autonomous Vehicles in a Contested Environment"
      },
      {
        "paperId": "393797551f0499860ba72c56346383374853c3b0",
        "title": "Multi-Robot Autonomous Exploration in Unknown Environment: A Review"
      },
      {
        "paperId": "6ebc3414e9ef9284b6b41980d18a2711c84891c4",
        "title": "A survey on socially aware robot navigation: Taxonomy and future challenges"
      },
      {
        "paperId": "834dd1b330108cba9502aff5caadee7e9a655c63",
        "title": "Higher Order Probabilistic Analysis of Network Trajectories of Intelligent Agents in Thespian"
      },
      {
        "paperId": "3aa798832ebedbb927b2ffc811ef2f38798e053c",
        "title": "Stabilizing the square cylinder wake using deep reinforcement learning for different jet locations"
      },
      {
        "paperId": "d13300ceebeb4e856036a09bf3fabbf39c542fdf",
        "title": "Conditional Goal-Oriented Trajectory Prediction for Interacting Vehicles"
      },
      {
        "paperId": "b6f4b18fcf03b3ebe51dd686ea6fc8138329ac25",
        "title": "Deep Reinforcement Learning With Explicit Context Representation"
      },
      {
        "paperId": "0cb990b7ad4e232abb0751ebc7ef03783e8309ca",
        "title": "Do More with Less: Single-Model, Multi-Goal Architectures for Resource-Constrained Robots"
      },
      {
        "paperId": "ce37384218421d884f0aff8d3ca0242fbbda25fd",
        "title": "ComSD: Balancing Behavioral Quality and Diversity in Unsupervised Skill Discovery"
      },
      {
        "paperId": "5eceb4d435f1eb86d9b211ffc1f76c87f79aa2c8",
        "title": "Practical Probabilistic Model-based Deep Reinforcement Learning by Integrating Dropout Uncertainty and Trajectory Sampling"
      },
      {
        "paperId": "90c7e3f6d1b10fa31d1c2b7c3413805eee0607d8",
        "title": "Hierarchical Adversarial Inverse Reinforcement Learning"
      },
      {
        "paperId": "bfb514f67a6681355550ffe170426048ca6a18dd",
        "title": "Assisted driving system based on federated reinforcement learning"
      },
      {
        "paperId": "92de97975875713fb99996986f336294bd17037c",
        "title": "Challengesand Solutions for Autonomous Ground Robot Scene Understanding and Navigation in Unstructured Outdoor Environments: A Review"
      },
      {
        "paperId": "f9797152156278f898991a8cac0ac7fc54d64a79",
        "title": "Deep Learning for Visual Localization and Mapping: A Survey"
      },
      {
        "paperId": "ffc9581ca58b9cea840a17db29e182055ca10d9f",
        "title": "Deep reinforcement learning-aided autonomous navigation with landmark generators"
      },
      {
        "paperId": "e9cdd9d0a40029f9e658877c570ac901108ea0a4",
        "title": "Learning Heterogeneous Relation Graph and Value Regularization Policy for Visual Navigation"
      },
      {
        "paperId": "5b46acd0640a64eef70dbcfc41b4aad0efb74711",
        "title": "Long-Term Tracking of Evasive Urban Target Based on Intention Inference and Deep Reinforcement Learning"
      },
      {
        "paperId": "f47b555697f382e65690057d484a0639753e67c4",
        "title": "Model-Free Guidance Method for Drones in Complex Environments Using Direct Policy Exploration and Optimization"
      },
      {
        "paperId": "e83b8576ed07c4288bcd79563622f28b1da9aaa3",
        "title": "Inventory Robots: Performance Evaluation of an RFID-Based Navigation Strategy"
      },
      {
        "paperId": "8be32c97492669905b5a867076234de7b8afdb6e",
        "title": "Adaptive control for multi-agent systems with actuator fault via reinforcement learning and its application on multi-unmanned surface vehicle"
      },
      {
        "paperId": "8182d213f68ff3c96a12e22eaaa089079e98c3d4",
        "title": "EMExplorer: an episodic memory enhanced autonomous exploration strategy with Voronoi domain conversion and invalid action masking"
      },
      {
        "paperId": "3093ae7692f47cb5ac40d23e0505d6e8af27e62b",
        "title": "Fast Robot Hierarchical Exploration Based on Deep Reinforcement Learning"
      },
      {
        "paperId": "50b9563789cbaabf0254348d0d1bf8c5a13bf3d8",
        "title": "Modeling limit order trading with a continuous action policy for deep reinforcement learning"
      },
      {
        "paperId": "fa2a7e6c9c2fe5c764ffe13585f269940bdd25a9",
        "title": "Reinforcement Learning-Based Model Predictive Control for Discrete-Time Systems"
      },
      {
        "paperId": "1c3b758f7baf4401ed201390e9d763d4bca2d6a6",
        "title": "Comparison of Different Domain Randomization Methods for Policy Transfer in Reinforcement Learning"
      },
      {
        "paperId": "d7140abe3979fb68487d8c87477f6c2f1651e2de",
        "title": "STExplorer: A Hierarchical Autonomous Exploration Strategy with Spatio-temporal Awareness for Aerial Robots"
      },
      {
        "paperId": "9e11c850056a181b4ef7ea4268dea23df9cc7e30",
        "title": "Efficient Autonomous Exploration and Mapping in Unknown Environments"
      },
      {
        "paperId": "5dd28612deeb24cabc80f30b5819b277a2218f7e",
        "title": "On the role and opportunities in teamwork design for advanced multi-robot search systems"
      },
      {
        "paperId": "70d554b0535bc005e4bc0135cd2a926507030ef2",
        "title": "Autonomous exploration through deep reinforcement learning"
      },
      {
        "paperId": "549081870517b0947ba00fb833821a87b43cd321",
        "title": "A Sampling-Based Distributed Exploration Method for UAV Cluster in Unknown Environments"
      },
      {
        "paperId": "d48fae1c4129ab54d1447a86afe8415095d60bc5",
        "title": "NeuronsMAE: A Novel Multi-Agent Reinforcement Learning Environment for Cooperative and Competitive Multi-Robot Tasks"
      },
      {
        "paperId": "879b4debf3233fccb17aa943d283c1d0cf8232ea",
        "title": "NeuronsGym: A Hybrid Framework and Benchmark for Robot Tasks with Sim2Real Policy Learning"
      },
      {
        "paperId": "077713f636633ac5c16e7413c9d08ba285adc901",
        "title": "ARiADNE: A Reinforcement learning approach using Attention-based Deep Networks for Exploration"
      },
      {
        "paperId": "03dfa5d89000cce2e609fc985e99302b286a3797",
        "title": "Autonomous highway driving using reinforcement learning with safety check system based on time-to-collision"
      },
      {
        "paperId": "82a5a9d694425b9cee2e612671e9d179738c3b0b",
        "title": "RRT Based Frontier Point Detection for 2D Autonomous Exploration"
      },
      {
        "paperId": "0703ddc4117355fad8f021883dfa8bd68237ce14",
        "title": "3D Global Path Planning Optimization for Cellular-Connected UAVs under Link Reliability Constraint"
      },
      {
        "paperId": "7d167d70176d0e58342f7080e3e6b3cd1aa15616",
        "title": "Dynamic-Horizon Model-Based Value Estimation With Latent Imagination"
      },
      {
        "paperId": "0afcfc4d3fde581b306ab6071241f71d6116832a",
        "title": "Conditional Goal-oriented Trajectory Prediction for Interacting Vehicles with Vectorized Representation"
      },
      {
        "paperId": "0b1070f1f2232e69ff93ecde47bf7c3ef04131e7",
        "title": "Offline Reinforcement Learning for Autonomous Driving with Real World Driving Data"
      },
      {
        "paperId": "83c0353a25e2c675d013581ae9775324757f51f4",
        "title": "BNAS-v2: Memory-Efficient and Performance-Collapse-Prevented Broad Neural Architecture Search"
      },
      {
        "paperId": "f7c1f3af5709ec762461d3d410eb43b4b652c9f1",
        "title": "MSVIPER: Improved Policy Distillation for Reinforcement-Learning-Based Robot Navigation"
      },
      {
        "paperId": "57e62822d1f7ebba53aca99fd9b8d646df88e8bf",
        "title": "Task-Agnostic Learning to Accomplish New Tasks"
      },
      {
        "paperId": "3fe351b19fb9d35f9d947cd93b5a378631ca3388",
        "title": "Dynamics-Adaptive Continual Reinforcement Learning via Progressive Contextualization"
      },
      {
        "paperId": "6b24218838b3b69481290ec5e94002a8f65aa70e",
        "title": "A survey of visual navigation: From geometry to embodied AI"
      },
      {
        "paperId": "263f352d9d2071680429bef9bdee0a1098692578",
        "title": "Efficient Exploration for Multi-Agent Reinforcement Learning via Transferable Successor Features"
      },
      {
        "paperId": "00711701fb1abe8b5523281dcce1fb3aed6680cb",
        "title": "SIM: A Scenario IMagination Based Deep Reinforcement Learning Method for Outdoor Transportation Environment Exploration"
      },
      {
        "paperId": "234b145737d5d7b3439ea73ffa2b826485681663",
        "title": "A Neural Network-Based Navigation Approach for Autonomous Mobile Robot Systems"
      },
      {
        "paperId": "7e5cbb92a4bc6e7d2e0598768bf29ad3d83b68e9",
        "title": "Neurons Perception Dataset for RoboMaster AI Challenge"
      },
      {
        "paperId": "772d8c3b3962faf2db842ed6202babfef2a112be",
        "title": "End-to-End Autonomous Exploration for Mobile Robots in Unknown Environments through Deep Reinforcement Learning"
      },
      {
        "paperId": "87c7f02df2a61b4b71a0324d3b4597a232360c00",
        "title": "HTRON: Efficient Outdoor Navigation with Sparse Rewards via Heavy Tailed Adaptive Reinforce Algorithm"
      },
      {
        "paperId": "49cc565483139ac43b7fc64d6dc370eacb816423",
        "title": "A Survey on Active Simultaneous Localization and Mapping: State of the Art and New Frontiers"
      },
      {
        "paperId": "b3e67b2e955949c23ec81662701e7a0a676f3317",
        "title": "A Fully Controllable Agent in the Path Planning using Goal-Conditioned Reinforcement Learning"
      },
      {
        "paperId": "f6290eb3ca2890ddc779b0b0e3ac16f5ad4106cb",
        "title": "Anti-Martingale Proximal Policy Optimization"
      },
      {
        "paperId": "1f668eeb2579c4403fb43ff3ec08671f736dedef",
        "title": "Simultaneous Double Q-learning with Conservative Advantage Learning for Actor-Critic Methods"
      },
      {
        "paperId": "c8cdd1f1553828294e83086647d35565ac7bc137",
        "title": "\u03b5\u2013greedy automated indentation of cementitious materials for phase mechanical properties determination"
      },
      {
        "paperId": "67e9fad0394d099f91963aca8cdac1c0f2dd191c",
        "title": "Deep-Attack over the Deep Reinforcement Learning"
      },
      {
        "paperId": "511b1437d5d9b79069b918179ddf9138ff911c9d",
        "title": "Efficient Bayesian Policy Reuse With a Scalable Observation Model in Deep Reinforcement Learning"
      },
      {
        "paperId": "e79f4c32c67ff9d4f99614213e72da8c409b695b",
        "title": "Cognitive Path Planning With Spatial Memory Distortion"
      },
      {
        "paperId": "ef97ec3e7b95a80234cb2c66fd377a6e2dee67cb",
        "title": "TrajGen: Generating Realistic and Diverse Trajectories With Reactive and Feasible Agent Behaviors for Autonomous Driving"
      },
      {
        "paperId": "f27602ad405501ca8dd2150f8bbe470bf2b9208c",
        "title": "Reinforcement Learning based Voice Interaction to Clear Path for Robots in Elevator Environment"
      },
      {
        "paperId": "b47c1d24ae02eb5228e60ca0402d3401fc3fc4bd",
        "title": "Learning for Robot Decision Making under Distribution Shift: A Survey"
      },
      {
        "paperId": "7c46b07b4d0de219cc749a529ae0d531ff8095ad",
        "title": "Depthwise Convolution for Multi-Agent Communication With Enhanced Mean-Field Approximation"
      },
      {
        "paperId": "8e93c526b67c98f4dfb52ffbfed97878b047a1dc",
        "title": "A Soar-Based Space Exploration Algorithm for Mobile Robots"
      },
      {
        "paperId": "032ed55af9f75cef20a8da61a32d7cf86dd89268",
        "title": "Multi-task Safe Reinforcement Learning for Navigating Intersections in Dense Traffic"
      },
      {
        "paperId": "b3558e36bdaed906f3dd8acce35caed4b4110ab0",
        "title": "Minimax Q-learning design for H\u221e control of linear discrete-time systems"
      },
      {
        "paperId": "7103c85b33086bc5122c9a212d010307f24aa228",
        "title": "State-of-the-Art Development of Complex Systems and Their Simulation Methods"
      },
      {
        "paperId": "fe609b4a2ab1ee99c50602f7ac1a2df8496356b5",
        "title": "Reinforcement Learning for Mobile Robotics Exploration: A Survey"
      },
      {
        "paperId": "e14be0c78e9576ac11d09c55189770f203872248",
        "title": "Event-Triggered Communication Network With Limited-Bandwidth Constraint for Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "abb6bd82d3774c26ec50e485aaaf373090d51068",
        "title": "Embodied scene description"
      },
      {
        "paperId": "cccce7dbb495f19e1d3b7944d2c18b9d0815bf63",
        "title": "Mapless Navigation: Learning UAVs Motion forExploration of Unknown Environments"
      },
      {
        "paperId": "3862d41354417d0bbcb74fcee7bafb7a06247df3",
        "title": "Robotic odor source localization via adaptive bio-inspired navigation using fuzzy inference methods"
      },
      {
        "paperId": "588bd57e1171ff5c551ee4902b3473f7bcf011d9",
        "title": "A Survey of Sim-to-Real Transfer Techniques Applied to Reinforcement Learning for Bioinspired Robots"
      },
      {
        "paperId": "d4da158a6883d5c56824de3bbf9ec060591b7008",
        "title": "Learning to Navigate in a VUCA Environment: Hierarchical Multi-expert Approach"
      },
      {
        "paperId": "c68c93a623a44d300cdf1877841c83b504da4f72",
        "title": "Autonomous Exploration of Mobile Robots via Deep Reinforcement Learning Based on Spatiotemporal Information on Graph"
      },
      {
        "paperId": "24e291fba483071eeeb49ed534fcc76b95142ee5",
        "title": "CNN-G: Convolutional Neural Network Combined With Graph for Image Segmentation With Theoretical Analysis"
      },
      {
        "paperId": "fa583b02b83e81982da3bea64fcb6899d9c5cd20",
        "title": "UNMAS: Multiagent Reinforcement Learning for Unshaped Cooperative Scenarios"
      },
      {
        "paperId": "68d83d822cf0d3f93ee5e7ad44f718213c0c1253",
        "title": "Auxiliary Heuristics for Frontier Based Planners"
      },
      {
        "paperId": "e57e5f638d47fe2930f10326979a1f8c830e348a",
        "title": "Catastrophic Interference in Reinforcement Learning: A Solution Based on Context Division and Knowledge Distillation"
      },
      {
        "paperId": "06ff13295b49df4da36363198065173b8b4807c5",
        "title": "A Deep Reinforcement Learning Method with Action Switching for Autonomous Navigation"
      },
      {
        "paperId": "02c01b727ff8c0b086c30d2b0f3245b3daf95767",
        "title": "Learning Robot Exploration Strategy With 4D Point-Clouds-Like Information as Observations"
      },
      {
        "paperId": "42d03c691241e48a2eb060c4a17d4334ed8fdf30",
        "title": "Deep Reinforcement Learning-based UAV Navigation and Control: A Soft Actor-Critic with Hindsight Experience Replay Approach"
      },
      {
        "paperId": "002e3c1fdedf8a77d7e97b19a2d02d8ca21f7520",
        "title": "Autonomous exploration and navigation of mine countermeasures USV in complex unknown environment"
      },
      {
        "paperId": "c8d71108c906bc3576c9f144196f9a4740de0ec7",
        "title": "An Overview of Federated Learning at the Edge and Distributed Ledger Technologies for Robotic and Autonomous Systems"
      },
      {
        "paperId": "4ef678ab88fdcd0eeaff8b36cc8ecd74bc85115e",
        "title": "Active Mapping and Robot Exploration: A Survey"
      },
      {
        "paperId": "b0ba430fb4fdd9ebaf2b1e5e333cbdc95b5fd82d",
        "title": "Pose-guided End-to-end Visual Navigation"
      },
      {
        "paperId": "9b3caaebd64a29ea068df9923603c30d6cdec85d",
        "title": "BNAS: Efficient Neural Architecture Search Using Broad Scalable Architecture"
      },
      {
        "paperId": "5d25aaf23057ec209bbe20ab2ddb1a4d8d7c7032",
        "title": "Intelligent Exploration Approaches Based on Utility Functions Optimization for Multi-Agent Environment Applications"
      },
      {
        "paperId": "4d0fe6de29720fc358348ca9f5d48edf7ece6518",
        "title": "OPT-GAN: A Broad-Spectrum Global Optimizer for Black-Box Problems by Learning Distribution"
      },
      {
        "paperId": "fc7630bfe66cbf3d25ae99c30150df4196b7e8b0",
        "title": "Searching and Tracking an Unknown Number of Targets: A Learning-Based Method Enhanced with Maps Merging"
      },
      {
        "paperId": "e810f688efb42f132c77d6216a7721ee67b40272",
        "title": "Balancing Excitation and Inhibition of Spike Neuron Using Deep Q Network (DQN)"
      },
      {
        "paperId": "ad1680efbfd58d88e5f9e96179c31917d49dae08",
        "title": "Adaptive Dynamic Programming for Control: A Survey and Recent Advances"
      },
      {
        "paperId": "d50dc64f2ba8be1733a8daad143874daec06b00f",
        "title": "Curriculum-Based Deep Reinforcement Learning for Quantum Control"
      },
      {
        "paperId": "95dccab48b1299183356a23c411f08f158ff08d3",
        "title": "High-Speed Robot Navigation using Predicted Occupancy Maps"
      },
      {
        "paperId": "7d18f4de570994e8626a148d3a35730f4ca663bb",
        "title": "Device Placement Optimization for Deep Neural Networks via One-shot Model and Reinforcement Learning"
      },
      {
        "paperId": "16b03f654b259eaeb744b185191490a953af3d5f",
        "title": "Learning of Long-Horizon Sparse-Reward Robotic Manipulator Tasks With Base Controllers"
      },
      {
        "paperId": "37151639a0f6a59a4d21821c10520459a1c20c91",
        "title": "Instance Weighted Incremental Evolution Strategies for Reinforcement Learning in Dynamic Environments"
      },
      {
        "paperId": "f18cd315123f59f6d168188019071b582bf409b6",
        "title": "Dynamic Horizon Value Estimation for Model-based Reinforcement Learning"
      },
      {
        "paperId": "f76861735c0fdae618f2308e473fe44abea8b396",
        "title": "Faster Gradient-based NAS Pipeline Combining Broad Scalable Architecture with Confident Learning Rate"
      },
      {
        "paperId": "bcf05632914146831ce52a5c56322b2127f8c89d",
        "title": "Collaborative Multi-Robot Systems for Search and Rescue: Coordination and Perception"
      },
      {
        "paperId": "8885ca43cab3a72d939ef626c8cfafbe3273aac9",
        "title": "Reinforcement Learning-Based Optimal Tracking Control of an Unknown Unmanned Surface Vehicle"
      },
      {
        "paperId": "a365d7819697984d8e7e2ae65ec5a1f15f4e3e25",
        "title": "Lifelong Incremental Reinforcement Learning With Online Bayesian Inference"
      },
      {
        "paperId": "be6f591cfa15a23a13ea09b336f794601ba5eeca",
        "title": "Towards Embodied Scene Description"
      },
      {
        "paperId": "62d70c2db1cfcae533180d3565740bd737e32f5c",
        "title": "BiFNet: Bidirectional Fusion Network for Road Segmentation"
      },
      {
        "paperId": "99622515c5656af37c85f3c5d1eac20f47cb56df",
        "title": "Map-less Navigation: A Single DRL-based Controller for Robots with Varied Dimensions"
      },
      {
        "paperId": "d34e7ba25e9df2418b9a39de0d3c4afd8aa1d3a8",
        "title": "BNAS:An Efficient Neural Architecture Search Approach Using Broad Scalable Architecture"
      },
      {
        "paperId": "0c39349e1c495b911077abb3e5aebc29a590f891",
        "title": "Perception and Navigation in Autonomous Systems in the Era of Learning: A Survey"
      },
      {
        "paperId": "08068cb09f9bed2048480918f53ab220e30f988b",
        "title": "DIBNN: A Dual-Improved-BNN Based Algorithm for Multi-Robot Cooperative Area Search in Complex Obstacle Environments"
      },
      {
        "paperId": "22ac22cd53930df1644391a5652e5e521bfb307e",
        "title": "Hierarchical multi-robot navigation and formation in unknown environments via deep reinforcement learning and distributed optimization"
      },
      {
        "paperId": "22b8f760584c65e3abb807e7e14524365577e112",
        "title": "A Controllable Agent by Subgoals in Path Planning Using Goal-Conditioned Reinforcement Learning"
      },
      {
        "paperId": "f77662ecbeb77c50103d988efe56af25a10e37d1",
        "title": "Hierarchical path planner for unknown space exploration using reinforcement learning-based intelligent frontier selection"
      },
      {
        "paperId": "016a9b8cc59b92f5226b2a756f9b575a24f4ea24",
        "title": "Adaptive Noise-based Evolutionary Reinforcement Learning With Maximum Entropy"
      },
      {
        "paperId": "b67152b04c255b563ecb59ef55223405a2821f04",
        "title": "Edge Technologies for Disaster Management: A Survey of Social Media and Artificial Intelligence Integration"
      },
      {
        "paperId": "dabfc799907ba9fe4fbd7eef79019c8600585e34",
        "title": "Mobile Service Robot Path Planning Using Deep Reinforcement Learning"
      },
      {
        "paperId": "637fc98b8da42af6b6ed32c62d6a66636b18892b",
        "title": "ComSD: Balancing Behavioral Quality and Diversity in Unsupervised Skill Discovery"
      },
      {
        "paperId": "148100b6b4826efac0cda88d1e82061c614f02a9",
        "title": "Reinforcement Learning Approach to Clear Paths of Robots in Elevator Environment"
      },
      {
        "paperId": "7b751ac7dc5da5080e1bfb8510b90490c5f75da0",
        "title": "TVENet: Transformer-based Visual Exploration Network for Mobile Robot in Unseen Environment"
      },
      {
        "paperId": "33a3defface2cb3295811d8717945cd3ebb7f5eb",
        "title": "Motion Planning for Mobile Robots\u2014Focusing on Deep Reinforcement Learning: A Systematic Review"
      },
      {
        "paperId": "fb2036224b4d536828d0156ab8c1f2f6bc88bb07",
        "title": "A Novel Behavioral Strategy for RoboCode Platform Based on Deep Q-Learning"
      },
      {
        "paperId": "0bd11a0a5c7adfc2bc589ab639c7bd7ca18856c4",
        "title": "A Deep Learning Approach for the Mobile-Robot Motion Control System"
      },
      {
        "paperId": "e570f5a8b3e82f2c56749d2db6fc90c3f7c2b268",
        "title": "OPT-GAN: Black-Box Global Optimization via Generative Adversarial Nets"
      },
      {
        "paperId": "f5c0c5adb5c3f14f9bf5ac95fe29662823a6f990",
        "title": "Speeding-Up Action Learning in a Social Robot With Dyna-Q+: A Bioinspired Probabilistic Model Approach"
      },
      {
        "paperId": "31bef5154feab4dde7e30759e08c36d7001fec9f",
        "title": "DDPG-Based Decision-Making Strategy of Adaptive Cruising for Heavy Vehicles Considering Stability"
      },
      {
        "paperId": "8647a1fc37e8d6087ba54f129f05eb38f6ba6968",
        "title": "Waypoint Mobile Robot Exploration Based on Biologically Inspired Algorithms"
      },
      {
        "paperId": "7ff09f598f3fa3c171e791a741de2802c1be0253",
        "title": "Collaborative Multi-Robot Search and Rescue: Planning, Coordination, Perception, and Active Vision"
      },
      {
        "paperId": "ba3719cb73046896dc3af3f709436e417b100571",
        "title": "Balancing Both Diversity and Exploration in Unsupervised Discovery of Complex Behaviors"
      },
      {
        "paperId": "12a996a5d6238fcdc491c3272387644ed9e435e0",
        "title": "Selective Learning for Sample-Ef\ufb01cient Training in Multi-Agent Sparse Reward Tasks"
      },
      {
        "paperId": "023ffe9d48a8ef4b36203b7649b311b45f88682b",
        "title": "TORINO Repository ISTITUZIONALE"
      }
    ],
    "score": 41.2
  },
  {
    "id": "12075ea34f5fbe32ec5582786761ab34d401209b",
    "title": "Exploration in Deep Reinforcement Learning: From Single-Agent to Multiagent Domain",
    "authors": [
      "Tianpei Yang",
      "Hongyao Tang",
      "Chenjia Bai",
      "Jinyi Liu",
      "Jianye Hao",
      "Zhaopeng Meng",
      "Peng Liu",
      "Zhen Wang"
    ],
    "year": 2021,
    "citationCount": 120,
    "abstract": "Deep reinforcement learning (DRL) and deep multiagent reinforcement learning (MARL) have achieved significant success across a wide range of domains, including game artificial intelligence (AI), autonomous vehicles, and robotics. However, DRL and deep MARL agents are widely known to be sample inefficient that millions of interactions are usually needed even for relatively simple problem settings, thus preventing the wide application and deployment in real-industry scenarios. One bottleneck challenge behind is the well-known exploration problem, i.e., how efficiently exploring the environment and collecting informative experiences that could benefit policy learning toward the optimal ones. This problem becomes more challenging in complex environments with sparse rewards, noisy distractions, long horizons, and nonstationary co-learners. In this article, we conduct a comprehensive survey on existing exploration methods for both single-agent RL and multiagent RL. We start the survey by identifying several key challenges to efficient exploration. Then, we provide a systematic survey of existing approaches by classifying them into two major categories: uncertainty-oriented exploration and intrinsic motivation-oriented exploration. Beyond the above two main branches, we also include other notable exploration methods with different ideas and techniques. In addition to algorithmic analysis, we provide a comprehensive and unified empirical comparison of different exploration methods for DRL on a set of commonly used benchmarks. According to our algorithmic and empirical investigation, we finally summarize the open problems of exploration in DRL and deep MARL and point out a few future directions.",
    "url": "https://www.semanticscholar.org/paper/12075ea34f5fbe32ec5582786761ab34d401209b",
    "pdf_url": "https://arxiv.org/pdf/2109.06668.pdf",
    "venue": "IEEE Transactions on Neural Networks and Learning Systems",
    "publicationDate": "2021-09-14",
    "externalIds": {
      "ArXiv": "2109.06668",
      "DBLP": "journals/tnn/HaoYTBLMLW24",
      "DOI": "10.1109/TNNLS.2023.3236361",
      "CorpusId": 250493352,
      "PubMed": "37021882"
    },
    "references": [
      {
        "paperId": "da1e404ae194dcbcce198f5335f49f090403bbd7",
        "title": "Exploration in Deep Reinforcement Learning: A Survey"
      },
      {
        "paperId": "d8da5d28807fe5f8af2e6942c67738bd9f12a1ec",
        "title": "Safe Reinforcement Learning by Imagining the Near Future"
      },
      {
        "paperId": "0ba7f2e592dda172bc3d07f88cdcf2a57830deec",
        "title": "Towards Safe Reinforcement Learning with a Safety Editor Policy"
      },
      {
        "paperId": "a4c72445eeb95cca44a889247b18ffebef54331a",
        "title": "DOPE: Doubly Optimistic and Pessimistic Exploration for Safe Reinforcement Learning"
      },
      {
        "paperId": "e227f71d7d84ed9a76278510a46f9b7db286ba92",
        "title": "Episodic Multi-agent Reinforcement Learning with Curiosity-Driven Exploration"
      },
      {
        "paperId": "b5bb16e1de8bce93ab78bf578090d1d64ebd25c2",
        "title": "Dynamic Bottleneck for Robust Self-Supervised Exploration"
      },
      {
        "paperId": "c73157e860991103ea56a336ed241c74b5ac4a6f",
        "title": "HyAR: Addressing Discrete-Continuous Action Reinforcement Learning via Hybrid Action Representation"
      },
      {
        "paperId": "feb7f993c402e2663d20bbafa83c11e6db3dfe6b",
        "title": "Cooperative Exploration for Multi-Agent Deep Reinforcement Learning"
      },
      {
        "paperId": "1c389140489e5cadbf2725e7f4dcbb076c1b87b7",
        "title": "Adaptive Multigradient Recursive Reinforcement Learning Event-Triggered Tracking Control for Multiagent Systems"
      },
      {
        "paperId": "c0ee9dc3f231db21f8d12401df6709e6f318c22b",
        "title": "End-to-End Hierarchical Reinforcement Learning With Integrated Subgoal Discovery"
      },
      {
        "paperId": "42cc5f45d59a694813e990b29631be4e32bc6453",
        "title": "MADE: Exploration via Maximizing Deviation from Explored Regions"
      },
      {
        "paperId": "6d2f554fbd24715c2268d64f90b53a5f19044774",
        "title": "Principled Exploration via Optimistic Bootstrapping and Backward Induction"
      },
      {
        "paperId": "1b7910f83d23a4e513eecc8335a8d31b9919d76e",
        "title": "A Reinforcement Learning-Based Vehicle Platoon Control Strategy for Reducing Energy Consumption in Traffic Oscillations"
      },
      {
        "paperId": "8962c86cd1aad408a662f9268431c9bbdf7fdf30",
        "title": "Co-Imitation Learning without Expert Demonstration"
      },
      {
        "paperId": "a50f7bcf8a998f3de11bc085b0f4dea32be19783",
        "title": "Reinforcement Learning with Prototypical Representations"
      },
      {
        "paperId": "3098e236a15908743cea2e83ca07d1992e7c0f6c",
        "title": "DFAC Framework: Factorizing the Value Function via Quantile Mixture for Multi-Agent Distributional Q-Learning"
      },
      {
        "paperId": "2da671acc2398b24d40be54c0070aa047cbe0832",
        "title": "Semicentralized Deep Deterministic Policy Gradient in Cooperative StarCraft Games"
      },
      {
        "paperId": "ff3f7fbebe2159bb97eb79686eaf338ded8fc325",
        "title": "Self-Imitation Advantage Learning"
      },
      {
        "paperId": "a334f9897a330abddf99cfec0b5a70f751e9497b",
        "title": "Conservative Safety Critics for Exploration"
      },
      {
        "paperId": "a15d706d3ceed54466b25fe8c75136249991c4d3",
        "title": "Episodic Self-Imitation Learning with Hindsight"
      },
      {
        "paperId": "5b7258c32425adafb5f5bab77b8ed90a7d255d70",
        "title": "Variational Dynamic for Self-Supervised Exploration in Deep Reinforcement Learning"
      },
      {
        "paperId": "16e83f3f0f78ceb203746eeb88f1f5aae9ba3092",
        "title": "Deep reinforcement learning: a survey"
      },
      {
        "paperId": "40e398067833bef8780a4e4f611447691efc9256",
        "title": "Self-Imitation Learning in Sparse Reward Settings"
      },
      {
        "paperId": "54639ad9b8c616e69310b4d177e202d7571acca1",
        "title": "Learning Automata-Based Multiagent Reinforcement Learning for Optimization of Cooperative Tasks"
      },
      {
        "paperId": "d4d86f4a632d899ba5f511c1d0e7a7fbd44288c8",
        "title": "Novelty Search in representational space for sample efficient exploration"
      },
      {
        "paperId": "c86e90b4e934c418e52c3b03ac9bea97acb71d66",
        "title": "Robot Motor Skill Transfer With Alternate Learning in Two Spaces"
      },
      {
        "paperId": "48a4fcf3c8aea0ae464bb124872dc5672b44ebbb",
        "title": "Multi-Agent Safe Planning with Gaussian Processes"
      },
      {
        "paperId": "3322923efe97edd594ee35d755b7a5e19a4cd831",
        "title": "Fuzzy Approximation-Based Finite-Time Control for a Robot With Actuator Saturation Under Time-Varying Constraints of Work Space"
      },
      {
        "paperId": "6fb1b9628e9fc14a8c387657e50203b0fcdad8c6",
        "title": "Active World Model Learning in Agent-rich Environments with Progress Curiosity"
      },
      {
        "paperId": "324effa5a7c737257b675f85bd93d777fc486878",
        "title": "SUNRISE: A Simple Unified Framework for Ensemble Learning in Deep Reinforcement Learning"
      },
      {
        "paperId": "629d0ce250581471f07083bbab95f23623b00201",
        "title": "Responsive Safety in Reinforcement Learning by PID Lagrangian Methods"
      },
      {
        "paperId": "57ffba5ea5e602b5365752e19a6f85a716db1d8b",
        "title": "Bandit Algorithms"
      },
      {
        "paperId": "0655bd7f2722b9ffe69613ac2c11bcb9b5eb9aa3",
        "title": "Verifiably safe exploration for end-to-end reinforcement learning"
      },
      {
        "paperId": "aca6d5f3866372a4506cf15773ae298f18c3f453",
        "title": "A comprehensive survey of anomaly detection techniques for high dimensional big data"
      },
      {
        "paperId": null,
        "title": "Potential Driven Reinforcement Learning for Hard Exploration Tasks"
      },
      {
        "paperId": "7d3699cf0e8a822cd8ae99b8eeed146930d24a0f",
        "title": "On Optimism in Model-Based Reinforcement Learning"
      },
      {
        "paperId": "e1527da2e9d1df57691fa2e963f667d1f430457c",
        "title": "Reinforcement Learning With Task Decomposition for Cooperative Multiagent Systems"
      },
      {
        "paperId": "3a07e0157a0c8b6321496b01c299319cadd5ec15",
        "title": "Efficient Model-Based Reinforcement Learning through Optimistic Policy Search and Planning"
      },
      {
        "paperId": "178dd0e014b8c315c8e91af2b6115611b7d56b0b",
        "title": "Self-Imitation Learning via Generalized Lower Bound Q-learning"
      },
      {
        "paperId": "3a71c306eb6232658c9e5fd48aed1ef3befe5fbe",
        "title": "Planning to Explore via Self-Supervised World Models"
      },
      {
        "paperId": "5f81689daeda82f0b1e08da37573032b657f0c47",
        "title": "Conservative Uncertainty Estimation By Fitting Prior Networks"
      },
      {
        "paperId": "9ec0bd60df62b997bb371e65d254434f8851ea9b",
        "title": "Projection-Based Constrained Policy Optimization"
      },
      {
        "paperId": "2b2735cffb0d2321a456363880ff5671e80df4cb",
        "title": "On Bonus Based Exploration Methods In The Arcade Learning Environment"
      },
      {
        "paperId": "616ac6772508e72084360158078058dfa8bda7e7",
        "title": "First return, then explore"
      },
      {
        "paperId": "9efb64f20ab1f157ca9f4050d4aaacf6c3f9b2b2",
        "title": "CURL: Contrastive Unsupervised Representations for Reinforcement Learning"
      },
      {
        "paperId": "9651e1987a39d100dc0f6696a2077199e5075ea2",
        "title": "Agent57: Outperforming the Atari Human Benchmark"
      },
      {
        "paperId": "6d5acdfa087409a2dd59c425cb6c68030f90b33b",
        "title": "Exploring Unknown States with Action Balance"
      },
      {
        "paperId": "db0be3a0c2d554092c9880755ac86e1052235f17",
        "title": "Efficient exploration of zero-sum stochastic games"
      },
      {
        "paperId": "3134dc06e80bf955cc72985ef8b3e32d4bc6183b",
        "title": "Strategic Exploration in Reinforcement Learning - New Algorithms and Learning Guarantees"
      },
      {
        "paperId": "086159600bede14e00f96043c733d4f3b45855aa",
        "title": "Never Give Up: Learning Directed Exploration Strategies"
      },
      {
        "paperId": "4dc5be46f267e8fa0213c12d374dab5e46377d28",
        "title": "Intrinsic Motivation for Encouraging Synergistic Behavior"
      },
      {
        "paperId": "39b2551f109fd0495abfca1ab0bb81311c1b3996",
        "title": "Explore, Discover and Learn: Unsupervised Discovery of State-Covering Skills"
      },
      {
        "paperId": "d99028fbc40e0bb11b2c8e5a0d844d2ecfdeba4a",
        "title": "Ready Policy One: World Building Through Active Learning"
      },
      {
        "paperId": "d0cf6bc0d44e2af3135d238a54c58dcd5c2d4e84",
        "title": "Provably Efficient Exploration in Policy Optimization"
      },
      {
        "paperId": "d3c3628260695a6174ab9d9fdcd6106997023ec4",
        "title": "What Can Learned Intrinsic Rewards Capture?"
      },
      {
        "paperId": "0cc956565c7d249d4197eeb1dbab6523c648b2c9",
        "title": "Dream to Control: Learning Behaviors by Latent Imagination"
      },
      {
        "paperId": "54d4a221db5a91a2487b1610374843fafff5a23d",
        "title": "Multi-Agent Reinforcement Learning: A Selective Overview of Theories and Algorithms"
      },
      {
        "paperId": "e30fa08b1ef8f3e7a2394a4467dc0eddcff04681",
        "title": "Keeping Your Distance: Solving Sparse Reward Tasks Using Self-Balancing Shaped Rewards"
      },
      {
        "paperId": "86687ad06378954f57cc01922b0369d97e75fd19",
        "title": "Frequentist Regret Bounds for Randomized Least-Squares Value Iteration"
      },
      {
        "paperId": "007fd689a806de99f053a0f5ef90aedb44f9ec7e",
        "title": "Better Exploration with Optimistic Actor-Critic"
      },
      {
        "paperId": "338367ca4d97f107dab9165b1f98f8e30de8a6b8",
        "title": "A New Framework for Multi-Agent Reinforcement Learning - Centralized Training and Exploration with Decentralized Execution via Policy Distillation"
      },
      {
        "paperId": "c38e624c8d176153a23ada19978f334c9ec94bae",
        "title": "MAVEN: Multi-Agent Variational Exploration"
      },
      {
        "paperId": "46106fcd08223540d080f674b26770c1fa8a52ff",
        "title": "Influence-Based Multi-Agent Exploration"
      },
      {
        "paperId": "a15aafd000e1c350a198e06dc846eadea7413e40",
        "title": "Learning Transferable Domain Priors for Safe Exploration in Reinforcement Learning"
      },
      {
        "paperId": "895735cace0de940aa647dbafc046b7f30316fe5",
        "title": "A survey on intrinsic motivation in reinforcement learning"
      },
      {
        "paperId": "b3e20a79d802baee478351d111e22e0e513ba376",
        "title": "Efficient Exploration with Self-Imitation Learning via Trajectory-Conditioned Policy"
      },
      {
        "paperId": "d423fa6cd0f4d088941d9fe4bebd834d0137c9b9",
        "title": "Provably Efficient Reinforcement Learning with Linear Function Approximation"
      },
      {
        "paperId": "ffb3886a253ff927bcc46b78e00409893865a68e",
        "title": "Dynamics-Aware Unsupervised Discovery of Skills"
      },
      {
        "paperId": "94c731fd8982a24867e69e585d7887b8762e4736",
        "title": "MULEX: Disentangling Exploitation from Exploration in Deep RL"
      },
      {
        "paperId": "2e01adc302623c68feea2cfd206a24c593ab27ba",
        "title": "Growing Action Spaces"
      },
      {
        "paperId": "ee87cf824562db74b8555637652b22aa30589343",
        "title": "Directed Exploration for Reinforcement Learning"
      },
      {
        "paperId": "cfec54f9208dcbb2c558aa7ff45f57be2533d046",
        "title": "Efficient Exploration via State Marginal Matching"
      },
      {
        "paperId": "d10d406a3bd115f8abb24a4d2c127caa502516b4",
        "title": "Exploration with Unreliable Intrinsic Reward in Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "5a22ce57b02c8aa446c793435a2235bbe6afbc65",
        "title": "Exploration via Hindsight Goal Generation"
      },
      {
        "paperId": "51fe965c579a689d1dc466f2313227a07eb22126",
        "title": "Safety Augmented Value Estimation From Demonstrations (SAVED): Safe Deep Model-Based RL for Sparse Cost Robotic Tasks"
      },
      {
        "paperId": "c01dd6bc93ed01124079938cae19aba040273fc6",
        "title": "Coordinated Exploration via Intrinsic Rewards for Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "801eb90352e603b1573a04a36804417d1c9496f0",
        "title": "Learning latent state representation for speeding up exploration"
      },
      {
        "paperId": "c60d789bf93dee52fc1e076c005cfb8385c84719",
        "title": "Curiosity-Bottleneck: Exploration By Distilling Task-Specific Novelty"
      },
      {
        "paperId": "87a40a2d506fec06d67dc8f14ce2c708a789a2b4",
        "title": "Self-Supervised Exploration via Disagreement"
      },
      {
        "paperId": "7f90db07c8ca85657cbc1a92e6e42c0389672e38",
        "title": "Dead-ends and Secure Exploration in Reinforcement Learning"
      },
      {
        "paperId": "d6285ff3dcb15c1da84dcbc98141be10ef0e8dd1",
        "title": "Estimating Risk and Uncertainty in Deep Reinforcement Learning"
      },
      {
        "paperId": "d569567c994b443ead90c709f0f1718ed6f0ec50",
        "title": "Maximum Entropy-Regularized Multi-Goal Reinforcement Learning"
      },
      {
        "paperId": "ec9cf6922aae97b7c728693e78bd5cb812cc4e1e",
        "title": "Learning Novel Policies For Tasks"
      },
      {
        "paperId": "f6b218453170edcbb51e49dd44ba2f83af53ef92",
        "title": "Distributional Reinforcement Learning for Efficient Exploration"
      },
      {
        "paperId": "bca27f37a748de0c3dfc7a03d4bfb7b9df84d59a",
        "title": "An Efficient Reachability-Based Framework for Provably Safe Autonomous Navigation in Unknown Environments"
      },
      {
        "paperId": "e739403b08e22f4846e17fe8b2a1a9eb7b568d62",
        "title": "Learning Action Representations for Self-supervised Visual Exploration"
      },
      {
        "paperId": "4909a6fec9e39dfe38856b3d54b69dbad6100457",
        "title": "Reachability-Based Safety Guarantees using Efficient Initializations"
      },
      {
        "paperId": "048459fcf6befce76d277a31ead3f58e1b2de32d",
        "title": "Scheduled Intrinsic Drive: A Hierarchical Take on Intrinsically Motivated Exploration"
      },
      {
        "paperId": "ec80965b72de5076a183026ba182068f6e0a928a",
        "title": "Deep Multi-Agent Reinforcement Learning with Discrete-Continuous Hybrid Action Spaces"
      },
      {
        "paperId": "7388826b5ee00efe17cb7f19a623d9b5e955ae70",
        "title": "Skew-Fit: State-Covering Self-Supervised Reinforcement Learning"
      },
      {
        "paperId": "82055eed2ba0d7156a54c586249742c848e5d565",
        "title": "The StarCraft Multi-Agent Challenge"
      },
      {
        "paperId": "ee248883b124a80bdbbc686e906145ccdeadc834",
        "title": "Competitive Experience Replay"
      },
      {
        "paperId": "4c54efd3b9ce84f4268b165f5896c0692a50bcd1",
        "title": "Learning Action Representations for Reinforcement Learning"
      },
      {
        "paperId": "c520bf47db3360ae3a52219771390a354ed8a91f",
        "title": "Go-Explore: a New Approach for Hard-Exploration Problems"
      },
      {
        "paperId": "74469a8da66694787da45a092006f4de6ac8ee68",
        "title": "Likelihood Quantile Networks for Coordinating Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "5d4ebbaa1ef8feeac885e2869f45c0276c18834f",
        "title": "Learning Montezuma's Revenge from a Single Demonstration"
      },
      {
        "paperId": "3f67003e17f4e64493600d16b646dea280e55cca",
        "title": "On Infusing Reachability-Based Safety Assurance within Probabilistic Planning Frameworks for Human-Robot Vehicle Interactions"
      },
      {
        "paperId": "3995dab2b1c3f0a036352e47a324b0edb8542b4f",
        "title": "Single-Model Uncertainties for Deep Learning"
      },
      {
        "paperId": "e0121c1d2dc5c8cc77f4b1570e28f2443ece2a8f",
        "title": "Social Influence as Intrinsic Motivation for Multi-Agent Deep Reinforcement Learning"
      },
      {
        "paperId": "72a75aeab2c918394dc9af2408fd8e1076ae39ac",
        "title": "Successor Uncertainties: exploration and uncertainty in temporal difference learning"
      },
      {
        "paperId": "3f43f08611cbcfba62bb9e0c5339c2a8f0cc3e4b",
        "title": "A survey and critique of multiagent deep reinforcement learning"
      },
      {
        "paperId": "328233806be18fe15176ce71debaecd95e771fcf",
        "title": "Parametrized Deep Q-Networks Learning: Reinforcement Learning with Discrete-Continuous Hybrid Action Space"
      },
      {
        "paperId": "51ecd565f8a394b1907b3d37722435a573ce7b72",
        "title": "EMI: Exploration with Mutual Information"
      },
      {
        "paperId": "6ab8aca1f727e379632292e1ec4a24ea2739cf89",
        "title": "Model-Based Active Exploration"
      },
      {
        "paperId": "4cb3fd057949624aa4f0bbe7a6dcc8777ff04758",
        "title": "Exploration by Random Network Distillation"
      },
      {
        "paperId": "8ede7ddf99986d69562455bc8d69222fc3e27350",
        "title": "Recurrent Experience Replay in Distributed Reinforcement Learning"
      },
      {
        "paperId": "c46d80f83813fba0e8363a0ab36a19fba062540e",
        "title": "Learning Actionable Representations with Goal-Conditioned Policies"
      },
      {
        "paperId": "bf604ae3ddd5adec55554921b37f04035b7350a7",
        "title": "Contingency-Aware Exploration in Reinforcement Learning"
      },
      {
        "paperId": "fdfeeb14bbde2ab31b18e56b92d362dcd1b14f71",
        "title": "Episodic Curiosity through Reachability"
      },
      {
        "paperId": "a25b645c3d24f91164230a0ac5bb2d4ec88c1538",
        "title": "Information-Directed Exploration for Deep Reinforcement Learning"
      },
      {
        "paperId": "a91c0ef920b87d8913f1559ca22b26c604b3fd20",
        "title": "DHER: Hindsight Experience Replay for Dynamic Goals"
      },
      {
        "paperId": "ca14dce53be20d3d23d4f0db844a8389ab619db3",
        "title": "Large-Scale Study of Curiosity-Driven Learning"
      },
      {
        "paperId": "9f67b3edc67a35c884bd532a5e73fa3a7f3660d8",
        "title": "Count-Based Exploration with the Successor Representation"
      },
      {
        "paperId": "5f8645a8474017f52e4d1d4b4a0ca95d8b39f66f",
        "title": "Variational Option Discovery Algorithms"
      },
      {
        "paperId": "3aadab924520c58be81781aafd51e6807e9c4576",
        "title": "Visual Reinforcement Learning with Imagined Goals"
      },
      {
        "paperId": "03cc81e98942bdafd994af7a1d1e62a68ff8b682",
        "title": "Is Q-learning Provably Efficient?"
      },
      {
        "paperId": "c27078d60737ea10e8ca4f05acd114fef29c8276",
        "title": "A Tutorial on Bayesian Optimization"
      },
      {
        "paperId": "d397f4cf400f6ffcb1b8e3db27bb75966a0513cf",
        "title": "Self-Imitation Learning"
      },
      {
        "paperId": "f802802b3af5a22b79ac65d033ba3cbee33da91b",
        "title": "Randomized Prior Functions for Deep Reinforcement Learning"
      },
      {
        "paperId": "5fd3ce235f5fcebd3d2807f710b060add527183b",
        "title": "Deep Curiosity Search: Intra-Life Exploration Improves Performance on Challenging Deep Reinforcement Learning Problems"
      },
      {
        "paperId": "cb7c479a36520da1caeeec67db10772351a390c6",
        "title": "Reward Constrained Policy Optimization"
      },
      {
        "paperId": "653bdfb3c35621ee04ee5d5253dc7e3a422d69e1",
        "title": "Learning Self-Imitating Diverse Policies"
      },
      {
        "paperId": "7da97ba0cc89b90a2e95c94b215e91328333f6a8",
        "title": "Scalable Coordinated Exploration in Concurrent Reinforcement Learning"
      },
      {
        "paperId": "35271d36cb20bf8d716e79c9dd15d738d955a931",
        "title": "On Learning Intrinsic Rewards for Policy Gradient Methods"
      },
      {
        "paperId": "86430b50921c7aecb212b4cd0a43a73b091cf95f",
        "title": "Learning to Navigate in Cities Without a Map"
      },
      {
        "paperId": "ffc211476f2e40e79466ffc198c919a97da3bb76",
        "title": "QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "bf8d58faf972ad0a1026c0a7c5577c07996ef3a7",
        "title": "Hierarchical Deep Reinforcement Learning for Continuous Action Control"
      },
      {
        "paperId": "4b864f3f052fe3e9c5e6a449f243e5bf01ac4790",
        "title": "A Multi-Objective Deep Reinforcement Learning Framework"
      },
      {
        "paperId": "4debb99c0c63bfaa97dd433bc2828e4dac81c48b",
        "title": "Addressing Function Approximation Error in Actor-Critic Methods"
      },
      {
        "paperId": "5b01eaef54a653ba03ddd5a978690380fbc19bfc",
        "title": "Diversity is All You Need: Learning Skills without a Reward Function"
      },
      {
        "paperId": "89e9c05ea3b45628beba8134fd5c873dd55003e8",
        "title": "DORA The Explorer: Directed Outreaching Reinforcement Action-Selection"
      },
      {
        "paperId": "c0d96d1ea69855a5a3abf614f17095c29b3339a4",
        "title": "Semi-parametric Topological Memory for Navigation"
      },
      {
        "paperId": "601a2d349fc26d7b82f905e924e2f91b0ac4b310",
        "title": "Distributed Prioritized Experience Replay"
      },
      {
        "paperId": "3f5eefd759da6e85feb55134f5ad7b1f4af8ee3d",
        "title": "Efficient Exploration Through Bayesian Deep Q-Networks"
      },
      {
        "paperId": "7d475bac8987969c4e695bfc41661cce5ea2d545",
        "title": "Coordinated Exploration in Concurrent Reinforcement Learning"
      },
      {
        "paperId": "4e1cff5dee4671db7138a938469a2d062cb5f025",
        "title": "Information Directed Sampling and Bandits with Heteroscedastic Noise"
      },
      {
        "paperId": "7f567df97dc7e099d96e6c590ddf5aef8c5b11c4",
        "title": "Safe Exploration in Continuous Action Spaces"
      },
      {
        "paperId": "6f40b8d2e73ec0bcc4d12ec2c7c8387d53c7727b",
        "title": "Efficient exploration with Double Uncertain Value Networks"
      },
      {
        "paperId": "1fa1f04b80f057e477549e6b9798fab7c7e57db5",
        "title": "Hindsight policy gradients"
      },
      {
        "paperId": "fe3e91e40a950c6b6601b8f0a641884774d949ae",
        "title": "Distributional Reinforcement Learning with Quantile Regression"
      },
      {
        "paperId": "6e745266a5c85980e75f9d637d4d23cfc030cfaf",
        "title": "Uncertainty-driven Imagination for Continuous Deep Reinforcement Learning"
      },
      {
        "paperId": "3b290ffa1f4f8226e326f00984acecdfbe9e28bf",
        "title": "Revisiting the Arcade Learning Environment: Evaluation Protocols and Open Problems for General Agents"
      },
      {
        "paperId": "bdf6572b67a6c5d8aacd39e1826db2c5c8f85716",
        "title": "The Uncertainty Bellman Equation and Exploration"
      },
      {
        "paperId": "498238a3bd5fd322fc3ce1572e33bbe3853a356f",
        "title": "Deep Reinforcement Learning: A Brief Survey"
      },
      {
        "paperId": "033cfd4f920a88f8809da45a5d498250147916ab",
        "title": "Coordinated Versus Decentralized Exploration In Multi-Agent Multi-Armed Bandits"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "30ff82cebce6fdc2957043c4085a426414474d78",
        "title": "Trial without Error: Towards Safe Reinforcement Learning via Human Intervention"
      },
      {
        "paperId": "c1f4ef741242d629d1f56e442a09a7ba29595a0e",
        "title": "A Distributional Perspective on Reinforcement Learning"
      },
      {
        "paperId": "c0cba3fbc696102ba38c9d754a1f22d9e9eb6366",
        "title": "A Tutorial on Thompson Sampling"
      },
      {
        "paperId": "429ed4c9845d0abd1f8204e1d7705919559bc2a2",
        "title": "Hindsight Experience Replay"
      },
      {
        "paperId": "3fab4d28be5ff73f42c8b307117aa8d680345ad9",
        "title": "Noisy Networks for Exploration"
      },
      {
        "paperId": "0f810eb4777fd05317951ebaa7a3f5835ee84cf4",
        "title": "Count-Based Exploration in Feature Space for Reinforcement Learning"
      },
      {
        "paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
        "title": "Attention is All you Need"
      },
      {
        "paperId": "7c3ece1ba41c415d7e81cfa5ca33a8de66efd434",
        "title": "Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments"
      },
      {
        "paperId": "142497432fe179ddb6ffe600c64a837ec6179550",
        "title": "Parameter Space Noise for Exploration"
      },
      {
        "paperId": "7a4193d0b042643a8bb9ec262ed7f9d509bdb12e",
        "title": "Constrained Policy Optimization"
      },
      {
        "paperId": "2b292ff89d808fba10579871591a22f1649cd039",
        "title": "Counterfactual Multi-Agent Policy Gradients"
      },
      {
        "paperId": "9d528f7e641c922bddd83f4af687806d685490d6",
        "title": "Feature Control as Intrinsic Motivation for Hierarchical Reinforcement Learning"
      },
      {
        "paperId": "225ab689f41cef1dc18237ef5dab059a49950abf",
        "title": "Curiosity-Driven Exploration by Self-Supervised Prediction"
      },
      {
        "paperId": "7586bdad167178c32271000b5914944bae9a6dc0",
        "title": "Posterior sampling for reinforcement learning: worst-case regret bounds"
      },
      {
        "paperId": "3167b590e47b08828555938d3126fde1bb3c038e",
        "title": "Stein Variational Policy Gradient"
      },
      {
        "paperId": "a441728f9fd6af1946368240162a72c2028c8cb1",
        "title": "Deep Exploration via Randomized Value Functions"
      },
      {
        "paperId": "8499a250422a3c66357367c8d5fa504de5424c59",
        "title": "Intrinsic Motivation and Automatic Curricula via Asymmetric Self-Play"
      },
      {
        "paperId": "d6757aedcb53142bc439ec64bfd0b056d99b1881",
        "title": "Surprise-Based Intrinsic Motivation for Deep Reinforcement Learning"
      },
      {
        "paperId": "12f67fb182bc934fc95ce97acff553d83e2ca72e",
        "title": "Count-Based Exploration with Neural Density Models"
      },
      {
        "paperId": "4eb38b3460606a4042b04fc52d0044ab948b4a17",
        "title": "EX2: Exploration with Exemplar Models for Deep Reinforcement Learning"
      },
      {
        "paperId": "9172cd6c253edf7c3a1568e03577db20648ad0c4",
        "title": "Reinforcement Learning with Deep Energy-Based Policies"
      },
      {
        "paperId": "9f1e9e56d80146766bc2316efbc54d8b770a23df",
        "title": "Deep Reinforcement Learning: An Overview"
      },
      {
        "paperId": "afb42208cc499ede10a65af0dbe598e08556370d",
        "title": "Variational Intrinsic Control"
      },
      {
        "paperId": "0fcb2034e31a2bc2f12a2b1363d0d77baf445fdf",
        "title": "#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning"
      },
      {
        "paperId": "885fe11ed7ab81c8609ccddb3e10f62577c04ab9",
        "title": "BBQ-Networks: Efficient Exploration in Deep Reinforcement Learning for Task-Oriented Dialogue Systems"
      },
      {
        "paperId": "d8686b657b61a37da351af2952aabd8b281de408",
        "title": "Successor Features for Transfer in Reinforcement Learning"
      },
      {
        "paperId": "0936352b78a52bc5d2b5e3f04233efc56664af51",
        "title": "Conditional Image Generation with PixelCNN Decoders"
      },
      {
        "paperId": "6e90fd78e8a3b98af3954aae5209703aa966603e",
        "title": "Unifying Count-Based Exploration and Intrinsic Motivation"
      },
      {
        "paperId": "2b10281297ee001a9f3f4ea1aa9bea6b638c27df",
        "title": "OpenAI Gym"
      },
      {
        "paperId": "a9beebe284b2c70895d4f51fe14fc50eda41fc60",
        "title": "Safe Exploration in Finite Markov Decision Processes with Gaussian Processes"
      },
      {
        "paperId": "a473f545318325ba23b7a6b477485d29777ba873",
        "title": "ViZDoom: A Doom-based AI research platform for visual reinforcement learning"
      },
      {
        "paperId": "317cd4522b1f4a6f889743578143bb8823623f8b",
        "title": "VIME: Variational Information Maximizing Exploration"
      },
      {
        "paperId": "3c3861c607fb79f3fbf79552018724617fc8ba1b",
        "title": "A Deep Hierarchical Approach to Lifelong Learning in Minecraft"
      },
      {
        "paperId": "bd10f1610b2029b402af7bab465f2e4eca98c1d8",
        "title": "Regret Analysis of the Anytime Optimally Confident UCB Algorithm"
      },
      {
        "paperId": "35b05886694ffaa0d5431b0510d2daa4560f37af",
        "title": "Reinforcement Learning of POMDPs using Spectral Methods"
      },
      {
        "paperId": "4b63e34276aa98d5345efa7fe09bb06d8a9d8f52",
        "title": "Deep Exploration via Bootstrapped DQN"
      },
      {
        "paperId": "69e76e16740ed69f4dc55361a3d319ac2f1293dd",
        "title": "Asynchronous Methods for Deep Reinforcement Learning"
      },
      {
        "paperId": "846aedd869a00c09b40f1f1f35673cb22bc87490",
        "title": "Mastering the game of Go with deep neural networks and tree search"
      },
      {
        "paperId": "4c05d7caa357148f0bbd61720bdd35f0bc05eb81",
        "title": "Dueling Network Architectures for Deep Reinforcement Learning"
      },
      {
        "paperId": "c6170fa90d3b2efede5a2e1660cb23e1c824f2ca",
        "title": "Prioritized Experience Replay"
      },
      {
        "paperId": "49da57cf2900cd50d64a6d63d45e1bccd454fcbb",
        "title": "Deep Reinforcement Learning in Parameterized Action Space"
      },
      {
        "paperId": "3b9732bb07dc99bde5e1f9f75251c6ea5039373e",
        "title": "Deep Reinforcement Learning with Double Q-Learning"
      },
      {
        "paperId": "024006d4c2a89f7acacc6e4438d156525b60a98f",
        "title": "Continuous control with deep reinforcement learning"
      },
      {
        "paperId": "e4257bc131c36504a04382290cbc27ca8bb27813",
        "title": "Action-Conditional Video Prediction using Deep Networks in Atari Games"
      },
      {
        "paperId": "2470fcf0f89082de874ac9133ccb3a8667dd89a8",
        "title": "Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models"
      },
      {
        "paperId": "909f736182963dcee2cb2e8c567e2069192b4749",
        "title": "Bootstrapped Thompson Sampling and Deep Exploration"
      },
      {
        "paperId": "f35de4f9b1a7c4d3fa96a0d2ab1bf8937671f6b6",
        "title": "Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning"
      },
      {
        "paperId": "340f48901f72278f6bf78a04ee5b01df208cc508",
        "title": "Human-level control through deep reinforcement learning"
      },
      {
        "paperId": "449532187c94af3dd3aa55e16d2c50f7854d2199",
        "title": "Trust Region Policy Optimization"
      },
      {
        "paperId": "687d0e59d5c35f022ce4638b3e3a6142068efc94",
        "title": "Deterministic Policy Gradient Algorithms"
      },
      {
        "paperId": "1389772b8a0f9c7fc43057f9da41a7d0ebf0308b",
        "title": "Generalization and Exploration via Randomized Value Functions"
      },
      {
        "paperId": "d01e3414ca706eda917576d947ece811b5cbcdde",
        "title": "Empowerment - an Introduction"
      },
      {
        "paperId": "789783016fb708abbc061790612ebe91273c05d3",
        "title": "(More) Efficient Reinforcement Learning via Posterior Sampling"
      },
      {
        "paperId": "76a3325fcb27a22cb5cef0e801a5e2b808c4648a",
        "title": "On Bayesian Upper Confidence Bounds for Bandit Problems"
      },
      {
        "paperId": "5a9ef216bf11f222438fff130c778267d39a9564",
        "title": "Practical Variational Inference for Neural Networks"
      },
      {
        "paperId": "751ee5e11cb826a315d8737f6a5f34423d95fae9",
        "title": "Barycenters in the Wasserstein Space"
      },
      {
        "paperId": "7f9505054e9573044ce5419e6ba02507c0e2265e",
        "title": "Epistemic planning for single- and multi-agent systems"
      },
      {
        "paperId": "0cafe2903b097fc042782c359cb231ea34ef7ed3",
        "title": "Near-optimal Regret Bounds for Reinforcement Learning"
      },
      {
        "paperId": "4aece8df7bd59e2fbfedbf5729bba41abc56d870",
        "title": "A Comprehensive Survey of Multiagent Reinforcement Learning"
      },
      {
        "paperId": "e8077729ef723d71a0b7492f2f271ae413b5a4d5",
        "title": "What is Intrinsic Motivation? A Typology of Computational Approaches"
      },
      {
        "paperId": "aebd8bab5cff769fed204dba35112e364a47e504",
        "title": "Bayesian surprise attracts human attention"
      },
      {
        "paperId": "dac78569eced221e2eab6ad375f9e33aea944bc8",
        "title": "Coordination in multiagent reinforcement learning: a Bayesian approach"
      },
      {
        "paperId": "103f6fe35033f9327611ddafde74a2b544072980",
        "title": "Using Confidence Bounds for Exploitation-Exploration Trade-offs"
      },
      {
        "paperId": "42bb0ac384fb87933be67f63b98d90a45d2fe6e9",
        "title": "Similarity estimation techniques from rounding algorithms"
      },
      {
        "paperId": "1e1d35136b1bf3b13ef6b53f6039f39d9ee820e3",
        "title": "Finite-time Analysis of the Multiarmed Bandit Problem"
      },
      {
        "paperId": "48cce5ee49facf75eeb12832c387452424b645dd",
        "title": "A Bayesian Framework for Reinforcement Learning"
      },
      {
        "paperId": "94066dc12fe31e96af7557838159bde598cb4f10",
        "title": "Policy Invariance Under Reward Transformations: Theory and Application to Reward Shaping"
      },
      {
        "paperId": "cb2bcb5ca8e8a53010bccf8feeb53c3c18066e83",
        "title": "Exploration Strategies for Model-based Learning in Multi-agent Systems: Exploration Strategies"
      },
      {
        "paperId": "b90b5b0cf05bc63f768f55322381f5cfbee6ce1c",
        "title": "Bayesian Q-Learning"
      },
      {
        "paperId": "c2e8806f0bd1d504bcb395ef1f6fe509a023a048",
        "title": "Improving Generalization for Temporal Difference Learning: The Successor Representation"
      },
      {
        "paperId": "4c9e36a0dbdc3e7b20e38bd288a804c05c77e9fa",
        "title": "Asymptotically efficient adaptive allocation rules"
      },
      {
        "paperId": "ee2cd1d17f833d3c157a1016a778c7c22af555a2",
        "title": "ON THE LIKELIHOOD THAT ONE UNKNOWN PROBABILITY EXCEEDS ANOTHER IN VIEW OF THE EVIDENCE OF TWO SAMPLES"
      },
      {
        "paperId": "da1ff1b01b2c036af68d04b045398b01a2adca92",
        "title": "DQMIX: A Distributional Perspective on Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "8d15f17ea8f807efe8801d236b7218b6659ac1d9",
        "title": "NovelD: A Simple yet Effective Exploration Criterion"
      },
      {
        "paperId": "bc4b2acfc8520a931211d39464eb9f7a52aeebcc",
        "title": "Safe Exploration for Constrained Reinforcement Learning with Provable Guarantees"
      },
      {
        "paperId": "9f873fecd24e0cfb800249a30ecd8e3b3155e709",
        "title": "Non-Crossing Quantile Regression for Distributional Reinforcement Learning"
      },
      {
        "paperId": "a28899f255c1ca35b6254adc1a0cd64fc20c2ce9",
        "title": "Memory Based Trajectory-conditioned Policies for Learning from Sparse Rewards"
      },
      {
        "paperId": "f083f8bd3c440074012091f7bf445353a79622e7",
        "title": "Propagating Uncertainty in Reinforcement Learning via Wasserstein Barycenters"
      },
      {
        "paperId": "7706a6aa39fedb5cff6c954d81a825b140216240",
        "title": "Curriculum-guided Hindsight Experience Replay"
      },
      {
        "paperId": "3f963a700f3d89aebefde51a7e895a145964af53",
        "title": "LIIR: Learning Individual Intrinsic Reward in Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "4d0f6a6ffcd6ab04732ff76420fd9f8a7bb649c3",
        "title": "Benchmarking Safe Exploration in Deep Reinforcement Learning"
      },
      {
        "paperId": "a181fb5a42ad8fe2cc27b5542fa40384e9a8d72c",
        "title": "Deep Variational Information Bottleneck"
      },
      {
        "paperId": null,
        "title": "engineering from Tianjin University, Tianjin"
      },
      {
        "paperId": "35e50b6aa2e0b288f1cd6b1e97bd33e978b53231",
        "title": "Combating Reinforcement Learning's Sisyphean Curse with Intrinsic Fear"
      },
      {
        "paperId": "c0f2c4104ef6e36bb67022001179887e6600d24d",
        "title": "A comprehensive survey on safe reinforcement learning"
      },
      {
        "paperId": "fae8bbf868681b83d91b2fec6c840d4d2b32005b",
        "title": "Intrinsic Motivation and Reinforcement Learning"
      },
      {
        "paperId": null,
        "title": "\u201cGaussian process optimization in the bandit setting: No regret and experimental design,\u201d"
      },
      {
        "paperId": "6695bc3c81a7f41864a36495bae8d906169a35a9",
        "title": "Multi-agent Reinforcement Learning in Stochastic Single and Multi-stage Games"
      },
      {
        "paperId": "4c915c1eecb217c123a36dc6d3ce52d12c742614",
        "title": "Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning"
      },
      {
        "paperId": "b55987b4cfff292dd121ee03c46b41f4f696136e",
        "title": "Intrinsic and Extrinsic Motivations: Classic Definitions and New Directions."
      },
      {
        "paperId": "97efafdb4a3942ab3efba53ded7413199f79c054",
        "title": "Reinforcement Learning: An Introduction"
      },
      {
        "paperId": "3edde3071399bde24bc2f77d872c491cfbc25dad",
        "title": "Estimating the mean and variance of the target probability distribution"
      },
      {
        "paperId": "572379c1d56e7e19422ae38218ee228c61aefb2f",
        "title": "Asymptotically Efficient Adaptive Allocation Rules"
      },
      {
        "paperId": null,
        "title": "We analyze the strengths and weaknesses of representative articles on exploration for DRL and deep MARL, along with their abilities in addressing different challenges"
      },
      {
        "paperId": null,
        "title": "degrees in computer science and technology"
      },
      {
        "paperId": null,
        "title": "a) Distributed training greatly improves exploration and learning performance generally"
      },
      {
        "paperId": null,
        "title": "b) Q -network with parametric noise brings stable performance improvement compared to a deterministic Q -network"
      },
      {
        "paperId": null,
        "title": "Results show"
      },
      {
        "paperId": null,
        "title": "Atari 2600 Archives: Montezuma\u2019s Revenge"
      }
    ],
    "cited_by": [
      {
        "paperId": "554d7f3caf533137abb6e1a52bbdcad021d9904c",
        "title": "A cooperative jamming mode adjustment method based on Multi-Agent reinforcement learning"
      },
      {
        "paperId": "00bdcfc16614df3febade2f75131684280dfc8c1",
        "title": "A systematic survey and meta-analysis of the segment anything model in remote sensing image processing: Challenges, advances, applications, and opportunities"
      },
      {
        "paperId": "d4cc1918e071be4148579a4984ee2a5ea682e31e",
        "title": "Learn the Ropes, Then Trust the Wins: Self-imitation with Progressive Exploration for Agentic Reinforcement Learning"
      },
      {
        "paperId": "6716528259cc5c6ba5074a7131dc3acb24368145",
        "title": "Diffusion Policies with Offline and Inverse Reinforcement Learning for Promoting Physical Activity in Older Adults Using Wearable Sensors"
      },
      {
        "paperId": "fe2327b6142877900a887a7506d3be051e3e9169",
        "title": "A unified data-driven approach under deep reinforcement learning with direct control responses for microgrid operations"
      },
      {
        "paperId": "e828365babaf3e6dcde42610b61e35de5f4214e4",
        "title": "Dynamic data partitioning strategy for distributed learning on heterogeneous edge system"
      },
      {
        "paperId": "5ace567c3572a682a74a4cded34a1505ef1dfacb",
        "title": "Entropy-driven multi agent deep reinforcement learning for resilient distribution networks: coordinating MESS and microgrids"
      },
      {
        "paperId": "7cefb07ebc9bb3ea7c667937d1016ba4758858df",
        "title": "HAEPO: History-Aggregated Exploratory Policy Optimization"
      },
      {
        "paperId": "9933d3339e221f52b226f1a4cc421c478c1866eb",
        "title": "Hierarchical Deep Deterministic Policy Gradient for Autonomous Maze Navigation of Mobile Robots"
      },
      {
        "paperId": "d76197c08465bd154ac6e38c3f3bd456e35cd28d",
        "title": "Application of LLMs to Multi-Robot Path Planning and Task Allocation"
      },
      {
        "paperId": "2b92f3967164529d489c690f0514c3f589b20ee4",
        "title": "Efficient Skill Discovery via Regret-Aware Optimization"
      },
      {
        "paperId": "6b3f8a2c83bf21f7aada810fc0e40fff3bc95e95",
        "title": "Wasserstein Barycenter Soft Actor-Critic"
      },
      {
        "paperId": "28b9c38ef461c391a94291aae3741d04d211e060",
        "title": "Curriculum Learning With Counterfactual Group Relative Policy Advantage For Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "8652f560860cdad3b6866f06614e64a04c66cee6",
        "title": "The Role of Environment Complexity in Exploration Strategies: A Comparative Analysis of CartPole and Lunar Lander"
      },
      {
        "paperId": "c2ee951899f1d8a1a9f316e4460863fb17c8d799",
        "title": "Robust adaptive maximum-entropy linear quadratic regulator"
      },
      {
        "paperId": "2da7b4c4be3c497966ec4db017c0994b028313b8",
        "title": "LSTM-Augmented DRL for Generalisable Energy Management of Hydrogen-Hybrid Ship Propulsion Systems"
      },
      {
        "paperId": "7ea90e1065eda27044a4c063af081f8e34d38dbf",
        "title": "Applications for Reinforcement Learning in Robotics: A Comprehensive Review"
      },
      {
        "paperId": "2804325883f8607677d8717e8297d21e3212069b",
        "title": "Learning to Coordinate With Different Teammates via Team Probing"
      },
      {
        "paperId": "8bef0d2975a2c65dc9ea422237f527becda87035",
        "title": "Leveraging Partial SMILES Validation Scheme for Enhanced Drug Design in Reinforcement Learning Frameworks"
      },
      {
        "paperId": "94b003e62d8dac70b2c68bad7c1e81e10ee85516",
        "title": "Multi-Agent Reinforcement Learning for Resources Allocation Optimization: A Survey"
      },
      {
        "paperId": "0c030a9a97308dde930768f268b6478976b56213",
        "title": "Rule-Based Assessment of Reinforcement Learning Practices Using Large Language Models"
      },
      {
        "paperId": "9b43c951b0e2573a62b7b7705e5f7608e4c891dd",
        "title": "Multi-Agent Deep Q-Network Based Ant Colony Optimization for Advanced Network Intrusion Detection"
      },
      {
        "paperId": "0d1754f3569a92d1fb4602d3acc45e293b59d7ef",
        "title": "Cooperative Multiagent Learning and Exploration With Min\u2013Max Intrinsic Motivation"
      },
      {
        "paperId": "608fed75d4d0e3ace23a26853c2765d2ec58dc36",
        "title": "Collaborative Anti-Jamming for Multi-UAV Communications Based on Deep Reinforcement Learning"
      },
      {
        "paperId": "1ecf72291a652e1434a4f38a053b16ff18670334",
        "title": "A Review of Smart Grid Evolution and Reinforcement Learning: Applications, Challenges and Future Directions"
      },
      {
        "paperId": "33193ebe3893181ad7febda86b3f880628d4159f",
        "title": "Curiosity-driven exploration based on hierarchical vision transformer for deep reinforcement learning with sparse rewards"
      },
      {
        "paperId": "c8d8513bcde9c9225baa0eef14ecdd399cb96d0a",
        "title": "A Survey of Reinforcement Learning-Based Motion Planning for Autonomous Driving: Lessons Learned from a Driving Task Perspective"
      },
      {
        "paperId": "e9b81d25508bf9b176527247c96557248de3fb83",
        "title": "Bridging the Gap Between MLOps and RLOps: An Industry 4.0 Case Study on Architectural Design Decisions in Practice"
      },
      {
        "paperId": "6ca038d26014b2fa80c5434129893bc503023fca",
        "title": "Inverse RL Scene Dynamics Learning for Nonlinear Predictive Control in Autonomous Vehicles"
      },
      {
        "paperId": "62a30950428d33031590d1af176cd22fdf335b38",
        "title": "Adventurer: Exploration with BiGAN for Deep Reinforcement Learning"
      },
      {
        "paperId": "b48acb3baf1df79d3949dd9e89ed3a480761608b",
        "title": "DERRIC: Decentralized Reinforced RAN Intelligent Controller Orchestration for 6G Networks"
      },
      {
        "paperId": "ce82fa698ba600d5377072fdc71016dd886d3d52",
        "title": "A Deep Reinforcement Learning-Based Evolutionary Algorithm for Distributed Heterogeneous Green Hybrid Flowshop Scheduling"
      },
      {
        "paperId": "b5f04855594a6777dac7cfcc7edb47b683118b95",
        "title": "An enhanced deep reinforcement learning approach for efficient, effective, and equitable disaster relief distribution"
      },
      {
        "paperId": "7954cafa0a6ec888d54078c66dfe411f133d8edb",
        "title": "OpenBench: A New Benchmark and Baseline for Semantic Navigation in Smart Logistics"
      },
      {
        "paperId": "c83d5716c24fe3f4e69f09357d599d451b5b683f",
        "title": "Road preview method for active suspension based on reinforcement learning"
      },
      {
        "paperId": "7017ea7db81044674425567b69f23416edd51b9e",
        "title": "Hybrid Action Based Reinforcement Learning for Multi-Objective Compatible Autonomous Driving"
      },
      {
        "paperId": "07a2166105dd0dd3fd88b09a853eeed556d5bce5",
        "title": "Robust Multi\u2010Agent Reinforcement Learning Against Adversarial Attacks for Cooperative Self\u2010Driving Vehicles"
      },
      {
        "paperId": "e2bbde71b67167343d409c0992ccacfc21fa012c",
        "title": "Cognition-Oriented Multiagent Reinforcement Learning"
      },
      {
        "paperId": "5ef1e8d31fc583bf5bbe89bff1a2a1390de29926",
        "title": "Federated Deep Reinforcement Learning for Prediction-Based Network Slice Mobility in 6G Mobile Networks"
      },
      {
        "paperId": "6237073d961575b2220f8f2e51c74a1b8a08184b",
        "title": "Performance enhancement of artificial intelligence: A survey"
      },
      {
        "paperId": "a1daa87f8b4233876852c2343b3be6c2bca04dfe",
        "title": "Improved exploration-exploitation trade-off through adaptive prioritized experience replay"
      },
      {
        "paperId": "19168378df48c3a39200dbfb6ccbde878138852c",
        "title": "Traffic navigation via reinforcement learning with episodic-guided prioritized experience replay"
      },
      {
        "paperId": "4c7b14a87b3342d2ed954c3a45d917b9b4fab141",
        "title": "Multi-Agent Deep Q-Network with Layer-based Communication Channel for Autonomous Internal Logistics Vehicle Scheduling in Smart Manufacturing"
      },
      {
        "paperId": "000e4094246d2f8ebbf2942c948b16dfe64c985b",
        "title": "Transient gas path fault diagnosis of aero-engine based on domain adaptive offline reinforcement learning"
      },
      {
        "paperId": "bec3fffeab99d6058412605b6f6bd2fb8cf5eae8",
        "title": "Historical Decision-Making Regularized Maximum Entropy Reinforcement Learning"
      },
      {
        "paperId": "ef99a7046a33a0232c4509016385f64f50f35bfe",
        "title": "TF-DDRL: A Transformer-Enhanced Distributed DRL Technique for Scheduling IoT Applications in Edge and Cloud Computing Environments"
      },
      {
        "paperId": "815a86be3d83f950cfa76480b2b49cb5d0c5e46f",
        "title": "A Safety Modulator Actor-Critic Method in Model-Free Safe Reinforcement Learning and Application in UAV Hovering"
      },
      {
        "paperId": "a1e7eec013c8a6898d3fd5ad540722c9fe5ed5e9",
        "title": "GravMAD: Grounded Spatial Value Maps Guided Action Diffusion for Generalized 3D Manipulation"
      },
      {
        "paperId": "402534ce1c0e88a834c5d841c952fc1e0de1bf98",
        "title": "Multi-agent Reinforcement Learning for Dynamic Dispatching in Material Handling Systems"
      },
      {
        "paperId": "0b09e5296493c0978cf87ee0d9a287b2b15832d2",
        "title": "Asynchronous Fractional Multi-Agent Deep Reinforcement Learning for Age-Minimal Mobile Edge Computing"
      },
      {
        "paperId": "c4dfc0fd5a55a9d5bbf2f0b0ecaed67bb6941d94",
        "title": "State-Novelty Guided Action Persistence in Deep Reinforcement Learning"
      },
      {
        "paperId": "17a4e9ee072cf54a56bd68c7201eb29ac4dfc441",
        "title": "Optimization of Urban Target Area Accessibility for Multi-UAV Data Gathering Based on Deep Reinforcement Learning"
      },
      {
        "paperId": "1ca137888bba74eaba2f99a89405e6e9bce5229d",
        "title": "The Exploration-Exploitation Dilemma Revisited: An Entropy Perspective"
      },
      {
        "paperId": "9b001d73ff9804370c29cf57f20eeaacf334bcce",
        "title": "Improving Cooperation via Joint Intrinsic Motivation Exploration in Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "6870bc255b6a03f00dea8ddca6593e5cff0986bb",
        "title": "Boosting Weak-to-Strong Agents in Multiagent Reinforcement Learning via Balanced PPO"
      },
      {
        "paperId": "5b14a8bf47cce68636c5608ddb0df04e9ab5c2c3",
        "title": "Recurrence resonance - noise-enhanced dynamics in recurrent neural networks"
      },
      {
        "paperId": "c4ab71a436c71cc145b93f40daaa0dca2d9ef4be",
        "title": "CADRL: Category-Aware Dual-Agent Reinforcement Learning for Explainable Recommendations over Knowledge Graphs"
      },
      {
        "paperId": "6396ebec974b66eeba9b599495893f2b41308b26",
        "title": "VL-TGS: Trajectory Generation and Selection Using Vision Language Models in Mapless Outdoor Environments"
      },
      {
        "paperId": "ef16155ac1632e38f6ae8a5720a6446e85b9e051",
        "title": "Discretizing Continuous Action Space With Unimodal Probability Distributions for On-Policy Reinforcement Learning"
      },
      {
        "paperId": "5707daff6ebd7f2a8a46d8cd520a1a69a4da6cfd",
        "title": "Generative subgoal oriented multi-agent reinforcement learning through potential field"
      },
      {
        "paperId": "ba87d3a7897d5b214f1a919a0c5b2b65fbc83602",
        "title": "Phased Continuous Exploration Method for Cooperative Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "6ffcf0e29dee9b8d1d3312687af88c2bd3b657b9",
        "title": "GenSafe: A Generalizable Safety Enhancer for Safe Reinforcement Learning Algorithms Based on Reduced Order Markov Decision Process Model"
      },
      {
        "paperId": "d39b12a480a7c01e1515b6358eb5cab882338a35",
        "title": "Constrained Ensemble Exploration for Unsupervised Skill Discovery"
      },
      {
        "paperId": "92dd54f82ba7687dc6ec88fed4c09731750c11b9",
        "title": "Improving the Traffic Engineering of SDN networks by using Local Multi-Agent Deep Reinforcement Learning"
      },
      {
        "paperId": "203e515826b759dc9c81fe83735ab728b9015937",
        "title": "Provably Efficient Information-Directed Sampling Algorithms for Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "d784d79945e4f6d4968c24b0dc436c5e11f17d06",
        "title": "FPGA Divide-and-Conquer Placement using Deep Reinforcement Learning"
      },
      {
        "paperId": "79212c45bc37664f78d41393820ba475ae306778",
        "title": "Research on Intelligent Matching Technology of Power Grid Dispatching Automation Emergency Plan Based on Large Model and Small Sample LoRA Fine-Tuning Algorithm"
      },
      {
        "paperId": "8b8da7ab213ac4bd56adbda0b4137244cb0db7bf",
        "title": "Emergent Braitenberg-style Behaviours for Navigating the ViZDoom 'My Way Home' Labyrinth"
      },
      {
        "paperId": "42258b105bdad2bc0a9358c5c4a89534a8395a24",
        "title": "Theoretical approaches to AI in supply chain optimization: Pathways to efficiency and resilience"
      },
      {
        "paperId": "4a891a4140092c6eedc165a7195cace80b9a1464",
        "title": "Deep Reinforcement Learning for Solving Vehicle Routing Problems With Backhauls"
      },
      {
        "paperId": "a581ce4148f831860b7d71603826b32104cb64e8",
        "title": "Mutual information oriented deep skill chaining for multi-agent reinforcement learning"
      },
      {
        "paperId": "8121a4dc2daec112be196801a9e4ecb697a8d7dc",
        "title": "PORTAL: Automatic Curricula Generation for Multiagent Reinforcement Learning"
      },
      {
        "paperId": "5e9e298b1ca33dd7ebffdba6bbd4095cb9a7483f",
        "title": "Multi-Fidelity Reinforcement Learning for Time-Optimal Quadrotor Re-planning"
      },
      {
        "paperId": "6771cb4ad3863f8dc574c5f57cf902d87a7ee881",
        "title": "Improve exploration in deep reinforcement learning for UAV path planning using state and action entropy"
      },
      {
        "paperId": "08e84c939b88fc50aaa74ef76e202e61a1ad940b",
        "title": "StepCoder: Improve Code Generation with Reinforcement Learning from Compiler Feedback"
      },
      {
        "paperId": "e0860c03f5fca797d19ea64981b392850f27a8d0",
        "title": "Energy management for demand response in networked greenhouses with multi-agent deep reinforcement learning"
      },
      {
        "paperId": "25523c3e70daedcd0f1172844fcbafe30b8933f1",
        "title": "Bridging Evolutionary Algorithms and Reinforcement Learning: A Comprehensive Survey"
      },
      {
        "paperId": "514e399ba9d486e559467c5e97aa2d0251dff283",
        "title": "Cooperative multi-agent game based on reinforcement learning"
      },
      {
        "paperId": "a02856b4834911730e28e9a41ccde42f7f9b8d4e",
        "title": "A Novel Inverse Design Method for Morphing Airfoil Based on Deep Reinforcement Learning"
      },
      {
        "paperId": "157e99edff3ac38903a2aae33c54b988648e8f38",
        "title": "Environment-Aware Adaptive Reinforcement Learning-Based Routing for Vehicular Ad Hoc Networks"
      },
      {
        "paperId": "91edf9d897ddfa4c39f013d6acc0f2a0905ef789",
        "title": "OVD-Explorer: Optimism Should Not Be the Sole Pursuit of Exploration in Noisy Environments"
      },
      {
        "paperId": "fac1b12a72d8f4872b6d9714b417111339d02662",
        "title": "The State of AI-Empowered Backscatter Communications: A Comprehensive Survey"
      },
      {
        "paperId": "4768692b14a86edc398199a089e2e35ab271e2ca",
        "title": "Multi-agent Reinforcement Learning: A Comprehensive Survey"
      },
      {
        "paperId": "286c8637e10c7f2100b6a901adbc8cf760b86404",
        "title": "Exploring into the Unseen: Enhancing Language-Conditioned Policy Generalization with Behavioral Information"
      },
      {
        "paperId": "8aed0cdc475f768cf5194b6fd14b8c04ae9a923a",
        "title": "WToE: Learning When to Explore in Multiagent Reinforcement Learning"
      },
      {
        "paperId": "d941de3a4e6a709e882f49a3346f369acbc1a1fd",
        "title": "Balancing exploration and exploitation in episodic reinforcement learning"
      },
      {
        "paperId": "339e4b4aaa5c43660bf48756066433e8a2045187",
        "title": "Small batch deep reinforcement learning"
      },
      {
        "paperId": "88c9da6423547cee897b8ea6244bd3c58df525d8",
        "title": "Delay Optimization of IoT-Edge Computing in Smart Grid Using Deep Reinforcement Learning"
      },
      {
        "paperId": "48803629958416c71ab01838d44cd63d91d960d2",
        "title": "When to Switch: Planning and Learning for Partially Observable Multi-Agent Pathfinding"
      },
      {
        "paperId": "1e966abf91e8461dfbe4d663479b22655d0b7d2e",
        "title": "Prioritized Trajectory Replay: A Replay Memory for Data-driven Reinforcement Learning"
      },
      {
        "paperId": "8c88c693d5dc2802d3b76b85740e1f04fdaaf801",
        "title": "Large sequence models for sequential decision-making: a survey"
      },
      {
        "paperId": "1c592b81a8661f21008b41d8391849ce4c886ccc",
        "title": "Mnemonic Dictionary Learning for Intrinsic Motivation in Reinforcement Learning"
      },
      {
        "paperId": "fb1ac8ef7ea13616f86fafcd3f7eeb06460c2afa",
        "title": "ENOTO: Improving Offline-to-Online Reinforcement Learning with Q-Ensembles"
      },
      {
        "paperId": "16025740c52efdf75a990424acddd078d167d44c",
        "title": "Semantically Aligned Task Decomposition in Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "ac01b6062fec7a726426f9ff8c5b0dfc3f321bd7",
        "title": "Behavior Contrastive Learning for Unsupervised Skill Discovery"
      },
      {
        "paperId": "10b81feb827394fd6c906a1b2799c749288bb7c7",
        "title": "SVDE: Scalable Value-Decomposition Exploration for Cooperative Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "5a2457f1cf0971c34208e695af6c1d39c7bb83a4",
        "title": "Latent-Conditioned Policy Gradient for Multi-Objective Deep Reinforcement Learning"
      },
      {
        "paperId": "6fdbe62e8bedbbdb6292a37cd239208d93f2949b",
        "title": "Progress and summary of reinforcement learning on energy management of MPS-EV"
      },
      {
        "paperId": "8cb359a03b499319c05eb7d36726341579dbe56f",
        "title": "EUCLID: Towards Efficient Unsupervised Reinforcement Learning with Multi-choice Dynamics Model"
      },
      {
        "paperId": "5dae4c53515b6117e08b7c4b7db2fe99ab575b36",
        "title": "A Policy Resonance Approach to Solve the Problem of Responsibility Diffusion in Multiagent Reinforcement Learning"
      },
      {
        "paperId": "71b963a0ef50f9dfe5656ad367a10fcb821a81b2",
        "title": "Image Augmentation-Based Momentum Memory Intrinsic Reward for Sparse Reward Visual Scenes"
      },
      {
        "paperId": "697edf4e095898cd1781dff50ddbe686ae22bafc",
        "title": "Curious Explorer: A Provable Exploration Strategy in Policy Learning"
      },
      {
        "paperId": "8967120f8cb2193dd3ef22e5e3f09cfe245e4433",
        "title": "Rethinking Exploration and Experience Exploitation in Value-Based Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "da29bb55f985c22185269515f2a0b38cc40b29da",
        "title": "Representation-driven sampling and adaptive policy resetting for improving multi-Agent reinforcement learning"
      },
      {
        "paperId": "88057e51f82f805d43159fcc5847eb50ef85a09e",
        "title": "NM-TD3: A Hybrid Noise-Driven TD3 Algorithm With Long-Term Reward Propagation for Mobile Robot Path Planning"
      },
      {
        "paperId": "2dd4b3a0f5acc3d018990f1f52ad44a6fd346f89",
        "title": "Rollout Total Correlation for Deep Reinforcement Learning"
      },
      {
        "paperId": "f48d1e9a5fdf72fa0ef147ef7b49cded7fd556bf",
        "title": "Deep Reinforcement Learning for AoI Minimization in UAV-Aided Data Collection for WSN and IoT Applications: A Survey"
      },
      {
        "paperId": "6c52b08f99b9b6243e0347dd3125b52e7d6ce165",
        "title": "Deep Reinforcement Learning for AoI Minimization in UAV-aided Data Collection for WSN and IoT: a Survey"
      },
      {
        "paperId": "14e905168ce90539c066a761fc2a983dbd3a3350",
        "title": "StepCoder: Improving Code Generation with Reinforcement Learning from Compiler Feedback"
      },
      {
        "paperId": "b31c76815615c16cc8505dbb38d2921f921c029d",
        "title": "A Coverage-Guided Fuzzing Method for Automatic Software Vulnerability Detection Using Reinforcement Learning-Enabled Multi-Level Input Mutation"
      },
      {
        "paperId": "08b6f22bab698a3a241de5fe94a8ff6638fadaae",
        "title": "End-to-End Autonomous Driving in CARLA: A Survey"
      },
      {
        "paperId": "8a6d78ae6bab1715bd78750e84ce608a85e95ace",
        "title": "A Reinforcement Learning Approach Combined With Scope Loss Function for Crime Prediction on Twitter (X)"
      },
      {
        "paperId": "e16703e43611f275868c7983e10ae7f538868f20",
        "title": "CPEG: Leveraging Consistency Policy with Consensus Guidance for Multi-agent Exploration"
      },
      {
        "paperId": "58d8ecc4409489492fc7f85ea97032f138abf924",
        "title": "Value-Evolutionary-Based Reinforcement Learning"
      },
      {
        "paperId": "6d74a24971dd6d82b176d7a78d1e9bca334f9691",
        "title": "Improving Cooperative Multi-Agent Exploration via Surprise Minimization and Social Influence Maximization"
      },
      {
        "paperId": "8ae6c7cff8bb2d766f5f9d3585cd262032378b33",
        "title": "Ensemble-based Offline-to-Online Reinforcement Learning: From Pessimistic Learning to Optimistic Exploration"
      },
      {
        "paperId": "d07b2e39a62dbb8889ee9b6c12538d1f3383348d",
        "title": "Adapting Model-Free Reinforcement Learning for Boss Fights in Hollow Knight: a reward shaping approach"
      },
      {
        "paperId": "40af701a8248e401726eef0084c5ed284c967c5f",
        "title": "Neurobehavior of exploring AI agents"
      },
      {
        "paperId": "f8564d05a3bf9a178849ed60ae523e3b07f50363",
        "title": "Efficient Exploration in Multi-Agent Reinforcement Learning via Farsighted Self-Direction"
      },
      {
        "paperId": "f3cd185c2dcbb888bdf9e6c3397e02e366a8236c",
        "title": "Reinforcement Learning for FPGA Placement"
      }
    ],
    "score": 30.0
  },
  {
    "id": "dc05886db1e6f17f4489d867477b38fe13e31783",
    "title": "Network Randomization: A Simple Technique for Generalization in Deep Reinforcement Learning",
    "authors": [
      "Kimin Lee",
      "Kibok Lee",
      "Jinwoo Shin",
      "Honglak Lee"
    ],
    "year": 2019,
    "citationCount": 178,
    "abstract": "Deep reinforcement learning (RL) agents often fail to generalize to unseen environments (yet semantically similar to trained agents), particularly when they are trained on high-dimensional state spaces, such as images. In this paper, we propose a simple technique to improve a generalization ability of deep RL agents by introducing a randomized (convolutional) neural network that randomly perturbs input observations. It enables trained agents to adapt to new domains by learning robust features invariant across varied and randomized environments. Furthermore, we consider an inference method based on the Monte Carlo approximation to reduce the variance induced by this randomization. We demonstrate the superiority of our method across 2D CoinRun, 3D DeepMind Lab exploration and 3D robotics control tasks: it significantly outperforms various regularization and data augmentation methods for the same purpose.",
    "url": "https://www.semanticscholar.org/paper/dc05886db1e6f17f4489d867477b38fe13e31783",
    "pdf_url": "https://arxiv.org/pdf/1910.05396.pdf",
    "venue": "International Conference on Learning Representations",
    "publicationDate": "2019-10-11",
    "externalIds": {
      "DBLP": "conf/iclr/LeeLSL20",
      "ArXiv": "1910.05396",
      "MAG": "2997103132",
      "CorpusId": 213597045
    },
    "references": [
      {
        "paperId": "633266814150ab66f0474d7b9a6807b729c7e0af",
        "title": "Environment Probing Interaction Policies"
      },
      {
        "paperId": "3562fefb64cd63ac1a6a0adbaa83ae73dd674243",
        "title": "Unsupervised Data Augmentation"
      },
      {
        "paperId": "61c8d69bb0e217e5103d888ef99692b51afaf74f",
        "title": "Domain Randomization for Active Pose Estimation"
      },
      {
        "paperId": "b79fe48ae523dc66185aa04df2dac7041afa8683",
        "title": "Learning Not to Learn: Training Deep Neural Networks With Biased Data"
      },
      {
        "paperId": "42de54e614110c0c0a0bbbfee045e11e53eb4a7d",
        "title": "Deep Online Learning via Meta-Learning: Continual Adaptation for Model-Based RL"
      },
      {
        "paperId": "f9717d29840f4d8f1cc19d1b1e80c5d12ec40608",
        "title": "A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play"
      },
      {
        "paperId": "ef2bc452812d6005ab0a66af6c3f97b6b0ba837e",
        "title": "Quantifying Generalization in Reinforcement Learning"
      },
      {
        "paperId": "050a89a91b3e828c841972a81f17807f82c79713",
        "title": "SURREAL: Open-Source Reinforcement Learning Framework and Robot Manipulation Benchmark"
      },
      {
        "paperId": "0f50b7483f1b200ebf88c4dd7698de986399a0f3",
        "title": "ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness"
      },
      {
        "paperId": "4cb3fd057949624aa4f0bbe7a6dcc8777ff04758",
        "title": "Exploration by Random Network Distillation"
      },
      {
        "paperId": "567eb0ab96fb7f2869e9c4c204da9f2cf422d946",
        "title": "Generalization and Regularization in DQN"
      },
      {
        "paperId": "caea502325b6a82b1b437c62585992609b5aa542",
        "title": "Assessing Generalization in Deep Reinforcement Learning"
      },
      {
        "paperId": "a622be547caf0b1223626de5e69377c20ae11265",
        "title": "A Dissection of Overfitting and Generalization in Continuous Reinforcement Learning"
      },
      {
        "paperId": "f802802b3af5a22b79ac65d033ba3cbee33da91b",
        "title": "Randomized Prior Functions for Deep Reinforcement Learning"
      },
      {
        "paperId": "0d38284025bd0eabf92399b35df189ae1e034236",
        "title": "Transfer Learning for Related Reinforcement Learning Tasks via Image-to-Image Translation"
      },
      {
        "paperId": "705bbc4dcd475f9230863771da6596e1f677a92d",
        "title": "Playing hard exploration games by watching YouTube"
      },
      {
        "paperId": "f723eb3e7159f07b97464c8d947d15e78612abe4",
        "title": "AutoAugment: Learning Augmentation Policies from Data"
      },
      {
        "paperId": "041d99f442dc22cf51118dc992095be9aa0972e0",
        "title": "A Study on Overfitting in Deep Reinforcement Learning"
      },
      {
        "paperId": "80196cdfcd0c6ce2953bf65a7f019971e2026386",
        "title": "IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures"
      },
      {
        "paperId": "c27db32efa8137cbf654902f8f728f338e55cd1c",
        "title": "Mastering the game of Go without human knowledge"
      },
      {
        "paperId": "0af8cdb71ce9e5bf37ad2a11f05af293cfe62172",
        "title": "Sim-to-Real Transfer of Robotic Control with Dynamics Randomization"
      },
      {
        "paperId": "3b290ffa1f4f8226e326f00984acecdfbe9e28bf",
        "title": "Revisiting the Arcade Learning Environment: Evaluation Protocols and Open Problems for General Agents"
      },
      {
        "paperId": "6127b8dc39497a2388a0fce2512595e0dcb7121b",
        "title": "StarCraft II: A New Challenge for Reinforcement Learning"
      },
      {
        "paperId": "eb35fdc11a325f21a8ce0ca65058f7480a2fc91f",
        "title": "Improved Regularization of Convolutional Neural Networks with Cutout"
      },
      {
        "paperId": "a2141a5ec0c65ea0a9861ae562f4c9fb8020d197",
        "title": "DARLA: Improving Zero-Shot Transfer in Reinforcement Learning"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "4b1c6f6521da545892f3f5dc39461584d4a27ec0",
        "title": "Virtual Adversarial Training: A Regularization Method for Supervised and Semi-Supervised Learning"
      },
      {
        "paperId": "32ceb28e45a445df4d89df281bb0e3ab5aab1a2a",
        "title": "Domain randomization for transferring deep neural networks from simulation to the real world"
      },
      {
        "paperId": "236b40f3144b95cd84779484c8269092122920aa",
        "title": "Tactics of Adversarial Attack on Deep Reinforcement Learning Agents"
      },
      {
        "paperId": "f96478d0694f18384934fc19a2655170f32e2d8c",
        "title": "Deep Direct Reinforcement Learning for Financial Signal Representation and Trading"
      },
      {
        "paperId": "c8c16a56d2a9520197da9a1546f517db5f19b204",
        "title": "Adversarial Attacks on Neural Network Policies"
      },
      {
        "paperId": "282a380fb5ac26d99667224cef8c630f6882704f",
        "title": "Learning to reinforcement learn"
      },
      {
        "paperId": "5582bebed97947a41e3ddd9bd1f284b73f1648c2",
        "title": "Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization"
      },
      {
        "paperId": "53c9443e4e667170acc60ca1b31a0ec7151fe753",
        "title": "Progressive Neural Networks"
      },
      {
        "paperId": "571b0750085ae3d939525e62af510ee2cee9d5ea",
        "title": "Improved Techniques for Training GANs"
      },
      {
        "paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d",
        "title": "Deep Residual Learning for Image Recognition"
      },
      {
        "paperId": "1def5d3711ebd1d86787b1ed57c91832c5ddc90b",
        "title": "Actor-Mimic: Deep Multitask and Transfer Reinforcement Learning"
      },
      {
        "paperId": "1c4927af526d5c28f7c2cfa492ece192d80a61d4",
        "title": "Policy Distillation"
      },
      {
        "paperId": "d316c82c12cf4c45f9e85211ef3d1fa62497bff8",
        "title": "High-Dimensional Continuous Control Using Generalized Advantage Estimation"
      },
      {
        "paperId": "1d5972b32a9b5a455a6eef389de5b7fca25771ad",
        "title": "Domain-Adversarial Training of Neural Networks"
      },
      {
        "paperId": "340f48901f72278f6bf78a04ee5b01df208cc508",
        "title": "Human-level control through deep reinforcement learning"
      },
      {
        "paperId": "449532187c94af3dd3aa55e16d2c50f7854d2199",
        "title": "Trust Region Policy Optimization"
      },
      {
        "paperId": "995c5f5e62614fcb4d2796ad2faab969da51713e",
        "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"
      },
      {
        "paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8",
        "title": "Adam: A Method for Stochastic Optimization"
      },
      {
        "paperId": "bee044c8e8903fb67523c1f8c105ab4718600cdb",
        "title": "Explaining and Harnessing Adversarial Examples"
      },
      {
        "paperId": "e74f9b7f8eec6ba4704c206b93bc8079af3da4bd",
        "title": "ImageNet Large Scale Visual Recognition Challenge"
      },
      {
        "paperId": "d891dc72cbd40ffaeefdc79f2e7afe1e530a23ad",
        "title": "Intriguing properties of neural networks"
      },
      {
        "paperId": "ea9d2a2b4ce11aaf85136840c65f3bc9c03ab649",
        "title": "Understanding the difficulty of training deep feedforward neural networks"
      },
      {
        "paperId": null,
        "title": "Dataset. The original database is a set of 25,000 images of dogs and cats for training and 12,500 images for testing"
      },
      {
        "paperId": "c68796f833a7151f0a63d1d1608dc902b4fdc9b6",
        "title": "GENERATIVE ADVERSARIAL NETS"
      },
      {
        "paperId": null,
        "title": "the matrix are sampled from the standard uniform distribution U(0.8"
      },
      {
        "paperId": "1c200fd0057b32e1c67084999d2912f3dec6e1cd",
        "title": "DARLA: Improving Zero-Shot Transfer in Reinforcement Learning"
      },
      {
        "paperId": "34f25a8704614163c4095b3ee2fc969b60de4698",
        "title": "Dropout: a simple way to prevent neural networks from overfitting"
      },
      {
        "paperId": "1c46943103bd7b7a2c7be86859995a4144d1938b",
        "title": "Visualizing Data using t-SNE"
      },
      {
        "paperId": "d98d2d873c85d4d7528dc150b3cfe9faa8d758d6",
        "title": "Dynamic treatment regimes."
      },
      {
        "paperId": "6bc8db0c7444d9c07aad440393b2fd300fb3595c",
        "title": "Function Optimization using Connectionist Reinforcement Learning Algorithms"
      },
      {
        "paperId": null,
        "title": "The DeepMind Lab native action set consists of seven discrete actions encoded in integers"
      }
    ],
    "cited_by": [
      {
        "paperId": "7d9af098f91057adf11a5491994bc8acecc942d9",
        "title": "An effective framework with hybrid augmentation for visual reinforcement learning generalization"
      },
      {
        "paperId": "c4edc312dfba1a4249dbacf00323ddd678e26eeb",
        "title": "Multi-Group Equivariant Augmentation for Reinforcement Learning in Robot Manipulation"
      },
      {
        "paperId": "5ccdfea6bc459a8013edf19c0e499e2f38408947",
        "title": "ArtVIP: Articulated Digital Assets of Visual Realism, Modular Interaction, and Physical Fidelity for Robot Learning"
      },
      {
        "paperId": "8e7f34d1db614a5187cbc7e3e5fbd8f2e1c9a606",
        "title": "Optimizing Sensory Neurons: Nonlinear Attention Mechanisms for Accelerated Convergence in Permutation-Invariant Neural Networks for Reinforcement Learning"
      },
      {
        "paperId": "3b9356b4bafad8d15c819ee57aa68c822f605a73",
        "title": "How Ensembles of Distilled Policies Improve Generalisation in Reinforcement Learning"
      },
      {
        "paperId": "04ef5a9065a475505d8fc59f437fb875318ee1f0",
        "title": "Bidirectional Distillation: A Mixed-Play Framework for Multi-Agent Generalizable Behaviors"
      },
      {
        "paperId": "d0bdb53c12368390f9b5cf63b461a9844424d8f0",
        "title": "Modeling Unseen Environments with Language-guided Composable Causal Components in Reinforcement Learning"
      },
      {
        "paperId": "69f31afdb8d8e888d08a52418fd1e5ead39db0f5",
        "title": "Model-Agnostic and Efficient Mixup Augmentation Guided by Saliency Maps"
      },
      {
        "paperId": "0f45258b378610bc09fe809c9c407adc3480fd3b",
        "title": "Towards Fully Automated Decision-Making Systems for Greenhouse Control: Challenges and Opportunities"
      },
      {
        "paperId": "1ea9c6268f2fcc2b2498e11c197fe182c066ebef",
        "title": "Provable Zero-Shot Generalization in Offline Reinforcement Learning"
      },
      {
        "paperId": "aa3ef63a64ef5956a4ba48c5a858a1362411d772",
        "title": "Impoola: The Power of Average Pooling for Image-Based Deep Reinforcement Learning"
      },
      {
        "paperId": "85bbb7998007df7781c2b68c56b0126c7fbd253c",
        "title": "Gradient-Guided Annealing for Domain Generalization"
      },
      {
        "paperId": "75c97aba86eecf9688d4fa96090bde2edb58710a",
        "title": "Salience-Invariant Consistent Policy Learning for Generalization in Visual Reinforcement Learning"
      },
      {
        "paperId": "6333d86dd3cca715f0becc63066b2f4ba13604eb",
        "title": "Representation Convergence: Mutual Distillation is Secretly a Form of Regularization"
      },
      {
        "paperId": "43e70db1d549deccd0392edb991989dcd3317443",
        "title": "Reinforcement Learning from Wild Animal Videos"
      },
      {
        "paperId": "8d8aeabfc9e6466a9794e42724443ae8138c9e4e",
        "title": "Multi-Agent Network Randomization Method for Robust Knowledge Transfer in Deep Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "28f64c57e8b8f42baee22e85f960a8aa4438cb70",
        "title": "Reinforcement Learning with Euclidean Data Augmentation for State-Based Continuous Control"
      },
      {
        "paperId": "f40c2db4d5a48f85efbb2ac1674473634405150f",
        "title": "Mitigating Adversarial Perturbations for Deep Reinforcement Learning via Vector Quantization"
      },
      {
        "paperId": "01fc2697f974738a2d4316d24c197512c3857d4e",
        "title": "Exploration Implies Data Augmentation: Reachability and Generalisation in Contextual MDPs"
      },
      {
        "paperId": "bec628bc2d6aa38cc4e211929e8d2d93b0af4337",
        "title": "Focus On What Matters: Separated Models For Visual-Based RL Generalization"
      },
      {
        "paperId": "9a341628f5ddaa9ce178b40a70c4dbf009b5f69f",
        "title": "How to Learn Domain-Invariant Representations for Visual Reinforcement Learning: An Information-Theoretical Perspective"
      },
      {
        "paperId": "96b7d86276a2c3f66d401635ac03f65e27e061b2",
        "title": "Synthesizing Programmatic Policy for Generalization within Task Domain"
      },
      {
        "paperId": "47eb7c64d8d718fa33f874ebcf32657254ef834e",
        "title": "Efficient Learning and Control of String- Type Artificial Muscle Driven Robotic Systems"
      },
      {
        "paperId": "41b64f63e931a17975f1e17237f5d647deb4a053",
        "title": "A New Artificial Neural Network-Based Calibration Mechanism for ADCs: A Time-Interleaved ADC Case Study"
      },
      {
        "paperId": "08e5b48f68783e9dd069e81ab212ee01dc1189f4",
        "title": "Analyzing the Sensitivity to Policy-Value Decoupling in Deep Reinforcement Learning Generalization"
      },
      {
        "paperId": "d40768937f16b9e0469ae231ad9b61ec779a3554",
        "title": "Exploring Cross-Domain Few-Shot Classification via Frequency-Aware Prompting"
      },
      {
        "paperId": "fc803bc8be2d388a87a36f821b4cf564a536c41a",
        "title": "Explore-Go: Leveraging Exploration for Generalisation in Deep Reinforcement Learning"
      },
      {
        "paperId": "b1dd4c31d1c4bb75eba6db1c23774d6cc1036069",
        "title": "Improving Generalization in Aerial and Terrestrial Mobile Robots Control Through Delayed Policy Learning"
      },
      {
        "paperId": "c05d614840a1957879089d4ee6739980a7ed0a94",
        "title": "State Augmentation via Self-Supervision in Offline Multiagent Reinforcement Learning"
      },
      {
        "paperId": "ea9542d9cec6dca54664b4a58787c61ed35f2362",
        "title": "Causal Action Influence Aware Counterfactual Data Augmentation"
      },
      {
        "paperId": "0979e3fe644878519f2c168b26136b8d56a75af7",
        "title": "An Efficient Learning Control Framework With Sim-to-Real for String-Type Artificial Muscle-Driven Robotic Systems"
      },
      {
        "paperId": "2eae38f66752a49c29bb0050c1200a3fc557e3f7",
        "title": "Do Agents Dream of Electric Sheep?: Improving Generalization in Reinforcement Learning through Generative Learning"
      },
      {
        "paperId": "d2073b0947a1634fcad2417c89ffe3fa5881a1cb",
        "title": "Advancing Investment Frontiers: Industry-grade Deep Reinforcement Learning for Portfolio Optimization"
      },
      {
        "paperId": "7f260ddb35c5799495474cc619ac728ee541c162",
        "title": "Understanding What Affects the Generalization Gap in Visual Reinforcement Learning: Theory and Empirical Evidence"
      },
      {
        "paperId": "e8ac465288ffec739e6d460201ddd5b4aa0628a8",
        "title": "Efficient Training of Generalizable Visuomotor Policies via Control-Aware Augmentation"
      },
      {
        "paperId": "8929bb5e372fe36b85664fca0e6649a6f4dd2e49",
        "title": "A Survey Analyzing Generalization in Deep Reinforcement Learning"
      },
      {
        "paperId": "842971bbd2668c9e4d0b3bc983383227fbfe2aa1",
        "title": "The Generalization Gap in Offline Reinforcement Learning"
      },
      {
        "paperId": "02838468c414d12fdd1da7c60e1d2ce1ed359340",
        "title": "SCRL: Self-supervised Continual Reinforcement Learning for Domain Adaptation"
      },
      {
        "paperId": "0de41687f049cc3ae310ec5119ac0634070dffa4",
        "title": "Environment Agnostic Representation for Visual Reinforcement learning"
      },
      {
        "paperId": "b435bd3fd2c855a755061a5ace42fb444d4f81a7",
        "title": "Guide Your Agent with Adaptive Multimodal Rewards"
      },
      {
        "paperId": "a90a40f5468d5c59f1ecd4fb2696075fc061667b",
        "title": "Curriculum-based Sensing Reduction in Simulation to Real-World Transfer for In-hand Manipulation"
      },
      {
        "paperId": "76cf947057333b9ecb6e83c066d4f0dbcc278d3e",
        "title": "Adversarial Style Transfer for Robust Policy Optimization in Deep Reinforcement Learning"
      },
      {
        "paperId": "d293a75f529d7b1abb161480a95122c9a3ed6376",
        "title": "Causal Reinforcement Learning: A Survey"
      },
      {
        "paperId": "41dc40dc47dcd3ba65044395b20973b155233ff7",
        "title": "MoVie: Visual Model-Based Policy Adaptation for View Generalization"
      },
      {
        "paperId": "8ddb147c006cdeae6cd38f458e9c7c337d5b0965",
        "title": "GuidedMixup: An Efficient Mixup Strategy Guided by Saliency Maps"
      },
      {
        "paperId": "665776b85ac03cb3533208e3da74452042ecbd51",
        "title": "VIBR: Learning View-Invariant Value Functions for Robust Visual Control"
      },
      {
        "paperId": "c12732ee9f4272f43d70733b7eded76b010dfff9",
        "title": "The Role of Diverse Replay for Generalisation in Reinforcement Learning"
      },
      {
        "paperId": "1a73038804052a40c12aae696848ece2168f6da7",
        "title": "On the Importance of Exploration for Generalization in Reinforcement Learning"
      },
      {
        "paperId": "39550f6205931166a913489336fb35530fb81b0a",
        "title": "Explore to Generalize in Zero-Shot RL"
      },
      {
        "paperId": "3960ea842b8b89819f80cf2cd77adc321ff744a1",
        "title": "Policy Gradient Methods in the Presence of Symmetries and State Abstractions"
      },
      {
        "paperId": "610f370327f733b9dc50caa620c3256124296812",
        "title": "Train a Real-world Local Path Planner in One Hour via Partially Decoupled Reinforcement Learning and Vectorized Diversity"
      },
      {
        "paperId": "b0ce75b738dd5ae5244955755a5779d2bbcf1563",
        "title": "Simple Noisy Environment Augmentation for Reinforcement Learning"
      },
      {
        "paperId": "9f53e55d869f08f080f31b88e6f1aac0dc588c7e",
        "title": "C2RL: Convolutional-Contrastive Learning for Reinforcement Learning Based on Self-Pretraining for Strong Augmentation"
      },
      {
        "paperId": "9f917d3d98a58c5be130532acab9d5d227b4a996",
        "title": "Adversarial Policy Optimization in Deep Reinforcement Learning"
      },
      {
        "paperId": "18190e8a77bb5ae4678aa466a9a805ec1910f44e",
        "title": "Task Aware Dreamer for Task Generalization in Reinforcement Learning"
      },
      {
        "paperId": "5fb6ce8cb7b2e8ecc01e5dc9b5c253f9342d5099",
        "title": "Generalization in Visual Reinforcement Learning with the Reward Sequence Distribution"
      },
      {
        "paperId": "5fda3e94df50485dd7caa6940fa7435ec9bfe80c",
        "title": "Robust Representation Learning by Clustering with Bisimulation Metrics for Visual Reinforcement Learning with Distractions"
      },
      {
        "paperId": "186e7f7e12a1261584c119e198da7ac02858357e",
        "title": "PAC-Bayesian Soft Actor-Critic Learning"
      },
      {
        "paperId": "855bcc08e94a5bd82865b79b6bb9af4727e65726",
        "title": "Learning Generalizable Representations for Reinforcement Learning via Adaptive Meta-learner of Behavioral Similarities"
      },
      {
        "paperId": "0672e63cb2fb7a754e6549598440d3b0c66a002d",
        "title": "Pre-Trained Image Encoder for Generalizable Visual Reinforcement Learning"
      },
      {
        "paperId": "f451f5c07245affbdb774c5c0f326372495cacd4",
        "title": "Few-shot learning based on enhanced pseudo-labels and graded pseudo-labeled data selection"
      },
      {
        "paperId": "9fc21e2c94cb2ed51809a9c96eccce810ef22520",
        "title": "Denoising after Entropy-based Debiasing A Robust Training Method for Dataset Bias with Noisy Labels"
      },
      {
        "paperId": "490b6db3ce7cf9743c5a7c3125ab9dca240ce907",
        "title": "Deep Reinforcement Learning with Vector Quantized Encoding"
      },
      {
        "paperId": "fb090d312f6f33b7f9bb44f04f81304fb7a11bac",
        "title": "Learning Deep Sensorimotor Policies for Vision-Based Autonomous Drone Racing"
      },
      {
        "paperId": "d057ba27750d789e8ebe040c543976984052fc67",
        "title": "ADLight: A Universal Approach of Traffic Signal Control with Augmented Data Using Reinforcement Learning"
      },
      {
        "paperId": "9af7ab143a9e7517405f18c4d9703a121b7669fa",
        "title": "Beyond Simple Argumentation: Image Learnable Transformation For Efficient Reinforcement Learning"
      },
      {
        "paperId": "05c522667f9dee976764f805a2e509f7c05ca80e",
        "title": "RPM: Generalizable Behaviors for Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "336eea1e32e2b93c63f97931710d3de54d05a336",
        "title": "A Comprehensive Survey of Data Augmentation in Visual Reinforcement Learning"
      },
      {
        "paperId": "261c0be2ad10b605d39e0d13d3b28d11d171b4d4",
        "title": "Look where you look! Saliency-guided Q-networks for generalization in visual Reinforcement Learning"
      },
      {
        "paperId": "2d1b83626d9f55b94335d24037a253a9ac35d63d",
        "title": "Continuous MDP Homomorphisms and Homomorphic Policy Gradient"
      },
      {
        "paperId": "ee1cdcc52969b4a44f8bfce428036ea898b9e584",
        "title": "Style-Agnostic Reinforcement Learning"
      },
      {
        "paperId": "f3bf39ec3ff3464d234bd7ffe89199feeb4795c4",
        "title": "Generalizing Goal-Conditioned Reinforcement Learning with Variational Causal Reasoning"
      },
      {
        "paperId": "902df057953283e2d82f5c119cbdbe7557c3aaab",
        "title": "Adversarial Discriminative Feature Separation for Generalization in Reinforcement Learning"
      },
      {
        "paperId": "196d0e7f1ce738a4ec5b65120713f02f6f7204ac",
        "title": "Bootstrap State Representation using Style Transfer for Better Generalization in Deep Reinforcement Learning"
      },
      {
        "paperId": "78e2ff2fd6e2e29f8027a1c3d2cac0de1e347539",
        "title": "Self-Predictive Dynamics for Generalization of Vision-based Reinforcement Learning"
      },
      {
        "paperId": "04f6f0f3a5ffb36db233d05b116ad662626fd90d",
        "title": "Model Generation with Provable Coverability for Offline Reinforcement Learning"
      },
      {
        "paperId": "35cd12a6fe30e8fc532c37396f581bc8dc0c2a09",
        "title": "Mitigating Dataset Bias by Using Per-sample Gradient"
      },
      {
        "paperId": "4b516216d7d150a081fd74993bddf36b6b22c118",
        "title": "Chain of Thought Imitation with Procedure Cloning"
      },
      {
        "paperId": "9ba0718af84da2e51541c6fedca5320da4820fa3",
        "title": "Learning Task-relevant Representations for Generalization via Characteristic Functions of Reward Sequence Distributions"
      },
      {
        "paperId": "6ffb3065fd02938d2adf843048047a08c67ac39b",
        "title": "CCLF: A Contrastive-Curiosity-Driven Learning Framework for Sample-Efficient Reinforcement Learning"
      },
      {
        "paperId": "5f4b1e02f6c0f3fe09ee604f845c7c2b94d9e8c9",
        "title": "Domain Knowledge-Based Evolutionary Reinforcement Learning for Sensor Placement"
      },
      {
        "paperId": "56a73bb9748e4a09994fe8aedc645eded638109e",
        "title": "Local Feature Swapping for Generalization in Reinforcement Learning"
      },
      {
        "paperId": "de5789e9831cefe23ae5f0067229004f5ca4c581",
        "title": "Dynamic Noises of Multi-Agent Environments Can Improve Generalization: Agent-based Models meets Reinforcement Learning"
      },
      {
        "paperId": "f173003365d41a6fb575e29eff9a73ecc611e073",
        "title": "Feature-Attending Recurrent Modules for Generalizing Object-Centric Behavior"
      },
      {
        "paperId": "fb7e3594399f49d35e2e41af83f4fdb6ab399d54",
        "title": "Towards Autonomous Satellite Communications: An AI-based Framework to Address System-level Challenges"
      },
      {
        "paperId": "ae6c5559fc682802e86343c48409c470719836a7",
        "title": "Learning Generalizable Behavior via Visual Rewrite Rules"
      },
      {
        "paperId": "9839d9fce2a0aa39891effbf168474f5af728b6c",
        "title": "Hyper-parameter optimization based on soft actor critic and hierarchical mixture regularization"
      },
      {
        "paperId": "a8755cbdaca0ea9723fb81a06589a7837ea44619",
        "title": "A Survey of Zero-shot Generalisation in Deep Reinforcement Learning"
      },
      {
        "paperId": "a7d58bd29778ef0d15b9e9e3eb2f37a8cf1ea70c",
        "title": "Recurrent Model-Free RL Can Be a Strong Baseline for Many POMDPs"
      },
      {
        "paperId": "839020f1e33beea661141e47265b0745355875c2",
        "title": "Reinforcement Learning on Encrypted Data"
      },
      {
        "paperId": "15cdd86ff69bb5c0d89f9bd066874530f8ab0c0d",
        "title": "The Sensory Neuron as a Transformer: Permutation-Invariant Neural Networks for Reinforcement Learning"
      },
      {
        "paperId": "75397aa4447e10b1c2ce36295a9da5bd8be7b034",
        "title": "A review of mobile robot motion planning methods: from classical motion planning workflows to reinforcement learning-based architectures"
      },
      {
        "paperId": "a6dd4e15b4e6b09d0c0584d64db2f335fad033f9",
        "title": "Active Reinforcement Learning over MDPs"
      },
      {
        "paperId": "ef24c31e07443b5d08dfbc822bff06acb7bc7cbd",
        "title": "Why Generalization in RL is Difficult: Epistemic POMDPs and Implicit Partial Observability"
      },
      {
        "paperId": "75f425319694047f763f02f2a07912cd5621cfa4",
        "title": "Evaluating the progress of Deep Reinforcement Learning in the real world: aligning domain-agnostic and domain-specific research"
      },
      {
        "paperId": "d4ef2123249f9fe04791d842136532844eb124cc",
        "title": "MixStyle Neural Networks for Domain Generalization and Adaptation"
      },
      {
        "paperId": "b61a5a02afe37db26b064aac81fbf62d1cacb57b",
        "title": "Generalization of Reinforcement Learning with Policy-Aware Adversarial Data Augmentation"
      },
      {
        "paperId": "fc415d3fb9c23f62c8eabab40288584d3d43d4f2",
        "title": "Programmatic Modeling and Generation of Real-Time Strategic Soccer Environments for Reinforcement Learning"
      },
      {
        "paperId": "42cc5f45d59a694813e990b29631be4e32bc6453",
        "title": "MADE: Exploration via Maximizing Deviation from Explored Regions"
      },
      {
        "paperId": "7ad1b82507b61c7113c4bde17fa3d89bb256cff3",
        "title": "SECANT: Self-Expert Cloning for Zero-Shot Generalization of Visual Policies"
      },
      {
        "paperId": "0fc049d7dd0f195fdead752e532c03bb11be1d80",
        "title": "Vision-Language Navigation with Random Environmental Mixup"
      },
      {
        "paperId": "f096baa9f77a9ff112aea94757b5601005038d36",
        "title": "Taxonomy of Machine Learning Safety: A Survey and Primer"
      },
      {
        "paperId": "921ce7f386bdc08de9672748e02950244b17f306",
        "title": "RobustNav: Towards Benchmarking Robustness in Embodied Navigation"
      },
      {
        "paperId": "f6ba17a626c41125dd481bd460914f8b51edc3f6",
        "title": "Scenic4RL: Programmatic Modeling and Generation of Reinforcement Learning Environments"
      },
      {
        "paperId": "a1a9a02deb172cfe0ec0c3c4fb70b32ae5d09ab4",
        "title": "Improving Generalization in Meta-RL with Imaginary Tasks from Latent Dynamics Mixture"
      },
      {
        "paperId": "9ae2b8438ef37c89d50da0f06cca14478c02bc8b",
        "title": "InsertionNet - A Scalable Solution for Insertion"
      },
      {
        "paperId": "ff5cf71c0b8c8842524d5008badca5130f90aaed",
        "title": "Cross-Domain Few-Shot Classification via Adversarial Task Augmentation"
      },
      {
        "paperId": "4f6eafafc9563a5b904535078df7e74afe39ef59",
        "title": "Domain Generalization with MixStyle"
      },
      {
        "paperId": "3f98db799b0e7932ee44667a25028dc84e05b822",
        "title": "Broaden Your Views for Self-Supervised Video Learning"
      },
      {
        "paperId": "b249fe4e5e2bada6655ce5d61e7f50da5d471cb4",
        "title": "Domain Generalization: A Survey"
      },
      {
        "paperId": "8272d2f9412e9151c023011205227859a5021177",
        "title": "Decoupling Value and Policy for Generalization in Reinforcement Learning"
      },
      {
        "paperId": "7fc04f8031598da6510bce4a7e5b4722305ca5b0",
        "title": "State Entropy Maximization with Random Encoders for Efficient Exploration"
      },
      {
        "paperId": "3b9c93876303d2a0f96d44e2392f54c7cc61b376",
        "title": "Efficient Scheduling of Data Augmentation for Deep Reinforcement Learning"
      },
      {
        "paperId": "60056a89051545aae35d721adc62677f2ea3ee05",
        "title": "Adversarially Guided Actor-Critic"
      },
      {
        "paperId": "39caa9091318480f3241117ecefba897548cc42a",
        "title": "Co-Mixup: Saliency Guided Joint Mixup with Supermodular Diversity"
      },
      {
        "paperId": "7428f65393c19a6ca6381693767cb4f643a49a5c",
        "title": "Contrastive Behavioral Similarity Embeddings for Generalization in Reinforcement Learning"
      },
      {
        "paperId": "695e51a0e15418d4163e0ddfec9502872a235ecd",
        "title": "Intervention Design for Effective Sim2Real Transfer"
      },
      {
        "paperId": "9e380323c2a6db51450253e3ce5ed72bddbe64a0",
        "title": "Instance based Generalization in Reinforcement Learning"
      },
      {
        "paperId": "3caf26a81f8b8be27b8ebc09bdb93416b4c40fe6",
        "title": "Improving Generalization in Reinforcement Learning with Mixture Regularization"
      },
      {
        "paperId": "2dd28b14430b88fefa48402d2adda860d7e55ff8",
        "title": "Measuring Visual Generalization in Continuous Control from Pixels"
      },
      {
        "paperId": "b27ad18e20d27efe8a9fbc54b1c2dcef8b2da19f",
        "title": "Robust and Generalizable Visual Representation Learning via Random Convolutions"
      },
      {
        "paperId": "2338e0568bbd56cfe025dcdb891fad0631a1ed7c",
        "title": "MDP Homomorphic Networks: Group Symmetries in Reinforcement Learning"
      },
      {
        "paperId": "022622e024890d6e044ac50e2da6b44c59bdf418",
        "title": "The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization"
      },
      {
        "paperId": "05c82617cdaa16c9dc17c32f3cb5ed4a7182b13e",
        "title": "Automatic Data Augmentation for Generalization in Deep Reinforcement Learning"
      },
      {
        "paperId": "90974d9e0df8466a50338601e839fa0ea69c9872",
        "title": "Transient Non-stationarity and Generalisation in Deep Reinforcement Learning"
      },
      {
        "paperId": "43cfafb95bbfc2974437c517a6d63107ecfca12d",
        "title": "The Impact of Non-stationarity on Generalisation in Deep Reinforcement Learning"
      },
      {
        "paperId": "744139d65c3bf6da6a6acd384a32d94a06f44f62",
        "title": "Reinforcement Learning with Augmented Data"
      },
      {
        "paperId": "6509c28c459e07021a2505258715442b2ff0d72b",
        "title": "Reinforcement Learning Generalization with Surprise Minimization"
      },
      {
        "paperId": "98091f1defa7f1f291b2be761a47a1f0dd585044",
        "title": "Obstacle Tower Without Human Demonstrations: How Far a Deep Feed-Forward Network Goes with Reinforcement Learning"
      },
      {
        "paperId": "8cf62055fa0faab9c325f4b30415f5b0dc285434",
        "title": "Neuroevolution of self-interpretable agents"
      },
      {
        "paperId": "2d0e625fca0efc079bb523baf1ff78c46ff4916e",
        "title": "Rotation, Translation, and Cropping for Zero-Shot Generalization"
      },
      {
        "paperId": "8d814620a1ca77e745bc8a33b96b86148f2804fe",
        "title": "Leveraging Procedural Generation to Benchmark Reinforcement Learning"
      },
      {
        "paperId": "6a8a95d429e1aade7bff06b0088a02f98a3e2396",
        "title": "A review on Deep Reinforcement Learning for Fluid Mechanics"
      },
      {
        "paperId": "209c5f472d1bba81c8dd7812a4f88cb301b4f34a",
        "title": "Mix-Spectrum for Generalization in Visual Reinforcement Learning"
      },
      {
        "paperId": "70252cd6032f457ad83d0d55704a6ddd2a709fbf",
        "title": "Natural Language-based State Representation in Deep Reinforcement Learning"
      },
      {
        "paperId": "67e61e2adb6cd423f3353b236d2e1ef17bc05351",
        "title": "Towards Understanding How to Reduce Generalization Gap in Visual Reinforcement Learning"
      },
      {
        "paperId": "8aa7ad3e6dca112e004077c2e92c1fcbfc746a4e",
        "title": "The Dormant Neuron Phenomenon in Multi-Agent Reinforcement Learning Value Factorization"
      },
      {
        "paperId": "4319cfbf2b34a0f8b2ba86a7e7c66229858f4649",
        "title": "A Simple Framework for Generalization in Visual RL under Dynamic Scene Perturbations"
      },
      {
        "paperId": "dbaad1db24400832ec23ef43e619dc06a48ce71d",
        "title": "Understanding What Affects Generalization Gap in Visual Reinforcement Learning: Theory and Empirical Evidence"
      },
      {
        "paperId": "ff6b270c987808f8b633b9c927bbd20786ecab30",
        "title": "RPM: Generalizable Multi-Agent Policies for Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "8d3e11562620572cf7e43cd8a87ad4cfe1400481",
        "title": "Learning robust representation for reinforcement learning with distractions by reward sequence prediction"
      },
      {
        "paperId": "111ada0d13076516770657cf17c119ecf3379dfb",
        "title": "Leveraging Value-awareness for Online and Offline Model-based Reinforcement Learning"
      },
      {
        "paperId": "34eaa5026f66adcd7ad77a320a0869639c5de0bd",
        "title": "Reward Informed Dreamer for Task Generalization in Reinforcement Learning"
      },
      {
        "paperId": "7365d4b97e38ca7fcd8ac3db194854b59def1d42",
        "title": "Feature-Attending Recurrent Modules for Generalization in Reinforcement Learning"
      },
      {
        "paperId": "d3eaa8481faa3bfe4430b2ca27863f6ee01aa431",
        "title": "Frequency-Enhanced Data Augmentation for Vision-and-Language Navigation"
      },
      {
        "paperId": "1ec78f256f6a46ed38b184841484f62ca3591795",
        "title": "COOM: A Game Benchmark for Continual Reinforcement Learning"
      },
      {
        "paperId": "9bc63d773bc04cca984ad6b9f80949b64e56e536",
        "title": "Parameterizing Non-Parametric Meta-Reinforcement Learning Tasks via Subtask Decomposition"
      },
      {
        "paperId": "5ba117627b6ceb2ddf418ed9b300898a1fd6a0aa",
        "title": "Constrained Reinforcement Learning for Autonomous Farming: Challenges and Opportunities"
      },
      {
        "paperId": "f2a2401a35b6b892d43642b31700e83e88b2ebb8",
        "title": "SURF: Semi-supervised Reward Learning with Data Augmentation for Feedback-efficient Preference-based Reinforcement Learning"
      },
      {
        "paperId": "a93a0177ee261d9fa7610b74959ecaf992addc34",
        "title": "Assisting Reinforcement Learning in Real-time Strategy Environments with SCENIC"
      },
      {
        "paperId": "f4ef329dd9e84fa2a4fa7634c6f255eb4035f6d6",
        "title": "Towards Anomaly Detection in Reinforcement Learning"
      },
      {
        "paperId": "8207fadf3f782151bce470141b59b30efa10a246",
        "title": "Improving Generalization with Cross-State Behavior Matching in Deep Reinforcement Learning"
      },
      {
        "paperId": "79624e3dd1087258092fe459fe53b6f4f2eeb140",
        "title": "Look where you look! Saliency-guided Q-networks for visual RL tasks"
      },
      {
        "paperId": "f64555e9de9025f7cf3d33b2a031a517d629f6d5",
        "title": "A Swapping Target Q-Value Technique for Data Augmentation in Offline Reinforcement Learning"
      },
      {
        "paperId": "3863c91ef1eb65e7121096fafd5587577f047bca",
        "title": "Overleaf Example"
      },
      {
        "paperId": "c763f000b7292726c6d5e7429fee51fc616f02d0",
        "title": "Learning Representations via a Robust Behavioral Metric for Deep Reinforcement Learning"
      },
      {
        "paperId": "9ccaafada3a654df5fb3a658a6e6f803df683050",
        "title": "Spectrum Random Masking for Generalization in Image-based Reinforcement Learning"
      },
      {
        "paperId": "dff2e7721d11826f841ca99f9fedc664c5266d67",
        "title": "Automatic Data Augmentation for Generalization in Reinforcement Learning"
      },
      {
        "paperId": "7286dbffdd6ecfb62bfd4f00308e2863c8aa482f",
        "title": "Time Matters in Using Data Augmentation for Vision-based Deep Reinforcement Learning"
      },
      {
        "paperId": "8f97a1fb0ff661ca3ae101763041c814f7cbc37e",
        "title": "Improving Generalization in Reinforcement Learning\u2013Based Trading by Using a Generative Adversarial Market Model"
      },
      {
        "paperId": "ad948b4d54e75de1f39d0d70ce447d50b85fe14b",
        "title": "Practical Machine Learning Safety: A Survey and Primer"
      },
      {
        "paperId": "082b186e843f93e1af815f99be0b9d313587d6eb",
        "title": "Recurrent Model-Free RL is a Strong Baseline for Many POMDPs"
      },
      {
        "paperId": "42edbc3c29af476c27f102b3de9f04e56b5c642d",
        "title": "A Survey of Generalisation in Deep Reinforcement Learning"
      },
      {
        "paperId": "17c31980c6bedaf2f7c48d25940ad86a8f882a9f",
        "title": "Reinforcement learning in Connect 4 Game"
      },
      {
        "paperId": "c3e2665d510d0d9b3c7b7147f8dc767986d6213d",
        "title": "A New Open-Source Off-road Environment for Benchmark Generalization of Autonomous Driving"
      },
      {
        "paperId": "d779908d8358febf9643e777fd72f4cb1df61620",
        "title": "Domain Randomization for Deep Reinforcement Learning in Self-Driving Environments"
      },
      {
        "paperId": "3db9302cc0c669cbd08fab37c5e7bd51d1158885",
        "title": "Conditioning of Reinforcement Learning Agents and its Policy Regularization Application"
      },
      {
        "paperId": "b0bf922e8d13b0382905bd8591d1a0af72c31445",
        "title": "Exploring and Exploiting Conditioning of Reinforcement Learning Agents"
      },
      {
        "paperId": "887eeac6a7d5a3531a25a6e1f33fdef86a1516f1",
        "title": "Sample-Efficient Deep Reinforcement Learning with Visual Domain Randomization"
      },
      {
        "paperId": "82a086019eba6caae0a4a7d3c71c0cb885eb7e00",
        "title": "C ROSS -S TATE S ELF -C ONSTRAINT FOR FEATURE GEN - ERALIZATION IN DEEP REINFORCEMENT LEARNING"
      },
      {
        "paperId": "842c274fcc6b230c782111a2290eef358e9f72e3",
        "title": "Appendix A Application of mixreg to PPO and Rainbow"
      },
      {
        "paperId": "45557656a127d2e49c36b0bbc2fcc3d01d4fe667",
        "title": "Detection of earthquake precursors using neural networks"
      },
      {
        "paperId": "55ada11edad8df6649f8889697787f8ae5f87035",
        "title": "A Survey on Generalization in Deep Reinforcement Learning"
      },
      {
        "paperId": "42da79f3516623ab00416e79ac0e23ebda544538",
        "title": "A Study of Generalization in Offline Reinforcement Learning"
      },
      {
        "paperId": "b67e2a42ed51b6bd6b8b8fff61c09137df5f7d63",
        "title": "Supplementary Materials of \u201cSpectrum Random Masking for Generalization in Image-based Reinforcement Learning\u201d"
      },
      {
        "paperId": "0a983b241f9d2936d632cd48a349d7b685b4ed98",
        "title": "Reinforcement Learning with Augmentation Invariant Representation: A Non-contrastive Approach"
      },
      {
        "paperId": "c6b46886116a0bc47c6fa90d16ca6799c35255b3",
        "title": "A Formal Unification of Generalization in Deep Reinforcement Learning"
      },
      {
        "paperId": "bf242d3a3fcb500e2ab873b7205c35d3dcbd603f",
        "title": "Learning Robust Representations for Visual Reinforcement Learning via Task-Relevant Mask Sampling"
      }
    ],
    "score": 29.666666666666664
  },
  {
    "id": "6bf9b58b8f418fb2922762550fb78bb22c83c0f8",
    "title": "Variational Policy Gradient Method for Reinforcement Learning with General Utilities",
    "authors": [
      "Junyu Zhang",
      "Alec Koppel",
      "A. S. Bedi",
      "Csaba Szepesvari",
      "Mengdi Wang"
    ],
    "year": 2020,
    "citationCount": 146,
    "abstract": "In recent years, reinforcement learning (RL) systems with general goals beyond a cumulative sum of rewards have gained traction, such as in constrained problems, exploration, and acting upon prior experiences. In this paper, we consider policy optimization in Markov Decision Problems, where the objective is a general concave utility function of the state-action occupancy measure, which subsumes several of the aforementioned examples as special cases. Such generality invalidates the Bellman equation. As this means that dynamic programming no longer works, we focus on direct policy search. Analogously to the Policy Gradient Theorem \\cite{sutton2000policy} available for RL with cumulative rewards, we derive a new Variational Policy Gradient Theorem for RL with general utilities, which establishes that the parametrized policy gradient may be obtained as the solution of a stochastic saddle point problem involving the Fenchel dual of the utility function. We develop a variational Monte Carlo gradient estimation algorithm to compute the policy gradient based on sample paths. We prove that the variational policy gradient scheme converges globally to the optimal policy for the general objective, though the optimization problem is nonconvex. We also establish its rate of convergence of the order $O(1/t)$ by exploiting the hidden convexity of the problem, and proves that it converges exponentially when the problem admits hidden strong convexity. Our analysis applies to the standard RL problem with cumulative rewards as a special case, in which case our result improves the available convergence rate.",
    "url": "https://www.semanticscholar.org/paper/6bf9b58b8f418fb2922762550fb78bb22c83c0f8",
    "pdf_url": "https://arxiv.org/pdf/2007.02151.pdf",
    "venue": "Neural Information Processing Systems",
    "publicationDate": "2020-07-04",
    "externalIds": {
      "DBLP": "conf/nips/ZhangKBSW20",
      "MAG": "3038915804",
      "ArXiv": "2007.02151",
      "CorpusId": 220363593
    },
    "references": [
      {
        "paperId": "3ef5ece6e9da0b084abef4b6a7c3daa73063136a",
        "title": "Generalization Bounds for Stochastic Saddle Point Problems"
      },
      {
        "paperId": "b76ee983dd28a388ee95a2ab4cf70e7b6f169e12",
        "title": "On the Global Convergence Rates of Softmax Policy Gradient Methods"
      },
      {
        "paperId": "1f4b7de4a6357eb14d0b3d46e6b3f918a9885f29",
        "title": "Cautious Reinforcement Learning via Distributional Risk in the Dual Domain"
      },
      {
        "paperId": "a2ae87883e63685ab989ab39bf13deed6de63684",
        "title": "Reinforcement Learning via Fenchel-Rockafellar Duality"
      },
      {
        "paperId": "da77c149fdb32d99e4dbb50597b905f30ace2323",
        "title": "Sample Efficient Policy Gradient Methods with Recursive Variance Reduction"
      },
      {
        "paperId": "893a16bfc2e14c4eec45470f76083632470fc41c",
        "title": "Neural Policy Gradient Methods: Global Optimality and Rates of Convergence"
      },
      {
        "paperId": "f7f8f05eb2798272fc3a61443d45f2aa47e65135",
        "title": "On the Theory of Policy Gradient Methods: Optimality, Approximation, and Distribution Shift"
      },
      {
        "paperId": "6509486691e16dbe6cbe13a4fffa8112acae1af3",
        "title": "Optimality and Approximation with Policy Gradient Methods in Markov Decision Processes"
      },
      {
        "paperId": "2e72f9e9cf75203c085a46bfa1eee6939c60b203",
        "title": "LQR through the Lens of First Order Methods: Discrete-time Case"
      },
      {
        "paperId": "34c65ff9691581ef251faeb35d4b48d37ce76a4c",
        "title": "Neural Proximal/Trust Region Policy Optimization Attains Globally Optimal Policy"
      },
      {
        "paperId": "67d88ff410f0fc812866cf0949fa76c8327a56bc",
        "title": "Global Convergence of Policy Gradient Methods to (Almost) Locally Optimal Policies"
      },
      {
        "paperId": "cd67592e68f2da07e53338c39fa7ccf3559660ea",
        "title": "Probability Functional Descent: A Unifying Perspective on GANs, Variational Inference, and Reinforcement Learning"
      },
      {
        "paperId": "adab275554eb745cdc21fd6c526ec55a5b2ef362",
        "title": "Provably Efficient Maximum Entropy Exploration"
      },
      {
        "paperId": "43c3bfffdcd313c549b2045980855ea001d6f13b",
        "title": "Numerical Optimization"
      },
      {
        "paperId": "108b2ea6100d71fd25e3f71743c1e5c4674d6d8c",
        "title": "Stochastic Variance-Reduced Policy Gradient"
      },
      {
        "paperId": "9c0b482314dc8c10875b217b86cc5df5a4fe4bdf",
        "title": "Global Convergence of Policy Gradient Methods for the Linear Quadratic Regulator"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "7a4193d0b042643a8bb9ec262ed7f9d509bdb12e",
        "title": "Constrained Policy Optimization"
      },
      {
        "paperId": "f955b82bbf1c8c6a7265acab1c31d4b0eaa4fe52",
        "title": "Efficiency of minimizing compositions of convex functions and smooth maps"
      },
      {
        "paperId": "372e760481b2c02b025c9fce151fb1c565ccb36f",
        "title": "Policy gradient in Lipschitz Markov Decision Processes"
      },
      {
        "paperId": "449532187c94af3dd3aa55e16d2c50f7854d2199",
        "title": "Trust Region Policy Optimization"
      },
      {
        "paperId": "dc5021a589afb15819e768733a1fc6476ea8b0a0",
        "title": "Adaptive Step-Size for Policy Gradient Methods"
      },
      {
        "paperId": "c5c9bc4d7ee8b04a591018995a8c2d3470655e66",
        "title": "Online learning in episodic Markovian decision processes by relative entropy policy search"
      },
      {
        "paperId": "8efbf5e7da2b14bc47c678c5ef36ec49d0663a6e",
        "title": "Mean-Variance Optimization in Markov Decision Processes"
      },
      {
        "paperId": "9e4b219ee0fb9dfbd03e7a6e65c2cc934feb001d",
        "title": "A General Projection Property for Distribution Families"
      },
      {
        "paperId": "ed271815949e2fd283fd62a8e52a5b30cc769594",
        "title": "Natural actor-critic algorithms"
      },
      {
        "paperId": "52087aa2426a1dea55e41982b2dc7b6956483885",
        "title": "Regularization Techniques for Learning with Matrices"
      },
      {
        "paperId": "f1af9d46eaee472ceb53b6de6bbf8c30ac281716",
        "title": "Lectures on Stochastic Programming: Modeling and Theory"
      },
      {
        "paperId": "4e5dfb0b1e54412e799eb0e86d552956cc3a5f54",
        "title": "A survey of robot learning from demonstration"
      },
      {
        "paperId": "379daa0dabeffa72b9a0d8c48b361236c03fb302",
        "title": "Introduction to Nonparametric Estimation"
      },
      {
        "paperId": "d889403623f3e4717b233fc4ed19718f950e3428",
        "title": "Stochastic Approximation: A Dynamical Systems Viewpoint"
      },
      {
        "paperId": "523b4ce1c2a1336962444abc1dec215756c2f3e6",
        "title": "Approximately Optimal Approximate Reinforcement Learning"
      },
      {
        "paperId": "f868ec8eb5e1ba13f0c0092e68f72d49a65679c3",
        "title": "Risk-Sensitive Optimal Control for Markov Decision Processes with Monotone Cost"
      },
      {
        "paperId": "b18833db0de9393d614d511e60821a1504fc6cd1",
        "title": "A Natural Policy Gradient"
      },
      {
        "paperId": "a20f0ce0616def7cc9a87446c228906cd5da093b",
        "title": "Policy Gradient Methods for Reinforcement Learning with Function Approximation"
      },
      {
        "paperId": "abe0f00db308460ec36618d2a59238c58d8affcd",
        "title": "Actor-Critic - Type Learning Algorithms for Markov Decision Processes"
      },
      {
        "paperId": "3cc2608fd77b9b65f5bd378e8797b2ab1b8acde7",
        "title": "Constrained Markov Decision Processes"
      },
      {
        "paperId": "a9da09d1e63686706d64782e654d69f13fd292ad",
        "title": "Learning from Demonstration"
      },
      {
        "paperId": "cfe5191354fd9188c52500042fe205f31dd14c5f",
        "title": "Reinforcement Learning of Non-Markov Decision Processes"
      },
      {
        "paperId": "b1a621ca86757f4c4bd7fe52a1344e81963b68d4",
        "title": "On Finding Optimal Policies for Markov Decision Chains: A Unifying Framework for Mean-Variance-Tradeoffs"
      },
      {
        "paperId": "a9cd8efe9184dddb1bedbbec3a356c4dfb22fe63",
        "title": "Markov Decision Processes: Discrete Stochastic Dynamic Programming"
      },
      {
        "paperId": "70ff0e77f20f7307dab1f2dd6594649344c56801",
        "title": "Survey of linear programming for standard and nonstandard Markovian control problems. Part I: Theory"
      },
      {
        "paperId": "03b7e51c52084ac1db5118342a00b5fbcfc587aa",
        "title": "Q-learning"
      },
      {
        "paperId": "05fde9039a3c7db11a7e0f933a59fe94505c46ad",
        "title": "Variance-Penalized Markov Decision Processes"
      },
      {
        "paperId": "a91635f8d0e7fb804efd1c38d9c24ee952ba7076",
        "title": "Learning to predict by the methods of temporal differences"
      },
      {
        "paperId": "19ae102bc2b674c80835c21fb649a38b18b6d4e3",
        "title": "Some Remarks on Finite Horizon Markovian Decision Models"
      },
      {
        "paperId": "a053754dde7cb6f891bbf2d95cd8a1f8d0ad5814",
        "title": "Studies in Linear and Non-Linear Programming."
      },
      {
        "paperId": "f4e3738a90f9cc806a25c8739e0d8b892ee7d1ff",
        "title": "Stochastic Estimation of the Maximum of a Regression Function"
      },
      {
        "paperId": "34ddd8865569c2c32dec9bf7ffc817ff42faaa01",
        "title": "A Stochastic Approximation Method"
      },
      {
        "paperId": null,
        "title": "With the above two properties, Theorem 1 of Zhang et al"
      },
      {
        "paperId": null,
        "title": "Clever step-size rules have also been designed to ensure convergence to second-order stationary points under general settings"
      },
      {
        "paperId": null,
        "title": "Another line of work focused on only on per-step value increase, i.e., policy improvement bounds Pirotta et al"
      },
      {
        "paperId": "4c915c1eecb217c123a36dc6d3ce52d12c742614",
        "title": "Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning"
      },
      {
        "paperId": "ac4af1df88e178386d782705acc159eaa0c3904a",
        "title": "Actor-Critic Algorithms"
      },
      {
        "paperId": "54d70dd3b97eb317644851bb527eb2d4b3ff25d5",
        "title": "Linear Programming and Finite Markovian Control Problems"
      },
      {
        "paperId": "32e350939b8d5f690a5a0ca12742c9c9b80992e8",
        "title": "Non-Markovian Processes"
      }
    ],
    "cited_by": [
      {
        "paperId": "6f87ef66a74b035d84b14797c4b63692d7176e7e",
        "title": "On the Convergence of Policy Mirror Descent with Temporal Difference Evaluation"
      },
      {
        "paperId": "66cdd58c568cea165912ee94c2e28f154e0950f6",
        "title": "Policy Gradient with Self-Attention for Model-Free Distributed Nonlinear Multi-Agent Games"
      },
      {
        "paperId": "a3e2b77e49f60477ed8eda5a9db33b6a2b7f02ae",
        "title": "Bayesian Risk-Sensitive Policy Optimization For MDPs With General Loss Functions"
      },
      {
        "paperId": "7da2f1e938c7d9e55795669950f9743691eba788",
        "title": "The Geometry of Nonlinear Reinforcement Learning"
      },
      {
        "paperId": "aebbac54f2b5b5c5283cc787f7a6c4f7875ed26d",
        "title": "Improving Sample Efficiency Through Stability Enhancement in Deep-Reinforcement Learning"
      },
      {
        "paperId": "a00f717e19fc00043e8285187f634fd4682c303b",
        "title": "Algorithms going wild \u2013 A review of machine learning techniques for terrestrial ecology"
      },
      {
        "paperId": "397407e1d4f8717955c858e5d0bdfcae8fedb484",
        "title": "Solving General-Utility Markov Decision Processes in the Single-Trial Regime with Online Planning"
      },
      {
        "paperId": "437c59aa592d016dc782e4568a3dea484a175553",
        "title": "Online Episodic Convex Reinforcement Learning"
      },
      {
        "paperId": "c125fd1e849691f4ff13b3aabdc75f6f2490f05b",
        "title": "Is there Value in Reinforcement Learning?"
      },
      {
        "paperId": "834b432deacf3fd15111245e1f06896432bc7bd9",
        "title": "(Online) Convex Optimization for Demand-Side Management: Application to Thermostatically Controlled Loads"
      },
      {
        "paperId": "f12785fed716ada1272c1c09028ce5c41547fd65",
        "title": "Stable In-Hand Manipulation With Finger-Specific Multi-Agent Shadow Critic Consensus and Information Sharing"
      },
      {
        "paperId": "3d31926d0dfa18125ba68662131022eded2874a9",
        "title": "Towards Unsupervised Multi-Agent Reinforcement Learning via Task-Agnostic Exploration"
      },
      {
        "paperId": "95f7ba9242f527d8696a355e6d31f7a9cc858772",
        "title": "Stochastic Control for Fine-tuning Diffusion Models: Optimality, Regularity, and Convergence"
      },
      {
        "paperId": "7fe39ba9ecace5fde118c74a1f6007f24b090db4",
        "title": "Operator Splitting for Convex Constrained Markov Decision Processes"
      },
      {
        "paperId": "234b0a44c5b57949b4c6941591e01f31eede5ece",
        "title": "Occupancy Information Ratio: Infinite-Horizon, Information-Directed, Parameterized Policy Search"
      },
      {
        "paperId": "de2f158ac50afa168f2daa24bef100b5b5b6401e",
        "title": "Path Planning for Robots Combined with Zero-Shot and Hierarchical Reinforcement Learning in Novel Environments"
      },
      {
        "paperId": "9d512bec75bd2a3b5e00b56e76d92e994340b258",
        "title": "Kernel-Based Function Approximation for Average Reward Reinforcement Learning: An Optimist No-Regret Algorithm"
      },
      {
        "paperId": "b87eb504a47b3df461e290ab0977749376252ad4",
        "title": "Convex Markov Games: A New Frontier for Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "a5ad202ef7fd2e8e46a87e77111a865a1a42009a",
        "title": "Towards Scalable General Utility Reinforcement Learning: Occupancy Approximation, Sample Complexity and Global Optimality"
      },
      {
        "paperId": "84a857d340f0901ba38766f3b58486be50eefcbe",
        "title": "A Prospect-Theoretic Policy Gradient Algorithm for Behavioral Alignment in Reinforcement Learning"
      },
      {
        "paperId": "85c961d5b3fea95b48f94c0461782e887a8b3b0f",
        "title": "Zeroth-Order Policy Gradient for Reinforcement Learning from Human Feedback without Reward Inference"
      },
      {
        "paperId": "0039e67cd656c4c00b0aa2d3d69312d175bdc952",
        "title": "The Number of Trials Matters in Infinite-Horizon General-Utility Markov Decision Processes"
      },
      {
        "paperId": "fb2bce3ecbdbdfb9b0589a010f899562ac31af51",
        "title": "Geometric Active Exploration in Markov Decision Processes: the Benefit of Abstraction"
      },
      {
        "paperId": "cb7ea0176eec1f809f3bc3a34aeb279d44dda612",
        "title": "Global Reinforcement Learning: Beyond Linear and Convex Rewards via Submodular Semi-gradient Methods"
      },
      {
        "paperId": "02cae99f3cfbedb19e2c060cedbb454d175a6417",
        "title": "Optimal dynamic fixed-mix portfolios based on reinforcement learning with second order stochastic dominance"
      },
      {
        "paperId": "4c3e6e691f0c6c1c567bbee86b40c7dcc5af13d8",
        "title": "MetaCURL: Non-stationary Concave Utility Reinforcement Learning"
      },
      {
        "paperId": "bdc9865d33533b28573ea0703b0e33302def8c07",
        "title": "Inverse Concave-Utility Reinforcement Learning is Inverse Game Theory"
      },
      {
        "paperId": "ad2864293056732a410b0eae9e60a9703eb94238",
        "title": "Teamwork Reinforcement Learning With Concave Utilities"
      },
      {
        "paperId": "f8e6a9d32fd8c2a9f30515ab241af383ea3aa4d2",
        "title": "Carbon emission causal discovery and multi-step forecasting for global cities"
      },
      {
        "paperId": "34cdfb3549e546f58202cfaa0f3a22a8df7a74a1",
        "title": "A Dual Perspective of Reinforcement Learning for Imposing Policy Constraints"
      },
      {
        "paperId": "b66b805a15723b953107d462e6e7f6b0226bfef2",
        "title": "Imitation learning from imperfect demonstrations for AUV path tracking and obstacle avoidance"
      },
      {
        "paperId": "c43bea8559a35a9799b804f337f1c9244da14e00",
        "title": "On the Global Convergence of Policy Gradient in Average Reward Markov Decision Processes"
      },
      {
        "paperId": "9fc959a7938d529ccd48969d8e782ab7aa12b7a2",
        "title": "Taming Nonconvex Stochastic Mirror Descent with General Bregman Divergence"
      },
      {
        "paperId": "49adbaa388de5fb679929ff419280ea474035c59",
        "title": "Double Duality: Variational Primal-Dual Policy Optimization for Constrained Reinforcement Learning"
      },
      {
        "paperId": "99d61aa0894413aac41892eb98d9a4c66eb72bf6",
        "title": "On the limitations of Markovian rewards to express multi-objective, risk-sensitive, and modal tasks"
      },
      {
        "paperId": "0b834b82df0a595d10a03a7a3782dd50c828c7c5",
        "title": "On the Stochastic (Variance-Reduced) Proximal Gradient Method for Regularized Expected Reward Optimization"
      },
      {
        "paperId": "49924abbd4d4f32aaf3b136863e925605bf5a408",
        "title": "Quantum Advantage Actor-Critic for Reinforcement Learning"
      },
      {
        "paperId": "959938894a65226b679582372c03f4b1150e218d",
        "title": "Global Convergence of Natural Policy Gradient with Hessian-aided Momentum Variance Reduction"
      },
      {
        "paperId": "d32a4c5df494f09539b08682856c283aa9a96c92",
        "title": "Adversarially Trained Weighted Actor-Critic for Safe Offline Reinforcement Learning"
      },
      {
        "paperId": "5b6d6e88a7cb50d996ccb28775d622f05b29c81a",
        "title": "Stochastic Optimization under Hidden Convexity"
      },
      {
        "paperId": "d37a1b401a8d5a24431d1e1ef0cbb7b25f05704f",
        "title": "Neural Network Approximation for Pessimistic Offline Reinforcement Learning"
      },
      {
        "paperId": "e07c5534b00608599515ac41c9750071a0735470",
        "title": "Efficient Model-Based Concave Utility Reinforcement Learning through Greedy Mirror Descent"
      },
      {
        "paperId": "c3fd795235d76edf168ca4840ff6dd1becb3e307",
        "title": "Projected Policy Gradient Converges in a Finite Number of Iterations"
      },
      {
        "paperId": "f2d48347491eb71f7e73e6b4e858c0bd2b90ace1",
        "title": "On the Convergence of Projected Policy Gradient for Any Constant Step Sizes"
      },
      {
        "paperId": "f2d871fdef5a0f1254835ff9c38079f1884d49e5",
        "title": "Stable In-hand Manipulation with Finger Specific Multi-agent Shadow Reward"
      },
      {
        "paperId": "dcf04310c51bb7a3ac7b0ad68205eb5af4aa8696",
        "title": "Diversifying AI: Towards Creative Chess with AlphaZero"
      },
      {
        "paperId": "c5779b1e50b9a1aed4485e4420ae417a25859d07",
        "title": "Invex Programs: First Order Algorithms and Their Convergence"
      },
      {
        "paperId": "1c68161d9ea0c2b6d101a3f7349b7c205d877fe5",
        "title": "Active Coverage for PAC Reinforcement Learning"
      },
      {
        "paperId": "8a770673c27abc768363922b17f7af39f84ef904",
        "title": "Local Analysis of Entropy-Regularized Stochastic Soft-Max Policy Gradient Methods"
      },
      {
        "paperId": "00bd80faaee3e5404110f915794d1982fedefa43",
        "title": "A Single-Loop Deep Actor-Critic Algorithm for Constrained Reinforcement Learning with Provable Convergence"
      },
      {
        "paperId": "b2dde612d47a9d747f758579dc6eef3294c24ab7",
        "title": "Reinforcement Learning with General Utilities: Simpler Variance Reduction and Large State-Action Space"
      },
      {
        "paperId": "d102a6b4ebf6848095c7a44117e6a1ee0fd8a1f7",
        "title": "On the Linear Convergence of Policy Gradient under Hadamard Parameterization"
      },
      {
        "paperId": "55e9de9e89eeae52dd50c41eb9f334383af2f924",
        "title": "Scalable Primal-Dual Actor-Critic Method for Safe Multi-Agent RL with General Utilities"
      },
      {
        "paperId": "423bf19b4cc2b23aaa7aba74d3fad7934ea3c096",
        "title": "Inverse Reinforcement Learning with the Average Reward Criterion"
      },
      {
        "paperId": "e92bc8d005874e821aea728084e310587d5dc451",
        "title": "Achieving the Asymptotically Optimal Sample Complexity of Offline Reinforcement Learning: A DRO-Based Approach"
      },
      {
        "paperId": "9c0ec2041591257056ba5a9ec4087d83db3b87a9",
        "title": "A Coupled Flow Approach to Imitation Learning"
      },
      {
        "paperId": "081390fc1bc34dca2a191266a0cd3e9f815c33df",
        "title": "What can online reinforcement learning with function approximation benefit from general coverage conditions?"
      },
      {
        "paperId": "3721c70ed660a73592ae4adad46de5cba6abd63d",
        "title": "Information-Directed Policy Search in Sparse-Reward Settings via the Occupancy Information Ratio"
      },
      {
        "paperId": "e195f648de80d4d1a744d7dfe0929586b0fc4387",
        "title": "Policy Gradient Converges to the Globally Optimal Policy for Nearly Linear-Quadratic Regulators"
      },
      {
        "paperId": "0ba3b0f7d480760fd66308a0f08ae5535590cdf4",
        "title": "n-Step Temporal Difference Learning with Optimal n"
      },
      {
        "paperId": "00f742790f41f52b1bf5ab49d802293ecf3c211e",
        "title": "Deep Reinforcement Learning for Cost-Effective Medical Diagnosis"
      },
      {
        "paperId": "bbd37e1bb19566182b216c3076ba8cf30ac4a78f",
        "title": "Scalable Multi-Agent Reinforcement Learning with General Utilities"
      },
      {
        "paperId": "cf8626c23d1d22405b9cc8061044ce7d8f8adf77",
        "title": "Provably Efficient Offline Goal-Conditioned Reinforcement Learning with General Function Approximation and Single-Policy Concentrability"
      },
      {
        "paperId": "6424daf5ccaa409788804c46efadc39ae60538e7",
        "title": "Stochastic Policy Gradient Methods: Improved Sample Complexity for Fisher-non-degenerate Policies"
      },
      {
        "paperId": "b47e3d1cebb549d4ed9f420fc2ebadabc8997a9c",
        "title": "Importance Weighted Actor-Critic for Optimal Conservative Offline Reinforcement Learning"
      },
      {
        "paperId": "0f71a43a5cd29368bcc2f2ff5e8769f69c20fff6",
        "title": "A Novel Framework for Policy Mirror Descent with General Parametrization and Linear Convergence"
      },
      {
        "paperId": "63888b247d2e42c9b9a64e15bbf4a89877fd797f",
        "title": "Optimal Conservative Offline RL with General Function Approximation via Augmented Lagrangian"
      },
      {
        "paperId": "be4c0c6d3590401ea41a0eca481beb9ea20b22c1",
        "title": "Proximal Mean Field Learning in Shallow Neural Networks"
      },
      {
        "paperId": "f7e93c8adec97784ec0e18d4fd56d982a8922e0d",
        "title": "Linear Convergence of Natural Policy Gradient Methods with Log-Linear Policies"
      },
      {
        "paperId": "5574d0508f8fc800bef8fa2d6637b2b0cd744778",
        "title": "Policy Gradient for Reinforcement Learning with General Utilities"
      },
      {
        "paperId": "a8bffbe10134b53c6182556afe688fe41d32feb3",
        "title": "On the convex formulations of robust Markov decision processes"
      },
      {
        "paperId": "63e2e4468af5522fef6686e36a2ade2506f70186",
        "title": "Cross Apprenticeship Learning Framework: Properties and Solution Approaches"
      },
      {
        "paperId": "c3b849ac59741497ca70e9ceabb9367b50b22f42",
        "title": "Improved Policy Optimization for Online Imitation Learning"
      },
      {
        "paperId": "4a9018b08ced98a84998348bd55d5be26af5947f",
        "title": "Reinforcement Learning of Space Robotic Manipulation with Multiple Safety Constraints"
      },
      {
        "paperId": "18247aad4c2f600c301234b2bacfa48643cc34f4",
        "title": "Multi-Agent Reinforcement Learning with General Utilities via Decentralized Shadow Reward Actor-Critic"
      },
      {
        "paperId": "7a55e20aa5306526053590c4899bd7c7777dfa30",
        "title": "Anchor-Changing Regularized Natural Policy Gradient for Multi-Objective Reinforcement Learning"
      },
      {
        "paperId": "5791a11c247c1fbb810fd306542c79c22e54baaf",
        "title": "Convergence and sample complexity of natural policy gradient primal-dual methods for constrained MDPs"
      },
      {
        "paperId": "9edd5da9ef53dc8526bdf9af3e1dd40fe593a1a7",
        "title": "On linear and super-linear convergence of Natural Policy Gradient algorithm"
      },
      {
        "paperId": "ee626478aafe6496c86512660820933538059ef2",
        "title": "Discovering Policies with DOMiNO: Diversity Optimization Maintaining Near Optimality"
      },
      {
        "paperId": "a4b2b1e20e441512cf2581f58a8edfbb3f62fec9",
        "title": "Stochastic Second-Order Methods Improve Best-Known Sample Complexity of SGD for Gradient-Dominated Functions"
      },
      {
        "paperId": "79f0b2613b3fa27f39efcb8b7703dfc8e54bfc66",
        "title": "Policy Gradient Method For Robust Reinforcement Learning"
      },
      {
        "paperId": "f3a63a840185fa8c5e0db4bbe12afa7d3de7d029",
        "title": "Jump-Start Reinforcement Learning"
      },
      {
        "paperId": "dfa750f194f85c33de40c2b40c01b669fc085d7c",
        "title": "A Distributed Algorithm for Measure-valued Optimization with Additive Objective"
      },
      {
        "paperId": "399bbcecf674a55fb1a91b178a5be4a8999bb0ac",
        "title": "The Importance of Non-Markovianity in Maximum State Entropy Exploration"
      },
      {
        "paperId": "fa950a27da0281095f7be7d8a2224391dcbd247b",
        "title": "Challenging Common Assumptions in Convex Reinforcement Learning"
      },
      {
        "paperId": "af24b35eaa09a0f6a10499f91b7e023dd927df37",
        "title": "Engram-Driven Videography"
      },
      {
        "paperId": "792325a827ac69bd8ece3f128cf257a2a4568beb",
        "title": "Optimal Estimation of Off-Policy Policy Gradient via Double Fitted Iteration"
      },
      {
        "paperId": "6e0c6f6b5355700e53e239655387e6fc6c029972",
        "title": "Occupancy Information Ratio: Infinite-Horizon, Information-Directed, Parameterized Policy Search"
      },
      {
        "paperId": "83245afa59b9030f6149a954408660872b738fc6",
        "title": "On the Convergence Rates of Policy Gradient Methods"
      },
      {
        "paperId": "708b04a43ceb2a914652d2b3a8b5afe28b1875ba",
        "title": "DeCOM: Decomposed Policy for Constrained Cooperative Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "1378b2b507fbbc6d475835eeb740f1c7426d7be5",
        "title": "Convergence and Optimality of Policy Gradient Methods in Weakly Smooth Settings"
      },
      {
        "paperId": "6058044b9e244bbcf1dcb7f7366abcd6f8f1bdb3",
        "title": "Beyond Exact Gradients: Convergence of Stochastic Soft-Max Policy Gradient Methods With Entropy Regularization"
      },
      {
        "paperId": "50deb88edf64ed9164a82f359b0cf71c1bfa9db0",
        "title": "On the Global Optimum Convergence of Momentum-based Policy Gradient"
      },
      {
        "paperId": "7f4c3b16b41c6d0fa6f7116aff80a545d23b2e47",
        "title": "Primal-Dual First-Order Methods for Affinely Constrained Multi-block Saddle Point Problems"
      },
      {
        "paperId": "24fda3cbf8b776aea69ef4f2d5ef11f92d3d4011",
        "title": "Theoretical Guarantees of Fictitious Discount Algorithms for Episodic Reinforcement Learning and Global Convergence of Policy Gradient Methods"
      },
      {
        "paperId": "85d44bb076fa060345132ee5149cd73d30dbe417",
        "title": "Concave Utility Reinforcement Learning with Zero-Constraint Violations"
      },
      {
        "paperId": "1e00e8760472dab0fe1632a04a037d52d227d3a6",
        "title": "Provable Benefits of Actor-Critic Methods for Offline Reinforcement Learning"
      },
      {
        "paperId": "c39bb480b2584b2d6636223e98c5d11f5c78f3ad",
        "title": "A general sample complexity analysis of vanilla policy gradient"
      },
      {
        "paperId": "7b200d0435ca9594f16d37e1cbea665928d9a9a8",
        "title": "A Unified Off-Policy Evaluation Approach for General Value Function"
      },
      {
        "paperId": "9e198f46517288ca2665866b518a3494f4ba8f7a",
        "title": "On the Sample Complexity and Metastability of Heavy-tailed Policy Search in Continuous Control"
      },
      {
        "paperId": "046bc091cbe8354d965cb157a44a9934621198ad",
        "title": "Concave Utility Reinforcement Learning: the Mean-field Game viewpoint"
      },
      {
        "paperId": "5c37023c35fc1c95565d56b4fc4821fcf768651a",
        "title": "Reward is enough for convex MDPs"
      },
      {
        "paperId": "b07cd172cac44021e292bf7e90cb60728135bcf8",
        "title": "MARL with General Utilities via Decentralized Shadow Reward Actor-Critic"
      },
      {
        "paperId": "c1d17bf4fca65c2396555e5d7c9413857c9b919f",
        "title": "Joint Optimization of Concave Scalarized Multi-Objective Reinforcement Learning with Policy Gradient Based Algorithm"
      },
      {
        "paperId": "156b41fbf8364a1b43405109bf173f8edfffe784",
        "title": "Finite-Sample Analysis of Off-Policy Natural Actor\u2013Critic With Linear Function Approximation"
      },
      {
        "paperId": "3fc931f90360a8791e7a2566fcdbacf1200117dd",
        "title": "Policy Mirror Descent for Regularized Reinforcement Learning: A Generalized Framework with Linear Convergence"
      },
      {
        "paperId": "13de4b51300e341d0fd31408d88ad366b1028d3a",
        "title": "Frank-Wolfe Methods in Probability Space"
      },
      {
        "paperId": "cdab74596700b9cfbd92d2801290d317a08ebc36",
        "title": "On the Linear Convergence of Natural Policy Gradient Algorithm"
      },
      {
        "paperId": "0bcc734246586966596b1aa22efac2565224ebee",
        "title": "Bridging Offline Reinforcement Learning and Imitation Learning: A Tale of Pessimism"
      },
      {
        "paperId": "2ef267d7f163b28ce9e56b5136f55486d83caa04",
        "title": "Softmax policy gradient methods can take exponential time to converge"
      },
      {
        "paperId": "edbe8b72ad7bd4d0cb90245f862acafc0647c115",
        "title": "Softmax Policy Gradient Methods Can Take Exponential Time to Converge"
      },
      {
        "paperId": "8b6edbdf22c6417b265838c6b1d8c35520abdc2d",
        "title": "On the Convergence and Sample Efficiency of Variance-Reduced Policy Gradient Method"
      },
      {
        "paperId": "33d9980d90a8dcdaf2ee2105e84fe6a1076d7b6e",
        "title": "Distributionally-Constrained Policy Optimization via Unbalanced Optimal Transport"
      },
      {
        "paperId": "094cd58f07e7d82084cc8d88eb447c7f0234dc35",
        "title": "Regularized Policies are Reward Robust"
      },
      {
        "paperId": "9f8a70e9188a913317e97ba1874c8f763fd13c04",
        "title": "Is Pessimism Provably Efficient for Offline RL?"
      },
      {
        "paperId": "7b76f49832c86c2e08132bfd3410587f99e8ceb6",
        "title": "Sample Efficient Reinforcement Learning with REINFORCE"
      },
      {
        "paperId": "ed9abb4e7222171f226dafb96e3e8bb5cd081f8d",
        "title": "A Note on the Linear Convergence of Policy Gradient Methods"
      },
      {
        "paperId": "6874b196102f1de902bef1679ec26337bde6891a",
        "title": "Policy Gradient From Demonstration and Curiosity"
      },
      {
        "paperId": "716fd9bf371675b08cc14325aed2fd4030df8980",
        "title": "Model-Free Offline Reinforcement Learning with Enhanced Robustness"
      },
      {
        "paperId": "dacc3a8d45968616f220628dc0db8d5d78c1a389",
        "title": "MaxMin-RLHF: Towards Equitable Alignment of Large Language Models with Diverse Human Preferences"
      },
      {
        "paperId": "bf4ef6a4acbb3fece21510765a4ad6585440fd0b",
        "title": "Safe and Efficient: A Primal-Dual Method for Offline Convex CMDPs under Partial Data Coverage"
      },
      {
        "paperId": "c53b000f403cdbd0741f1f2a3004876fada69adb",
        "title": "Generalizing Objective-Specification in Markov Decision Processes"
      },
      {
        "paperId": "5ff63c548a44c7eed8f389126b6691d2698954d5",
        "title": "Convex Markov Games: A Framework for Fairness, Imitation, and Creativity in Multi-Agent Learning"
      },
      {
        "paperId": "ca9552869441ab0f42aab60bb53510261ca042dc",
        "title": "Single-Loop Deep Actor-Critic for Constrained Reinforcement Learning With Provable Convergence"
      },
      {
        "paperId": "593b54b5063ae634a2262dca02b6f2ed2b96bdf2",
        "title": "A Unified Principle of Pessimism for Offline Reinforcement Learning under Model Mismatch"
      },
      {
        "paperId": "2418d6703faea16ec978649c281b89e0b70f15dd",
        "title": "Convex Reinforcement Learning in Finite Trials"
      },
      {
        "paperId": "720f78c522b0101ef9053f96055a4a31e50f36d5",
        "title": "A M IRROR D ESCENT APPROACH FOR M EAN F IELD C ONTROL APPLIED TO DEMANDE - SIDE MANAGEMENT"
      },
      {
        "paperId": "b5a4303a1b0c8c822cc24bb9f2b08dae1246a836",
        "title": "Provable bene\ufb01ts of general coverage conditions in ef\ufb01cient online reinforcement learning with function approximation"
      },
      {
        "paperId": "460c3bcb289b0771accead06c9bfc10f9bbc7795",
        "title": "Multi-Objective Reinforcement Learning: Convexity, Stationarity and Pareto Optimality"
      },
      {
        "paperId": "9f1812fc0f6f8aeaff5bc94b23a3fe61f31f7d68",
        "title": "Bayesian inference approach for entropy regularized reinforcement learning with stochastic dynamics"
      },
      {
        "paperId": "2ff62bc3025181772045bb7d3e344a45f6e51342",
        "title": "Distributionally Robust Optimization Efficiently Solves Offline Reinforcement Learning"
      },
      {
        "paperId": "eb3aabdb6494217df771b9615d7a721f010ed5fb",
        "title": "Stochastic Second-Order Methods Provably Beat SGD For Gradient-Dominated Functions"
      },
      {
        "paperId": "77ef50bb715b1e640fbdf0b08e0e39a35e2d3140",
        "title": "Optimal Estimation of Policy Gradient via Double Fitted Iteration"
      },
      {
        "paperId": "0885bd4a087450be349b97fb34b8d702519f56fb",
        "title": "Policy gradient finds global optimum of nearly linear-quadratic control systems"
      },
      {
        "paperId": "87665b61f29bd9803d4056110a9d760ae45e8e38",
        "title": "A Unifying Framework of Off-Policy General Value Function Evaluation"
      },
      {
        "paperId": "af373874d31ce55327c6266fe889aca17ac73afb",
        "title": "Achieving Zero Constraint Violation for Concave Utility Constrained Reinforcement Learning via Primal-Dual Approach"
      },
      {
        "paperId": "e08d0137fae4ab8fc673facec3345c8eccd052f5",
        "title": "Joint Optimization of Multi-Objective Reinforcement Learning with Policy Gradient Based Algorithm"
      },
      {
        "paperId": "43a47a52e99faa36073906157458139cd8de6efb",
        "title": "Global Convergence of Stochastic Policy Gradient Methods with Momentum and Entropy"
      },
      {
        "paperId": "de1eb0b5c003d3869daba7b65511736c59f1be0e",
        "title": "On the Global Convergence of Momentum-based Policy Gradient"
      },
      {
        "paperId": "bcfd8ca8d2e1ff419f9f767bbe4b0c407bec26fc",
        "title": "Modified Frank Wolfe in Probability Space"
      },
      {
        "paperId": "4fe04877d209e19788ba149e4fef2ff842bd7bea",
        "title": "Pessimism Meets Invariance: Provably Efficient Offline Mean-Field Multi-Agent RL"
      },
      {
        "paperId": null,
        "title": "Optimal Transport in Reinforcement Learning"
      },
      {
        "paperId": "fe4f9d8e18c4e70d3f75fc0d1d343815be6a5245",
        "title": "Natural Policy Gradient Primal-Dual Method for Constrained Markov Decision Processes"
      },
      {
        "paperId": "ee1a0124690927ba1906cea13bc85fe327057753",
        "title": "E\ufb00icient Model-Based Concave Utility Reinforcement Learning through Greedy Mirror Descent"
      },
      {
        "paperId": "9844ebddb34b08afb3241d4b2610673a75a60265",
        "title": "E\ufb03cient Model-Based Concave Utility Reinforcement Learning through Greedy Mirror Descent"
      },
      {
        "paperId": "d793973eddfc577a2f1e409c7904f8ef9f18e1f8",
        "title": "Policy Optimization for Robust Average Cost MDPs"
      }
    ],
    "score": 29.200000000000003
  },
  {
    "id": "6ce21379ffac786207632d16ea7d6e3eb150f910",
    "title": "Deep Reinforcement Learning for Dynamic Flexible Job Shop Scheduling with Random Job Arrival",
    "authors": [
      "Jingru Chang",
      "Dong Yu",
      "Y. Hu",
      "Wuwei He",
      "Haoyu Yu"
    ],
    "year": 2022,
    "citationCount": 82,
    "abstract": "The production process of a smart factory is complex and dynamic. As the core of manufacturing management, the research into the flexible job shop scheduling problem (FJSP) focuses on optimizing scheduling decisions in real time, according to the changes in the production environment. In this paper, deep reinforcement learning (DRL) is proposed to solve the dynamic FJSP (DFJSP) with random job arrival, with the goal of minimizing penalties for earliness and tardiness. A double deep Q-networks (DDQN) architecture is proposed and state features, actions and rewards are designed. A soft \u03b5-greedy behavior policy is designed according to the scale of the problem. The experimental results show that the proposed DRL is better than other reinforcement learning (RL) algorithms, heuristics and metaheuristics in terms of solution quality and generalization. In addition, the soft \u03b5-greedy strategy reasonably balances exploration and exploitation, thereby improving the learning efficiency of the scheduling agent. The DRL method is adaptive to the dynamic changes of the production environment in a flexible job shop, which contributes to the establishment of a flexible scheduling system with self-learning, real-time optimization and intelligent decision-making.",
    "url": "https://www.semanticscholar.org/paper/6ce21379ffac786207632d16ea7d6e3eb150f910",
    "pdf_url": "https://doi.org/10.3390/pr10040760",
    "venue": "Processes",
    "publicationDate": "2022-04-13",
    "externalIds": {
      "DOI": "10.3390/pr10040760",
      "CorpusId": 248194368
    },
    "references": [
      {
        "paperId": "63b8fc21a3e13192e73d95d9c6a575c32ca47f8e",
        "title": "Robustness Assessment of Asynchronous Advantage Actor-Critic Based on Dynamic Skewness and Sparseness Computation: A Parallel Computing View"
      },
      {
        "paperId": "94620ee5cbefebc36637958431f1d6cbfc612ff9",
        "title": "Problem-specific multi-objective invasive weed optimization algorithm for reconnaissance mission scheduling problem"
      },
      {
        "paperId": "67080495f92e76e80693bf13f26b5ae9cef8d421",
        "title": "Intelligent Decision-Making of Scheduling for Dynamic Permutation Flowshop via Deep Reinforcement Learning"
      },
      {
        "paperId": "141405facf8967fc885f0f810b7354b846b4d6c8",
        "title": "An effective Iterated Greedy algorithm for the distributed permutation flowshop scheduling with due windows"
      },
      {
        "paperId": "83ef266b8dc362807db165feec186abfa5992691",
        "title": "Adaptive scheduling for assembly job shop with uncertain assembly times based on dual Q-learning"
      },
      {
        "paperId": "39415bb121fdb116297b0d389ee12c1ffa937a92",
        "title": "Dynamic scheduling for flexible job shop with new job insertions by deep reinforcement learning"
      },
      {
        "paperId": "47e450de95b2b939d1350a224ae93a5ca785e3aa",
        "title": "NSGA\u2010III for solving dynamic flexible job shop scheduling problem considering deterioration effect"
      },
      {
        "paperId": "34b373a0e5f5bcf2294a05203057beb74e894563",
        "title": "The dynamics of behavior: Review of Sutton and Barto:\n Reinforcement Learning\n :\n An Introduction\n (2\n nd\n ed.)"
      },
      {
        "paperId": "f27632d6ba71ac4bdbb4ba48edfde74007af5577",
        "title": "Intelligent scheduling of discrete automated production line via deep reinforcement learning"
      },
      {
        "paperId": "d23c7bcb68682ccaa0870fc7c4b19c52ceda7cdb",
        "title": "Performing Deep Recurrent Double Q-Learning for Atari Games"
      },
      {
        "paperId": "6113178d1d40e7915e63bcd872fffbc29c9fa6b5",
        "title": "Flexible Job-Shop Rescheduling for New Job Insertion by Using Discrete Jaya Algorithm"
      },
      {
        "paperId": "6373e136f7cec69894388ba843ac93a0c9b1a8bb",
        "title": "Adaptive job shop scheduling strategy based on weighted Q-learning algorithm"
      },
      {
        "paperId": "3d0503b2a989a19231bd51ff69e75df0a578d056",
        "title": "An adaptive neuro-fuzzy inference system for makespan estimation of flexible manufacturing system assembly shop: a case study"
      },
      {
        "paperId": "de6c771ef6089753dcabedffe4a5b68c55aeaf73",
        "title": "Algorithms based on VNS for solving the Single Machine Scheduling Problem with Earliness and Tardiness Penalties"
      },
      {
        "paperId": "0d97681e96ace76864ab6628b3fbad6516aa6e56",
        "title": "Real-time scheduling for a smart factory using a reinforcement learning approach"
      },
      {
        "paperId": "63f9e6a1d28e66c0ad9c1566b7cdd1df14ce59e0",
        "title": "A reinforcement learning approach to parameter estimation in dynamic job shop scheduling"
      },
      {
        "paperId": "4ae5d2eae19af2ce057b2e97874a019267fef8f5",
        "title": "A distributed approach solving partially flexible job-shop scheduling problem with a Q-learning effect"
      },
      {
        "paperId": "5d381d47bb57ea57a864fd7dc2ea5ebf3c01c27c",
        "title": "An improved quantum genetic algorithm based on MAGTD for dynamic FJSP"
      },
      {
        "paperId": "69e76e16740ed69f4dc55361a3d319ac2f1293dd",
        "title": "Asynchronous Methods for Deep Reinforcement Learning"
      },
      {
        "paperId": "f5079168f914014e9cfe888bdfd58b7bac2b1fe9",
        "title": "Implementing Smart Factory of Industrie 4.0: An Outlook"
      },
      {
        "paperId": "2de9da418cd4e29a5797320277aac284497b72ab",
        "title": "A two-stage artificial bee colony algorithm scheduling flexible job-shop scheduling problem with new job insertion"
      },
      {
        "paperId": "4c05d7caa357148f0bbd61720bdd35f0bc05eb81",
        "title": "Dueling Network Architectures for Deep Reinforcement Learning"
      },
      {
        "paperId": "3b9732bb07dc99bde5e1f9f75251c6ea5039373e",
        "title": "Deep Reinforcement Learning with Double Q-Learning"
      },
      {
        "paperId": "340f48901f72278f6bf78a04ee5b01df208cc508",
        "title": "Human-level control through deep reinforcement learning"
      },
      {
        "paperId": "2319a491378867c7049b3da055c5df60e1671158",
        "title": "Playing Atari with Deep Reinforcement Learning"
      },
      {
        "paperId": "644a079073969a92674f69483c4a85679d066545",
        "title": "Double Q-learning"
      },
      {
        "paperId": "e1933c9ca478a336d67efa56606223a261bf9a8b",
        "title": "Heuristic, optimal, static, and dynamic schedules when processing times are uncertain"
      },
      {
        "paperId": "17b6915eccac15c3b2ece23889e9f7898b9c6915",
        "title": "Job-shop scheduling with multi-purpose machines"
      },
      {
        "paperId": "be5f6a576cda7e56bf2beef996685d111e9ebd7b",
        "title": "The Complexity of Flowshop and Jobshop Scheduling"
      },
      {
        "paperId": "d2fc27a97e0d0de3f07f21d5a56eafef54c358d8",
        "title": "Dynamic multi-objective scheduling for flexible job shop by deep reinforcement learning"
      },
      {
        "paperId": "172bbeeae7679dd86452dcdab610241703a07f42",
        "title": "Towards Energy Efficient Scheduling and Rescheduling for Dynamic Flexible Job Shop Problem"
      },
      {
        "paperId": null,
        "title": "An Improvement of Reinforcement Learning Approach to Permutational Flow Shop Scheduling Problem"
      },
      {
        "paperId": null,
        "title": "Q-Learning Based Dynamic Singe Machine Scheduling"
      },
      {
        "paperId": null,
        "title": "A DDQN algorithm model of \ufb02exible dynamic scheduling is proposed and state features, actions and rewards for the scheduling agent have been designed"
      },
      {
        "paperId": null,
        "title": "The agent determines the scheduling rule"
      },
      {
        "paperId": null,
        "title": "The \ufb02exible production shop performs a t : O ij is processed on M k and the production environment state is transferred to s t+ 1"
      },
      {
        "paperId": "bf265b2db566b5de50e97bc4a8b6193f174318c3",
        "title": "Problem-speci\ufb01c multi-objective invasive weed optimization algorithm for reconnaissance mission scheduling problem"
      }
    ],
    "cited_by": [
      {
        "paperId": "fd0e907795c20b1c1b4c575cfae14f32f5eb7872",
        "title": "Scalable multi-agent reinforcement learning for factory-wide dynamic scheduling in semiconductor manufacturing"
      },
      {
        "paperId": "cf9271587532f930e9e3eb14ddf851d1753914ad",
        "title": "Dynamic scheduling for dual-resource constrained flexible job-shop via semantic-aware graph modelling and deep reinforcement learning"
      },
      {
        "paperId": "84683fcbb64aa4061f2836375bca7399e9107086",
        "title": "Dynamic scheduling for flexible job-shop with quality abnormality caused by machine fault"
      },
      {
        "paperId": "498daf67397b59a4c7afabb64201edfd7ceb8380",
        "title": "Multi-objective dynamic flexible job shop scheduling using multi-head network-based deep reinforcement learning"
      },
      {
        "paperId": "f0f8ee25506a275d650b2570d76e6337d3e00109",
        "title": "Train small, deploy large: Scaling multi-agent reinforcement learning for multi-stage manufacturing lines"
      },
      {
        "paperId": "1551994b708baee862f0e242a8bfe8c958022001",
        "title": "A novel dynamic scheduling model for application in multimode approach"
      },
      {
        "paperId": "912ba25c6e15ef2cafd83a2ef382e911eb21cf83",
        "title": "Carbon reduction effective advanced planning and scheduling (C-APS) system: framework, functions and empirical results"
      },
      {
        "paperId": "36aba46b65de71ff5913946f4574ea9242766576",
        "title": "A Production Scheduling Framework for Reinforcement Learning Under Real-World Constraints"
      },
      {
        "paperId": "43c3022645cf858e06fa947e9616989d0beb687f",
        "title": "Data-driven hierarchical multi-policy deep reinforcement learning framework for multi-objective multiplicity dynamic flexible job shop scheduling"
      },
      {
        "paperId": "f05dafac0b53563f07876f5ae4b320944d6b65df",
        "title": "A deep reinforcement learning approach with graph attention network and multi-signal differential reward for dynamic hybrid flow shop scheduling problem"
      },
      {
        "paperId": "eda96dd01e93749f97446a4f84f48d9811aff8eb",
        "title": "Adaptive Bias Generalized Rollout Policy Adaptation on the Flexible Job-Shop Scheduling Problem"
      },
      {
        "paperId": "4e41d5b0f254641a921ac3da5b60ad171a8c1f88",
        "title": "Research on the Energy Efficiency Optimization Scheduling Method of Multi-Load Trolleys in Workshops"
      },
      {
        "paperId": "fdfa27541f1be979df785ba59eba1ff09de9ec8e",
        "title": "Deep reinforcement learning for job shop scheduling problems: A comprehensive literature review"
      },
      {
        "paperId": "c4a005fbb1813066b5b69749c23fee83af34d55a",
        "title": "Hybrid Flow Shop Scheduling through Reinforcement Learning: A systematic literature review"
      },
      {
        "paperId": "e09fe1fcedff59e47146de7f346a4a2123d2fc12",
        "title": "Learn to optimise for job shop scheduling: a survey with comparison between genetic programming and reinforcement learning"
      },
      {
        "paperId": "89be67a4798c8fcccab13442e6d02e8aa9c4a705",
        "title": "Dynamic scheduling for multi-objective flexible job shop via deep reinforcement learning"
      },
      {
        "paperId": "53fd2a0775953ceff53ba55de7ed9f2d8b3682db",
        "title": "PTB: A deep reinforcement learning method for flexible logistics service combination problem with spatial-temporal constraint"
      },
      {
        "paperId": "d34f26471e12bf75d2cc8fd8ad30f2c9f8b394b8",
        "title": "Research on dynamic job shop scheduling problem with AGV based on DQN"
      },
      {
        "paperId": "ea46af9ecda5e4077dd02e78f3edffcfd596abc0",
        "title": "Deep Reinforcement Learning for Adaptive Job Shop Scheduling in Robotic Cells: Handling Disruptions Effectively"
      },
      {
        "paperId": "faff1309ee2a81522b0dd8530bcb845f33ccc533",
        "title": "A dynamic flexible job shop scheduling method based on collaborative agent reinforcement learning"
      },
      {
        "paperId": "9dcd79706db07d9df3d3adc2a8746be1460343e3",
        "title": "An effective two-stage memetic algorithm for the dynamic flexible job-shop scheduling problem with job inspection"
      },
      {
        "paperId": "de45441c7fe4910b01fabe0f739226be1dd7d8fb",
        "title": "Multi-Objective Optimization in Industry 5.0: Human-Centric AI Integration for Sustainable and Intelligent Manufacturing"
      },
      {
        "paperId": "46e0bcc31be4069876dc2849b21214eba339d5c7",
        "title": "Review on ensemble meta-heuristics and reinforcement learning for manufacturing scheduling problems"
      },
      {
        "paperId": "db57e36cc379924ec825f812550e1519a4d3fdcf",
        "title": "Deep reinforcement learning-based dynamic scheduling for resilient and sustainable manufacturing: A systematic review"
      },
      {
        "paperId": "b2434bc2141e72c68e336413cae0ec48825e644b",
        "title": "Outpatient scheduling problem in smart hospital with two-agent deep reinforcement learning algorithm"
      },
      {
        "paperId": "450f5fd2bd55029ac2cdff6a72c14b70f91d2d0e",
        "title": "A literature review of reinforcement learning methods applied to job-shop scheduling problems"
      },
      {
        "paperId": "7e69d2364aa334aaced769a4276c51c6eab4a8ca",
        "title": "Knowledge graph-enhanced multi-agent reinforcement learning for adaptive scheduling in smart manufacturing"
      },
      {
        "paperId": "8396cb42c0e80e2e75cad4ccfce526a597c2a736",
        "title": "A Resilience Assurance Process Model for Enhancing Virtual Reality Architectural Design"
      },
      {
        "paperId": "03742bdd05bbf4a642d0204523be3364e6f4b10a",
        "title": "Dynamic production scheduling and maintenance planning under opportunistic grouping"
      },
      {
        "paperId": "b9343cc084e2b52bcc9acb8da20c8b3c1710b06b",
        "title": "Two-stage double deep Q-network algorithm considering external non-dominant set for multi-objective dynamic flexible job shop scheduling problems"
      },
      {
        "paperId": "ee1f380cee9cbb56b3c119ff5cd38bb01c7f34f3",
        "title": "Dynamic flexible job-shop scheduling by multi-agent reinforcement learning with reward-shaping"
      },
      {
        "paperId": "282fe6cb284cee620c234eaa453c5a6adbdf4f5a",
        "title": "Scalable Multi-agent Reinforcement Learning for Factory-wide Dynamic Scheduling"
      },
      {
        "paperId": "f1d5871767bdea4142b399d5a3e0324f0e3b1db5",
        "title": "Towards the application of machine learning in digital twin technology: a multi-scale review"
      },
      {
        "paperId": "526711d5628b45d5a20582de52a14c552c6a4f78",
        "title": "Dynamic flexible job shop scheduling based on deep reinforcement learning"
      },
      {
        "paperId": "66770b3963b98286644d3da67fdf8a86cb235545",
        "title": "Optimisation of remanufacturing supply chain with dual recycling channels under improved deep reinforcement learning algorithm"
      },
      {
        "paperId": "c8e129ac6ca26592a2acc85ada558dd52a61d9cd",
        "title": "A Review on Reinforcement Learning in Production Scheduling: An Inferential Perspective"
      },
      {
        "paperId": "92264b2b20f38ed9b2e2f0d9c719e5fb0199c351",
        "title": "Review of Cloud Service Composition for Intelligent Manufacturing"
      },
      {
        "paperId": "0d751ee6a64070442efcbf12ff1c34d5bb43bae6",
        "title": "Design patterns of deep reinforcement learning models for job shop scheduling problems"
      },
      {
        "paperId": "d12b4810540d7d75b99735989de6aa8f3a3dd0d4",
        "title": "A Double Deep Q-Network framework for a flexible job shop scheduling problem with dynamic job arrivals and urgent job insertions"
      },
      {
        "paperId": "160938c3ff1caf44cf7e090a1be07e9e88cda819",
        "title": "Real-time stochastic flexible flow shop scheduling in a credit factory with model-based reinforcement learning"
      },
      {
        "paperId": "201d954ec0336e96994928ec515163771e2bd7f0",
        "title": "Dynamic Stability-Aware Scheduling based on Dueling Double Deep Q-Network in FJSP"
      },
      {
        "paperId": "c2ef1a10ecc39de38972b7439610e3b603390556",
        "title": "Deep Learning-Based Dynamic Scheduling for Semiconductor Manufacturing with High Uncertainty of Automated Material Handling System Capability"
      },
      {
        "paperId": "65a58205ed00263e864151f5da53ee9dd7b21f7c",
        "title": "An improved memetic algorithm for distributed hybrid flow shop scheduling problem with operation inspection and reprocessing"
      },
      {
        "paperId": "14888a3e936941f7fcdfd2683a2563fcb16c37e3",
        "title": "Genetic Programming and Reinforcement Learning on Learning Heuristics for Dynamic Scheduling: A Preliminary Comparison"
      },
      {
        "paperId": "b57bf62d5954610b34f3ac0719e88d837774c25c",
        "title": "Robust-optimization-guiding deep reinforcement learning for chemical material production scheduling"
      },
      {
        "paperId": "92ca8804c2c07ec3e950abb98b01e5e31392e567",
        "title": "Q-learning guided algorithms for bi-criteria minimization of total flow time and makespan in no-wait permutation flowshops"
      },
      {
        "paperId": "62700396ade388145a9919465d8542c73cc0cdbd",
        "title": "An adaptive scheduling framework for dynamic scheduling problem with random job arrivals and cancellations"
      },
      {
        "paperId": "27be3363e7583ebb76979decbcb3d94cf25c9c9e",
        "title": "Dynamic scheduling mechanism for intelligent workshop with deep reinforcement learning method based on multi-agent system architecture"
      },
      {
        "paperId": "b84e1ffba7e3d8bf9ab3988416ecaf0acecf4259",
        "title": "Deep reinforcement learning for dynamic distributed job shop scheduling problem with transfers"
      },
      {
        "paperId": "89a7ae134e605f743e49a4c94da8175baf37be6e",
        "title": "A discrete event simulator to implement deep reinforcement learning for the dynamic flexible job shop scheduling problem"
      },
      {
        "paperId": "d8ece8f4e59635eeea48f86cd863d6445ea951e0",
        "title": "Research on flexible job-shop scheduling problem based on variation-reinforcement learning"
      },
      {
        "paperId": "24dfb67dbfbcd924f0caa353bbb4425302c96603",
        "title": "Federated deep reinforcement learning for dynamic job scheduling in cloud-edge collaborative manufacturing systems"
      },
      {
        "paperId": "71658fed5a7d6f9edd0247db7b4a01061099464a",
        "title": "\u00c7izelgeleme Problemlerinin \u00c7\u00f6z\u00fcm\u00fcnde Peki\u015ftirmeli \u00d6\u011frenme Etkisinin Analizi"
      },
      {
        "paperId": "2d0a0d5de83ac63832cacb28d6d56380b8c969b9",
        "title": "Survey on Genetic Programming and Machine Learning Techniques for Heuristic Design in Job Shop Scheduling"
      },
      {
        "paperId": "eaf4bcc91fa73f5ca286c2b47edac1c7ba01f03a",
        "title": "Scheduling for the Flexible Job-Shop Problem with a Dynamic Number of Machines Using Deep Reinforcement Learning"
      },
      {
        "paperId": "15005bbd3622edb94c4f01278f6c711bdb7a068b",
        "title": "Job shop smart manufacturing scheduling by deep reinforcement learning"
      },
      {
        "paperId": "75e16c61dcf6a23a4098b65e1eb2d9473e60b335",
        "title": "Dynamic Job-Shop Scheduling Based on Transformer and Deep Reinforcement Learning"
      },
      {
        "paperId": "4e6bb2659405dbc6dfa3887b4563a01289872a9a",
        "title": "Automatic Generation of Energy-Efficient Dispatching Rules for Dynamic Flexible Job Shop Scheduling"
      },
      {
        "paperId": "3f3d28010b37bee371d99d7f379f6451b0ed953f",
        "title": "Challenges Towards VR Technology: VR Architecture Optimization"
      },
      {
        "paperId": "c22b9e2ab74d229953c9143f43bd61de1adcbb98",
        "title": "Deep reinforcement learning for machine scheduling: Methodology, the state-of-the-art, and future directions"
      },
      {
        "paperId": "d49dbc7f54b98bdca6415b67814e0aba63abab53",
        "title": "A cooperative hierarchical deep reinforcement learning based multi-agent method for distributed job shop scheduling problem with random job arrivals"
      },
      {
        "paperId": "4df5bf5ee1a9fc5cb8610c013b7b1b6a45c0b0b8",
        "title": "A multi-agent double Deep-Q-network based on state machine and event stream for flexible job shop scheduling problem"
      },
      {
        "paperId": "e843c4f3502bde7072c799a70ee71fdc4de3dec7",
        "title": "Distributed Dynamics Scheduling Based Reinforcement Learning: Importance and Challenges"
      },
      {
        "paperId": "d2af404f98af3fe3ef82b6bd0dd0ffddd56a6b9a",
        "title": "A novel priority dispatch rule generation method based on graph neural network and reinforcement learning for distributed job-shop scheduling"
      },
      {
        "paperId": "6dbedbc41303f941c83e6f301d00e83fb13c630e",
        "title": "Efficient Multi-Objective Optimization on Dynamic Flexible Job Shop Scheduling Using Deep Reinforcement Learning Approach"
      },
      {
        "paperId": "29d0e9dbd9fb0d2357b6b531642e63ed28ba7ac6",
        "title": "Manufacturing in the Age of Human-Centric and Sustainable Industry 5.0: Application to Holonic, Flexible, Reconfigurable and Smart Manufacturing Systems"
      },
      {
        "paperId": "7796204bd53de774d0d0c94295c31a056e15ff1d",
        "title": "Smart scheduling of dynamic job shop based on discrete event simulation and deep reinforcement learning"
      },
      {
        "paperId": "c06d937493a72cf1628bf945cec49819e0f1b527",
        "title": "Dynamic production scheduling towards self-organizing mass personalization: A multi-agent dueling deep reinforcement learning approach"
      },
      {
        "paperId": "d99487d2dfa8e6f13512ecc216a9e7e1e2cee237",
        "title": "Combining Reinforcement Learning Algorithms with Graph Neural Networks to Solve Dynamic Job Shop Scheduling Problems"
      },
      {
        "paperId": "52c0ccf0e0eee682df802c0a3b73a4825a92268f",
        "title": "A Multi-Objective Flexible Job Shop Intelligent Scheduling System Based on Soft Dueling Double Deep Q-Networks"
      },
      {
        "paperId": "1e562db3b9f636701e67720f5a356ff6b2d2c8f9",
        "title": "An Algebraic Approach to the Solutions of the Open Shop Scheduling Problem"
      },
      {
        "paperId": "83a3031f3533d6870abb5b37de91f6bfeca5e19a",
        "title": "Dynamic distributed flexible job-shop scheduling problem considering operation inspection"
      },
      {
        "paperId": "b831a722ffa93beb6aac782c5884f793cb42e54d",
        "title": "Deep reinforcement learning in smart manufacturing: A review and prospects"
      },
      {
        "paperId": "b0e9dfec2316a9cb188a31402d5ae9c824358256",
        "title": "Multi-Task Multi-Agent Reinforcement Learning for Real-Time Scheduling of a Dual-Resource Flexible Job Shop with Robots"
      },
      {
        "paperId": "759efac6127304d3de8f992c230dc17ebec94c32",
        "title": "Hierarchical Reinforcement Learning for Multi-Objective Real-Time Flexible Scheduling in a Smart Shop Floor"
      },
      {
        "paperId": "55f436eaf1c9382750dacae1d473ffaa1cd2c22e",
        "title": "Research on an Adaptive Real-Time Scheduling Method of Dynamic Job-Shop Based on Reinforcement Learning"
      },
      {
        "paperId": "504a20671d56be804663a71ccb3694f3f45534fe",
        "title": "A Hierarchical Multi-Action Deep Reinforcement Learning Method for Dynamic Distributed Job-Shop Scheduling Problem With Job Arrivals"
      },
      {
        "paperId": "011c2e5d7f06d722bedfb321cb89069c9cc6dd6b",
        "title": "Multi-Agent Reinforcement Learning for Distributed Flexible Job Shop Scheduling With Random Job Arrival"
      },
      {
        "paperId": "4bd397e907d69cc5a632ddcbaff2bd54bf037c4a",
        "title": "Flexible Job Shop Scheduling Based on Energy Consumption of Method Research"
      },
      {
        "paperId": "3321c58d6879649f29bcfc18173e079c76d777fd",
        "title": "Reinforcement Learning for Flexibility and Efficient Production"
      },
      {
        "paperId": "ac353dc230e18a9f778caedd44c6e2ae70b8c08e",
        "title": "Niching Genetic Programming to Learn Actions for Deep Reinforcement Learning in Dynamic Flexible Scheduling"
      },
      {
        "paperId": "bed69ce7f1ef022707768339392de32bb231db25",
        "title": "A Parallel Learning Approach for the Flexible Job Shop Scheduling Problem"
      }
    ],
    "score": 27.333333333333332
  },
  {
    "id": "2e5dc4b028e670d1b9ac0b4e15580cdb7197cb68",
    "title": "Exploration in Deep Reinforcement Learning: A Comprehensive Survey",
    "authors": [
      "Tianpei Yang",
      "Hongyao Tang",
      "Chenjia Bai",
      "Jinyi Liu",
      "Jianye Hao",
      "Zhaopeng Meng",
      "Peng Liu"
    ],
    "year": 2021,
    "citationCount": 103,
    "abstract": "",
    "url": "https://www.semanticscholar.org/paper/2e5dc4b028e670d1b9ac0b4e15580cdb7197cb68",
    "pdf_url": null,
    "venue": "arXiv.org",
    "publicationDate": "",
    "externalIds": {
      "DBLP": "journals/corr/abs-2109-06668",
      "CorpusId": 237503750
    },
    "references": [],
    "cited_by": [
      {
        "paperId": "f0870be65f480a57d28f792f89246cae90431a5f",
        "title": "Improving RL Exploration for LLM Reasoning through Retrospective Replay"
      },
      {
        "paperId": "0d1754f3569a92d1fb4602d3acc45e293b59d7ef",
        "title": "Cooperative Multiagent Learning and Exploration With Min\u2013Max Intrinsic Motivation"
      },
      {
        "paperId": "c6a9edbfb61cf40cf9b3398491069e4fd7d66384",
        "title": "Unsupervised Representation Learning in Deep Reinforcement Learning: A Review"
      },
      {
        "paperId": "62a30950428d33031590d1af176cd22fdf335b38",
        "title": "Adventurer: Exploration with BiGAN for Deep Reinforcement Learning"
      },
      {
        "paperId": "ebb3767d1aafe735d249cf5a1e7e761cff810c9a",
        "title": "DRLMutation: A Comprehensive Framework for Mutation Testing in Deep Reinforcement Learning Systems"
      },
      {
        "paperId": "d998f98fa0d50465e669eab537b88f872c2e28f7",
        "title": "Investigating Tax Evasion Emergence Using Dual Large Language Model and Deep Reinforcement Learning Powered Agent-based Simulation"
      },
      {
        "paperId": "fef45b603e6966b09ce8367037947cd24e9e90a3",
        "title": "Maneuvering Decision Modeling Method Based on Attitude Controller"
      },
      {
        "paperId": "5289125d538f8758336298abaf39694bea0b4338",
        "title": "Provably and Practically Efficient Adversarial Imitation Learning with General Function Approximation"
      },
      {
        "paperId": "f915f1ac47ec34914e4fca885c5a32594ac422bb",
        "title": "Cost-Aware Dispersed Resource Probing and Offloading at the Edge: A User-Centric Online Layered Learning Approach"
      },
      {
        "paperId": "5fce137ed994f6923bc3ea456cf3e34f33bf0f7a",
        "title": "Bidirectional Influence and Interaction for Multiagent Reinforcement Learning"
      },
      {
        "paperId": "402534ce1c0e88a834c5d841c952fc1e0de1bf98",
        "title": "Multi-agent Reinforcement Learning for Dynamic Dispatching in Material Handling Systems"
      },
      {
        "paperId": "ab70b795fac8c5a38318c0482f3992b55f64e539",
        "title": "Application of Inhomogeneous QMIX in Various Architectures to Solve Dynamic Scheduling in Manufacturing Environments"
      },
      {
        "paperId": "15a4f40c9f98fa8c808c2208ebe49a24808c8242",
        "title": "Data Efficient Deep Reinforcement Learning With Action-Ranked Temporal Difference Learning"
      },
      {
        "paperId": "e949e8c1ad3345190d12acc5d4aa59d46cd1a043",
        "title": "Preference-Guided Reinforcement Learning for Efficient Exploration"
      },
      {
        "paperId": "17682d6f13dcac703092e0f1500a4197e46bbf2c",
        "title": "Safe Reinforcement Learning for Power System Control: A Review"
      },
      {
        "paperId": "db9983fe995c19bdb4603f1dd349fad17f29df46",
        "title": "Model-Free Active Exploration in Reinforcement Learning"
      },
      {
        "paperId": "9445a7fcf495ba13169533d591cefaddadaf6f43",
        "title": "World Models with Hints of Large Language Models for Goal Achieving"
      },
      {
        "paperId": "12dca5f462ede31e1e071776138952df1895e8ed",
        "title": "Enhancing Vehicle Aerodynamics with Deep Reinforcement Learning in Voxelised Models"
      },
      {
        "paperId": "c12379e627f5bfc49e2694ed331a4e906377eaae",
        "title": "vMFER: von Mises-Fisher Experience Resampling Based on Uncertainty of Gradient Directions for Policy Improvement of Actor-Critic Algorithms"
      },
      {
        "paperId": "cba305c44959b7d069ea0dcfa16989471618bc67",
        "title": "InterCoop: Spatio-Temporal Interaction Aware Cooperative Perception for Networked Vehicles"
      },
      {
        "paperId": "5059dd54b9add9e0a27af4e5f85a4683eaf7b99f",
        "title": "Nuclear Norm Maximization-Based Curiosity-Driven Reinforcement Learning"
      },
      {
        "paperId": "066e8b0cf649a796d5be1919d02b669f0c64fd15",
        "title": "Deep Reinforcement Learning based channel allocation (DRLCA) in Cognitive Radio Networks"
      },
      {
        "paperId": "f4d0cfab836de8a9b8e3938760b9e750b097610b",
        "title": "Using deep Reinforcement Learning to Optimize the Motivational Incentive Mechanism of Online English Learners"
      },
      {
        "paperId": "58be7f98ee1373d0ede9e1b5850466142e49165f",
        "title": "Action Space-Independent Exploration Methods in Multi-Agent Deep Reinforcement Learning for Wireless Power Allocation"
      },
      {
        "paperId": "b4eb6ba3d86eeb505345c61c982e85305f8e82a1",
        "title": "Is Exploration All You Need? Effective Exploration Characteristics for Transfer in Reinforcement Learning"
      },
      {
        "paperId": "e4885f2498ed24d258fd208e041181d226d6e90d",
        "title": "Dynamic Memory-Based Curiosity: A Bootstrap Approach for Exploration in Reinforcement Learning"
      },
      {
        "paperId": "8121a4dc2daec112be196801a9e4ecb697a8d7dc",
        "title": "PORTAL: Automatic Curricula Generation for Multiagent Reinforcement Learning"
      },
      {
        "paperId": "66d7d369b00d4855455ce55dd7c9c1419da0c1d0",
        "title": "BEHAVIOR-1K: A Human-Centered, Embodied AI Benchmark with 1, 000 Everyday Activities and Realistic Simulation"
      },
      {
        "paperId": "4906c6ebf6e4108fabc983dae6ebd8802fd2d593",
        "title": "Revolutionizing Road Safety: CNN-based Traffic Sign Recognition"
      },
      {
        "paperId": "b261a0a152e59568c1291f2c2db8f21a7521d979",
        "title": "Settling Decentralized Multi-Agent Coordinated Exploration by Novelty Sharing"
      },
      {
        "paperId": "08e84c939b88fc50aaa74ef76e202e61a1ad940b",
        "title": "StepCoder: Improve Code Generation with Reinforcement Learning from Compiler Feedback"
      },
      {
        "paperId": "15005bbd3622edb94c4f01278f6c711bdb7a068b",
        "title": "Job shop smart manufacturing scheduling by deep reinforcement learning"
      },
      {
        "paperId": "25523c3e70daedcd0f1172844fcbafe30b8933f1",
        "title": "Bridging Evolutionary Algorithms and Reinforcement Learning: A Comprehensive Survey"
      },
      {
        "paperId": "0ce319f9d55dc33fb16fa69e8c2de77fc5a93a2a",
        "title": "Trajectory-Oriented Policy Optimization with Sparse Rewards"
      },
      {
        "paperId": "884c69074ec6bdedd11b079d2b4eab65acbe8675",
        "title": "Policy Optimization with Smooth Guidance Learned from State-Only Demonstrations"
      },
      {
        "paperId": "e658cc60b922ff23d17e4864f4c38174f153a72b",
        "title": "Colored Noise in PPO: Improved Exploration and Performance Through Correlated Action Sampling"
      },
      {
        "paperId": "956c47ae72723519414aebf5278ba9c727257350",
        "title": "Active Reinforcement Learning for Robust Building Control"
      },
      {
        "paperId": "6f270100c677f2c6281f8751f5df7a55ff35d061",
        "title": "Weighted bootstrapped DQN: efficient exploration via uncertainty quantification"
      },
      {
        "paperId": "096013115cc78698a1305b8bbd7bdbadfab21b20",
        "title": "Adaptive algorithms for shaping behavior"
      },
      {
        "paperId": "0d9a121563e727c017671f06b7c884829124ae94",
        "title": "Optimization Algorithm Research and Design of Chip Manufacturing Virtual System Based on SECS/GEM"
      },
      {
        "paperId": "f74dc24fa2f916df9bad1d541450a2a743bf59a4",
        "title": "Data-driven hospitals staff and resources allocation using agent-based simulation and deep reinforcement learning"
      },
      {
        "paperId": "6eb46737bf0ef916a7f906ec6a8da82a45ffb623",
        "title": "Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "d293a75f529d7b1abb161480a95122c9a3ed6376",
        "title": "Causal Reinforcement Learning: A Survey"
      },
      {
        "paperId": "8c88c693d5dc2802d3b76b85740e1f04fdaaf801",
        "title": "Large sequence models for sequential decision-making: a survey"
      },
      {
        "paperId": "92d118852f6bf22f4698bec4115d6fd7f12ff84e",
        "title": "Multi-agent Exploration with Sub-state Entropy Estimation"
      },
      {
        "paperId": "fd3f34a37e3963d3c28b6d389ce4448ff9fd0c33",
        "title": "A Unified Uncertainty-Aware Exploration: Combining Epistemic and Aleatory Uncertainty"
      },
      {
        "paperId": "e14517d853e892154d8e5d9b97e3679aeee8c904",
        "title": "Exploring Progressive Hybrid-Degraded Image Processing for Homography Estimation"
      },
      {
        "paperId": "889852f35393904170fca83517edfbf13ccc4474",
        "title": "Multi-agent Decision-making at Unsignalized Intersections with Reinforcement Learning from Demonstrations"
      },
      {
        "paperId": "c4d4167a363b3f63b53b126509edb951b32117c1",
        "title": "Having multiple selves helps learning agents explore and adapt in complex changing worlds"
      },
      {
        "paperId": "f20414c457290adb21b33dd858f8612dd525d573",
        "title": "On Effectiveness of Exploration Strategies in Deep Reinforcement Learning for Power Allocation in Multi-Carrier Wireless Systems"
      },
      {
        "paperId": "d0e4f72ffb94a52850f5e979a665632b1247dffe",
        "title": "Concept Drift Adaptation Methods under the Deep Learning Framework: A Literature Review"
      },
      {
        "paperId": "260da118e8657aa799297693ac188c56abdbaef4",
        "title": "A multi-strategy particle swarm optimization framework based on deep reinforcement learning"
      },
      {
        "paperId": "566d039cccef6ad89338865104ffe1c35143e79e",
        "title": "FLEX: an Adaptive Exploration Algorithm for Nonlinear Systems"
      },
      {
        "paperId": "28e536d4b425a8743af9b074ddb11baba66f8b47",
        "title": "Reinforcement Learning with Knowledge Representation and Reasoning: A Brief Survey"
      },
      {
        "paperId": "5045c2a64a4ea41d8da9ee8eb279498db1ff3d78",
        "title": "Planning Goals for Exploration"
      },
      {
        "paperId": "10b81feb827394fd6c906a1b2799c749288bb7c7",
        "title": "SVDE: Scalable Value-Decomposition Exploration for Cooperative Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "11c140a5d0ee210bb1df46bdfdb305e375d8c907",
        "title": "Fast exploration and learning of latent graphs with aliased observations"
      },
      {
        "paperId": "8642da7317c928f69b1850161eef3824a4bcd136",
        "title": "Task Scheduling Based on Adaptive Priority Experience Replay on Cloud Platforms"
      },
      {
        "paperId": "4bb9f478cb464049100abafc77b6491b6e5b731a",
        "title": "Evolutionary Reinforcement Learning: A Survey"
      },
      {
        "paperId": "8370bfa2d4c6f6a4c6413f59b82a81c63e15f1f0",
        "title": "SCRIMP: Scalable Communication for Reinforcement- and Imitation-Learning-Based Multi-Agent Pathfinding"
      },
      {
        "paperId": "ca2b3078c1d700c4770688e3f33b719ee2ec7049",
        "title": "Partial-Information Q-Learning for General Two-Player Stochastic Games"
      },
      {
        "paperId": "9e13ffe77bc1ae88dc9e89ddf6fdaa12f15c165f",
        "title": "Data-Driven Robotic Manipulation of Cloth-like Deformable Objects: The Present, Challenges and Future Prospects"
      },
      {
        "paperId": "e9f1d5f5c366e2f18a31ec34175b9eb6795ae82b",
        "title": "Learning to Forecast Aleatoric and Epistemic Uncertainties over Long Horizon Trajectories"
      },
      {
        "paperId": "77cb8b0ef4cd4076d3ce519acb321b35370ec846",
        "title": "Efficient Exploration Through Bootstrapped and Bayesian Deep Q-Networks for Joint Power Control and Beamforming in mmWave Networks"
      },
      {
        "paperId": "c2e96bdbcb6075bf551012a043c273844e25fee8",
        "title": "Uncertainty-aware transfer across tasks using hybrid model-based successor feature reinforcement learning\u2606"
      },
      {
        "paperId": "c451ae7305b185d1fa588256504ae653b21ab7d8",
        "title": "Self-Motivated Multi-Agent Exploration"
      },
      {
        "paperId": "96433de43674b7297422db6c21c5490501f08b68",
        "title": "A Review of Deep Reinforcement Learning Approaches for Smart Manufacturing in Industry 4.0 and 5.0 Framework"
      },
      {
        "paperId": "cbd7b03a3d031b39f7a8203d594c1da6be396947",
        "title": "Mixed Time-Frame Training for Reinforcement Learning"
      },
      {
        "paperId": "915f52bfbbb813bf50a1824282c0b490a6c30dd1",
        "title": "Curiosity in hindsight"
      },
      {
        "paperId": "6fdbe62e8bedbbdb6292a37cd239208d93f2949b",
        "title": "Progress and summary of reinforcement learning on energy management of MPS-EV"
      },
      {
        "paperId": "2bc3eeb2adc8deb8f13cce27ceeced302695a1fd",
        "title": "Monotonic Quantile Network for Worst-Case Offline Reinforcement Learning"
      },
      {
        "paperId": "ac3fd058adc8109bcb2666caa6fff8eba425e5f6",
        "title": "Safety Correction from Baseline: Towards the Risk-aware Policy in Robotics via Dual-agent Reinforcement Learning"
      },
      {
        "paperId": "3ebac906c03bc55a1f95c38421ed4f561f36a506",
        "title": "Task Phasing: Automated Curriculum Learning from Demonstrations"
      },
      {
        "paperId": "200fe438d93ee79b3cf68e66977068afe843c7ef",
        "title": "The Role of Exploration for Task Transfer in Reinforcement Learning"
      },
      {
        "paperId": "bc1e038c8cf52db0ef5cabca152971f057e5fc4e",
        "title": "Curiosity-driven Exploration in VizDoom"
      },
      {
        "paperId": "f54fd65a4a8fa0b03290a5de4b0be79f545aced4",
        "title": "Unsupervised Representation Learning in Deep Reinforcement Learning: A Review"
      },
      {
        "paperId": "89ae946f74e75d1e7a9ca5d9d44c0c610e70d41b",
        "title": "Self-Supervised Exploration via Temporal Inconsistency in Reinforcement Learning"
      },
      {
        "paperId": "b132857368165f15092b4e5d7e158c284e897645",
        "title": "Dynamic Memory-based Curiosity: A Bootstrap Approach for Exploration"
      },
      {
        "paperId": "d038814af60dd92d14325a986a9f06a37dbbd0a9",
        "title": "Reinforcement Learning using Reward Expectations in Scenarios with Aleatoric Uncertainties"
      },
      {
        "paperId": "77e8638d2f48a434e024470a1019103e02e0709f",
        "title": "An Improved Multi-Objective Deep Reinforcement Learning Algorithm Based on Envelope Update"
      },
      {
        "paperId": "4b1489711d6fbbd16c25ec5d6ac4d34df4003464",
        "title": "Distributional Actor-Critic Ensemble for Uncertainty-Aware Continuous Control"
      },
      {
        "paperId": "3f66a70a181e9cd1cd8d0760c6c5e7a6f3ff2006",
        "title": "Action Noise in Off-Policy Deep Reinforcement Learning: Impact on Exploration and Performance"
      },
      {
        "paperId": "2d71f3da1037a4b0631bc2506694f6b3b90a9b0b",
        "title": "RORL: Robust Offline Reinforcement Learning via Conservative Smoothing"
      },
      {
        "paperId": "8e25eaa421da5042884105b51a0b6009c152d7f3",
        "title": "Uncertainty-Aware Portfolio Management With Risk-Sensitive Multiagent Network"
      },
      {
        "paperId": "85cd1064f7b82d52c1864d35223d5f45144721ff",
        "title": "From Dirichlet to Rubin: Optimistic Exploration in RL without Bonuses"
      },
      {
        "paperId": "d8729b4e8c2da774c8faa7a7cd32bea919743aec",
        "title": "Modularity benefits reinforcement learning agents with competing homeostatic drives"
      },
      {
        "paperId": "8879f8ae62b9ab3e8ec54d9f6a0b3ab5ee3dbda9",
        "title": "PMIC: Improving Multi-Agent Reinforcement Learning with Progressive Mutual Information Collaboration"
      },
      {
        "paperId": "76e86347243661adf0ef96271982961f3117af10",
        "title": "From Psychological Curiosity to Artificial Curiosity: Curiosity-Driven Learning in Artificial Intelligence Tasks"
      },
      {
        "paperId": "07ae9fde025f71dc40f8b31c25b33469c9a5a5e8",
        "title": "Uncertainty-aware Low-Rank Q-Matrix Estimation for Deep Reinforcement Learning"
      },
      {
        "paperId": "b5bb16e1de8bce93ab78bf578090d1d64ebd25c2",
        "title": "Dynamic Bottleneck for Robust Self-Supervised Exploration"
      },
      {
        "paperId": "ac89d4156c66f792cacbd29600d4cf0cfead71f3",
        "title": "Deep Reinforcement Learning Versus Evolution Strategies: A Comparative Survey"
      },
      {
        "paperId": "9542b5277563000f70962b91d0c4a4f48faf3b08",
        "title": "Policy Optimization with Smooth Guidance Rewards Learned from Sparse-Reward Demonstrations"
      },
      {
        "paperId": "14e905168ce90539c066a761fc2a983dbd3a3350",
        "title": "StepCoder: Improving Code Generation with Reinforcement Learning from Compiler Feedback"
      },
      {
        "paperId": "fea6f2bdda718075013da8c4d7c67835234cff80",
        "title": "Towards Safety Assurance of Uncertainty-Aware Reinforcement Learning Agents"
      },
      {
        "paperId": "f8331e032902a289bdd5f7c68c499edabc9ea9a0",
        "title": "Impossibly Good Experts and How to Follow Them"
      },
      {
        "paperId": "efc61b691035f27eea462cd3c21563226e9ba8d8",
        "title": "Farsighter: Efficient Multi-Step Exploration for Deep Reinforcement Learning"
      },
      {
        "paperId": "4f72e1cb39c1dda6d288a9a756c4137dd7f1396b",
        "title": "Consistent epistemic planning for MADRL"
      },
      {
        "paperId": "e11d1c239a2ade8a5c20b5ee42b7e558851b84ee",
        "title": "API: Boosting Multi-Agent Reinforcement Learning via Agent-Permutation-Invariant Networks"
      },
      {
        "paperId": "7db22589c2151d3f4c5c24b038154d00c51308f8",
        "title": "Exploration and Communication for Partially Observable Collaborative Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "3c5a4f6f966a813adb5bd50cd980f4b525844073",
        "title": "Exploration in Sequential Recommender Systems via Graph Representations"
      },
      {
        "paperId": "69764fcc646e4c608ac08eeb4c784cf8465268d2",
        "title": "BEHAVIOR-1K: A Benchmark for Embodied AI with 1, 000 Everyday Activities and Realistic Simulation"
      },
      {
        "paperId": "5ba117627b6ceb2ddf418ed9b300898a1fd6a0aa",
        "title": "Constrained Reinforcement Learning for Autonomous Farming: Challenges and Opportunities"
      },
      {
        "paperId": "b0db918cf7b7144d5c84112ccd52e006dd1f2df3",
        "title": "Characteristics of Effective Exploration for Transfer in Reinforcement Learning"
      }
    ],
    "score": 25.75
  },
  {
    "id": "f593dc96b20ce8427182e773e3b2192d707706a8",
    "title": "Hierarchical Planning Through Goal-Conditioned Offline Reinforcement Learning",
    "authors": [
      "Jinning Li",
      "Chen Tang",
      "M. Tomizuka",
      "Wei Zhan"
    ],
    "year": 2022,
    "citationCount": 65,
    "abstract": "Offline Reinforcement learning (RL) has shown potent in many safe-critical tasks in robotics where exploration is risky and expensive. However, it still struggles to acquire skills in temporally extended tasks. In this paper, we study the problem of offline RL for temporally extended tasks. We propose a hierarchical planning framework, consisting of a low-level goal-conditioned RL policy and a high-level goal planner. The low-level policy is trained via offline RL. We improve the offline training to deal with out-of-distribution goals by a perturbed goal sampling process. The high-level planner selects intermediate sub-goals by taking advantages of model-based planning methods. It plans over future sub-goal sequences based on the learned value function of the low-level policy. We adopt a Conditional Variational Autoencoder to sample meaningful high-dimensional sub-goal candidates and to solve the high-level long-term strategy optimization problem. We evaluate our proposed method in long-horizon driving and robot navigation tasks. Experiments show that our method outperforms baselines with different hierarchical designs and other regular planners without hierarchy in these complex tasks.",
    "url": "https://www.semanticscholar.org/paper/f593dc96b20ce8427182e773e3b2192d707706a8",
    "pdf_url": "https://arxiv.org/pdf/2205.11790.pdf",
    "venue": "IEEE Robotics and Automation Letters",
    "publicationDate": "2022-05-24",
    "externalIds": {
      "ArXiv": "2205.11790",
      "DBLP": "journals/ral/LiTTZ22",
      "DOI": "10.48550/arXiv.2205.11790",
      "CorpusId": 249017535
    },
    "references": [
      {
        "paperId": "798da6871400a119e70a46dba963e0bcdf2986d5",
        "title": "Dealing with the Unknown: Pessimistic Offline Reinforcement Learning"
      },
      {
        "paperId": "324e6d276151a87951f314cad78fa097fe9188e9",
        "title": "A Workflow for Offline Model-Free Robotic Reinforcement Learning"
      },
      {
        "paperId": "0bcc734246586966596b1aa22efac2565224ebee",
        "title": "Bridging Offline Reinforcement Learning and Imitation Learning: A Tale of Pessimism"
      },
      {
        "paperId": "30256fd41d471e8c6731c732e41ba865321ced7d",
        "title": "S4RL: Surprisingly Simple Self-Supervision for Offline Reinforcement Learning"
      },
      {
        "paperId": "2ae78684565a58c2bf7d3da7c91ae7215da82f0f",
        "title": "A Safe Hierarchical Planning Framework for Complex Driving Scenarios based on Reinforcement Learning"
      },
      {
        "paperId": "f204041dd567025217adc8070ca292e89cc80488",
        "title": "COG: Connecting New Skills to Past Experience with Offline Reinforcement Learning"
      },
      {
        "paperId": "0a321a38ba98499f17a2423f84972de29a5b2e7f",
        "title": "OPAL: Offline Primitive Discovery for Accelerating Offline Reinforcement Learning"
      },
      {
        "paperId": "28db20a81eec74a50204686c3cf796c42a020d2e",
        "title": "Conservative Q-Learning for Offline Reinforcement Learning"
      },
      {
        "paperId": "5e7bc93622416f14e6948a500278bfbe58cd3890",
        "title": "Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems"
      },
      {
        "paperId": "a326d9f2d2d351001fece788165dbcbb524da2e4",
        "title": "D4RL: Datasets for Deep Data-Driven Reinforcement Learning"
      },
      {
        "paperId": "bbac680797af0f7ce4cdcc6430ff001fa0dfe670",
        "title": "Learning to Generalize Across Long-Horizon Tasks from Human Demonstrations"
      },
      {
        "paperId": "7b4848bad51ebd38fb068e73abc3c6d865fd692f",
        "title": "Planning with Goal-Conditioned Policies"
      },
      {
        "paperId": "5e9764f45e7ea6206594deb94753a5cad4e31a1a",
        "title": "IRIS: Implicit Reinforcement without Interaction at Scale for Learning Control from Offline Robot Manipulation Data"
      },
      {
        "paperId": "3fdac1028bae028e1185b10f4264980a3f78d391",
        "title": "Attention-based Hierarchical Deep Reinforcement Learning for Lane Change Behaviors in Autonomous Driving"
      },
      {
        "paperId": "8c54e8575e7c17a4097838305915e6e7b00fd4af",
        "title": "Relay Policy Learning: Solving Long-Horizon Tasks via Imitation and Reinforcement Learning"
      },
      {
        "paperId": "9001698e033524864d4d45f051a5ba362d4afd9e",
        "title": "When to Trust Your Model: Model-Based Policy Optimization"
      },
      {
        "paperId": "b668ba0900ddcdacd0a07ff9983172f525c3c4d6",
        "title": "Goal-conditioned Imitation Learning"
      },
      {
        "paperId": "e0889fcee1acd985af76a3907d5d0029bf260be9",
        "title": "Search on the Replay Buffer: Bridging Planning and Reinforcement Learning"
      },
      {
        "paperId": "5534cdc24c8fd3c962dee54fdfbd0f942bfce70f",
        "title": "Model-free Deep Reinforcement Learning for Urban Autonomous Driving"
      },
      {
        "paperId": "39b7007e6f3dd0744833f292f07ed77973503bfd",
        "title": "Data-Efficient Hierarchical Reinforcement Learning"
      },
      {
        "paperId": "811df72e210e20de99719539505da54762a11c6d",
        "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor"
      },
      {
        "paperId": "ebf0615fc4d98cf1dbe527c79146ce1e50dce9af",
        "title": "CARLA: An Open Urban Driving Simulator"
      },
      {
        "paperId": "880a018f1e0a587b39a4ca9a5c9f6ba4029e2ea1",
        "title": "End-to-End Driving Via Conditional Imitation Learning"
      },
      {
        "paperId": "429ed4c9845d0abd1f8204e1d7705919559bc2a2",
        "title": "Hindsight Experience Replay"
      },
      {
        "paperId": "88880d88073a99107bbc009c9f4a4197562e1e44",
        "title": "Safe Model-based Reinforcement Learning with Stability Guarantees"
      },
      {
        "paperId": "3f25e17eb717e5894e0404ea634451332f85d287",
        "title": "Learning Structured Output Representation using Deep Conditional Generative Models"
      },
      {
        "paperId": "5dc2a215bd7cd5bdd3a0baa8c967575632696fac",
        "title": "Universal Value Function Approximators"
      },
      {
        "paperId": "7d5bc454ba9335ce488adeead662d782a8c55f04",
        "title": "Deep learning helicopter dynamics models"
      },
      {
        "paperId": "9aa1d909544fd9ffe061b84a90eb344ac303e6d9",
        "title": "The MAXQ Method for Hierarchical Reinforcement Learning"
      },
      {
        "paperId": null,
        "title": "\u201cHierarchical reinforcement learning for self-driving decision-making without reliance on labelled driving data,\u201d"
      },
      {
        "paperId": "ebfa3fb595ca0299ab5249c69374c98b74bb62b3",
        "title": "A Tutorial on the Cross-Entropy Method"
      },
      {
        "paperId": "6df43f70f383007a946448122b75918e3a9d6682",
        "title": "Learning to Achieve Goals"
      }
    ],
    "cited_by": [
      {
        "paperId": "79b48999a593c146bd78ca24f7fcfb8759168265",
        "title": "Task-Oriented Enhanced Self-Learning Speed Synchronization Control of Flexible Coupled Dual-Motor Systems"
      },
      {
        "paperId": "79b151befc13141bf04175a9938f2d9b370b5c86",
        "title": "Structural Information-based Hierarchical Diffusion for Offline Reinforcement Learning"
      },
      {
        "paperId": "1e8054456f5a8571737f98b535e4be58a0cc5cbf",
        "title": "GenAI-based Multi-Agent Reinforcement Learning towards Distributed Agent Intelligence: A Generative-RL Agent Perspective"
      },
      {
        "paperId": "5e8531f03bfc21635b8c388560dacbfb208251af",
        "title": "Hierarchical Reinforcement Learning and Value Optimization for Challenging Quadruped Locomotion"
      },
      {
        "paperId": "a40f0c96f319ffd51d561c5bb9665b8349989205",
        "title": "Horizon Reduction Makes RL Scalable"
      },
      {
        "paperId": "2a01356f76d65e03b28a1f301415236eb7eded2c",
        "title": "Reachability Weighted Offline Goal-conditioned Resampling"
      },
      {
        "paperId": "38a21e167802805ff392a42463ff1d9516c9b524",
        "title": "MRSD: Multi-Resolution Skill Discovery for HRL Agents"
      },
      {
        "paperId": "9766b84b3c1ce58a76d8358f41afaadcb7929aae",
        "title": "Extracting Visual Plans from Unlabeled Videos via Symbolic Guidance"
      },
      {
        "paperId": "3ab4e3f3f00417b409fdda1270142cb7ff21052f",
        "title": "Extendable Long-Horizon Planning via Hierarchical Multiscale Diffusion"
      },
      {
        "paperId": "883e6f7eac0436788e9ce7d9a6bc3b735229d701",
        "title": "Intelligent redundant manipulation for long-horizon operations with multiple goal-conditioned hierarchical learning"
      },
      {
        "paperId": "0ca4c732ecde2a99a5ee2870f3ee30c18f773201",
        "title": "From Screens to Scenes: A Survey of Embodied AI in Healthcare"
      },
      {
        "paperId": "5689ae64aa5595a2d54f60b2a45980c844fe9310",
        "title": "Proposing Hierarchical Goal-Conditioned Policy Planning in Multi-Goal Reinforcement Learning"
      },
      {
        "paperId": "52a878e320ac448c8417ec876fa3f6cbd6bc7b4f",
        "title": "The Teaching and Learning Strategy: Do We Need Classroom to Transfer Knowledge and Values to Our Students?"
      },
      {
        "paperId": "cefc25eb1e5e5c91a856df46802bcfa53aad6f0f",
        "title": "OGBench: Benchmarking Offline Goal-Conditioned RL"
      },
      {
        "paperId": "ec98b763c19d063664f43b95a8296aed5a074cc0",
        "title": "GHIL-Glue: Hierarchical Control with Filtered Subgoal Images"
      },
      {
        "paperId": "6a6e65717bad3a3badc1ec7ea23d691e54a06791",
        "title": "Adaptive and transparent decision-making in autonomous robots through graph-structured world models"
      },
      {
        "paperId": "e68aa17c9919c79c71d5ceaabfd426ccd75d9132",
        "title": "Boosting Hierarchical Reinforcement Learning with Meta-Learning for Complex Task Adaptation"
      },
      {
        "paperId": "863d5568c3e30fca5333cf695f963413223b9600",
        "title": "Temporal Abstraction in Reinforcement Learning with Offline Data"
      },
      {
        "paperId": "26a151261d061d9c46b97188504134960ac8ee5c",
        "title": "Adaptive Prediction Ensemble: Improving Out-of-Distribution Generalization of Motion Forecasting"
      },
      {
        "paperId": "38741bd55f2b590b0d6b06d84ba17eb31e90c9e0",
        "title": "Recent advances in reinforcement learning-based autonomous driving behavior planning: A survey"
      },
      {
        "paperId": "ef8ee73dcc6b9d88aa76d44191daacf361de37ff",
        "title": "Adaptive Skill Selection for Effective Exploration of Action Space"
      },
      {
        "paperId": "276310799cb3a97748a0c2d80fcd0d24ebf9febb",
        "title": "Energy-Aware Hierarchical Reinforcement Learning Based on the Predictive Energy Consumption Algorithm for Search and Rescue Aerial Robots in Unknown Environments"
      },
      {
        "paperId": "44e49dd16c7b7a368f24aeafc1d2435ea572c0f8",
        "title": "PlanDQ: Hierarchical Plan Orchestration via D-Conductor and Q-Performer"
      },
      {
        "paperId": "e1496c2595487e9d6acffb27606cfd3d51548822",
        "title": "Path-Following Navigation in Crowds With Deep Reinforcement Learning"
      },
      {
        "paperId": "2abdd73d48fc5bbf7cfe0240eed17eeb9471266e",
        "title": "Real-Data-Driven Offline Reinforcement Learning for Autonomous Vehicle Speed Decision Making"
      },
      {
        "paperId": "9961d3b33eeb5d4822f2d9d44f0ebb1b549f4cd8",
        "title": "VAPOR: Legged Robot Navigation in Unstructured Outdoor Environments using Offline Reinforcement Learning"
      },
      {
        "paperId": "873df5299e76d825ea4eeba9d34ba1c6ce6d9d83",
        "title": "Trajectory Planning for Autonomous Vehicle Using Iterative Reward Prediction in Reinforcement Learning"
      },
      {
        "paperId": "ca8fa01a225be159c8430609933764228140d8db",
        "title": "Effective Offline Robot Learning With Structured Task Graph"
      },
      {
        "paperId": "42173bbe136835c25365b4d63c3de151d1f221be",
        "title": "Diffusion-Reinforcement Learning Hierarchical Motion Planning in Adversarial Multi-agent Games"
      },
      {
        "paperId": "d27da1ba65fa958e45837120fad1c25e7017d80c",
        "title": "Unsupervised Zero-Shot Reinforcement Learning via Functional Reward Encodings"
      },
      {
        "paperId": "38c728d05bdf75553f44efcb95883bbf99fc7162",
        "title": "Single-Reset Divide & Conquer Imitation Learning"
      },
      {
        "paperId": "e14c5e17c55bd6fe6ad06bd0fac26a87ff0b2b3c",
        "title": "Goal-Conditioned Hierarchical Reinforcement Learning With High-Level Model Approximation"
      },
      {
        "paperId": "14f2e7a63cf43c41e7dc6a4a527243422d5e3c36",
        "title": "Simple Hierarchical Planning with Diffusion"
      },
      {
        "paperId": "be3f84d8f56329b0478204ee44faf7de2543265f",
        "title": "GPTMORE: Generative Pre-trained Transformer for Model-based Offline Reinforcement Learning"
      },
      {
        "paperId": "f73db11a195beec70fc5c74a9d2c9e60f322f869",
        "title": "Backward Learning for Goal-Conditioned Policies"
      },
      {
        "paperId": "06cb176ff32fbf498f0df93016a46dbbb333d533",
        "title": "Goal-conditioned Offline Planning from Curious Exploration"
      },
      {
        "paperId": "707d85c8ec94ced2cccc8cb242868d944f7a5163",
        "title": "Hybrid Search for Efficient Planning with Completeness Guarantees"
      },
      {
        "paperId": "4c20b3151f57e339765bad5982f819286e045ddc",
        "title": "Efficient Sim-to-real Transfer of Contact-Rich Manipulation Skills with Online Admittance Residual Learning"
      },
      {
        "paperId": "bb247435f722a689c0568760e73a9544d5c20933",
        "title": "Hieros: Hierarchical Imagination on Structured State Space Sequence World Models"
      },
      {
        "paperId": "02aaccf045178a0ad70906941f21b1eabde34227",
        "title": "Efficient Planning with Latent Diffusion"
      },
      {
        "paperId": "82ff15c65a7ab93f1c43cc7f383ba8fd9415923d",
        "title": "Uncertainty-Aware Decision Transformer for Stochastic Driving Environments"
      },
      {
        "paperId": "341975e953eae14932b708d7f80e378d11013a57",
        "title": "Boosting Offline Reinforcement Learning for Autonomous Driving with Hierarchical Latent Skills"
      },
      {
        "paperId": "c3207f4155fc4ac26e38e940255a2ecc2180a948",
        "title": "Guided Online Distillation: Promoting Safe Reinforcement Learning by Offline Demonstration"
      },
      {
        "paperId": "59c6c9bb2e34eb6e91fc5b72d8beea924c1037c8",
        "title": "VAPOR: Legged Robot Navigation in Outdoor Vegetation Using Offline Reinforcement Learning"
      },
      {
        "paperId": "f18587247e4769ad0efd96a0286b012d856ba214",
        "title": "HIQL: Offline Goal-Conditioned RL with Latent States as Actions"
      },
      {
        "paperId": "67f336d34b1e803ff1ac519b88e513973acdec7c",
        "title": "Landmark Guided Active Exploration with State-specific Balance Coefficient"
      },
      {
        "paperId": "6a42f6362afa3a1a0936f7a6a8927d04a2285cc5",
        "title": "Balancing Exploration and Exploitation in Hierarchical Reinforcement Learning via Latent Landmark Graphs"
      },
      {
        "paperId": "284dc52d96fcc9fccedc7867fd9e71468f904914",
        "title": "Skill-Critic: Refining Learned Skills for Hierarchical Reinforcement Learning"
      },
      {
        "paperId": "43806158cabbcbca4efd58f75a82c882c085d60d",
        "title": "IQL-TD-MPC: Implicit Q-Learning for Hierarchical Model Predictive Control"
      },
      {
        "paperId": "a8e99d7691976c89076073413cd2128f660638a6",
        "title": "Goal-conditioned Offline Reinforcement Learning through State Space Partitioning"
      },
      {
        "paperId": "05d1346ab3aea298c4828023973d5021fe74a833",
        "title": "Environment Transformer and Policy Optimization for Model-Based Offline Reinforcement Learning"
      },
      {
        "paperId": "329d809a69eada0521da1b536bb9db9348d51868",
        "title": "Hierarchical Imitation Learning with Vector Quantized Models"
      },
      {
        "paperId": "b0cdf3658d6a85fda507674e30cd3dce2808493c",
        "title": "Learning Goal-Conditioned Policies Offline with Self-Supervised Reward Shaping"
      },
      {
        "paperId": "545817cae3b50138d90bbfdf095607a3fb53342a",
        "title": "Safe Reinforcement Learning using Data-Driven Predictive Control"
      },
      {
        "paperId": "7b92a6e111fe59e23e783a471b099657f5d08a58",
        "title": "BusyBot: Learning to Interact, Reason, and Plan in a BusyBoard Environment"
      },
      {
        "paperId": "3f288fdea7fb6aceb76fd0521c0aab51d71c7cbb",
        "title": "Key-State-Conditioned Diffusion Models for Trajectory Planning"
      },
      {
        "paperId": "60713a58c50aaa7965bb83afc729b6a44d20d58c",
        "title": "Pre-Training Goal-based Models for Sample-Efficient Reinforcement Learning"
      },
      {
        "paperId": "bd2755fd96e20183f642628f818d7d161d3d0f0b",
        "title": "Hierarchical Diffusion for Offline Decision Making"
      },
      {
        "paperId": "66e2858d214688dcd8c47365084750d23ad5597b",
        "title": "An Expansive Latent Planner for Long-horizon Visual Offline Reinforcement Learning"
      },
      {
        "paperId": "6d2753eac1b984c277374011106ed2ecad5156a4",
        "title": "Landmark Guided Active Exploration with Stable Low-level Policy Learning"
      },
      {
        "paperId": "c27fafad6b209cada969c2c571e0f7b54a9773cc",
        "title": "VAPOR: Legged Robot Navigation in Outdoor Vegetation Using Offline Reinforcement Learning"
      },
      {
        "paperId": "a396c6d86efa55c91894fdd3b700142142f9efdb",
        "title": "Skill-Critic: Refining Learned Skills for Reinforcement Learning"
      },
      {
        "paperId": "5d1b96fbdffd03694da1eba1b0e5974b01074b02",
        "title": "Expansive Latent Planning for Sparse Reward Offline Reinforcement Learning"
      },
      {
        "paperId": "cfbbce0359dc8e3d49952b4c55b3e7ecb32ee2a0",
        "title": "Overleaf Example"
      }
    ],
    "score": 21.666666666666664
  },
  {
    "id": "cc9f2fd320a279741403c4bfbeb91179803c428c",
    "title": "Reward Uncertainty for Exploration in Preference-based Reinforcement Learning",
    "authors": [
      "Xi-Xi Liang",
      "Katherine Shu",
      "Kimin Lee",
      "P. Abbeel"
    ],
    "year": 2022,
    "citationCount": 59,
    "abstract": "Conveying complex objectives to reinforcement learning (RL) agents often requires meticulous reward engineering. Preference-based RL methods are able to learn a more flexible reward model based on human preferences by actively incorporating human feedback, i.e. teacher's preferences between two clips of behaviors. However, poor feedback-efficiency still remains a problem in current preference-based RL algorithms, as tailored human feedback is very expensive. To handle this issue, previous methods have mainly focused on improving query selection and policy initialization. At the same time, recent exploration methods have proven to be a recipe for improving sample-efficiency in RL. We present an exploration method specifically for preference-based RL algorithms. Our main idea is to design an intrinsic reward by measuring the novelty based on learned reward. Specifically, we utilize disagreement across ensemble of learned reward models. Our intuition is that disagreement in learned reward model reflects uncertainty in tailored human feedback and could be useful for exploration. Our experiments show that exploration bonus from uncertainty in learned reward improves both feedback- and sample-efficiency of preference-based RL algorithms on complex robot manipulation tasks from MetaWorld benchmarks, compared with other existing exploration methods that measure the novelty of state visitation.",
    "url": "https://www.semanticscholar.org/paper/cc9f2fd320a279741403c4bfbeb91179803c428c",
    "pdf_url": "https://arxiv.org/pdf/2205.12401.pdf",
    "venue": "International Conference on Learning Representations",
    "publicationDate": "2022-05-24",
    "externalIds": {
      "DBLP": "journals/corr/abs-2205-12401",
      "ArXiv": "2205.12401",
      "DOI": "10.48550/arXiv.2205.12401",
      "CorpusId": 249062649
    },
    "references": [
      {
        "paperId": "51965de80f86432d42749427db1e5bb0fa1e204c",
        "title": "B-Pref: Benchmarking Preference-Based Reinforcement Learning"
      },
      {
        "paperId": "a6fdb277d0a4b09899f802bda3359f5c2021a156",
        "title": "Recursively Summarizing Books with Human Feedback"
      },
      {
        "paperId": "45f573f302dc7e77cbc5d1a74ccbac3564bbebc8",
        "title": "PEBBLE: Feedback-Efficient Interactive Reinforcement Learning via Relabeling Experience and Unsupervised Pre-training"
      },
      {
        "paperId": "0295df1b9d11e6e49b20119410fd755a4d7781af",
        "title": "Behavior From the Void: Unsupervised Active Pre-Training"
      },
      {
        "paperId": "7fc04f8031598da6510bce4a7e5b4722305ca5b0",
        "title": "State Entropy Maximization with Random Encoders for Efficient Exploration"
      },
      {
        "paperId": "324effa5a7c737257b675f85bd93d777fc486878",
        "title": "SUNRISE: A Simple Unified Framework for Ensemble Learning in Deep Reinforcement Learning"
      },
      {
        "paperId": "5ab5d94f1a9ffeb076cad515854bbe0ea9c49db5",
        "title": "A Policy Gradient Method for Task-Agnostic Exploration"
      },
      {
        "paperId": "18f44e605082388ffc0a59b895ff70b01cfc0034",
        "title": "Quantifying Differences in Reward Functions"
      },
      {
        "paperId": "3a71c306eb6232658c9e5fd48aed1ef3befe5fbe",
        "title": "Planning to Explore via Self-Supervised World Models"
      },
      {
        "paperId": "061bfda5b75e03dcb15ee4e94e841c2aeeaeccbf",
        "title": "Active preference-based Gaussian process regression for reward learning and optimization"
      },
      {
        "paperId": "9efb64f20ab1f157ca9f4050d4aaacf6c3f9b2b2",
        "title": "CURL: Contrastive Unsupervised Representations for Reinforcement Learning"
      },
      {
        "paperId": "0bc855f84668b35cb65618d996d09f6e434d28c9",
        "title": "Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning"
      },
      {
        "paperId": "cfec54f9208dcbb2c558aa7ff45f57be2533d046",
        "title": "Efficient Exploration via State Marginal Matching"
      },
      {
        "paperId": "87a40a2d506fec06d67dc8f14ce2c708a789a2b4",
        "title": "Self-Supervised Exploration via Disagreement"
      },
      {
        "paperId": "adab275554eb745cdc21fd6c526ec55a5b2ef362",
        "title": "Provably Efficient Maximum Entropy Exploration"
      },
      {
        "paperId": "c6f913e4baa7f2c85363c0625c87003ad3b3a14c",
        "title": "Scalable agent alignment via reward modeling: a research direction"
      },
      {
        "paperId": "3e6cde685fdf321d7edf9319f7b07c01ff79c11a",
        "title": "Reward learning from human preferences and demonstrations in Atari"
      },
      {
        "paperId": "48692cce7e9e49bab6e012524fbe403edcb22b91",
        "title": "Batch Active Preference-Based Learning of Reward Functions"
      },
      {
        "paperId": "4cb3fd057949624aa4f0bbe7a6dcc8777ff04758",
        "title": "Exploration by Random Network Distillation"
      },
      {
        "paperId": "1485c5ac93b3f01a5f3ef7824eb428c8bff39ba3",
        "title": "UCB EXPLORATION VIA Q-ENSEMBLES"
      },
      {
        "paperId": "811df72e210e20de99719539505da54762a11c6d",
        "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor"
      },
      {
        "paperId": "59094d64844ee21e32560fb08db6d53cc3af0c51",
        "title": "Inverse Reward Design"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "e27467ca84c5c1f7239a6e643843c1b97e35671f",
        "title": "Active Preference-Based Learning of Reward Functions"
      },
      {
        "paperId": "5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd",
        "title": "Deep Reinforcement Learning from Human Preferences"
      },
      {
        "paperId": "225ab689f41cef1dc18237ef5dab059a49950abf",
        "title": "Curiosity-Driven Exploration by Self-Supervised Prediction"
      },
      {
        "paperId": "12f67fb182bc934fc95ce97acff553d83e2ca72e",
        "title": "Count-Based Exploration with Neural Density Models"
      },
      {
        "paperId": "0fcb2034e31a2bc2f12a2b1363d0d77baf445fdf",
        "title": "#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning"
      },
      {
        "paperId": "e86f71ca2948d17b003a5f068db1ecb2b77827f7",
        "title": "Concrete Problems in AI Safety"
      },
      {
        "paperId": "6e90fd78e8a3b98af3954aae5209703aa966603e",
        "title": "Unifying Count-Based Exploration and Intrinsic Motivation"
      },
      {
        "paperId": "317cd4522b1f4a6f889743578143bb8823623f8b",
        "title": "VIME: Variational Information Maximizing Exploration"
      },
      {
        "paperId": "4b63e34276aa98d5345efa7fe09bb06d8a9d8f52",
        "title": "Deep Exploration via Bootstrapped DQN"
      },
      {
        "paperId": "69e76e16740ed69f4dc55361a3d319ac2f1293dd",
        "title": "Asynchronous Methods for Deep Reinforcement Learning"
      },
      {
        "paperId": "2470fcf0f89082de874ac9133ccb3a8667dd89a8",
        "title": "Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models"
      },
      {
        "paperId": "449532187c94af3dd3aa55e16d2c50f7854d2199",
        "title": "Trust Region Policy Optimization"
      },
      {
        "paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8",
        "title": "Adam: A Method for Stochastic Optimization"
      },
      {
        "paperId": "b3af2e367d7297775c71fa9a61b0b49fb888bc38",
        "title": "A Bayesian Approach for Policy Learning from Trajectory Preference Queries"
      },
      {
        "paperId": "cbbcf7b0ae2d7503836a5a1db42dd3104579972d",
        "title": "Preference-Based Policy Learning"
      },
      {
        "paperId": "c54174bd1a98b1ae1fb111b32950fce538f32007",
        "title": "Online human training of a myoelectric prosthesis controller via actor-critic reinforcement learning"
      },
      {
        "paperId": "1339ae4f951c235283f1e01db8461a818b2700e3",
        "title": "An optimization approach to rough terrain locomotion"
      },
      {
        "paperId": "3adde98a328a48b5a49c23fa1ae6c3c89056b9f1",
        "title": "Nearest Neighbor Estimates of Entropy"
      },
      {
        "paperId": "7d47ee5f84103529f84297c98c21dadb4742e3ff",
        "title": "RANK ANALYSIS OF INCOMPLETE BLOCK DESIGNS THE METHOD OF PAIRED COMPARISONS"
      },
      {
        "paperId": null,
        "title": "We use raw state space to compute entropy"
      },
      {
        "paperId": null,
        "title": "2019) We train an ensemble of 5 forward dynamics model gi, following original results"
      },
      {
        "paperId": "84082634110fcedaaa32632f6cc16a034eedb2a0",
        "title": "A Survey of Preference-Based Reinforcement Learning Methods"
      },
      {
        "paperId": "97efafdb4a3942ab3efba53ded7413199f79c054",
        "title": "Reinforcement Learning: An Introduction"
      },
      {
        "paperId": "a37a7bbd8ba95e134089621e2674a7ac753fa4f5",
        "title": "THE ROLE OF EXPLORATION IN LEARNING CONTROL"
      }
    ],
    "cited_by": [
      {
        "paperId": "03c62352af0a484dc151b380ae693f334c84b2ab",
        "title": "STAIR: Addressing Stage Misalignment through Temporal-Aligned Preference Reinforcement Learning"
      },
      {
        "paperId": "9cd1302cb9cbfa9f6b3ede8a4f3b811de02b8414",
        "title": "Learning Real-World Acrobatic Flight from Human Preferences"
      },
      {
        "paperId": "206f07a961b480948bb3839a85f6897f3535e019",
        "title": "TREND: Tri-Teaching for Robust Preference-based Reinforcement Learning with Demonstrations"
      },
      {
        "paperId": "6ade71e1bed2e0a949c88b1024b75ac95eabea80",
        "title": "Efficient Process Reward Model Training via Active Learning"
      },
      {
        "paperId": "551a5e52bc72df8251a2f66fc9652faabfcacdb7",
        "title": "Flora: Sample-Efficient Preference-Based Rl Via Low-Rank Style Adaptation of Reward Functions"
      },
      {
        "paperId": "14722ec8da3fb1b50a21be93fff4e1af3e4bdda3",
        "title": "An Exploration Method for Deep Reinforcement Learning Based on State Prioritized Replay"
      },
      {
        "paperId": "add637341443b4980988cd2096074f06019d6341",
        "title": "Subtask-Aware Visual Reward Learning from Segmented Demonstrations"
      },
      {
        "paperId": "1ed2e682e787f992fddcb8be3ad8ea75b7435f94",
        "title": "LEASE: Offline Preference-based Reinforcement Learning with High Sample Efficiency"
      },
      {
        "paperId": "ebf4b3404d60a666cae1aa137db9da1a0296255a",
        "title": "RAT: Adversarial Attacks on Deep Reinforcement Agents for Targeted Behaviors"
      },
      {
        "paperId": "9c5405435ead3e117f4973a39bd4d39426d4769a",
        "title": "Novelty Encouraged Beam Clustering Search for Multi-Objective De Novo Diverse Drug Design"
      },
      {
        "paperId": "a581f4ea6caee47253c0813e081b5c89d7c3bd3a",
        "title": "A Human-Machine Reinforcement Learning Framework with Multi-dimensional Human Feedback Fusion"
      },
      {
        "paperId": "8c131d842b59a2a807a97cb39fb0f8a22fdb39a9",
        "title": "Mixing corrupted preferences for robust and feedback-efficient preference-based reinforcement learning"
      },
      {
        "paperId": "21af67962cfbad37a766baf61dbed691290ce8aa",
        "title": "Learning Hidden Subgoals under Temporal Ordering Constraints in Reinforcement Learning"
      },
      {
        "paperId": "99d59b4ec0a2b0860da49f11fe698282af43dff8",
        "title": "Towards Reliable Alignment: Uncertainty-aware RLHF"
      },
      {
        "paperId": "235c869a47bcc6f29dfbd2731f09c346356d7c48",
        "title": "Artificial Neural Networks and Machine Learning - ICANN 2024 - 33rd International Conference on Artificial Neural Networks, Lugano, Switzerland, September 17-20, 2024, Proceedings, Part IV"
      },
      {
        "paperId": "781d3c7fe990303f56f5da39eba3253f152e3505",
        "title": "Multi-Type Preference Learning: Empowering Preference-Based Reinforcement Learning with Equal Preferences"
      },
      {
        "paperId": "272c30121d6ae571c53767958193f1af73cd734a",
        "title": "S-EPOA: Overcoming the Indistinguishability of Segments with Skill-Driven Preference-Based Reinforcement Learning"
      },
      {
        "paperId": "8525434adbf25984e55c78063c71bcb958d364e4",
        "title": "Listwise Reward Estimation for Offline Preference-based Reinforcement Learning"
      },
      {
        "paperId": "0327596d1c83506035a2372eb649d5c16d9515be",
        "title": "Comprehensive Overview of Reward Engineering and Shaping in Advancing Reinforcement Learning Applications"
      },
      {
        "paperId": "e949e8c1ad3345190d12acc5d4aa59d46cd1a043",
        "title": "Preference-Guided Reinforcement Learning for Efficient Exploration"
      },
      {
        "paperId": "62555bd1938384ce9a00973b1a15899e71b72ddc",
        "title": "Safety through feedback in Constrained RL"
      },
      {
        "paperId": "8e4fb48e0882066cc44636fbd812607909e191a7",
        "title": "Efficient Preference-based Reinforcement Learning via Aligned Experience Estimation"
      },
      {
        "paperId": "8f2363d60ee56bf7e7ad29a47c3ad64290361e85",
        "title": "A Preference-based Reinforcement Learning Approach Using Reward Exploration for Decision Making"
      },
      {
        "paperId": "5008574062623b3269ba9e39d37e319770101d6e",
        "title": "SEQUEL: Semi-Supervised Preference-based RL with Query Synthesis via Latent Interpolation"
      },
      {
        "paperId": "6b3fb301ab6bcc97b278f7a059631445babcf051",
        "title": "Improving Instruction Following in Language Models through Proxy-Based Uncertainty Estimation"
      },
      {
        "paperId": "3e65e47eb6361213715b3098659ca75fbb172e4f",
        "title": "Leveraging Sub-Optimal Data for Human-in-the-Loop Reinforcement Learning"
      },
      {
        "paperId": "865bf47dedfec416221504708cd1f039050b0a2d",
        "title": "Impact of Preference Noise on the Alignment Performance of Generative Language Models"
      },
      {
        "paperId": "d38717a79a55cad8ed20d96ffe71136d0bbea9af",
        "title": "Hindsight PRIORs for Reward Learning from Human Preferences"
      },
      {
        "paperId": "d9616c72ac9a98101867a58591ae21f49847f216",
        "title": "Can You Rely on Synthetic Labellers in Preference-Based Reinforcement Learning? It's Complicated"
      },
      {
        "paperId": "2b6f6977daf8525a101905d2b6d2a6d0aa59529b",
        "title": "Decoding Global Preferences: Temporal and Cooperative Dependency Modeling in Multi-Agent Preference-Based Reinforcement Learning"
      },
      {
        "paperId": "6ceca40e971add0ad0d3b304b79c157079838ba6",
        "title": "Sample-Efficient Preference-based Reinforcement Learning with Dynamics Aware Rewards"
      },
      {
        "paperId": "87f1b39c320e1fc71584a231855523167a5588ff",
        "title": "RIME: Robust Preference-based Reinforcement Learning with Noisy Preferences"
      },
      {
        "paperId": "5f3c4145842792e5bc7de14f918fc381be00b06f",
        "title": "Uni-RLHF: Universal Platform and Benchmark Suite for Reinforcement Learning with Diverse Human Feedback"
      },
      {
        "paperId": "c9c2322379a9300315bb3258513e4d5406754670",
        "title": "Resilient Constrained Reinforcement Learning"
      },
      {
        "paperId": "aa83690628456d1b197716f8d45f08f8f2d48ffc",
        "title": "Human-AI Collaboration in Real-World Complex Environment with Reinforcement Learning"
      },
      {
        "paperId": "90ec1b95ab1579673f0a4b81dc5a76dfc955b2a5",
        "title": "Promptable Behaviors: Personalizing Multi-Objective Rewards from Human Preferences"
      },
      {
        "paperId": "d1326c7464235f01d66f4b0be9d346b57bc92c67",
        "title": "Reinforcement Learning from Statistical Feedback: the Journey from AB Testing to ANT Testing"
      },
      {
        "paperId": "3395268b0944a25d388498370cc1be21bf644958",
        "title": "Sample Complexity of Preference-Based Nonparametric Off-Policy Evaluation with Deep Networks"
      },
      {
        "paperId": "7c4a6bd094d02d79918bf254ae325719e6bc549c",
        "title": "Autonomous UAV Navigation in Complex Environments using Human Feedback"
      },
      {
        "paperId": "8e0e795463a2007497c9c257f9de337c53f1c4b9",
        "title": "Rating-based Reinforcement Learning"
      },
      {
        "paperId": "6eb46737bf0ef916a7f906ec6a8da82a45ffb623",
        "title": "Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "aa3d0904c2e1c74b5111f3950ace2a8267c86a2e",
        "title": "STRAPPER: Preference-based Reinforcement Learning via Self-training Augmentation and Peer Regularization"
      },
      {
        "paperId": "875730429d0e5d92b8ac4f81e95b3fcde9116ca2",
        "title": "Boosting Feedback Efficiency of Interactive Reinforcement Learning by Adaptive Learning from Scores"
      },
      {
        "paperId": "07bc3859970cdca573c3f0105541a123e6951253",
        "title": "Proportional Aggregation of Preferences for Sequential Decision Making"
      },
      {
        "paperId": "3211678bf2f01c5ea28bc67399b956b6273478cb",
        "title": "PEARL: Zero-shot Cross-task Preference Alignment and Robust Reward Learning for Robotic Manipulation"
      },
      {
        "paperId": "b7f74db45b05e79489541ad80c2332969a7d2e6f",
        "title": "Query-Policy Misalignment in Preference-Based Reinforcement Learning"
      },
      {
        "paperId": "c28af43206867cb5529164e3dd6d9ea8b7cfe7f2",
        "title": "Active Reward Learning from Multiple Teachers"
      },
      {
        "paperId": "c6478decdfff11ccbd085967c2f83aea11927a46",
        "title": "Preference Transformer: Modeling Human Preferences using Transformers for RL"
      },
      {
        "paperId": "535708777863633f7cc7fd6f4a716b1483c7100f",
        "title": "Reinforcement Learning from Diverse Human Preferences"
      },
      {
        "paperId": "18d750263f1dfe43374e8791cefa580a511c2098",
        "title": "Few-Shot Preference Learning for Human-in-the-Loop RL"
      },
      {
        "paperId": "0fd6c747b48526ba4abc05b4ae9260f93718ce8f",
        "title": "Efficient Meta Reinforcement Learning for Preference-based Fast Adaptation"
      },
      {
        "paperId": "1cc13cff6f12d450457f51eb8d5d8e20bce47b56",
        "title": "Skill-Based Reinforcement Learning with Intrinsic Reward Matching"
      },
      {
        "paperId": "c2366e759a97c2b21e9fc35c5e2f6b377eca38ab",
        "title": "Transformers are Adaptable Task Planners"
      },
      {
        "paperId": "d571bf08a03f2e91269835ca97e64fd59c44545b",
        "title": "ROSCOM: Robust Safe Reinforcement Learning on Stochastic Constraint Manifolds"
      },
      {
        "paperId": "9ecb46066c2e8934f8853183414562ee2d461afe",
        "title": "Towards human-like questioning: Knowledge base question generation with bias-corrected reinforcement learning from human feedback"
      },
      {
        "paperId": "8c2ed19db86875489a5f661ef68b6ba91ad15a94",
        "title": "Zero-shot Preference Learning for Offline RL via Optimal Transport"
      },
      {
        "paperId": "3a8da4edb5ad549fe01a0096a9e892c8f6658726",
        "title": "Reinforcement Learning from Human Feedback for Cyber-Physical Systems: On the Potential of Self-Supervised Pretraining"
      },
      {
        "paperId": "00591c2a5a599fd7b0d56335fdf6088db7cd693d",
        "title": "BATTLE: T OWARDS B EHAVIOR - ORIENTED A DVER - SARIAL A TTACKS AGAINST D EEP R EINFORCEMENT L EARNING"
      },
      {
        "paperId": "ccdd35ef5487e3ebb6aa11aa2fbbbfaaf2208a8f",
        "title": "Optimizing Reward Models with Proximal Policy Exploration in Preference-Based Reinforcement Learning"
      }
    ],
    "score": 19.666666666666664
  },
  {
    "id": "3c3093f5f8e9601637612fcfb8f160f116fa30e4",
    "title": "Diversity-Driven Exploration Strategy for Deep Reinforcement Learning",
    "authors": [
      "Zhang-Wei Hong",
      "Tzu-Yun Shann",
      "Shih-Yang Su",
      "Yi-Hsiang Chang",
      "Chun-Yi Lee"
    ],
    "year": 2018,
    "citationCount": 128,
    "abstract": "Efficient exploration remains a challenging research problem in reinforcement learning, especially when an environment contains large state spaces, deceptive local optima, or sparse rewards. To tackle this problem, we present a diversity-driven approach for exploration, which can be easily combined with both off- and on-policy reinforcement learning algorithms. We show that by simply adding a distance measure to the loss function, the proposed methodology significantly enhances an agent's exploratory behaviors, and thus preventing the policy from being trapped in local optima. We further propose an adaptive scaling method for stabilizing the learning process. Our experimental results in Atari 2600 show that our method outperforms baseline approaches in several tasks in terms of mean scores and exploration efficiency.",
    "url": "https://www.semanticscholar.org/paper/3c3093f5f8e9601637612fcfb8f160f116fa30e4",
    "pdf_url": "https://arxiv.org/pdf/1802.04564.pdf",
    "venue": "Neural Information Processing Systems",
    "publicationDate": "2018-02-12",
    "externalIds": {
      "MAG": "3093171323",
      "DBLP": "journals/corr/abs-1802-04564",
      "ArXiv": "1802.04564",
      "CorpusId": 3633374
    },
    "references": [
      {
        "paperId": "811df72e210e20de99719539505da54762a11c6d",
        "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor"
      },
      {
        "paperId": "2064020586d5832b55f80a7dffea1fd90a5d94dd",
        "title": "Improving Exploration in Evolution Strategies for Deep Reinforcement Learning via a Population of Novelty-Seeking Agents"
      },
      {
        "paperId": "2b6f2b163372e3417b687cc43313f2a630e7bca7",
        "title": "Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "3fab4d28be5ff73f42c8b307117aa8d680345ad9",
        "title": "Noisy Networks for Exploration"
      },
      {
        "paperId": "142497432fe179ddb6ffe600c64a837ec6179550",
        "title": "Parameter Space Noise for Exploration"
      },
      {
        "paperId": "225ab689f41cef1dc18237ef5dab059a49950abf",
        "title": "Curiosity-Driven Exploration by Self-Supervised Prediction"
      },
      {
        "paperId": "a441728f9fd6af1946368240162a72c2028c8cb1",
        "title": "Deep Exploration via Randomized Value Functions"
      },
      {
        "paperId": "0fcb2034e31a2bc2f12a2b1363d0d77baf445fdf",
        "title": "#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning"
      },
      {
        "paperId": "0936352b78a52bc5d2b5e3f04233efc56664af51",
        "title": "Conditional Image Generation with PixelCNN Decoders"
      },
      {
        "paperId": "6e90fd78e8a3b98af3954aae5209703aa966603e",
        "title": "Unifying Count-Based Exploration and Intrinsic Motivation"
      },
      {
        "paperId": "317cd4522b1f4a6f889743578143bb8823623f8b",
        "title": "VIME: Variational Information Maximizing Exploration"
      },
      {
        "paperId": "4b63e34276aa98d5345efa7fe09bb06d8a9d8f52",
        "title": "Deep Exploration via Bootstrapped DQN"
      },
      {
        "paperId": "69e76e16740ed69f4dc55361a3d319ac2f1293dd",
        "title": "Asynchronous Methods for Deep Reinforcement Learning"
      },
      {
        "paperId": "846aedd869a00c09b40f1f1f35673cb22bc87490",
        "title": "Mastering the game of Go with deep neural networks and tree search"
      },
      {
        "paperId": "024006d4c2a89f7acacc6e4438d156525b60a98f",
        "title": "Continuous control with deep reinforcement learning"
      },
      {
        "paperId": "1c86f7608aaebc486f37c2bf4d774b3f95499b58",
        "title": "Learning deep neural network policies with continuous memory states"
      },
      {
        "paperId": "2470fcf0f89082de874ac9133ccb3a8667dd89a8",
        "title": "Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models"
      },
      {
        "paperId": "340f48901f72278f6bf78a04ee5b01df208cc508",
        "title": "Human-level control through deep reinforcement learning"
      },
      {
        "paperId": "449532187c94af3dd3aa55e16d2c50f7854d2199",
        "title": "Trust Region Policy Optimization"
      },
      {
        "paperId": "687d0e59d5c35f022ce4638b3e3a6142068efc94",
        "title": "Deterministic Policy Gradient Algorithms"
      },
      {
        "paperId": "b354ee518bfc1ac0d8ac447eece9edb69e92eae1",
        "title": "MuJoCo: A physics engine for model-based control"
      },
      {
        "paperId": "f82e4ff4f003581330338aaae71f60316e58dd26",
        "title": "The Arcade Learning Environment: An Evaluation Platform for General Agents"
      },
      {
        "paperId": "25b8211adafb5a1863558992adc2a693fb082eb4",
        "title": "Evolving a diversity of virtual creatures through novelty search and local competition"
      },
      {
        "paperId": "0de77eceda6308618132204b28755ac1e63648c5",
        "title": "Abandoning Objectives: Evolution Through the Search for Novelty Alone"
      },
      {
        "paperId": "e0456b744af84f07cb5e750217463214f96c921e",
        "title": "Relative Entropy Policy Search"
      },
      {
        "paperId": "1325bbd04a3e5a7e8137cf2edf9cbca7fc6fd55d",
        "title": "Novelty Search and the Problem with Objectives"
      },
      {
        "paperId": "97efafdb4a3942ab3efba53ded7413199f79c054",
        "title": "Reinforcement Learning: An Introduction"
      }
    ],
    "cited_by": [
      {
        "paperId": "30ba9d20a1ebac6ff89ff3fb5df6fb3989f685a4",
        "title": "Noise-driven enhancement for exploration: Deep reinforcement learning for UAV autonomous navigation in complex environments"
      },
      {
        "paperId": "d4e568f40de7d095f8ac0c73500d3b160ddb3cd6",
        "title": "Diversity-Enhanced Reasoning for Subjective Questions"
      },
      {
        "paperId": "9c9eb42629b24feb04fdf778584af0094603f815",
        "title": "Boosting Exploration in Reinforcement Learning for Sparse Reward Tasks"
      },
      {
        "paperId": "5b576bebf7d18d8dc2ed90ef53cab0a8a283d1a6",
        "title": "Exploration by Random Reward Perturbation"
      },
      {
        "paperId": "7cbe01c8adb4ba539225d11e8e1d7851fc4944c5",
        "title": "AAV Visual Navigation in the Large-Scale Outdoor Environment: A Semantic-Map-Based Cognitive Escape Reinforcement Learning Method"
      },
      {
        "paperId": "4a0be5039b2d462fedafec282ac19dce5746dad8",
        "title": "Diversity-Aware Policy Optimization for Large Language Model Reasoning"
      },
      {
        "paperId": "eb35b90ef8e384151a1cf072d7609bd19e1508ef",
        "title": "Improving Human-AI Coordination through Adversarial Training and Generative Models"
      },
      {
        "paperId": "5475444b8f7d48ca5643c3c36c8caf2cce52ed40",
        "title": "KEA: Keeping Exploration Alive by Proactively Coordinating Exploration Strategies"
      },
      {
        "paperId": "d794ae75523580326e63a3a8ab75b871a0cda830",
        "title": "Dynamic collision avoidance for maritime autonomous surface ships based on deep Q-network with velocity obstacle method"
      },
      {
        "paperId": "1c4b4fb0db4d332bf3ff85f96b3c6accd281e8ef",
        "title": "Adaptive multi-model fusion learning for sparse-reward reinforcement learning"
      },
      {
        "paperId": "ea2f030235313983c283c137a9911624a20bca42",
        "title": "Safe Multiagent Coordination via Entropic Exploration"
      },
      {
        "paperId": "3eb25b4eb27808f9a6c619cdcd764140d691dea8",
        "title": "From Exploration to Mastery: Enabling LLMs to Master Tools via Self-Driven Interactions"
      },
      {
        "paperId": "6ba6da5c9c2929f0912728b27dea0853f0c402ce",
        "title": "Toward Debugging Deep Reinforcement Learning Programs with RLExplorer"
      },
      {
        "paperId": "27b32b7f56c6c9bf38afe78c6ff67bde4519a931",
        "title": "Adaptive Mixture Importance Sampling for Automated Ads Auction Tuning"
      },
      {
        "paperId": "4c2bf778ad14aaddbb66a02a2ab2735f7e75f93a",
        "title": "Diversifying Policies With Non-Markov Dispersion to Expand the Solution Space"
      },
      {
        "paperId": "91876fbf6d3da5ac4e16de5ac53a46e3a3f2cc50",
        "title": "Highly Efficient Self-Adaptive Reward Shaping for Reinforcement Learning"
      },
      {
        "paperId": "d06737f9395e592f35ef251e09bea1c18037b096",
        "title": "Random Latent Exploration for Deep Reinforcement Learning"
      },
      {
        "paperId": "339a6b9d286b7aab8c6e8860986b08d32e001549",
        "title": "Airline dynamic pricing with patient customers using deep exploration-based reinforcement learning"
      },
      {
        "paperId": "62abe6068da654be8266c11ac1c864c515c9f6ce",
        "title": "Quality-diversity based semi-autonomous teleoperation using reinforcement learning"
      },
      {
        "paperId": "a4b41d74cdfc9651e754fc66b9be66d7c683bc6b",
        "title": "Effective State Space Exploration with Phase State Graph Generation and Goal-based Path Planning"
      },
      {
        "paperId": "2329c383c531030dc7eea3c4b8927b18f112cba7",
        "title": "Exclusively Penalized Q-learning for Offline Reinforcement Learning"
      },
      {
        "paperId": "85464bc1e32cb71ef1920c9ee2c802befa74c6fe",
        "title": "Graph Reinforcement Learning for Courses of Action Analysis"
      },
      {
        "paperId": "45818304e3c689702cf9ef35454804403a2e3b70",
        "title": "A Self-Adaptive Robustness Optimization Method With Evolutionary Multi-Agent for IoT Topology"
      },
      {
        "paperId": "d5c901576caf85141fb3b88f1a2c5178fa87a9a2",
        "title": "Phasic Diversity Optimization for Population-Based Reinforcement Learning"
      },
      {
        "paperId": "bf14dfedb5f9ab84bdd8cdbf9c81b12df796ce76",
        "title": "Path planning algorithms in the autonomous driving system: A comprehensive review"
      },
      {
        "paperId": "4b91451f23e9af14c1a72e99c4394170786e0870",
        "title": "Incremental reinforcement learning for multi-objective analog circuit design acceleration"
      },
      {
        "paperId": "74c72743da5d484180642f4d44ecd8c305cb03cf",
        "title": "Developing a Flying Explorer for Autonomous Digital Modelling in Wild Unknowns"
      },
      {
        "paperId": "5cd9d01459fb90d5799d077898003e5ade202f41",
        "title": "Adaptive trajectory-constrained exploration strategy for deep reinforcement learning"
      },
      {
        "paperId": "8d3d55cb1ec19725660f08b989761dd6a26749e6",
        "title": "Hierarchical Reinforcement Learning for Air Combat at DARPA's AlphaDogfight Trials"
      },
      {
        "paperId": "bb3268d3a4549115d6bf48eeba749035e58da273",
        "title": "An Adaptive Federated Reinforcement Learning Framework with Proximal Policy Optimization for Autonomous Driving"
      },
      {
        "paperId": "d4bf28b15215f90c00c5af47080b580d1f412052",
        "title": "Information Content Exploration"
      },
      {
        "paperId": "2f0a2a2f66638198099229f2597f56a631871d7b",
        "title": "Increasing Entropy to Boost Policy Gradient Performance on Personalization Tasks"
      },
      {
        "paperId": "f378b5076c4430734be74a6972dffb35c97236c4",
        "title": "Energy management of buildings with energy storage and solar photovoltaic: A diversity in experience approach for deep reinforcement learning agents"
      },
      {
        "paperId": "ae97b8457d37811114654b8c23af1dc66445f9c8",
        "title": "MEERL: Maximum Experience Entropy Reinforcement Learning Method for Navigation and Control of Automated Vehicles"
      },
      {
        "paperId": "c9f798a2b2fbbf67dbde219e0505b9ab01df8bcb",
        "title": "Generative Model-Based Testing on Decision-Making Policies"
      },
      {
        "paperId": "dcf04310c51bb7a3ac7b0ad68205eb5af4aa8696",
        "title": "Diversifying AI: Towards Creative Chess with AlphaZero"
      },
      {
        "paperId": "07c75f0a7dd52e9c70e9159dd4e8780eef37c671",
        "title": "Robust Driving Policy Learning with Guided Meta Reinforcement Learning"
      },
      {
        "paperId": "90f47503dc72377e354b69c83befa86819fd0ecb",
        "title": "Policy Space Diversity for Non-Transitive Games"
      },
      {
        "paperId": "082abad8d0eca6f154676c733d619163a56064f6",
        "title": "Learning Diverse and Efficient Goal-reaching Policies for Robot Motion Planning"
      },
      {
        "paperId": "8181900d33fc3afcf800eaf7f270c5b90e94a1b3",
        "title": "Temporal Inconsistency-Based Intrinsic Reward for Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "d7c757528926611cc6b42c8afe370feeb49ab5b8",
        "title": "A Cover Time Study of a non-Markovian Algorithm"
      },
      {
        "paperId": "e9090851d13ce106a0f674c5c19768df1611a170",
        "title": "Learning Diverse Risk Preferences in Population-based Self-play"
      },
      {
        "paperId": "0b95df8da7c721c8d91f9b1e0a0a7a09e70451d2",
        "title": "Toward Evaluating Robustness of Reinforcement Learning with Adversarial Policy"
      },
      {
        "paperId": "51fba4b8047148c21f9bceadb9fdcabc7462e848",
        "title": "Order from Chaos: Leveraging the Order in Time as Intrinsic Reward"
      },
      {
        "paperId": "f811132d3a737ee1c71b52706f0cd78b8904f056",
        "title": "Learning Diverse Policies with Soft Self-Generated Guidance"
      },
      {
        "paperId": "4f8741413149a52c4cc3dd6b05597b0a3d892c7f",
        "title": "Deep reinforcement learning for resource allocation of mobile communication systems with device\u2010to\u2010device underlay"
      },
      {
        "paperId": "135e90ca09b6d483b86ddf2fb9f901348f0e0d11",
        "title": "Policy Dispersion in Non-Markovian Environment"
      },
      {
        "paperId": "53d661d536965daf4ff6dcfd3b7e42ffa9061d78",
        "title": "Diverse Policy Optimization for Structured Action Space"
      },
      {
        "paperId": "1e2e056b43bf6fc1691fe1f9f02c38a0c9d39fb5",
        "title": "Safe Deep Reinforcement Learning by Verifying Task-Level Properties"
      },
      {
        "paperId": "27d2df5d338ac0c68b44d2bbe1694a71ce12eb95",
        "title": "Diversity Through Exclusion (DTE): Niche Identification for Reinforcement Learning through Value-Decomposition"
      },
      {
        "paperId": "77cb8b0ef4cd4076d3ce519acb321b35370ec846",
        "title": "Efficient Exploration Through Bootstrapped and Bayesian Deep Q-Networks for Joint Power Control and Beamforming in mmWave Networks"
      },
      {
        "paperId": "b65e9e7f500437a01848ac6fde6692e6b241de85",
        "title": "AutoCost: Evolving Intrinsic Cost for Zero-violation Reinforcement Learning"
      },
      {
        "paperId": "d5781022f211bc2bc8eaeeb574e0fef86a58ec45",
        "title": "PushWorld: A benchmark for manipulation planning with tools and movable obstacles"
      },
      {
        "paperId": "19257d9dacfd4582ffe0943a21e8b19e9530f93f",
        "title": "Generalization through Diversity: Improving Unsupervised Environment Design"
      },
      {
        "paperId": "7e287a03b8d4073ec05537eb100dee91ec292195",
        "title": "PECAN: Leveraging Policy Ensemble for Context-Aware Zero-Shot Human-AI Coordination"
      },
      {
        "paperId": "b759f3fcf2459013c710bc0b000c46c8e70f9bf8",
        "title": "PRUDEX-Compass: Towards Systematic Evaluation of Reinforcement Learning in Financial Markets"
      },
      {
        "paperId": "96433de43674b7297422db6c21c5490501f08b68",
        "title": "A Review of Deep Reinforcement Learning Approaches for Smart Manufacturing in Industry 4.0 and 5.0 Framework"
      },
      {
        "paperId": "41e41f650fc1926eda019be894ad96ab56315860",
        "title": "CIM: Constrained Intrinsic Motivation for Sparse-Reward Continuous Control"
      },
      {
        "paperId": "afa519d9f4104dd35764d7ff2187f4995084ab7c",
        "title": "Winner Takes It All: Training Performant RL Populations for Combinatorial Optimization"
      },
      {
        "paperId": "59efbf91c81b5837c24c84023e1a9339e0e41fce",
        "title": "QDN: An Efficient Value Decomposition Method for Cooperative Multi-agent Deep Reinforcement Learning"
      },
      {
        "paperId": "03446d8730c4bc5cb3abc9d8c13e8e6d3898e2ae",
        "title": "Open-Ended Diverse Solution Discovery with Regulated Behavior Patterns for Cross-Domain Adaptation"
      },
      {
        "paperId": "8d03f604071ed94f3165380c745c2fae5c613d87",
        "title": "Reinforcement learning with experience replay and adaptation of action dispersion"
      },
      {
        "paperId": "ede53ce63a31e7398b657463415e7429399a6bb0",
        "title": "Generating Teammates for Training Robust Ad Hoc Teamwork Agents via Best-Response Diversity"
      },
      {
        "paperId": "649e713a760143f49c20ed4019fbf5434f89b596",
        "title": "Safety-informed mutations for evolutionary deep reinforcement learning"
      },
      {
        "paperId": "3826f86bae9ab38277ec9bec0b46a57bd001e375",
        "title": "Dynamics-aware novelty search with behavior repulsion"
      },
      {
        "paperId": "d78e611208de3b80982df0ce7b1957d1477374ab",
        "title": "Generalized Policy Improvement Algorithms With Theoretically Supported Sample Reuse"
      },
      {
        "paperId": "b75ff79617904f769c5f61cea3693e021757c600",
        "title": "Exploring Safer Behaviors for Deep Reinforcement Learning"
      },
      {
        "paperId": "795a6060f4cd9b61636e13c37ce0adbc6fee06fb",
        "title": "Soft Actor-Critic with Inhibitory Networks for Retraining UAV Controllers Faster"
      },
      {
        "paperId": "6eac59511159a027489e4cc507c05d55ab2d105c",
        "title": "Beyond Rewards: a Hierarchical Perspective on Offline Multiagent Behavioral Analysis"
      },
      {
        "paperId": "ee626478aafe6496c86512660820933538059ef2",
        "title": "Discovering Policies with DOMiNO: Diversity Optimization Maintaining Near Optimality"
      },
      {
        "paperId": "714b7c61d81687d093fd8b7d6d6737d582a4c9b7",
        "title": "Towards Applicable Reinforcement Learning: Improving the Generalization and Sample Efficiency with Policy Ensemble"
      },
      {
        "paperId": "baf2722c7a28912a6eb57ede95ae72509102c596",
        "title": "Sampling diversity driven exploration with state difference guidance"
      },
      {
        "paperId": "8d2feca35eeff419aefb7d63ab6b51b2241df8c9",
        "title": "Adaptive flow-induced vibration control using distributed sensor/actuator networks with gated recurrent units and genetic algorithms"
      },
      {
        "paperId": "89eed2e92270db2789cfb0cf00b387877809ad7a",
        "title": "Continuously Discovering Novel Strategies via Reward-Switching Policy Optimization"
      },
      {
        "paperId": "1bb82660573bbaf01572041da16842ee2398ae39",
        "title": "Learning Robust Real-Time Cultural Transmission without Human Data"
      },
      {
        "paperId": "da1e404ae194dcbcce198f5335f49f090403bbd7",
        "title": "Exploration in Deep Reinforcement Learning: A Survey"
      },
      {
        "paperId": "74e3c42e7005a0b371b0ff0cf601cbc8bb3c38bf",
        "title": "Soft Actor-Critic with Inhibitory Networks for Faster Retraining"
      },
      {
        "paperId": "b6d149124307ed0ab765e6e1a663d635b08d3f5d",
        "title": "Effects of Different Optimization Formulations in Evolutionary Reinforcement Learning on Diverse Behavior Generation"
      },
      {
        "paperId": "3be84e24b144541a8cd9030526ef2b8ef2cbfe54",
        "title": "A Survey of Exploration Methods in Reinforcement Learning"
      },
      {
        "paperId": "6e38788795a1c2122c5e5a8707e5e0d9e18e4630",
        "title": "A Policy Efficient Reduction Approach to Convex Constrained Deep Reinforcement Learning"
      },
      {
        "paperId": "feb7f993c402e2663d20bbafa83c11e6db3dfe6b",
        "title": "Cooperative Exploration for Multi-Agent Deep Reinforcement Learning"
      },
      {
        "paperId": "2d84f9f1483a73acff0f349826c6bc0ac4025075",
        "title": "Adaptable Agent Populations via a Generative Model of Policies"
      },
      {
        "paperId": "41d17c9a24acae59463e2186be1b7949fd772025",
        "title": "Exploration in policy optimization through multiple paths"
      },
      {
        "paperId": "ac5a4ac3852aae49c0819fe8383dd4cedd514499",
        "title": "A Max-Min Entropy Framework for Reinforcement Learning"
      },
      {
        "paperId": "42cc5f45d59a694813e990b29631be4e32bc6453",
        "title": "MADE: Exploration via Maximizing Deviation from Explored Regions"
      },
      {
        "paperId": "032731295fb9434a82b31fdb5e50309a2cbfef3d",
        "title": "Discovering Diverse Nearly Optimal Policies withSuccessor Features"
      },
      {
        "paperId": "ffa0819840ad3477e11bf6c9f297826a46ea7759",
        "title": "Hierarchies of Planning and Reinforcement Learning for Robot Navigation"
      },
      {
        "paperId": "f2d84d67987740d69bec2153ab7db8f46a2d80d5",
        "title": "Hierarchical Reinforcement Learning for Air-to-Air Combat"
      },
      {
        "paperId": "094cd58f07e7d82084cc8d88eb447c7f0234dc35",
        "title": "Regularized Policies are Reward Robust"
      },
      {
        "paperId": "9d88a9cd8c76843f491bec63ce0b178932899fd7",
        "title": "A Survey of Deep RL and IL for Autonomous Driving Policy Learning"
      },
      {
        "paperId": "b405764900fd2ec3979b16633056e0e6434973a8",
        "title": "Ridge Rider: Finding Diverse Solutions by Following Eigenvectors of the Hessian"
      },
      {
        "paperId": "2be22d8a3f39c7bcd1c7639b849e640a0003c831",
        "title": "Harnessing Distribution Ratio Estimators for Learning Agents with Quality and Diversity"
      },
      {
        "paperId": "20473d88bb151fd94b05ea6ce3950113f4e9d10f",
        "title": "How do Offline Measures for Exploration in Reinforcement Learning behave?"
      },
      {
        "paperId": "412a0de4d59f7c8e91219853af35fdcf5f3deb83",
        "title": "Behaviorally Diverse Traffic Simulation via Reinforcement Learning"
      },
      {
        "paperId": "2796d668f47fe53335438908876d29db08fae7ea",
        "title": "Exploration Strategy based on Validity of Actions in Deep Reinforcement Learning"
      },
      {
        "paperId": "5b7258c32425adafb5f5bab77b8ed90a7d255d70",
        "title": "Variational Dynamic for Self-Supervised Exploration in Deep Reinforcement Learning"
      },
      {
        "paperId": "a2b00e570440619bbe6a483df6b0da46b1aae665",
        "title": "Fast and slow curiosity for high-level exploration in reinforcement learning"
      },
      {
        "paperId": "1c0fdbf90e6eff01397543a1db5a2eb5e406e0bf",
        "title": "Handling Black Swan Events in Deep Learning with Diversely Extrapolated Neural Networks"
      },
      {
        "paperId": "fca28a48e3a01b9ad4c269511d9d23ff22ccd051",
        "title": "Non-local Policy Optimization via Diversity-regularized Collaborative Exploration"
      },
      {
        "paperId": "a13e1cb07cc35666b5a5c384c92416126f87fee2",
        "title": "Diversity Actor-Critic: Sample-Aware Entropy Regularization for Sample-Efficient Exploration"
      },
      {
        "paperId": "0c62a64c6e758d4cd2d968ca39d41f0f6579c14f",
        "title": "Effective Diversity in Population-Based Reinforcement Learning"
      },
      {
        "paperId": "129983331ca874142a3e8eb2d93d820bdf1f9aca",
        "title": "Deep Reinforcement Learning for Autonomous Driving: A Survey"
      },
      {
        "paperId": "3f7443ea3b8a4007ede7045433a80ae9b07da64c",
        "title": "DiversityGAN: Diversity-Aware Vehicle Motion Prediction via Latent Semantic Sampling"
      },
      {
        "paperId": "f89233364051f83997be5f0d34c3372dd6758f5e",
        "title": "Bayesian Curiosity for Efficient Exploration in Reinforcement Learning"
      },
      {
        "paperId": "c9849e184301443d1761dda5603c6a17d4b2af19",
        "title": "Multi-Path Policy Optimization"
      },
      {
        "paperId": "3dd63dab767343aec85d88bbc1e281a47ea07410",
        "title": "Maximum Entropy Diverse Exploration: Disentangling Maximum Entropy Reinforcement Learning"
      },
      {
        "paperId": "521538b3bc1fd64fb0d4dc71881edbeff03c642a",
        "title": "Attraction-Repulsion Actor-Critic for Continuous Control Reinforcement Learning"
      },
      {
        "paperId": "214d4d52b72a4980fd42d59dacd2a5a62bc8d691",
        "title": "Deep Reinforcement Learning in Atari 2600 Games"
      },
      {
        "paperId": "9fd8f5739c4594ea3aadfd12a206bfeb85e1ca47",
        "title": "Learning Interpretable Relational Structures of Hinge-loss Markov Random Fields"
      },
      {
        "paperId": "39e299cde9053346284b074d8ca58e083ad85cbd",
        "title": "Learning-Driven Exploration for Reinforcement Learning"
      },
      {
        "paperId": "0afedce86320fbb798b161e88584c93caaf6d5a8",
        "title": "Diversity-Inducing Policy Gradient: Using Maximum Mean Discrepancy to Find a Set of Diverse Policies"
      },
      {
        "paperId": "6b36776e5c0473d82cbdd2c92cd97cca7925ae08",
        "title": "TibGM: A Transferable and Information-Based Graphical Model Approach for Reinforcement Learning"
      },
      {
        "paperId": "593e03ca76fbe2b7500c8c30a1ade3d73dbf1f02",
        "title": "Diverse Exploration via Conjugate Policies for Policy Gradient Methods"
      },
      {
        "paperId": "f0f9f500566b75f5ec355d094ae68ac534c03da3",
        "title": "Measuring Mutual Policy Divergence for Multi-Agent Sequential Exploration"
      },
      {
        "paperId": "24f4a82476142c3d0b48da9855fe0698585485aa",
        "title": "Effective Diversity in Unsupervised Environment Design"
      },
      {
        "paperId": "98abf6219d648a18292686670668aef7177dc64a",
        "title": "Quality-Similar Diversity via Population Based Reinforcement Learning"
      },
      {
        "paperId": "996b706e06a0e74e0bb34c4c32c6f21ef7268b15",
        "title": "Generating Diverse Teammates to Train Robust Agents For Ad Hoc Teamwork"
      },
      {
        "paperId": "10c045a0e1e9b68a3e8a6cc2adb66bec96659984",
        "title": "Learning About Progress From Experts"
      },
      {
        "paperId": "31c7ad46bf12a9cb81ad4af463de786356020f32",
        "title": "Efficient and Stable Off-policy Training via Behavior-aware Evolutionary Learning"
      },
      {
        "paperId": "a2e0f316b9bfe24f66464edb55a8615b01f38904",
        "title": "Trajectory Diversity for Zero-Shot Coordination"
      },
      {
        "paperId": "a1a2accfe05a257dcfeaebc56114f27cf37de82f",
        "title": "Deriving Subgoals Using Network Distillation"
      },
      {
        "paperId": "3ad034cab358a202e9e8055a77b2ec71b31e3861",
        "title": "Enhanced Off-Policy Reinforcement Learning With Focused Experience Replay"
      },
      {
        "paperId": "97258b89e7d30498f5449efb5be11a518af517d6",
        "title": "A Performance-Based Start State Curriculum Framework for Reinforcement Learning"
      },
      {
        "paperId": "ddf2d3dfb9d3aca14e7d66c650399b0069d7aa88",
        "title": "Learning with target classification auxiliary task for semantic navigation"
      },
      {
        "paperId": "770d79c89ca27de2efd143b16a81fe98497820a5",
        "title": "Swarm-inspired Reinforcement Learning via Collaborative Inter-agent Knowledge Distillation"
      },
      {
        "paperId": "d0088ebdf25c7320e763ee1fec60ab2ea84f0e74",
        "title": "E 2 RL \u2013 Ef\ufb01cient Exploration in Reinforcement Learning"
      },
      {
        "paperId": "a69e2e4abf3006f5d448b65a2e92ffb3dd986e66",
        "title": "Research Article Learning Diverse Policies with Soft Self-Generated Guidance"
      },
      {
        "paperId": "14add08f5e53aac68a29c59d9f9a527c9cc81b77",
        "title": "Central Lancashire Online Knowledge (CLoK)"
      }
    ],
    "score": 18.285714285714285
  },
  {
    "id": "b2004f4f19bc6ccae8e4afc554f39142870100f5",
    "title": "MoDem: Accelerating Visual Model-Based Reinforcement Learning with Demonstrations",
    "authors": [
      "Nicklas Hansen",
      "Yixin Lin",
      "H. Su",
      "Xiaolong Wang",
      "Vikash Kumar",
      "A. Rajeswaran"
    ],
    "year": 2022,
    "citationCount": 54,
    "abstract": "Poor sample efficiency continues to be the primary challenge for deployment of deep Reinforcement Learning (RL) algorithms for real-world applications, and in particular for visuo-motor control. Model-based RL has the potential to be highly sample efficient by concurrently learning a world model and using synthetic rollouts for planning and policy improvement. However, in practice, sample-efficient learning with model-based RL is bottlenecked by the exploration challenge. In this work, we find that leveraging just a handful of demonstrations can dramatically improve the sample-efficiency of model-based RL. Simply appending demonstrations to the interaction dataset, however, does not suffice. We identify key ingredients for leveraging demonstrations in model learning -- policy pretraining, targeted exploration, and oversampling of demonstration data -- which forms the three phases of our model-based RL framework. We empirically study three complex visuo-motor control domains and find that our method is 150%-250% more successful in completing sparse reward tasks compared to prior approaches in the low data regime (100K interaction steps, 5 demonstrations). Code and videos are available at: https://nicklashansen.github.io/modemrl",
    "url": "https://www.semanticscholar.org/paper/b2004f4f19bc6ccae8e4afc554f39142870100f5",
    "pdf_url": "https://arxiv.org/pdf/2212.05698.pdf",
    "venue": "International Conference on Learning Representations",
    "publicationDate": "2022-12-12",
    "externalIds": {
      "DBLP": "journals/corr/abs-2212-05698",
      "ArXiv": "2212.05698",
      "DOI": "10.48550/arXiv.2212.05698",
      "CorpusId": 254198961
    },
    "references": [
      {
        "paperId": "4535ea7eda92b5363e0faa8dedc33b3873aca74e",
        "title": "On the Feasibility of Cross-Task Transfer with Model-Based Reinforcement Learning"
      },
      {
        "paperId": "449b33a39fd6d4e71dce06f6401a94f33e836f04",
        "title": "Graph Inverse Reinforcement Learning from Diverse Videos"
      },
      {
        "paperId": "25bc06b508b2c63b9faf77881e528530b147b988",
        "title": "DayDreamer: World Models for Physical Robot Learning"
      },
      {
        "paperId": "31d629bb161d8199e18b6f2ed7e4ecbda10b6797",
        "title": "Masked World Models for Visual Control"
      },
      {
        "paperId": "65fc1f1c567801fee3788974e753cdbf934f07e9",
        "title": "Video PreTraining (VPT): Learning to Act by Watching Unlabeled Online Videos"
      },
      {
        "paperId": "5922f437512158970c417f4413bface021df5f78",
        "title": "A Generalist Agent"
      },
      {
        "paperId": "c9bdc9ad2c3cf3230ba9aac7b5783ab411f0d204",
        "title": "R3M: A Universal Visual Representation for Robot Manipulation"
      },
      {
        "paperId": "523acd658742fb9c978e3f7638c09d7ce78af719",
        "title": "Masked Visual Pre-training for Motor Control"
      },
      {
        "paperId": "d46eef8b0d46cfdebefe9941a0f60aeeed31ded0",
        "title": "Temporal Difference Learning for Model Predictive Control"
      },
      {
        "paperId": "7b3d26bd1d65ed5937c76043b5cd058260d8469f",
        "title": "The Unsurprising Effectiveness of Pre-Trained Vision Models for Control"
      },
      {
        "paperId": "7ed1566a286068f39effa67ae5c7489dbea06414",
        "title": "VRL3: A Data-Driven Framework for Visual Deep Reinforcement Learning"
      },
      {
        "paperId": "1d803f07e4591bd67c358eef715bcd443e821894",
        "title": "BC-Z: Zero-Shot Task Generalization with Robotic Imitation Learning"
      },
      {
        "paperId": "5e39c030eb7e3ad516cbe1bdcd7b8c4a9e51a6a9",
        "title": "The Surprising Effectiveness of Representation Learning for Visual Imitation"
      },
      {
        "paperId": "3032844d6ac6882ccb03e7a2c22a0026b210ac05",
        "title": "What Matters in Learning from Offline Human Demonstrations for Robot Manipulation"
      },
      {
        "paperId": "e06c005e98281af455c454ce2478285f6f3afeca",
        "title": "Mastering Visual Continuous Control: Improved Data-Augmented Reinforcement Learning"
      },
      {
        "paperId": "9f2e34581ca03160e8fd8b770203a5e7c2a902d3",
        "title": "RRL: Resnet as representation for Reinforcement Learning"
      },
      {
        "paperId": "390e248117f8bafad8b47b8e799352980c2f3f70",
        "title": "Stabilizing Deep Q-Learning with ConvNets and Vision Transformers under Data Augmentation"
      },
      {
        "paperId": "61ff6872fa07788acdbe85b1319554a5fb9ce0db",
        "title": "XIRL: Cross-embodiment Inverse Reinforcement Learning"
      },
      {
        "paperId": "5c9815f7d907b79b088cdf49665be9b8dc89e5e7",
        "title": "A Framework for Efficient Robotic Manipulation"
      },
      {
        "paperId": "b44bb1762640ed72091fd5f5fdc20719a6dc24af",
        "title": "Mastering Atari with Discrete World Models"
      },
      {
        "paperId": "59d8b07813df4a9d7fb750fe65bb54dabf160788",
        "title": "Visual Imitation Made Easy"
      },
      {
        "paperId": "0272b14dd471fe7b81df703af1b71d7600b77215",
        "title": "Accelerating Online Reinforcement Learning with Offline Datasets"
      },
      {
        "paperId": "744139d65c3bf6da6a6acd384a32d94a06f44f62",
        "title": "Reinforcement Learning with Augmented Data"
      },
      {
        "paperId": "6568423cfaca7e24c88ea208cb0e67129e43aa9b",
        "title": "Image Augmentation Is All You Need: Regularizing Deep Reinforcement Learning from Pixels"
      },
      {
        "paperId": "9efb64f20ab1f157ca9f4050d4aaacf6c3f9b2b2",
        "title": "CURL: Contrastive Unsupervised Representations for Reinforcement Learning"
      },
      {
        "paperId": "f2282e5386061b7ebaa6e7e6286cd985d3af0601",
        "title": "State-Only Imitation Learning for Dexterous Manipulation"
      },
      {
        "paperId": "b40f74087b7e069327ca1d29fd20d7065647ee64",
        "title": "Grasping in the Wild: Learning 6DoF Closed-Loop Grasping From Low-Cost Demonstrations"
      },
      {
        "paperId": "c39fb7a46335c23f7529dd6f9f980462fd38653a",
        "title": "Mastering Atari, Go, chess and shogi by planning with a learned model"
      },
      {
        "paperId": "0bc855f84668b35cb65618d996d09f6e434d28c9",
        "title": "Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning"
      },
      {
        "paperId": "00a5cdb5768fdfabfa9decad28271afde9880579",
        "title": "End-to-End Robotic Reinforcement Learning without Reward Engineering"
      },
      {
        "paperId": "c520bf47db3360ae3a52219771390a354ed8a91f",
        "title": "Go-Explore: a New Approach for Hard-Exploration Problems"
      },
      {
        "paperId": "12c0751b4f51ed833172a713b7e32390032ead93",
        "title": "Soft Actor-Critic Algorithms and Applications"
      },
      {
        "paperId": "54cd5a5ddd286442fa94da7ec344a7e76b9a6ccd",
        "title": "Visual Foresight: Model-Based Deep Reinforcement Learning for Vision-Based Robotic Control"
      },
      {
        "paperId": "df350766b53d4db23790a0408b0d2c7a185cff74",
        "title": "Multiple Interactions Made Easy (MIME): Large Scale Demonstrations Data for Imitation"
      },
      {
        "paperId": "4cb3fd057949624aa4f0bbe7a6dcc8777ff04758",
        "title": "Exploration by Random Network Distillation"
      },
      {
        "paperId": "41cca0b0a27ba363ca56e7033569aeb1922b0ac9",
        "title": "Recurrent World Models Facilitate Policy Evolution"
      },
      {
        "paperId": "e75271c7f985752a8dcce2347ce922d7f4cd3df1",
        "title": "SOLAR: Deep Structured Latent Representations for Model-Based Reinforcement Learning"
      },
      {
        "paperId": "35da1cd669ad5492a6358ea53aea95de28d39ded",
        "title": "Behavioral Cloning from Observation"
      },
      {
        "paperId": "4debb99c0c63bfaa97dd433bc2828e4dac81c48b",
        "title": "Addressing Function Approximation Error in Actor-Critic Methods"
      },
      {
        "paperId": "a9a3ed69c94a3e1c08ef1f833d9199f57736238b",
        "title": "DeepMind Control Suite"
      },
      {
        "paperId": "b864f89eaa91120e04e8c62eb0b36568ab4244a8",
        "title": "Deep Imitation Learning for Complex Manipulation Tasks from Virtual Reality Teleoperation"
      },
      {
        "paperId": "e010ba3ff5744604cdbfe44a733e2a98649ee907",
        "title": "Learning Complex Dexterous Manipulation with Deep Reinforcement Learning and Demonstrations"
      },
      {
        "paperId": "c28ec2a40a2c77e20d64cf1c85dc931106df8e83",
        "title": "Overcoming Exploration in Reinforcement Learning with Demonstrations"
      },
      {
        "paperId": "5c57bb5630835a05eb1c3d0df3e12d6180d75de2",
        "title": "One-Shot Imitation Learning"
      },
      {
        "paperId": "e86f71ca2948d17b003a5f068db1ecb2b77827f7",
        "title": "Concrete Problems in AI Safety"
      },
      {
        "paperId": "4ab53de69372ec2cd2d90c126b6a100165dc8ed1",
        "title": "Generative Adversarial Imitation Learning"
      },
      {
        "paperId": "0e3cc46583217ec81e87045a4f9ae3478a008227",
        "title": "End to End Learning for Self-Driving Cars"
      },
      {
        "paperId": "024006d4c2a89f7acacc6e4438d156525b60a98f",
        "title": "Continuous control with deep reinforcement learning"
      },
      {
        "paperId": "3177334d5ef8e0ece30913b4692b86801f0845c5",
        "title": "Model Predictive Path Integral Control using Covariance Variable Importance Sampling"
      },
      {
        "paperId": "b6b8a1b80891c96c28cc6340267b58186157e536",
        "title": "End-to-End Training of Deep Visuomotor Policies"
      },
      {
        "paperId": "2319a491378867c7049b3da055c5df60e1671158",
        "title": "Playing Atari with Deep Reinforcement Learning"
      },
      {
        "paperId": "79ab3c49903ec8cb339437ccf5cf998607fc313e",
        "title": "A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning"
      },
      {
        "paperId": "e0456b744af84f07cb5e750217463214f96c921e",
        "title": "Relative Entropy Policy Search"
      },
      {
        "paperId": "be2f6bb4e262d7015b931c12eb1559827ba597dc",
        "title": "Robot learning from demonstration"
      },
      {
        "paperId": null,
        "title": "174 and J"
      },
      {
        "paperId": null,
        "title": "2022) uses shaped rewards (as opposed to our main results that use sparse rewards)"
      },
      {
        "paperId": null,
        "title": "Meta-World based on task difficulty following the categorization"
      },
      {
        "paperId": null,
        "title": "2021a) for DMControl when applicable, but choose to use a unified"
      },
      {
        "paperId": null,
        "title": "2022) we apply image shift augmentation (Kostrikov et al., 2020) to all observations"
      },
      {
        "paperId": null,
        "title": "2018), following the experimental setups of Seo et al"
      },
      {
        "paperId": null,
        "title": "Learning from Demonstration and Adaptation of Biped Locomotion"
      },
      {
        "paperId": null,
        "title": "and 193 W"
      },
      {
        "paperId": "7786bc6c25ba38ff0135f1bdad192f6b3c4ad0b3",
        "title": "ALVINN, an autonomous land vehicle in a neural network"
      },
      {
        "paperId": "c7dbf5ed7e9b63e104adb4e18bfbc98f5d6afdae",
        "title": "R-MAX - A General Polynomial Time Algorithm for Near-Optimal Reinforcement Learning"
      },
      {
        "paperId": "97efafdb4a3942ab3efba53ded7413199f79c054",
        "title": "Reinforcement Learning: An Introduction"
      },
      {
        "paperId": "58b7d478a8ced7fc8967ce21f4448b0f1a3f79b2",
        "title": "on pre - training"
      },
      {
        "paperId": null,
        "title": "pi): Sequential( (0): Linear(in_features=Z, out_features=512) (1): ELU(alpha=1.0) (2): Linear(in_features=512, out_features=512 (3): ELU(alpha=1.0) (4): Linear(in_features=512, out_features=A))"
      },
      {
        "paperId": null,
        "title": "Linear(in_features=Z, out_features=256) (1)"
      },
      {
        "paperId": null,
        "title": "ELU(alpha=1.0)"
      },
      {
        "paperId": null,
        "title": "Linear(in_features=512, out_features=1)) (pi): Sequential( (0): Linear(in_features=Z, out_features=512)"
      }
    ],
    "cited_by": [
      {
        "paperId": "b38af7b162f9c4bef03975f3832410b2456718fd",
        "title": "EXPO: Stable Reinforcement Learning with Expressive Policies"
      },
      {
        "paperId": "e0daefa92aecf41d5191f1fb6560dd3db28982a6",
        "title": "CueLearner: Bootstrapping and local policy adaptation from relative feedback"
      },
      {
        "paperId": "5a33c4bdc25b7e286ab2883ed4d48c049a4ac6de",
        "title": "IDAGC: Adaptive Generalized Human-Robot Collaboration via Human Intent Estimation and Multimodal Policy Learning"
      },
      {
        "paperId": "6e19a3ed0f582faa74d3909aaf74d8997100739f",
        "title": "Reinforcement Learning via Implicit Imitation Guidance"
      },
      {
        "paperId": "33f24a2ac0248a72627b1ea42651e5b0700f34b6",
        "title": "Multi-Stage Manipulation with Demonstration-Augmented Reward, Policy, and World Model Learning"
      },
      {
        "paperId": "02dfba49e62d9f1d796e0eaa3500bb51bf5e657a",
        "title": "Physics-Driven Data Generation for Contact-Rich Manipulation via Trajectory Optimization"
      },
      {
        "paperId": "119c3a39d30dac52dce812dc0900a2a800c15c1e",
        "title": "TDMPBC: Self-Imitative Reinforcement Learning for Humanoid Robot Control"
      },
      {
        "paperId": "c5c5a0d4cfe702e2c432789bb0034efdec7af678",
        "title": "Learning from Suboptimal Data in Continuous Control via Auto-Regressive Soft Q-Network"
      },
      {
        "paperId": "7f624dbf7f08c6c1e91245f746e6c942b0a41221",
        "title": "Metacognition for Unknown Situations and Environments (MUSE)"
      },
      {
        "paperId": "d3f4c938b5a31f82402ec838fc3f2458860f6f4b",
        "title": "The Surprising Ineffectiveness of Pre-Trained Visual Representations for Model-Based Reinforcement Learning"
      },
      {
        "paperId": "2302faac0a41449bfa92f1edbf041ea83ae872be",
        "title": "So You Think You Can Scale Up Autonomous Robot Data Collection?"
      },
      {
        "paperId": "f29f88e08923ce474f17b2c5bc955ac19e585297",
        "title": "Exploring the Edges of Latent State Clusters for Goal-Conditioned Reinforcement Learning"
      },
      {
        "paperId": "8431320f50a398d5ae6ad8158e374342f48fbb63",
        "title": "Learning World Models for Unconstrained Goal Navigation"
      },
      {
        "paperId": "93b41399a5d733db856ade26d9947d4c49b77850",
        "title": "Jacta: A Versatile Planner for Learning Dexterous and Whole-body Manipulation"
      },
      {
        "paperId": "25908892225280f00bbef3fe555e2683ed50fd72",
        "title": "Continuous Control with Coarse-to-fine Reinforcement Learning"
      },
      {
        "paperId": "ed7d213de959004feab13b1f713a6116a9dfa320",
        "title": "ATraDiff: Accelerating Online Reinforcement Learning with Imaginary Trajectories"
      },
      {
        "paperId": "1a4238a1dd6068f59b3daa8923a6612a2f98cd12",
        "title": "DEER: A Delay-Resilient Framework for Reinforcement Learning with Variable Delays"
      },
      {
        "paperId": "dbc1aedb83146afa97dad658ee8761902cf25d6b",
        "title": "Hierarchical World Models as Visual Whole-Body Humanoid Controllers"
      },
      {
        "paperId": "320a68d93dbc3da7d75bb3183c071423647564e2",
        "title": "World Models for General Surgical Grasping"
      },
      {
        "paperId": "6534cd07cf88680dd38747597bb6990b6fa64eb0",
        "title": "Efficient Imitation Learning with Conservative World Models"
      },
      {
        "paperId": "e70d671b3333e2f437e00f5dbab98c29d412bc80",
        "title": "Learning Latent Dynamic Robust Representations for World Models"
      },
      {
        "paperId": "0fecc3a349cf4bc0f430cb8b9a69e8540ee10905",
        "title": "Reverse Forward Curriculum Learning for Extreme Sample and Demonstration Efficiency in Reinforcement Learning"
      },
      {
        "paperId": "7204134c8307af08fecc2c9f79cc1d67667e1b42",
        "title": "Think Holistically, Act Down-to-Earth: A Semantic Navigation Strategy With Continuous Environmental Representation and Multi-Step Forward Planning"
      },
      {
        "paperId": "703e2af67a6e261a7848ca8ec4b3dd6d546c8181",
        "title": "Point Cloud Models Improve Visual Robustness in Robotic Learners"
      },
      {
        "paperId": "eb184154416801d7c53bd632f20d95e34e28cf35",
        "title": "ViViDex: Learning Vision-Based Dexterous Manipulation from Human Videos"
      },
      {
        "paperId": "d3a1b01192f8e96097ca2fbb717d343191d62528",
        "title": "RH20T-P: A Primitive-Level Robotic Dataset Towards Composable Generalization Agents"
      },
      {
        "paperId": "4186f74da6d793c91c5ca4d8a10cf2f8638eade6",
        "title": "RLfOLD: Reinforcement Learning from Online Demonstrations in Urban Autonomous Driving"
      },
      {
        "paperId": "8bb32652e0a935b6ba1f54bd3d39cad80db09908",
        "title": "3D Diffusion Policy"
      },
      {
        "paperId": "4d1eadaa9a04e86aef2278ae13eb9fce644c9fc5",
        "title": "Learning and Leveraging World Models in Visual Representation Learning"
      },
      {
        "paperId": "b28fcb0b6a5a51040f93824a171b4dc8b8c5c71a",
        "title": "Diffusion World Model"
      },
      {
        "paperId": "fef99d99507a8fcad9e211bd9aade1ced0f97b41",
        "title": "R\u00d7R: Rapid eXploration for Reinforcement Learning via Sampling-based Reset Distributions and Imitation Pre-training"
      },
      {
        "paperId": "00f776a588e13e4c9278e91f7c10c536f6dc8c2d",
        "title": "Imitation Bootstrapped Reinforcement Learning"
      },
      {
        "paperId": "c96645997967df130d425932f06f3cfe89962d0f",
        "title": "Model-Based Runtime Monitoring with Interactive Imitation Learning"
      },
      {
        "paperId": "8eeccf1033fd759c868b29ccb6aa70f5ffd1887c",
        "title": "TD-MPC2: Scalable, Robust World Models for Continuous Control"
      },
      {
        "paperId": "9d10416615d6b0ffb6e18389f35566542ed92d19",
        "title": "Finetuning Offline World Models in the Real World"
      },
      {
        "paperId": "0ff912d300c56a26bc7817579602665675652028",
        "title": "Blending Imitation and Reinforcement Learning for Robust Policy Improvement"
      },
      {
        "paperId": "3209365d135ce56b31c2e907962ea38ca68e9090",
        "title": "H-InDex: Visual Reinforcement Learning with Hand-Informed Representations for Dexterous Manipulation"
      },
      {
        "paperId": "e2d773b4d2f8bc1c9b35324bdbe43455a7c43559",
        "title": "MoDem-V2: Visuo-Motor World Models for Real-World Robot Manipulation"
      },
      {
        "paperId": "b3d213883226192fd2f59f0413ceb4610249362a",
        "title": "RePo: Resilient Model-Based Reinforcement Learning by Regularizing Posterior Predictability"
      },
      {
        "paperId": "6eddfde78d5f6b10206f00675102fe3ddef8ff89",
        "title": "Structured World Models from Human Videos"
      },
      {
        "paperId": "dea2e300e007902c43c75897c2fad14a730711d9",
        "title": "Goal-Conditioned Predictive Coding for Offline Reinforcement Learning"
      },
      {
        "paperId": "41dc40dc47dcd3ba65044395b20973b155233ff7",
        "title": "MoVie: Visual Model-Based Policy Adaptation for View Generalization"
      },
      {
        "paperId": "c7e4a463de5223810ae8c618bfdb54ee98ba3e60",
        "title": "TorchRL: A data-driven decision-making library for PyTorch"
      },
      {
        "paperId": "b39f0bebd19ab2e253997af95f34c78042c3c585",
        "title": "Imitating Task and Motion Planning with Visuomotor Transformers"
      },
      {
        "paperId": "7572bf46bd1d895c92f367b2b46c205cfeb2e967",
        "title": "Masked Trajectory Models for Prediction, Representation, and Control"
      },
      {
        "paperId": "326f6a8011e43322c433751b9cc31fd56564621c",
        "title": "Where are we in the search for an Artificial Visual Cortex for Embodied Intelligence?"
      },
      {
        "paperId": "5aeef5fc2533f8deeefb73688040279acad67e96",
        "title": "Bridging Imitation and Online Reinforcement Learning: An Optimistic Tale"
      },
      {
        "paperId": "0518ecaada7166ff10e7064b49b737827b0105ee",
        "title": "Continual Visual Reinforcement Learning with A Life-Long World Model"
      },
      {
        "paperId": "d466bccd0a984864092f55d3414f7750821b45a4",
        "title": "Learning Sparse Control Tasks from Pixels by Latent Nearest-Neighbor-Guided Explorations"
      },
      {
        "paperId": "bd38cbbb346a347cb5b60ac4a133b3d73cb44e07",
        "title": "Efficient Online Reinforcement Learning with Offline Data"
      },
      {
        "paperId": "668ef8248bf0ecfaf36cc6a6c65a4f136b976858",
        "title": "On Pre-Training for Visuo-Motor Control: Revisiting a Learning-from-Scratch Baseline"
      },
      {
        "paperId": "4535ea7eda92b5363e0faa8dedc33b3873aca74e",
        "title": "On the Feasibility of Cross-Task Transfer with Model-Based Reinforcement Learning"
      },
      {
        "paperId": "1ec3409eaad97a256e4177efeafd411dae9a344d",
        "title": "Goal-Conditioned Predictive Coding as an Implicit Planner for Offline Reinforcement Learning"
      },
      {
        "paperId": "73875b765e82cbca10f3f773529027423e108cd3",
        "title": "RH20T-P: A Primitive-Level Robotic Manipulation Dataset towards Composable Generalization Agents in Real-world Scenarios"
      }
    ],
    "score": 18.0
  },
  {
    "id": "61f371768cdc093828f432660e22f7a17f22e2af",
    "title": "Offline Meta-Reinforcement Learning with Online Self-Supervision",
    "authors": [
      "Vitchyr H. Pong",
      "Ashvin Nair",
      "Laura M. Smith",
      "Catherine Huang",
      "S. Levine"
    ],
    "year": 2021,
    "citationCount": 70,
    "abstract": "Meta-reinforcement learning (RL) methods can meta-train policies that adapt to new tasks with orders of magnitude less data than standard RL, but meta-training itself is costly and time-consuming. If we can meta-train on offline data, then we can reuse the same static dataset, labeled once with rewards for different tasks, to meta-train policies that adapt to a variety of new tasks at meta-test time. Although this capability would make meta-RL a practical tool for real-world use, offline meta-RL presents additional challenges beyond online meta-RL or standard offline RL settings. Meta-RL learns an exploration strategy that collects data for adapting, and also meta-trains a policy that quickly adapts to data from a new task. Since this policy was meta-trained on a fixed, offline dataset, it might behave unpredictably when adapting to data collected by the learned exploration strategy, which differs systematically from the offline data and thus induces distributional shift. We propose a hybrid offline meta-RL algorithm, which uses offline data with rewards to meta-train an adaptive policy, and then collects additional unsupervised online data, without any reward labels to bridge this distribution shift. By not requiring reward labels for online collection, this data can be much cheaper to collect. We compare our method to prior work on offline meta-RL on simulated robot locomotion and manipulation tasks and find that using additional unsupervised online data collection leads to a dramatic improvement in the adaptive capabilities of the meta-trained policies, matching the performance of fully online meta-RL on a range of challenging domains that require generalization to new tasks.",
    "url": "https://www.semanticscholar.org/paper/61f371768cdc093828f432660e22f7a17f22e2af",
    "pdf_url": "https://arxiv.org/pdf/2107.03974.pdf",
    "venue": "International Conference on Machine Learning",
    "publicationDate": "2021-07-08",
    "externalIds": {
      "DBLP": "journals/corr/abs-2107-03974",
      "ArXiv": "2107.03974",
      "CorpusId": 235765453
    },
    "references": [
      {
        "paperId": "2f3efe44083af91cef562c1a3451eee2f8601d22",
        "title": "WebGPT: Browser-assisted question-answering with human feedback"
      },
      {
        "paperId": "adcae35901c36325478a03b647e14222a53ea9fc",
        "title": "What Can I Do Here? Learning New Skills by Imagining Visual Affordances"
      },
      {
        "paperId": "1eecf98de4024893bb51ed2e26b9d186e305c519",
        "title": "Semi-supervised reward learning for offline reinforcement learning"
      },
      {
        "paperId": "23472dde7735dbe88b300662a0421aca52692075",
        "title": "Offline Learning from Demonstrations and Unlabeled Experience"
      },
      {
        "paperId": "acf24ff124d9359d0404ed77967d292fc2e0a342",
        "title": "MELD: Meta-Reinforcement Learning from Images via Latent State Models"
      },
      {
        "paperId": "17908c7db26985704c00dd4521932f25c43dbe17",
        "title": "Offline Meta-Reinforcement Learning with Advantage Weighting"
      },
      {
        "paperId": "9342fce9c5a69f545a778ca7e885ba9d63af928f",
        "title": "Offline Meta Reinforcement Learning"
      },
      {
        "paperId": "759ae1234d46e2d1399ce9d642724738a766ed22",
        "title": "Meta-Gradient Reinforcement Learning with an Objective Discovered Online"
      },
      {
        "paperId": "0272b14dd471fe7b81df703af1b71d7600b77215",
        "title": "Accelerating Online Reinforcement Learning with Offline Datasets"
      },
      {
        "paperId": "596579e8e72efdc3cdcc6f7faa84545c2b9b7423",
        "title": "Learning Adaptive Exploration Strategies in Dynamic Environments Through Informed Policy Regularization"
      },
      {
        "paperId": "5e7bc93622416f14e6948a500278bfbe58cd3890",
        "title": "Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems"
      },
      {
        "paperId": "a326d9f2d2d351001fece788165dbcbb524da2e4",
        "title": "D4RL: Datasets for Deep Data-Driven Reinforcement Learning"
      },
      {
        "paperId": "1803722f786b901a744bc363c0ebdc51902ceceb",
        "title": "Learning Agile Robotic Locomotion Skills by Imitating Animals"
      },
      {
        "paperId": "4813be9599249bc024aad1d674c8d1da3479d532",
        "title": "Unsupervised Curricula for Visual Meta-Reinforcement Learning"
      },
      {
        "paperId": "fcb4d00462eefa53a496b45acb87fa0d258d3500",
        "title": "Positive-Unlabeled Reward Learning"
      },
      {
        "paperId": "0bc855f84668b35cb65618d996d09f6e434d28c9",
        "title": "Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning"
      },
      {
        "paperId": "361e953f792a585496834ee14216b94d0ce9ae74",
        "title": "VariBAD: A Very Good Method for Bayes-Adaptive Deep RL via Meta-Learning"
      },
      {
        "paperId": "2c819870111efb9fa70e359c15e2031e992c2b4a",
        "title": "Improving Generalization in Meta Reinforcement Learning using Learned Objectives"
      },
      {
        "paperId": "1534d860ab98b64d23ffa741d0ae52b6cadbf503",
        "title": "Benchmarking Batch Deep Reinforcement Learning Algorithms"
      },
      {
        "paperId": "a436e41141a9d9c5dfc44d16fade67b9a3b76778",
        "title": "Test-Time Training with Self-Supervision for Generalization under Distribution Shifts"
      },
      {
        "paperId": "78aea5a51feb90e988a4d00302616e56d1be5eb0",
        "title": "Scaling data-driven robotics with reward sketching and batch reinforcement learning"
      },
      {
        "paperId": "9be492858863c8c7c24be1ecb75724de5086bd8e",
        "title": "Behavior Regularized Offline Reinforcement Learning"
      },
      {
        "paperId": "4d6fbd57707038bc18537a6a73e80a3ec2d2298b",
        "title": "Disentangled Cumulants Help Successor Representations Transfer to New Tasks"
      },
      {
        "paperId": "82b4b03a4659d6e04bd7cbf51d6e08fde1348dbd",
        "title": "Stabilizing Off-Policy Q-Learning via Bootstrapping Error Reduction"
      },
      {
        "paperId": "b4e69b0172d69c80f83366c296b6222805360445",
        "title": "SQIL: Imitation Learning via Reinforcement Learning with Sparse Rewards"
      },
      {
        "paperId": "28a3b7f35b22c0140c9b9cbd8cdb0cfc03723181",
        "title": "REPLAB: A Reproducible Low-Cost Arm Benchmark Platform for Robotic Learning"
      },
      {
        "paperId": "9a3c9a0ac460c7891d03d56146f2d566c7e0fb08",
        "title": "Meta reinforcement learning as task inference"
      },
      {
        "paperId": "4625628163a2ee0e6cd320cd7a14b4ccded2a631",
        "title": "Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables"
      },
      {
        "paperId": "12c0751b4f51ed833172a713b7e32390032ead93",
        "title": "Soft Actor-Critic Algorithms and Applications"
      },
      {
        "paperId": "5285cb8faada5de8a92a47622950f6cfd476ac1d",
        "title": "Off-Policy Deep Reinforcement Learning without Exploration"
      },
      {
        "paperId": "c6f913e4baa7f2c85363c0625c87003ad3b3a14c",
        "title": "Scalable agent alignment via reward modeling: a research direction"
      },
      {
        "paperId": "8ede7ddf99986d69562455bc8d69222fc3e27350",
        "title": "Recurrent Experience Replay in Distributed Reinforcement Learning"
      },
      {
        "paperId": "b65a6be07ce9c86797e6917258cf5ba45273ee73",
        "title": "Unsupervised Control Through Non-Parametric Discriminative Rewards"
      },
      {
        "paperId": "c456941a270cb2040eed4abfb39150508caf920c",
        "title": "ProMP: Proximal Meta-Policy Search"
      },
      {
        "paperId": "3aadab924520c58be81781aafd51e6807e9c4576",
        "title": "Visual Reinforcement Learning with Imagined Goals"
      },
      {
        "paperId": "f650f1fd44ab0778d30577f8c2077b2ff58830da",
        "title": "Transfer in Deep Reinforcement Learning Using Successor Features and Generalised Policy Improvement"
      },
      {
        "paperId": "b93317f61c6ed99542da9d1d691ded9732c16c1c",
        "title": "Unsupervised Meta-Learning for Reinforcement Learning"
      },
      {
        "paperId": "4d357ffc1cf60d3f34b5345899619882791474bb",
        "title": "Deep Reinforcement Learning for General Video Game AI"
      },
      {
        "paperId": "2a49a71c9d40051a03c4445fe49025bc75d9eeb6",
        "title": "Meta-Gradient Reinforcement Learning"
      },
      {
        "paperId": "68c108795deef06fa929d1f6e96b75dbf7ce8531",
        "title": "Meta-Reinforcement Learning of Structured Exploration Strategies"
      },
      {
        "paperId": "852c931b5d9f9d4256befd725ee4185945c4964c",
        "title": "Temporal Difference Models: Model-Free Deep RL for Model-Based Control"
      },
      {
        "paperId": "330d56c3641cddd8c78440e768cd80795f23cab4",
        "title": "Unsupervised Learning of Goal Spaces for Intrinsically Motivated Goal Exploration"
      },
      {
        "paperId": "565af8f2ef461b1d7368f3e9899e0f576e4f0a24",
        "title": "Learning an Embedding Space for Transferable Robot Skills"
      },
      {
        "paperId": "80196cdfcd0c6ce2953bf65a7f019971e2026386",
        "title": "IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures"
      },
      {
        "paperId": "59094d64844ee21e32560fb08db6d53cc3af0c51",
        "title": "Inverse Reward Design"
      },
      {
        "paperId": "5e2c4e7b3302549b3718601c44d9af6c7554efef",
        "title": "Learning Robust Rewards with Adversarial Inverse Reinforcement Learning"
      },
      {
        "paperId": "a762ae907b7dd71a59bd8bd98aba69dfe2de13a2",
        "title": "Emergence of Locomotion Behaviours in Rich Environments"
      },
      {
        "paperId": "429ed4c9845d0abd1f8204e1d7705919559bc2a2",
        "title": "Hindsight Experience Replay"
      },
      {
        "paperId": "1fdf0319b4a1db62c611980351e5f4c2f08958cd",
        "title": "GEP-PG: Decoupling Exploration and Exploitation in Deep Reinforcement Learning Algorithms"
      },
      {
        "paperId": "5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd",
        "title": "Deep Reinforcement Learning from Human Preferences"
      },
      {
        "paperId": "c889d6f98e6d79b89c3a6adf8a921f88fa6ba518",
        "title": "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks"
      },
      {
        "paperId": "954b01151ff13aef416d27adc60cd9a076753b1a",
        "title": "RL$^2$: Fast Reinforcement Learning via Slow Reinforcement Learning"
      },
      {
        "paperId": "d8686b657b61a37da351af2952aabd8b281de408",
        "title": "Successor Features for Transfer in Reinforcement Learning"
      },
      {
        "paperId": "4ab53de69372ec2cd2d90c126b6a100165dc8ed1",
        "title": "Generative Adversarial Imitation Learning"
      },
      {
        "paperId": "10a4992ece5baea79326a8878a6244eeacbc6af5",
        "title": "Deep Successor Reinforcement Learning"
      },
      {
        "paperId": "2b10281297ee001a9f3f4ea1aa9bea6b638c27df",
        "title": "OpenAI Gym"
      },
      {
        "paperId": "494e2d5b40dcebde349f9872c7317e5003f9c5d2",
        "title": "Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection"
      },
      {
        "paperId": "04162cb8cfaa0f7e37586823ff4ad0bff09ed21d",
        "title": "Guided Cost Learning: Deep Inverse Optimal Control via Policy Optimization"
      },
      {
        "paperId": "9b41a7f3e4a70eb5bc4113cbc6237b6120b7efc4",
        "title": "End-to-End Goal-Driven Web Navigation"
      },
      {
        "paperId": "024006d4c2a89f7acacc6e4438d156525b60a98f",
        "title": "Continuous control with deep reinforcement learning"
      },
      {
        "paperId": "5dc2a215bd7cd5bdd3a0baa8c967575632696fac",
        "title": "Universal Value Function Approximators"
      },
      {
        "paperId": "340f48901f72278f6bf78a04ee5b01df208cc508",
        "title": "Human-level control through deep reinforcement learning"
      },
      {
        "paperId": "b354ee518bfc1ac0d8ac447eece9edb69e92eae1",
        "title": "MuJoCo: A physics engine for model-based control"
      },
      {
        "paperId": "70e10a5459c6f1aaf346ee4f2dcc837151fbe75c",
        "title": "Efficient Reductions for Imitation Learning"
      },
      {
        "paperId": "bfca2e08a2c9d1eb8dc2ce553c72f027daef484e",
        "title": "Teleoperation of a robot manipulator using a vision-based human-robot interface"
      },
      {
        "paperId": "f65020fc3b1692d7989e099d6b6e698be5a50a93",
        "title": "Apprenticeship learning via inverse reinforcement learning"
      },
      {
        "paperId": "f69e05a32fd1541bb41d981bdb013366c9150a85",
        "title": "Is imitation learning the route to humanoid robots?"
      },
      {
        "paperId": "516a540b0737c886d514f200ed84d6c77d0c93bb",
        "title": "Towards Real Robot Learning in the Wild: A Case Study in Bipedal Locomotion"
      },
      {
        "paperId": null,
        "title": "Experimental Details C.1. Data Collection Difference from Prior Work BOReL and MACAW were both developed assuming several orders of magnitude more data than the regime that we tested"
      },
      {
        "paperId": "9a208167b153e6dfc3327415068ae4a7a6dcd006",
        "title": "Reinforcement Learning Algorithm for Partially Observable Markov Decision Problems"
      },
      {
        "paperId": "6df43f70f383007a946448122b75918e3a9d6682",
        "title": "Learning to Achieve Goals"
      },
      {
        "paperId": null,
        "title": "r \u03c6 d by minimizing L actor , L critic , L reward with samples z, h . 6: for iteration n = 1, 2, . . . , N online do self-supervised phase 7: Collect trajectory \u03c4 with \u03c0 \u03b8 (a | s, z)"
      },
      {
        "paperId": null,
        "title": "Q-function Q w , encoder q \u03c6e , and decoder r \u03c6 d . 2: for iteration n = 1, 2, . . . , N offline do offline phase 3: Sample buffer D i \u223c D and two histories from buffer h"
      },
      {
        "paperId": null,
        "title": "Update \u03c0 \u03b8 , Q w by minimizing L actor , L critic with samples z, h"
      },
      {
        "paperId": null,
        "title": "Use h offline to label the rewards in \u03c4 , as in Equation (4), and add the resulting data to D i . 10: Sample buffer D i \u223c D and two histories from buffer h"
      },
      {
        "paperId": null,
        "title": "Pybullet, a python module for physics simulation for games, robotics and machine learning"
      }
    ],
    "cited_by": [
      {
        "paperId": "62ad04f000834aa66cc19a7a81983d5f7a8227ff",
        "title": "Scenario-Free Autonomous Driving With Multi-Task Offline-to-Online Reinforcement Learning"
      },
      {
        "paperId": "f093a3cdb9aafc1fe686d0653b5cbd843f3d1c73",
        "title": "LiveStream Meta-DAMS: Multipath Scheduler Using Hybrid Meta Reinforcement Learning for Live Video Streaming"
      },
      {
        "paperId": "dc1033ec5430f4462da65e585e3ab0bcaad337bc",
        "title": "Observations Meet Actions: Learning Control-Sufficient Representations for Robust Policy Generalization"
      },
      {
        "paperId": "a98587acde8e1df366d7c4964b5a08ed979aefb2",
        "title": "Behavioral Exploration: Learning to Explore via In-Context Adaptation"
      },
      {
        "paperId": "e4ba644f7a50bb6ea59be876ccf9c313fdd0539e",
        "title": "Your Offline Policy is Not Trustworthy: Bilevel Reinforcement Learning for Sequential Portfolio Optimization"
      },
      {
        "paperId": "eb263fdb7995cf14dc97ffb57c04e34c8f2be498",
        "title": "Robotic Paper Wrapping by Learning Force Control"
      },
      {
        "paperId": "981063181df8255e41f153907458fa657e6ba647",
        "title": "Global\u2013Local Decomposition of Contextual Representations in Meta-Reinforcement Learning"
      },
      {
        "paperId": "f9d426bb53eaac8fe017dad62566da638d5a5dc4",
        "title": "PRISM: A Robust Framework for Skill-based Meta-Reinforcement Learning with Noisy Demonstrations"
      },
      {
        "paperId": "a1d13c95feece848d1bfbb8467bb3f5ad04e9cda",
        "title": "Offline prompt reinforcement learning method based on feature extraction"
      },
      {
        "paperId": "e3a5c71b39ae964df815ae9c9ad87980ef1d483c",
        "title": "User-level Social Multimedia Traffic Anomaly Detection with Meta-Learning"
      },
      {
        "paperId": "6ec0c8668d0cd595fcc2c6a780b312c815c223a4",
        "title": "Diversification of Adaptive Policy for Effective Offline Reinforcement Learning"
      },
      {
        "paperId": "04033424011a78072f5c0060427bad406f7b1cf6",
        "title": "Efficient Offline Meta-Reinforcement Learning via Robust Task Representations and Adaptive Policy Generation"
      },
      {
        "paperId": "9455a8e8dff9533e21c4fee4d8a2686a8991692a",
        "title": "Enhancing Offline Reinforcement Learning via Dynamics-Aware Mixup"
      },
      {
        "paperId": "a4b41d74cdfc9651e754fc66b9be66d7c683bc6b",
        "title": "Effective State Space Exploration with Phase State Graph Generation and Goal-based Path Planning"
      },
      {
        "paperId": "fa4e16615abc6d25d82da0c9c867a3e0a66bdc5a",
        "title": "To Err is Robotic: Rapid Value-Based Trial-and-Error during Deployment"
      },
      {
        "paperId": "0778ce4d368b39a68fbfeb9d5f70989c89b4a02a",
        "title": "Urban-Focused Multi-Task Offline Reinforcement Learning with Contrastive Data Sharing"
      },
      {
        "paperId": "15a666fcd1557394d922b7ff2cabdbd77b914936",
        "title": "Pretraining Decision Transformers with Reward Prediction for In-Context Multi-task Structured Bandit Learning"
      },
      {
        "paperId": "b04c769c92f2bc58c5f9e6cf220480bae26405d6",
        "title": "In-Context Decision Transformer: Reinforcement Learning via Hierarchical Chain-of-Thought"
      },
      {
        "paperId": "3c79194ec98038f3af4c29d67b360bb610e1d996",
        "title": "Decision Mamba: Reinforcement Learning via Hybrid Selective Sequence Modeling"
      },
      {
        "paperId": "69af0195ca0adf428fa7575dafbc6b332d02a1f5",
        "title": "Adaptive Multi-Agent Coordination among Different Team Attribute Tasks via Contextual Meta-Reinforcement Learning"
      },
      {
        "paperId": "81e48c8050d71779a2cd81aac2953df4149e275d",
        "title": "Reinforcement Learning-Based Framework for the Intelligent Adaptation of User Interfaces"
      },
      {
        "paperId": "63eb1afe96024c84bd4ba178842e615dae872475",
        "title": "MESA: Cooperative Meta-Exploration in Multi-Agent Learning through Exploiting State-Action Space Structure"
      },
      {
        "paperId": "58809aaa27514639450aed689b1cc12a72fb588b",
        "title": "Federated reinforcement learning for robot motion planning with zero-shot generalization"
      },
      {
        "paperId": "c4972899ee166d076ab5ed8107bd1ea12de0ab9a",
        "title": "Disentangling Policy from Offline Task Representation Learning via Adversarial Data Augmentation"
      },
      {
        "paperId": "c734971c6000e3f2769ab5165d00816af80dd76f",
        "title": "In-context Exploration-Exploitation for Reinforcement Learning"
      },
      {
        "paperId": "d27da1ba65fa958e45837120fad1c25e7017d80c",
        "title": "Unsupervised Zero-Shot Reinforcement Learning via Functional Reward Encodings"
      },
      {
        "paperId": "968dc4fae222137e115d5d771ea0095002b5fc76",
        "title": "Towards an Information Theoretic Framework of Context-Based Offline Meta-Reinforcement Learning"
      },
      {
        "paperId": "feb2255dbad1c829fff48922fbf0169d1b3ee4df",
        "title": "Generalizable Task Representation Learning for Offline Meta-Reinforcement Learning with Data Limitations"
      },
      {
        "paperId": "d31def7275a87c53b8646f449ba4b467cd25efce",
        "title": "Decoupling Meta-Reinforcement Learning with Gaussian Task Contexts and Skills"
      },
      {
        "paperId": "842971bbd2668c9e4d0b3bc983383227fbfe2aa1",
        "title": "The Generalization Gap in Offline Reinforcement Learning"
      },
      {
        "paperId": "fa1e3a5c9f6e269d2a1ddbfe768a85b19ad94cf8",
        "title": "Generalization to New Sequential Decision Making Tasks with In-Context Learning"
      },
      {
        "paperId": "05603c65d813105eb136e867abfc412ea1735cf0",
        "title": "Accelerating Exploration with Unlabeled Prior Data"
      },
      {
        "paperId": "8f9c2761662a3fab59fed11f22d3b2cfe5cd7f8e",
        "title": "Context Shift Reduction for Offline Meta-Reinforcement Learning"
      },
      {
        "paperId": "e93ea52b9080d2b8f441f3223039354e05d3118a",
        "title": "Successive Model-Agnostic Meta-Learning for Few-Shot Fault Time Series Prognosis"
      },
      {
        "paperId": "736dbbb1b7aea6a126bc62e32be43018a04976f0",
        "title": "Transformers as Decision Makers: Provable In-Context Reinforcement Learning via Supervised Pretraining"
      },
      {
        "paperId": "34b4fe8415b0fb59008546612c91269d5d6a1349",
        "title": "Meta-DAMS: Delay-Aware Multipath Scheduler using Hybrid Meta Reinforcement Learning"
      },
      {
        "paperId": "8bffc877582e2943e580e49d882760c973cba8a4",
        "title": "Digital Twin and Meta RL Empowered Fast-Adaptation of Joint User Scheduling and Task Offloading for Mobile Industrial IoT"
      },
      {
        "paperId": "8432b4b8a33ab65f6e596d9b3168e9dd8b385f28",
        "title": "Diagnosing and exploiting the computational demands of videos games for deep reinforcement learning"
      },
      {
        "paperId": "3091f9561341bfa4bd2544e6c625c2dfd19520cf",
        "title": "Bootstrapping Adaptive Human-Machine Interfaces with Offline Reinforcement Learning"
      },
      {
        "paperId": "57eaab020ecbfab78c94d5201bee1517e7a60d2a",
        "title": "Efficient Unified Demosaicing for Bayer and Non-Bayer Patterned Image Sensors"
      },
      {
        "paperId": "0ba721d29e93f51235f4306e6f06c0762a10c90d",
        "title": "Robotic Manipulation Datasets for Offline Compositional Reinforcement Learning"
      },
      {
        "paperId": "5bac7d00035bc1e246a34f9ee3152b290f97bb92",
        "title": "Supervised Pretraining Can Learn In-Context Reinforcement Learning"
      },
      {
        "paperId": "a0d87da6f822789da3b7c2c832594d1a542848ee",
        "title": "Uncertainty-Aware Data Augmentation for Offline Reinforcement Learning"
      },
      {
        "paperId": "146748b26209afac66a3c0a52fea8b72065b2b2c",
        "title": "Decoupled Prioritized Resampling for Offline RL"
      },
      {
        "paperId": "7553c10f5b6dec33e0562430659adfabf9dff536",
        "title": "Offline Meta Reinforcement Learning with In-Distribution Online Adaptation"
      },
      {
        "paperId": "07f72693aff855ca920dd303ae2e49b057087d5f",
        "title": "MRLCC: an adaptive cloud task scheduling method based on meta reinforcement learning"
      },
      {
        "paperId": "32528ba99cfa9a5d6d7b190f41a90b8f6125270e",
        "title": "Learning Excavation of Rigid Objects with Offline Reinforcement Learning"
      },
      {
        "paperId": "9e13ffe77bc1ae88dc9e89ddf6fdaa12f15c165f",
        "title": "Data-Driven Robotic Manipulation of Cloth-like Deformable Objects: The Present, Challenges and Future Prospects"
      },
      {
        "paperId": "936551c6026df296e27a649a34f379357fb57a74",
        "title": "Mnemosyne: Learning to Train Transformers with Transformers"
      },
      {
        "paperId": "e89d9a5c97f69c7c6f06440ed0bbbc56152042e5",
        "title": "Train Hard, Fight Easy: Robust Meta Reinforcement Learning"
      },
      {
        "paperId": "f18015a05a834fc0068ad38b7bfdd6d2fbd8cb6b",
        "title": "Reinforcement learning with Demonstrations from Mismatched Task under Sparse Reward"
      },
      {
        "paperId": "72abe936b6d3603c69dbda2c36eccf78c04923b9",
        "title": "Multi-Environment Pretraining Enables Transfer to Action Limited Datasets"
      },
      {
        "paperId": "0fd1a48877f62d8fa5e4c9adc8189e84b507a349",
        "title": "Contextual Transformer for Offline Meta Reinforcement Learning"
      },
      {
        "paperId": "ec743f83514e0c4845226684945b57dc6eeeccf1",
        "title": "Towards Data-Driven Offline Simulations for Online Reinforcement Learning"
      },
      {
        "paperId": "33777d1de46f275aa0911954abc483c249eda9d1",
        "title": "Learning on the Job: Self-Rewarding Offline-to-Online Finetuning for Industrial Insertion of Novel Connectors from Vision"
      },
      {
        "paperId": "860bc4f071f35d6d8529a52c2c1858d030779a6a",
        "title": "In-context Reinforcement Learning with Algorithm Distillation"
      },
      {
        "paperId": "05c4bfd0b16fd0ab0e72e0866d0a5bfec5ad7ded",
        "title": "Pre-Training for Robots: Offline RL Enables Learning New Tasks in a Handful of Trials"
      },
      {
        "paperId": "0ac948bc087603176b47c4af29ef7f240d27b541",
        "title": "Don't Start From Scratch: Leveraging Prior Data to Automate Robotic Reinforcement Learning"
      },
      {
        "paperId": "a5f22a3377830e8040287726b0bf5656d26efdb3",
        "title": "Robust Task Representations for Offline Meta-Reinforcement Learning via Contrastive Learning"
      },
      {
        "paperId": "41479416d858bacfaa01bfc068ceb86466d542e9",
        "title": "User-Interactive Offline Reinforcement Learning"
      },
      {
        "paperId": "23bac2542b145bf2fcd17d7fa0a02ae03d0a45f7",
        "title": "Skill-based Meta-Reinforcement Learning"
      },
      {
        "paperId": "387a17823d7c47c0bd3390a124708933032989e0",
        "title": "Generalized Decision Transformer for Offline Hindsight Information Matching"
      },
      {
        "paperId": "357de3483ba31c57484f481ae312cf0f822268dc",
        "title": "Offline Meta-Reinforcement Learning for Industrial Insertion"
      },
      {
        "paperId": "7fb342bac6a3e2ec48abfa6215987321117e226a",
        "title": "Test-Time Model Adaptation for Visual Question Answering With Debiased Self-Supervisions"
      },
      {
        "paperId": "7bc194b28a4f640dd26ecdd3fe33414f334d65b8",
        "title": "Pre-Trained Multi-Goal Transformers with Prompt Optimization for Efficient Online Adaptation"
      },
      {
        "paperId": "c0dab9e704f83c58187be8f68c10152fdcb35f14",
        "title": "Avoiding Undesired Future with Minimal Cost in Non-Stationary Environments"
      },
      {
        "paperId": "2d39ce0421b85f0b5525ac0f282ea2a6599553f3",
        "title": "P RE -T RAINING FOR R OBOTS : L EVERAGING D IVERSE M ULTITASK D ATA VIA O FFLINE RL"
      },
      {
        "paperId": "32b0dbf9e13fe75b181be910c5e31b47941327ae",
        "title": "Pre-Training for Robots: Offline RL Enables Learning New Tasks from a Handful of Trials"
      },
      {
        "paperId": "42da79f3516623ab00416e79ac0e23ebda544538",
        "title": "A Study of Generalization in Offline Reinforcement Learning"
      },
      {
        "paperId": "c44a35764c5e84387edb2d3112666633b841fd52",
        "title": "P RETRAINING E NABLES T RANSFER TO A CTION L IMITED D ATASETS"
      }
    ],
    "score": 17.5
  },
  {
    "id": "1d6beec78e720beab5adb0a5fce6fa6b846aed1a",
    "title": "Local and global stimuli in reinforcement learning",
    "authors": [
      "Danyang Jia",
      "Hao Guo",
      "Z. Song",
      "Lei Shi",
      "Xinyang Deng",
      "M. Perc",
      "Zhen Wang"
    ],
    "year": 2021,
    "citationCount": 70,
    "abstract": "In efforts to resolve social dilemmas, reinforcement learning is an alternative to imitation and exploration in evolutionary game theory. While imitation and exploration rely on the performance of neighbors, in reinforcement learning individuals alter their strategies based on their own performance in the past. For example, according to the Bush\u2013Mosteller model of reinforcement learning, an individual\u2019s strategy choice is driven by whether the received payoff satisfies a preset aspiration or not. Stimuli also play a key role in reinforcement learning in that they can determine whether a strategy should be kept or not. Here we use the Monte Carlo method to study pattern formation and phase transitions towards cooperation in social dilemmas that are driven by reinforcement learning. We distinguish local and global players according to the source of the stimulus they experience. While global players receive their stimuli from the whole neighborhood, local players focus solely on individual performance. We show that global players play a decisive role in ensuring cooperation, while local players fail in this regard, although both types of players show properties of \u2018moody cooperators\u2019. In particular, global players evoke stronger conditional cooperation in their neighborhoods based on direct reciprocity, which is rooted in the emerging spatial patterns and stronger interfaces around cooperative clusters.",
    "url": "https://www.semanticscholar.org/paper/1d6beec78e720beab5adb0a5fce6fa6b846aed1a",
    "pdf_url": "https://doi.org/10.1088/1367-2630/ac170a",
    "venue": "New Journal of Physics",
    "publicationDate": "",
    "externalIds": {
      "DOI": "10.1088/1367-2630/ac170a",
      "CorpusId": 236985854
    },
    "references": [
      {
        "paperId": "4768a7aa8b0c27036306f8a27884c1486f5566cd",
        "title": "Emotion expressions shape human social norms and reputations"
      },
      {
        "paperId": "61930b6e06b5ac5ebfb94a47a7b03cb9cccac88b",
        "title": "Dynamics of heuristics selection for cooperative behaviour"
      },
      {
        "paperId": "f2730aabf3b4e7daac81c4593ee7f6b950f2ac8c",
        "title": "When to (or not to) trust intelligent machines: Insights from an evolutionary game theory analysis of trust in repeated games"
      },
      {
        "paperId": "c5a2d8c976074bca2cbf15cde4fb1c65908bbd4b",
        "title": "Evolutionary dynamics drives role specialization in a community of players"
      },
      {
        "paperId": "86fe7bc979256f236fa2e086e360a7807fdda051",
        "title": "Behavioural patterns behind the demise of the commons across different cultures"
      },
      {
        "paperId": "300311188b65041aa649de594ea325b47c5df4f6",
        "title": "Effect of memory, intolerance, and second-order reputation on cooperation."
      },
      {
        "paperId": "229a4da823b71161a5ea78c9890d2d2e1be400e4",
        "title": "Strategy equilibrium in dilemma games with off-diagonal payoff perturbations."
      },
      {
        "paperId": "73e833325a0e584b26d3898fe5d90b7a93b75378",
        "title": "Heterogeneity in evolutionary games: an analysis of the risk perception"
      },
      {
        "paperId": "044ef37d375b2465cd2cc74a5f34ba7bff3dd1e5",
        "title": "A novel route to cyclic dominance in voluntary social dilemmas"
      },
      {
        "paperId": "6395e2d1fbc8d3764046081eead6a5c2267289aa",
        "title": "Evolutionary dynamics of higher-order interactions in social networks"
      },
      {
        "paperId": "29d2809dc67353f19d0f4a31d46c2f24a4af0d65",
        "title": "Sentiment contagion dilutes prisoner's dilemmas on social networks"
      },
      {
        "paperId": "16a7381faad842f2b37e4d6a237f1b1d30367d90",
        "title": "Assortativity provides a narrow margin for enhanced cooperation on multilayer networks"
      },
      {
        "paperId": "af0ab93c77a81aae1ce8c78789caa6e21af2181e",
        "title": "Replicator population dynamics of group interactions: Broken symmetry, thresholds for metastability, and macroscopic behavior."
      },
      {
        "paperId": "57a32ac5e2bb7b6a118ceecc225d755f96b8a77b",
        "title": "Exploiting a cognitive bias promotes cooperation in social dilemma experiments"
      },
      {
        "paperId": "06fd05dfd130c921e1039a5dc1edd3f5d5534bbb",
        "title": "Partners and rivals in direct reciprocity"
      },
      {
        "paperId": "fa0b921894dec18150bac23d5fea228a8760e1d0",
        "title": "Social norm complexity and past reputations in the evolution of cooperation"
      },
      {
        "paperId": "b790285740e9f56040de820b048b38dd121cc9b5",
        "title": "Social Manifestation of Guilt Leads to Stable Cooperation in Multi-Agent Systems"
      },
      {
        "paperId": "f106c1e7aa78f37db3b7a5ea3130a878e2d6a65e",
        "title": "Reinforcement learning accounts for moody conditional cooperation behavior: experimental results"
      },
      {
        "paperId": "4075631571071ab6cfed8a73a9f18ee83ecb2db9",
        "title": "Stochastic win-stay-lose-shift strategy with dynamic aspirations in evolutionary social dilemmas."
      },
      {
        "paperId": "67cf43e6cd1432a0916660f861a68c9b77783e7d",
        "title": "Self-adaptive win-stay-lose-shift reference selection mechanism promotes cooperation on a square lattice"
      },
      {
        "paperId": "2c9daaa33176520d2744796918faf6eb1c72177c",
        "title": "Reinforcement Learning Explains Conditional Cooperation and Its Moody Cousin"
      },
      {
        "paperId": "450378592dfb3d12a3404954ebe508d2d0deaa62",
        "title": "Statistical physics of the spatial Prisoner\u2019s Dilemma with memory-aware agents"
      },
      {
        "paperId": "0983aa4a72d059ffcd761bab5862948447121144",
        "title": "Social exclusion in finite populations."
      },
      {
        "paperId": "fa8baa511c959227dbb90519ee3eb61d7c52312a",
        "title": "Synergy between intention recognition and commitments in cooperation dilemmas"
      },
      {
        "paperId": "a1fe94240551af9deb60cecba94a6858328244d9",
        "title": "Transition from reciprocal cooperation to persistent behaviour in social dilemmas at the end of adolescence"
      },
      {
        "paperId": "6961b6472ec55d65c518c1e3177f7cc7f72138f2",
        "title": "Theory of Mind: Did Evolution Fool Us?"
      },
      {
        "paperId": "fc290b5d7580bdae06c54e932357b7074901b1ce",
        "title": "Corpus-Based Intention Recognition in Cooperation Dilemmas"
      },
      {
        "paperId": "1b6797ae037c5b53d4b0cccd688d153c4b0ef701",
        "title": "Evolutionary advantages of adaptive rewarding"
      },
      {
        "paperId": "9b5942ad789e62feb0bc9e7281afc0df915332d7",
        "title": "Evolution of in-group favoritism"
      },
      {
        "paperId": "b50cb2b35281fcb0ca717069f65e3e1c49e3dcd3",
        "title": "Cooperation and the evolution of intelligence"
      },
      {
        "paperId": "d7575b85f8c041bbd39c96ea7803dd1f9e872a0b",
        "title": "The evolution of antisocial punishment in optional public goods games."
      },
      {
        "paperId": "72669f1964fa2ba08881c3d180309f0dd2d5cb99",
        "title": "Intention recognition promotes the emergence of cooperation"
      },
      {
        "paperId": "1dd1baa60f21ba8b1842d925c30c3806cb9e0192",
        "title": "Phase Diagrams for the Spatial Public Goods Game with Pool-Punishment"
      },
      {
        "paperId": "b890f67a1406d70642afb3499a32a801bd85ca24",
        "title": "Emergent hierarchical structures in multiadaptive games."
      },
      {
        "paperId": "e53c7b81a1a1957a5416486a29c82dcf437b6265",
        "title": "Numerical analysis of a reinforcement learning model with the dynamic aspiration level in the iterated Prisoner's dilemma."
      },
      {
        "paperId": "d08fa114f21c7f0f3fc85d7009e93b7e81bf69d1",
        "title": "Phase diagrams for three-strategy evolutionary prisoner's dilemma games on regular graphs."
      },
      {
        "paperId": "6a75894d7af7f5113b9b76ca7435a9da6cf29079",
        "title": "Reinforcement Learning Dynamics in Social Dilemmas"
      },
      {
        "paperId": "75a737fa3aceef975be1fff478fc3805eb249255",
        "title": "Transient and asymptotic dynamics of reinforcement learning in games"
      },
      {
        "paperId": "3fed759edb9f326f7c9c3ffb009dd287ab4c8103",
        "title": "Social semantics: altruism, cooperation, mutualism, strong reciprocity and group selection"
      },
      {
        "paperId": "d2232e96f8648fe2298f90391c8663fbfe0e5c84",
        "title": "The evolution of cooperation and altruism \u2013 a general framework and a classification of models"
      },
      {
        "paperId": "981070ec4cd4584a46cfc05116dd6d906d6af622",
        "title": "Evolutionary games on graphs"
      },
      {
        "paperId": "ee557d8918504b3098de11e696b9b5c484702ae1",
        "title": "A simple rule for the evolution of cooperation on graphs and social networks"
      },
      {
        "paperId": "82f5e2c905b0e1e555dd896ebcd5602aa5cb2cee",
        "title": "Evolutionary cycles of cooperation and defection."
      },
      {
        "paperId": "2feedd23395f882e40dc46f864f2a536c0fe0234",
        "title": "Evolutionary Dynamics of Biological Games"
      },
      {
        "paperId": "f1967d2d7187f6b5a10c2b09a37ec31ead4496ba",
        "title": "Learning dynamics in social dilemmas"
      },
      {
        "paperId": "bd04f2eb5184c58a711cad5ab1694f93689d2621",
        "title": "Which One Should I Imitate"
      },
      {
        "paperId": "5aaedf6663f2ade415cb0c6fbd0a7e65837f9e4a",
        "title": "Spatial games and the maintenance of cooperation."
      },
      {
        "paperId": "f892cfaafc9f6d404b10039f04a363ae34130ad8",
        "title": "Learning to cooperate with Pavlov an adaptive strategy for the iterated Prisoner's Dilemma with noise"
      },
      {
        "paperId": "f472362ec34cec502a368e62f0a27f321062f43c",
        "title": "A strategy of win-stay, lose-shift that outperforms tit-for-tat in the Prisoner's Dilemma game"
      },
      {
        "paperId": "a690eb4c7ab6c2733478fa9ac3379fa2cdea7271",
        "title": "Evolutionary games and spatial chaos"
      },
      {
        "paperId": "9eeda763c7935895884ff29d729f700e18a1580b",
        "title": "Learning to Cooperate: Stochastic and Tacit Collusion in Social Exchange"
      },
      {
        "paperId": "be12970bee7fd5a7f2da0866e8f42122d010c87e",
        "title": "The Calculus of Selfishness"
      },
      {
        "paperId": "8851953ef486615fce803bda2e40aec97cbb5547",
        "title": "Multi-agent Reinforcement Learning: An Overview"
      },
      {
        "paperId": "cd3860777a944f291e9323cb6da091d733696114",
        "title": "Social semantics : altruism , cooperation , mutualism , strong reciprocity and group selection"
      },
      {
        "paperId": "2819bda4902ad41c5f3b60737632c25a3d75dcd2",
        "title": "Why Imitate, and If So, How?, : A Boundedly Rational Approach to Multi-armed Bandits"
      },
      {
        "paperId": "6cdab800a01bea607bcce8f0e5568d09b196279a",
        "title": "Finite automata play repeated prisoner's dilemma with information processing costs"
      },
      {
        "paperId": "a8d4b71b6e5c58ae1c8195c96e4cbebf5af4a502",
        "title": "Tit for tat in heterogeneous populations"
      },
      {
        "paperId": null,
        "title": "Monte Carlo Simulations in Statistical Physics (Berlin: Springer"
      },
      {
        "paperId": "da390f82cd1c0f50c99a759f22436f6b9abb9b35",
        "title": "TIT FOR TAT in sticklebacks and the evolution of cooperation"
      },
      {
        "paperId": null,
        "title": "The Evolution of Cooperation (New York: Basic Books"
      },
      {
        "paperId": null,
        "title": "Stochastic Models for Learning (New York: Wiley"
      }
    ],
    "cited_by": [
      {
        "paperId": "d717038bf1a6157f19bdf5d3f656738f6a72ee87",
        "title": "Dynamic strategy update mechanisms facilitate cooperation"
      },
      {
        "paperId": "fd9bf0f55f68134950abbc357d38f50fce568bc1",
        "title": "Emotion-coupled Q-learning with cognitive bias enhances cooperation in evolutionary prisoner\u2019s dilemma games"
      },
      {
        "paperId": "7723c7c1659fe408eb04a46e9ddedb852c7a6857",
        "title": "Traffic accident severity prediction and analysis via spatio-temporal deep learning: A ConvLSTM-transformer approach"
      },
      {
        "paperId": "b137e697b4272cdae10286a21a10fe3f7a2ec2ff",
        "title": "Bio-inspired mechanism promotes cooperation in spatial public goods games"
      },
      {
        "paperId": "cd63f674c3069719cc55a60ff23c2ce3d6da5ac0",
        "title": "Evolution of cooperation in spatial public goods games driven by reinforcement learning and environmental feedback"
      },
      {
        "paperId": "33ff439e14ebdb86941b5c98f18914f9835ef352",
        "title": "Effect of adaptive migration with interaction intensity on the evolution of cooperation"
      },
      {
        "paperId": "accd1baeadb79a6b524f733d329eb5ceaa2f580b",
        "title": "Q-Learning-Driven Adaptive Rewiring for Cooperative Control in Heterogeneous Networks"
      },
      {
        "paperId": "62c0eb658f453f8168582d5b03e9a1ec099110d1",
        "title": "Tax credit promote cooperation in a punishment-based spatial public goods game"
      },
      {
        "paperId": "18849ddfc8667528d59f9fd78f1bead76581e790",
        "title": "TUC-PPO: Team Utility-Constrained Proximal Policy Optimization for Spatial Public Goods Games"
      },
      {
        "paperId": "a79ceae3360feb24dacd11bc204c80ebd299f0dd",
        "title": "PPO-ACT: Proximal Policy Optimization with Adversarial Curriculum Transfer for Spatial Public Goods Games"
      },
      {
        "paperId": "b3147a2a9e93e33c5798a8e91da807f2967fd1b3",
        "title": "On evolution of agent behavior under limited gaming time with reinforcement learning"
      },
      {
        "paperId": "a1ba93699631838cf47f3cceaea40b6de838f8bf",
        "title": "The coevolution of cooperation: Integrating Q-learning and occasional social interactions in evolutionary games"
      },
      {
        "paperId": "03a1ff32c4f1c09bb90fe5c23a5891857859e94b",
        "title": "Promoting cooperation in the voluntary prisoner's dilemma game via reinforcement learning."
      },
      {
        "paperId": "a82b10d483b7aa9d4ca55bae2424595eeb8c6d91",
        "title": "Enhancing cooperation in dynamic networks through reinforcement-learning-based rewiring strategies"
      },
      {
        "paperId": "23ac813e8493c14ea26cad47715182d7b52ccf2c",
        "title": "Analyzing the channels of information dissemination: Investigating abrupt transitions in resource investment."
      },
      {
        "paperId": "9acfb258e27867391654d45fa0797d7cd9f562ba",
        "title": "Cooperation resonance based on link strategy reinforcement learning and conformity."
      },
      {
        "paperId": "1a8ce05c9babf9d1b0ad220177291eaf3a8a4923",
        "title": "Distributed UAV swarms for 3D urban area coverage with incomplete information using event-triggered hierarchical reinforcement learning"
      },
      {
        "paperId": "8490bc2f582ea94250d801077a4f12447cdbb851",
        "title": "Hypergraph-Based Model for Modeling Multi-Agent Q-Learning Dynamics in Public Goods Games"
      },
      {
        "paperId": "8902b88698d62e5ff390aa9b532ad649d9434baf",
        "title": "Cooperation in public goods games: Leveraging other-regarding reinforcement learning on hypergraphs."
      },
      {
        "paperId": "ac8085bf705ed1af7a376fbf6e25fabebe679d92",
        "title": "Learning and propagation: Evolutionary dynamics in spatial public goods games through combined Q-learning and Fermi rule"
      },
      {
        "paperId": "515af2b47e2b4352eed466e28c6fe3ed5652650e",
        "title": "Cooperative behavior in multi-agent systems with intrinsic learners and extrinsic imitators"
      },
      {
        "paperId": "defc382578a3f316614521031f2ea8bf64c8fcae",
        "title": "A double-edged sword: diverse interactions in hypergraphs"
      },
      {
        "paperId": "4eb3954963bb64d742dc7d0ac2b9a1fe2fd1ef37",
        "title": "Incorporating reputation into reinforcement learning can promote cooperation on hypergraphs"
      },
      {
        "paperId": "fa2017ac444ab8edffcc1d790810cb9f478c0d57",
        "title": "Reputation-Based Interaction Promotes Cooperation With Reinforcement Learning"
      },
      {
        "paperId": "3090f825a505071f0418f6aacecda89a78797731",
        "title": "Evolution of cooperation with Q-learning: The impact of information perception."
      },
      {
        "paperId": "89b5d79eac0782411e47639d44c1bae29b5a65e1",
        "title": "Intersecting reinforcement learning and deep factor methods for optimizing locality and globality in forecasting: A review"
      },
      {
        "paperId": "59c5955049bb843fa32245174ab78678d3ecb0db",
        "title": "Fast Pareto set approximation for multi-objective flexible job shop scheduling via parallel preference-conditioned graph reinforcement learning"
      },
      {
        "paperId": "59c4931f084193404c8514b428d9a9ef4aa4d6eb",
        "title": "Catalytic evolution of cooperation in a population with behavioral bimodality."
      },
      {
        "paperId": "85f5f3b46482e0b7536fb3e4456ddcf1a264023f",
        "title": "Evolutionary game theory combined with reinforcement learning synthesis - A comprehensive survey"
      },
      {
        "paperId": "2eb8c68607d180ac28f42a3d7454256e26e70750",
        "title": "Evolution of cooperation on reinforcement-learning driven-adaptive networks."
      },
      {
        "paperId": "5cf6ae8113ace2430eb600a68c3367516d47f71f",
        "title": "Causal Reinforcement Learning in Iterated Prisoner\u2019s Dilemma"
      },
      {
        "paperId": "3864d526f2855bb055130180a1881b3e473f35eb",
        "title": "\u2018I don\u2019t want to play with you anymore\u2019: dynamic partner judgements in moody reinforcement learners playing the prisoner\u2019s dilemma"
      },
      {
        "paperId": "5a533ed8635f3005c05d56bb0c01e67c3b4b715d",
        "title": "Preferential selection based on adaptive attractiveness induce by reinforcement learning promotes cooperation"
      },
      {
        "paperId": "9e0ca3307efd99d2b347540b5b01aa5a3b03cb50",
        "title": "Mixed strategy approach destabilizes cooperation in finite populations with clustering coefficient."
      },
      {
        "paperId": "bb99f015ad0ec9baf101ea698d63052ec8e431b4",
        "title": "Emergence of anti-coordinated patterns in snowdrift game by reinforcement learning"
      },
      {
        "paperId": "a494008c9d3c63677e258982e8ccb1dffc157e5f",
        "title": "Exploring cooperative evolution with tunable payoff\u2019s loners using reinforcement learning"
      },
      {
        "paperId": "e05b1fdf1e369519829e91f2f9e429dca46d826f",
        "title": "Optimal coordination of resources: A solution from reinforcement learning"
      },
      {
        "paperId": "1a3b3eb2bcfe7b0b3bc0191de165fb32e0238616",
        "title": "Multi-agent deep reinforcement learning based real-time planning approach for responsive customized bus routes"
      },
      {
        "paperId": "44587386211d79183d32442c7035848b04a91f54",
        "title": "Cooperative behavior under the influence of multiple experienced guiders in Prisoner's dilemma game"
      },
      {
        "paperId": "841a0669567c64fadd7aa0b7923cd281fb64a7e0",
        "title": "A Simulation Modeling Study of Cooperation Evolution Based on Subjective Learning and Influence Preference"
      },
      {
        "paperId": "3773aade17eae498417aca1ecddd945f38e25b18",
        "title": "A Q-learning based multi-strategy integrated artificial bee colony algorithm with application in unmanned vehicle path planning"
      },
      {
        "paperId": "9aa7f186a964d362b486a6d95f7543598e15afc2",
        "title": "Emergence of cooperation in two-agent repeated games with reinforcement learning"
      },
      {
        "paperId": "deaebbbfe8132586435d0873ad8fac3db2b7495f",
        "title": "Reinforcement learning relieves the vaccination dilemma."
      },
      {
        "paperId": "7b58f9932bb553bf7a3baae41503ed25a0f42f97",
        "title": "Multi-strategy multi-objective differential evolutionary algorithm with reinforcement learning"
      },
      {
        "paperId": "1ea5d24d00ea257b4f9e2ba3c164c564738713c3",
        "title": "Coevolution of cognition and cooperation in structured populations under reinforcement learning"
      },
      {
        "paperId": "a5540875201f023dc82af25a3b6f593a472c58e1",
        "title": "Extending Q-learning to continuous and mixed strategy games based on spatial reciprocity"
      },
      {
        "paperId": "a5b2bd2728deb02bf74bd603fab687d0fb66d593",
        "title": "Synergistic effects of adaptive reward and reinforcement learning rules on cooperation"
      },
      {
        "paperId": "6a47f66671073fc30c65dbf61073fb4a0cc36518",
        "title": "Protection and improvement of indirect identity cognition on the spatial evolution of cooperation"
      },
      {
        "paperId": "55449afcf953eb2af22fbbcd401f41c03821e545",
        "title": "Network adaption based on environment feedback promotes cooperation in co-evolutionary games"
      },
      {
        "paperId": "c0d9615a85083267a7835de7c1cfa0ad73c48842",
        "title": "A reinforcement learning-based strategy updating model for the cooperative evolution"
      },
      {
        "paperId": "87a7add6c993802694efa47509ff314383b16417",
        "title": "Q-learning-based migration leading to spontaneous emergence of segregation"
      },
      {
        "paperId": "bf42aeb8d8dcd16f47c21d42e55926f949cf639f",
        "title": "Incorporating social payoff into reinforcement learning promotes cooperation."
      },
      {
        "paperId": "095e379192378bf0c4f3a1e408683b8fe6ed5fd3",
        "title": "Intrinsic fluctuations of reinforcement learning promote cooperation"
      },
      {
        "paperId": "a2171a8a5d38e4ded44f769d80baa5edf99aef3c",
        "title": "Reinforcement learning facilitates an optimal interaction intensity for cooperation"
      },
      {
        "paperId": "fdf4821ad69d5fd11bd32b9f9b969b7673eb6f6d",
        "title": "The influence of experienced guider on cooperative behavior in the Prisoner's dilemma game"
      },
      {
        "paperId": "c5ec79e7a491fdc20eca1ed713ae8b6f9f06e9d4",
        "title": "Convergence analysis of distributed population dynamics based on second-order Delaunay triangulation"
      },
      {
        "paperId": "1e6669289d4f87001d7ce0dbe1caf0b84afba1f4",
        "title": "A novel belief \u03c72 ${\\chi }^{2}$ divergence for multisource information fusion and its application in pattern classification"
      },
      {
        "paperId": "2533e788fa9cb0589db93cf6ac53ea65cd9f3140",
        "title": "Multi-player snowdrift game on scale-free simplicial complexes"
      },
      {
        "paperId": "e87097e0ddacb675e3b374a7bc3cf7e55930f5db",
        "title": "Coupling group selection and network reciprocity in social dilemmas through multilayer networks"
      },
      {
        "paperId": "9a1ed83514ddc1759229ece77b043dbb973e2ffa",
        "title": "L\u00e9vy noise promotes cooperation in the prisoner\u2019s dilemma game with reinforcement learning"
      },
      {
        "paperId": "20c13fdb81ca3913ffc725cdd68a58e2ec3f7de2",
        "title": "Impact of resource-based conditional interaction on cooperation in spatial social dilemmas"
      },
      {
        "paperId": "c66c2b39c6f2f87aae4963db96de0979fa20e092",
        "title": "Empty nodes affect conditional cooperation under reinforcement learning"
      },
      {
        "paperId": "affa7dc507cf5d4f70ef2d49c423fa2e2112e961",
        "title": "Stimuli strategy and learning dynamics promote the wisdom of crowds"
      },
      {
        "paperId": "d299bd8966efab7613d6724a30e7ce0cff4b56b2",
        "title": "Collective behavior decision based on edge dynamics"
      },
      {
        "paperId": "347630b29badb2901e5bf0ddc025961ce0da2581",
        "title": "The guidance of neutral human populations maintains cooperation in the prisoner's dilemma game"
      },
      {
        "paperId": "e35b8a652d1b64b8723d2be49b6747f667734291",
        "title": "Q-learning driven cooperative evolution with dual-reputation incentive mechanisms"
      },
      {
        "paperId": "815ea01fa72eaf0c0d3eee7c8cf3999f0dce7b3f",
        "title": "The evolution of cooperation and global synchronization in the evolutionary Kuramoto dilemma combined with the prisoner's dilemma"
      },
      {
        "paperId": "4df0283151335bb24caa63f845128fd69a259c3b",
        "title": "Forecasting Floods Using Deep Learning Models: A Longitudinal Case Study of Chenab River, Pakistan"
      },
      {
        "paperId": "1f3dd557da854d7aba5195a5ebeafa3ed0b24907",
        "title": "Cooperation dynamics on hypergraphs with punishment and Q-learning"
      },
      {
        "paperId": "e6832f81c0689152199c48d8665f4211bfab1891",
        "title": "Event-triggered prescribed performance control of the multiplayer game nonlinear system via integral reinforcement learning"
      }
    ],
    "score": 17.5
  },
  {
    "id": "ba44a95f1a8bc5765438d03c01137799e930c88d",
    "title": "Barrier Lyapunov Function-Based Safe Reinforcement Learning for Autonomous Vehicles With Optimized Backstepping",
    "authors": [
      "Yuxiang Zhang",
      "Xiaoling Liang",
      "Dongyu Li",
      "S. Ge",
      "B. Gao",
      "Hong Chen",
      "Tong-heng Lee"
    ],
    "year": 2022,
    "citationCount": 51,
    "abstract": "Guaranteed safety and performance under various circumstances remain technically critical and practically challenging for the wide deployment of autonomous vehicles. Safety-critical systems in general, require safe performance even during the reinforcement learning (RL) period. To address this issue, a Barrier Lyapunov Function-based safe RL (BLF-SRL) algorithm is proposed here for the formulated nonlinear system in strict-feedback form. This approach appropriately arranges and incorporates the BLF items into the optimized backstepping control method to constrain the state-variables in the designed safety region during learning. Wherein, thus, the optimal virtual/actual control in every backstepping subsystem is decomposed with BLF items and also with an adaptive uncertain item to be learned, which achieves safe exploration during the learning process. Then, the principle of Bellman optimality of continuous-time Hamilton\u2013Jacobi\u2013Bellman equation in every backstepping subsystem is satisfied with independently approximated actor and critic under the framework of actor-critic through the designed iterative updating. Eventually, the overall system control is optimized with the proposed BLF-SRL method. It is furthermore noteworthy that the variance of the attained control performance under uncertainty is also reduced with the proposed method. The effectiveness of the proposed method is verified with two motion control problems for autonomous vehicles through appropriate comparison simulations.",
    "url": "https://www.semanticscholar.org/paper/ba44a95f1a8bc5765438d03c01137799e930c88d",
    "pdf_url": "https://doi.org/10.1109/TNNLS.2022.3186528",
    "venue": "IEEE Transactions on Neural Networks and Learning Systems",
    "publicationDate": "2022-07-12",
    "externalIds": {
      "DBLP": "journals/tnn/ZhangLLGGCL24",
      "DOI": "10.1109/TNNLS.2022.3186528",
      "CorpusId": 250454072,
      "PubMed": "35820012"
    },
    "references": [],
    "cited_by": [
      {
        "paperId": "d509900a84aa92e107c766918adeba5ef1c5bc07",
        "title": "SAC-Loco: Safe and Adjustable Compliant Quadrupedal Locomotion"
      },
      {
        "paperId": "ed504ae1e9fc8767e7c8b25772ebd983c8345dc9",
        "title": "A Review On Safe Reinforcement Learning Using Lyapunov and Barrier Functions"
      },
      {
        "paperId": "5940608e096a166b3c628f5fcdee414063d7c6d5",
        "title": "Fixed-time optimal tracking control for nonlinear pure-feedback multi-agent systems with multiple constraints"
      },
      {
        "paperId": "4fa2cdc05729a524d7673b4371abc36d25a78ce2",
        "title": "Prescribed performance tracking control of autonomous surface vehicles with reinforcement learning"
      },
      {
        "paperId": "e4e6b87556d49569a2a630f26e6d616dd9926ea3",
        "title": "Resilient Safe Optimized Backstepping Control for High-Order Strict-Feedback System"
      },
      {
        "paperId": "ae7a54eed5b97b0c034c5f34f372de057052ffee",
        "title": "Parallel Multistep Evaluation With Efficient Data Utilization for Safe Neural Critic Control and Its Application to Orbital Maneuver Systems"
      },
      {
        "paperId": "b90758aaa86e2620d7e5abb4bd0e128a4aaace91",
        "title": "Event-triggered optimal fault-tolerant control for a class of uncertain strict-feedback nonlinear systems"
      },
      {
        "paperId": "6142ec21ab7a2a714d5df4ab79ca255c91c860cc",
        "title": "Human\u2013Machine Shared Stabilization Control Based on Safe Adaptive Dynamic Programming With Bounded Rationality"
      },
      {
        "paperId": "583437e42fc6563101b3f02e2764d7a28b52b816",
        "title": "Neuro\u2010Observer Based Adaptive Fixed\u2010Time Fault\u2010Tolerant Control for Uncertain Teleoperation System With Input Saturation and Asymmetric Time\u2010Varying Output Constraints"
      },
      {
        "paperId": "f620e2315b5d8ce3cfc7e8271c4449a811a947b2",
        "title": "Hybrid safe reinforcement learning: Tackling distribution shift and outliers with the Student-t\u2019s process"
      },
      {
        "paperId": "3f4b6e6f9de8abb370f56de20fe679f3fd859353",
        "title": "Finite-time neuro-optimal control for constrained nonlinear systems through reinforcement learning"
      },
      {
        "paperId": "ce8fd5f05ebe5bc0ca325c8c8a7b5133a6834eb5",
        "title": "AI Safety for Everyone"
      },
      {
        "paperId": "30b1261daa4efab03f23a96d196d78c2ca392fdb",
        "title": "Embedded Model Predictive Control for Torque Distribution Optimization of Electric Vehicles"
      },
      {
        "paperId": "21463b8f055613617af8ea053c8c837499343c1b",
        "title": "Lateral Control of Autonomous Vehicles Using Barrier Lyapunov Function"
      },
      {
        "paperId": "597b57536e943466f60ae01028bd2031e5e4b982",
        "title": "Safety-Critical Control of 4WDEV Trajectory Tracking via Adaptive Control Barrier Function"
      },
      {
        "paperId": "bc018a88259ecce447875757d96fbb4bdc36394e",
        "title": "Bi-Layered Synchronized Optimization Control With Prescribed Performance for Vehicle Platoon"
      },
      {
        "paperId": "a38fef5edb8c0b1d79139f4052630a5af27faf1e",
        "title": "Cooperative Perception With Localization Uncertainty: A Cubature Split Covariance Intersection Framework"
      },
      {
        "paperId": "bec3fffeab99d6058412605b6f6bd2fb8cf5eae8",
        "title": "Historical Decision-Making Regularized Maximum Entropy Reinforcement Learning"
      },
      {
        "paperId": "0d53309488d26260ec39047829accdc1c3b2d648",
        "title": "Reinforcement Learning-Based Time-Synchronized Optimized Control for Affine Systems"
      },
      {
        "paperId": "77c03906c76ba81f961dcb81593fd1f601235997",
        "title": "Agile Decision-Making and Safety-Critical Motion Planning for Emergency Autonomous Vehicles"
      },
      {
        "paperId": "229ee280cd7c3bd12385745cd8561c56e5816a01",
        "title": "Observer-Based Optimal Backstepping Security Control for Nonlinear Systems Using Reinforcement Learning Strategy"
      },
      {
        "paperId": "86d91686ba35ae78f317df6aa61bbb59d0f25d81",
        "title": "State transition learning with limited data for safe control of switched nonlinear systems"
      },
      {
        "paperId": "9572903f67172cbc56812b8eee5799f2fa20e53c",
        "title": "A Safe Self-evolution Algorithm for Autonomous Driving Based on Data-Driven Risk Quantification Model"
      },
      {
        "paperId": "30f5d7d359866b0d397c8a095ba31119193d6801",
        "title": "Optimized Backstepping-Based Containment Control for Multiagent Systems With Deferred Constraints Using a Universal Nonlinear Transformation"
      },
      {
        "paperId": "3143e7f41079f00819cd2f310ecc6887ba87fd0a",
        "title": "Position Tracking Control for Pneumatic Servo System Subject to State Constraints and Voltage Saturation"
      },
      {
        "paperId": "e25bef9c9e1ff101a18b1794a0cb9f27eb980b7e",
        "title": "Asymmetric Integral Barrier Lyapunov Function-Based Human\u2013Robot Interaction Control for Human-Compliant Space-Constrained Muscle Strength Training"
      },
      {
        "paperId": "c551b302204f7b5627183ff0e0ee6822c35d4797",
        "title": "Adaptive tracking control for constrained nonlinear nonstrict-feedback switched stochastic systems with unknown control directions"
      },
      {
        "paperId": "2947209f2451e96d658a0c3aaa57c80e3493615b",
        "title": "Dynamic Event-Triggered ADP Optimized Tracking Control of Marine Surface Vessels With Asymmetric Time-Varying Full-State Constraint"
      },
      {
        "paperId": "22059156abe3c2a80ee5f1acea8f52c2e3536e86",
        "title": "Learn From Safe Experience: Safe Reinforcement Learning for Task Automation of Surgical Robot"
      },
      {
        "paperId": "038015e8437b2a5f9808328b94981f49ce959f40",
        "title": "Safe Reinforcement Learning and Adaptive Optimal Control With Applications to Obstacle Avoidance Problem"
      },
      {
        "paperId": "9666f945576821305836fab419f343337daec069",
        "title": "Symbolic Regression Data-Model-Informed Eco-Control for Maritime Carbon Emissions Reduction"
      },
      {
        "paperId": "31b9c8be2c4f9ab1f7887b35dfa6f2228ee462c2",
        "title": "Consensus-based vehicle platoon control considering human physiological-psychological comfort"
      },
      {
        "paperId": "7c272c155f7d8d4d18c67722362cbc9ed8e3d574",
        "title": "Adaptive Robust Control for Fuzzy Mechanical Systems in Confined Spaces: Nash Game Optimization Design"
      },
      {
        "paperId": "3b78b7b6cf2402bc7f7833fc0ff632c6a04af93f",
        "title": "Backstepping based neural H\u221e optimal tracking control for nonlinear state constrained systems with input delay and disturbances"
      },
      {
        "paperId": "f3483a3a03cd9d0cbbf00438351a9a243f2fd81c",
        "title": "Optimized Backstepping Cooperative Control for Output-Constrained Stochastic Nonlinear Network Systems via a Multibridge-Hole Function"
      },
      {
        "paperId": "8ad178bcda9bb9d1c93ff80858414d5639986f80",
        "title": "Safety-Certified Multi-Target Circumnavigation With Autonomous Surface Vehicles via Neurodynamics-Driven Distributed Optimization"
      },
      {
        "paperId": "2a9accbc670e71041b8e75ad1c1bcc7e04a54599",
        "title": "CVaR-Constrained Policy Optimization for Safe Reinforcement Learning"
      },
      {
        "paperId": "3a69da89418bfcd675ab7b5e6225b23b240cf9d1",
        "title": "Shielded Planning Guided Data-Efficient and Safe Reinforcement Learning"
      },
      {
        "paperId": "7b04fa746bc05755b4e9e378f879162845bc74a0",
        "title": "Learn Zero-Constraint-Violation Safe Policy in Model-Free Constrained Reinforcement Learning"
      },
      {
        "paperId": "380ed1a9ab9dbbb69b84b33dea3fa1bffa44f9a9",
        "title": "Dynamic Event-Triggered Fixed-Time Tracking Control for State-Constrained Nonlinear Systems With Dead Zone Based on Fast Fixed-Time Filters"
      },
      {
        "paperId": "1198646b80e5de0fe4db1e96c7e1863463b06522",
        "title": "Synchronized Optimization With Prescribed Performance for High-order Strict-feedback System"
      },
      {
        "paperId": "41ab5561b70113b73c1b56e1717909b214b4110b",
        "title": "Modeling Risk in Reinforcement Learning: A Literature Mapping"
      },
      {
        "paperId": "8dd0b3326db8c00dea149496f0cb589174ebfe71",
        "title": "Two\u2010layer leader\u2010follower optimal affine formation maneuver control for networked unmanned surface vessels with input saturations"
      },
      {
        "paperId": "77d81c6766fcfc894613020ba756f2ab6f464870",
        "title": "Data-Driven Optimal Control for Lateral Stability of Vehicle in Straight-Line Driving via Adaptive Dynamic Programming"
      },
      {
        "paperId": "e2c4a757d1c592a381eea9b9206bdf49657207d8",
        "title": "Output-Positive Adaptive Control of Hyperbolic PDE-ODE Cascades"
      },
      {
        "paperId": "eaf5a017ee33652adda7382cb16f0759827cb72c",
        "title": "Avoidance Navigation Based on Offline Pre-Training Reinforcement Learning"
      },
      {
        "paperId": "3f533127fc65a03fd48f75385eb280d840d19b6b",
        "title": "Convolutional Neural Network-Based Lane-Change Strategy via Motion Image Representation for Automated and Connected Vehicles"
      },
      {
        "paperId": "cc11db06e9d55b256e2fe84f6eff0ea69fc932f1",
        "title": "Deep Reinforcement Learning design of safe, stable and robust control for sloshing-affected space launch vehicles"
      },
      {
        "paperId": "7b7d15197214144a3e9fc20b529787159d0c15e9",
        "title": "Active fault-tolerant path tracking control for autonomous vehicles considering actuator fault and model mismatch"
      },
      {
        "paperId": "7da5783122381127bfb421cda31ab3bd029e5472",
        "title": "Safety-Critical Automated Surface Vessels MIMO Control With Adaptive Control Barrier Functions Under Model Uncertainties"
      },
      {
        "paperId": "c7a40a1f2cc55412e255dd9b6438da6d9170f954",
        "title": "Brain-Inspired Multimodal Navigation With Multiscale Hippocampal\u2013Entorhinal Neural Network"
      }
    ],
    "score": 17.0
  },
  {
    "id": "d27edce419e1c731c0aeb9e1842d1e022b2cc6ab",
    "title": "Offline Meta Reinforcement Learning - Identifiability Challenges and Effective Data Collection Strategies",
    "authors": [
      "Ron Dorfman",
      "Idan Shenfeld",
      "Aviv Tamar"
    ],
    "year": 2021,
    "citationCount": 64,
    "abstract": "",
    "url": "https://www.semanticscholar.org/paper/d27edce419e1c731c0aeb9e1842d1e022b2cc6ab",
    "pdf_url": null,
    "venue": "Neural Information Processing Systems",
    "publicationDate": "",
    "externalIds": {
      "DBLP": "conf/nips/DorfmanST21",
      "CorpusId": 245016968
    },
    "references": [],
    "cited_by": [
      {
        "paperId": "a98587acde8e1df366d7c4964b5a08ed979aefb2",
        "title": "Behavioral Exploration: Learning to Explore via In-Context Adaptation"
      },
      {
        "paperId": "846f52edcdee8534b9bd2a97e13357526f439987",
        "title": "Learning Task Belief Similarity with Latent Dynamics for Meta-Reinforcement Learning"
      },
      {
        "paperId": "1dff76d605ff55f20adab9ce3e25b7ff01483df6",
        "title": "Reflect-then-Plan: Offline Model-Based Planning through a Doubly Bayesian Lens"
      },
      {
        "paperId": "f198ff3bcfb1e8d8e46ce616fc5d701bc64c5e7a",
        "title": "Deep Reinforcement Learning Data Collection for Bayesian Inference of Hidden Markov Models"
      },
      {
        "paperId": "8b3107bbd2d02585a9ab36312a70128c2cfc72f3",
        "title": "Distilling Reinforcement Learning Algorithms for In-Context Model-Based Planning"
      },
      {
        "paperId": "74d9b4d4e6be9f5aaf4a2b96f5216c401c4685c8",
        "title": "Task-Aware Virtual Training: Enhancing Generalization in Meta-Reinforcement Learning for Out-of-Distribution Tasks"
      },
      {
        "paperId": "c2c0e1ec3e006ebd6533aae98131128dc1358f0d",
        "title": "Efficient Online Reinforcement Learning Fine-Tuning Need Not Retain Offline Data"
      },
      {
        "paperId": "08e16c8ac9e36767404814076ba647264227e0d5",
        "title": "Preference Adaptive and Sequential Text-to-Image Generation"
      },
      {
        "paperId": "3da26d9d73644bb43c93727fe58190fb053817b8",
        "title": "SCORE: Simple Contrastive Representation and Reset-Ensemble for offline meta-reinforcement learning"
      },
      {
        "paperId": "2b0d7cf925cb749df5f6078d57b786f82b8994d9",
        "title": "Uncertainty-based Offline Variational Bayesian Reinforcement Learning for Robustness under Diverse Data Corruptions"
      },
      {
        "paperId": "753989375fed1c872d6f367de3320f37490a01cc",
        "title": "Bayes Adaptive Monte Carlo Tree Search for Offline Model-based Reinforcement Learning"
      },
      {
        "paperId": "758d793ff75062a6d75e721bddbd9798954270ac",
        "title": "SCOML: Trajectory Planning Based on Self-Correcting Meta-Reinforcement Learning in Hybrid Terrain for Mobile Robot"
      },
      {
        "paperId": "aff313386b09581d59c09d75bb882dc24c2c1d9f",
        "title": "Skills Regularized Task Decomposition for Multi-task Offline Reinforcement Learning"
      },
      {
        "paperId": "04033424011a78072f5c0060427bad406f7b1cf6",
        "title": "Efficient Offline Meta-Reinforcement Learning via Robust Task Representations and Adaptive Policy Generation"
      },
      {
        "paperId": "0778ce4d368b39a68fbfeb9d5f70989c89b4a02a",
        "title": "Urban-Focused Multi-Task Offline Reinforcement Learning with Contrastive Data Sharing"
      },
      {
        "paperId": "cff9757a142847f5af1db1d9319e66288da2322d",
        "title": "Memory Sequence Length of Data Sampling Impacts the Adaptation of Meta-Reinforcement Learning Agents"
      },
      {
        "paperId": "b04c769c92f2bc58c5f9e6cf220480bae26405d6",
        "title": "In-Context Decision Transformer: Reinforcement Learning via Hierarchical Chain-of-Thought"
      },
      {
        "paperId": "3c79194ec98038f3af4c29d67b360bb610e1d996",
        "title": "Decision Mamba: Reinforcement Learning via Hybrid Selective Sequence Modeling"
      },
      {
        "paperId": "a43f426d3dc8d6781cc786ff5c270f1f254d2e10",
        "title": "Offline Reinforcement Learning from Datasets with Structured Non-Stationarity"
      },
      {
        "paperId": "1ea63847ffbade109b46ddfd797cc7cd57fb2406",
        "title": "Scrutinize What We Ignore: Reining In Task Representation Shift Of Context-Based Offline Meta Reinforcement Learning"
      },
      {
        "paperId": "b708bcc30c3387c19d71ab1b60c7c8b51569abc6",
        "title": "Offline Reinforcement Learning with Domain-Unlabeled Data"
      },
      {
        "paperId": "4d3ffd8feca81d750aa30122b49cc1e874e70c1a",
        "title": "MAMBA: an Effective World Model Approach for Meta-Reinforcement Learning"
      },
      {
        "paperId": "c4972899ee166d076ab5ed8107bd1ea12de0ab9a",
        "title": "Disentangling Policy from Offline Task Representation Learning via Adversarial Data Augmentation"
      },
      {
        "paperId": "c734971c6000e3f2769ab5165d00816af80dd76f",
        "title": "In-context Exploration-Exploitation for Reinforcement Learning"
      },
      {
        "paperId": "d27da1ba65fa958e45837120fad1c25e7017d80c",
        "title": "Unsupervised Zero-Shot Reinforcement Learning via Functional Reward Encodings"
      },
      {
        "paperId": "58984fca52a7448b1c207808ad24e7dd74b8efc6",
        "title": "DynaMITE-RL: A Dynamic Model for Improved Temporal Meta-Reinforcement Learning"
      },
      {
        "paperId": "968dc4fae222137e115d5d771ea0095002b5fc76",
        "title": "Towards an Information Theoretic Framework of Context-Based Offline Meta-Reinforcement Learning"
      },
      {
        "paperId": "feb2255dbad1c829fff48922fbf0169d1b3ee4df",
        "title": "Generalizable Task Representation Learning for Offline Meta-Reinforcement Learning with Data Limitations"
      },
      {
        "paperId": "972e9f467b9f4b35859bb3af30e6ad9c56ea1240",
        "title": "In-Context Reinforcement Learning for Variable Action Spaces"
      },
      {
        "paperId": "d31def7275a87c53b8646f449ba4b467cd25efce",
        "title": "Decoupling Meta-Reinforcement Learning with Gaussian Task Contexts and Skills"
      },
      {
        "paperId": "842971bbd2668c9e4d0b3bc983383227fbfe2aa1",
        "title": "The Generalization Gap in Offline Reinforcement Learning"
      },
      {
        "paperId": "b09a04cd571506270bd174a1f81680f858f6205b",
        "title": "Meta Reinforcement Learning based Energy Management in Microgrids under Extreme Weather Events"
      },
      {
        "paperId": "05603c65d813105eb136e867abfc412ea1735cf0",
        "title": "Accelerating Exploration with Unlabeled Prior Data"
      },
      {
        "paperId": "8f9c2761662a3fab59fed11f22d3b2cfe5cd7f8e",
        "title": "Context Shift Reduction for Offline Meta-Reinforcement Learning"
      },
      {
        "paperId": "dd60413d221db7163812228f003f93a8f28be698",
        "title": "Multi-granularity fusion resource allocation algorithm based on dual-attention deep reinforcement learning and lifelong learning architecture in heterogeneous IIoT"
      },
      {
        "paperId": "736dbbb1b7aea6a126bc62e32be43018a04976f0",
        "title": "Transformers as Decision Makers: Provable In-Context Reinforcement Learning via Supervised Pretraining"
      },
      {
        "paperId": "0ba721d29e93f51235f4306e6f06c0762a10c90d",
        "title": "Robotic Manipulation Datasets for Offline Compositional Reinforcement Learning"
      },
      {
        "paperId": "1633f4662d6ccae0bb5df3ee9ef8c3f20ed9e3af",
        "title": "Contextual Pre-Planning on Reward Machine Abstractions for Enhanced Transfer in Deep Reinforcement Learning"
      },
      {
        "paperId": "5bac7d00035bc1e246a34f9ee3152b290f97bb92",
        "title": "Supervised Pretraining Can Learn In-Context Reinforcement Learning"
      },
      {
        "paperId": "70c6a7f772dc606e0f81ecd5cd3cebd3b4406270",
        "title": "Provably Efficient Offline Reinforcement Learning with Perturbed Data Sources"
      },
      {
        "paperId": "2bd6dd744cf1b0799e1e47b3ffd1306bff49dacb",
        "title": "ContraBAR: Contrastive Bayes-Adaptive Deep RL"
      },
      {
        "paperId": "7553c10f5b6dec33e0562430659adfabf9dff536",
        "title": "Offline Meta Reinforcement Learning with In-Distribution Online Adaptation"
      },
      {
        "paperId": "06ac0fab02c229139b6cc76532b4a70ba9fb449b",
        "title": "On First-Order Meta-Reinforcement Learning with Moreau Envelopes"
      },
      {
        "paperId": "231d6947a0651d3449020b728dd42c46cde81cda",
        "title": "Prompt-Tuning Decision Transformer with Preference Ranking"
      },
      {
        "paperId": "eabfc1846af8f09d6c65e45cb04f83fa397f6799",
        "title": "Meta-Reinforcement Learning Based on Self-Supervised Task Representation Learning"
      },
      {
        "paperId": "5de530b1a5ec826fabe772b724e853714ae4a829",
        "title": "On Context Distribution Shift in Task Representation Learning for Offline Meta RL"
      },
      {
        "paperId": "c5fc646d2c7466affa9eae7c69ac2d0b44d888ba",
        "title": "Learning to Control Autonomous Fleets from Observation via Offline Reinforcement Learning"
      },
      {
        "paperId": "71446fc2294f5a70de6cf6fba9f8f4af0be4899e",
        "title": "Risk Sensitive Dead-end Identification in Safety-Critical Offline Reinforcement Learning"
      },
      {
        "paperId": "2f130f118ee696d6a97cf22b2e02738d550c52e8",
        "title": "One Risk to Rule Them All: A Risk-Sensitive Perspective on Model-Based Offline Reinforcement Learning"
      },
      {
        "paperId": "0fd1a48877f62d8fa5e4c9adc8189e84b507a349",
        "title": "Contextual Transformer for Offline Meta Reinforcement Learning"
      },
      {
        "paperId": "ec743f83514e0c4845226684945b57dc6eeeccf1",
        "title": "Towards Data-Driven Offline Simulations for Online Reinforcement Learning"
      },
      {
        "paperId": "860bc4f071f35d6d8529a52c2c1858d030779a6a",
        "title": "In-context Reinforcement Learning with Algorithm Distillation"
      },
      {
        "paperId": "42306a959a67d237e90d96b4823a98642978ea79",
        "title": "Meta-learning Parameterized Skills"
      },
      {
        "paperId": "aaf4449f9242ed9fa4d9e2fd6236ce09bbdcf46d",
        "title": "Reinforcement Learning with Brain-Inspired Modulation can Improve Adaptation to Environmental Changes"
      },
      {
        "paperId": "5903fa23a67f15d5e56c5939b83e256ad9469411",
        "title": "Robust Policy Learning over Multiple Uncertainty Sets"
      },
      {
        "paperId": "5b37ecab1039b50102fac9e11dd02b0158ef742c",
        "title": "How to Leverage Unlabeled Data in Offline Reinforcement Learning"
      },
      {
        "paperId": "9faecf3e18a833f2d49b030d591cc2ded0b54336",
        "title": "Towards Continual Reinforcement Learning: A Review and Perspectives"
      },
      {
        "paperId": "d80e74e8ed6789b0901a3365f7185cb1ae9a991e",
        "title": "Defending Against Label-Only Attacks via Meta-Reinforcement Learning"
      },
      {
        "paperId": "4c7abbb0f30ffa7f214569c5fbf3e74be2d1ad16",
        "title": "Scrutinize What We Ignore: Reining Task Representation Shift In Context-Based Offline Meta Reinforcement Learning"
      },
      {
        "paperId": "0d5f39e2d6a77b2f1d28fe5ea7ead32950f133c8",
        "title": "Cost-aware Offline Safe Meta Reinforcement Learning with Robust In-Distribution Online Task Adaptation"
      },
      {
        "paperId": "bd2755fd96e20183f642628f818d7d161d3d0f0b",
        "title": "Hierarchical Diffusion for Offline Decision Making"
      },
      {
        "paperId": "a7f1be908de231bb72928fa984289501c0796823",
        "title": "One Risk to Rule Them All: Addressing Distributional Shift in Offline Reinforcement Learning via Risk-Aversion"
      },
      {
        "paperId": "42da79f3516623ab00416e79ac0e23ebda544538",
        "title": "A Study of Generalization in Offline Reinforcement Learning"
      },
      {
        "paperId": "f5a68960d3225dfbdcbda5b8af0ab5481cdd286f",
        "title": "In-Context Generalization to New Tasks From Unlabeled Observation Data"
      }
    ],
    "score": 16.0
  },
  {
    "id": "116fc10b798e3294b00e92f2b8053d0c89ad9182",
    "title": "A robot exploration strategy based on Q-learning network",
    "authors": [
      "L. Tai",
      "Ming Liu"
    ],
    "year": 2016,
    "citationCount": 143,
    "abstract": "",
    "url": "https://www.semanticscholar.org/paper/116fc10b798e3294b00e92f2b8053d0c89ad9182",
    "pdf_url": "https://doi.org/10.1109/RCAR.2016.7784001",
    "venue": "International Conference on Real-time Computing and Robotics",
    "publicationDate": "2016-06-06",
    "externalIds": {
      "MAG": "2562788852",
      "DBLP": "conf/rcar/LeiM16",
      "DOI": "10.1109/RCAR.2016.7784001",
      "CorpusId": 18538556
    },
    "references": [],
    "cited_by": [
      {
        "paperId": "b6f3942454362c0e61cd8f30e9fbb7a617623fac",
        "title": "GUIDE: A Diffusion-Based Autonomous Robot Exploration Framework Using Global Graph Inference"
      },
      {
        "paperId": "6572aa14da7ed9c29ca76a40aa87c567c22daa96",
        "title": "Collision Avoidance Based on Stochastic Model Predictive Control in Collaboration Between ROV and AUV"
      },
      {
        "paperId": "994714fb3fc35033a1e76cf8e274ee40f73b54e8",
        "title": "ViT-Enabled Task-Driven Autonomous Heuristic Navigation Based on Deep Reinforcement Learning"
      },
      {
        "paperId": "7010ee3f085c2625bac2d5820e18c1a9cc041cbf",
        "title": "CTSAC: Curriculum-Based Transformer Soft Actor-Critic for Goal-Oriented Robot Exploration"
      },
      {
        "paperId": "eadd4318b0d30b7f22c66be297ac2ada591793f5",
        "title": "Application and optimization of intelligent traffic signal control and networked autonomous vehicles in mixed traffic environments"
      },
      {
        "paperId": "13a10692635fb708da34068de7c981cf5af8a577",
        "title": "Research on human-computer interaction personalized experience algorithm based on user experience theory"
      },
      {
        "paperId": "e2c46456ca0d77b7ef92ec4245b7b30988f93cf5",
        "title": "Gymnasium Library Interface for Multi Access Edge Computing"
      },
      {
        "paperId": "4f4ca6cd7811f96e477fc85ac7be599ee70bfbd7",
        "title": "GRRL: Goal-Guided Risk-Inspired Reinforcement Learning for Efficient Autonomous Driving in Off- Road Environment"
      },
      {
        "paperId": "c5c9f167f87940323aa1b687213d330c93e26903",
        "title": "Hybrid Machine Learning and Reinforcement Learning Framework for Adaptive UAV Obstacle Avoidance"
      },
      {
        "paperId": "5403811fc896edecb1491878fc80e6c2606021cd",
        "title": "Mobile Robot Path Planning using Deep Deterministic Policy Gradient with Differential Gaming (DDPG-DG) exploration"
      },
      {
        "paperId": "674dcbe843e4c65ca0d79c2dc34a701b60732885",
        "title": "The evolution of the flip-it game in cybersecurity: Insights from the past to the future"
      },
      {
        "paperId": "41055707ebc12b6e2ca5395002670b349a84a067",
        "title": "A Soft Actor-Critic Deep Reinforcement-Learning-Based Robot Navigation Method Using LiDAR"
      },
      {
        "paperId": "4ad0611a0d0cb765e5b01c76e48b930db18b39e3",
        "title": "VO-Safe Reinforcement Learning for Drone Navigation"
      },
      {
        "paperId": "2ddb2c6c4ecb66ea29f46babbb030dccaf8b9dd5",
        "title": "Design and Development of Multi-Agent Reinforcement Learning Intelligence on the Robotarium Platform for Embedded System Applications"
      },
      {
        "paperId": "547eedad008b1fbe133d82ab2ae74b59381b5d1f",
        "title": "Deep Reinforcement Learning for Mobile Robot Path Planning"
      },
      {
        "paperId": "7861031d9b3b9f570d8d9ea89b8245dc5e5ec63f",
        "title": "An MIU-based deep embedded clustering model for urban functional zoning from remote sensing images and VGI data"
      },
      {
        "paperId": "5a96f59ae208723f983495b3761cdad70543341c",
        "title": "Investigating the Impact of Curriculum Learning on Reinforcement Learning for Improved Navigational Capabilities in Mobile Robots"
      },
      {
        "paperId": "556c63e3a18dcfce8118b23eeb506646b77eef0e",
        "title": "Collision-Free Robot Navigation in Crowded Environments using Learning based Convex Model Predictive Control"
      },
      {
        "paperId": "983f4f7c610fca3019fe8cea37a71b79efd0da89",
        "title": "Cognitive intelligence in industrial robots and manufacturing"
      },
      {
        "paperId": "4147049222500ff9154a28ddc824a6cc625eb9e3",
        "title": "Enhanced Deep Q-Learning for 2D Self-Driving Cars: Implementation and Evaluation on a Custom Track Environment"
      },
      {
        "paperId": "4fa421d90d28d3c6a74bc8e9a5db6330c1ccc0b0",
        "title": "Penerapan Metode Fuzzy Q-Learning Pada Robot Berkaki Enam Pemadam Api Dalam Pencarian Jalur Tercepat Menuju Titik Api"
      },
      {
        "paperId": "b9232e9fc02b9eea96cb6a4804cdfce77dcfe56f",
        "title": "Autonomous navigation of mobile robots in unknown environments using off-policy reinforcement learning with curriculum learning"
      },
      {
        "paperId": "14f85356e407dbb0f8ad7a44d81e91f4ef746247",
        "title": "Development and evaluation of Reinforcement Learning models for the FOSSBot Open-Source educational robot"
      },
      {
        "paperId": "393797551f0499860ba72c56346383374853c3b0",
        "title": "Multi-Robot Autonomous Exploration in Unknown Environment: A Review"
      },
      {
        "paperId": "a8be40f357b75f79ea8ff50bebaa9d1c827df460",
        "title": "uWSC Aircraft Simulator: A Gazebo-based model for uncrewed weight-shift control aircraft flight simulation"
      },
      {
        "paperId": "a8b960a8b28e99095f06633c248f75ab7024d91b",
        "title": "A Review of Deep Reinforcement Learning Algorithms for Mobile Robot Path Planning"
      },
      {
        "paperId": "b59738dd26b49a5f42133a6b89ce74772e13b594",
        "title": "Autonomous Navigation of Wheelchairs in Indoor Environments using Deep Reinforcement Learning and Computer Vision"
      },
      {
        "paperId": "7806b60b7645887a82c8712f58bb5c5e1339a4f9",
        "title": "DREAM: Decentralized Reinforcement Learning for Exploration and Efficient Energy Management in Multi-Robot Systems"
      },
      {
        "paperId": "456b579078d2c9348c4fb82486f9cb117ab90e29",
        "title": "Using Deep Convolutional Neural Networks to Abstract Obstacle Avoidance for Indoor Environments"
      },
      {
        "paperId": "8e02a933d835ecc32f2e44d4222c3170e82dfdb3",
        "title": "Multimodal fusion for autonomous navigation via deep reinforcement learning with sparse rewards and hindsight experience replay"
      },
      {
        "paperId": "cbed306230184ec14896523f7ccbbae7e6a9a3b1",
        "title": "Remembrance of things perceived: Adding thalamocortical function to artificial neural networks"
      },
      {
        "paperId": "ff8f60bda66e227f4b10f547fb02db5cb4c44331",
        "title": "Mobile robot monocular vision-based obstacle avoidance algorithm using a deep neural network"
      },
      {
        "paperId": "04aaf2a2ab1b38f9a98cc8f19211bd345c59d476",
        "title": "Machine Learning for Emergency Management: A Survey and Future Outlook"
      },
      {
        "paperId": "672ffccf28ee76f621cb0113230b063b889d0fd3",
        "title": "Mobile robot path planning based on improved A-star_DWA fusion algorithm"
      },
      {
        "paperId": "6644bc9f0e7ba43571593aa19b635397a828332a",
        "title": "Localisation-Safe Reinforcement Learning for Mapless Navigation"
      },
      {
        "paperId": "01ea8fe8304a141bdc4c286a8449838b76e38b9a",
        "title": "Multi-robot Cooperative Obstacle Avoidance Based on Improved Artificial Potential Field Method"
      },
      {
        "paperId": "52c474e7ccddd1169354ca314d314dd3ee754eb9",
        "title": "Autonomous Robot Navigation in Crowd"
      },
      {
        "paperId": "2fed4e83d6fa6b75829e493d783583a73bf9f847",
        "title": "Unknown area exploration for robots with energy constraints using a modified Butterfly Optimization Algorithm"
      },
      {
        "paperId": "6311be5ab59287a956b2687db970ed8c82d3091f",
        "title": "Neuro-Planner: A 3D Visual Navigation Method for MAV With Depth Camera Based on Neuromorphic Reinforcement Learning"
      },
      {
        "paperId": "70faa92ec1f7ddf554cba646803d18bfdcf39312",
        "title": "Leveraging Expert Demonstration Features for Deep Reinforcement Learning in Floor Cleaning Robot Navigation"
      },
      {
        "paperId": "3458b6ee6286ec089929c48edb040f20fd688576",
        "title": "Deep Reinforcement Learning Based Mobile Robot Navigation in Unknown Environment with Continuous Action Space"
      },
      {
        "paperId": "a74020571757d531b06529f79086e1520ecaad2f",
        "title": "Path planning of improved DQN based on quantile regression"
      },
      {
        "paperId": "1f577f6f0785c7967bef4ac6d0ab0370c2815b4d",
        "title": "Occupancy Reward-Driven Exploration with Deep Reinforcement Learning for Mobile Robot System"
      },
      {
        "paperId": "c98d5ec0d7c99399f7bb7764dc2c1a32b716ffe4",
        "title": "Deep Reinforcement Learning based Robot Navigation in Dynamic Environments using Occupancy Values of Motion Primitives"
      },
      {
        "paperId": "4ea992a5dbd8556476b10dea8972df4fab3d7047",
        "title": "The Determination of Reward Function in AGV Motion Control Based on DQN"
      },
      {
        "paperId": "772d8c3b3962faf2db842ed6202babfef2a112be",
        "title": "End-to-End Autonomous Exploration for Mobile Robots in Unknown Environments through Deep Reinforcement Learning"
      },
      {
        "paperId": "327b0fa0bb5f1fc0e924ed15ba2415758473d51e",
        "title": "Pressure control based on reinforcement learning strategy of the pneumatic relays for an electric-pneumatic braking system"
      },
      {
        "paperId": "4e592808d4f1e49f2e268b0018c4be35e563fbef",
        "title": "Smooth Fictitious Play in Stochastic Games with Perturbed Payoffs and Unknown Transitions"
      },
      {
        "paperId": "58ea6612150736c931cac0e6051774544ba81a36",
        "title": "Deep Reinforcement Learning for Next-Best-View Planning in Agricultural Applications"
      },
      {
        "paperId": "7a4788979c15aefe7d3a4b2223997f49c5212d26",
        "title": "Memory-enhanced deep reinforcement learning for UAV navigation in 3D environment"
      },
      {
        "paperId": "3ae5be7ecbb06a653ef4656e5a4b693d03db70ca",
        "title": "Path Planning Method of Mobile Robot Using Improved Deep Reinforcement Learning"
      },
      {
        "paperId": "08d3b6c3b4e07a91a71ee3d80e51cf0b405fe3d5",
        "title": "A UAV Coverage Path Planning Algorithm Based on Double Deep Q-Network"
      },
      {
        "paperId": "33037512d7d010c8d0efe9056b0bac15c937e076",
        "title": "A Review of Mobile Robot Path Planning Based on Deep Reinforcement Learning Algorithm"
      },
      {
        "paperId": "ba107fdb842ee0218dffa34d7d7da7a20289a900",
        "title": "Deep reinforcement learning-based rehabilitation robot trajectory planning with optimized reward functions"
      },
      {
        "paperId": "9d72322f16a292295975cd9d093f45517b4aa6d8",
        "title": "Fictitious Play and Best-Response Dynamics in Identical Interest and Zero-Sum Stochastic Games"
      },
      {
        "paperId": "a57f9224396263cd4e51c25b1d9686d840cfdd74",
        "title": "Mapless navigation based on continuous deep reinforcement learning"
      },
      {
        "paperId": "8a632dc222061e4dab752b5150dc4beef4dc01fe",
        "title": "Autonomous construction hoist system based on deep reinforcement learning in high-rise building construction"
      },
      {
        "paperId": "02c01b727ff8c0b086c30d2b0f3245b3daf95767",
        "title": "Learning Robot Exploration Strategy With 4D Point-Clouds-Like Information as Observations"
      },
      {
        "paperId": "0e7ab22e676e6a4448895d55dba85f98f335f8df",
        "title": "Double BP Q-Learning Algorithm for Local Path Planning of Mobile Robot"
      },
      {
        "paperId": "bc950b2e4a5c8611afb493bbc9cf6c92c8b87f86",
        "title": "Multi-robot exploration in task allocation problem"
      },
      {
        "paperId": "069ff749aeb465db48ac0d72989d268ba7e0afbd",
        "title": "A Tabu list strategy based DQN for AAV mobility in indoor single-path environment: Implementation and performance evaluation"
      },
      {
        "paperId": "e36ed836a490edc49d708443cf48c61a1bcc3f84",
        "title": "Extendable Navigation Network based Reinforcement Learning for Indoor Robot Exploration"
      },
      {
        "paperId": "a313ceacc4803d3e58c981a04f3e3661d548cbf2",
        "title": "Deep Q\u2010network implementation for simulated autonomous vehicle control"
      },
      {
        "paperId": "bae901b2011089906a0d43446cf92e9891ecf594",
        "title": "Reinforcement learning for robot research: A comprehensive review and open issues"
      },
      {
        "paperId": "f532f6fcb1bd0e296ba0c9991fdb9c450d77fc68",
        "title": "Modeling hesitancy in airport choice: A comparison of discrete choice and machine learning methods"
      },
      {
        "paperId": "bd2bbf71a57cbe3dd03256739ae87e0013214411",
        "title": "Drone Deep Reinforcement Learning: A Review"
      },
      {
        "paperId": "7cdd906d8b97bd36fb68b492ab0523ff77b2e4f1",
        "title": "Vision-Based Mobile Robot Controllers: A Scientific Review"
      },
      {
        "paperId": "4ef678ab88fdcd0eeaff8b36cc8ecd74bc85115e",
        "title": "Active Mapping and Robot Exploration: A Survey"
      },
      {
        "paperId": "1b725e3bdde0761d7cf6cb295cd621a54877a9a2",
        "title": "Goal-Driven Autonomous Exploration Through Deep Reinforcement Learning"
      },
      {
        "paperId": "58f8e125fa137cb0952276a780074afe1e072a49",
        "title": "A Validation Approach for Deep Reinforcement Learning of a Robotic Arm in a 3D Simulated Environment"
      },
      {
        "paperId": "8824b30f28f2326a870567135da8e4eacfa3cc30",
        "title": "Object-oriented Map Exploration and Construction Based on Auxiliary Task Aided DRL"
      },
      {
        "paperId": "945b537ca30a2d1285956d23eeb61a2f0c95c3e7",
        "title": "Reinforcement Learning based Method for Autonomous Navigation of Mobile Robots in Unknown Environments"
      },
      {
        "paperId": "4fcda6d3c69fbdbb6568912dfbf5c183e352a297",
        "title": "Incremental Learning for Autonomous Navigation of Mobile Robots based on Deep Reinforcement Learning"
      },
      {
        "paperId": "3a72f1348a82c972b629caa98350f63e305873a7",
        "title": "Aerial-DeepSearch: Distributed Multi-Agent Deep Reinforcement Learning for Search Missions"
      },
      {
        "paperId": "e8490a70c3ab009e5fe46416f019fee12bab379a",
        "title": "Integrating Deep-Learning-Based Image Completion and Motion Planning to Expedite Indoor Mapping"
      },
      {
        "paperId": "0eea3e0e327b69f7421dd0e2a8bb945171cc6251",
        "title": "Deep-Learning-Aided Path Planning and Map Construction for Expediting Indoor Mapping"
      },
      {
        "paperId": "e6655ada71be867e430797d82d360e1eee7f570d",
        "title": "Adversarial Attack against Deep Reinforcement Learning with Static Reward Impact Map"
      },
      {
        "paperId": "58438a6f0610667a06b141ff9347cd508587a5dc",
        "title": "Finite-Time Analysis for Double Q-learning"
      },
      {
        "paperId": "690dbdaf448045246b4109ebeeb2ef145e117e57",
        "title": "Deep Reinforcement Learning for Indoor Mobile Robot Path Planning"
      },
      {
        "paperId": "86a70a83dc311c4fa8b9206665b59d888286346a",
        "title": "Mobile Robot Navigation Using Deep Reinforcement Learning in Unknown Environments"
      },
      {
        "paperId": "1ddb3e940ccff3eaf64e91007d2c835b3add51ac",
        "title": "A 3D Simulation Environment and Navigation Approach for Robot Navigation via Deep Reinforcement Learning in Dense Pedestrian Environment"
      },
      {
        "paperId": "029e2a424878b2e2a42b66084f1411f081cd9da5",
        "title": "Path planning for intelligent robots based on deep Q-learning with experience replay and heuristic knowledge"
      },
      {
        "paperId": "d1f667f1a8f0201bea69d0ce5277ba7e5e9876f6",
        "title": "An Enhanced Direction Calibration Based on Reinforcement Learning for Indoor Localization System"
      },
      {
        "paperId": "1a33a38db41272aad1420a95811dad27dbde4ea6",
        "title": "A DQN Based Mobile Actor Node Control in WSAN: Simulation Results of Different Distributions of Events Considering Three-Dimensional Environment"
      },
      {
        "paperId": "3f8cfb901c6cfbc408690e9ce5329ec94b04c79f",
        "title": "Learning a Motion Policy to Navigate Environments with Structured Uncertainty"
      },
      {
        "paperId": "0c39349e1c495b911077abb3e5aebc29a590f891",
        "title": "Perception and Navigation in Autonomous Systems in the Era of Learning: A Survey"
      },
      {
        "paperId": "5bd815eac4ab1481d0c26208182bbea01ddbf8b9",
        "title": "Perception and Decision-Making of Autonomous Systems in the Era of Learning: An Overview"
      },
      {
        "paperId": "5420c2b33f899a07fb1ae4c4886eed1a6f7ac3fc",
        "title": "An Overview of Perception and Decision-Making in Autonomous Systems in the Era of Learning"
      },
      {
        "paperId": "2f1845858f83ddd6d5b32e14dfad77bd933054eb",
        "title": "Robot Path Planning in Dynamic Environments Based on Deep Reinforcement Learning"
      },
      {
        "paperId": "6013825e2fe9ab9dc85567c7aae7e7337c90af8a",
        "title": "Vision based Indoor Obstacle Avoidance using a Deep Convolutional Neural Network"
      },
      {
        "paperId": "3d782c6016f301fc441a6eae37f45dd083de0c1a",
        "title": "UCAV Path Planning Algorithm Based on Deep Reinforcement Learning"
      },
      {
        "paperId": "de91573fa09fcc043a9ef23750c6f70d2007e689",
        "title": "Mobile Robot Obstacle Avoidance Based on Deep Reinforcement Learning"
      },
      {
        "paperId": "9902a0fbc5c743168b3eb58d091b9c0b6442ffbe",
        "title": "Collaborative Multi-Agent Tracking based on Distributed Learning"
      },
      {
        "paperId": "889a98bc20a2661079a1c6ab80b7754c7996983c",
        "title": "Ai Motion Control \u2013 A Generic Approach to Develop Control Policies for Robotic Manipulation Tasks"
      },
      {
        "paperId": "43c145cf272e8aad5506d321def94cd275a2f030",
        "title": "A Deep Reinforcement Learning Based Mapless Navigation Algorithm Using Continuous Actions"
      },
      {
        "paperId": "40e0639c5a67182bea708d9c71cae57c15e3ad72",
        "title": "A Navigation Scheme for a Random Maze using Reinforcement Learning with Quadrotor Vision"
      },
      {
        "paperId": "0c4c8a6d8d6fec2f5ae1becc90bea4d39b040902",
        "title": "Learned Map Prediction for Enhanced Mobile Robot Exploration"
      },
      {
        "paperId": "1d3507b037684cd609be2261b5a465d0ac62ec0a",
        "title": "Learning Motion Planning Policies in Uncertain Environments through Repeated Task Executions"
      },
      {
        "paperId": "14d74716da5571aa6fdfd200ba35949f477a7a4c",
        "title": "Design of a Deep Q-Network Based Simulation System for Actuation Decision in Ambient Intelligence"
      },
      {
        "paperId": "7b4dd3364a74ee6490996013ea2a2d69d3828e7d",
        "title": "Mapless Motion Planning System for an Autonomous Underwater Vehicle Using Policy Gradient-based Deep Reinforcement Learning"
      },
      {
        "paperId": "cc13c12259d70d6ef09e38ce6cdfcf3d23db6d86",
        "title": "Deep Reinforcement Learning Robot for Search and Rescue Applications: Exploration in Unknown Cluttered Environments"
      },
      {
        "paperId": "efb26acbbdd7f47d235aad80304f51f269e6166c",
        "title": "Performance Analysis of Deep Neural Network and Stacked Autoencoder for Image Classification"
      },
      {
        "paperId": "ebb7d4eb048f44096e5537198cbabce1a01b259d",
        "title": "3D visual-inertial odometry and autonomous mobile robot exploration with learned map prediction"
      },
      {
        "paperId": "7d63cd7883a352ae61a9992ed0a943c70d76f207",
        "title": "An End-to-End Deep Reinforcement Learning-Based Intelligent Agent Capable of Autonomous Exploration in Unknown Environments"
      },
      {
        "paperId": "59a67109662c34c45d94bd7759ecf5d62dae2513",
        "title": "Vision Memory for Target Object Navigation Using Deep Reinforcement Learning: An Empirical Study"
      },
      {
        "paperId": "3255daa9c3fb73001bacc1e92590d10f471d2867",
        "title": "Greedy Algorithms for Sparse Sensor Placement via Deep Learning."
      },
      {
        "paperId": "a85c8db7b3dfb69e221f5b834e2c103e8fc2195f",
        "title": "Autonomous Exploration, Reconstruction, and Surveillance of 3D Environments Aided by Deep Learning"
      },
      {
        "paperId": "67c79fd8f015bd9806f49ae9469bf373fb1a0db2",
        "title": "A Deep Q-Network Based Simulation System for Actor Node Mobility Control in WSANs Considering Three-Dimensional Environment: A Comparison Study for Normal and Uniform Distributions"
      },
      {
        "paperId": "580aecbad87c0d9c563744ba0dd108179549bd3f",
        "title": "Memory-based reinforcement learning algorithm for autonomous exploration in unknown environment"
      },
      {
        "paperId": "ab992a28fea6df98b9da12189ba12d57c99ba4d6",
        "title": "GOSELO: Goal-Directed Obstacle and Self-Location Map for Robot Navigation Using Reactive Neural Networks"
      },
      {
        "paperId": "59c23afa9e53f97a28da3abbecb3d1eb8d05985b",
        "title": "A Survey on Deep Learning Methods for Robot Vision"
      },
      {
        "paperId": "6b58d4f09980700e61d379fda2e7c05f470a55b8",
        "title": "Autonomous overtaking decision making of driverless bus based on deep Q-learning method"
      },
      {
        "paperId": "2025446a618a0f8367fbde552a7dd60823a9f269",
        "title": "Effective lazy training method for deep q-network in obstacle avoidance and path planning"
      },
      {
        "paperId": "3943d227455558ab91baf7ea502d470e44435ce9",
        "title": "Performance Evaluation of a Deep Q-Network Based Simulation System for Actor Node Mobility Control in Wireless Sensor and Actor Networks Considering Three-Dimensional Environment"
      },
      {
        "paperId": "0787534368ed2f51de96977ee50a39cc7066641d",
        "title": "Performance Evaluation of a Deep Q-Network Based Simulation System for Actor Node Mobility Control in Wireless Sensor and Actor Networks Considering Different Distributions of Events"
      },
      {
        "paperId": "058feddc2d06479925445e7c5a65f691e9d9e83d",
        "title": "Autonomous lane keeping based on approximate Q-learning"
      },
      {
        "paperId": "e2ff17a779366f9860c28c4dc67d2dd346327183",
        "title": "Design and Implementation of a Simulation System Based on Deep Q-Network for Mobile Actor Node Control in Wireless Sensor and Actor Networks"
      },
      {
        "paperId": "b99f42939f80e870ff3b901accc24d8063793d63",
        "title": "Deep-learning in Mobile Robotics - from Perception to Control Systems: A Survey on Why and Why not"
      },
      {
        "paperId": "8a8f5ec1d76ab3308434b8fccc2b5e67907226ac",
        "title": "Mobile robots exploration through cnn-based reinforcement learning"
      },
      {
        "paperId": "8d218715b292f423799bb574e24bd03a6fb861dd",
        "title": "A Survey of Deep Network Solutions for Learning Control in Robotics: From Reinforcement to Imitation"
      },
      {
        "paperId": "987f5538e29a38883111cc9e999caa02e96627d3",
        "title": "PCA-aided fully convolutional networks for semantic segmentation of multi-channel fMRI"
      },
      {
        "paperId": "01dfe1868e8abc090b1485482929f65743e23743",
        "title": "Towards Cognitive Exploration through Deep Reinforcement Learning for Mobile Robots"
      },
      {
        "paperId": "fdc9a534dd436c28b983740f9d180791aa4287b1",
        "title": "A deep-network solution towards model-less obstacle avoidance"
      },
      {
        "paperId": "a4fb4a1c6e4063b1ce76223a9cbf734c30f1a76d",
        "title": "Research, Design and Control Mobile Robots for Intelligent Navigation Based on ROS Programming"
      },
      {
        "paperId": "46305eb8bdbd82a69f7fd58b0263f73c5ebf9562",
        "title": "Towards Reducing Human Supervision in Fielded Multi-Agent Systems"
      },
      {
        "paperId": "3159663ab55037dc24fd4801e1f6bf3dc930cfd8",
        "title": "Cognitive Computing \u2013 ICCC 2022: 6th International Conference, Held as Part of the Services Conference Federation, SCF 2022, Honolulu, HI, USA, December 10-14, 2022, Proceedings"
      },
      {
        "paperId": "56bc3d321448aeb4fee6b9349b35d08a7a537a08",
        "title": "Multi-stage Path Planning Strategy for Intelligent Cleaning Robot"
      },
      {
        "paperId": "bc7c179daa5a0a252fc3815203c29aaaa9b6359c",
        "title": "Goal-Driven Autonomous Mapping Through Deep Reinforcement Learning and Planning-Based Navigation"
      },
      {
        "paperId": "6074fddf85e114ebec80447a1c3508e807f4d943",
        "title": "A Memory-Greedy Policy With Guaranteed Convergence for Accelerating Reinforcement Learning"
      },
      {
        "paperId": "c8e03687572492b155f494179861f832a716663c",
        "title": "Best-Response Dynamics and Fictitious Play in Identical Interest Stochastic Games"
      },
      {
        "paperId": "e8f3781b7d07bbbd1cc5ef9a31f08a111842c935",
        "title": "Proposal and Evaluation of a Tabu List Based DQN for AAV Mobility"
      },
      {
        "paperId": "4722b09d12327be352a887c7ecdd6d5e2f8064a1",
        "title": "A Systematic Review on Reinforcement Learning-Based Robotics Within the Last Decade"
      },
      {
        "paperId": "f6171601d61904a8b17dcaddebaac8ea7411d180",
        "title": "Autonomous Navigation for Omnidirectional Robot Based on Deep Reinforcement Learning"
      },
      {
        "paperId": "26932700c1f8eca71b559bfd3532fa298eb1d02f",
        "title": "Masters Thesis: End-to-End Motion Planning"
      },
      {
        "paperId": "62bead8a56b2b5318239f256d158672a26aa4f38",
        "title": "Deep Q-Learning Algorithm for Solving Inverse Kinematics of Four-Link Manipulator"
      },
      {
        "paperId": "a5d041f76e1bd4f7e1a7e926e78f7e8c04a11e93",
        "title": "Design and Implementation of a DQN Based AAV"
      },
      {
        "paperId": "c391e553f66f4117b924ad00dfaecfc443021335",
        "title": "Target Search Control of AUV in Underwater Environment With Deep Reinforcement Learning"
      },
      {
        "paperId": "c0f331586f304bda8f767b74f7a3c0b883454cbc",
        "title": "GreenML : A methodology for fair evaluation of machine learning algorithms with respect to resource consumption"
      },
      {
        "paperId": "c6eb7c40df6362402cb5a8dd10fcbbdfd426d719",
        "title": "MOBILE ROBOT OBSTACLE AVOIDANCE BASE ON DEEP REINFORCEMENT LEARNING"
      },
      {
        "paperId": "05550696a0a1f2ca9c444aada0011019d9ce84f9",
        "title": "Deep Reinforcement Learning With Optimized Reward Functions for Robotic Trajectory Planning"
      },
      {
        "paperId": "3f481d4074667bd0e64a415050aec4398f27ad74",
        "title": "Path Planning Amidst Moving Obstacles"
      },
      {
        "paperId": "31bb764acdb0610e5be2f75bd7291d1041fe87e7",
        "title": "Imitating the Brain: Autonomous Robots Harnessing the Power of Artificial Neural Networks"
      },
      {
        "paperId": "2af0919351418cff07e9ca75fa2b5ff9784eced1",
        "title": "Ai Motion Control \u2013 A Generic Approach to Develop Control Ai Motion Control \u2013 A Generic Approach to Develop Control Policies for Robotic Manipulation Tasks Policies for Robotic Manipulation Tasks"
      }
    ],
    "score": 15.888888888888888
  },
  {
    "id": "0f810eb4777fd05317951ebaa7a3f5835ee84cf4",
    "title": "Count-Based Exploration in Feature Space for Reinforcement Learning",
    "authors": [
      "Jarryd Martin",
      "S. N. Sasikumar",
      "Tom Everitt",
      "Marcus Hutter"
    ],
    "year": 2017,
    "citationCount": 126,
    "abstract": "We introduce a new count-based optimistic exploration algorithm for Reinforcement Learning (RL) that is feasible in environments with high-dimensional state-action spaces. The success of RL algorithms in these domains depends crucially on generalisation from limited training experience. Function approximation techniques enable RL agents to generalise in order to estimate the value of unvisited states, but at present few methods enable generalisation regarding uncertainty. This has prevented the combination of scalable RL algorithms with efficient exploration strategies that drive the agent to reduce its uncertainty. We present a new method for computing a generalised state visit-count, which allows the agent to estimate the uncertainty associated with any state. Our \\phi-pseudocount achieves generalisation by exploiting same feature representation of the state space that is used for value function approximation. States that have less frequently observed features are deemed more uncertain. The \\phi-Exploration-Bonus algorithm rewards the agent for exploring in feature space rather than in the untransformed state space. The method is simpler and less computationally expensive than some previous proposals, and achieves near state-of-the-art results on high-dimensional RL benchmarks.",
    "url": "https://www.semanticscholar.org/paper/0f810eb4777fd05317951ebaa7a3f5835ee84cf4",
    "pdf_url": "https://arxiv.org/pdf/1706.08090.pdf",
    "venue": "International Joint Conference on Artificial Intelligence",
    "publicationDate": "2017-06-25",
    "externalIds": {
      "DBLP": "conf/ijcai/MartinSEH17",
      "MAG": "2950980798",
      "ArXiv": "1706.08090",
      "DOI": "10.24963/ijcai.2017/344",
      "CorpusId": 24715002
    },
    "references": [
      {
        "paperId": "7056e205e6829b3dc789ec35eac82f6e31923c2c",
        "title": "Exploration in Feature Space for Reinforcement Learning"
      },
      {
        "paperId": "12f67fb182bc934fc95ce97acff553d83e2ca72e",
        "title": "Count-Based Exploration with Neural Density Models"
      },
      {
        "paperId": "0fcb2034e31a2bc2f12a2b1363d0d77baf445fdf",
        "title": "#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning"
      },
      {
        "paperId": "6e90fd78e8a3b98af3954aae5209703aa966603e",
        "title": "Unifying Count-Based Exploration and Intrinsic Motivation"
      },
      {
        "paperId": "4931c91f4b30eb122def1e697abc096f14c48987",
        "title": "Learning values across many orders of magnitude"
      },
      {
        "paperId": "4b63e34276aa98d5345efa7fe09bb06d8a9d8f52",
        "title": "Deep Exploration via Bootstrapped DQN"
      },
      {
        "paperId": "69e76e16740ed69f4dc55361a3d319ac2f1293dd",
        "title": "Asynchronous Methods for Deep Reinforcement Learning"
      },
      {
        "paperId": "42b4d4087360228c5e4a8f24a01ebba9aba6d06a",
        "title": "State of the Art Control of Atari Games Using Shallow Reinforcement Learning"
      },
      {
        "paperId": "4c05d7caa357148f0bbd61720bdd35f0bc05eb81",
        "title": "Dueling Network Architectures for Deep Reinforcement Learning"
      },
      {
        "paperId": "3b9732bb07dc99bde5e1f9f75251c6ea5039373e",
        "title": "Deep Reinforcement Learning with Double Q-Learning"
      },
      {
        "paperId": "d5ed07113ddcd038062525a5a54550c012ac9a74",
        "title": "Massively Parallel Methods for Deep Reinforcement Learning"
      },
      {
        "paperId": "2470fcf0f89082de874ac9133ccb3a8667dd89a8",
        "title": "Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models"
      },
      {
        "paperId": "340f48901f72278f6bf78a04ee5b01df208cc508",
        "title": "Human-level control through deep reinforcement learning"
      },
      {
        "paperId": "449532187c94af3dd3aa55e16d2c50f7854d2199",
        "title": "Trust Region Policy Optimization"
      },
      {
        "paperId": "1389772b8a0f9c7fc43057f9da41a7d0ebf0308b",
        "title": "Generalization and Exploration via Randomized Value Functions"
      },
      {
        "paperId": "f82e4ff4f003581330338aaae71f60316e58dd26",
        "title": "The Arcade Learning Environment: An Evaluation Platform for General Agents"
      },
      {
        "paperId": "be4318c4f7fbc821d9b80f569156b7ace81743c9",
        "title": "Context Tree Switching"
      },
      {
        "paperId": "5d8e1eeeb0e4b0e0846a355532d0f9452249e68a",
        "title": "Reinforcement Learning in Finite MDPs: PAC Analysis"
      },
      {
        "paperId": "4282669947ca340de9f93a9a2a38007fe542195d",
        "title": "Near-Bayesian exploration in polynomial time"
      },
      {
        "paperId": "237a1cf18ed83bb3ad852b34f443c6c1ff3336c1",
        "title": "An analysis of model-based Interval Estimation for Markov Decision Processes"
      },
      {
        "paperId": "849aaec99f3684505876a6d0f3e9a73621a8daff",
        "title": "An empirical evaluation of interval estimation for Markov decision processes"
      },
      {
        "paperId": "4c9e36a0dbdc3e7b20e38bd288a804c05c77e9fa",
        "title": "Asymptotically efficient adaptive allocation rules"
      },
      {
        "paperId": "890351f25fc6cf00e5b40759c5956c84aa73bca9",
        "title": "Journal of Computer and System Sciences"
      },
      {
        "paperId": null,
        "title": "International Conference on Machine Learning"
      },
      {
        "paperId": null,
        "title": "In IEEE Data Compression Conference"
      },
      {
        "paperId": "ff62aa2bfd3b4db63307c8a684e3c4edb39d990c",
        "title": "Planning"
      },
      {
        "paperId": "a9e5a40b0ff5c40d2db7b73490922e115576adb5",
        "title": "On the sample complexity of reinforcement learning."
      },
      {
        "paperId": "97efafdb4a3942ab3efba53ded7413199f79c054",
        "title": "Reinforcement Learning: An Introduction"
      },
      {
        "paperId": "bc6014884d291555d92b8dbef4635a1a9e192962",
        "title": "Integrated Architectures for Learning, Planning, and Reacting Based on Approximating Dynamic Programming"
      },
      {
        "paperId": "1545b302f5961bbd35d065294c2c545e7c81679b",
        "title": "Advances in applied mathematics"
      },
      {
        "paperId": "572379c1d56e7e19422ae38218ee228c61aefb2f",
        "title": "Asymptotically Efficient Adaptive Allocation Rules"
      }
    ],
    "cited_by": [
      {
        "paperId": "5b576bebf7d18d8dc2ed90ef53cab0a8a283d1a6",
        "title": "Exploration by Random Reward Perturbation"
      },
      {
        "paperId": "675dce487447b58e2d41dff4b41f0dbaae54b051",
        "title": "Reward-Guided Subspace Fusion and Spotlight for Multi-Agent State Space Exploration"
      },
      {
        "paperId": "47fccd1b5e3faa0e9184c8b401c3b3486717e697",
        "title": "Imagine, Verify, Execute: Memory-Guided Agentic Exploration with Vision-Language Models"
      },
      {
        "paperId": "26db19fe78c808fbed583016736dc40a7df9e20b",
        "title": "Exploration-Driven Generative Interactive Environments"
      },
      {
        "paperId": "c6a9edbfb61cf40cf9b3398491069e4fd7d66384",
        "title": "Unsupervised Representation Learning in Deep Reinforcement Learning: A Review"
      },
      {
        "paperId": "1f327387a82b973518408d5f0c09843688661f2e",
        "title": "Model-Based Exploration in Monitored Markov Decision Processes"
      },
      {
        "paperId": "a45e59f6586a9e43ac69658abb2b90d678a1ce5a",
        "title": "Episodic Novelty Through Temporal Distance"
      },
      {
        "paperId": "23d29d67af61dfdad2a5708d1e5505b2bd542cef",
        "title": "VCSAP: Online reinforcement learning exploration method based on visitation count of state-action pairs"
      },
      {
        "paperId": "2a8017583a38e36e99f0f805ad4bb9037029de49",
        "title": "Representational similarity modulates neural and behavioral signatures of novelty"
      },
      {
        "paperId": "b3fc201f3b149ba1ec0b491673722fafea654458",
        "title": "PreND: Enhancing Intrinsic Motivation in Reinforcement Learning through Pre-trained Network Distillation"
      },
      {
        "paperId": "da4cd9a5eb032aed42f3fc39626b27bf0a9e2257",
        "title": "Life, uh, Finds a Way: Hyperadaptability by Behavioral Search"
      },
      {
        "paperId": "9b001d73ff9804370c29cf57f20eeaacf334bcce",
        "title": "Improving Cooperation via Joint Intrinsic Motivation Exploration in Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "91876fbf6d3da5ac4e16de5ac53a46e3a3f2cc50",
        "title": "Highly Efficient Self-Adaptive Reward Shaping for Reinforcement Learning"
      },
      {
        "paperId": "819c6525743938908d79ef25f4bb804921075323",
        "title": "Robust Q-Learning for finite ambiguity sets"
      },
      {
        "paperId": "ce32e5dae73775c13b749a27fc53219912ab4b59",
        "title": "CMBE: Curiosity-driven Model-Based Exploration for Multi-Agent Reinforcement Learning in Sparse Reward Settings"
      },
      {
        "paperId": "f86367bd4e1eca13271068ecefd149cdc6d57b3d",
        "title": "Autoencoder Reconstruction Model for Long-Horizon Exploration"
      },
      {
        "paperId": "6bf550ce7f10a4347d448eb810a92e1a5cfffa4b",
        "title": "Beyond Optimism: Exploration With Partially Observable Rewards"
      },
      {
        "paperId": "769d8fdf6520c52e8767ee6d54f6417cf6e7904e",
        "title": "RLeXplore: Accelerating Research in Intrinsically-Motivated Reinforcement Learning"
      },
      {
        "paperId": "5059dd54b9add9e0a27af4e5f85a4683eaf7b99f",
        "title": "Nuclear Norm Maximization-Based Curiosity-Driven Reinforcement Learning"
      },
      {
        "paperId": "e4885f2498ed24d258fd208e041181d226d6e90d",
        "title": "Dynamic Memory-Based Curiosity: A Bootstrap Approach for Exploration in Reinforcement Learning"
      },
      {
        "paperId": "87ddd7811eddfa67609f4ff8d10fb2f6ba42d94f",
        "title": "Exploration and Anti-Exploration with Distributional Random Network Distillation"
      },
      {
        "paperId": "91edf9d897ddfa4c39f013d6acc0f2a0905ef789",
        "title": "OVD-Explorer: Optimism Should Not Be the Sole Pursuit of Exploration in Noisy Environments"
      },
      {
        "paperId": "8536ac41553421c0cd0ef59882e8e7252aa7d895",
        "title": "Backtracking Exploration for Reinforcement Learning"
      },
      {
        "paperId": "fb0e167cfcb5bebd7aaeace128c40cbed47509f6",
        "title": "Never Explore Repeatedly in Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "232902398c8402c6dcd96e7e0920814ef8dfeb8a",
        "title": "Model-based Offline Reinforcement Learning with Count-based Conservatism"
      },
      {
        "paperId": "3b52e4252b06380d2bf8129a5f38aa34b458d4f8",
        "title": "Pseudo Value Network Distillation for High-Performance Exploration"
      },
      {
        "paperId": "1c592b81a8661f21008b41d8391849ce4c886ccc",
        "title": "Mnemonic Dictionary Learning for Intrinsic Motivation in Reinforcement Learning"
      },
      {
        "paperId": "d16feddfeca2617ca2127b7d134ceb78ce8a8b40",
        "title": "A Study of Global and Episodic Bonuses for Exploration in Contextual MDPs"
      },
      {
        "paperId": "4da7c38585ad044f62914a7411cb1878c7df7c4e",
        "title": "Self-supervised network distillation: An effective approach to exploration in sparse reward environments"
      },
      {
        "paperId": "adeea51d2851c60bce90bbc9e34540180900188c",
        "title": "Selective Uncertainty Propagation in Offline RL"
      },
      {
        "paperId": "528eaca08fac125c5c65c6d5d3f636d92ebd0517",
        "title": "Automatic Intrinsic Reward Shaping for Exploration in Deep Reinforcement Learning"
      },
      {
        "paperId": "b065c42b1f40368ff1dacb950c4817a4d18b3ed9",
        "title": "Foundation Models for Semantic Novelty in Reinforcement Learning"
      },
      {
        "paperId": "1f0c593cb5530661a538d7e857797dacd57cdd83",
        "title": "LECO: Learnable Episodic Count for Task-Specific Intrinsic Reward"
      },
      {
        "paperId": "0351a103915d2f10d1915b4892a988d9b15df406",
        "title": "Exploration via Elliptical Episodic Bonuses"
      },
      {
        "paperId": "c64a99ec780d5e1c6ba242a82e72e96eb0c3ef8a",
        "title": "An Information-Theoretic Perspective on Intrinsic Motivation in Reinforcement Learning: A Survey"
      },
      {
        "paperId": "ae412e9981d6bc9ffa22c543a2fca4d8312c2986",
        "title": "Rewarding Episodic Visitation Discrepancy for Exploration in Reinforcement Learning"
      },
      {
        "paperId": "0c9b5412bcef781b001222a8952c104af84889f5",
        "title": "Self-supervised Sequential Information Bottleneck for Robust Exploration in Deep Reinforcement Learning"
      },
      {
        "paperId": "f54fd65a4a8fa0b03290a5de4b0be79f545aced4",
        "title": "Unsupervised Representation Learning in Deep Reinforcement Learning: A Review"
      },
      {
        "paperId": "89ae946f74e75d1e7a9ca5d9d44c0c610e70d41b",
        "title": "Self-Supervised Exploration via Temporal Inconsistency in Reinforcement Learning"
      },
      {
        "paperId": "45f7c8a7cd577def78fbfff59bb802c67669da95",
        "title": "Turning Zeroes into Non-Zeroes: Sample Efficient Exploration with Monte Carlo Graph Search"
      },
      {
        "paperId": "5c0b56d9440ea70c79d1d0032d46c14e06f541f5",
        "title": "Reactive Exploration to Cope with Non-Stationarity in Lifelong Reinforcement Learning"
      },
      {
        "paperId": "395d89ba308890901ad19905f7556cab0a1a6a27",
        "title": "Towards Understanding How Machines Can Learn Causal Overhypotheses"
      },
      {
        "paperId": "ee5b628d998cf4d532f0607e4e2568246f42027b",
        "title": "Nuclear Norm Maximization Based Curiosity-Driven Learning"
      },
      {
        "paperId": "0b8ef454e35c9fb6c3c6050d2d79e295ff2a7ac7",
        "title": "Count-Based Exploration via Embedded State Space for Deep Reinforcement Learning"
      },
      {
        "paperId": "baf2722c7a28912a6eb57ede95ae72509102c596",
        "title": "Sampling diversity driven exploration with state difference guidance"
      },
      {
        "paperId": "f60dd378987c05e54b577c857012b815744c2951",
        "title": "Learning Attentional and Gated Communication via Curiosity"
      },
      {
        "paperId": "1bf46fd55008c3fe2dd531c5cdb97dceafd6b217",
        "title": "Semantic Exploration from Language Abstractions and Pretrained Representations"
      },
      {
        "paperId": "5eacd2d983270690b1cecfaeab34998fbc78400b",
        "title": "Improving the exploration efficiency of DQNs via the confidence bound methods"
      },
      {
        "paperId": "808dd538f0970668a48a611a5627e0babd36740b",
        "title": "Intrinsically-Motivated Reinforcement Learning: A Brief Introduction"
      },
      {
        "paperId": "da1e404ae194dcbcce198f5335f49f090403bbd7",
        "title": "Exploration in Deep Reinforcement Learning: A Survey"
      },
      {
        "paperId": "026dc8d3cbb360bdd12d19c924bc633221c9b423",
        "title": "Learning Causal Overhypotheses through Exploration in Children and Computational Models"
      },
      {
        "paperId": "ba9b7f2998002529061532486226496883e86e30",
        "title": "Improving Intrinsic Exploration with Language Abstractions"
      },
      {
        "paperId": "6f07784838ac8847c94e038ed5c288013c372dba",
        "title": "CIExplore: Curiosity and Influence-based Exploration in Multi-Agent Cooperative Scenarios with Sparse Rewards"
      },
      {
        "paperId": "c4d91bca6066282da421671ab06ac1e156f19851",
        "title": "Anti-Concentrated Confidence Bonuses for Scalable Exploration"
      },
      {
        "paperId": "718d9ecaa866109abbe4a9b97c522c6030b06929",
        "title": "Exploring More When It Needs in Deep Reinforcement Learning"
      },
      {
        "paperId": "12075ea34f5fbe32ec5582786761ab34d401209b",
        "title": "Exploration in Deep Reinforcement Learning: From Single-Agent to Multiagent Domain"
      },
      {
        "paperId": "3be84e24b144541a8cd9030526ef2b8ef2cbfe54",
        "title": "A Survey of Exploration Methods in Reinforcement Learning"
      },
      {
        "paperId": "bc18f1ec5209f8ebacb91eff42927a33664bc1a2",
        "title": "Influence-based Reinforcement Learning for Intrinsically-motivated Agents"
      },
      {
        "paperId": "694cac5ed2db350922d404cea214ab5f38a1e3ee",
        "title": "Goal-driven active learning"
      },
      {
        "paperId": "e7735be4f0fc427515fd7206ebc714356b97e71a",
        "title": "The Benchmark Lottery"
      },
      {
        "paperId": "9db6f170fcd0b6aa0e6f41caa07cb2202130b0b5",
        "title": "Novelty and MCTS"
      },
      {
        "paperId": "6f0d0c10741dda1622d25104b09daa5e815373ec",
        "title": "Importance Sampling based Exploration in Q Learning"
      },
      {
        "paperId": "ac5a4ac3852aae49c0819fe8383dd4cedd514499",
        "title": "A Max-Min Entropy Framework for Reinforcement Learning"
      },
      {
        "paperId": "f4153ad13fffc3b587b3249914d08477f8258b30",
        "title": "Novelty is not surprise: Human exploratory and adaptive behavior in sequential decision-making"
      },
      {
        "paperId": "0f4f6d1ee869ed496534d7eda51ac0f0110ce3fc",
        "title": "Don't Do What Doesn't Matter: Intrinsic Motivation with Action Usefulness"
      },
      {
        "paperId": "a188c3b58e71657ecfcddc37359aca51213e2187",
        "title": "Rank the Episodes: A Simple Approach for Exploration in Procedurally-Generated Environments"
      },
      {
        "paperId": "376e0853411acb4e5732c587471d0a3910689b20",
        "title": "Adapting Behavior via Intrinsic Reward: A Survey and Empirical Study"
      },
      {
        "paperId": "3f1988c5399dc501805bed7ecfec491383386b18",
        "title": "Novelty is not Surprise: Exploration and learning in human sequential decision-making"
      },
      {
        "paperId": "a2b00e570440619bbe6a483df6b0da46b1aae665",
        "title": "Fast and slow curiosity for high-level exploration in reinforcement learning"
      },
      {
        "paperId": "5f978b3829acad6ac8b3372d2fa8d38a45b96d3d",
        "title": "Noisy Agents: Self-supervised Exploration by Predicting Auditory Events"
      },
      {
        "paperId": "5f4e973204815f10372a47d7ff742b0b01935cca",
        "title": "Learning Intrinsically Motivated Options to Stimulate Policy Exploration"
      },
      {
        "paperId": "a13e1cb07cc35666b5a5c384c92416126f87fee2",
        "title": "Diversity Actor-Critic: Sample-Aware Entropy Regularization for Sample-Efficient Exploration"
      },
      {
        "paperId": "5b6b2ce6e809d8f54416a3d04e0ef474094be779",
        "title": "Exploring Exploration: Comparing Children with RL Agents in Unified Environments"
      },
      {
        "paperId": "616ac6772508e72084360158078058dfa8bda7e7",
        "title": "First return, then explore"
      },
      {
        "paperId": "6d5acdfa087409a2dd59c425cb6c68030f90b33b",
        "title": "Exploring Unknown States with Action Balance"
      },
      {
        "paperId": "bebe8ffb0c357ac0c7eea2556f817b03ee22b570",
        "title": "RIDE: Rewarding Impact-Driven Exploration for Procedurally-Generated Environments"
      },
      {
        "paperId": "0e54edd0c55c0cb63e719b501e41790c75a2c73a",
        "title": "Exploration-exploitation dilemma in Reinforcement Learning under various form of prior knowledge. (Impact des connaissances a priori sur le compromis exploration-exploitation en apprentissage par renforcement)"
      },
      {
        "paperId": "895735cace0de940aa647dbafc046b7f30316fe5",
        "title": "A survey on intrinsic motivation in reinforcement learning"
      },
      {
        "paperId": "855112d5c051616353f11180b9801c0ee09f2fa5",
        "title": "Deriving Subgoals Autonomously to Accelerate Learning in Sparse Reward Domains"
      },
      {
        "paperId": "12e9394725a085a89114d2c981ce4a76ec6f37cf",
        "title": "Efficient and Scalable Exploration via Estimation-Error"
      },
      {
        "paperId": "c6011b09783150bcda711c8d3176385875020a60",
        "title": "\u00c9tude de la motivation intrins\u00e8que en apprentissage par renforcement"
      },
      {
        "paperId": "c6b8d35da220d274b561bc0affa12810990e720a",
        "title": "Adapting Behaviour via Intrinsic Reward: A Survey and Empirical Study"
      },
      {
        "paperId": "81768cdace11e14982d3aba9060059e1133d83fc",
        "title": "Learning Efficient and Effective Exploration Policies with Counterfactual Meta Policy"
      },
      {
        "paperId": "47231dec371ac92a5caabf64508ed5332cf7e4f8",
        "title": "Beyond Exponentially Discounted Sum: Automatic Learning of Return Function"
      },
      {
        "paperId": "edcd3a5e8e0e6fd27f95c34348404ea449b6927d",
        "title": "Mega-Reward: Achieving Human-Level Play without Extrinsic Rewards"
      },
      {
        "paperId": "bd29ddb1ee5f1bb1a728e72109dec472359c1c29",
        "title": "Deep Policies for Width-Based Planning in Pixel Domains"
      },
      {
        "paperId": "b8c0071c74e04ea1598ed2a208cbc255656f50b0",
        "title": "A Meta-MDP Approach to Exploration for Lifelong Reinforcement Learning"
      },
      {
        "paperId": "c520bf47db3360ae3a52219771390a354ed8a91f",
        "title": "Go-Explore: a New Approach for Hard-Exploration Problems"
      },
      {
        "paperId": "93ed1bfd12ddd23f7bf9410774cafac204189d56",
        "title": "Visual novelty, curiosity, and intrinsic reward in machine learning and the brain"
      },
      {
        "paperId": "cf4a4d18da4aa335cd18e095bdfff21290afebb0",
        "title": "Malthusian Reinforcement Learning"
      },
      {
        "paperId": "f06b33e59ff17d891604fb5f0f81831819b9da64",
        "title": "Exploration Bonus for Regret Minimization in Undiscounted Discrete and Continuous Markov Decision Processes"
      },
      {
        "paperId": "bf02f308c752140ef0fe16c20829a64ea2a4d3f9",
        "title": "Context-Dependent Upper-Confidence Bounds for Directed Exploration"
      },
      {
        "paperId": "72a75aeab2c918394dc9af2408fd8e1076ae39ac",
        "title": "Successor Uncertainties: exploration and uncertainty in temporal difference learning"
      },
      {
        "paperId": "bf604ae3ddd5adec55554921b37f04035b7350a7",
        "title": "Contingency-Aware Exploration in Reinforcement Learning"
      },
      {
        "paperId": "74e12851de2d542aa2aef7b8a39ef021a5802689",
        "title": "Composing Complex Skills by Learning Transition Policies"
      },
      {
        "paperId": "9f67b3edc67a35c884bd532a5e73fa3a7f3660d8",
        "title": "Count-Based Exploration with the Successor Representation"
      },
      {
        "paperId": "73abd0733e0125c123e6a0b23472772a9c343e5f",
        "title": "Region Growing Curriculum Generation for Reinforcement Learning"
      },
      {
        "paperId": "c52dddfbb4a3af4cc5e72849fe965c62801539e7",
        "title": "Counting to Explore and Generalize in Text-based Games"
      },
      {
        "paperId": "1876d84a53ffc173331d6a4f9f1b3172fea2442c",
        "title": "Improving width-based planning with compact policies"
      },
      {
        "paperId": "e3abf4ee7448c051833981cc0e5347a7af5cb3f3",
        "title": "Between Progress and Potential Impact of AI: the Neglected Dimensions"
      },
      {
        "paperId": "427b05d77d56a1047fcc12b66e4aa507a99bafa8",
        "title": "State Distribution-Aware Sampling for Deep Q-Learning"
      },
      {
        "paperId": "7056e205e6829b3dc789ec35eac82f6e31923c2c",
        "title": "Exploration in Feature Space for Reinforcement Learning"
      },
      {
        "paperId": "3b290ffa1f4f8226e326f00984acecdfbe9e28bf",
        "title": "Revisiting the Arcade Learning Environment: Evaluation Protocols and Open Problems for General Agents"
      },
      {
        "paperId": "6b71505aadb185a5a671e0a0003a56f1aaaad9a8",
        "title": "Hashing Over Predicted Future Frames for Informed Exploration of Deep Reinforcement Learning"
      },
      {
        "paperId": "d1627e5dd7c6656aa8c16d861677ac631c5c4301",
        "title": "Universal Reinforcement Learning Algorithms: Survey and Experiments"
      },
      {
        "paperId": "54cb726595cd8c03f59f49c9a97b76b4d3f932f2",
        "title": "Rethinking Exploration in Reinforcement Learning with Effective Metric-Based Exploration Bonus"
      },
      {
        "paperId": "4956e36809a74f2bd3b8018153bccbf19f7a552f",
        "title": "Deep Reinforcement Learning for Autonomous Cyber Operations: A Survey"
      },
      {
        "paperId": "50ae7b1b47dc00ae2d75b946e262fd5d584e47c9",
        "title": "Modern Value Based Reinforcement Learning: A Chronological Review"
      },
      {
        "paperId": "1c0465b2ad51e261cab27990353f2002d96a389c",
        "title": "An information-theoretic perspective on intrinsic motivation in reinforcement learning: a survey"
      },
      {
        "paperId": "9ade87e97647c20b59894250c84fb4be85941449",
        "title": "Adapting Behaviour via Intrinsic Reward"
      },
      {
        "paperId": "eeb19f4f8fbcc82dc604f7f7f5a4ce83e7224fab",
        "title": "Intrinsic Motivated Multi-Agent Communication"
      },
      {
        "paperId": "2e5dc4b028e670d1b9ac0b4e15580cdb7197cb68",
        "title": "Exploration in Deep Reinforcement Learning: A Comprehensive Survey"
      },
      {
        "paperId": "128d70ab6bbbd181ab562bf337091f72a709ead4",
        "title": "Intrinsic Motivation Model Based on Reward Gating"
      },
      {
        "paperId": "a1a2accfe05a257dcfeaebc56114f27cf37de82f",
        "title": "Deriving Subgoals Using Network Distillation"
      },
      {
        "paperId": "99846049e6836bbc2189703406f92d91e01b9557",
        "title": "N OISY A GENTS : S ELF - SUPERVISED E XPLORATION BY P REDICTING A UDITORY E VENTS"
      },
      {
        "paperId": "4e50d52f0b59561e3902b997ce8a48633351491e",
        "title": "An Empirical Study of Exploration Strategies for Model-Free Reinforcement Learning"
      },
      {
        "paperId": "daf93749171e3575cca5b584a4146b247692c86f",
        "title": "Learning and planning in videogames via task decomposition"
      },
      {
        "paperId": "e37260307362b869c025d170c5833453e3d1e4ea",
        "title": "An Empirical and Conceptual Categorization of Value-based Exploration Methods"
      },
      {
        "paperId": "6222c8b38521e38ebbe6ea4868d4aaafa619a335",
        "title": "Exploration Bonus for Regret Minimization in Discrete and Continuous Average Reward MDPs"
      },
      {
        "paperId": "11900ac111b5d50fc7a6d5c5f1d8f4fb0c009b22",
        "title": "Efficient and Robust Learning on Elaborated Gaits with Curriculum Learning"
      },
      {
        "paperId": "77a31a4601444a3f7aeed15061b08684d0bea92b",
        "title": "Exploration-Exploitation Trade-off in Deep Reinforcement Learning"
      },
      {
        "paperId": "69ee223f249537e484cd1d4186ed1926dca98a76",
        "title": "Ego-Networks"
      },
      {
        "paperId": "8b1e149ae23ea7839c9e3a2bd063c354ff7075d0",
        "title": "Towards Safe Artificial General Intelligence"
      },
      {
        "paperId": "6f2d49eb5df124e374a78836ea1d4a398ee4a0a2",
        "title": "LiFE: Deep Exploration via Linear-Feature Bonus in Continuous Control"
      },
      {
        "paperId": "a09104ea6439e6039c2db641bbc9f344b04124d7",
        "title": "Making Something Out of Nothing: Monte Carlo Graph Search in Sparse Reward Environments"
      },
      {
        "paperId": "d90759d8495302b30e8b67bd259b79df0e61e28d",
        "title": "Digital Commons@Georgia Southern Digital Commons@Georgia Southern"
      }
    ],
    "score": 15.75
  },
  {
    "id": "468a3e85a2da0908c6f371ad83f6bcf6b6fdf193",
    "title": "Adaptive Informative Path Planning Using Deep Reinforcement Learning for UAV-based Active Sensing",
    "authors": [
      "Julius R\u00fcckin",
      "Liren Jin",
      "Marija Popovic"
    ],
    "year": 2021,
    "citationCount": 63,
    "abstract": "Aerial robots are increasingly being utilized for environmental monitoring and exploration. However, a key challenge is efficiently planning paths to maximize the information value of acquired data as an initially unknown environment is explored. To address this, we propose a new approach for informative path planning based on deep reinforcement learning (RL). Combining recent advances in RL and robotic applications, our method combines tree search with an offline-learned neural network predicting informative sensing actions. We introduce several components making our approach applicable for robotic tasks with high-dimensional state and large action spaces. By deploying the trained network during a mission, our method enables sample-efficient online replanning on platforms with limited computational resources. Simulations show that our approach performs on par with existing methods while reducing runtime by 8-10\u00d7. We validate its performance using real-world surface temperature data.",
    "url": "https://www.semanticscholar.org/paper/468a3e85a2da0908c6f371ad83f6bcf6b6fdf193",
    "pdf_url": "https://arxiv.org/pdf/2109.13570.pdf",
    "venue": "IEEE International Conference on Robotics and Automation",
    "publicationDate": "2021-09-28",
    "externalIds": {
      "ArXiv": "2109.13570",
      "DBLP": "conf/icra/RuckinJP22",
      "DOI": "10.1109/icra46639.2022.9812025",
      "CorpusId": 238198483
    },
    "references": [
      {
        "paperId": "98d282579593987aa13c17717b12207a3af86c68",
        "title": "Autonomous Exploration Under Uncertainty via Deep Reinforcement Learning on Graphs"
      },
      {
        "paperId": "f7bd0dbffe7764807ef6e3a7ed6c3cd6bbcb6131",
        "title": "Informative Path Planning for Anomaly Detection in Environment Exploration and Monitoring"
      },
      {
        "paperId": "a810c51986e6ef28ecc405e0a347d664862b23b5",
        "title": "Adaptive Informative Path Planning with Multimodal Sensing"
      },
      {
        "paperId": "c39fb7a46335c23f7529dd6f9f980462fd38653a",
        "title": "Mastering Atari, Go, chess and shogi by planning with a learned model"
      },
      {
        "paperId": "194b6f8b9fe95717d0c2c94ba3336663df601e4b",
        "title": "DeepIG: Multi-Robot Information Gathering With Deep Reinforcement Learning"
      },
      {
        "paperId": "f244ffb549a61806d00f614e70fa1c3fbe5fffc6",
        "title": "Accelerating Self-Play Learning in Go"
      },
      {
        "paperId": "b45e2cc10dd98fc97dff7497ffb9bcf855d3c66b",
        "title": "Informative Path Planning for Active Field Mapping under Localization Uncertainty"
      },
      {
        "paperId": "cc13c12259d70d6ef09e38ce6cdfcf3d23db6d86",
        "title": "Deep Reinforcement Learning Robot for Search and Rescue Applications: Exploration in Unknown Cluttered Environments"
      },
      {
        "paperId": "f9717d29840f4d8f1cc19d1b1e80c5d12ec40608",
        "title": "A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play"
      },
      {
        "paperId": "551408f9022b45716842afef9efd741c6635dfb3",
        "title": "An informative path planning framework for UAV-based terrain monitoring"
      },
      {
        "paperId": "526c702605cf2a13405fbddc2cea556fbd4cf98a",
        "title": "UAV route planning for active disease classification"
      },
      {
        "paperId": "9f2fb6899e046e1f42792ba6f22b0877149062eb",
        "title": "A disciplined approach to neural network hyper-parameters: Part 1 - learning rate, batch size, momentum, and weight decay"
      },
      {
        "paperId": "811df72e210e20de99719539505da54762a11c6d",
        "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor"
      },
      {
        "paperId": "c27db32efa8137cbf654902f8f728f338e55cd1c",
        "title": "Mastering the game of Go without human knowledge"
      },
      {
        "paperId": "15b1661cbc70236140cfe221fe09c51eaf9fadd0",
        "title": "The Effects of Memory Replay in Reinforcement Learning"
      },
      {
        "paperId": "dcd4bf5124c4813388dca713df2a6e8c568c0a9d",
        "title": "Online Algorithms for POMDPs with Continuous State, Action, and Observation Spaces"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "64e277012cadb7b7d0894f21f6f2bcd67994cfb5",
        "title": "Adaptive continuous\u2010space informative path planning for online environmental monitoring"
      },
      {
        "paperId": "0b2d197f90f3bce607a4465ea7312995f024a1a0",
        "title": "Sampling-based robotic information gathering algorithms"
      },
      {
        "paperId": "a9c7f3a32865088a1113ba415178cbd14db2b663",
        "title": "A survey on coverage path planning for robotics"
      },
      {
        "paperId": "80fe90592097d53ff1cfe79e8dd467cf3ab3cb21",
        "title": "Active Learning for Level Set Estimation"
      },
      {
        "paperId": "6cccdf3b5ad455d1703c1aafb48688b903e0cf3e",
        "title": "Branch and bound for informative path planning"
      },
      {
        "paperId": "9364c99f98a9f9380468f8496b118c90ae7c3424",
        "title": "Robots for Environmental Monitoring: Significant Advancements and Applications"
      },
      {
        "paperId": "c37f1baac3c8ba30250084f067167ac3837cf6fd",
        "title": "A Survey of Monte Carlo Tree Search Methods"
      },
      {
        "paperId": "87ef59eccd797bdc1405b2f4da05f452a11c004f",
        "title": "A Comparative Study of CMA-ES on Large Scale Global Optimisation"
      },
      {
        "paperId": "f115acbd94451f58a5bc5062c1d7e6707e5be1f0",
        "title": "Monte-Carlo Planning in Large POMDPs"
      },
      {
        "paperId": "11fa695a4e6b0f20a396edc4010d4990c8d29fe9",
        "title": "Monte-Carlo Tree Search: A New Framework for Game AI"
      },
      {
        "paperId": "3436e3aa37ec66ad42daa957893530012fb3ef16",
        "title": "Assessment of Unmanned Aerial Vehicles Imagery for Quantitative Monitoring of Wheat Crop in Small Plots"
      },
      {
        "paperId": "c6dffdbef653b95a127caf50a5a08dabeff64cfd",
        "title": "A UAV Search and Rescue Scenario with Human Body Detection and Geolocalization"
      },
      {
        "paperId": "2246a5ac64fa7607b5dea036f4c6e4793cf8e507",
        "title": "Global A-Optimal Robot Exploration in SLAM"
      },
      {
        "paperId": "ed8a65d9f23b70f8ff37da5e119819f53b448751",
        "title": "An Exact Algorithm for Maximum Entropy Sampling"
      },
      {
        "paperId": "21b58c8aba44c173493e418a797a1f36c6dae8a9",
        "title": "ERFNet: Efficient Residual Factorized ConvNet for Real-Time Semantic Segmentation"
      }
    ],
    "cited_by": [
      {
        "paperId": "754ce9d6eb954a7476f5f38753007bfd6ff84c0f",
        "title": "Improved BFS-based path planning algorithm with finite time generalized suboptimal search incorporating fixed-wing UAV flight constraints for complex low-altitude airspace"
      },
      {
        "paperId": "d210b8381b57b96636e099ec9b7657121a36f655",
        "title": "Look as You Leap: Planning Simultaneous Motion and Perception for High-DOF Robots"
      },
      {
        "paperId": "48bdca86ddbe843fdb01c68f7d478257b9b3c257",
        "title": "IRIS: An Information Path Planning Method Based on Reinforcement Learning and Information-Directed Sampling"
      },
      {
        "paperId": "c09a31f17996fed4ee7192fa348dbdae72f32d66",
        "title": "A two stage-based approach for swarm UAVs landing in unknown 3D environments"
      },
      {
        "paperId": "f04517cd932e1812944694e8204fea4a663ed26b",
        "title": "Learning Agile Tensile Perching for Aerial Robots from Demonstrations"
      },
      {
        "paperId": "89688b1c40ce021039fddb999cccb9413ed8f2ba",
        "title": "Maximizing Aerial Detection of Organic Objects in Non-Exhaustively Searchable Survey Areas"
      },
      {
        "paperId": "297a3c69d0a631188071773d18c90ec528a44fcb",
        "title": "Attention-based Learning for 3D Informative Path Planning"
      },
      {
        "paperId": "a492350a05c1c46274e32781a922c4d315c67c92",
        "title": "Drone Path Planning Based on Policy in the Latent Action Space with Perturbation and Conservative Q-Learning"
      },
      {
        "paperId": "10b9ab294a9b696385102f84a25a7767c6ea68b7",
        "title": "Fully Differentiable Adaptive Informative Path Planning"
      },
      {
        "paperId": "c669c42beab213a3f0039ec2068a2f9a2903e542",
        "title": "Expert-guided Path Planning Based on RRT-star and Deep Reinforcement Learning"
      },
      {
        "paperId": "0aa9d3efffbbdc4e019b6196f86cb03f74146783",
        "title": "PREUS: Proactive and Robust Edge-UAV Systems for Autonomous Monitoring in Dynamic Environments"
      },
      {
        "paperId": "0f45258b378610bc09fe809c9c407adc3480fd3b",
        "title": "Towards Fully Automated Decision-Making Systems for Greenhouse Control: Challenges and Opportunities"
      },
      {
        "paperId": "193bae34c6a9676654ce211816172fe072ffc9f0",
        "title": "IA-TIGRIS: An Incremental and Adaptive Sampling-Based Planner for Online Informative Path Planning"
      },
      {
        "paperId": "7b14edcd7b0c55ba2b4ec65f12578670faf87508",
        "title": "Optimal Coupled Sensor Placement and Path-Planning in Unknown Time-Varying Environments"
      },
      {
        "paperId": "d26f39acb622934f54c039cd3293ad9e7a6c4777",
        "title": "Informative Path Planning based on Global and Local Active Sensing for Environmental Field Mapping"
      },
      {
        "paperId": "94726930e2074fac85352de65917040abc9f8175",
        "title": "Towards Map-Agnostic Policies for Adaptive Informative Path Planning"
      },
      {
        "paperId": "b5b9010a81bd3d22f162a49eac22f681106249b8",
        "title": "DyPNIPP: Predicting Environment Dynamics for RL-based Robust Informative Path Planning"
      },
      {
        "paperId": "aa07e4660e1637e90c06fcf12bc2dcee53df976a",
        "title": "OffRIPP: Offline RL-based Informative Path Planning"
      },
      {
        "paperId": "767b711ae85b8f51b83ff89ea6fa07d053ad4879",
        "title": "Multi-UAV active sensing for precision agriculture via Bayesian fusion"
      },
      {
        "paperId": "8b9f3d1c27eea8409be3514e72f01d884cd9195e",
        "title": "SOScheduler: Toward Proactive and Adaptive Wildfire Suppression via Multi-UAV Collaborative Scheduling"
      },
      {
        "paperId": "76c2eec0c7f2f3c33307448161e7fb964989f075",
        "title": "Coupled Sensor Configuration and Planning in Unknown Dynamic Environments with Context-Relevant Mutual Information-based Sensor Placement"
      },
      {
        "paperId": "d921e237f1534ce6f777dceb2f1ab281d63e9d5e",
        "title": "An Active Search Strategy with Multiple Unmanned Aerial Systems for Multiple Targets"
      },
      {
        "paperId": "66baf80896cd23e317f00c78f04cc29db647717f",
        "title": "Hierarchical Informative Path Planning for Active Sensing and Field Reconstruction"
      },
      {
        "paperId": "f2369129c50869b857c73909eaeac78e8da1c465",
        "title": "Trajectory Optimization for Adaptive Informative Path Planning with Multimodal Sensing"
      },
      {
        "paperId": "5a43a8b91973777bb78a19b0ed7f0a3011c4da19",
        "title": "Learning-based methods for adaptive informative path planning"
      },
      {
        "paperId": "f0ac1895b8c132e5776ca8141d84e8932c2460c3",
        "title": "Progressive Hierarchical Deep Reinforcement Learning for defect wafer test"
      },
      {
        "paperId": "9b9615790ae08a6f2992ba9fde2f0f37378b7ff1",
        "title": "A survey on autonomous environmental monitoring approaches: towards unifying active sensing and reinforcement learning"
      },
      {
        "paperId": "8d4619229ae6a6d79664638c8519f328e665f8f0",
        "title": "Iterative Risk Aware PRM Path Planning Algorithm for Autonomous Unknown Environments Exploration"
      },
      {
        "paperId": "5c7e665c9b6ffcfd19a2dfbe58669e1eae3fd330",
        "title": "Approximate Sequential Optimization for Informative Path Planning"
      },
      {
        "paperId": "52b246303dc2f56012b4eeb42adf5514924ed18a",
        "title": "Deep Reinforcement Learning With Dynamic Graphs for Adaptive Informative Path Planning"
      },
      {
        "paperId": "eab706d04cadcdd65d120108cb8c055d91a1a375",
        "title": "Invariant Representations Learning with Future Dynamics"
      },
      {
        "paperId": "b9232e9fc02b9eea96cb6a4804cdfce77dcfe56f",
        "title": "Autonomous navigation of mobile robots in unknown environments using off-policy reinforcement learning with curriculum learning"
      },
      {
        "paperId": "3816b1e75fc568bdc039c85ec3e8de8ed1d2c04e",
        "title": "Uncertainty-aware visually-attentive navigation using deep neural networks"
      },
      {
        "paperId": "4b1941b1e0c63037e8a4b97a32a1b837e9c8a249",
        "title": "Overcome the Fear Of Missing Out: Active Sensing UAV Scanning for Precision Agriculture"
      },
      {
        "paperId": "a9d519799ab955f18b5c61e0476314b2e932d497",
        "title": "Semi-Supervised Active Learning for Semantic Segmentation in Unknown Environments Using Informative Path Planning"
      },
      {
        "paperId": "f51f5d8a0d7c08f8973ec671209c56f981f27000",
        "title": "Multi-Robot Heterogeneous Adversarial Coverage"
      },
      {
        "paperId": "f395042515a47b771871d428a89ed1e18aec8423",
        "title": "Gradient-based Local Next-best-view Planning for Improved Perception of Targeted Plant Nodes"
      },
      {
        "paperId": "8586a11c17c088eddc585f32ad434bb34989b921",
        "title": "UAV Agile Navigation Method for Unknown Environment via Deep Reinforcement Learning"
      },
      {
        "paperId": "d07130abf304b3deb555ff4af78de681640d2a64",
        "title": "SoRTS: Learned Tree Search for Long Horizon Social Robot Navigation"
      },
      {
        "paperId": "1d62d2258be61963a6004154f5cdd90539e8ef23",
        "title": "Multi-Robot Informative Path Planning from Regression with Sparse Gaussian Processes"
      },
      {
        "paperId": "76e69e1e92cd3a702835ead2efb027247e379970",
        "title": "Informative Path Planning for Scalar Dynamic Reconstruction Using Coregionalized Gaussian Processes and a Spatiotemporal Kernel"
      },
      {
        "paperId": "73388fe1b01cc62c93ec9351d6beec5a804b06c8",
        "title": "Deep Reinforcement Learning Integrated RRT Algorithm for Path Planning"
      },
      {
        "paperId": "c53ba14427710624c7200aeffa3a0e455da593d3",
        "title": "Informative Trajectory Planning Using Reinforcement Learning for Minimum-Time Exploration of Spatiotemporal Fields"
      },
      {
        "paperId": "bc8f4d071120da036605b6fbe8f9ad44d0d9d693",
        "title": "Decision-Theoretic Approaches for Robotic Environmental Monitoring - A Survey"
      },
      {
        "paperId": "3fe17292a9bb15340a5dcd675b58625f4e7f2086",
        "title": "An Informative Path Planning for Multi-granularity Collaborative Search via Multiple UAVs"
      },
      {
        "paperId": "5a2cc5d3171fc88b0b868cf3595747d3d5d50df7",
        "title": "Adaptive Robotic Information Gathering via non-stationary Gaussian processes"
      },
      {
        "paperId": "3fc75313e994013a09aa718e000444be171b1d5e",
        "title": "Region coverage-aware path planning for unmanned aerial vehicles: A systematic review"
      },
      {
        "paperId": "8907315e8e04e8082a3c2529aa1e886eda2399cd",
        "title": "Intent-based Deep Reinforcement Learning for Multi-agent Informative Path Planning"
      },
      {
        "paperId": "a2e38c89d871c814dd6a4687362af3b94137d520",
        "title": "Learned Parameter Selection for Robotic Information Gathering"
      },
      {
        "paperId": "e5f6393836802966c02529293620e25afe08c78b",
        "title": "Multi-UAV Adaptive Path Planning Using Deep Reinforcement Learning"
      },
      {
        "paperId": "3b3db41a08ba2c95b796aa3486dd4f8ec6922710",
        "title": "Classical versus reinforcement learning algorithms for unmanned aerial vehicle network communication and coverage path planning: A systematic literature review"
      },
      {
        "paperId": "154754bc38f0ff5ae3ca595e9acb782301cd0851",
        "title": "Reinforcement Learning for Agile Active Target Sensing with a UAV"
      },
      {
        "paperId": "711837536188ca043539ad26e9d3f71aa488cccb",
        "title": "Information Map Prediction based on Learning Network for Reinforced Autonomous Exploration"
      },
      {
        "paperId": "3802a0e9c636bfd308ce03a2fd4a6e3a1e7911b2",
        "title": "Graph Neural Networks for Multi-Robot Active Information Acquisition"
      },
      {
        "paperId": "c7d985dc93c17460efeaac840047c4a0d1815b57",
        "title": "Sequential Bayesian Optimization for Adaptive Informative Path Planning with Multimodal Sensing"
      },
      {
        "paperId": "0becf0e32bb237e975633b2cfa6c2aebde2079e1",
        "title": "Feature Space Exploration For Planning Initial Benthic AUV Surveys"
      },
      {
        "paperId": "9dd9c6f4c9dcf2f58bd7852224b45dbe0f43e061",
        "title": "Informative Trajectory Planning for Air-Ground Cooperative Monitoring of Spatiotemporal Fields"
      },
      {
        "paperId": "5b7ba8e4250568d0400235ff9a413ac253a90625",
        "title": "Disaster-Aware Path Planning Based on Reinforcement Learning for Postearthquake Emergency Response"
      },
      {
        "paperId": "70c3339b68b5c9c1afb8dc3bb408802985b26deb",
        "title": "ViPER: Visibility-based Pursuit-Evasion via Reinforcement Learning"
      },
      {
        "paperId": "e55828461ae9894cde1a11810039b349ab6da008",
        "title": "CAtNIPP: Context-Aware Attention-based Network for Informative Path Planning"
      },
      {
        "paperId": "8732c4ab0c47c4f7b0c05bafbbb3311e53aa32ba",
        "title": "Robotics and Autonomous Systems"
      },
      {
        "paperId": "09910f215baa1351deca96644cac8c14a7f272db",
        "title": "Enfoque general y sistem\u00b4atico para percepci\u00b4on activa sem\u00b4antica en rob\u00b4otica"
      },
      {
        "paperId": "7472b2ce929ff48ff284032afa7eb04d7f1b4169",
        "title": "Towards Robust Informative Path Planning for Spatiotemporal Environments"
      }
    ],
    "score": 15.75
  },
  {
    "id": "535d184eadf47fa17ce4073b6e2f180783e85300",
    "title": "Curiosity-driven Exploration for Mapless Navigation with Deep Reinforcement Learning",
    "authors": [
      "Oleksii Zhelo",
      "Jingwei Zhang",
      "L. Tai",
      "Ming Liu",
      "Wolfram Burgard"
    ],
    "year": 2018,
    "citationCount": 108,
    "abstract": "This paper investigates exploration strategies of Deep Reinforcement Learning (DRL) methods to learn navigation policies for mobile robots. In particular, we augment the normal external reward for training DRL algorithms with intrinsic reward signals measured by curiosity. We test our approach in a mapless navigation setting, where the autonomous agent is required to navigate without the occupancy map of the environment, to targets whose relative locations can be easily acquired through low-cost solutions (e.g., visible light localization, Wi-Fi signal localization). We validate that the intrinsic motivation is crucial for improving DRL performance in tasks with challenging exploration requirements. Our experimental results show that our proposed method is able to more effectively learn navigation policies, and has better generalization capabilities in previously unseen environments. A video of our experimental results can be found at this https URL",
    "url": "https://www.semanticscholar.org/paper/535d184eadf47fa17ce4073b6e2f180783e85300",
    "pdf_url": "https://arxiv.org/pdf/1804.00456.pdf",
    "venue": "arXiv.org",
    "publicationDate": "2018-04-02",
    "externalIds": {
      "ArXiv": "1804.00456",
      "MAG": "2796447411",
      "DBLP": "journals/corr/abs-1804-00456",
      "CorpusId": 4568724
    },
    "references": [
      {
        "paperId": "4d694fd2d82c05eabdc5d0e3cd963e5a83fc11b8",
        "title": "Learning Sample-Efficient Target Reaching for Mobile Robots"
      },
      {
        "paperId": "bca91c27d811588409a6ce78853c593f64215bda",
        "title": "Towards Optimally Decentralized Multi-Robot Collision Avoidance via Deep Reinforcement Learning"
      },
      {
        "paperId": "c5324d9d6ed4ba9245741b34a3964e44c5ab02d2",
        "title": "Plugo: a VLC Systematic Perspective of Large-scale Indoor Localization"
      },
      {
        "paperId": "3ea6113b1dd5f43cb8c6ec79d648b934aad9d697",
        "title": "Neural SLAM"
      },
      {
        "paperId": "225ab689f41cef1dc18237ef5dab059a49950abf",
        "title": "Curiosity-Driven Exploration by Self-Supervised Prediction"
      },
      {
        "paperId": "799c0e461332570ecde97e13266fecde8476efe3",
        "title": "Virtual-to-real deep reinforcement learning: Continuous control of mobile robots for mapless navigation"
      },
      {
        "paperId": "1a0d50fd4a3e52b25c0a662b687daeb8ea963b4b",
        "title": "Deep reinforcement learning with successor features for navigation across similar environments"
      },
      {
        "paperId": "d35b05f440b5ba00d9429139edef7182bf9f7ce7",
        "title": "Learning to Navigate in Complex Environments"
      },
      {
        "paperId": "7af7f2f539cd3479faae4c66bbef49b0f66202fa",
        "title": "Target-driven visual navigation in indoor scenes using deep reinforcement learning"
      },
      {
        "paperId": "69e76e16740ed69f4dc55361a3d319ac2f1293dd",
        "title": "Asynchronous Methods for Deep Reinforcement Learning"
      },
      {
        "paperId": "65c540d9683b5422bbe6d837ef52d6271c8387a6",
        "title": "WiFi signal strength-based robot indoor localization"
      },
      {
        "paperId": "3d2218b17e7898a222e5fc2079a3f1531990708f",
        "title": "I and J"
      },
      {
        "paperId": "3f8d7bdfc3ed0ff793f1236730486b3d5cf946aa",
        "title": "Probabilistic robotics"
      },
      {
        "paperId": "12b03af504d0960334c77567dab38791bf0f739a",
        "title": "AND T"
      },
      {
        "paperId": "73f36ff3a6d340606e09d2d0091da27a13af7a6f",
        "title": "and as an in"
      }
    ],
    "cited_by": [
      {
        "paperId": "a555208cd9bad078efa92fd1a130ba7c3b4a550d",
        "title": "Sensing, Social, and Motion Intelligence in Embodied Navigation: A Comprehensive Survey"
      },
      {
        "paperId": "9933d3339e221f52b226f1a4cc421c478c1866eb",
        "title": "Hierarchical Deep Deterministic Policy Gradient for Autonomous Maze Navigation of Mobile Robots"
      },
      {
        "paperId": "1ad9eeced789731f6f4c7efc625ca5280494c63b",
        "title": "Enter the Void - Planning to Seek Entropy When Reward is Scarce"
      },
      {
        "paperId": "f101a0d74e37a995c6cb226e6851bbbd32671adc",
        "title": "A DRL Approach for Mapless Transportation of Arbitrary Objects"
      },
      {
        "paperId": "505c656e7a28e80fe8429ae5fa76ca85e1453506",
        "title": "Deep Reinforcement Learning for Localisability\u2010Aware Mapless Navigation"
      },
      {
        "paperId": "ba753d97dc5b2614686fe5c57be297b539aa6334",
        "title": "Adaptive network approach to exploration-exploitation trade-off in reinforcement learning."
      },
      {
        "paperId": "987ca5e6523173d41550d5f217ce6cffeb638ba7",
        "title": "Trajectory Aware Deep Reinforcement Learning Navigation Using Multichannel Cost Maps"
      },
      {
        "paperId": "be28d0d9fca84486f135b616b5b15ab9d0670ccd",
        "title": "Roaming with Robots: Utilizing Artificial Curiosity in Global Path Planning for Autonomous Mobile Robots"
      },
      {
        "paperId": "1b7a9bdcdab269551a6439ed8108eb2ecbedc29f",
        "title": "Neuroplastic Expansion in Deep Reinforcement Learning"
      },
      {
        "paperId": "073007b08b777916f079220532c5d7930e41fdce",
        "title": "Production-Logistics Collaborative Scheduling in Dynamic Flexible Job Shops via Multi-Objective Deep Reinforcement Learning*"
      },
      {
        "paperId": "d21e8935ac67bb88d561effc141d9f1292037b6c",
        "title": "Efficient Hierarchical Reinforcement Learning for Mapless Navigation With Predictive Neighbouring Space Scoring"
      },
      {
        "paperId": "fd0c04712a597404ae12ae31e07a6f893a8c854d",
        "title": "LiDAR-Based End-to-End Active SLAM Using Deep Reinforcement Learning in Large-Scale Environments"
      },
      {
        "paperId": "a4a19413b7c377713b9a1b6de8c205aebdbb029b",
        "title": "PIEDQN: An Active-exploration Framework Capable of Prioritizing Experience Replay in Autonomous Driving"
      },
      {
        "paperId": "1dc5db40e1654b90e812ad45d5a92a74c2f05d65",
        "title": "Mobile robot navigation based on intrinsic reward mechanism with TD3 algorithm"
      },
      {
        "paperId": "83852eac1d5686096b34785b5acd541456825896",
        "title": "Improving Policy Optimization via \u03b5-Retrain"
      },
      {
        "paperId": "e369808bf95bb33b3039a0a2725cf672cccefb8a",
        "title": "AutoExplorers: Autoencoder-Based Strategies for High-Entropy Exploration in Unknown Environments for Mobile Robots"
      },
      {
        "paperId": "bfdb99adacfc21fb38cd50482e437ed533ba28ce",
        "title": "Curiosity model policy optimization for robotic manipulator tracking control with input saturation in uncertain environment"
      },
      {
        "paperId": "e43570af44c30631487ed1e67098772150c0bd48",
        "title": "Skill Q-Network: Learning Adaptive Skill Ensemble for Mapless Navigation in Unknown Environments"
      },
      {
        "paperId": "b0f4b4ee69a7bb593ec0b10637ce4227a5cec084",
        "title": "Deep Reinforcement Learning for Autonomous Driving with an Auxiliary Actor Discriminator"
      },
      {
        "paperId": "97dae7f609af53e63dc461f2ff0ae739b6d40da1",
        "title": "AGV Path Planning Using Curiosity-Driven Deep Reinforcement Learning"
      },
      {
        "paperId": "d5b865b7ecfb6af32a4c56fc8cfaab965dec146e",
        "title": "MSN: Mapless Short-Range Navigation Based on Time Critical Deep Reinforcement Learning"
      },
      {
        "paperId": "1715eef14840a732ed386c4f5c01a15585321930",
        "title": "Decentralized Multi-Robot Navigation in Unknown Environments via Hierarchical Deep Reinforcement Learning"
      },
      {
        "paperId": "d74fd1c7ad9a5f585884dcaa1c8ba000bdc8cee5",
        "title": "A Region-Based Approach to Monocular Mapless Navigation Using Deep Learning Techniques"
      },
      {
        "paperId": "c22473db445de5cc874f15b4e616192d770d17c2",
        "title": "Lighthouses and Global Graph Stabilization: Active SLAM for Low-compute, Narrow-FoV Robots"
      },
      {
        "paperId": "eaed3c98ca1dd4862748df3948683e218f9d4e04",
        "title": "Visual Episodic Memory-based Exploration"
      },
      {
        "paperId": "9e11c850056a181b4ef7ea4268dea23df9cc7e30",
        "title": "Efficient Autonomous Exploration and Mapping in Unknown Environments"
      },
      {
        "paperId": "64fded0296d7c2ec6e022e6951902ff50cf1971a",
        "title": "Deep Reinforcement Learning-Based Mapless Crowd Navigation with Perceived Risk of the Moving Crowd for Mobile Robots"
      },
      {
        "paperId": "9c412cc81e34fb66e1254419eeaa24380018079a",
        "title": "Reshaping Viscoelastic-String Path-Planner (RVP)"
      },
      {
        "paperId": "12376e0a2bac51d1eae7e102179252d1f1948fa2",
        "title": "Optimizing pedestrian simulation based on expert trajectory guidance and deep reinforcement learning"
      },
      {
        "paperId": "3bbd4fe74a3f0be2dfb809295700b92f05003114",
        "title": "Goal-Guided Transformer-Enabled Reinforcement Learning for Efficient Autonomous Navigation"
      },
      {
        "paperId": "6644bc9f0e7ba43571593aa19b635397a828332a",
        "title": "Localisation-Safe Reinforcement Learning for Mapless Navigation"
      },
      {
        "paperId": "031265825d67730c56e40b2020726bfeb2895222",
        "title": "Spatial Consciousness Model of Intrinsic Reward in Partially Observable Environments"
      },
      {
        "paperId": "8c4706176e96a3eda1ecf45864c3b39447022de0",
        "title": "Mapless Navigation Based on VDAS-PPO Deep Reinforcement Learning"
      },
      {
        "paperId": "7ed0e765bc34c8527fa172517775bbc90c4edd52",
        "title": "Ensemble Reinforcement Learning in Continuous Spaces - A Hierarchical Multi-Step Approach for Policy Training"
      },
      {
        "paperId": "15b1151a6c95e9ec00f419c90c712b13fd021e1c",
        "title": "Deep Reinforcement Learning: A Survey"
      },
      {
        "paperId": "f6b99e1ccd757d39aef9f0d8ceac31221cdefe81",
        "title": "Unguided Self-exploration in Narrow Spaces with Safety Region Enhanced Reinforcement Learning for Ackermann-steering Robots"
      },
      {
        "paperId": "1f577f6f0785c7967bef4ac6d0ab0370c2815b4d",
        "title": "Occupancy Reward-Driven Exploration with Deep Reinforcement Learning for Mobile Robot System"
      },
      {
        "paperId": "2cd18b125cbebd514f20b97c4904993b71ecccb6",
        "title": "Path planning for multiple agents in an unknown environment using soft actor critic and curriculum learning"
      },
      {
        "paperId": "772d8c3b3962faf2db842ed6202babfef2a112be",
        "title": "End-to-End Autonomous Exploration for Mobile Robots in Unknown Environments through Deep Reinforcement Learning"
      },
      {
        "paperId": "49cc565483139ac43b7fc64d6dc370eacb816423",
        "title": "A Survey on Active Simultaneous Localization and Mapping: State of the Art and New Frontiers"
      },
      {
        "paperId": "7c4d30347784d09c759449b95ca45af6f9886cdb",
        "title": "Reshaping Local Path Planner"
      },
      {
        "paperId": "5e6a33f1530c1b21e9b9a93fedc82ee52ffeab7b",
        "title": "Verifying Learning-Based Robotic Navigation Systems"
      },
      {
        "paperId": "71b963a0ef50f9dfe5656ad367a10fcb821a81b2",
        "title": "Image Augmentation-Based Momentum Memory Intrinsic Reward for Sparse Reward Visual Scenes"
      },
      {
        "paperId": "dc1c4918a8a3a91bb30ce6ef780aaa9cfb40a3b9",
        "title": "Fast and Compute-Efficient Sampling-Based Local Exploration Planning via Distribution Learning"
      },
      {
        "paperId": "42d413feb8479b72da06f3d47d78fcb0d2f861f2",
        "title": "Using Deep Reinforcement Learning with Automatic Curriculum earning for Mapless Navigation in Intralogistics"
      },
      {
        "paperId": "77749b0edc37313f210f3f328a9a1ccac28775f3",
        "title": "Pseudo Reward and Action Importance Classification for Sparse Reward Problem"
      },
      {
        "paperId": "0d47e7042f650be5f594bf70b355a612129ea4c2",
        "title": "A Deep Safe Reinforcement Learning Approach for Mapless Navigation"
      },
      {
        "paperId": "2ecd943f7d2f147470577a2ae6fb00af2ea8325d",
        "title": "Curriculum learning for safe mapless navigation"
      },
      {
        "paperId": "fe609b4a2ab1ee99c50602f7ac1a2df8496356b5",
        "title": "Reinforcement Learning for Mobile Robotics Exploration: A Survey"
      },
      {
        "paperId": "a57f9224396263cd4e51c25b1d9686d840cfdd74",
        "title": "Mapless navigation based on continuous deep reinforcement learning"
      },
      {
        "paperId": "c293307df4fc4c9e9763d74440db309ce5954230",
        "title": "Using perception cues for context-aware navigation in dynamic outdoor environments"
      },
      {
        "paperId": "d5edf4a308c791e899263f99af5d313ffd0e9765",
        "title": "Hindsight Intermediate Targets for Mapless Navigation With Deep Reinforcement Learning"
      },
      {
        "paperId": "d4da158a6883d5c56824de3bbf9ec060591b7008",
        "title": "Learning to Navigate in a VUCA Environment: Hierarchical Multi-expert Approach"
      },
      {
        "paperId": "f3770a49e8a297814135c5b21a0947f42046e260",
        "title": "Prioritized Indoor Exploration with a Dynamic Deadline"
      },
      {
        "paperId": "41872d5173d4ceaef1a5359dae8ad4198caf6b66",
        "title": "Knowledge is reward: Learning optimal exploration by predictive reward cashing"
      },
      {
        "paperId": "75397aa4447e10b1c2ce36295a9da5bd8be7b034",
        "title": "A review of mobile robot motion planning methods: from classical motion planning workflows to reinforcement learning-based architectures"
      },
      {
        "paperId": "d34096caae244d07c8defba9841e8a229143df70",
        "title": "Mapless Humanoid Navigation Using Learned Latent Dynamics"
      },
      {
        "paperId": "45becb260e3c711fd0392e682412bfab2f6438ab",
        "title": "Environment Prediction to Enhance the Navigation System of Water Pipeline Inspection Platforms"
      },
      {
        "paperId": "987392d2e0b2969d61cd9c95d95fe4aba5e71c4e",
        "title": "CURIOSITY-DRIVEN REINFORCEMENT LEARNING AGENT FOR MAPPING UNKNOWN INDOOR ENVIRONMENTS"
      },
      {
        "paperId": "4241878f081b4a492ba6aa10f60131544c790b5b",
        "title": "BND*-DDQN: Learn to Steer Autonomously Through Deep Reinforcement Learning"
      },
      {
        "paperId": "85266f1f23cc1c6eb130ca9b6ad9b9fc7ed40524",
        "title": "Soft Actor-Critic for Navigation of Mobile Robots"
      },
      {
        "paperId": "4f7f18b7299210c3aad3d16d416e47957dd0c9d5",
        "title": "Automatically Addressing System for Ultrasound-Guided Renal Biopsy Training Based on Augmented Reality"
      },
      {
        "paperId": "ac6a1262023834d31febf503306b0d48154fa1cd",
        "title": "Policy Augmentation: An Exploration Strategy For Faster Convergence of Deep Reinforcement Learning Algorithms"
      },
      {
        "paperId": "5ca0dbb82b8f01c0e9f4b7fb6ea4f54582f15bfe",
        "title": "View-Action Representation Learning for Active First-Person Vision"
      },
      {
        "paperId": "74a8b43ae6d254500397c0c3d0cafee33a02e013",
        "title": "Autonomous Navigation in Complex Environments using Memory-Aided Deep Reinforcement Learning"
      },
      {
        "paperId": "aca23172ad426f78ef6d44065e8bcb9bccfe0cf9",
        "title": "Evaluate, explain, and explore the state more exactly: an improved Actor-Critic algorithm for complex environment"
      },
      {
        "paperId": "cbb2f2b3e126944ce0da2032eed1e7d8965cea5d",
        "title": "Motion planning and control for mobile robot navigation using machine learning: a survey"
      },
      {
        "paperId": "854ff2b01a3d386e53325a13c0b75759df8a860a",
        "title": "Motion Control for Mobile Robot Navigation Using Machine Learning: a Survey"
      },
      {
        "paperId": "80e99f2dbe9e64a6e2c2b135c976880ae34b8f62",
        "title": "A Deep Reinforcement Learning Approach for Active SLAM"
      },
      {
        "paperId": "66204e092907bca91d314cedfb54a820d8c10103",
        "title": "Robot training in virtual environments using Reinforcement Learning techniques"
      },
      {
        "paperId": "1f603cc03e18dcf8f99ed467a34a0f071fb01172",
        "title": "Learn to Navigate Maplessly With Varied LiDAR Configurations: A Support Point-Based Approach"
      },
      {
        "paperId": "85fb6cbddd92fd75f56cff6aca01eb28cf53e3e3",
        "title": "Research on Mobile Robot Path Planning Based on Deep Reinforcement Learning"
      },
      {
        "paperId": "e59c193e143fe2a74423fac3dea6fcad88d9b859",
        "title": "REINFORCEMENT LEARNING HELPS SLAM: LEARNING TO BUILD MAPS"
      },
      {
        "paperId": "1ddb3e940ccff3eaf64e91007d2c835b3add51ac",
        "title": "A 3D Simulation Environment and Navigation Approach for Robot Navigation via Deep Reinforcement Learning in Dense Pedestrian Environment"
      },
      {
        "paperId": "f29d649d84d326f1e4c9e3b20eb298d61ab62963",
        "title": "Learning Reward Function with Matching Network for Mapless Navigation"
      },
      {
        "paperId": "52a6657cf1cb3cc847695a386bd65b5eea34bc13",
        "title": "Deep Reinforcement Learning-Based Automatic Exploration for Navigation in Unknown Environment"
      },
      {
        "paperId": "440e5e26bfbd7e6976ac2141bdea50a7a2e0bba6",
        "title": "SnapNav: Learning Mapless Visual Navigation with Sparse Directional Guidance and Visual Reference"
      },
      {
        "paperId": "e69be11dbb71974b1d5585f71401d2f62cb5f422",
        "title": "Deep Reinforcement Learning (DRL): Another Perspective for Unsupervised Wireless Localization"
      },
      {
        "paperId": "0922e2c1d971ec7670ab9cbba7230a661eec36ce",
        "title": "Toward Location-Enabled IoT (LE-IoT): IoT Positioning Techniques, Error Sources, and Error Mitigation"
      },
      {
        "paperId": "94ae155283af084ba8130d4def0cf131b56c93ae",
        "title": "Location-Enabled IoT (LE-IoT): A Survey of Positioning Techniques, Error Sources, and Mitigation"
      },
      {
        "paperId": "99622515c5656af37c85f3c5d1eac20f47cb56df",
        "title": "Map-less Navigation: A Single DRL-based Controller for Robots with Varied Dimensions"
      },
      {
        "paperId": "5bd815eac4ab1481d0c26208182bbea01ddbf8b9",
        "title": "Perception and Decision-Making of Autonomous Systems in the Era of Learning: An Overview"
      },
      {
        "paperId": "5420c2b33f899a07fb1ae4c4886eed1a6f7ac3fc",
        "title": "An Overview of Perception and Decision-Making in Autonomous Systems in the Era of Learning"
      },
      {
        "paperId": "0d7db8c672a67c0f91226cd7ef0dd34f05c05d99",
        "title": "Learning fine-grained control for mapless navigation"
      },
      {
        "paperId": "05fb3049ccfe037e1baa73cc0022ccda621dd2e7",
        "title": "Mapless Navigation among Dynamics with Social-safety-awareness: a reinforcement learning approach from 2D laser scans"
      },
      {
        "paperId": "a67a410a8cc9690a19221ef6538d097e670b8cb1",
        "title": "Unmanned Vehicle Path Planning for Unknown Off-road Environments with Sparse Waypoints"
      },
      {
        "paperId": "fbcb419eb13d60e267a476e1c70cd6b9b6d0b891",
        "title": "Navigation behavioural decision-making of MASS based on deep reinforcement learning and artificial potential field"
      },
      {
        "paperId": "f84e1ab127729d1a039b0a094bc2a3c0d4e5beb1",
        "title": "Navigation in Unknown Dynamic Environments Based on Deep Reinforcement Learning"
      },
      {
        "paperId": "43c145cf272e8aad5506d321def94cd275a2f030",
        "title": "A Deep Reinforcement Learning Based Mapless Navigation Algorithm Using Continuous Actions"
      },
      {
        "paperId": "73fe9858dcf6de10d37a5b3bd58d6f8564983908",
        "title": "Using RGB Image as Visual Input for Mapless Robot Navigation"
      },
      {
        "paperId": "d233ab5a613e70eba952b12a73897460ff6b7db9",
        "title": "Toward Computational Motivation for Multi-Agent Systems and Swarms"
      },
      {
        "paperId": "ada701a35f9c1fb4c6de3e64a3330b950159ecaf",
        "title": "Learn to Steer through Deep Reinforcement Learning"
      },
      {
        "paperId": "3a5e2201b23232413b7cdef998aca7379be17b72",
        "title": "Computer Science & Information Technology"
      },
      {
        "paperId": "ec9a1de6cfeb06198377359709c7049001ad38f4",
        "title": "VR-Goggles for Robots: Real-to-Sim Domain Adaptation for Visual Control"
      },
      {
        "paperId": "ee0a26f22e5f41ab42e2468b1937293129a1a6f8",
        "title": "Efficient Multi-Agent Exploration in Area Coverage Under Spatial and Resource Constraints"
      },
      {
        "paperId": "000c3004d2191fd115d1190cca963486e71b9e00",
        "title": "Trajectory Progress-Based Prioritizing and Intrinsic Reward Mechanism for Robust Training of Robotic Manipulations"
      },
      {
        "paperId": "590c7bc9796d462e26f9043930811f6b75ae9383",
        "title": "GOME-NGU: Visual Navigation Under Sparse Reward via Goal-Oriented Memory Encoder With Never Give Up"
      },
      {
        "paperId": "eaf492a2113be82ecef2d8a6e832b028ee35527a",
        "title": "Deep Learning Techniques for Visual SLAM: A Survey"
      },
      {
        "paperId": "3e1240d050b65fb0541299c8f6809d6148830f7d",
        "title": "Towards Robustness Analysis for Adaptive Artificial Intelligence in Multi-Autonomous agent systems"
      },
      {
        "paperId": "5e9519b1a0d65ee8457c1712fc1c75ff0126eca9",
        "title": "Hierarchical Training of Deep Ensemble Policies for Reinforcement Learning in Continuous Spaces"
      },
      {
        "paperId": "a2181892edc22821cbb4d7b234c9a4d829941dd2",
        "title": "STUDY OF PROBABILISTIC TOPIC REPRESENTATIONS FOR THE CLASSIFICATION OF GENOMIC"
      },
      {
        "paperId": "2717ee5dd336dcfba039b3939e7526b2beb18260",
        "title": "Verification and Evaluation of Computer and Communication Systems: 14th International Conference, VECoS 2020, Xi'an, China, October 26\u201327, 2020, Proceedings"
      },
      {
        "paperId": "3d44fb77a910ac0f26866b47a7e4f4d0c2e25f27",
        "title": "Entropy-Based Exploration for Mobile Robot Navigation: A Learning-Based Approach"
      },
      {
        "paperId": "79aab4f858bde9d12bfae8c78c9e227a22bfe27c",
        "title": "Historical Information for Enhancing Q-learning in Maze Navigation Tasks"
      },
      {
        "paperId": "bd26c5677f2b2a9fc7ff7e0dd0d640fc6db5d322",
        "title": "On the Emergence of Biologically-Plausible Representation in Recurrent Neural Networks for Navigation"
      },
      {
        "paperId": "4f82f6375571c2704e279d3c301292b52f12aebc",
        "title": "Robotics and Autonomous Systems"
      },
      {
        "paperId": "f65ac213e0159210a1155e735b33f9878684bdf8",
        "title": "Ef\ufb01cient Hierarchical Reinforcement Learning for Mapless Navigation with Predictive Neighbouring Space Scoring"
      },
      {
        "paperId": "7d5b55c8facc8eca1972c5d220791831dba65ad9",
        "title": "Improving Policy Optimization via \ud835\udc3f -Retrain"
      }
    ],
    "score": 15.428571428571427
  },
  {
    "id": "0d82360a4da311a277607db355dda3f196e8eb3d",
    "title": "Deep Interactive Reinforcement Learning for Path Following of Autonomous Underwater Vehicle",
    "authors": [
      "Qilei Zhang",
      "Jinying Lin",
      "Q. Sha",
      "Bo He",
      "Guangliang Li"
    ],
    "year": 2020,
    "citationCount": 77,
    "abstract": "Autonomous underwater vehicle (AUV) plays an increasingly important role in ocean exploration. Existing AUVs are usually not fully autonomous and generally limited to pre-planning or pre-programming tasks. Reinforcement learning (RL) and deep reinforcement learning have been introduced into the AUV design and research to improve its autonomy. However, these methods are still difficult to apply directly to the actual AUV system because of the sparse rewards and low learning efficiency. In this paper, we proposed a deep interactive reinforcement learning method for path following of AUV by combining the advantages of deep reinforcement learning and interactive RL. In addition, since the human trainer cannot provide human rewards for AUV when it is running in the ocean and AUV needs to adapt to a changing environment, we further propose a deep reinforcement learning method that learns from both human rewards and environmental rewards at the same time. We test our methods in two path following tasks\u2014straight line and sinusoids curve following of AUV by simulating in the Gazebo platform. Our experimental results show that with our proposed deep interactive RL method, AUV can converge faster than a DQN learner from only environmental reward. Moreover, AUV learning with our deep RL from both human and environmental rewards can also achieve a similar or even better performance than that with deep interactive RL and can adapt to the actual environment by further learning from environmental rewards.",
    "url": "https://www.semanticscholar.org/paper/0d82360a4da311a277607db355dda3f196e8eb3d",
    "pdf_url": "https://arxiv.org/pdf/2001.03359.pdf",
    "venue": "IEEE Access",
    "publicationDate": "2020-01-10",
    "externalIds": {
      "ArXiv": "2001.03359",
      "DBLP": "journals/access/ZhangLSHL20",
      "MAG": "3000514046",
      "DOI": "10.1109/ACCESS.2020.2970433",
      "CorpusId": 210156941
    },
    "references": [
      {
        "paperId": "8e75e1e295946dbdf5ca2b40948edce843441511",
        "title": "Human-Centered Reinforcement Learning: A Survey"
      },
      {
        "paperId": "179c66e30cdbfb06b51fdb405b79c19b6ec1b221",
        "title": "Social interaction for efficient agent learning from human reward"
      },
      {
        "paperId": "13529c2e9a43d387c58ad0c06a13d440b3f8d16f",
        "title": "Deep reinforcement learning based optimal trajectory tracking control of autonomous underwater vehicle"
      },
      {
        "paperId": "630bf4c0b7e5abd276ec38468f490b7d6222f3a2",
        "title": "Interactive Learning from Policy-Dependent Human Feedback"
      },
      {
        "paperId": "ab688b92a0fc790c4f3641d0f94a7d61cdf54231",
        "title": "Adaptive Neural Network Control of AUVs With Control Input Nonlinearities Using Reinforcement Learning"
      },
      {
        "paperId": "9c5da87411d0f3ab6726e0a7ee46f0ee23f2bbec",
        "title": "UUV Simulator: A Gazebo-based package for underwater intervention and multi-robot simulation"
      },
      {
        "paperId": "c6170fa90d3b2efede5a2e1660cb23e1c824f2ca",
        "title": "Prioritized Experience Replay"
      },
      {
        "paperId": "340f48901f72278f6bf78a04ee5b01df208cc508",
        "title": "Human-level control through deep reinforcement learning"
      },
      {
        "paperId": "8497e28e813e4d81f46be0c7908b5427f4fdbccd",
        "title": "Learning behaviors via human-delivered discrete feedback: modeling implicit feedback strategies to speed up learning"
      },
      {
        "paperId": "6a15d77e434a8249a1b2e5183c9330e328151fb8",
        "title": "Learning by demonstration applied to underwater intervention"
      },
      {
        "paperId": "193edd20cae92c6759c18ce93eeea96afd9528eb",
        "title": "Deep learning in neural networks: An overview"
      },
      {
        "paperId": "65438e0ba226c1f97bd8a36333ebc3297b1a32fd",
        "title": "Reinforcement learning in robotics: A survey"
      },
      {
        "paperId": "e6d773c913b1254940012b4e3f7abfa73b35f6d2",
        "title": "Towards valve turning with an AUV using Learning by Demonstration"
      },
      {
        "paperId": "16e777d97c95626bf99de5be05d23be038cfb149",
        "title": "Autonomous robotic valve turning: A hierarchical learning approach"
      },
      {
        "paperId": "80128eac4063571da71f8e731ac6b137dd208e24",
        "title": "Learning from human-generated reward"
      },
      {
        "paperId": "256c3bd45ab7452bb51721eb25d3367bb654225e",
        "title": "Interactively shaping agents via human reinforcement: the TAMER framework"
      },
      {
        "paperId": "4e5dfb0b1e54412e799eb0e86d552956cc3a5f54",
        "title": "A survey of robot learning from demonstration"
      },
      {
        "paperId": "97fbc110ffb5d658e0e1d47218cb7b0b86ab4e25",
        "title": "Policy gradient based Reinforcement Learning for real autonomous underwater cable tracking"
      },
      {
        "paperId": "c86ba8c7b306e5927e8667011ce40402750e0b52",
        "title": "Teachable robots: Understanding human teaching behavior to build more effective robot learners"
      },
      {
        "paperId": "819e12c46c95add85a4bbfcffa8fe6ba5b1924a4",
        "title": "PID control system analysis, design, and technology"
      },
      {
        "paperId": "5848580d4b40bcca6e16fdc42dc67df8ee33747d",
        "title": "Line-of-sight path following of underactuated marine craft"
      },
      {
        "paperId": "94066dc12fe31e96af7557838159bde598cb4f10",
        "title": "Policy Invariance Under Reward Transformations: Theory and Application to Reward Shaping"
      },
      {
        "paperId": "09a4abe29ee5265d59192143f9cf3ed5e245159a",
        "title": "Learning control of underwater robotic vehicles"
      },
      {
        "paperId": "916ceefae4b11dadc3ee754ce590381c568c90de",
        "title": "A direct adaptive method for faster backpropagation learning: the RPROP algorithm"
      },
      {
        "paperId": "03b7e51c52084ac1db5118342a00b5fbcfc587aa",
        "title": "Q-learning"
      },
      {
        "paperId": "9cd8193a66cf53143cbba6ccb0c7b9c2ebf2452b",
        "title": "Self-improving reactive agents based on reinforcement learning, planning and teaching"
      },
      {
        "paperId": null,
        "title": "QILEI ZHANG received the Bachelor's degree in electronic information science and technology from the School of Information Science and Engineering"
      },
      {
        "paperId": null,
        "title": "Her current research interests include reinforcement learning, human agent/robot interaction"
      },
      {
        "paperId": "2e6fff7822cfd93e1addb4c790be0ae4cc34164c",
        "title": "Socially intelligent autonomous agents that learn from human reward"
      },
      {
        "paperId": "5477147ec61b85a9667002fe32671edcc35642ed",
        "title": "UvA-DARE ( Digital Academic Repository ) Using Informative Behavior to Increase Engagement While Learning from Human Reward"
      },
      {
        "paperId": "ba61f007dd296368900e20247a29cb409fa619a6",
        "title": "Behavior Adaptation by Means of Reinforcement Learning"
      },
      {
        "paperId": null,
        "title": "He worked at Qing-dao Bailing Technology Co., Ltd. and Alcatel-Lucent Qingdao R & D Center from 2010 to 2014 as a software engineer"
      },
      {
        "paperId": null,
        "title": "received the B.S. degree in communication engineering from Ocean University of China"
      },
      {
        "paperId": null,
        "title": "He was a Researcher with Nanyang Technological University"
      },
      {
        "paperId": null,
        "title": "con-trol theory and control engineering from Harbin Institute of Technology, Harbin, China"
      },
      {
        "paperId": "97efafdb4a3942ab3efba53ded7413199f79c054",
        "title": "Reinforcement Learning: An Introduction"
      },
      {
        "paperId": "26e85b438135f6defa1326d73db147e8bd51ae41",
        "title": "Learning machine learning"
      },
      {
        "paperId": "145a42e83ec142a125da3ad845ee95027ef702e5",
        "title": "Ieee Transactions on Systems, Man and Cybernetics\u2014part C: Applications and Reviews 1 a Survey of Actor-critic Reinforcement Learning: Standard and Natural Policy Gradients"
      },
      {
        "paperId": "d7c8b0033f7756208e7e4517cc4df93fcee02651",
        "title": "Training a Robot via Human Feedback : A Case Study"
      },
      {
        "paperId": null,
        "title": "decision-making and software system in underwater vehicle"
      },
      {
        "paperId": null,
        "title": "Preparation of Papers for IEEE TRANSACTIONS and JOURNALS"
      }
    ],
    "cited_by": [
      {
        "paperId": "331a27ce74d77daca4f34fe9c79df0db5617f125",
        "title": "Design and optimization of a bionic jellyfish robot for enhanced underwater propulsion efficiency"
      },
      {
        "paperId": "90e0915d58bc6734d012344d5ae0bee74496bdfc",
        "title": "Path Planning for Transoceanic Underwater Glider Based on Hybrid Reinforcement Learning Algorithm"
      },
      {
        "paperId": "606a5e4dc4ebf3a335ff6dd59da9ae4400b5d7b8",
        "title": "Navigating with Ocean Currents: A Novel Methodology for AUV Path Planning"
      },
      {
        "paperId": "3681c861aaf4506b12b4354e6e45617fa0ba215b",
        "title": "A Review of Path Following, Trajectory Tracking, and Formation Control for Autonomous Underwater Vehicles"
      },
      {
        "paperId": "19b20ce06245e9f13fdd3c30f4ff55e8552868b8",
        "title": "Efficient algorithms for navigation of underwater vehicles with communication constraints. An overview"
      },
      {
        "paperId": "90e28a202605b4e8a10f0e0806ccec2eb852786e",
        "title": "Deep reinforcement learning from human preferences for ROV path tracking"
      },
      {
        "paperId": "58ab23e22077091157a0803a31187c59a0a8d90d",
        "title": "Novel deep reinforcement learning based collision avoidance approach for path planning of robots in unknown environment"
      },
      {
        "paperId": "c39a2f2cddf744552e143ce33e19b6b1aaf2208b",
        "title": "The docking control system of an autonomous underwater vehicle combining intelligent object recognition and deep reinforcement learning"
      },
      {
        "paperId": "8503bf22c469bdabc2e59e77f3c6e6ba0195c592",
        "title": "Comparative Analysis of Modified Q-Learning and DQN for Autonomous Robot Navigation"
      },
      {
        "paperId": "368d4b29ae334cf77676e90f0656623247a4c19e",
        "title": "Efficient Multi-Task 3D Path Planning for AUV in Complex Marine Environment"
      },
      {
        "paperId": "b86eb39220b00e9ddbe58a94054b76ae9ee03a1d",
        "title": "An Application of Deep Learning Techniques in Trajectory Planning of Autonomous Vehicles"
      },
      {
        "paperId": "0b9a413b5fca6c50999ae18c1a2d83c656c75f73",
        "title": "A Robust Autonomous UAV Control System Based on Kalman Filter,LSTM, MLP and DQN"
      },
      {
        "paperId": "24cc736bc879c7d5b5c806695316fd1dc2cfc92f",
        "title": "Deep reinforcement learning-based pitch attitude control of a beaver-like underwater robot"
      },
      {
        "paperId": "5ddb4b9d9c58dbe026ebf7c6c78875bbeb4c711a",
        "title": "Human-In-The-Loop Machine Learning for Safe and Ethical Autonomous Vehicles: Principles, Challenges, and Opportunities"
      },
      {
        "paperId": "e6eda7a7e9f3089c39ac88c5a31d8642e4892450",
        "title": "Enhancing Highway Driving: High Automated Vehicle Decision Making in a Complex Multi-Body Simulation Environment"
      },
      {
        "paperId": "56fc53536c4dea75a6e27f8f0e1dee43ce8c7c0d",
        "title": "Review on path planning methods for autonomous underwater vehicle"
      },
      {
        "paperId": "72f6d5a596bfd836eab52c66334c628bb885eb87",
        "title": "Multi-AUV cooperative control and autonomous obstacle avoidance study"
      },
      {
        "paperId": "065ecc98b3f26346d12d509b34a5c57c310bb7eb",
        "title": "Sample-Observed Soft Actor-Critic Learning for Path Following of a Biomimetic Underwater Vehicle"
      },
      {
        "paperId": "e35d0a7ba97269f57ef16f018dcb87762f3b97bd",
        "title": "Advancing Pandemic Preparedness through a Data-Driven Hybrid Simulation Model"
      },
      {
        "paperId": "8171394a03ddc0e94ed4c8b599da6364bdf97ece",
        "title": "A jellyfish robot based on two-bar and four-spring tensegrity structures"
      },
      {
        "paperId": "40bdebfcf4ddcbdf542a718d285ef4ac1e57d407",
        "title": "Using Reinforcement Learning for Hydrobatic Maneuvering with Autonomous Underwater Vehicles"
      },
      {
        "paperId": "3edc687d4af481efb35ee4c17fcc0189bb60cc6d",
        "title": "AUV Surfacing Control With Adversarial Attack Against DLaaS Framework"
      },
      {
        "paperId": "944190b69f08e2c9f363db64154689685777d0d9",
        "title": "Multi-Agent Generative Adversarial Interactive Self-Imitation Learning for AUV Formation Control and Obstacle Avoidance"
      },
      {
        "paperId": "9690f2d8d9a1c571968d26a4d73298848b559fb0",
        "title": "FUSION SPARSE AND SHAPING REWARD FUNCTION IN SOFT ACTOR-CRITIC DEEP REINFORCEMENT LEARNING FOR MOBILE ROBOT NAVIGATION"
      },
      {
        "paperId": "7dd3603447fc4dfc5e7218cba5e7c89a7a7c99fb",
        "title": "Path Planning of a Mobile Robot for a Dynamic Indoor Environment Based on an SAC-LSTM Algorithm"
      },
      {
        "paperId": "75b25ac6999bb248d6f241d7709cdbc22005f2ee",
        "title": "Learn Buddy : Path Following Lab Assistant Robot"
      },
      {
        "paperId": "b0e0fd28573758ff036d3e8284ba74d02b9fc636",
        "title": "Robust adaptive path following control of autonomous underwater vehicle with uncertainties and communication bandwidth limitation"
      },
      {
        "paperId": "fe2d040b59b79107d9fb64d03a4e8ec805210f98",
        "title": "Range-Only Single Beacon Based Multisensor Fusion Positioning for AUV"
      },
      {
        "paperId": "fb9b9af6f7049cf8ccf6a8a293d458db766e012c",
        "title": "SWiMM DEEPeR: A Simulated Underwater Environment for Tracking Marine Mammals Using Deep Reinforcement Learning and BlueROV2"
      },
      {
        "paperId": "c8ef7b0d2416d27740f24f7f101206baf8106949",
        "title": "Federated Discrete Reinforcement Learning for Automatic Guided Vehicle Control"
      },
      {
        "paperId": "44eadc0f3dfb45b6a9fd96f9ed54911dc8734d8c",
        "title": "Dynamic robotic tracking of underwater targets using reinforcement learning"
      },
      {
        "paperId": "5fcb0a589fa0306a5f4aaa91d12562c8adb8c501",
        "title": "Energy-Efficient Space\u2013Air\u2013Ground\u2013Ocean-Integrated Network Based on Intelligent Autonomous Underwater Glider"
      },
      {
        "paperId": "9ea5f3837872abb6b117c7e7aa9a78bd7bfbb64b",
        "title": "Design and Simulation-Based Optimization of an Intelligent Autonomous Cruise Control System"
      },
      {
        "paperId": "23eafcd32e7483a298e3b2d80cc6c53122165093",
        "title": "Adaptive Formation Motion Planning and Control of Autonomous Underwater Vehicles Using Deep Reinforcement Learning"
      },
      {
        "paperId": "2f5fbd38829f6afc44e76d285c390c0f6a06cb29",
        "title": "Cooperative Artificial Intelligence for underwater robotic swarm"
      },
      {
        "paperId": "d74ba4e2e65958e81b4dcb03ebbd8b0c0776b58b",
        "title": "A Robust State Feedback Optimal Control Law with Backstepping Approach for Steering Control of an Autonomous Underwater Vehicle Using Semi-definite Programming"
      },
      {
        "paperId": "3f9247f0b71340ce3e6b70e0f3ee0748223ce7f5",
        "title": "Intelligent Path Planning Technologies of Underwater Vehicles: a Review"
      },
      {
        "paperId": "b729df6c2d483826b4b23f75eab57510276b9fbd",
        "title": "Survey on traditional and AI based estimation techniques for hydrodynamic coefficients of autonomous underwater vehicle"
      },
      {
        "paperId": "f44b6db8682b00630e962dfec0dbd30d4ac57787",
        "title": "Path-following optimal control of autonomous underwater vehicle based on deep reinforcement learning"
      },
      {
        "paperId": "47222dce7eb6457c29959c25fc4b8d1cbaa3adfb",
        "title": "Autonomous Underwater Vehicle Path Planning Method of Soft Actor\u2013Critic Based on Game Training"
      },
      {
        "paperId": "9174f728bc4381e8ffe4aacb161a9aed42575a04",
        "title": "Deep Reinforcement Learning Based Path Planning for Mobile Robots Using Time-Sensitive Reward"
      },
      {
        "paperId": "cfc4723590deb7e60cda7119f46417a3577e7b71",
        "title": "Deep reinforcement learning for adaptive path planning and control of an autonomous underwater vehicle"
      },
      {
        "paperId": "2e7df83fc836f813ce9c53ae02a5151408cf5070",
        "title": "Optimization of autonomous vehicle speed control mechanisms using hybrid DDPG-SHAP-DRL-stochastic algorithm"
      },
      {
        "paperId": "b771be4b290f255267ccde5705fbbcd1874675c7",
        "title": "Autonomous underwater vehicle formation control and obstacle avoidance using multi-agent generative adversarial imitation learning"
      },
      {
        "paperId": "8a861e3e97fb2f81248ff8005ea774b7b953defe",
        "title": "Dynamic Target Tracking of Autonomous Underwater Vehicle Based on Deep Reinforcement Learning"
      },
      {
        "paperId": "f881362c85f9858d227ed6b106dbb80c5379ab2a",
        "title": "The Current Trends of Deep Learning in Autonomous Vehicles: A Review"
      },
      {
        "paperId": "02230f4075108698fec7c79c4b38eac20dc786b5",
        "title": "Continuous Control for Autonomous Underwater Vehicle Path Following Using Deep Interactive Reinforcement Learning"
      },
      {
        "paperId": "a14505553eda5ae26380d4317d6358491f238e41",
        "title": "Generative adversarial interactive imitation learning for path following of autonomous underwater vehicle"
      },
      {
        "paperId": "aa6192341a3ae7cb1d947f757e6410193d51b62d",
        "title": "Combining reinforcement learning and conventional control to improve automatic guided vehicles tracking of complex trajectories"
      },
      {
        "paperId": "497e7c3a822f6a238e71bee5c2dcc067ea2edac0",
        "title": "Controller Design of Tracking WMR System Based on Deep Reinforcement Learning"
      },
      {
        "paperId": "0c7a55e5ce0869ae71eed975d16813cc5924999a",
        "title": "AUV position tracking and trajectory control based on fast-deployed deep reinforcement learning method"
      },
      {
        "paperId": "46ca03fe3d09b508a44c5386e06ca8d7c21562dd",
        "title": "An Experimental Safety Response Mechanism for an Autonomous Moving Robot in a Smart Manufacturing Environment Using Q-Learning Algorithm and Speech Recognition"
      },
      {
        "paperId": "1295d11d7f3fb96f9cf8cac439b1730f3b3ddda9",
        "title": "Routing failure prediction and repairing for AUV-assisted underwater acoustic sensor networks in uncertain ocean environments"
      },
      {
        "paperId": "9be214ab02095b9ef5683c56d5de050a35c9c2ef",
        "title": "Finite-time robust tracking control of an autonomous underwater vehicle in the presence of uncertainties and external current disturbances"
      },
      {
        "paperId": "56a7050d3810d761ba021ddda8bc6a00564519a5",
        "title": "Shaping Progressive Net of Reinforcement Learning for Policy Transfer with Human Evaluative Feedback"
      },
      {
        "paperId": "30cf42fb4f38599e5da6183cc0ffe6989d7afdee",
        "title": "AUV Trajectory Tracking Models and Control Strategies: A Review"
      },
      {
        "paperId": "fda0451c2f17c0996440f2cb98c4a3b594af102b",
        "title": "Path planning and obstacle avoidance for AUV: A review"
      },
      {
        "paperId": "80f3557db6c1a62dbfd34ff1751dee08794d9dab",
        "title": "A Safety Response Mechanism for an Autonomous Moving Robot in a Small Manufacturing Environment using Q-learning Algorithm and Speech Recognition"
      },
      {
        "paperId": "de902fd62cda23bf38b1fea21254ee01dd163b52",
        "title": "System Identification and Controller Design of a Novel Autonomous Underwater Vehicle"
      },
      {
        "paperId": "3a5e7e438f4e02a3abcaed9ed179939ba474ffe2",
        "title": "Deep Reinforcement Learning for Mapless Navigation of a Hybrid Aerial Underwater Vehicle with Medium Transition"
      },
      {
        "paperId": "5ba7d5cd9619106424ef6d60da7a7ae82a5c57cc",
        "title": "AUV Path Following Control using Deep Reinforcement Learning Under the Influence of Ocean Currents"
      },
      {
        "paperId": "9908cf79585faac7e198dcf2776526997ec7728c",
        "title": "Research on Motion Attitude Control of Under-actuated Autonomous Underwater Vehicle Based on Deep Reinforcement Learning"
      },
      {
        "paperId": "f79fec39171eb67376051c83292f5db101957653",
        "title": "Decision-Making Strategy on Highway for Autonomous Vehicles Using Deep Reinforcement Learning"
      },
      {
        "paperId": "40f85c3f1223286e8b1ef988ecea614b6c312dec",
        "title": "COLREG-Compliant Collision Avoidance for Unmanned Surface Vehicle Using Deep Reinforcement Learning"
      },
      {
        "paperId": "87ede3885308d93e215bc7852385c5cd14924c12",
        "title": "Underwater Simulators Analysis for Digital Twinning"
      },
      {
        "paperId": "913b810bf51b2edd11544a60460c426488bacf1c",
        "title": "Rotor Wake Investigation Under Autorotation Condition in Water Tunnel"
      },
      {
        "paperId": "2b1e50548b0c14cd31e19118bf76820f0c1921ca",
        "title": "A Review of Artificial Neural Networks Applications in Maritime Industry"
      },
      {
        "paperId": "6b8187250d2466a47f67964429e4d7121c6f991f",
        "title": "Active Inference Integrated with Imitation Learning for Autonomous Driving"
      },
      {
        "paperId": "c9563a9ea9ef0d887a98a759d3f5de698869dd10",
        "title": "A Multiagent Deep Reinforcement Learning Approach for Path Planning in Autonomous Surface Vehicles: The Ypacara\u00ed Lake Patrolling Case"
      },
      {
        "paperId": "3e15417de901d4b060bf79754eea308ecd983354",
        "title": "A Review of Physics Simulators for Robotic Applications"
      },
      {
        "paperId": "33a3defface2cb3295811d8717945cd3ebb7f5eb",
        "title": "Motion Planning for Mobile Robots\u2014Focusing on Deep Reinforcement Learning: A Systematic Review"
      },
      {
        "paperId": "f9b8f685c354e57c2e23565bade71bd8236d3f59",
        "title": "Human-Augmented Prescriptive Analytics With Interactive Multi-Objective Reinforcement Learning"
      },
      {
        "paperId": "6cc3f9517c1c823cf8de8b68f407c0dd76bf6b47",
        "title": "A Deep Reinforcement Learning Approach for the Patrolling Problem of Water Resources Through Autonomous Surface Vehicles: The Ypacarai Lake Case"
      },
      {
        "paperId": "fc5207e2dc7a9c29a00b6bad2d71a5e65ad1e820",
        "title": "Uncertainity and Noise Aware Decision Making for Autonomous Vehicles -A Bayesian Approach"
      },
      {
        "paperId": "246db5025910975d41086ac05c78f6a9c86ca6f5",
        "title": "Review on Nonlinear Control Strategies for Trajectory Tracking of AUVs"
      },
      {
        "paperId": "d5e0ab7923daf89c54b8e4cf44aea206ad10194b",
        "title": "Efficient tracking of ocean eddies using unmanned underwater vehicles"
      },
      {
        "paperId": "de05985e162b1e5f743d291ed416f89408f9ef40",
        "title": "Research Repositor y Deep reinforcement learning-based pitch attitude control of a beaver-like underwater robot"
      }
    ],
    "score": 15.4
  },
  {
    "id": "f6b218453170edcbb51e49dd44ba2f83af53ef92",
    "title": "Distributional Reinforcement Learning for Efficient Exploration",
    "authors": [
      "B. Mavrin",
      "Shangtong Zhang",
      "Hengshuai Yao",
      "Linglong Kong",
      "Kaiwen Wu",
      "Yaoliang Yu"
    ],
    "year": 2019,
    "citationCount": 92,
    "abstract": "In distributional reinforcement learning (RL), the estimated distribution of value function models both the parametric and intrinsic uncertainties. We propose a novel and efficient exploration method for deep RL that has two components. The first is a decaying schedule to suppress the intrinsic uncertainty. The second is an exploration bonus calculated from the upper quantiles of the learned distribution. In Atari 2600 games, our method outperforms QR-DQN in 12 out of 14 hard games (achieving 483 \\% average gain across 49 games in cumulative rewards over QR-DQN with a big win in Venture). We also compared our algorithm with QR-DQN in a challenging 3D driving simulator (CARLA). Results show that our algorithm achieves near-optimal safety rewards twice faster than QRDQN.",
    "url": "https://www.semanticscholar.org/paper/f6b218453170edcbb51e49dd44ba2f83af53ef92",
    "pdf_url": "https://arxiv.org/pdf/1905.06125.pdf",
    "venue": "International Conference on Machine Learning",
    "publicationDate": "2019-05-13",
    "externalIds": {
      "MAG": "2951191893",
      "ArXiv": "1905.06125",
      "DBLP": "journals/corr/abs-1905-06125",
      "CorpusId": 155092294
    },
    "references": [
      {
        "paperId": "79fe495ce79ea200af7df54f8c8b4d4939d27be8",
        "title": "Exploration in the Face of Parametric and Intrinsic Uncertainties"
      },
      {
        "paperId": "11abfd523249b41ad1661712fbdd73fc4a1332f2",
        "title": "Reinforcing Classical Planning for Adversary Driving Scenarios"
      },
      {
        "paperId": "13a346aa425c0708da8f1990ef1bc64edfd93bbc",
        "title": "Single-step Options for Adversary Driving"
      },
      {
        "paperId": "73d8257a4dd39abd9fbb2c878cbee7b641b10611",
        "title": "Deep Reinforcement Learning with Decorrelation"
      },
      {
        "paperId": "8d23d6432c27843040f51dcf0191877f7a9994e9",
        "title": "Robust Statistics"
      },
      {
        "paperId": "e93c016291d9f5da54f1c1cfc2575110fe5580b9",
        "title": "QUOTA: The Quantile Option Architecture for Reinforcement Learning"
      },
      {
        "paperId": "0754c909c72440e13b5399d7ada34c2734dbcd1b",
        "title": "Towards Comprehensive Maneuver Decisions for Lane Change Using Reinforcement Learning"
      },
      {
        "paperId": "d85623ffae865f9ef386644dd02d0ea2d6a8c8de",
        "title": "Implicit Quantile Networks for Distributional Reinforcement Learning"
      },
      {
        "paperId": "3995b02fb3d4be47de8cc1a178fe577a445ac5e7",
        "title": "Exploration by Distributional Reinforcement Learning"
      },
      {
        "paperId": "a710fdd04a3d5c633af29820dc9571e4a39bfd6f",
        "title": "An Analysis of Categorical Distributional Reinforcement Learning"
      },
      {
        "paperId": "d355e339298fc2ab920688c1709d4ba6476a2bc6",
        "title": "Distributed Distributional Deterministic Policy Gradients"
      },
      {
        "paperId": "1485c5ac93b3f01a5f3ef7824eb428c8bff39ba3",
        "title": "UCB EXPLORATION VIA Q-ENSEMBLES"
      },
      {
        "paperId": "ae42ae50da4192aa12cf5cffb45585a9ea99ff6d",
        "title": "DeepTraffic: Driving Fast through Dense Traffic with Deep Reinforcement Learning"
      },
      {
        "paperId": "88432e56c8d56faab6f249fbdc9d02592b1278a2",
        "title": "DeepTraffic: Crowdsourced Hyperparameter Tuning of Deep Reinforcement Learning Systems for Multi-Agent Dense Traffic Navigation"
      },
      {
        "paperId": "fe3e91e40a950c6b6601b8f0a641884774d949ae",
        "title": "Distributional Reinforcement Learning with Quantile Regression"
      },
      {
        "paperId": "ebf0615fc4d98cf1dbe527c79146ce1e50dce9af",
        "title": "CARLA: An Open Urban Driving Simulator"
      },
      {
        "paperId": "7de84e88a31176fbe4ea579069b34e6abcc9b32e",
        "title": "Monte-Carlo Tree Search vs. Model-Predictive Controller: A Track-Following Example"
      },
      {
        "paperId": "0ab3f7ecbdc5a33565a234215604a6ca9d155a33",
        "title": "Rainbow: Combining Improvements in Deep Reinforcement Learning"
      },
      {
        "paperId": "3b290ffa1f4f8226e326f00984acecdfbe9e28bf",
        "title": "Revisiting the Arcade Learning Environment: Evaluation Protocols and Open Problems for General Agents"
      },
      {
        "paperId": "bdf6572b67a6c5d8aacd39e1826db2c5c8f85716",
        "title": "The Uncertainty Bellman Equation and Exploration"
      },
      {
        "paperId": "c1f4ef741242d629d1f56e442a09a7ba29595a0e",
        "title": "A Distributional Perspective on Reinforcement Learning"
      },
      {
        "paperId": "12f67fb182bc934fc95ce97acff553d83e2ca72e",
        "title": "Count-Based Exploration with Neural Density Models"
      },
      {
        "paperId": "0fcb2034e31a2bc2f12a2b1363d0d77baf445fdf",
        "title": "#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning"
      },
      {
        "paperId": "6e90fd78e8a3b98af3954aae5209703aa966603e",
        "title": "Unifying Count-Based Exploration and Intrinsic Motivation"
      },
      {
        "paperId": "340f48901f72278f6bf78a04ee5b01df208cc508",
        "title": "Human-level control through deep reinforcement learning"
      },
      {
        "paperId": "f82e4ff4f003581330338aaae71f60316e58dd26",
        "title": "The Arcade Learning Environment: An Evaluation Platform for General Agents"
      },
      {
        "paperId": "76a3325fcb27a22cb5cef0e801a5e2b808c4648a",
        "title": "On Bayesian Upper Confidence Bounds for Bandit Problems"
      },
      {
        "paperId": "8948464fc8dbe49311fcd6610f96bcd75a03bae0",
        "title": "Parametric Return Density Estimation for Reinforcement Learning"
      },
      {
        "paperId": "e60f3c1cb857daa3233f2c5b17b6f111ff86698c",
        "title": "Algorithms for Reinforcement Learning"
      },
      {
        "paperId": "636e7dbf3487b8e682d840e9768200b1fde7fac9",
        "title": "Quantile Regression"
      },
      {
        "paperId": "63e2ca52df9c2ef5728e6326dcc74755ac3161ca",
        "title": "A theoretical analysis of Model-Based Interval Estimation"
      },
      {
        "paperId": "103f6fe35033f9327611ddafde74a2b544072980",
        "title": "Using Confidence Bounds for Exploitation-Exploration Trade-offs"
      },
      {
        "paperId": "adee4315eb8ef0fcf4e610c3d041ed8a8869a495",
        "title": "Coherent Measures of Risk"
      },
      {
        "paperId": "d82829c9fd9cfba8a44efe5ba048d3332a1671fc",
        "title": "Dynamic Programming"
      },
      {
        "paperId": "03b7e51c52084ac1db5118342a00b5fbcfc587aa",
        "title": "Q-learning"
      },
      {
        "paperId": "a91635f8d0e7fb804efd1c38d9c24ee952ba7076",
        "title": "Learning to predict by the methods of temporal differences"
      },
      {
        "paperId": "86e287d8c4c3705b46aa15309541fdcda9105106",
        "title": "The variance of left-truncated continuous nonnegative distributions"
      },
      {
        "paperId": "4c9e36a0dbdc3e7b20e38bd288a804c05c77e9fa",
        "title": "Asymptotically efficient adaptive allocation rules"
      },
      {
        "paperId": "93b7a6a43c5e6ff300cf2be8bd9482b71bf86302",
        "title": "Markov Decision Processes with a New Optimality Criterion: Discrete Time"
      },
      {
        "paperId": "b7c1bdf7d9c92f7d38aeb2ca106189bffc903f5c",
        "title": "Reinforcement learning - an introduction, 2nd Edition"
      },
      {
        "paperId": "c09db7439505f49a0958f68e782df94b3807341a",
        "title": "Regression Quantiles"
      },
      {
        "paperId": "25cf23145d0578600bc04b5b22bb88f6394caa53",
        "title": "Tail Conditional Variance for Elliptically Contoured Distributions"
      },
      {
        "paperId": "1cc90c5283679dc71d58f20175b55eea857924c8",
        "title": "Value at Risk: The New Benchmark for Managing Financial Risk"
      },
      {
        "paperId": "97efafdb4a3942ab3efba53ded7413199f79c054",
        "title": "Reinforcement Learning: An Introduction"
      },
      {
        "paperId": "36e07bdcb9f8aa618d6d077238d3de2e7818a0f5",
        "title": "Robust statistics: the approach based on influence functions"
      },
      {
        "paperId": "c87d57da3b1f2b467ef4995d30df832ee2281107",
        "title": "On robust estimation of the location parameter"
      },
      {
        "paperId": null,
        "title": "Due to time pressure, name was forgotten during submiting"
      },
      {
        "paperId": null,
        "title": "please use this correct author list. The mistake was \ufb01xed the arxiv version of this paper."
      }
    ],
    "cited_by": [
      {
        "paperId": "d1c6fae67b3a48f4b90fa471077b534e1e87183b",
        "title": "Risk-Sensitive RL for Alleviating Exploration Dilemmas in Large Language Models"
      },
      {
        "paperId": "07e6c608efa499a1877b58bcd6d2fd8eb536dd6d",
        "title": "Stochastic Path Planning in Correlated Obstacle Fields"
      },
      {
        "paperId": "de2e28de06b77e444577eca7f6af5c2d354d79be",
        "title": "ADDQ: Adaptive Distributional Double Q-Learning"
      },
      {
        "paperId": "33193ebe3893181ad7febda86b3f880628d4159f",
        "title": "Curiosity-driven exploration based on hierarchical vision transformer for deep reinforcement learning with sparse rewards"
      },
      {
        "paperId": "3a0a898a7205663fd19e37700fcc2c1c698edaf5",
        "title": "Optimal Transport-Guided Safety in Temporal Difference Reinforcement Learning"
      },
      {
        "paperId": "48ea067b2c84680117ef5fc3bc01eb0e66907e66",
        "title": "Tackling Uncertainties in Multi-Agent Reinforcement Learning through Integration of Agent Termination Dynamics"
      },
      {
        "paperId": "c4dfc0fd5a55a9d5bbf2f0b0ecaed67bb6941d94",
        "title": "State-Novelty Guided Action Persistence in Deep Reinforcement Learning"
      },
      {
        "paperId": "777c5e1df416c32b4ee12eb33eead320d671a0af",
        "title": "Enhanced Safety in Autonomous Driving: Integrating a Latent State Diffusion Model for End-to-End Navigation"
      },
      {
        "paperId": "db9983fe995c19bdb4603f1dd349fad17f29df46",
        "title": "Model-Free Active Exploration in Reinforcement Learning"
      },
      {
        "paperId": "85e3fbeb7b90ac71a2912870273c4928c457bdb1",
        "title": "Pessimistic value iteration for multi-task data sharing in Offline Reinforcement Learning"
      },
      {
        "paperId": "91edf9d897ddfa4c39f013d6acc0f2a0905ef789",
        "title": "OVD-Explorer: Optimism Should Not Be the Sole Pursuit of Exploration in Noisy Environments"
      },
      {
        "paperId": "034b015f0a8e8f1921e2e4d8c854c939228c4e02",
        "title": "Pitfall of Optimism: Distributional Reinforcement Learning by Randomizing Risk Criterion"
      },
      {
        "paperId": "a5cf1cf614a591c02053e63c3f6e9b8a9ce6cd4c",
        "title": "Distributional Soft Actor-Critic With Three Refinements"
      },
      {
        "paperId": "89455038e4e86388696a68a8b9558666268e44f3",
        "title": "DRL-ORA: Distributional Reinforcement Learning with Online Risk Adaption"
      },
      {
        "paperId": "1e5b393cc44372387d0a07d7dfa0295d740a4276",
        "title": "Learning Risk-Aware Quadrupedal Locomotion using Distributional Reinforcement Learning"
      },
      {
        "paperId": "a747f8a1a5ed771b858b9b466c4b9c57a774c010",
        "title": "Uncertainties in Onboard Algorithms for Autonomous Vehicles: Challenges, Mitigation, and Perspectives"
      },
      {
        "paperId": "701f193b1ffe3c5a9196214974b4b8940baa470b",
        "title": "Robust Quadrupedal Locomotion via Risk-Averse Policy Learning"
      },
      {
        "paperId": "14852c1a50c167091c846b29c203dd1458cf462c",
        "title": "Bag of Policies for Distributional Deep Exploration"
      },
      {
        "paperId": "84c50dfd732b6f4ff5d79d94bb4a905b007ba7c1",
        "title": "Distributional and hierarchical reinforcement learning for physical systems with noisy state observations and exogenous perturbations"
      },
      {
        "paperId": "24309826dd2f3a490ed54c4a482496d1d19cb56d",
        "title": "Variance Control for Distributional Reinforcement Learning"
      },
      {
        "paperId": "a5187bbf4bf8bd905dbbc70a60d606ca54408d51",
        "title": "Diverse Projection Ensembles for Distributional Reinforcement Learning"
      },
      {
        "paperId": "bd2318a4b19eb73e3faf0180f4a09bcfebe5026c",
        "title": "PACER: A Fully Push-forward-based Distributional Reinforcement Learning Algorithm"
      },
      {
        "paperId": "1a73038804052a40c12aae696848ece2168f6da7",
        "title": "On the Importance of Exploration for Generalization in Reinforcement Learning"
      },
      {
        "paperId": "fd3f34a37e3963d3c28b6d389ce4448ff9fd0c33",
        "title": "A Unified Uncertainty-Aware Exploration: Combining Epistemic and Aleatory Uncertainty"
      },
      {
        "paperId": "cdbe0964ec7d90555c9f08d8d4f0c5b6688293a5",
        "title": "A Unified Framework for Factorizing Distributional Value Functions for Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "c71c72e29c8e5b1e0a32fc90d24bdc66726c3d24",
        "title": "Distributional Reinforcement Learning with Dual Expectile-Quantile Regression"
      },
      {
        "paperId": "d2b61fffff1ff1f79f11eeb008b73e1d6e445338",
        "title": "Learning Adaptable Risk-Sensitive Policies to Coordinate in Multi-Agent General-Sum Games"
      },
      {
        "paperId": "d26ea31d844d51f11f0703dfae54fdfe9c0d1092",
        "title": "Wasserstein Actor-Critic: Directed Exploration via Optimism for Continuous-Actions Control"
      },
      {
        "paperId": "7e6b0600c15c8894117df4dab96a3f57a8605e54",
        "title": "Toward Risk-based Optimistic Exploration for Cooperative Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "c2e96bdbcb6075bf551012a043c273844e25fee8",
        "title": "Uncertainty-aware transfer across tasks using hybrid model-based successor feature reinforcement learning\u2606"
      },
      {
        "paperId": "8fe1f7d407423813e6cfd1e880703e2a2d6aa6de",
        "title": "Quasi-optimal Reinforcement Learning with Continuous Actions"
      },
      {
        "paperId": "4da360c81092ecd90ef87d63cc899bc59e6094c4",
        "title": "Multi-Robot Learning Dynamic Obstacle Avoidance in Formation With Information-Directed Exploration"
      },
      {
        "paperId": "9fc755d76a2849d0944095f7479a49b39a5cd974",
        "title": "Quantile Constrained Reinforcement Learning: A Reinforcement Learning Framework Constraining Outage Probability"
      },
      {
        "paperId": "6e9c1dbd61e11d5abc0fcb88bc98100d82e2cac8",
        "title": "Anomaly Detection in Industrial IoT Using Distributional Reinforcement Learning and Generative Adversarial Networks"
      },
      {
        "paperId": "c3d407cc2e3f8394e5e68c020db035e7f54fa2ab",
        "title": "How Does Return Distribution in Distributional Reinforcement Learning Help Optimization?"
      },
      {
        "paperId": "5c80c7c898bd880d65ee1565f627acd454bd7413",
        "title": "Adaptive Discount Factor for Deep Reinforcement Learning in Continuing Tasks with Uncertainty"
      },
      {
        "paperId": "c28cca3538c949b223194a00e14c3575b323ef2f",
        "title": "Normality-Guided Distributional Reinforcement Learning for Continuous Control"
      },
      {
        "paperId": "d038814af60dd92d14325a986a9f06a37dbbd0a9",
        "title": "Reinforcement Learning using Reward Expectations in Scenarios with Aleatoric Uncertainties"
      },
      {
        "paperId": "4b1489711d6fbbd16c25ec5d6ac4d34df4003464",
        "title": "Distributional Actor-Critic Ensemble for Uncertainty-Aware Continuous Control"
      },
      {
        "paperId": "bd8aaab29fa16f40ef016393ba7ca30127abab58",
        "title": "Risk Perspective Exploration in Distributional Reinforcement Learning"
      },
      {
        "paperId": "c3a2e7d52d975b50c9e98e6cca4a7449b3068c2f",
        "title": "Mean-Semivariance Policy Optimization via Risk-Averse Reinforcement Learning"
      },
      {
        "paperId": "f646aee0f654ab1505dff75582c1d8058d1e0571",
        "title": "Conformal Off-Policy Prediction"
      },
      {
        "paperId": "ac294926299b96e21e88ae8cd0182f17af5c2fb1",
        "title": "Learning Generalizable Risk-Sensitive Policies to Coordinate in Decentralized Multi-Agent General-Sum Games"
      },
      {
        "paperId": "fc31f432580ba762a31e60ab3a21f0f155fac5b4",
        "title": "Distributional Hamilton-Jacobi-Bellman Equations for Continuous-Time Reinforcement Learning"
      },
      {
        "paperId": "8e25eaa421da5042884105b51a0b6009c152d7f3",
        "title": "Uncertainty-Aware Portfolio Management With Risk-Sensitive Multiagent Network"
      },
      {
        "paperId": "3a5b68ef4736e235aa596eb6728f284258e11d18",
        "title": "The Sufficiency of Off-Policyness and Soft Clipping: PPO Is Still Insufficient according to an Off-Policy Measure"
      },
      {
        "paperId": "a339a36ba1b58fa9acf014e7888f301881ead74e",
        "title": "A Systematic Study of Deep Q-Networks and Its Variations"
      },
      {
        "paperId": "65410ccf94032cc3f378d89aae7d34c17807dab7",
        "title": "Adaptive Risk-Tendency: Nano Drone Navigation in Cluttered Environments with Distributional Reinforcement Learning"
      },
      {
        "paperId": "23abe79046d7f7b430d5d21b6a93598d0aa1b9c2",
        "title": "Pessimistic Bootstrapping for Uncertainty-Driven Offline Reinforcement Learning"
      },
      {
        "paperId": "b9d78a820bede3cd9a95c4b0978bb60730622e10",
        "title": "Distributional Reinforcement Learning with Regularized Wasserstein Loss"
      },
      {
        "paperId": "3cb13b61804d5ff0d96c7cca127087171c880beb",
        "title": "Exponential Bellman Equation and Improved Regret Bounds for Risk-Sensitive Reinforcement Learning"
      },
      {
        "paperId": "49e793dcf244cca26366bfdc883d999f53fe545e",
        "title": "Damped Anderson Mixing for Deep Reinforcement Learning: Acceleration, Convergence, and Stabilization"
      },
      {
        "paperId": "3c7ab4db4149b686defcbb5bbec09f1977292db7",
        "title": "The Benefits of Being Categorical Distributional: Uncertainty-aware Regularized Exploration in Reinforcement Learning"
      },
      {
        "paperId": "8dc0af9b9f63b14f7b2714c38532788902a3a306",
        "title": "Autonomous Blimp Control using Deep Reinforcement Learning"
      },
      {
        "paperId": "08ead5077f3d683ff6c46cff81b4e1852a0d127b",
        "title": "Exploring the Training Robustness of Distributional Reinforcement Learning Against Noisy State Observations"
      },
      {
        "paperId": "12075ea34f5fbe32ec5582786761ab34d401209b",
        "title": "Exploration in Deep Reinforcement Learning: From Single-Agent to Multiagent Domain"
      },
      {
        "paperId": "5aa1045136814bbff763c9897d116ac4177d1f94",
        "title": "Explainable reinforcement learning in production control of job shop manufacturing system"
      },
      {
        "paperId": "363ba1006cedaf56fa4b7a0073e847a18f6711ef",
        "title": "ADER: Adapting between Exploration and Robustness for Actor-Critic Methods"
      },
      {
        "paperId": "59d129cb6021e2f1ca1aeda68d185aab59c190dc",
        "title": "Exploration via Distributional Reinforcement Learning with Epistemic and Aleatoric Uncertainty Estimation*"
      },
      {
        "paperId": "10ac0ad7595e1e0299e052b24d3fc75bc1bc340c",
        "title": "GMAC: A Distributional Perspective on Actor-Critic Framework"
      },
      {
        "paperId": "58b17c5a115c8440c530c6242a95076618107bf7",
        "title": "Non-decreasing Quantile Function Network with Efficient Exploration for Distributional Reinforcement Learning"
      },
      {
        "paperId": "db5ec14be596e3c52cd2fa29473927cb70239973",
        "title": "Bayesian Distributional Policy Gradients"
      },
      {
        "paperId": "49775f20431c4a605c5dcc7111c9fe785bf00c62",
        "title": "On the Convergence and Optimality of Policy Gradient for Markov Coherent Risk"
      },
      {
        "paperId": "3098e236a15908743cea2e83ca07d1992e7c0f6c",
        "title": "DFAC Framework: Factorizing the Value Function via Quantile Mixture for Multi-Agent Distributional Q-Learning"
      },
      {
        "paperId": "9e227c934191011a59fa7c818ee735c90cadf0d3",
        "title": "Amortized Variational Deep Q Network"
      },
      {
        "paperId": "6722d1de9caa53a80e18028922a38e4f75e7f518",
        "title": "A Novel Approach to EEG Neurofeedback via Reinforcement Learning"
      },
      {
        "paperId": "1ea156c25b2e735e371e6700bbd9ae11d33c5755",
        "title": "Distributional Reinforcement Learning in the Brain"
      },
      {
        "paperId": "76a15540274a9940d22df70a70e7f8cad340d6c0",
        "title": "Controlling Overestimation Bias with Truncated Mixture of Continuous Distributional Quantile Critics"
      },
      {
        "paperId": "db0be3a0c2d554092c9880755ac86e1052235f17",
        "title": "Efficient exploration of zero-sum stochastic games"
      },
      {
        "paperId": "64e827872f7dd3283486cac356ea5b2c1cb9c5df",
        "title": "Data Efficient Training for Reinforcement Learning with Adaptive Behavior Policy Sharing"
      },
      {
        "paperId": "831e7cbafed2dca05db1e7f5ef16d1a7614f44ec",
        "title": "Learning to Reach Goals via Iterated Supervised Learning"
      },
      {
        "paperId": "9b9eff0f9760447db5ccfddd5d11b36ff8595a2e",
        "title": "Being Optimistic to Be Conservative: Quickly Learning a CVaR Policy"
      },
      {
        "paperId": "ffba26a38d4b3c25d2984266f72f72889b2413ff",
        "title": "Learning To Reach Goals Without Reinforcement Learning"
      },
      {
        "paperId": "73d8257a4dd39abd9fbb2c878cbee7b641b10611",
        "title": "Deep Reinforcement Learning with Decorrelation"
      },
      {
        "paperId": "941e31df751884e8319d275ecaa5697407001347",
        "title": "Uncertainty-Aware Rank-One MIMO Q Network Framework for Accelerated Offline Reinforcement Learning"
      },
      {
        "paperId": "50cfa8f4f98be6435fee58df1f876c1b95db083c",
        "title": "Quasi-optimal Learning with Continuous Treatments"
      },
      {
        "paperId": "67e07e0064f88503ded910ebf693b9c476209e07",
        "title": "Distributional Reinforcement Learning with Dual Expectile-Quantile Regression"
      },
      {
        "paperId": "49c8a03c38267939b6c02b2fc56a11e45d3675da",
        "title": "Distributional Reinforcement Learning with Online Risk-awareness Adaption"
      },
      {
        "paperId": "b38cebf9bb052df790a4788dbdddfde9b9c29d8d",
        "title": "DSAC-T: Distributional Soft Actor-Critic with Three Refinements"
      },
      {
        "paperId": "9180f8f576d9d3a90b91a316061b49c168e1c214",
        "title": "I NTERPRETING D ISTRIBUTIONAL R EINFORCEMENT L EARNING : A R EGULARIZATION P ERSPECTIVE"
      },
      {
        "paperId": "592efdf40b60dd68d31452058e213b4859b15a94",
        "title": "Interpreting Distributional Reinforcement Learning: Regularization and Optimization Perspectives"
      },
      {
        "paperId": "7d4b8e25861f0813d16784a5f679930b364df0a0",
        "title": "Uncertainty-Aware Reinforcement Learning for Risk-Sensitive Player Evaluation in Sports Game"
      },
      {
        "paperId": "d1164285427613fa70132d2756347555984e828a",
        "title": "Distributional Reinforcement Learning for Risk-Sensitive Policies"
      },
      {
        "paperId": "2e5dc4b028e670d1b9ac0b4e15580cdb7197cb68",
        "title": "Exploration in Deep Reinforcement Learning: A Comprehensive Survey"
      },
      {
        "paperId": "cf28c18a733810bb7f50a2fc43e0e57867da7656",
        "title": "A Distributional Perspective on Value Function Factorization Methods for Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "dc0ee76b50b9906b20222489ec363fa8cb04026f",
        "title": "Exploring the Robustness of Distributional Reinforcement Learning against Noisy State Observations"
      },
      {
        "paperId": "682f89ca0d2355d861fdae3a44f8c54893058639",
        "title": "Towards Understanding Distributional Reinforcement Learning: Regularization, Optimization, Acceleration and Sinkhorn Algorithm"
      },
      {
        "paperId": "278a35456a230d1152299ea289f3b92a153fb84a",
        "title": "Non-crossing quantile regression for deep reinforcement learning"
      },
      {
        "paperId": "99fa17deb374468cd7e3f738ce4aebc1b7f4010b",
        "title": "Distributional Reinforcement Learning for Optimal Execution"
      },
      {
        "paperId": "3658dada1d583f449d0571b0cf5508bf41a25d23",
        "title": "Deep Q-Networks"
      },
      {
        "paperId": "a325ce6b0eae680ce0b8b53ff0be89c58a2fc68a",
        "title": "Distributional Reinforcement Learning by Sinkhorn Divergence"
      },
      {
        "paperId": "6078a06aa53d0069afc3abc9dcf5dd93206e93ee",
        "title": "Distributional Monte-Carlo Planning with Thompson Sampling in Stochastic Environments"
      }
    ],
    "score": 15.333333333333332
  },
  {
    "id": "1f4484086d210a2c44efe5eef0a2b42647822abf",
    "title": "Breaking the Sample Complexity Barrier to Regret-Optimal Model-Free Reinforcement Learning",
    "authors": [
      "Gen Li",
      "Laixi Shi",
      "Yuxin Chen",
      "Yuantao Gu",
      "Yuejie Chi"
    ],
    "year": 2021,
    "citationCount": 59,
    "abstract": "\n Achieving sample efficiency in online episodic reinforcement learning (RL) requires optimally balancing exploration and exploitation. When it comes to a finite-horizon episodic Markov decision process with $S$ states, $A$ actions and horizon length $H$, substantial progress has been achieved toward characterizing the minimax-optimal regret, which scales on the order of $\\sqrt{H^2SAT}$ (modulo log factors) with $T$ the total number of samples. While several competing solution paradigms have been proposed to minimize regret, they are either memory-inefficient, or fall short of optimality unless the sample size exceeds an enormous threshold (e.g. $S^6A^4 \\,\\mathrm{poly}(H)$ for existing model-free methods).\n To overcome such a large sample size barrier to efficient RL, we design a novel model-free algorithm, with space complexity $O(SAH)$, that achieves near-optimal regret as soon as the sample size exceeds the order of $SA\\,\\mathrm{poly}(H)$. In terms of this sample size requirement (also referred to the initial burn-in cost), our method improves\u2014by at least a factor of $S^5A^3$\u2014upon any prior memory-efficient algorithm that is asymptotically regret-optimal. Leveraging the recently introduced variance reduction strategy (also called reference-advantage decomposition), the proposed algorithm employs an early-settled reference update rule, with the aid of two Q-learning sequences with upper and lower confidence bounds. The design principle of our early-settled variance reduction method might be of independent interest to other RL settings that involve intricate exploration\u2013exploitation trade-offs.",
    "url": "https://www.semanticscholar.org/paper/1f4484086d210a2c44efe5eef0a2b42647822abf",
    "pdf_url": "https://arxiv.org/pdf/2110.04645.pdf",
    "venue": "Neural Information Processing Systems",
    "publicationDate": "2021-10-09",
    "externalIds": {
      "ArXiv": "2110.04645",
      "DBLP": "journals/corr/abs-2110-04645",
      "DOI": "10.1093/imaiai/iaac034",
      "CorpusId": 238582812
    },
    "references": [
      {
        "paperId": "dc4304bcf4b4ba2da815a1b777f2c8b92cf22e91",
        "title": "Model\u2010Based Reinforcement Learning"
      },
      {
        "paperId": "b799c782f168b0a02ebab9e50ff38ded1bc79aee",
        "title": "Optimistic posterior sampling for reinforcement learning: worst-case regret bounds"
      },
      {
        "paperId": "e1ac9023f2991ebc840b99d6f0203201a3adfaf4",
        "title": "Pessimistic Q-Learning for Offline Reinforcement Learning: Towards Optimal Sample Complexity"
      },
      {
        "paperId": "31f5755044ba4b05cff59b8e83bd049057311925",
        "title": "Sample-Efficient Reinforcement Learning for Linearly-Parameterized MDPs with a Generative Model"
      },
      {
        "paperId": "f896bc2d93102c98e1b2d57b0d922642e4197b9b",
        "title": "Sample-Efficient Reinforcement Learning Is Feasible for Linearly Realizable MDPs with Limited Revisiting"
      },
      {
        "paperId": "6056961e0e4b5fb8bdce7aa64a8499e5a38da7df",
        "title": "UCB Momentum Q-learning: Correcting the bias without forgetting"
      },
      {
        "paperId": "a7d8c132589e4e90da4f417f81fe2068afed9091",
        "title": "Is Q-Learning Minimax Optimal? A Tight Sample Complexity Analysis"
      },
      {
        "paperId": "dca78329156aa009a61bb54d76f9f481d4204c7a",
        "title": "A Lyapunov Theory for Finite-Sample Guarantees of Asynchronous Q-Learning and TD-Learning Variants"
      },
      {
        "paperId": "0f492be75b051168d96450ad3b7d21bb23ab0883",
        "title": "Near-Optimal Offline Reinforcement Learning via Double Variance Reduction"
      },
      {
        "paperId": "157c830c85fc7ee4aa360c72fa7bb9426de5f5b2",
        "title": "Provably Efficient Reinforcement Learning with Linear Function Approximation Under Adaptivity Constraints"
      },
      {
        "paperId": "0b0c82e33d3328246b6adc3ef2b55be9b606a0cd",
        "title": "Episodic Reinforcement Learning in Finite MDPs: Minimax Lower Bounds Revisited"
      },
      {
        "paperId": "a31adb9690f5a39aab9a2438037b734335c868cc",
        "title": "Variance-Reduced Methods for Machine Learning"
      },
      {
        "paperId": "4817babbcf49da7f1f05462d96563e62a84218aa",
        "title": "Nearly Minimax Optimal Reinforcement Learning for Discounted MDPs"
      },
      {
        "paperId": "58438a6f0610667a06b141ff9347cd508587a5dc",
        "title": "Finite-Time Analysis for Double Q-learning"
      },
      {
        "paperId": "93e5b29890d3fbd7f8732bafbcad310e0c78078e",
        "title": "Is Reinforcement Learning More Difficult Than Bandits? A Near-optimal Algorithm Escaping the Curse of Horizon"
      },
      {
        "paperId": "bac8033fd076c981895c392b78f5450b4a84a984",
        "title": "Momentum Q-learning with Finite-Sample Convergence Guarantee"
      },
      {
        "paperId": "108068a5be22ddd2134c230c6c623ad63ed22f9f",
        "title": "Model-Based Multi-Agent RL in Zero-Sum Markov Games with Near-Optimal Sample Complexity"
      },
      {
        "paperId": "57ffba5ea5e602b5365752e19a6f85a716db1d8b",
        "title": "Bandit Algorithms"
      },
      {
        "paperId": "7d3699cf0e8a822cd8ae99b8eeed146930d24a0f",
        "title": "On Optimism in Model-Based Reinforcement Learning"
      },
      {
        "paperId": "8a0ec47a51ed130a83a599c5c81c570fe1689cf3",
        "title": "Q-learning with Logarithmic Regret"
      },
      {
        "paperId": "8ba68aaa2e799a2f4c7d3463a8c4d214525969b7",
        "title": "A Model-free Learning Algorithm for Infinite-horizon Average-reward MDPs with Near-optimal Regret"
      },
      {
        "paperId": "0538928f1937389eea0d6ba2ad4be09854d74fda",
        "title": "Model-Free Reinforcement Learning: from Clipped Pseudo-Regret to Sample Complexity"
      },
      {
        "paperId": "3f01a03a284ad2d1745e6d0d378384bcab49bd3e",
        "title": "Sample Complexity of Asynchronous Q-Learning: Sharper Analysis and Variance Reduction"
      },
      {
        "paperId": "ad6770658096a738d0ca0abcd56b392c80017488",
        "title": "Breaking the Sample Size Barrier in Model-Based Reinforcement Learning with a Generative Model"
      },
      {
        "paperId": "8046dbd4ebc3fda0fcc43d9110c0d2d940052980",
        "title": "Almost Optimal Model-Free Reinforcement Learning via Reference-Advantage Decomposition"
      },
      {
        "paperId": "e906088fe764efc5a6213d29ed04605ccca0829c",
        "title": "Is Temporal Difference Learning Optimal? An Instance-Dependent Analysis"
      },
      {
        "paperId": "2fd5067ad36c423db1d718fa67709521dd09e987",
        "title": "$\\gamma$-Regret for Non-Episodic Reinforcement Learning."
      },
      {
        "paperId": "9ee544d094c0673d30867bbc5ef511b0da2f1d4b",
        "title": "Finite-Sample Analysis of Stochastic Approximation Using Smooth Convex Envelopes"
      },
      {
        "paperId": "5fe5376f88ff94e67ef7f8677a7cb70820bb8338",
        "title": "Finite-Time Analysis of Asynchronous Stochastic Approximation and Q-Learning"
      },
      {
        "paperId": "d72af20c174ed39f45a1538c2fe9444c26686243",
        "title": "Reanalysis of Variance Reduced Temporal Difference Learning"
      },
      {
        "paperId": "8e98719529f8e029210b6c31d54e7486ee00d0af",
        "title": "Is a Good Representation Sufficient for Sample Efficient Reinforcement Learning?"
      },
      {
        "paperId": "9172c8e49be3bf204d3cecfad6ba1a7b4ddd2207",
        "title": "Variance-reduced Q-learning is minimax optimal"
      },
      {
        "paperId": "10f2b0f9744aec689b2fc75740bf8d9d9c1896e3",
        "title": "Model-Based Reinforcement Learning with a Generative Model is Minimax Optimal"
      },
      {
        "paperId": "6101d16008e536740adb97588616af51fe392950",
        "title": "Provably Efficient Q-Learning with Low Switching Cost"
      },
      {
        "paperId": "e460cccfa9c09bcbcd7913b179dbe78866334bec",
        "title": "Stochastic approximation with cone-contractive operators: Sharp \ud835\udcc1\u221e-bounds for Q-learning"
      },
      {
        "paperId": "6246b3d27f6a9f211126cafe39d8c8a7f6ed06f4",
        "title": "Tight Regret Bounds for Model-Based Reinforcement Learning with Greedy Policies"
      },
      {
        "paperId": "225cc727daeca281f4f932a70765e0a32d849d6b",
        "title": "Q-learning with UCB Exploration is Sample Efficient for Infinite-Horizon MDP"
      },
      {
        "paperId": "6dae703128d9caff2623eb8dfe2526dc6ad7aff5",
        "title": "A Theoretical Analysis of Deep Q-Learning"
      },
      {
        "paperId": "f14ea2243fe74cbf1a4ea86cebed1fb9547c3a7c",
        "title": "Tighter Problem-Dependent Regret Bounds in Reinforcement Learning without Domain Knowledge using Value Function Bounds"
      },
      {
        "paperId": "03cc81e98942bdafd994af7a1d1e62a68ff8b682",
        "title": "Is Q-learning Provably Efficient?"
      },
      {
        "paperId": "a50adbbed1f8097d0a0b1b0fc0a1c8b5fa938a7a",
        "title": "Variance reduced value iteration and faster algorithms for solving Markov decision processes"
      },
      {
        "paperId": "67d10cea808937089d6492acff14ad9ef156e3c5",
        "title": "Minimax Regret Bounds for Reinforcement Learning"
      },
      {
        "paperId": "db4d0e45560ceda35b6212036513bd4ab59ce99d",
        "title": "SARAH: A Novel Method for Machine Learning Problems Using Stochastic Recursive Gradient"
      },
      {
        "paperId": "5dcc07acb63cc909c5be701c1c88fef3718ba326",
        "title": "Unifying PAC and Regret: Uniform PAC Bounds for Episodic Reinforcement Learning"
      },
      {
        "paperId": "4ce25912f8f0b7bcac53cbc4d8e0ca867f2109bb",
        "title": "Stochastic Variance Reduction Methods for Policy Evaluation"
      },
      {
        "paperId": "4475369e936d1375cea79a9d2e19ffe18eefd0c1",
        "title": "On Lower Bounds for Regret in Reinforcement Learning"
      },
      {
        "paperId": "43c05444fbc239321f6676f3cd539cac34fde7b8",
        "title": "Accelerating Stochastic Gradient Descent using Predictive Variance Reduction"
      },
      {
        "paperId": "2de057e8c2d285646708393f7adf4bec8bf1f4e7",
        "title": "Error bounds for constant step-size Q-learning"
      },
      {
        "paperId": "af61e43831d35ec4f4db5e60f227629d2f39a437",
        "title": "On the Sample Complexity of Reinforcement Learning with a Generative Model"
      },
      {
        "paperId": "fc930ea96445f010b8ba33ad81b4e2223c4bdabe",
        "title": "Topics in Random Matrix Theory"
      },
      {
        "paperId": "115dcc3078148f638115f1d7d23bd09710957f44",
        "title": "Speedy Q-Learning"
      },
      {
        "paperId": "7adf76bdc94f3a9054187c96fe7a2c34cbebf354",
        "title": "FREEDMAN'S INEQUALITY FOR MATRIX MARTINGALES"
      },
      {
        "paperId": "83c3e0371aa52d1ed6be3201b31f0233c0222f4c",
        "title": "UCB revisited: Improved regret bounds for the stochastic multi-armed bandit problem"
      },
      {
        "paperId": "71886c33a34c6effe14f465ebe2806383e0d76a3",
        "title": "REGAL: A Regularization based Algorithm for Reinforcement Learning in Weakly Communicating MDPs"
      },
      {
        "paperId": "0cafe2903b097fc042782c359cb231ea34ef7ed3",
        "title": "Near-optimal Regret Bounds for Reinforcement Learning"
      },
      {
        "paperId": "187f3f984e6f375178f41827ab90c4e748773fa7",
        "title": "PAC model-free reinforcement learning"
      },
      {
        "paperId": "f913bda6d242bb968a68b19daaf4206913df5c8b",
        "title": "A Generalization Error for Q-Learning"
      },
      {
        "paperId": "a4fc537e82dfa6d0788bfdebf6b300ec1a4b2275",
        "title": "Learning Rates for Q-learning"
      },
      {
        "paperId": "e58c2e9042c29ed3972e35a1a9a2db03a1d288b7",
        "title": "The Asymptotic Convergence-Rate of Q-learning"
      },
      {
        "paperId": "fd77430f6f5c5e35e8a45ff3478032b680fa0b0c",
        "title": "To all authors"
      },
      {
        "paperId": "59b50a775542e87f078db35b868ac10ab43d4c75",
        "title": "Learning from delayed rewards"
      },
      {
        "paperId": "a82db864e472b5aa6313596ef9919f64e3363b1f",
        "title": "Dynamic Programming and Optimal Control, Two Volume Set"
      },
      {
        "paperId": "621c03cd67b0b7bac665f3c7887481b4b42f269c",
        "title": "Asynchronous Stochastic Approximation and Q-Learning"
      },
      {
        "paperId": "a9cd8efe9184dddb1bedbbec3a356c4dfb22fe63",
        "title": "Markov Decision Processes: Discrete Stochastic Dynamic Programming"
      },
      {
        "paperId": "03b7e51c52084ac1db5118342a00b5fbcfc587aa",
        "title": "Q-learning"
      },
      {
        "paperId": "4c9e36a0dbdc3e7b20e38bd288a804c05c77e9fa",
        "title": "Asymptotically efficient adaptive allocation rules"
      },
      {
        "paperId": "531aab3000f5373d2c99c2c171b7185f83f3a167",
        "title": "On Tail Probabilities for Martingales"
      },
      {
        "paperId": "34ddd8865569c2c32dec9bf7ffc817ff42faaa01",
        "title": "A Stochastic Approximation Method"
      },
      {
        "paperId": "0597b6577478bfb6f1438a71db38f2f3f348c84f",
        "title": "Variance Reduced Policy Evaluation with Smooth Function Approximation"
      },
      {
        "paperId": "adb2004bbd2b028e3c43bdc2ffb1975538135189",
        "title": "Provably Efficient Q-learning with Function Approximation via Distribution Shift Error Checking Oracle"
      },
      {
        "paperId": null,
        "title": "Non-asymptotic sample analysis and probably approximately correct (PAC) bounds have seen extensive developments in the last several years"
      },
      {
        "paperId": "6a50cfcd6eec045e322c5e9b7792605240d69142",
        "title": "Near-Optimal Time and Sample Complexities for Solving Markov Decision Processes with a Generative Model"
      },
      {
        "paperId": null,
        "title": "beating the state-of-the-art model-free algorithms by achieving optimality for the entire sample size range"
      },
      {
        "paperId": null,
        "title": "The seminal idea of variance reduction was originally proposed to accelerate finite-sum stochastic optimization"
      },
      {
        "paperId": null,
        "title": "2021a) for the synchronous setting (the case with access to a generative model or a simulator), and the works of Even-Dar and Mansour"
      },
      {
        "paperId": null,
        "title": "2016) with the purpose of facilitating comparison with Bartlett and"
      },
      {
        "paperId": null,
        "title": "Advances in neural information processing systems, pages 315\u2013323"
      },
      {
        "paperId": "a9e5a40b0ff5c40d2db7b73490922e115576adb5",
        "title": "On the sample complexity of reinforcement learning."
      },
      {
        "paperId": "26e17f6b62a7caec660b3356d49e879e6e0eeabc",
        "title": "The Nonstochastic Multiarmed Bandit Problem"
      },
      {
        "paperId": "4e9d797427cd56be90932d6092fc3b6282dfb96f",
        "title": "On the Convergence of Stochastic Iterative Dynamic Programming Algorithms"
      },
      {
        "paperId": "81ebc74099d55b184e9e54617606989ad856e2d4",
        "title": "Graduate studies in mathematics"
      },
      {
        "paperId": null,
        "title": "and the references therein. PAC bounds for synchronous and asynchronous Q-learning. Q-learning is arguably among the most famous model-free algorithms developed in the RL literature"
      },
      {
        "paperId": null,
        "title": "A Further related works"
      },
      {
        "paperId": null,
        "title": "Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content"
      },
      {
        "paperId": null,
        "title": "Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster"
      },
      {
        "paperId": null,
        "title": "data, models) or curating/releasing new assets... (a) If your work uses existing assets, did you cite the creators?"
      },
      {
        "paperId": null,
        "title": "Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope"
      },
      {
        "paperId": null,
        "title": "Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation"
      },
      {
        "paperId": null,
        "title": "Did you include the full text of instructions given to participants and screenshots"
      },
      {
        "paperId": null,
        "title": "Did you report error bars (e.g., with respect to the random seed after running experiments multiple times"
      },
      {
        "paperId": null,
        "title": "Have you read the ethics review guidelines and ensured that your paper conforms to them"
      },
      {
        "paperId": null,
        "title": "Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable?"
      },
      {
        "paperId": null,
        "title": "(a) Did you state the full set of assumptions of all theoretical results?"
      },
      {
        "paperId": null,
        "title": "Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL"
      },
      {
        "paperId": null,
        "title": "If you used crowdsourcing or conducted research with human subjects"
      },
      {
        "paperId": null,
        "title": "Did you describe the limitations of your work"
      }
    ],
    "cited_by": [
      {
        "paperId": "da121879296767263aa2217b248181a554ac886d",
        "title": "Provably Near-Optimal Distributionally Robust Reinforcement Learning in Online Settings"
      },
      {
        "paperId": "06249c7f0b53d340e7deb0d4de63a9aba1e3ae59",
        "title": "Online Robust Multi-Agent Reinforcement Learning under Model Uncertainties"
      },
      {
        "paperId": "985796f3006ffe41eecaa9096be68ccc2f1096c1",
        "title": "Statistical and Algorithmic Foundations of Reinforcement Learning"
      },
      {
        "paperId": "c3e8ea89d747c32972489fc4708811e216c3d39d",
        "title": "Provably Efficient and Agile Randomized Q-Learning"
      },
      {
        "paperId": "22d3dd735c8da424d98b33d23d01dbada6046be3",
        "title": "Sharp Gap-Dependent Variance-Aware Regret Bounds for Tabular MDPs"
      },
      {
        "paperId": "95148ebccb6062606261ce17199b3326fa601620",
        "title": "Regret-Optimal Q-Learning with Low Cost for Single-Agent and Federated Reinforcement Learning"
      },
      {
        "paperId": "475cb759ee7f3fa56c0cdfeac035907da6b8a115",
        "title": "Q-learning with Posterior Sampling"
      },
      {
        "paperId": "16461beac610c996ee87822e102f3fca30d8ef7f",
        "title": "When a Reinforcement Learning Agent Encounters Unknown Unknowns"
      },
      {
        "paperId": "518dda4b975106b7a206e0b3e1e15d659e3432be",
        "title": "Minimax Optimal Reinforcement Learning with Quasi-Optimism"
      },
      {
        "paperId": "f9dd1d4714c5043834b697b2adbc8d8487e7abb1",
        "title": "Data-Efficient Multi-Agent Spatial Planning with LLMs"
      },
      {
        "paperId": "dab91fc8eddb34879b1014b69945dc6575ba1742",
        "title": "Gap-Dependent Bounds for Federated Q-learning"
      },
      {
        "paperId": "c1cd531210bfb01bb3871bc6e13400ea702dbcc5",
        "title": "Increasing Information for Model Predictive Control with Semi-Markov Decision Processes"
      },
      {
        "paperId": "0b94bb75aacb78afb24915bdc5943cdaad349c09",
        "title": "Federated UCBVI: Communication-Efficient Federated Regret Minimization with Heterogeneous Agents"
      },
      {
        "paperId": "cf719230aea682499c43580a40f8fbb6c4181d97",
        "title": "Gap-Dependent Bounds for Q-Learning using Reference-Advantage Decomposition"
      },
      {
        "paperId": "e2b532fd6ce71a50270285ea9b9c46220144665d",
        "title": "State-free Reinforcement Learning"
      },
      {
        "paperId": "e40a8725fe979a74cc570bf0380976ee7549cb05",
        "title": "The Sample-Communication Complexity Trade-off in Federated Q-Learning"
      },
      {
        "paperId": "3cfd1e00bb0ecfbd1728049a332dbc97545e4d43",
        "title": "Optimistic Q-learning for average reward and episodic reinforcement learning"
      },
      {
        "paperId": "ef09d5e6fa6766cb5b5dfbda1a621539e741075e",
        "title": "Combinatorial Multivariant Multi-Armed Bandits with Applications to Episodic Reinforcement Learning and Beyond"
      },
      {
        "paperId": "c7303bed6a0f32ce345a60a8b71a5cb21da12d1f",
        "title": "Finding good policies in average-reward Markov Decision Processes without prior knowledge"
      },
      {
        "paperId": "2d7a8adad0b08335dad827e5c981da4e489f0975",
        "title": "Horizon-Free Regret for Linear Markov Decision Processes"
      },
      {
        "paperId": "8364ae9b3f7bd8533af069b854498b7e389176b8",
        "title": "Sample Efficient Reinforcement Learning with Partial Dynamics Knowledge"
      },
      {
        "paperId": "1675bce7fca4eb2171f68755e79c399060087f23",
        "title": "Probabilistic Inference in Reinforcement Learning Done Right"
      },
      {
        "paperId": "540b6f606c35544a8b6dd890d218b3290d570389",
        "title": "CIRLEM: a synergic integration of Collective Intelligence and Reinforcement learning in Energy Management for enhanced climate resilience and lightweight computation"
      },
      {
        "paperId": "140c8b11e69c5cb87655a27852b648e57380c9d4",
        "title": "Minimax Optimal Q Learning With Nearest Neighbors"
      },
      {
        "paperId": "4c3bdd75992aaa51950d8da12cfa2b4f4e0165cc",
        "title": "Settling the Sample Complexity of Online Reinforcement Learning"
      },
      {
        "paperId": "7a08a465b5d6d479e94576e3fcdc09ae13c2d9e4",
        "title": "Provably Efficient UCB-type Algorithms For Learning Predictive State Representations"
      },
      {
        "paperId": "0c92a067444273f8ad3ace54d59d31937b0508f3",
        "title": "Sharper Model-free Reinforcement Learning for Average-reward Markov Decision Processes"
      },
      {
        "paperId": "d116ba398a85cd838618796a10638b9d56b58250",
        "title": "Offline Quantum Reinforcement Learning in a Conservative Manner"
      },
      {
        "paperId": "d28d827c092096f7069a46ef636a590438da2321",
        "title": "Learning State Conditioned Linear Mappings for Low-Dimensional Control of Robotic Manipulators"
      },
      {
        "paperId": "2551852e940e8bbaf728049f3df3613380d4d48a",
        "title": "The Curious Price of Distributional Robustness in Reinforcement Learning with a Generative Model"
      },
      {
        "paperId": "9a09099b53349214fd3301f3e4dacf9a71716452",
        "title": "Regret-Optimal Model-Free Reinforcement Learning for Discounted MDPs with Short Burn-In Time"
      },
      {
        "paperId": "5279bc7a28aace6d516603e2e3d7cfa1abd3c0e8",
        "title": "The Blessing of Heterogeneity in Federated Q-learning: Linear Speedup and Beyond"
      },
      {
        "paperId": "0f5e5d2e14279137c59cf7a23b3bfed8405bbcda",
        "title": "Reward-agnostic Fine-tuning: Provable Statistical Benefits of Hybrid Reinforcement Learning"
      },
      {
        "paperId": "5071dda43602ad4dfbba90aced6122cfe2b16abf",
        "title": "Minimax-Optimal Reward-Agnostic Exploration in Reinforcement Learning"
      },
      {
        "paperId": "ac97c8e14033741627dad9f33f0212dbea9c4821",
        "title": "Sharp Variance-Dependent Bounds in Reinforcement Learning: Best of Both Worlds in Stochastic and Deterministic Environments"
      },
      {
        "paperId": "5bb567f9b6ecdac86345495871636fde48adc410",
        "title": "Off-Policy Deep Reinforcement Learning Algorithms for Handling Various Robotic Manipulator Tasks"
      },
      {
        "paperId": "203a1c0e5025489a52c030adbbc102a787685ee6",
        "title": "Unpacking Reward Shaping: Understanding the Benefits of Reward Engineering on Sample Complexity"
      },
      {
        "paperId": "86785b861d24776bdc0f123e73a8a401f25d72e4",
        "title": "Curriculum Reinforcement Learning using Optimal Transport via Gradual Domain Adaptation"
      },
      {
        "paperId": "11615db92fa259ef0904f67763956439dfa40ac8",
        "title": "Multi-armed Bandit Learning on a Graph"
      },
      {
        "paperId": "46a0c00d6ffdb919af244700f57831d64622c85c",
        "title": "Minimax-Optimal Multi-Agent RL in Markov Games With a Generative Model"
      },
      {
        "paperId": "cb8ba10995ed9509b31f326ad1f24d1eadfd8e62",
        "title": "Slowly Changing Adversarial Bandit Algorithms are Efficient for Discounted MDPs"
      },
      {
        "paperId": "5898955105e10baa17ad224263449e23f2a6c0ab",
        "title": "Horizon-Free Reinforcement Learning in Polynomial Time: the Power of Stationary Policies"
      },
      {
        "paperId": "8a20cc7983e748615e912e59688defc9b7db4be7",
        "title": "The Efficacy of Pessimism in Asynchronous Q-Learning"
      },
      {
        "paperId": "eeab9e912d9cd406f3e64bce21e9802e98f889c6",
        "title": "Target Network and Truncation Overcome The Deadly triad in Q-Learning"
      },
      {
        "paperId": "e1ac9023f2991ebc840b99d6f0203201a3adfaf4",
        "title": "Pessimistic Q-Learning for Offline Reinforcement Learning: Towards Optimal Sample Complexity"
      },
      {
        "paperId": "ee3c37ac108bce49db718cb654764864aa3e8048",
        "title": "When is Offline Two-Player Zero-Sum Markov Game Solvable?"
      },
      {
        "paperId": "668d882897d1dfd3cc4b59823388138c0764a46f",
        "title": "Online Sub-Sampling for Reinforcement Learning with General Function Approximation"
      },
      {
        "paperId": "f896bc2d93102c98e1b2d57b0d922642e4197b9b",
        "title": "Sample-Efficient Reinforcement Learning Is Feasible for Linearly Realizable MDPs with Limited Revisiting"
      },
      {
        "paperId": "e2381b66f552eacfb5f8ec3bdf6bb98c5e1e50f7",
        "title": "Near-Optimal Randomized Exploration for Tabular Markov Decision Processes"
      },
      {
        "paperId": "a7d8c132589e4e90da4f417f81fe2068afed9091",
        "title": "Is Q-Learning Minimax Optimal? A Tight Sample Complexity Analysis"
      },
      {
        "paperId": "ad6770658096a738d0ca0abcd56b392c80017488",
        "title": "Breaking the Sample Size Barrier in Model-Based Reinforcement Learning with a Generative Model"
      },
      {
        "paperId": "716fd9bf371675b08cc14325aed2fd4030df8980",
        "title": "Model-Free Offline Reinforcement Learning with Enhanced Robustness"
      },
      {
        "paperId": "b8f3ef5753c06ac70f65b26388fda033b9fe7888",
        "title": "Information-Directed Pessimism for Offline Reinforcement Learning"
      },
      {
        "paperId": "a90b86a9950d2a584fce2d17f2e8fb54b94fee3d",
        "title": "Slowly Changing Adversarial Bandit Algorithms are Provably Efficient for Discounted MDPs"
      },
      {
        "paperId": "2c688228af28b23576a31a132abe76b99571d3a0",
        "title": "Minimax-Optimal Multi-Agent RL in Zero-Sum Markov Games With a Generative Model"
      },
      {
        "paperId": "aa318bea537c928979015d38c65e9f3f82d930d4",
        "title": "Actuation Subspace Prediction with Neural Householder Transforms by Kerrick Johnstonbaugh"
      },
      {
        "paperId": "f857a918f025712c3340ca5023b69b60f993accc",
        "title": "When are Offline Two-Player Zero-Sum Markov Games Solvable?"
      },
      {
        "paperId": "9c8f7ce53e0ec9d0d0a53fdb0a051def427df718",
        "title": "Near-Optimal Randomized Exploration for Tabular MDP"
      },
      {
        "paperId": null,
        "title": "Deep Reinforcement Learning Off-policy Algorithms and Benchmark for Solving Various Robotic Manipulator Tasks"
      }
    ],
    "score": 14.75
  },
  {
    "id": "04615a9955bce148aa7ba29e864389c26e10523a",
    "title": "DEP-RL: Embodied Exploration for Reinforcement Learning in Overactuated and Musculoskeletal Systems",
    "authors": [
      "Pierre Schumacher",
      "D. Haeufle",
      "Dieter B\u00fcchler",
      "S. Schmitt",
      "G. Martius"
    ],
    "year": 2022,
    "citationCount": 44,
    "abstract": "Muscle-actuated organisms are capable of learning an unparalleled diversity of dexterous movements despite their vast amount of muscles. Reinforcement learning (RL) on large musculoskeletal models, however, has not been able to show similar performance. We conjecture that ineffective exploration in large overactuated action spaces is a key problem. This is supported by the finding that common exploration noise strategies are inadequate in synthetic examples of overactuated systems. We identify differential extrinsic plasticity (DEP), a method from the domain of self-organization, as being able to induce state-space covering exploration within seconds of interaction. By integrating DEP into RL, we achieve fast learning of reaching and locomotion in musculoskeletal systems, outperforming current approaches in all considered tasks in sample efficiency and robustness.",
    "url": "https://www.semanticscholar.org/paper/04615a9955bce148aa7ba29e864389c26e10523a",
    "pdf_url": "https://arxiv.org/pdf/2206.00484.pdf",
    "venue": "International Conference on Learning Representations",
    "publicationDate": "2022-05-30",
    "externalIds": {
      "ArXiv": "2206.00484",
      "DBLP": "journals/corr/abs-2206-00484",
      "DOI": "10.48550/arXiv.2206.00484",
      "CorpusId": 249240475
    },
    "references": [
      {
        "paperId": "cd6f1000349e93e2ba731c59802a763de69624c6",
        "title": "Evaluation of Methods for the Extraction of Spatial Muscle Synergies"
      },
      {
        "paperId": "1e7cf32cb71b0307efa00ab4a43def896b7b87ce",
        "title": "Robust walking control of a lower limb rehabilitation exoskeleton coupled with a musculoskeletal model via deep reinforcement learning"
      },
      {
        "paperId": "1e0f91b88b60b3bb7f35824d0526263bb6e39781",
        "title": "OstrichRL: A Musculoskeletal Ostrich Simulation to Study Bio-mechanical Locomotion"
      },
      {
        "paperId": "cfabb20df17109036a82934cc12b5c8e92223f6c",
        "title": "When should agents explore?"
      },
      {
        "paperId": "cb07622ecd658294e9b452b4942fc6e93ba35f76",
        "title": "Hindsight Experience Replay Improves Reinforcement Learning for Control of a MIMO Musculoskeletal Model of the Human Arm"
      },
      {
        "paperId": "eab48036490e71163b8cf527eea9e9473563940b",
        "title": "A geometry- and muscle-based control architecture for synthesising biological movement"
      },
      {
        "paperId": "c614fff2b8bd80fc442fadda4f571e22258cac4a",
        "title": "Modeling muscle function using experimentally determined subject-specific muscle properties."
      },
      {
        "paperId": "86fb6940a9d051b067f3cf32a28c9bd7ccb73f51",
        "title": "Action Priors for Large Action Spaces in Robotics"
      },
      {
        "paperId": "f5275f5eb6569ddb5ba9a959ede09875d56e3bac",
        "title": "Parrot: Data-Driven Behavioral Priors for Reinforcement Learning"
      },
      {
        "paperId": "96b550566afd105d0e2b28d66b677b64084c5d8f",
        "title": "Tonic: A Deep Reinforcement Learning Library for Fast Prototyping and Benchmarking"
      },
      {
        "paperId": "bfe1aba96fbd7a8396fa8975981edbed6a1fa688",
        "title": "Reinforcement learning control of a biomechanical model of the upper extremity"
      },
      {
        "paperId": "20473d88bb151fd94b05ea6ce3950113f4e9d10f",
        "title": "How do Offline Measures for Exploration in Reinforcement Learning behave?"
      },
      {
        "paperId": "467658037d60be494126f8c00974fb30593403f1",
        "title": "Learning to Represent Action Values as a Hypergraph on the Action Vertices"
      },
      {
        "paperId": "129327cdbd9e536408d652a506119d5b598a0cfd",
        "title": "Sample-efficient Cross-Entropy Method for Real-time Planning"
      },
      {
        "paperId": "9fbef1a676813d395ced6d5b7ba3c39f1db11e21",
        "title": "Deep reinforcement learning for modeling human locomotion control in neuromechanical simulation"
      },
      {
        "paperId": "df54925a14193d4aa730db8625861d42d108193f",
        "title": "Reinforcement Learning of Musculoskeletal Control from Functional Simulations"
      },
      {
        "paperId": "2eb5e92d2447d35dd1fd12cdb0b8da0c183ec4f9",
        "title": "Converting Biomechanical Models from OpenSim to MuJoCo"
      },
      {
        "paperId": "250cb20d0d3944f36ad914e86bd871455d5d5f2c",
        "title": "The effects of motor modularity on performance, learning and generalizability in upper-extremity reaching: a computational analysis"
      },
      {
        "paperId": "616ac6772508e72084360158078058dfa8bda7e7",
        "title": "First return, then explore"
      },
      {
        "paperId": "1aebf14c3fe7d10f47a9cedaf99faa79a2d1800d",
        "title": "In vitro virtual reality: an anatomically explicit musculoskeletal simulation powered by in vitro muscle using closed-loop tissue\u2013software interaction"
      },
      {
        "paperId": "af62b70e1eb89e4f412e8cc470021444f6de08f4",
        "title": "Growing Up Together: Structured Exploration for Large Action Spaces"
      },
      {
        "paperId": "54a9941279ffaaac37e1d623d3d9d30dbbd35aaa",
        "title": "Scalable muscle-actuated human simulation and control"
      },
      {
        "paperId": "2e01adc302623c68feea2cfd206a24c593ab27ba",
        "title": "Growing Action Spaces"
      },
      {
        "paperId": "50442d91a7729bc453a84186a6656b7b1e09f930",
        "title": "SCONE: Open Source Software for Predictive Simulation of Biological Motion"
      },
      {
        "paperId": "4e33e392cb9a5395bf2e8d2e0598ef41814f21b8",
        "title": "Synthesis of biologically realistic human motion using joint torque actuation"
      },
      {
        "paperId": "1d1aa637eeedc98a96cf4f444c39e6c756926aab",
        "title": "Artificial Intelligence for Prosthetics - challenge solutions"
      },
      {
        "paperId": "358bf076ce474a7a2426b7271f4a6b62ee212515",
        "title": "Learning Continuous Muscle Control for a Multi-joint Arm by Extending Proximal Policy Optimization with a Liquid State Machine"
      },
      {
        "paperId": "9a3238ae92d2168c408bafe12862c69636b3e92a",
        "title": "Systematic self-exploration of behaviors for robots in a dynamical systems framework"
      },
      {
        "paperId": "1e2ec73184f2ee8918d404c10b8775c91260646c",
        "title": "Learning to Control Redundant Musculoskeletal Systems with Neural Networks and SQP: Exploiting Muscle Properties"
      },
      {
        "paperId": "bfe12ed806f0bf9d17dc7e5bd5653294035d1caf",
        "title": "Learning to Run challenge solutions: Adapting reinforcement learning methods for neuromusculoskeletal environments"
      },
      {
        "paperId": "a8ef08940341381390d9a5672546354d0ce51328",
        "title": "Maximum a Posteriori Policy Optimisation"
      },
      {
        "paperId": "429ed4c9845d0abd1f8204e1d7705919559bc2a2",
        "title": "Hindsight Experience Replay"
      },
      {
        "paperId": "a89ba442ec2eb1e495c388045d66752fee06bbe6",
        "title": "Compliant control for soft robots: Emergent behavior of a tendon driven anthropomorphic arm"
      },
      {
        "paperId": "f21d8454260d18677aedbc291ccf3ddb5dd9ad1d",
        "title": "A lightweight robotic arm with pneumatic muscles for robot learning"
      },
      {
        "paperId": "3b2aff88ee03e82993c066c3e698d51da62d5496",
        "title": "Deep Reinforcement Learning in Large Discrete Action Spaces"
      },
      {
        "paperId": "4c05d7caa357148f0bbd61720bdd35f0bc05eb81",
        "title": "Dueling Network Architectures for Deep Reinforcement Learning"
      },
      {
        "paperId": "a44617135b87de71c917088edf0085b8a2c4d738",
        "title": "Comparative Sensitivity Analysis of Muscle Activation Dynamics"
      },
      {
        "paperId": "5dc2a215bd7cd5bdd3a0baa8c967575632696fac",
        "title": "Universal Value Function Approximators"
      },
      {
        "paperId": "40044d7c76c0d6159bf3853c5ff562009cffcb4e",
        "title": "Novel plasticity rule can explain the development of sensorimotor intelligence"
      },
      {
        "paperId": "bfbdc2b44c075db605f29e7128d7318300c449bc",
        "title": "Benchmarking of dynamic simulation predictions in two software platforms using an upper limb musculoskeletal model"
      },
      {
        "paperId": "28913848898439f1a3360326ba4e2f6deef497fa",
        "title": "LEARNING TO CONTROL THE THREE-LINK MUSCULOSKELETAL ARM USING ACTOR\u2013CRITIC REINFORCEMENT LEARNING ALGORITHM DURING REACHING MOVEMENT"
      },
      {
        "paperId": "2446caad4c79a5bf148b838d788bb1cf7f62d77e",
        "title": "Hill-type muscle model with serial damping and eccentric force-velocity relation."
      },
      {
        "paperId": "0a383c487508f2716149814d2efdeac9a369dccc",
        "title": "Reaching control of a full-torso, modelled musculoskeletal robot using muscle synergies emergent under reinforcement learning"
      },
      {
        "paperId": "0e24a9f518a95a99b6216f2f798d2b05ac70b11f",
        "title": "Control of position and movement is simplified by combined muscle spindle and Golgi tendon organ feedback."
      },
      {
        "paperId": "768ee79a64d235c6a8f9509d3af8d81771a6ab31",
        "title": "Information Driven Self-Organization of Complex Robotic Behaviors"
      },
      {
        "paperId": "b354ee518bfc1ac0d8ac447eece9edb69e92eae1",
        "title": "MuJoCo: A physics engine for model-based control"
      },
      {
        "paperId": "0a5c226d6b1a8eeada4f96945fc96720193e347d",
        "title": "A quadratic regulator-based heuristic for rapidly exploring state space"
      },
      {
        "paperId": "59bf2a4efdd6fce10f9d0e37ffc8ef689e35f315",
        "title": "Reinforcement learning in the brain"
      },
      {
        "paperId": "d87ad5a0f2ac284eba2258dda09ff838e0213326",
        "title": "Differential Hebbian learning"
      },
      {
        "paperId": "9c9e54726c53a3b16934717edcbd6ae249438c2b",
        "title": "OpenSim: Open-Source Software to Create and Analyze Dynamic Simulations of Movement"
      },
      {
        "paperId": "f37d3bb0ef5ad65c7f8c3c206f3ed631fd3a0a90",
        "title": "Gait selection in the ostrich: mechanical and metabolic characteristics of walking and running with and without an aerial phase"
      },
      {
        "paperId": "caf4eba5187db2cdf9d0446679b6f80e6752c226",
        "title": "On generating power law noise."
      },
      {
        "paperId": null,
        "title": "OstrichRL, 2022. URL https://github.com/vittorione94/ ostrichrl"
      },
      {
        "paperId": "d4e6b19cbec7f6a9ec60ca22295ca33426a248e1",
        "title": "Using time-correlated noise to encourage exploration and improve autonomous agents performance in Reinforcement Learning"
      },
      {
        "paperId": null,
        "title": "United Kingdom: International Foundation for Autonomous Agents and Multiagent Systems"
      },
      {
        "paperId": null,
        "title": "The Hyfydy simulation software, 11 2021"
      },
      {
        "paperId": null,
        "title": "2021) and the MuJoCo documentation for more details on the muscle model and its parametrization"
      },
      {
        "paperId": null,
        "title": "Odd Rune Lykkeb\u00f8"
      },
      {
        "paperId": null,
        "title": "OpenAI Pieter Abbeel, and Wojciech Zaremba"
      },
      {
        "paperId": "e0d36a03f7c30bea7bbc73fe73d5535bdc42a5ac",
        "title": "Computational modeling of muscle biomechanics"
      },
      {
        "paperId": "0a6e0da1953686e10fcee17245a2d19b8d646903",
        "title": "T UTORIAL Playful Machines Theoretical Foundation and Practical Realization of Self-Organizing Robots"
      },
      {
        "paperId": "5847630a69b5da648b764fc34a129e245eb1e3de",
        "title": "Rocking stumper and jumping snakes from a dynamical systems approach to artificial life"
      },
      {
        "paperId": "97efafdb4a3942ab3efba53ded7413199f79c054",
        "title": "Reinforcement Learning: An Introduction"
      },
      {
        "paperId": "874b3a63422eeaf24c14435ee6091ed48247bff3",
        "title": "Efficient memory-based learning for robot control"
      },
      {
        "paperId": "6aeb22e31b1d808754bfca8ba2bf597d92972d06",
        "title": "On the theory of brownian motion"
      },
      {
        "paperId": "810b9ffea4c74db3923336a22dc9563679cfe564",
        "title": "Conference Paper"
      },
      {
        "paperId": null,
        "title": "Vittorio La Barbera"
      },
      {
        "paperId": null,
        "title": "The initial position is slightly randomized from a standing position"
      }
    ],
    "cited_by": [
      {
        "paperId": "c49919b1a15babdb4fb41a5be18958e2d0a46229",
        "title": "Embodied sensorimotor control: computational modeling of the neural control of movement"
      },
      {
        "paperId": "7cb859c790ef7f3ed48ff5045dff69c4dee7001b",
        "title": "Arnold: a generalist muscle transformer policy"
      },
      {
        "paperId": "154a54db55cc1edb4f46fef5e0c6ac3e3a9a7fe9",
        "title": "Demystifying Reward Design in Reinforcement Learning for Upper Extremity Interaction: Practical Guidelines for Biomechanical Simulations in HCI"
      },
      {
        "paperId": "e4aa6349b5dd3b2828ba2a7f639a973099378f9e",
        "title": "Mind & Motion: Opportunities and Applications of Integrating Biomechanics and Cognitive Models in HCI"
      },
      {
        "paperId": "72ba16cc208543832c22a79e5bbaaf36c006a671",
        "title": "Where to Intervene: Action Selection in Deep Reinforcement Learning"
      },
      {
        "paperId": "c87af7b372dc1dc6b7132e3ce05b264db651cedf",
        "title": "Grounding Intelligence in Movement"
      },
      {
        "paperId": "58a00b1e88c9e771ecd9b1d745480f3e5d1e8c23",
        "title": "Motion Tracking with Muscles: Predictive Control of a Parametric Musculoskeletal Canine Model"
      },
      {
        "paperId": "be9287569ffb2b0feddd1174a411a81d3bbdfd7a",
        "title": "Bioinspired morphology and task curricula for learning locomotion in bipedal muscle-actuated systems"
      },
      {
        "paperId": "b9859757e60294b4229941e54261df658f1c710a",
        "title": "Hard Contacts with Soft Gradients: Refining Differentiable Simulators for Learning and Control"
      },
      {
        "paperId": "1285fcf412fdc93629301c7c606de6c6b1a5075f",
        "title": "Bipedal Balance Control with Whole-body Musculoskeletal Standing and Falling Simulations"
      },
      {
        "paperId": "5b0200720bdefbaabeeee6226c3c3d430ed491ca",
        "title": "Online Design Optimization of Passive Exoskeletons Using Fast Biomechanics Simulation and Reinforcement Learning"
      },
      {
        "paperId": "392d5b75eedd83e33068dbe5e61668eaa6429abf",
        "title": "Motion Control of High-Dimensional Musculoskeletal Systems with Hierarchical Model-Based Planning"
      },
      {
        "paperId": "c58f5c459f4c01f75459a94a8c64ec390da4ba65",
        "title": "iAssistADL: Intelligent Assistive Device for Patients with Neurodegenerative Movement Disorder: Concepts and First Implementations"
      },
      {
        "paperId": "60b7b81a49d024a5dfb98028c51386ccde8e57bc",
        "title": "Myoassist 0.1: Myosuite for Dexterity and Agility in Bionic Humans"
      },
      {
        "paperId": "1f17175b5321c318699198a1b2e02d84ea137869",
        "title": "Reinforcement learning-based motion imitation for physiologically plausible musculoskeletal motor control"
      },
      {
        "paperId": "bc8ed305a8ea55bb15b8fdee8f7d101faf451827",
        "title": "What Makes a Model Breathe? Understanding Reinforcement Learning Reward Function Design in Biomechanical User Simulation"
      },
      {
        "paperId": "90a5263f863242336d8e2e3cfd3767e41582ae5a",
        "title": "The Role of Tactile Sensing for Learning Reach and Grasp"
      },
      {
        "paperId": "1d942a134bb16aec9ec34afd7901ddd79d3f627d",
        "title": "Exploring Complex Human-Prosthetic Interactions: Musculoskeletal Models for Biomechanical Analysis"
      },
      {
        "paperId": "f5ce634abf9c83427752c327735bdc700934285e",
        "title": "Brain-like neural dynamics for behavioral control develop through reinforcement learning"
      },
      {
        "paperId": "7d0baf2e96895c0d7617d20be5fe1adb337fb1ed",
        "title": "Decoding the brain: From neural representations to mechanistic models"
      },
      {
        "paperId": "65bcb9e144a4cbc08c2db0edc887b41b1753985b",
        "title": "EgoPressure: A Dataset for Hand Pressure and Pose Estimation in Egocentric Vision"
      },
      {
        "paperId": "5d630f5270c6de5b749bbf137b5e139d3c04acf3",
        "title": "Acquiring musculoskeletal skills with curriculum-based reinforcement learning"
      },
      {
        "paperId": "a1936ddc4f988457ad2e9f7c71f1579ef0bab08f",
        "title": "DEP-SNN-RL:Spiking Neural Networks Reinforcement Learning in Musculoskeletal Systems"
      },
      {
        "paperId": "fb9e68e110d2e24d515d8cbf237738429ac6bd28",
        "title": "Exciting Action: Investigating Efficient Exploration for Learning Musculoskeletal Humanoid Locomotion"
      },
      {
        "paperId": "ac4d3691745f5304fbe9d4beb63e1789a3dd0ad8",
        "title": "SIM2VR: Towards Automated Biomechanical Testing in VR"
      },
      {
        "paperId": "e7fbe25cad71cbc058f2435499d8622790dcd11b",
        "title": "Generating Realistic Arm Movements in Reinforcement Learning: A Quantitative Comparison of Reward Terms and Task Requirements"
      },
      {
        "paperId": "fa15a09c026357bd096ee8839a6bfa2c4812a2df",
        "title": "Replication of Impedance Identification Experiments on a Reinforcement-Learning-Controlled Digital Twin of Human Elbows"
      },
      {
        "paperId": "134acf45a016fced57cc8f8e171eeb6e0015b0b8",
        "title": "Open the Black Box: Step-based Policy Updates for Temporally-Correlated Episodic Reinforcement Learning"
      },
      {
        "paperId": "c4a308e77d32c96e82017f27b031c84c7161bac4",
        "title": "Identifying Policy Gradient Subspaces"
      },
      {
        "paperId": "a81caf812223e3c34480f3889c0510a0844d639d",
        "title": "Self Model for Embodied Intelligence: Modeling Full-Body Human Musculoskeletal System and Locomotion Control with Hierarchical Low-Dimensional Representation"
      },
      {
        "paperId": "68493095e79e99870ddb4fd55df8ad892d3e1735",
        "title": "MyoDex: A Generalizable Prior for Dexterous Manipulation"
      },
      {
        "paperId": "0d9e19a5465e71196a35766ce3a6614b14d802cf",
        "title": "Natural and Robust Walking using Reinforcement Learning without Demonstrations in High-Dimensional Musculoskeletal Models"
      },
      {
        "paperId": "5c3d433550303f99f3d37e000632b144d1c0bcb6",
        "title": "SAR: Generalization of Physiological Dexterity via Synergistic Action Representation"
      },
      {
        "paperId": "bca7d3dec93b9d771c6c0d28c237043fd9709090",
        "title": "SAR: Generalization of Physiological Agility and Dexterity via Synergistic Action Representation"
      },
      {
        "paperId": "5e3a1ab688007ce33fb4b6075a7c223d7e5edc6e",
        "title": "Task-driven neural network models predict neural dynamics of proprioception"
      },
      {
        "paperId": "54952feedd88ca743dc6b01cfbc707d9254bbdb3",
        "title": "Latent Exploration for Reinforcement Learning"
      },
      {
        "paperId": "a8eef7817311e4681792559d777aa460d8ec7cc9",
        "title": "Learning with Muscles: Benefits for Data-Efficiency and Robustness in Anthropomorphic Tasks"
      },
      {
        "paperId": "4a06ec48e4b413ab2563273981018df82faac32e",
        "title": "Pink Noise Is All You Need: Colored Noise Exploration in Deep Reinforcement Learning"
      },
      {
        "paperId": "a1926ee74a379779f775b1aa616c715c21d23c4a",
        "title": "MyoDex : G ENERALIZABLE R EPRESENTATIONS FOR D EXTEROUS P HYSIOLOGICAL M ANIPULATION"
      },
      {
        "paperId": "97e5f79f2833165c8fd382228912d2cd58cb897f",
        "title": "Colored Noise Exploration in Reinforcement Learning"
      },
      {
        "paperId": "fdf59171921ea3f5dcf940b91dafa42edc6f2f84",
        "title": "Learning with Muscles: Bene\ufb01ts for Data-Ef\ufb01ciency and Robustness in Anthropomorphic Tasks"
      },
      {
        "paperId": "76fa17b85fe996166245488fa6104e84b94e17ec",
        "title": "MyoChallenge 2022: Learning contact-rich manipulation using a musculoskeletal hand"
      },
      {
        "paperId": "bf948731e2ceb39e4597f28cbf70252098a9d9b4",
        "title": "MyoChallenge 2024: Physiological Dexterity and Agility in Bionic Humans"
      },
      {
        "paperId": "4a256f8e1909bfff33c92e1e4968b1e82e08c707",
        "title": "MyoChallenge 2023 Towards Human-Level Dexterity and Agility"
      }
    ],
    "score": 14.666666666666666
  },
  {
    "id": "c64a99ec780d5e1c6ba242a82e72e96eb0c3ef8a",
    "title": "An Information-Theoretic Perspective on Intrinsic Motivation in Reinforcement Learning: A Survey",
    "authors": [
      "A. Aubret",
      "L. Matignon",
      "S. Hassas"
    ],
    "year": 2022,
    "citationCount": 44,
    "abstract": "The reinforcement learning (RL) research area is very active, with an important number of new contributions, especially considering the emergent field of deep RL (DRL). However, a number of scientific and technical challenges still need to be resolved, among which we acknowledge the ability to abstract actions or the difficulty to explore the environment in sparse-reward settings which can be addressed by intrinsic motivation (IM). We propose to survey these research works through a new taxonomy based on information theory: we computationally revisit the notions of surprise, novelty, and skill-learning. This allows us to identify advantages and disadvantages of methods and exhibit current outlooks of research. Our analysis suggests that novelty and surprise can assist the building of a hierarchy of transferable skills which abstracts dynamics and makes the exploration process more robust.",
    "url": "https://www.semanticscholar.org/paper/c64a99ec780d5e1c6ba242a82e72e96eb0c3ef8a",
    "pdf_url": "https://arxiv.org/pdf/2209.08890.pdf",
    "venue": "Entropy",
    "publicationDate": "2022-09-19",
    "externalIds": {
      "ArXiv": "2209.08890",
      "PubMedCentral": "9954873",
      "DBLP": "journals/entropy/AubretMH23",
      "DOI": "10.3390/e25020327",
      "CorpusId": 252367643,
      "PubMed": "36832693"
    },
    "references": [
      {
        "paperId": "e06eaabebe63c5f5118e66972482bb779d84ebee",
        "title": "C-GRAIL: Autonomous Reinforcement Learning of Multiple and Context-Dependent Goals"
      },
      {
        "paperId": "2fd346e7edf0ec9de793c96acb0a1e3f2f8a2718",
        "title": "BYOL-Explore: Exploration by Bootstrapped Prediction"
      },
      {
        "paperId": "baf2722c7a28912a6eb57ede95ae72509102c596",
        "title": "Sampling diversity driven exploration with state difference guidance"
      },
      {
        "paperId": "6296aa7cab06eaf058f7291040b320b5a83c0091",
        "title": "Generative Adversarial Networks"
      },
      {
        "paperId": "4545358c54c90dc5d5eb8f11a3c610b3eda88b55",
        "title": "Discovering and Achieving Goals via World Models"
      },
      {
        "paperId": "e55840992c735daa4b18fe80af7ab19d94bbbaad",
        "title": "Seeking Visual Discomfort: Curiosity-driven Representations for Reinforcement Learning"
      },
      {
        "paperId": "746b502903198dc8997d86f2997eb2da077154d8",
        "title": "Diversity-augmented intrinsic motivation for deep reinforcement learning"
      },
      {
        "paperId": "3be84e24b144541a8cd9030526ef2b8ef2cbfe54",
        "title": "A Survey of Exploration Methods in Reinforcement Learning"
      },
      {
        "paperId": "107e4ea37d2e5364893107a8ce072972c4a10dfb",
        "title": "Unsupervised Skill Discovery with Bottleneck Option Learning"
      },
      {
        "paperId": "988dae20df8d69869aa41097a05d821446cff621",
        "title": "DisTop: Discovering a Topological Representation to Learn Diverse and Rewarding Skills"
      },
      {
        "paperId": "541d2f57590b77e946be8dc1c128826cca461a4a",
        "title": "Hierarchical Reinforcement Learning"
      },
      {
        "paperId": "7a62d9d1f5617559fd446596cc286eeefd39b959",
        "title": "Sample Efficient Reinforcement Learning via Model-Ensemble Exploration and Exploitation"
      },
      {
        "paperId": "adcae35901c36325478a03b647e14222a53ea9fc",
        "title": "What Can I Do Here? Learning New Skills by Imagining Visual Affordances"
      },
      {
        "paperId": "4e9f9e1a69e1924aa9345d77510d43285900f465",
        "title": "Curious Representation Learning for Embodied Intelligence"
      },
      {
        "paperId": "4d12862b0c6daee864f6f5537270d95b88560ebc",
        "title": "Mutual Information State Intrinsic Control"
      },
      {
        "paperId": "0295df1b9d11e6e49b20119410fd755a4d7781af",
        "title": "Behavior From the Void: Unsupervised Active Pre-Training"
      },
      {
        "paperId": "a50f7bcf8a998f3de11bc085b0f4dea32be19783",
        "title": "Reinforcement Learning with Prototypical Representations"
      },
      {
        "paperId": "7fc04f8031598da6510bce4a7e5b4722305ca5b0",
        "title": "State Entropy Maximization with Random Encoders for Efficient Exploration"
      },
      {
        "paperId": "005acb881061eb8137e9d36a05a6a0bdf0026b61",
        "title": "Hierarchical Reinforcement Learning By Discovering Intrinsic Options"
      },
      {
        "paperId": "05ea14ad1a6fb3f2cd967da746d4686021edff3d",
        "title": "Geometric Entropic Exploration"
      },
      {
        "paperId": "63731272b4b2b9edbdb09c1c626dc8c520856909",
        "title": "Evaluating Agents without Rewards"
      },
      {
        "paperId": "34eb252cc43ba3577a1724792b035f7b5f378e37",
        "title": "Intrinsically Motivated Goal-Conditioned Reinforcement Learning: a Short Survey"
      },
      {
        "paperId": "9345e5f5631455b3a8a7dc86a42cb22ce22084e8",
        "title": "BeBold: Exploration Beyond the Boundary of Explored Regions"
      },
      {
        "paperId": "1a7471fb3d324a074bda8a7530e9e7749329cef9",
        "title": "Relative Variational Intrinsic Control"
      },
      {
        "paperId": "376e0853411acb4e5732c587471d0a3910689b20",
        "title": "Adapting Behavior via Intrinsic Reward: A Survey and Empirical Study"
      },
      {
        "paperId": "7e38476342ce1fcc8ef0dcd23686539395961769",
        "title": "Inductive biases for deep learning of higher-level cognition"
      },
      {
        "paperId": "ef8ff76a77f8bb237b37d06c9a8187d8c800ee61",
        "title": "Self-Calibrating Active Binocular Vision via Active Efficient Coding with Deep Autoencoders"
      },
      {
        "paperId": "57835c5ad5424f94ee75901c3113730f3900e656",
        "title": "Representation Learning via Invariant Causal Mechanisms"
      },
      {
        "paperId": "b33b735352c78743707f66e525f7cca65ff207b0",
        "title": "Latent World Models For Intrinsically Motivated Exploration"
      },
      {
        "paperId": "d4d86f4a632d899ba5f511c1d0e7a7fbd44288c8",
        "title": "Novelty Search in representational space for sample efficient exploration"
      },
      {
        "paperId": "6df5d79885bf1af65323a055684b2b92439a65c3",
        "title": "GRIMGEP: Learning Progress for Robust Goal Sampling in Visual Deep Reinforcement Learning"
      },
      {
        "paperId": "5dd561e231a1f9ce3c802add041dd3a601164270",
        "title": "Active World Model Learning with Progress Curiosity"
      },
      {
        "paperId": "5ab5d94f1a9ffeb076cad515854bbe0ea9c49db5",
        "title": "A Policy Gradient Method for Task-Agnostic Exploration"
      },
      {
        "paperId": "72260c19441259404ed24003d9e27588fb3613ae",
        "title": "Maximum Entropy Gain Exploration for Long Horizon Multi-goal Reinforcement Learning"
      },
      {
        "paperId": "019820cbb73d0651a913bb74cbfb713c8ad772df",
        "title": "ELSIM: End-to-end learning of reusable skills through intrinsic motivation"
      },
      {
        "paperId": "7d3699cf0e8a822cd8ae99b8eeed146930d24a0f",
        "title": "On Optimism in Model-Based Reinforcement Learning"
      },
      {
        "paperId": "e6e9d927d496d35a4c2b8bc17c1e4b8c8f413254",
        "title": "Emergent cooperation through mutual information maximization"
      },
      {
        "paperId": "38f93092ece8eee9771e61c1edaf11b1293cae1b",
        "title": "Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning"
      },
      {
        "paperId": "8090f2a78481795e3556a93122ed79aa014f2a95",
        "title": "Exploration by Maximizing Renyi Entropy for Reward-Free RL Framework"
      },
      {
        "paperId": "6d91886bb07d3af29244dd8a186e6dda2315fd9e",
        "title": "LEAF: Latent Exploration Along the Frontier"
      },
      {
        "paperId": "3a71c306eb6232658c9e5fd48aed1ef3befe5fbe",
        "title": "Planning to Explore via Self-Supervised World Models"
      },
      {
        "paperId": "616ac6772508e72084360158078058dfa8bda7e7",
        "title": "First return, then explore"
      },
      {
        "paperId": "c0142146ff9cd2ff84001bd6eb48aca88318ff81",
        "title": "Improving Robot Dual-System Motor Learning with Intrinsically Motivated Meta-Control and Latent-Space Experience Imagination"
      },
      {
        "paperId": "bebe8ffb0c357ac0c7eea2556f817b03ee22b570",
        "title": "RIDE: Rewarding Impact-Driven Exploration for Procedurally-Generated Environments"
      },
      {
        "paperId": "086159600bede14e00f96043c733d4f3b45855aa",
        "title": "Never Give Up: Learning Directed Exploration Strategies"
      },
      {
        "paperId": "7af72a461ed7cda180e7eab878efd5f35d79bbf4",
        "title": "A Simple Framework for Contrastive Learning of Visual Representations"
      },
      {
        "paperId": "39b2551f109fd0495abfca1ab0bb81311c1b3996",
        "title": "Explore, Discover and Learn: Unsupervised Discovery of State-Covering Skills"
      },
      {
        "paperId": "dfec80cded5b1393402644c4af26dda1e5e4c08f",
        "title": "Intrinsic motivations and open-ended learning"
      },
      {
        "paperId": "ce441c4bb6c13a50182d10ff13ae4bdaf15bf955",
        "title": "SMiRL: Surprise Minimizing RL in Dynamic Environments"
      },
      {
        "paperId": "904e83d995ffe115f5a3005ecdb161371616937f",
        "title": "Entropy Regularization with Discounted Future State Distribution in Policy Gradient Methods"
      },
      {
        "paperId": "007fd689a806de99f053a0f5ef90aedb44f9ec7e",
        "title": "Better Exploration with Optimistic Actor-Critic"
      },
      {
        "paperId": "081ef50ed2b05e92bd8d61e753a7af751fcbecff",
        "title": "Skill-based curiosity for intrinsically motivated reinforcement learning"
      },
      {
        "paperId": "786de34ae1ff786c28087041395b4cbb8d9e9b92",
        "title": "Automated curricula through setter-solver interactions"
      },
      {
        "paperId": "35257ba97d193f23f15e71a633a34e94dd3f5777",
        "title": "Why Does Hierarchy (Sometimes) Work So Well in Reinforcement Learning?"
      },
      {
        "paperId": "72799883b3378309e195a6bbd3c42fcaa88bd962",
        "title": "Efficient Intrinsically Motivated Robotic Grasping with Learning-Adaptive Imagination in Latent Space"
      },
      {
        "paperId": "4ffb0130c2e19033a1696c32dac2239f702c8dc4",
        "title": "Reinforcement learning"
      },
      {
        "paperId": "fdfa7ccfd5f118f859baa1f57cf2748d79e6c27b",
        "title": "Diverse Trajectory Forecasting with Determinantal Point Processes"
      },
      {
        "paperId": "ffb3886a253ff927bcc46b78e00409893865a68e",
        "title": "Dynamics-Aware Unsupervised Discovery of Skills"
      },
      {
        "paperId": "2fed116dea9c36914b52b55e0f9688ccf641ee07",
        "title": "Sub-policy Adaptation for Hierarchical Reinforcement Learning"
      },
      {
        "paperId": "55203cd25f9c03d7c6b691ba84e95bb82df0bc6f",
        "title": "Fast Task Inference with Variational Intrinsic Successor Features"
      },
      {
        "paperId": "cfec54f9208dcbb2c558aa7ff45f57be2533d046",
        "title": "Efficient Exploration via State Marginal Matching"
      },
      {
        "paperId": "801eb90352e603b1573a04a36804417d1c9496f0",
        "title": "Learning latent state representation for speeding up exploration"
      },
      {
        "paperId": "c60d789bf93dee52fc1e076c005cfb8385c84719",
        "title": "Curiosity-Bottleneck: Exploration By Distilling Task-Specific Novelty"
      },
      {
        "paperId": "87a40a2d506fec06d67dc8f14ce2c708a789a2b4",
        "title": "Self-Supervised Exploration via Disagreement"
      },
      {
        "paperId": "d569567c994b443ead90c709f0f1718ed6f0ec50",
        "title": "Maximum Entropy-Regularized Multi-Goal Reinforcement Learning"
      },
      {
        "paperId": "19effcaf416fbf0957337378078db45a1f645bd9",
        "title": "Vision-Based Robot Navigation through Combining Unsupervised Learning and Hierarchical Reinforcement Learning"
      },
      {
        "paperId": "7388826b5ee00efe17cb7f19a623d9b5e955ae70",
        "title": "Skew-Fit: State-Covering Self-Supervised Reinforcement Learning"
      },
      {
        "paperId": "16ca034b434c15e1da6eef50a67264137e1380bd",
        "title": "World Discovery Models"
      },
      {
        "paperId": "c520bf47db3360ae3a52219771390a354ed8a91f",
        "title": "Go-Explore: a New Approach for Hard-Exploration Problems"
      },
      {
        "paperId": "adab275554eb745cdc21fd6c526ec55a5b2ef362",
        "title": "Provably Efficient Maximum Entropy Exploration"
      },
      {
        "paperId": "4b61c25a86083c20730c9b12737ac6ac4178c364",
        "title": "An Introduction to Deep Reinforcement Learning"
      },
      {
        "paperId": "fea3e63c97c7292dc6fbcb3ffe7131eb54053986",
        "title": "Learning Latent Dynamics for Planning from Pixels"
      },
      {
        "paperId": "e0121c1d2dc5c8cc77f4b1570e28f2443ece2a8f",
        "title": "Social Influence as Intrinsic Motivation for Multi-Agent Deep Reinforcement Learning"
      },
      {
        "paperId": "3f56ac0e4b881d25268e83961b93ee95f2807bfb",
        "title": "CURIOUS: Intrinsically Motivated Modular Multi-Goal Reinforcement Learning"
      },
      {
        "paperId": "e4a89a978f747d0b548f5887b2380c5f618061f0",
        "title": "Near-Optimal Representation Learning for Hierarchical Reinforcement Learning"
      },
      {
        "paperId": "51ecd565f8a394b1907b3d37722435a573ce7b72",
        "title": "EMI: Exploration with Mutual Information"
      },
      {
        "paperId": "6ab8aca1f727e379632292e1ec4a24ea2739cf89",
        "title": "Model-Based Active Exploration"
      },
      {
        "paperId": "4cb3fd057949624aa4f0bbe7a6dcc8777ff04758",
        "title": "Exploration by Random Network Distillation"
      },
      {
        "paperId": "c30a6192b6cbb665aefe837230da1bcfbbf42153",
        "title": "Curiosity-Driven Experience Prioritization via Density Estimation"
      },
      {
        "paperId": "b65a6be07ce9c86797e6917258cf5ba45273ee73",
        "title": "Unsupervised Control Through Non-Parametric Discriminative Rewards"
      },
      {
        "paperId": "fdfeeb14bbde2ab31b18e56b92d362dcd1b14f71",
        "title": "Episodic Curiosity through Reachability"
      },
      {
        "paperId": "64cdf82a4eab0420f8f79a7f2d5613251fe20953",
        "title": "Learning Navigation Behaviors End-to-End With AutoRL"
      },
      {
        "paperId": "aaac05817084c73fa02cb329678fd84b98d1b6a3",
        "title": "From Babies to Robots: The Contribution of Developmental Robotics to Developmental Psychology"
      },
      {
        "paperId": "af3825437b627db1a99f946f7aa773ba8b03befd",
        "title": "Learning deep representations by mutual information estimation and maximization"
      },
      {
        "paperId": "ca14dce53be20d3d23d4f0db844a8389ab619db3",
        "title": "Large-Scale Study of Curiosity-Driven Learning"
      },
      {
        "paperId": "9f67b3edc67a35c884bd532a5e73fa3a7f3660d8",
        "title": "Count-Based Exploration with the Successor Representation"
      },
      {
        "paperId": "5f8645a8474017f52e4d1d4b4a0ca95d8b39f66f",
        "title": "Variational Option Discovery Algorithms"
      },
      {
        "paperId": "3aadab924520c58be81781aafd51e6807e9c4576",
        "title": "Visual Reinforcement Learning with Imagined Goals"
      },
      {
        "paperId": "0f710daa7bbba3350169f0bbb5d24f8db3e5199e",
        "title": "Self-Consistent Trajectory Autoencoder: Hierarchical Reinforcement Learning with Trajectory Embeddings"
      },
      {
        "paperId": "5fd3ce235f5fcebd3d2807f710b060add527183b",
        "title": "Deep Curiosity Search: Intra-Life Exploration Improves Performance on Challenging Deep Reinforcement Learning Problems"
      },
      {
        "paperId": "705bbc4dcd475f9230863771da6596e1f677a92d",
        "title": "Playing hard exploration games by watching YouTube"
      },
      {
        "paperId": "39b7007e6f3dd0744833f292f07ed77973503bfd",
        "title": "Data-Efficient Hierarchical Reinforcement Learning"
      },
      {
        "paperId": "4c852a954c3a74df410231d601857b7005076de9",
        "title": "Hierarchical Reinforcement Learning with Hindsight"
      },
      {
        "paperId": "5b01eaef54a653ba03ddd5a978690380fbc19bfc",
        "title": "Diversity is All You Need: Learning Skills without a Reward Function"
      },
      {
        "paperId": "84de7d27e2f6160f634a483e8548c499a2cda7fa",
        "title": "Spectral Normalization for Generative Adversarial Networks"
      },
      {
        "paperId": "811df72e210e20de99719539505da54762a11c6d",
        "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor"
      },
      {
        "paperId": "2064020586d5832b55f80a7dffea1fd90a5d94dd",
        "title": "Improving Exploration in Evolution Strategies for Deep Reinforcement Learning via a Population of Novelty-Seeking Agents"
      },
      {
        "paperId": "429ed4c9845d0abd1f8204e1d7705919559bc2a2",
        "title": "Hindsight Experience Replay"
      },
      {
        "paperId": "3fab4d28be5ff73f42c8b307117aa8d680345ad9",
        "title": "Noisy Networks for Exploration"
      },
      {
        "paperId": "0f810eb4777fd05317951ebaa7a3f5835ee84cf4",
        "title": "Count-Based Exploration in Feature Space for Reinforcement Learning"
      },
      {
        "paperId": "142497432fe179ddb6ffe600c64a837ec6179550",
        "title": "Parameter Space Noise for Exploration"
      },
      {
        "paperId": "11026fc42c4d30f9ea91ec8c32f8d75768b70f6d",
        "title": "Boltzmann Exploration Done Right"
      },
      {
        "paperId": "471f9742b4e32d8ee68f9ee493768ff0466a231d",
        "title": "Automatic Goal Generation for Reinforcement Learning Agents"
      },
      {
        "paperId": "225ab689f41cef1dc18237ef5dab059a49950abf",
        "title": "Curiosity-Driven Exploration by Self-Supervised Prediction"
      },
      {
        "paperId": "0b80b206a117692087fc17ee4f4141c80b11ee4d",
        "title": "Information socialtaxis and efficient collective behavior emerging in groups of information-seeking agents"
      },
      {
        "paperId": "e3b0ea7209731c47b582215c6c67f9c691ad9863",
        "title": "Deep Q-learning From Demonstrations"
      },
      {
        "paperId": "d6757aedcb53142bc439ec64bfd0b056d99b1881",
        "title": "Surprise-Based Intrinsic Motivation for Deep Reinforcement Learning"
      },
      {
        "paperId": "12f67fb182bc934fc95ce97acff553d83e2ca72e",
        "title": "Count-Based Exploration with Neural Density Models"
      },
      {
        "paperId": "8423cc50c18d68f797adaa4f571f5e4efbe325a5",
        "title": "A Laplacian Framework for Option Discovery in Reinforcement Learning"
      },
      {
        "paperId": "4eb38b3460606a4042b04fc52d0044ab948b4a17",
        "title": "EX2: Exploration with Exemplar Models for Deep Reinforcement Learning"
      },
      {
        "paperId": "afb42208cc499ede10a65af0dbe598e08556370d",
        "title": "Variational Intrinsic Control"
      },
      {
        "paperId": "0fcb2034e31a2bc2f12a2b1363d0d77baf445fdf",
        "title": "#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning"
      },
      {
        "paperId": "3deecaee4ec1a37de3cb10420eaabff067669e17",
        "title": "Stochastic Neural Networks for Hierarchical Reinforcement Learning"
      },
      {
        "paperId": "e2bd18c1039f27675bd64014117db648d969452e",
        "title": "Learning and Transfer of Modulated Locomotor Controllers"
      },
      {
        "paperId": "15b26d8cb35d7e795c8832fe08794224ee1e9f84",
        "title": "The Option-Critic Architecture"
      },
      {
        "paperId": "e86f71ca2948d17b003a5f068db1ecb2b77827f7",
        "title": "Concrete Problems in AI Safety"
      },
      {
        "paperId": "0936352b78a52bc5d2b5e3f04233efc56664af51",
        "title": "Conditional Image Generation with PixelCNN Decoders"
      },
      {
        "paperId": "6e90fd78e8a3b98af3954aae5209703aa966603e",
        "title": "Unifying Count-Based Exploration and Intrinsic Motivation"
      },
      {
        "paperId": "108e7f9f0b7ecf2d476b12b771cf9fb9d66638d5",
        "title": "GRAIL: A Goal-Discovering Robotic Architecture for Intrinsically-Motivated Learning"
      },
      {
        "paperId": "317cd4522b1f4a6f889743578143bb8823623f8b",
        "title": "VIME: Variational Information Maximizing Exploration"
      },
      {
        "paperId": "d7f3edc43707054aa9654520db5e661fea6a04c7",
        "title": "How Evolution May Work Through Curiosity-Driven Developmental Process"
      },
      {
        "paperId": "69e76e16740ed69f4dc55361a3d319ac2f1293dd",
        "title": "Asynchronous Methods for Deep Reinforcement Learning"
      },
      {
        "paperId": "024006d4c2a89f7acacc6e4438d156525b60a98f",
        "title": "Continuous control with deep reinforcement learning"
      },
      {
        "paperId": "df931ac9d80844394b356b0d05a77a40f291f7b2",
        "title": "Reward Shaping with Recurrent Neural Networks for Speeding up On-Line Policy Learning in Spoken Dialogue Systems"
      },
      {
        "paperId": "5dc2a215bd7cd5bdd3a0baa8c967575632696fac",
        "title": "Universal Value Function Approximators"
      },
      {
        "paperId": "2470fcf0f89082de874ac9133ccb3a8667dd89a8",
        "title": "Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models"
      },
      {
        "paperId": "0954ee594ae594c4bad59808ec4d596ab119ae3b",
        "title": "A non-parametric k-nearest neighbour entropy estimator"
      },
      {
        "paperId": "da6057368920585bcf2443295b98418840f1fc80",
        "title": "Weight Uncertainty in Neural Networks"
      },
      {
        "paperId": "340f48901f72278f6bf78a04ee5b01df208cc508",
        "title": "Human-level control through deep reinforcement learning"
      },
      {
        "paperId": "3523710ea363615bb52651f3279bcd350aa65dab",
        "title": "Conflict, arousal, and curiosity"
      },
      {
        "paperId": "f6ca9c148417d4167ba8b72f185a35649dc4b446",
        "title": "Skip Context Tree Switching"
      },
      {
        "paperId": "5f5dc5b9a2ba710937e2c413b37b053cd673df02",
        "title": "Auto-Encoding Variational Bayes"
      },
      {
        "paperId": "e6360cb923350d03d64fa34f004b7433f782058e",
        "title": "Which is the best intrinsic motivation signal for learning multiple skills?"
      },
      {
        "paperId": "d01e3414ca706eda917576d947ece811b5cbcdde",
        "title": "Empowerment - an Introduction"
      },
      {
        "paperId": "1695ff30aa141088a019fc29df6d128d6add06f5",
        "title": "Novelty or Surprise?"
      },
      {
        "paperId": "c408d277c0d94ac2406b7fc8adcd3fa41f5658d2",
        "title": "Incremental learning of skill collections based on intrinsic motivation"
      },
      {
        "paperId": "a7c8b076bc68a53019dc445079535bc79b7a098a",
        "title": "Intrinsically Motivated Learning in Natural and Artificial Systems"
      },
      {
        "paperId": "c1263c9d850c4012dd62e5513621cf40f4a3ff98",
        "title": "Learning and exploration in action-perception loops"
      },
      {
        "paperId": "aa5cc4649f0d26cceea6a71ba8eb676c7d9e7b62",
        "title": "Intrinsically Motivated Learning of Real-World Sensorimotor Skills with Developmental Constraints"
      },
      {
        "paperId": "b354ee518bfc1ac0d8ac447eece9edb69e92eae1",
        "title": "MuJoCo: A physics engine for model-based control"
      },
      {
        "paperId": "fe859f7e498ad093721c795b70d2e8380f980ce3",
        "title": "Exploration in Model-based Reinforcement Learning by Empirically Estimating Learning Progress"
      },
      {
        "paperId": "f82e4ff4f003581330338aaae71f60316e58dd26",
        "title": "The Arcade Learning Environment: An Evaluation Platform for General Agents"
      },
      {
        "paperId": "5a9ef216bf11f222438fff130c778267d39a9564",
        "title": "Practical Variational Inference for Neural Networks"
      },
      {
        "paperId": "fc50bef8feb5d8a9c2376bb7c3b9e41f415da45e",
        "title": "What are intrinsic motivations? A biological perspective"
      },
      {
        "paperId": "ad46f2fbb2cfe24507b25e6195d322d182248ace",
        "title": "The interaction of maturational constraints and intrinsic motivations in active motor development"
      },
      {
        "paperId": "5df39cc393907ea78fddf461b494b4c5b1b5a2e4",
        "title": "Planning to Be Surprised: Optimal Bayesian Exploration in Dynamic Environments"
      },
      {
        "paperId": "0c68c81c74930ef960825e685287b91fdc0bcafe",
        "title": "Autonomous Mental Development"
      },
      {
        "paperId": "33224ad0cdf6e2dc4893194dd587309c7887f0ba",
        "title": "Formal Theory of Creativity, Fun, and Intrinsic Motivation (1990\u20132010)"
      },
      {
        "paperId": "3c79e6ec5344181151905b20d4b4f4cca680a2ee",
        "title": "Intrinsically Motivated Reinforcement Learning: An Evolutionary Perspective"
      },
      {
        "paperId": "8ac68370386cb90e72189dd4d0b487bb3da19665",
        "title": "Exploring parameter space in reinforcement learning"
      },
      {
        "paperId": "8de174ab5419b9d3127695405efd079808e956e8",
        "title": "Curriculum learning"
      },
      {
        "paperId": "6dea11e362920bc36446c9a52f7982fcec1b4cd5",
        "title": "Driven by Compression Progress: A Simple Principle Explains Essential Aspects of Subjective Beauty, Novelty, Surprise, Interestingness, Attention, Curiosity, Creativity, Art, Science, Music, Jokes"
      },
      {
        "paperId": "237a1cf18ed83bb3ad852b34f443c6c1ff3336c1",
        "title": "An analysis of model-based Interval Estimation for Markov Decision Processes"
      },
      {
        "paperId": "84b5fbbc99ba52e3fa2989d1c09fa6e41979a56c",
        "title": "Tuning Bandit Algorithms in Stochastic Environments"
      },
      {
        "paperId": "e996344199af190bdd47ee31a652e76811a5487b",
        "title": "Variational inference for Dirichlet process mixtures"
      },
      {
        "paperId": "6cc0699a4a99132b7aecddb7f2e37d731e36bb43",
        "title": "Empowerment: a universal agent-centric measure of control"
      },
      {
        "paperId": "aebd8bab5cff769fed204dba35112e364a47e504",
        "title": "Bayesian surprise attracts human attention"
      },
      {
        "paperId": "bf1f8415b61d0afe31b2e901e888866c5b7e4f96",
        "title": "Motor primitives in vertebrates and invertebrates"
      },
      {
        "paperId": "7dbdb4209626fd92d2436a058663206216036e68",
        "title": "Elements of Information Theory"
      },
      {
        "paperId": "6ba30d04b0fa879b261cf9737e72bc9fdc03c656",
        "title": "Neural primitives for motion control"
      },
      {
        "paperId": "fa453f2ae1f944a61d7dd0ea576b5bd05d08b9ed",
        "title": "Critical period regulation."
      },
      {
        "paperId": "5729e9bc42a8c006fcdeaf6ca3434fabf49e7e7c",
        "title": "Estimating mutual information."
      },
      {
        "paperId": "3adde98a328a48b5a49c23fa1ae6c3c89056b9f1",
        "title": "Nearest Neighbor Estimates of Entropy"
      },
      {
        "paperId": "46c0ab398b680010da0362f76b9cf84324e4f5dd",
        "title": "A self-organising network that grows when required"
      },
      {
        "paperId": "d3d46afaee8a0f17b405d2f278c14c81802c2be5",
        "title": "Dopamine: generalization and bonuses"
      },
      {
        "paperId": "5127759530ce213f488af2859190697770f557f3",
        "title": "Slow Feature Analysis: Unsupervised Learning of Invariances"
      },
      {
        "paperId": "6722458c39f3142fac7f7bdaed930c092ea3f386",
        "title": "Autonomous Mental Development by Robots and Animals"
      },
      {
        "paperId": "5079ea296646fbbb0c1ceb8bbadf86c698c842ef",
        "title": "The Infinite Gaussian Mixture Model"
      },
      {
        "paperId": "0e7638dc16a5e5e9e46c91272bfb9c3dd242ef6d",
        "title": "Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning"
      },
      {
        "paperId": "94066dc12fe31e96af7557838159bde598cb4f10",
        "title": "Policy Invariance Under Reward Transformations: Theory and Application to Reward Shaping"
      },
      {
        "paperId": "116d7798c1123cf7fad4176e98f58fd49de4f8f1",
        "title": "Planning and Acting in Partially Observable Stochastic Domains"
      },
      {
        "paperId": "1d7d0e8c4791700defd4b0df82a26b50055346e0",
        "title": "An Information-Maximization Approach to Blind Separation and Blind Deconvolution"
      },
      {
        "paperId": "a9cd8efe9184dddb1bedbbec3a356c4dfb22fe63",
        "title": "Markov Decision Processes: Discrete Stochastic Dynamic Programming"
      },
      {
        "paperId": "1678bd32846b1aded5b1e80a617170812e80f562",
        "title": "Feudal Reinforcement Learning"
      },
      {
        "paperId": "94db34f4b68189bfcba22beab33ee3b54f10b876",
        "title": "Curious model-building control systems"
      },
      {
        "paperId": "16d70e8af45ca0ae2c1bb73f3be6628518d40b8f",
        "title": "Self-organization in a perceptual network"
      },
      {
        "paperId": "cde7b65dae0ce4de0dc57a809bfd8ea3ad56af65",
        "title": "THE ORIGINS OF INTELLIGENCE IN CHILDREN"
      },
      {
        "paperId": "5bfa93bf67b8371ca82bd9ba177b0bd5dc34fc43",
        "title": "Curiosity and exploration."
      },
      {
        "paperId": "03ac31d7a11df055fa0363686f8990037d10c927",
        "title": "The Origins of Intelligence in Children"
      },
      {
        "paperId": "5ad4e1ad3c4b47f66085bf466d0769c919969005",
        "title": "Development and Learning"
      },
      {
        "paperId": "843976b39e7f1a97bc120e7baa35a9b0f3382650",
        "title": "Behavior of Organisms"
      },
      {
        "paperId": "b8879d5beb8242c7a08d8d86ce3e6216c99e5f88",
        "title": "Provably Filtering Exogenous Distractors using Multistep Inverse Dynamics"
      },
      {
        "paperId": "1c0465b2ad51e261cab27990353f2002d96a389c",
        "title": "An information-theoretic perspective on intrinsic motivation in reinforcement learning: a survey"
      },
      {
        "paperId": "01e80034058d471e3e15ced46b54647aa443af15",
        "title": "Learning Subgoal Representations with Slow Dynamics"
      },
      {
        "paperId": "ab7e7a1382a1fc821f0e1f5afa38251de5a987d6",
        "title": "Variational Empowerment as Representation Learning for Goal-Conditioned Reinforcement Learning"
      },
      {
        "paperId": "e9094e2a33af929d67793d480fffa6836b1bfe07",
        "title": "Efficient Hierarchical Exploration with Stable Subgoal Representation Learning"
      },
      {
        "paperId": "adabcfc03fa110d40d6ab2a613a36dda9c12dfff",
        "title": "Exploration via Empowerment Gain: Combining Novelty, Surprise and Learning Progress"
      },
      {
        "paperId": "7bd944a5d9ccc99a6b7083bce0796b2ad9f07f7f",
        "title": "CURL: Contrastive Unsupervised Representations for Reinforcement Learning"
      },
      {
        "paperId": null,
        "title": "2020a. BeBold: Exploration Beyond"
      },
      {
        "paperId": null,
        "title": "J\u00fcrgen Konczak"
      },
      {
        "paperId": null,
        "title": "Long Beach, California, USA. 5779\u20135788"
      },
      {
        "paperId": null,
        "title": "Jan Storck, Sepp Hochreiter, J\u00fcrgen Schmidhuber, et al"
      },
      {
        "paperId": "af64497a418cf62016251360d9ba537604f7622a",
        "title": "Efficient Exploration in Reinforcement Learning"
      },
      {
        "paperId": "a181fb5a42ad8fe2cc27b5542fa40384e9a8d72c",
        "title": "Deep Variational Information Bottleneck"
      },
      {
        "paperId": "c166bfab42dd3d7c858d9e1802c5d5687574eeb9",
        "title": "The Nature Of Emotion Fundamental Questions"
      },
      {
        "paperId": "6e34b51ed35bd944cd15665ed5ea78599daa7563",
        "title": "Mutual Information As a Task-Independent Utility Function for Evolutionary Robotics"
      },
      {
        "paperId": "fae8bbf868681b83d91b2fec6c840d4d2b32005b",
        "title": "Intrinsic Motivation and Reinforcement Learning"
      },
      {
        "paperId": "aaaeb3da6a90fcd25b4c5399e84933b2d01b5e78",
        "title": "Intrinsically Motivated Learning Systems: An Overview"
      },
      {
        "paperId": "1325bbd04a3e5a7e8137cf2edf9cbca7fc6fd55d",
        "title": "Novelty Search and the Problem with Objectives"
      },
      {
        "paperId": "268b8f10a45e71f63daab6403bb453da31ae28a7",
        "title": "This PDF file includes: Materials and Methods"
      },
      {
        "paperId": "fb144a1d31aec3b2bece6a59bd11a876a9fafb34",
        "title": "Exploiting Open-Endedness to Solve Problems Through the Search for Novelty"
      },
      {
        "paperId": "21ab87c4263c74ca3f40f7ec1d95fa540a312256",
        "title": "How can we define intrinsic motivation ? \u2217"
      },
      {
        "paperId": "e11d96f0a67c0206aa6645583f6550ec5fe03401",
        "title": "Ongoing Emergence:A Core Concept in Epigenetic Robotics"
      },
      {
        "paperId": "b1d93aa19cbb5f50d78499a5574d4cb2cdefc377",
        "title": "Neural Development and Sensorimotor Control"
      },
      {
        "paperId": "e80a978640d82bfab318d949eaae52c55329a1a5",
        "title": "A Agent Environment StatesActions Rewards Critic B Agent Internal Environment Rewards Critic External Environment Sensations StatesDecisions Actions \" Organism \" Figure 1"
      },
      {
        "paperId": "7c2f71036b0a814dbcd495494e1679d440e305fc",
        "title": "MISEP -- Linear and Nonlinear ICA Based on Mutual Information"
      },
      {
        "paperId": "0a8149fb5aa8a5684e7d530c264451a5cb9250f5",
        "title": "Recent Advances in Hierarchical Reinforcement Learning"
      },
      {
        "paperId": "a28c4a55514bf6a8ddd536e2aeaa4fc6a31018c8",
        "title": "Recent Advances in Hierarchical Reinforcement Learning"
      },
      {
        "paperId": "b55987b4cfff292dd121ee03c46b41f4f696136e",
        "title": "Intrinsic and Extrinsic Motivations: Classic Definitions and New Directions."
      },
      {
        "paperId": "6b4d8cf155b77a63829920c7d77dbca1229b8595",
        "title": "The scientist in the crib : minds, brains, and how children learn"
      },
      {
        "paperId": "97efafdb4a3942ab3efba53ded7413199f79c054",
        "title": "Reinforcement Learning: An Introduction"
      },
      {
        "paperId": "4d98ce60f4f8ed822503b8d13b0605f8c5d74ca7",
        "title": "In Advances in Neural Information Processing Systems"
      },
      {
        "paperId": "2547be25e1e07728aa0966a0354e90664816d15e",
        "title": "REINFORCEMENT DRIVEN INFORMATION ACQUISITION IN NON-DETERMINISTIC ENVIRONMENTS"
      },
      {
        "paperId": "e67edbbc05855be34a1468c408c19fea0a8975e7",
        "title": "Structure and direction in thinking"
      },
      {
        "paperId": "abafd744e85dbc3122c5463e12f9edcab6770a11",
        "title": "This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination. IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION 1 Intrinsic Motivation Systems for Autonomous Mental Development"
      },
      {
        "paperId": "8c3b4aea7c83fc41cfa5bc045f070cf31b84c652",
        "title": "Neuroscience and Biobehavioral Reviews Theories and Computational Models of Affordance and Mirror Systems: an Integrative Review"
      }
    ],
    "cited_by": [
      {
        "paperId": "1ab4158cdc9df202fe7e0a1228427a43e7adc544",
        "title": "Novelty as a drive of human exploration in complex stochastic environments."
      },
      {
        "paperId": "b84b7f712b0afb0bd1a958fbfd907b2a698e3a5b",
        "title": "VendiRL: A Framework for Self-Supervised Reinforcement Learning of Diversely Diverse Skills"
      },
      {
        "paperId": "7a7b1d278b7ea8eb6b00418618df0a5afab9260b",
        "title": "How Should We Meta-Learn Reinforcement Learning Algorithms?"
      },
      {
        "paperId": "3ee5f99956a936b62ab84ae302c8f2bf222b3eb3",
        "title": "From Curiosity to Competence: How World Models Interact with the Dynamics of Exploration"
      },
      {
        "paperId": "9695f1fe17dbc3773c18f2e5b0971cff1ef0ce2a",
        "title": "Self-Adapting CPU Scheduling for Mixed Database Workloads via Hierarchical Deep Reinforcement Learning"
      },
      {
        "paperId": "7d211d697f5986a418e1823ed2ca3acedd31e0a8",
        "title": "Knowledge-Driven Visual Target Navigation: Dual Graph Navigation"
      },
      {
        "paperId": "47fccd1b5e3faa0e9184c8b401c3b3486717e697",
        "title": "Imagine, Verify, Execute: Memory-Guided Agentic Exploration with Vision-Language Models"
      },
      {
        "paperId": "9f95fa0ed30c8c7d9b0b0b3d7d6471fa92b14d61",
        "title": "Temporal\u2013orbitofrontal pathway regulates choices across physical reward and visual novelty"
      },
      {
        "paperId": "39ee5cf6d5c5566275015a23a370ba530f1422b0",
        "title": "Emergence of Goal-Directed Behaviors via Active Inference with Self-Prior"
      },
      {
        "paperId": "40f25f30f2b260add8abfbb30bf33512e84d980e",
        "title": "Intrinsically-Motivated Humans and Agents in Open-World Exploration"
      },
      {
        "paperId": "0b32d9781745580d9247b5418ee56c2ec5674aa9",
        "title": "InDRiVE: Intrinsic Disagreement based Reinforcement for Vehicle Exploration through Curiosity Driven Generalized World Model"
      },
      {
        "paperId": "df00398f659fc47b2261b5cba39bb25e3828f4d8",
        "title": "Towards a Formal Theory of the Need for Competence via Computational Intrinsic Motivation"
      },
      {
        "paperId": "a1ff433e2d852e886cf950be061326940e731af5",
        "title": "Learning english vocabulary via Instagram or YouTube: Surveying the impacts on motivation, growth mindfulness, willingness to communicate, and enjoyment from the lens of self-determination theory"
      },
      {
        "paperId": "5f557623b53913b8c7cd07359c8f72bcc756e2b3",
        "title": "The impact of intrinsic rewards on exploration in Reinforcement Learning"
      },
      {
        "paperId": "e4fef8d5864c5468100ca167639ef3fa374c0442",
        "title": "MaxInfoRL: Boosting exploration in reinforcement learning through information gain maximization"
      },
      {
        "paperId": "2a8017583a38e36e99f0f805ad4bb9037029de49",
        "title": "Representational similarity modulates neural and behavioral signatures of novelty"
      },
      {
        "paperId": "504edbc26687d3098d6f81f5439a9099edccae9d",
        "title": "Diversity Progress for Goal Selection in Discriminability-Motivated RL"
      },
      {
        "paperId": "d5d610d9d3afd81ff7bed3795aa06e7529072fe1",
        "title": "Constrained Skill Discovery: Quadruped Locomotion with Unsupervised Reinforcement Learning"
      },
      {
        "paperId": "a2f5879e01f52c3cb922481feec40d858c400be6",
        "title": "BAMDP Shaping: a Unified Framework for Intrinsic Motivation and Reward Shaping"
      },
      {
        "paperId": "d97d6c963131b9ac226ed91f3456e16fe8fd0b51",
        "title": "Employees\u2019 intrinsic motivation in mediating the relationship between perceived ambidextrous organizational culture and innovative behaviour in the Indian IT sector"
      },
      {
        "paperId": "f5bbde644e1d7c22f3685dae72e19e7a4afde79f",
        "title": "Image-Based Deep Reinforcement Learning with Intrinsically Motivated Stimuli: On the Execution of Complex Robotic Tasks"
      },
      {
        "paperId": "2451340ad1cc27629ee7181ed7630b3b4f0cc7ca",
        "title": "Interest, Emotional Intelligence, and Intellectual Intelligence on PJOK Learning Achievement in Middle School Students"
      },
      {
        "paperId": "f85dc53e8e4ae9af1e3fa2721aeb604041369b2a",
        "title": "Fear based Intrinsic Reward as a Barrier Function for Continuous Reinforcement Learning"
      },
      {
        "paperId": "6bf550ce7f10a4347d448eb810a92e1a5cfffa4b",
        "title": "Beyond Optimism: Exploration With Partially Observable Rewards"
      },
      {
        "paperId": "769d8fdf6520c52e8767ee6d54f6417cf6e7904e",
        "title": "RLeXplore: Accelerating Research in Intrinsically-Motivated Reinforcement Learning"
      },
      {
        "paperId": "7cc65b57dbe007c9b3bd66af4914d15558708400",
        "title": "A critique of motivation constructs to explain higher-order behavior: We should unpack the black box"
      },
      {
        "paperId": "ab34785a825507ee14e3e5d5ea9ff5106693f972",
        "title": "Leveraging Empowerment to Model Tool Use in Reinforcement Learning"
      },
      {
        "paperId": "60f37479ea261452a25573ff2a078a55319a9db8",
        "title": "Learning to identify and settle dilemmas through contextual user preferences"
      },
      {
        "paperId": "7d82f1021c3f385ec2fdd7b1a06ea71430e650e8",
        "title": "Hierarchical reinforcement learning with adaptive scheduling for robot control"
      },
      {
        "paperId": "aec92816584a2516936906fcb3319ea7f0421d50",
        "title": "Curiosity-driven exploration: foundations in neuroscience and computational modeling"
      },
      {
        "paperId": "bb247435f722a689c0568760e73a9544d5c20933",
        "title": "Hieros: Hierarchical Imagination on Structured State Space Sequence World Models"
      },
      {
        "paperId": "a6d59d99a589c593e6334798889347b277cc2ba4",
        "title": "The level of motivation and barrier to physical education learning: the perspective of boarding school students"
      },
      {
        "paperId": "d6c93f21cf521b2c239c6016b860838bcb48e797",
        "title": "Large-scale Passenger Behavior Learning and Prediction in Airport Terminals based on Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "8e86382d64cc68faae3ecf23334ce44babe6e5be",
        "title": "Designing Aquaculture Monitoring System Based on Data Fusion through Deep Reinforcement Learning (DRL)"
      },
      {
        "paperId": "4da7c38585ad044f62914a7411cb1878c7df7c4e",
        "title": "Self-supervised network distillation: An effective approach to exploration in sparse reward environments"
      },
      {
        "paperId": "26115e5d6f20c9e7560e1709f3c36847deffa148",
        "title": "A general Markov decision process formalism for action-state entropy-regularized reward maximization"
      },
      {
        "paperId": "cee43aad6a4039b2d412bc27dadde41040e8f3e0",
        "title": "Intrinsic Motivation in Model-based Reinforcement Learning: A Brief Review"
      },
      {
        "paperId": "d39eaba62c0033004e4061e3d0d65c328684eb14",
        "title": "Complex behavior from intrinsic motivation to occupy future action-state path space"
      },
      {
        "paperId": "e2a4b7e08733a98b2447294c042267d22040250d",
        "title": "Motivation as a key factor in lifelong learning"
      },
      {
        "paperId": "c68a3e4c085dc8c489f453e8fcb1089e6afc3147",
        "title": "Optimal Cogeneration Scheduling: A Comparison of Genetic and POMDP-Based Deep Reinforcement Learning Approaches"
      },
      {
        "paperId": "b8118fce1e4f34d44e0aaad9062bcfc665c98bf1",
        "title": "BAMDP Shaping: a Unified Theoretical Framework for Intrinsic Motivation and Reward Shaping"
      },
      {
        "paperId": "3368a9086d777e299b59a6bcf5f0144ec6673c58",
        "title": "Compressed information is all you need: unifying intrinsic motivations and representation learning"
      },
      {
        "paperId": "9a0f7481e36b6651c47b7bc39e6271ef0a7c3bf5",
        "title": "Prioritizing Compression Explains Human Perceptual Preferences"
      },
      {
        "paperId": "2673ef7de933b4d7e68f4a93298eddeb0f6f3dde",
        "title": "Learning Bimanual Manipulation"
      }
    ],
    "score": 14.666666666666666
  },
  {
    "id": "7d05987db045c56fa691da40e679cd328f0b68ef",
    "title": "Study on the application of reinforcement learning in the operation optimization of HVAC system",
    "authors": [
      "Xiaolei Yuan",
      "Yiqun Pan",
      "Jianrong Yang",
      "Weitong Wang",
      "Zhizhong Huang"
    ],
    "year": 2020,
    "citationCount": 72,
    "abstract": "",
    "url": "https://www.semanticscholar.org/paper/7d05987db045c56fa691da40e679cd328f0b68ef",
    "pdf_url": "https://doi.org/10.1007/s12273-020-0602-9",
    "venue": "Building Simulation",
    "publicationDate": "2020-03-23",
    "externalIds": {
      "MAG": "3013294057",
      "DOI": "10.1007/s12273-020-0602-9",
      "CorpusId": 216349600
    },
    "references": [],
    "cited_by": [
      {
        "paperId": "128c0c1381e1dc2ffa1e7a99aad3342e0924b0f2",
        "title": "Reward function design in reinforcement learning for HVAC Control: A review of thermal comfort and energy efficiency Trade-offs"
      },
      {
        "paperId": "828dc106252b41830ca2e5512c6d022dcf7d47bd",
        "title": "Intelligent control of air conditioning system for an electric vehicle: Based on reinforcement learning"
      },
      {
        "paperId": "18e14ffddb5aeb55bf303608dbf6002ed387a18c",
        "title": "Multi-agent distributed reinforcement learning for energy-efficient thermal comfort control in multi-zone buildings with diverse occupancy patterns"
      },
      {
        "paperId": "34759810a90c08d2869466967aa7ce94bfc57bb2",
        "title": "MuFlex: A Scalable, Physics-based Platform for Multi-Building Flexibility Analysis and Coordination"
      },
      {
        "paperId": "049ae3bbb43927d964918351d44d33d26b8b0ef8",
        "title": "Optimizing Grid-Interactive Buildings Demand Response: Sequence-based Decision-making Multi-Agent Policy Decomposition Deep Reinforcement Learning"
      },
      {
        "paperId": "10e74d91d518303777ce8b0ce13dc17bdf4238d4",
        "title": "Reinforcement Learning for Vehicle Air Conditioning Systems: Dynamic Optimization of Energy Efficiency and Occupant Thermal Comfort"
      },
      {
        "paperId": "21aad2533d1601ac4585c282ac153887d5494962",
        "title": "Energy Efficiency Optimization of Air Conditioning Systems Towards Low-Carbon Cleanrooms: Review and Future Perspectives"
      },
      {
        "paperId": "50e68a4cb9171afe2bf63496463c7fdf54f5931c",
        "title": "Home energy management system based on applied real-time load scheduling for self-consumption enhancement"
      },
      {
        "paperId": "0858a17f0a7b9c465c0b8a827815f69fe269a822",
        "title": "Adaptive Transfer Reinforcement Learning (TRL) for Cooling Water Systems with Uniform Agent Design and Multi-Agent Coordination"
      },
      {
        "paperId": "f5afafdd483eeeb602fe016351fa2f013f91d980",
        "title": "Optimizing HVACR System Efficiency and Comfort Levels Using Machine Learning-Based Control Methods"
      },
      {
        "paperId": "2380c5d1db4806a8e2d200c4fa4e4de9f4185f07",
        "title": "Towards Sustainable Energy Use: Reinforcement Learning for Demand Response in Commercial Buildings"
      },
      {
        "paperId": "e02fb48bbcfcca9b01bf093e2efb8f855af98931",
        "title": "Reinforcement Learning for Control and Optimization of Real Buildings: Identifying and Addressing Implementation Hurdles"
      },
      {
        "paperId": "ce41e001b7cf35fc024b987a1eb66e7499f5ff69",
        "title": "HVAC Control Based on Reinforcement Learning and Fuzzy Reasoning: Optimizing HVAC Supply Air Temperature, Flow Rate, and Velocity"
      },
      {
        "paperId": "8e1d652a03397478096595baa88f3a9c0c6db73d",
        "title": "An environment-adaptive SAC-based HVAC control of single-zone residential and office buildings"
      },
      {
        "paperId": "9f56d3cd688a92e3b85536e61a66716e7819dd7d",
        "title": "Exploring the Comprehensive Integration of Artificial Intelligence in Optimizing HVAC System Operations: A Review and Future Outlook"
      },
      {
        "paperId": "039f1e342f6add6497cf2b903b09abf8b9f3a6e6",
        "title": "Reinforcement Learning-Based HVAC System Operation Under Limited Data Acquisition"
      },
      {
        "paperId": "ec5a3d918d4a8eb1038561fc48f31a9af69a5534",
        "title": "Optimal Operation of Air Conditioning System in Solar Home for Peak Demand Reduction"
      },
      {
        "paperId": "4f7d0322b063798703c13a07cf952227312290d1",
        "title": "State of the art review on the HVAC occupant-centric control in different commercial buildings"
      },
      {
        "paperId": "51340de4f11abb5d73ab4bee1cf9a1e6852d643d",
        "title": "Study on Adaptation of Model-Agnostic Meta-Learning Integrated Deep Q-Learning for HVAC Control"
      },
      {
        "paperId": "e56fe8e52c4cf0c5cd0d8c4dae3835994a694bac",
        "title": "Application of reinforcement learning to deduce nuclear power plant severe accident scenario"
      },
      {
        "paperId": "042deff1b1011d3734ac6f1bef35c55e2f22e440",
        "title": "A Survey of Reinforcement Learning for Optimization in Automation"
      },
      {
        "paperId": "ef897db392741fc3ae8aae2ce01e820e02713548",
        "title": "Experimental evaluation of offline reinforcement learning for HVAC control in buildings"
      },
      {
        "paperId": "c8e129ac6ca26592a2acc85ada558dd52a61d9cd",
        "title": "A Review on Reinforcement Learning in Production Scheduling: An Inferential Perspective"
      },
      {
        "paperId": "b7b82e8aee958c9c634acebd87c61fe7dda9ae66",
        "title": "Exploring automated energy optimization with unstructured building data: A multi-agent based framework leveraging large language models"
      },
      {
        "paperId": "f30c9ee3f6ce36f6de857308cb12c5bf40b5fa00",
        "title": "Supervised Learning based Iterative Learning Control Platform for Optimal HVAC Start-Stop in a Real Building Context"
      },
      {
        "paperId": "ce14736426acf7acd9e253493f5ab5a1003e993d",
        "title": "Reinforcement learning for HVAC control in intelligent buildings: A technical and conceptual review"
      },
      {
        "paperId": "c5777eda819dc7b9e025b668fc2364ebc7ebae55",
        "title": "A comprehensive review of predictive control strategies in Heating, Ventilation, and Air-conditioning (HVAC): Model-free VS Model"
      },
      {
        "paperId": "6063b5660f7a965a4e826a4a5008d6d2286a953a",
        "title": "Online HVAC Optimization under Comfort Constraints via Reinforcement Learning"
      },
      {
        "paperId": "e7aa89c3354d059f4e00b2f1110398fbec662645",
        "title": "Regulating the imbalance for the container relocation problem: A deep reinforcement learning approach"
      },
      {
        "paperId": "385dc2ae16d17dd76189f9a468020d7938e5701e",
        "title": "Energy-Efficient HVAC Control based on Reinforcement Learning and Transfer Learning in a Residential Building"
      },
      {
        "paperId": "9a053aca36c1fe0b357409ce0931c7fb57757668",
        "title": "Intelligent control of electric vehicle air conditioning system based on deep reinforcement learning"
      },
      {
        "paperId": "adccef64c4d4e46c25aa9c476ae1235a48976340",
        "title": "Implementing a web-based optimized artificial intelligence system with metaheuristic optimization for improving building energy performance"
      },
      {
        "paperId": "4ae9df8ade8dff62bd03a108fa8884f9fbf54a03",
        "title": "Data-driven online energy management framework for HVAC systems: An experimental study"
      },
      {
        "paperId": "34c553e444c06d977b5c2579a51ec05df716478f",
        "title": "Indoor Temperature and Humidity Control System Based on Transfer Deep Reinforcement Learning"
      },
      {
        "paperId": "8829c97856655d3be2fe1bca0ae0ebb898350aec",
        "title": "Building Performance under Untypical Weather Conditions: A 40-Year Study of Hong Kong"
      },
      {
        "paperId": "2842a9d987c6caadea507d628aa73ceceba75671",
        "title": "A seasonal experimental study on a novel CdTe based multi-layer PV ventilated window system integrated with PCM under different operating modes"
      },
      {
        "paperId": "978654ef05fcb9190f99bdc28026343efcfe12e4",
        "title": "Energy-efficient control of indoor PM2.5 and thermal comfort in a real room using deep reinforcement learning"
      },
      {
        "paperId": "8fe778398d1ad5b57c19eb4185fec105d5a2ca63",
        "title": "MF\u02c62: Model-free reinforcement learning for modeling-free building HVAC control with data-driven environment construction in a residential building"
      },
      {
        "paperId": "5caba2858f375e8935027b42f03ff03b6c505ed6",
        "title": "A Comprehensive Review of the Applications of Machine Learning for HVAC"
      },
      {
        "paperId": "304b0b7f278e5215f573d03b212a5987dc628929",
        "title": "Optimization Control Strategy for a Central Air Conditioning System Based on AFUCB-DQN"
      },
      {
        "paperId": "4f50b2631edcc589be54746e0fee19352c01ac66",
        "title": "Indoor temperature preference setting control method for thermal comfort and energy saving based on reinforcement learning"
      },
      {
        "paperId": "7a2abe8a955597101359d2ada89babe7f822d5fe",
        "title": "System modeling for grid-interactive efficient building applications"
      },
      {
        "paperId": "d7a54c3edcd863813bc07599b4bfee1437dcf25b",
        "title": "Deep Reinforcement Learning-Based Joint Optimization Control of Indoor Temperature and Relative Humidity in Office Buildings"
      },
      {
        "paperId": "645db5b609a9ba1893f83e7d115d63d846f7bf9b",
        "title": "Energy Cost Driven Heating Control with Reinforcement Learning"
      },
      {
        "paperId": "90d5a78f6fbe9cf212e280d053bee89674dcd51d",
        "title": "Application of deep reinforcement learning to intelligent distributed humidity control system"
      },
      {
        "paperId": "fe6890bbfe72d37aa5284e1257ac6d6bad6ec5ed",
        "title": "Model free optimization of building cooling water systems with refined action space"
      },
      {
        "paperId": "c11aa0148438a3a4aa9903018ce956c705b45bd4",
        "title": "Energy Consumption Control Method for Oven based on DDPG"
      },
      {
        "paperId": "2b97c47d9b0be96c0c4d4c6f53f1565ee5d5b7e0",
        "title": "Quantifying the Effect of Index-Based Operation Logic for Building Environmental Control System\u2014Taking Shading as Example"
      },
      {
        "paperId": "c72c30d7c115d3958c50a30adb0954a248e8ae80",
        "title": "Multi-Strategy Improved Sparrow Search Algorithm and Application"
      },
      {
        "paperId": "5b625afb69a2fe08ae8f944a03cab088ef607f0f",
        "title": "Smart control of window and air cleaner for mitigating indoor PM2.5 with reduced energy consumption based on deep reinforcement learning"
      },
      {
        "paperId": "8d520cfa0e995dd6e1c0818b00dd70156eb8d191",
        "title": "Intelligent Distributed Temperature and Humidity Control Mechanism for Uniformity and Precision in the Indoor Environment"
      },
      {
        "paperId": "969500b3568407ea7a3335c082871081e1ba12dc",
        "title": "Investigation of VRF system cooling operation and performance in residential buildings based on large-scale dataset"
      },
      {
        "paperId": "08e9fb8b798c3e3e9854d172fdcd328275f97e3a",
        "title": "Trade-off decisions in a novel deep reinforcement learning for energy savings in HVAC systems"
      },
      {
        "paperId": "8c17f23765d5fa1df5f99dc81574e8b86aa59947",
        "title": "HVAC Operation Planning for Electric Bus Trips Based on Chance-Constrained Programming"
      },
      {
        "paperId": "0bc29e069f419cd95f0c87e6ff3b6767324747c4",
        "title": "End-to-End Deep Reinforcement Learning Control for HVAC Systems in Office Buildings"
      },
      {
        "paperId": "18cd7a0921c472f9cfda1db4d6135a35febaa35d",
        "title": "A Review of Reinforcement Learning Applications to Control of Heating, Ventilation and Air Conditioning Systems"
      },
      {
        "paperId": "1e0a9e378ad41f1c691cb8e0c509b8432fac08de",
        "title": "Airflow management and energy saving potentials at a high-density data center with stepped-like server placement"
      },
      {
        "paperId": "7cbc58a7dde83f45f97638ee2d17f133346b5094",
        "title": "A non-cooperative game-based distributed optimization method for chiller plant control"
      },
      {
        "paperId": "1a897b47759f0e9b921a2b05fce0948cfa679c26",
        "title": "Research on operation strategy of radiant cooling system based on intermittent operation characteristics"
      },
      {
        "paperId": "0d83dea3383871f0eb021d2443cc434d5544b13c",
        "title": "Designing a generalised reward for Building Energy Management Reinforcement Learning agents"
      },
      {
        "paperId": "009a9feff2c0a384c47cbb0e89e3ae5f50ac9eb7",
        "title": "Data-driven model predictive control for power demand management and fast demand response of commercial buildings using support vector regression"
      },
      {
        "paperId": "3ca6feb27b556f6967a852d03e88b8fb3b595a4f",
        "title": "Evaluating the Performance of Water Chillers Equipped with Constant- or Variable-Frequency Centrifugal Compressors"
      },
      {
        "paperId": "8fa45d8923b96c875ec8a78a759c8be0d02199c0",
        "title": "Individual thermal comfort prediction using classification tree model based on physiological parameters and thermal history in winter"
      },
      {
        "paperId": "3f12e01210587ba9c73a6c83f63c31d9c7ac2fdf",
        "title": "Demand response of district heating using model predictive control to prevent the draught risk of cold window in an office building"
      },
      {
        "paperId": "99f06e88e76f1af51d08d7adfb26d758ebc6acab",
        "title": "Advanced data analytics for enhancing building performances: From data-driven to big data-driven approaches"
      },
      {
        "paperId": "5ed6d320d0857fecd628316a2d274d4210ad7216",
        "title": "Fast prediction for multi-parameters (concentration, temperature and humidity) of indoor environment towards the online control of HVAC system"
      },
      {
        "paperId": "151fa0fa470b15b8d8db3ae337ff09e0956c8381",
        "title": "Fast prediction for multi-parameters (concentration, temperature and humidity) of indoor environment towards the online control of HVAC system"
      },
      {
        "paperId": "bd28860b5e8f213043e9147de63711a623a3281c",
        "title": "Novel occupancy detection method based on convolutional neural network model using PIR sensor and smart meter data"
      },
      {
        "paperId": "aff51af9ff71acae8a26cf6c2d9d0bfde065dce9",
        "title": "Forecast-based and data-driven reinforcement learning for residential heat pump operation"
      },
      {
        "paperId": "ba8d06f53f36ee562ded256b9a907cbf4dddd056",
        "title": "Real-Time Intelligent Thermal Comfort Prediction Model"
      },
      {
        "paperId": "96f2b33434bd4bafbaf8d4a129688e6fa626d8df",
        "title": "Reinforcement Learning for Residential Heat Pump Operation"
      },
      {
        "paperId": "f2d4334d7020d1883525fb492a140939fe21c2c3",
        "title": "Purdue e-Pubs Purdue e-Pubs"
      }
    ],
    "score": 14.4
  },
  {
    "id": "399806e861a2ef960a81b37b593c2176a728c399",
    "title": "Offline Reinforcement Learning as Anti-Exploration",
    "authors": [
      "Shideh Rezaeifar",
      "Robert Dadashi",
      "Nino Vieillard",
      "L'eonard Hussenot",
      "Olivier Bachem",
      "O. Pietquin",
      "M. Geist"
    ],
    "year": 2021,
    "citationCount": 57,
    "abstract": "Offline Reinforcement Learning (RL) aims at learning an optimal control from a fixed dataset, without interactions with the system. An agent in this setting should avoid selecting actions whose consequences cannot be predicted from the data. This is the converse of exploration in RL, which favors such actions. We thus take inspiration from the literature on bonus-based exploration to design a new offline RL agent. The core idea is to subtract a prediction-based exploration bonus from the reward, instead of adding it for exploration. This allows the policy to stay close to the support of the dataset and practically extends some previous pessimism-based offline RL methods to a deep learning setting with arbitrary bonuses. We also connect this approach to a more common regularization of the learned policy towards the data. Instantiated with a bonus based on the prediction error of a variational autoencoder, we show that our simple agent is competitive with the state of the art on a set of continuous control locomotion and manipulation tasks.",
    "url": "https://www.semanticscholar.org/paper/399806e861a2ef960a81b37b593c2176a728c399",
    "pdf_url": "https://arxiv.org/pdf/2106.06431.pdf",
    "venue": "AAAI Conference on Artificial Intelligence",
    "publicationDate": "2021-06-11",
    "externalIds": {
      "DBLP": "journals/corr/abs-2106-06431",
      "ArXiv": "2106.06431",
      "DOI": "10.1609/aaai.v36i7.20783",
      "CorpusId": 235417276
    },
    "references": [
      {
        "paperId": "0bcc734246586966596b1aa22efac2565224ebee",
        "title": "Bridging Offline Reinforcement Learning and Imitation Learning: A Tale of Pessimism"
      },
      {
        "paperId": "0dbef95ac15785e03b54a529a3d85128c19aa09a",
        "title": "Regularized Behavior Value Estimation"
      },
      {
        "paperId": "169fc06bbdadf008cb72a80762b5050bc5756ee4",
        "title": "Offline Reinforcement Learning with Pseudometric Learning"
      },
      {
        "paperId": "245682e8b3fa76f4a3e2991b5497577af95cbb3f",
        "title": "COMBO: Conservative Offline Model-Based Policy Optimization"
      },
      {
        "paperId": "5685abf9e7bb2c16449ae1eb181051e503602a55",
        "title": "Reinforcement Learning based Recommender Systems: A Survey"
      },
      {
        "paperId": "9f8a70e9188a913317e97ba1874c8f763fd13c04",
        "title": "Is Pessimism Provably Efficient for Offline RL?"
      },
      {
        "paperId": "11d6833bf57d9f482ee259501f5f1abead82404a",
        "title": "Leverage the Average: an Analysis of KL Regularization in Reinforcement Learning"
      },
      {
        "paperId": "86fa0f350b0ede3a86e31aa2900af551531ee570",
        "title": "The Importance of Pessimism in Fixed-Dataset Policy Optimization"
      },
      {
        "paperId": "5015e3f9220b5569d21a5cd0ed2bd10c1c621693",
        "title": "Munchausen Reinforcement Learning"
      },
      {
        "paperId": "fa7f88f77de02ae9389e514a1cd13083a624ec78",
        "title": "EMaQ: Expected-Max Q-Learning Operator for Simple Yet Effective Offline and Online RL"
      },
      {
        "paperId": "7acbdb961f67d50fef359066f2a1d7755cf16ee2",
        "title": "Critic Regularized Regression"
      },
      {
        "paperId": "0272b14dd471fe7b81df703af1b71d7600b77215",
        "title": "Accelerating Online Reinforcement Learning with Offline Datasets"
      },
      {
        "paperId": "28db20a81eec74a50204686c3cf796c42a020d2e",
        "title": "Conservative Q-Learning for Offline Reinforcement Learning"
      },
      {
        "paperId": "dea0f1c5949f8d898b9b6ff68226a781558e413c",
        "title": "MOPO: Model-based Offline Policy Optimization"
      },
      {
        "paperId": "5e7bc93622416f14e6948a500278bfbe58cd3890",
        "title": "Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems"
      },
      {
        "paperId": "a326d9f2d2d351001fece788165dbcbb524da2e4",
        "title": "D4RL: Datasets for Deep Data-Driven Reinforcement Learning"
      },
      {
        "paperId": "a595767fa35bcc84362f629fbc4d2d9b05d7342a",
        "title": "A survey of deep learning techniques for autonomous driving"
      },
      {
        "paperId": "ad14227e4f51276892ffc37aa43fd8750bb5eba8",
        "title": "Advantage-Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning"
      },
      {
        "paperId": "9be492858863c8c7c24be1ecb75724de5086bd8e",
        "title": "Behavior Regularized Offline Reinforcement Learning"
      },
      {
        "paperId": "82b4b03a4659d6e04bd7cbf51d6e08fde1348dbd",
        "title": "Stabilizing Off-Policy Q-Learning via Bootstrapping Error Reduction"
      },
      {
        "paperId": "2c0a703f036ef2d0ea4d97a123ec010cd73646b0",
        "title": "Random Expert Distillation: Imitation Learning via Expert Policy Support Estimation"
      },
      {
        "paperId": "b3b3d1d6d36ac203cd06c00bb37e66c000430275",
        "title": "A Theory of Regularized Markov Decision Processes"
      },
      {
        "paperId": "5285cb8faada5de8a92a47622950f6cfd476ac1d",
        "title": "Off-Policy Deep Reinforcement Learning without Exploration"
      },
      {
        "paperId": "f9717d29840f4d8f1cc19d1b1e80c5d12ec40608",
        "title": "A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play"
      },
      {
        "paperId": "4cb3fd057949624aa4f0bbe7a6dcc8777ff04758",
        "title": "Exploration by Random Network Distillation"
      },
      {
        "paperId": "ca14dce53be20d3d23d4f0db844a8389ab619db3",
        "title": "Large-Scale Study of Curiosity-Driven Learning"
      },
      {
        "paperId": "70f9968a356d840040a1c9207906f60376dc6bd4",
        "title": "Generative Probabilistic Novelty Detection with Adversarial Autoencoders"
      },
      {
        "paperId": "5f1e7c3c81d6a9e716eab660bb7536ecb204ef7d",
        "title": "Latent Space Autoregression for Novelty Detection"
      },
      {
        "paperId": "4debb99c0c63bfaa97dd433bc2828e4dac81c48b",
        "title": "Addressing Function Approximation Error in Actor-Critic Methods"
      },
      {
        "paperId": "811df72e210e20de99719539505da54762a11c6d",
        "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor"
      },
      {
        "paperId": "7eb0db941fbf19857631ec1c907f2888ea308779",
        "title": "Safe Policy Improvement with Baseline Bootstrapping"
      },
      {
        "paperId": "e010ba3ff5744604cdbfe44a733e2a98649ee907",
        "title": "Learning Complex Dexterous Manipulation with Deep Reinforcement Learning and Demonstrations"
      },
      {
        "paperId": "33690ff21ef1efb576410e656f2e60c89d0307d6",
        "title": "Deep Reinforcement Learning that Matters"
      },
      {
        "paperId": "2b75ba7f75170b73d913c515cc0deefef6c88f5f",
        "title": "Anomaly Detection with Robust Deep Autoencoders"
      },
      {
        "paperId": "225ab689f41cef1dc18237ef5dab059a49950abf",
        "title": "Curiosity-Driven Exploration by Self-Supervised Prediction"
      },
      {
        "paperId": "8db9df2eadea654f128c1887722c677c708e8a47",
        "title": "Deep Reinforcement Learning framework for Autonomous Driving"
      },
      {
        "paperId": "d6757aedcb53142bc439ec64bfd0b056d99b1881",
        "title": "Surprise-Based Intrinsic Motivation for Deep Reinforcement Learning"
      },
      {
        "paperId": "12f67fb182bc934fc95ce97acff553d83e2ca72e",
        "title": "Count-Based Exploration with Neural Density Models"
      },
      {
        "paperId": "0fcb2034e31a2bc2f12a2b1363d0d77baf445fdf",
        "title": "#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning"
      },
      {
        "paperId": "bd58d3265263a87159a3f7ed3a5e4c887c5c0792",
        "title": "Safe Policy Improvement by Minimizing Robust Baseline Regret"
      },
      {
        "paperId": "6e90fd78e8a3b98af3954aae5209703aa966603e",
        "title": "Unifying Count-Based Exploration and Intrinsic Motivation"
      },
      {
        "paperId": "f88a6f6fd6611543220482e6b3a5f379b7bf5049",
        "title": "Increasing the Action Gap: New Operators for Reinforcement Learning"
      },
      {
        "paperId": "3f25e17eb717e5894e0404ea634451332f85d287",
        "title": "Learning Structured Output Representation using Deep Conditional Generative Models"
      },
      {
        "paperId": "60fef33549f57f5cbb6712a510c3a444ab682429",
        "title": "Learning Deep Representations of Appearance and Motion for Anomalous Event Detection"
      },
      {
        "paperId": "024006d4c2a89f7acacc6e4438d156525b60a98f",
        "title": "Continuous control with deep reinforcement learning"
      },
      {
        "paperId": "01954aca519d210495832bf2b5963f64d9631a4b",
        "title": "RAAM: The Benefits of Robustness in Approximating Aggregated MDPs in Reinforcement Learning"
      },
      {
        "paperId": "471e452dc02edcb9c8c0ec446cc2eb22188dd86b",
        "title": "Offline policy evaluation across representations with applications to educational games"
      },
      {
        "paperId": "484ad17c926292fbe0d5211540832a8c8a8e958b",
        "title": "Stochastic Backpropagation and Approximate Inference in Deep Generative Models"
      },
      {
        "paperId": "2319a491378867c7049b3da055c5df60e1671158",
        "title": "Playing Atari with Deep Reinforcement Learning"
      },
      {
        "paperId": "1695ff30aa141088a019fc29df6d128d6add06f5",
        "title": "Novelty or Surprise?"
      },
      {
        "paperId": "65438e0ba226c1f97bd8a36333ebc3297b1a32fd",
        "title": "Reinforcement learning in robotics: A survey"
      },
      {
        "paperId": "bc6dff14a130c57a91d5a21339c23471faf1d46f",
        "title": "Et al"
      },
      {
        "paperId": "a96800c7b8e693f82a925298b9c177fa9b05e1fc",
        "title": "Learning CPG-based Biped Locomotion with a Policy Gradient Method: Application to a Humanoid Robot"
      },
      {
        "paperId": "41356b8998dd7ddf89429445320d82a269e3ab14",
        "title": "Tree-Based Batch Mode Reinforcement Learning"
      },
      {
        "paperId": "b750a17921d32936425e05f8b00b96569e2fc5a6",
        "title": "Least-Squares Policy Iteration"
      },
      {
        "paperId": "872d01b512def14d5a7d9dedecb4f8b3a67d979b",
        "title": "Marginal Mean Models for Dynamic Regimes"
      },
      {
        "paperId": "84b23b154ef3083839a4da8c460a1e1c110ea63b",
        "title": "Autonomous helicopter control using reinforcement learning policy search methods"
      },
      {
        "paperId": "96bc59ec6c682d2a70d9ee4c3aeb6253481463ad",
        "title": "An approach to learning mobile robot navigation"
      },
      {
        "paperId": "a9cd8efe9184dddb1bedbbec3a356c4dfb22fe63",
        "title": "Markov Decision Processes: Discrete Stochastic Dynamic Programming"
      },
      {
        "paperId": "2980dfe5c99658dc3e508d9d6e1d7f26e6fc8934",
        "title": "A possibility for implementing curiosity and boredom in model-building neural controllers"
      },
      {
        "paperId": null,
        "title": "and N"
      },
      {
        "paperId": "ef4f5a50837a7c1b3e87b9300ffc7ba00d461a0f",
        "title": "AUTO-ENCODING VARIATIONAL BAYES"
      },
      {
        "paperId": "f041ac53fba83674a23e0a4a3454f73b6112fe3c",
        "title": "New Recommendation System Using Reinforcement Learning"
      },
      {
        "paperId": "c7dbf5ed7e9b63e104adb4e18bfbc98f5d6afdae",
        "title": "R-MAX - A General Polynomial Time Algorithm for Near-Optimal Reinforcement Learning"
      },
      {
        "paperId": "24919c3d4de529d9ec4df381554d514c7b087760",
        "title": "Biped dynamic walking using reinforcement learning"
      }
    ],
    "cited_by": [
      {
        "paperId": "d56cfecbf01fd180e1b5084ec6bfc3a3bebff425",
        "title": "Noise-Guided Transport for Imitation Learning"
      },
      {
        "paperId": "b61529759b1f016e74962a384f13695e4ce0c1eb",
        "title": "LSTM-Enhanced TD3 and Behavior Cloning for UAV Trajectory Tracking Control"
      },
      {
        "paperId": "4558776c0e34bfa6d3533cbb9ad3ad16a473f54a",
        "title": "Mildly Conservative Regularized Evaluation for Offline Reinforcement Learning"
      },
      {
        "paperId": "229ef4d785297b353e3faae8817a9a54d9f168e7",
        "title": "Offline Reinforcement Learning with Penalized Action Noise Injection"
      },
      {
        "paperId": "e7f8edf4f95c9c1edc1ed1c577603914390580ac",
        "title": "Decision Flow Policy Optimization"
      },
      {
        "paperId": "7c5881743648a7dfa8ccbfb460ee8a27f358e473",
        "title": "Analytic Energy-Guided Policy Optimization for Offline Reinforcement Learning"
      },
      {
        "paperId": "1ea9c6268f2fcc2b2498e11c197fe182c066ebef",
        "title": "Provable Zero-Shot Generalization in Offline Reinforcement Learning"
      },
      {
        "paperId": "d88a3f7a64bd1e6b38381da8d627a78b9f399cdc",
        "title": "Beyond Shallow Behavior: Task-Efficient Value-Based Multi-Task Offline MARL via Skill Discovery"
      },
      {
        "paperId": "5d1861b5e4dc0cb94ce94918635a2d5b03e90a24",
        "title": "ELAPSE: Expand Latent Action Projection Space for policy optimization in Offline Reinforcement Learning"
      },
      {
        "paperId": "47387abd3fe650391ee84f8189e9c655c3dd067f",
        "title": "Large Language Model driven Policy Exploration for Recommender Systems"
      },
      {
        "paperId": "23d29d67af61dfdad2a5708d1e5505b2bd542cef",
        "title": "VCSAP: Online reinforcement learning exploration method based on visitation count of state-action pairs"
      },
      {
        "paperId": "90c2af458f55defe03c911d76f4e07ebee3a22d5",
        "title": "Hypercube Policy Regularization Framework for Offline Reinforcement Learning"
      },
      {
        "paperId": "e83c0c3bf031ead0547bea7ad5242e842169790c",
        "title": "Offline Model-Based Reinforcement Learning with Anti-Exploration"
      },
      {
        "paperId": "0d130b28c2496ec2aa5fa8f13e9ddcfced30b3f1",
        "title": "Dual Behavior Regularized Offline Deterministic Actor\u2013Critic"
      },
      {
        "paperId": "ea5a1a2c7d531100a445d26706779240c8894f2e",
        "title": "Sparsity-based Safety Conservatism for Constrained Offline Reinforcement Learning"
      },
      {
        "paperId": "22a9b6f22ec06b578cbcc9cdaacb7273b97a10c7",
        "title": "Judgmentally adjusted Q-values based on Q-ensemble for offline reinforcement learning"
      },
      {
        "paperId": "cb1d787eee5c84d01c315cb1dca3eef1c0ce38cb",
        "title": "Out-of-Distribution Adaptation in Offline RL: Counterfactual Reasoning via Causal Normalizing Flows"
      },
      {
        "paperId": "28b066f4d67b83a70f0295b8b70d2b44dc64cdc6",
        "title": "Balancing therapeutic effect and safety in ventilator parameter recommendation: An offline reinforcement learning approach"
      },
      {
        "paperId": "1c822cc4ef470ea91a70a34b9808dd4c2b39c6ff",
        "title": "Offline Reinforcement Learning with Behavioral Supervisor Tuning"
      },
      {
        "paperId": "6023082e39e69e732140ee11bf871f66130f8303",
        "title": "Grid-Mapping Pseudo-Count Constraint for Offline Reinforcement Learning"
      },
      {
        "paperId": "87ddd7811eddfa67609f4ff8d10fb2f6ba42d94f",
        "title": "Exploration and Anti-Exploration with Distributional Random Network Distillation"
      },
      {
        "paperId": "85e3fbeb7b90ac71a2912870273c4928c457bdb1",
        "title": "Pessimistic value iteration for multi-task data sharing in Offline Reinforcement Learning"
      },
      {
        "paperId": "24b2fdff9ab11e9d5782819edadb3cfdf41cd7db",
        "title": "Action-Quantized Offline Reinforcement Learning for Robotic Skill Learning"
      },
      {
        "paperId": "64170ce0f75998a1175a2f081238c80539391d29",
        "title": "Improving Offline-to-Online Reinforcement Learning with Q Conditioned State Entropy Exploration"
      },
      {
        "paperId": "5b19d4f3312370048407ee931f34f980ed653680",
        "title": "Adaptive Reward Shifting Based on Behavior Proximity for Offline Reinforcement Learning"
      },
      {
        "paperId": "731927972acdcd51da20c6a5f992521d74f34876",
        "title": "Offline Reinforcement Learning with On-Policy Q-Function Regularization"
      },
      {
        "paperId": "aeb466d5198024d5770863434f103814d80d7fcb",
        "title": "Beyond Conservatism: Diffusion Policies in Offline Multi-agent Reinforcement Learning"
      },
      {
        "paperId": "535c385767b188d8112f2748ce5433b744b18361",
        "title": "Design from Policies: Conservative Test-Time Adaptation for Offline Policy Optimization"
      },
      {
        "paperId": "c7439db4be4e9034472c852ca3a04e5a5d283b37",
        "title": "Beyond OOD State Actions: Supported Cross-Domain Offline Reinforcement Learning"
      },
      {
        "paperId": "2807f9c666335946113fb11dccadf36f8d78b772",
        "title": "A Simple Unified Uncertainty-Guided Framework for Offline-to-Online Reinforcement Learning"
      },
      {
        "paperId": "7312a2d0ec17241a139b5a8044914e06c12d93cc",
        "title": "Instructed Diffuser with Temporal Condition Guidance for Offline Reinforcement Learning"
      },
      {
        "paperId": "97a7968506a856ef25dfbbd48c13e57ab5a4b4b6",
        "title": "What is Essential for Unseen Goal Generalization of Offline Goal-conditioned RL?"
      },
      {
        "paperId": "1b6341b05471bb961b4e76467444d7b70c66bbb5",
        "title": "Revisiting the Minimalist Approach to Offline Reinforcement Learning"
      },
      {
        "paperId": "bd38cbbb346a347cb5b60ac4a133b3d73cb44e07",
        "title": "Efficient Online Reinforcement Learning with Offline Data"
      },
      {
        "paperId": "7f270c9b098727675c9d8b893e362b561d61f27e",
        "title": "Policy Expansion for Bridging Offline-to-Online Reinforcement Learning"
      },
      {
        "paperId": "7205658bdd14cc80d7c0b2057e88154aa50b7db6",
        "title": "Anti-Exploration by Random Network Distillation"
      },
      {
        "paperId": "b47e3d1cebb549d4ed9f420fc2ebadabc8997a9c",
        "title": "Importance Weighted Actor-Critic for Optimal Conservative Offline Reinforcement Learning"
      },
      {
        "paperId": "e9565e0242aed311888bf4dcc6fef06faf6fc61a",
        "title": "Confidence-Conditioned Value Functions for Offline Reinforcement Learning"
      },
      {
        "paperId": "e201436082ee00f2c8a73da4d19809c6f1772102",
        "title": "Q-Ensemble for Offline RL: Don't Scale the Ensemble, Scale the Batch Size"
      },
      {
        "paperId": "bacc496fc1adac13c2913638a75448e363a4b258",
        "title": "Offline RL With Realistic Datasets: Heteroskedasticity and Support Constraints"
      },
      {
        "paperId": "63888b247d2e42c9b9a64e15bbf4a89877fd797f",
        "title": "Optimal Conservative Offline RL with General Function Approximation via Augmented Lagrangian"
      },
      {
        "paperId": "c3be0dd9fed6d2638963a3e019303565ef82e2d7",
        "title": "Optimizing Pessimism in Dynamic Treatment Regimes: A Bayesian Learning Approach"
      },
      {
        "paperId": "adf34cf5b798260311d7238f22746fca829dc940",
        "title": "Age of Semantics in Cooperative Communications: To Expedite Simulation Towards Real via Offline Reinforcement Learning"
      },
      {
        "paperId": "ef416311538f91d85ac45b700d52b48772764120",
        "title": "Know Your Boundaries: The Necessity of Explicit Behavioral Cloning in Offline RL"
      },
      {
        "paperId": "e1ac9023f2991ebc840b99d6f0203201a3adfaf4",
        "title": "Pessimistic Q-Learning for Offline Reinforcement Learning: Towards Optimal Sample Complexity"
      },
      {
        "paperId": "23abe79046d7f7b430d5d21b6a93598d0aa1b9c2",
        "title": "Pessimistic Bootstrapping for Uncertainty-Driven Offline Reinforcement Learning"
      },
      {
        "paperId": "4ff8454c524163bbc5d25f6c8984b1c31ad057e4",
        "title": "Pessimistic Model-based Offline Reinforcement Learning under Partial Coverage"
      },
      {
        "paperId": "941e31df751884e8319d275ecaa5697407001347",
        "title": "Uncertainty-Aware Rank-One MIMO Q Network Framework for Accelerated Offline Reinforcement Learning"
      },
      {
        "paperId": "60601c268a5397efdbf70ab2795495c83633dea0",
        "title": "SpOiLer: Offline reinforcement learning using scaled penalties"
      },
      {
        "paperId": "65a5afa38450d30ac5e5fb27be93b92b8bfb1730",
        "title": "A Connection between One-Step RL and Critic Regularization in Reinforcement Learning"
      },
      {
        "paperId": "5234eace9af0dd7851d1431d1e5c458169ab79cd",
        "title": "A Robust Test for the Stationarity Assumption in Sequential Decision Making"
      },
      {
        "paperId": "efd5e5c3a34c65bef9f8cbcb8f9c3d07d1fa20e5",
        "title": "D ENSITY E STIMATION F OR C ONSERVATIVE Q-L EARNING"
      },
      {
        "paperId": "d0aa46553b6cae37cd5a1e9cd625f8fe564ef97f",
        "title": "Effective Offline RL Needs Going Beyond Pessimism: Representations and Distributional Shift"
      },
      {
        "paperId": "f1d2bf2891b07d7d589d047fceeecaa953dd6580",
        "title": "Beyond Conservatism in Offline Reinforcement Learning: The Importance of Effective Representations"
      },
      {
        "paperId": "70c69f4dbbaeb91f6841779106cd35fdb10427f6",
        "title": "O FFLINE R EINFORCEMENT L EARNING FROM H ETERO - SKEDASTIC D ATA V IA S UPPORT C ONSTRAINTS"
      },
      {
        "paperId": "3289944a9626301fec08960dae8cfd3ab74987d9",
        "title": "Pessimistic Model-based Offline RL: PAC Bounds and Posterior Sampling under Partial Coverage"
      },
      {
        "paperId": "6f67bd6bd2c9cdd02ed26b360d386e91ada59d9d",
        "title": "Architecture and technical approach for DT-based AI-training: state of the art"
      }
    ],
    "score": 14.25
  },
  {
    "id": "174be0bacee04d9eb13a698d484ab5ae441c1100",
    "title": "Effective deep Q-networks (EDQN) strategy for resource allocation based on optimized reinforcement learning algorithm",
    "authors": [
      "Fatma M. Talaat"
    ],
    "year": 2022,
    "citationCount": 42,
    "abstract": "The healthcare industry has always been an early adopter of new technology and a big benefactor of it. The use of reinforcement learning in the healthcare system has repeatedly resulted in improved outcomes.. Many challenges exist concerning the architecture of the RL method, measurement metrics, and model choice. More significantly, the validation of RL in authentic clinical settings needs further work. This paper presents a new Effective Resource Allocation Strategy (ERAS) for the Fog environment, which is suitable for Healthcare applications. ERAS tries to achieve effective resource management in the Fog environment via real-time resource allocating as well as prediction algorithms. Comparing the ERAS with the state-of-the-art algorithms, ERAS achieved the minimum Makespan as compared to previous resource allocation algorithms, while maximizing the Average Resource Utilization (ARU) and the Load Balancing Level (LBL). For each application, we further compared and contrasted the architecture of the RL models and the assessment metrics. In critical care, RL has tremendous potential to enhance decision-making. This paper presents two main contributions, (i) Optimization of the RL hyperparameters using PSO, and (ii) Using the optimized RL for the resource allocation and load balancing in the fog environment. Because of its exploitation, exploration, and capacity to get rid of local minima, the PSO has a significant significance when compared to other optimization methodologies.",
    "url": "https://www.semanticscholar.org/paper/174be0bacee04d9eb13a698d484ab5ae441c1100",
    "pdf_url": "https://doi.org/10.1007/s11042-022-13000-0",
    "venue": "Multimedia tools and applications",
    "publicationDate": "2022-05-05",
    "externalIds": {
      "DBLP": "journals/mta/Talaat22a",
      "DOI": "10.1007/s11042-022-13000-0",
      "CorpusId": 248587160
    },
    "references": [
      {
        "paperId": "fda3723e14c1c781fe48f3a08a2f512862e6fa3c",
        "title": "A collaborative healthcare framework for shared healthcare plan with ambient intelligence"
      },
      {
        "paperId": "175e8382106f6332c1fb875a615bd3b58633b342",
        "title": "Provably Efficient Online Hyperparameter Optimization with Population-Based Bandits"
      },
      {
        "paperId": "4a453a99144c1320e08e79cc3e3b3d9ae97f1fc8",
        "title": "Automated opportunistic osteoporotic fracture risk assessment using computed tomography scans to aid in FRAX underutilization"
      },
      {
        "paperId": "903979fbcb5d44147050276962e9ed8bf180bd75",
        "title": "Feasibility and acceptability of home use of a smartphone-based urine testing application among women in prenatal care."
      },
      {
        "paperId": "f37562789b906e66767051662f20dfad22c21644",
        "title": "Loss Function"
      },
      {
        "paperId": "2299cb18ab9ce89ab19b497eae37091339619572",
        "title": "Efficient Online Hyperparameter Adaptation for Deep Reinforcement Learning"
      },
      {
        "paperId": "16d10e6fd7987d19d0f49f193c936a5a411218d6",
        "title": "Improved Cancer Detection Using Artificial Intelligence: a Retrospective Evaluation of Missed Cancers on Mammography"
      },
      {
        "paperId": "25b26e67f1d426d92341be4242c759ff91e79905",
        "title": "Deep Reinforcement Learning Using Genetic Algorithm for Parameter Optimization"
      },
      {
        "paperId": "2f125ec28fbc405cc2d300980cd4711add6a4a7d",
        "title": "Bayesian Optimization in AlphaGo"
      },
      {
        "paperId": "39dfd28e59f709e57b0b10db5661b831c1d03409",
        "title": "Automatic treatment planning based on three\u2010dimensional dose distribution predicted from deep learning technique"
      },
      {
        "paperId": "cc07fc48ce2a381e7f39235cef5fd10b939182c4",
        "title": "The Artificial Intelligence Clinician learns optimal treatment strategies for sepsis in intensive care"
      },
      {
        "paperId": "e2bda2f8bd2afdb5f07e7bf24882688339e84ae9",
        "title": "Negative trials in critical care: why most research is probably wrong."
      },
      {
        "paperId": "737d18e8165867d87ab0660159ef297d66e55a1b",
        "title": "Towards fog driven IoT healthcare: challenges and framework of fog computing in healthcare"
      },
      {
        "paperId": "90d3418e87cf91ed4b6a9b00f5dd0021d2ecc34b",
        "title": "TextRay: Mining Clinical Reports to Gain a Broad Understanding of Chest X-rays"
      },
      {
        "paperId": "399dbf5ba3d00f37b8476bef04ad0dee90f85102",
        "title": "Optimal and Autonomous Control Using Reinforcement Learning: A Survey"
      },
      {
        "paperId": "ff9c3241394c5d39ec6145f671178fc7acf25812",
        "title": "Scientific evidence underlying the recommendations of critical care clinical practice guidelines: a lack of high level evidence"
      },
      {
        "paperId": "af10f3c1c0859aa620623f760c8a29e78f177f7f",
        "title": "Population Based Training of Neural Networks"
      },
      {
        "paperId": "498238a3bd5fd322fc3ce1572e33bbe3853a356f",
        "title": "Deep Reinforcement Learning: A Brief Survey"
      },
      {
        "paperId": "28e7f39873d4c4995452da5f90fce418091501f4",
        "title": "Online meta-learning by parallel algorithm competition"
      },
      {
        "paperId": "b393d1d1836c77d41b610a02c79d034e11b5d185",
        "title": "A Physician Advisory System for Chronic Heart Failure management based on knowledge patterns"
      },
      {
        "paperId": "5032c9285558ab9f3dc44f8e5ff54891a1f6bbd0",
        "title": "Optimal medication dosing from suboptimal clinical examples: A deep reinforcement learning approach"
      },
      {
        "paperId": "040c659c1331f4303cb900451df041d9660f044a",
        "title": "Fog-to-cloud Computing (F2C): The key technology enabler for dependable e-health services deployment"
      },
      {
        "paperId": "69e76e16740ed69f4dc55361a3d319ac2f1293dd",
        "title": "Asynchronous Methods for Deep Reinforcement Learning"
      },
      {
        "paperId": "846aedd869a00c09b40f1f1f35673cb22bc87490",
        "title": "Mastering the game of Go with deep neural networks and tree search"
      },
      {
        "paperId": "508b3797a2b8b862707767572cb7ad4fad200456",
        "title": "Fog Computing in Healthcare Internet of Things: A Case Study on ECG Feature Extraction"
      },
      {
        "paperId": "c434fce129f205a31871d51c0706b6aa9cfcb3b5",
        "title": "Doctor AI: Predicting Clinical Events via Recurrent Neural Networks"
      },
      {
        "paperId": "3b9732bb07dc99bde5e1f9f75251c6ea5039373e",
        "title": "Deep Reinforcement Learning with Double Q-Learning"
      },
      {
        "paperId": "2319a491378867c7049b3da055c5df60e1671158",
        "title": "Playing Atari with Deep Reinforcement Learning"
      },
      {
        "paperId": "70dd21ae645222af09e9d5b7a271b85db52c9ab1",
        "title": "A New, Evidence-based Estimate of Patient Harms Associated with Hospital Care"
      },
      {
        "paperId": "b7168ba6706836178b8ef4a850d281f33fc0bc89",
        "title": "Designing a pilot sequential multiple assignment randomized trial for developing an adaptive treatment strategy"
      },
      {
        "paperId": "644a079073969a92674f69483c4a85679d066545",
        "title": "Double Q-learning"
      },
      {
        "paperId": "7ac3423978584428d53605d2d9a8e814b1cb3f96",
        "title": "Activity-Aware Computing in Mobile Collaborative Working Environments"
      },
      {
        "paperId": "03e7be5137ad25ae6b3a8c6b3911563d0f1fd040",
        "title": "Implementing evidence-based medicine in general practice: a focus group based study"
      },
      {
        "paperId": "2dfc4e6847aef5e87a2c78eee4c6a58694c03b24",
        "title": "Multiple Model-Based Reinforcement Learning"
      },
      {
        "paperId": "5be3185153ed732ec58496e253808eefb20703dd",
        "title": "Making use of guidelines in clinical practice."
      },
      {
        "paperId": "90e38d07d19e994417bac2036e4941337e61741b",
        "title": "Reinforcement Learning: An Introduction, by Sutton, R.S. and Barto, A.G."
      },
      {
        "paperId": "cbc4032f61cb4c490abcd593f3ecc702ed7c9ea8",
        "title": "Hyperparameter Optimization for Deep Reinforcement Learning in Vehicle Energy Management"
      },
      {
        "paperId": null,
        "title": "'s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations"
      },
      {
        "paperId": "b06d433ea090194d071a1171ce57d1ccf8b7c010",
        "title": "Reinforcement Learning: State-of-the-Art"
      },
      {
        "paperId": null,
        "title": "Iteration \u2013 First Experiences with a Data Efficient Neural Reinforcement Learning Method"
      },
      {
        "paperId": "97efafdb4a3942ab3efba53ded7413199f79c054",
        "title": "Reinforcement Learning: An Introduction"
      },
      {
        "paperId": null,
        "title": "Dynamic programming and Markov process"
      },
      {
        "paperId": null,
        "title": "ii) Non-stationary targets: targets already change as the policy improves (better rewards so better Q--values)"
      },
      {
        "paperId": null,
        "title": "Advantages of NFQ"
      },
      {
        "paperId": null,
        "title": "i) Samples correlation: samples that occur consecutively are mostly very same which doesn \u2019 t add any diversity to the model"
      },
      {
        "paperId": null,
        "title": "The parameters for the supervised learning portion of the overall (RL) learning problem do not need to be modified"
      },
      {
        "paperId": null,
        "title": "Both the target network and the experience replay dramatically improve the performance of the algorithm"
      },
      {
        "paperId": null,
        "title": "Autonomous inverted autonomous helicopter flight via reinforcement learning"
      }
    ],
    "cited_by": [
      {
        "paperId": "4c6a14e573c0733b2ed2619acd890eba75ba91aa",
        "title": "Smart Resource Allocation in IoMT with UAV-Edge Federated Deep Learning"
      },
      {
        "paperId": "c62a102eed79475f2bdead0f574b52c5fa703352",
        "title": "Distributed security resource allocation strategy based on deep reinforcement learning"
      },
      {
        "paperId": "f5828fa2156dabd3ab3b6a502206e9347041b4e3",
        "title": "Analysis and Development Trends of Network Resource Allocation in Wireless Communications"
      },
      {
        "paperId": "b2da6f0306c623812f3f5149db89fa71a7c13fe9",
        "title": "Framework for deep reinforcement learning in Webots virtual environments"
      },
      {
        "paperId": "e88a534e006095cc1d1e2022ddbec665bd44fe65",
        "title": "Towards sustainable energy management: Leveraging explainable Artificial Intelligence for transparent and efficient decision-making"
      },
      {
        "paperId": "5b1cdbba50c556f0f06203a7f640e1c2a017b441",
        "title": "Machine Learning-Based Security Solutions for IoT Networks: A Comprehensive Survey"
      },
      {
        "paperId": "fd68eda1d22bcbd6813c59355d0196a3989f03b5",
        "title": "Reinforcement learning for healthcare operations management: methodological framework, recent developments, and future research directions"
      },
      {
        "paperId": "bc64b0f0ea0880700257bdeb2f8fd683f4a312b8",
        "title": "Enhancing the efficiency of lung cancer screening: predictive models utilizing deep learning from CT scans"
      },
      {
        "paperId": "0c096ee6e751dfd3c0ca5202ad662c74d7f8b134",
        "title": "Personalized learning path planning and optimization methods of vocational education combined with DQN"
      },
      {
        "paperId": "04355207dfe100661397123351b6421879f08e2a",
        "title": "Hierarchical Meta-Reinforcement Learning for Uncertainty-Aware Resource Allocation in C-V2X Networks"
      },
      {
        "paperId": "5026df32ed6a89c2110820f8fb1e7e7ec873b5ac",
        "title": "Application of deep reinforcement learning under biomechanical load optimization in warehouse site selection and material transportation path"
      },
      {
        "paperId": "0aa63e7faff7dce46acc98366444a61d395ebc24",
        "title": "AutoMR: A Universal Time Series Motion Recognition Pipeline"
      },
      {
        "paperId": "24c0843cf50918d6d206ee5d0ee2d039b4e12a80",
        "title": "Research on User Behavior Interest Feature Extraction and Accurate Advertising Recommendation Algorithm based on Deep Learning and Improved LDA Model"
      },
      {
        "paperId": "cd7b59bf4bd941d358178f50a3fbee134f3ba8dc",
        "title": "A Comprehensive Review of Existing Smart Summary Recommendation Model Enhanced Through RL and XAI Techniques"
      },
      {
        "paperId": "042eb56b4dccad39fef3c21acfa5e2a9acdebc1d",
        "title": "Revolutionizing cardiovascular health: integrating deep learning techniques for predictive analysis of personal key indicators in heart disease"
      },
      {
        "paperId": "b6bd15cb2357e200e5e7f21dfe79fe6b3260e2ca",
        "title": "Active Learning with Adaptive Reinforcement for Informative Sample Selection in Few-Shot Image Classification"
      },
      {
        "paperId": "594f22529624196f7e08865df4b93089f3016239",
        "title": "Real-Time Data Analysis: A Machine Learning Approach to Optimization"
      },
      {
        "paperId": "dfcb7791a958f6d57f5b860f493a472f26ae825f",
        "title": "Revolutionizing NPC Interaction: DQN with Huber Loss for Dynamic Behavior Modeling in Video Games"
      },
      {
        "paperId": "6fdc988d527727cf1c5d2bda0a00cf18f9f69406",
        "title": "Reinforcement Learning-Driven Active Few-Shot Learning Framework with Hyperparameter Optimization for Rice Pest Classification"
      },
      {
        "paperId": "5600291c13a9921119ce19d12f6c6eff6fa13a6b",
        "title": "Algorithm for Optimizing Financial Resource Allocation Scheduling Based on Deep Reinforcement Learning"
      },
      {
        "paperId": "593a5b69d17ef8d5b5674937da084ae5a96aaa3f",
        "title": "Improved prostate cancer diagnosis using a modified ResNet50-based deep learning architecture"
      },
      {
        "paperId": "3efa0420138b01df45bac0f1d77a9d28785e2215",
        "title": "Optimizing Patient Flow and Resource Allocation in Hospitals using AI"
      },
      {
        "paperId": "e229f311c928c4b95753d150c7d2be27e8835553",
        "title": "Personalized Healthcare Recommendations with Q-Learning Reinforcement Learning"
      },
      {
        "paperId": "e3b6199a29c7defb3e53a965aebcbf521a95fa4f",
        "title": "Toward interpretable credit scoring: integrating explainable artificial intelligence with deep learning for credit card default prediction"
      },
      {
        "paperId": "448182896d38cc532c4e4c7acd5a9a49e016f8d5",
        "title": "AI-Based Learning Techniques for Bladder Cancer Detection"
      },
      {
        "paperId": "fc888a65d74b771c37cdf95fd078dbe3c73a7dcd",
        "title": "The effect of consanguineous marriage on reading disability based on deep neural networks"
      },
      {
        "paperId": "2f0833cb9c89b23487bb8c3949993f1a1d570aed",
        "title": "A dueling DQN-based online resource allocation algorithm for cloud computing"
      },
      {
        "paperId": "27d57b81eeb3e49be2204b45d9543cb8cf39a6c9",
        "title": "Explainable Enhanced Recurrent Neural Network for lie detection using voice stress analysis"
      },
      {
        "paperId": "ed215c5d5904db5f4da849c4e30fba9723c041ef",
        "title": "Towards an Effective Service Allocation in Fog Computing"
      },
      {
        "paperId": "f2da8c1e7c2252c2cf0cf702dafa3bfb1e91d5e9",
        "title": "Machine learning-based solutions for resource management in fog computing"
      },
      {
        "paperId": "7d26347558fb7fcc4730b4e77b574d1d692f1221",
        "title": "An Intelligent Optimization Method for Wireless Communication Network Resources Based on Reinforcement Learning"
      },
      {
        "paperId": "f9de750a2b252941f29f45455bce6dd843e4879f",
        "title": "An improved fire detection approach based on YOLO-v8 for smart cities"
      },
      {
        "paperId": "bc776bdb55d1e96238bd89a59b8e5af9104f5ba9",
        "title": "Machine learning in detection and classification of leukemia using C-NMC_Leukemia"
      },
      {
        "paperId": "6902bd4c542a49e725416bbeffca0bcb6f82dcad",
        "title": "Exploring the effects of pandemics on transportation through correlations and deep learning techniques"
      },
      {
        "paperId": "166960b521129988f7a86915af01b64432cc545c",
        "title": "An overview of reinforcement learning techniques"
      },
      {
        "paperId": "c98e5e30efed38043bd6907429e75b432fc997cf",
        "title": "Stress monitoring using wearable sensors: IoT techniques in medical field"
      },
      {
        "paperId": "e87ea0e035ecd4139b64cc34a4178cf39ef4c659",
        "title": "A2M-LEUK: attention-augmented algorithm for blood cancer detection in children"
      },
      {
        "paperId": "00fec7edf909505a95339a60f03a9977667bcf64",
        "title": "Crop yield prediction algorithm (CYPA) in precision agriculture based on IoT techniques and climate changes"
      },
      {
        "paperId": "3e1a970531cd831c00a6275f75d135b4d1d55a44",
        "title": "DRLBTS: deep reinforcement learning-aware blockchain-based healthcare system"
      },
      {
        "paperId": "7c0b356f8f49a39227ad7e574f6d06f198e581dd",
        "title": "Real-time facial emotion recognition system among children with autism based on deep learning and IoT"
      },
      {
        "paperId": "559b0a96e085a57880a313a2a57e4f897585e450",
        "title": "A Review of Resource Management in Fog Computing: Machine Learning Perspective"
      },
      {
        "paperId": "d2a5369157a44b3fd121f28edf93ba65e95e774e",
        "title": "Reinforcement Learning for Dynamic and Predictive CPU Resource Management in Cloud Computing"
      }
    ],
    "score": 14.0
  },
  {
    "id": "65587d4927fccc30788d3dfc9b639567721ff393",
    "title": "Rethinking Reinforcement Learning for Recommendation: A Prompt Perspective",
    "authors": [
      "Xin Xin",
      "Tiago Pimentel",
      "Alexandros Karatzoglou",
      "Pengjie Ren",
      "Konstantina Christakopoulou",
      "Z. Ren"
    ],
    "year": 2022,
    "citationCount": 41,
    "abstract": "Modern recommender systems aim to improve user experience. As reinforcement learning (RL) naturally fits this objective---maximizing an user's reward per session---it has become an emerging topic in recommender systems. Developing RL-based recommendation methods, however, is not trivial due to the offline training challenge. Specifically, the keystone of traditional RL is to train an agent with large amounts of online exploration making lots of 'errors' in the process. In the recommendation setting, though, we cannot afford the price of making 'errors' online. As a result, the agent needs to be trained through offline historical implicit feedback, collected under different recommendation policies; traditional RL algorithms may lead to sub-optimal policies under these offline training settings. Here we propose a new learning paradigm---namely Prompt-Based Reinforcement Learning (PRL)---for the offline training of RL-based recommendation agents. While traditional RL algorithms attempt to map state-action input pairs to their expected rewards (e.g., Q-values), PRL directly infers actions (i.e., recommended items) from state-reward inputs. In short, the agents are trained to predict a recommended item given the prior interactions and an observed reward value---with simple supervised learning. At deployment time, this historical (training) data acts as a knowledge base, while the state-reward pairs are used as a prompt. The agents are thus used to answer the question: Which item should be recommended given the prior interactions & the prompted reward value? We implement PRL with four notable recommendation models and conduct experiments on two real-world e-commerce datasets. Experimental results demonstrate the superior performance of our proposed methods.",
    "url": "https://www.semanticscholar.org/paper/65587d4927fccc30788d3dfc9b639567721ff393",
    "pdf_url": "https://arxiv.org/pdf/2206.07353.pdf",
    "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
    "publicationDate": "2022-06-15",
    "externalIds": {
      "ArXiv": "2206.07353",
      "DBLP": "conf/sigir/XinPKRCR22",
      "DOI": "10.1145/3477495.3531714",
      "CorpusId": 249674368
    },
    "references": [
      {
        "paperId": "05482ebbd8ab8771b911b4c1b707bbed00d229c5",
        "title": "Supervised Advantage Actor-Critic for Recommender Systems"
      },
      {
        "paperId": "c0df9c92935823fe825e9a0b8305a76cb20419b2",
        "title": "Choosing the Best of Both Worlds: Diverse and Novel Recommendations through Multi-Objective Reinforcement Learning"
      },
      {
        "paperId": "cfd2b572196324561362cca8061084320a7cd23b",
        "title": "A Survey of Deep Reinforcement Learning in Recommender Systems: A Systematic Review and Future Directions"
      },
      {
        "paperId": "28692beece311a90f5fa1ca2ec9d0c2ce293d069",
        "title": "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"
      },
      {
        "paperId": "f864d4d2267abba15eb43db54f58286aef78292b",
        "title": "Offline Reinforcement Learning as One Big Sequence Modeling Problem"
      },
      {
        "paperId": "c1ad5f9b32d80f1c65d67894e5b8c2fdf0ae4500",
        "title": "Decision Transformer: Reinforcement Learning via Sequence Modeling"
      },
      {
        "paperId": "5685abf9e7bb2c16449ae1eb181051e503602a55",
        "title": "Reinforcement Learning based Recommender Systems: A Survey"
      },
      {
        "paperId": "9779f919685adee936835eff46914d3d69d9ccb9",
        "title": "Bias and Debias in Recommender System: A Survey and Future Directions"
      },
      {
        "paperId": "f7cd8231d45df354cbc42105991c84e86c6940af",
        "title": "Keeping Dataset Biases out of the Simulation: A Debiased Simulator for Reinforcement Learning based Recommender Systems"
      },
      {
        "paperId": "8d26dc21338cbfa5f68944b6ec19f00b2229989c",
        "title": "Self-Supervised Reinforcement Learning for Recommender Systems"
      },
      {
        "paperId": "28db20a81eec74a50204686c3cf796c42a020d2e",
        "title": "Conservative Q-Learning for Offline Reinforcement Learning"
      },
      {
        "paperId": "7a7a7847041e7b25febb1491d65d842a6c65927e",
        "title": "Training Agents using Upside-Down Reinforcement Learning"
      },
      {
        "paperId": "72973e49f453f678eb0b79b5fa5311b158f3909d",
        "title": "Reinforcement Learning Upside Down: Don't Predict Rewards - Just Map Them to Actions"
      },
      {
        "paperId": "59a916cdc943f0282908e6f3fa0360f4c5fb78d0",
        "title": "Stabilizing Transformers for Reinforcement Learning"
      },
      {
        "paperId": "ecba7d8d3981b88f6ae29ba8905c9229e1c8779a",
        "title": "SlateQ: A Tractable Decomposition for Reinforcement Learning with Recommendation Sets"
      },
      {
        "paperId": "690edf44e8739fd80bdfb76f40c9a4a222f3bba8",
        "title": "BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer"
      },
      {
        "paperId": "d48312697f84bc802272d1482762a0415664b283",
        "title": "Reinforcement Learning to Optimize Long-term User Engagement in Recommender Systems"
      },
      {
        "paperId": "16481307a862ddae514045b13b944ca043c36a4b",
        "title": "Generative Adversarial User Model for Reinforcement Learning Based Recommendation System"
      },
      {
        "paperId": "7cae81b4c706f4cc2265730d5791d46d884e3837",
        "title": "\"Deep reinforcement learning for search, recommendation, and online advertising: a survey\" by Xiangyu Zhao, Long Xia, Jiliang Tang, and Dawei Yin with Martin Vesely as coordinator"
      },
      {
        "paperId": "5285cb8faada5de8a92a47622950f6cfd476ac1d",
        "title": "Off-Policy Deep Reinforcement Learning without Exploration"
      },
      {
        "paperId": "f76ac3753cd0b5d584ad5eb04819a4dddba82891",
        "title": "Top-K Off-Policy Correction for a REINFORCE Recommender System"
      },
      {
        "paperId": "97faeefa771e8cc8e55159e2bd03e6f5eef249a8",
        "title": "Self-Attentive Sequential Recommendation"
      },
      {
        "paperId": "5babbf2ed9f6e36b83ed246927b270db320fe866",
        "title": "A Simple Convolutional Generative Network for Next Item Recommendation"
      },
      {
        "paperId": "7da41fd9949af81b84fb983524f850f7d47918fa",
        "title": "Reinforcement Learning to Rank in E-Commerce Search Engine: Formalization, Analysis, and Application"
      },
      {
        "paperId": "75a927501749c2cbc0e19a58f798f04de59df64a",
        "title": "Personalized Top-N Sequential Recommendation via Convolutional Sequence Embedding"
      },
      {
        "paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
        "title": "Attention is All you Need"
      },
      {
        "paperId": "641165c959554d8f03314778bd6dfb581d9a469e",
        "title": "Generalization and Equilibrium in Generative Adversarial Nets (GANs)"
      },
      {
        "paperId": "fea7e2bcbe852bcb33ce70a4d57664e954f0e82a",
        "title": "Fusing Similarity Models with Markov Chains for Sparse Sequential Recommendation"
      },
      {
        "paperId": "97fb4e3d45bb098e27e0071448b6152217bd35a5",
        "title": "Layer Normalization"
      },
      {
        "paperId": "4ab53de69372ec2cd2d90c126b6a100165dc8ed1",
        "title": "Generative Adversarial Imitation Learning"
      },
      {
        "paperId": "dc3e905bfb27d21675ee1720413e007b014b37d3",
        "title": "Safe and Efficient Off-Policy Reinforcement Learning"
      },
      {
        "paperId": "846aedd869a00c09b40f1f1f35673cb22bc87490",
        "title": "Mastering the game of Go with deep neural networks and tree search"
      },
      {
        "paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d",
        "title": "Deep Residual Learning for Image Recognition"
      },
      {
        "paperId": "e0021d61c2ab1334bc725852edd44597f4c65dff",
        "title": "Session-based Recommendations with Recurrent Neural Networks"
      },
      {
        "paperId": "340f48901f72278f6bf78a04ee5b01df208cc508",
        "title": "Human-level control through deep reinforcement learning"
      },
      {
        "paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8",
        "title": "Adam: A Method for Stochastic Optimization"
      },
      {
        "paperId": "bd07b71c84bdd0950e4f5bfcabf96f13853b40aa",
        "title": "General factorization framework for context-aware recommendations"
      },
      {
        "paperId": "df93596d4ed71d2863532c063c4c693711216abf",
        "title": "Factorization Machines"
      },
      {
        "paperId": "644a079073969a92674f69483c4a85679d066545",
        "title": "Double Q-learning"
      },
      {
        "paperId": "50d85cb114f7c5e779a6772f2931e77dddd54a5e",
        "title": "Factorizing personalized Markov chains for next-basket recommendation"
      },
      {
        "paperId": "d4bbcc842f22547eaf5884251eaa68251895dccb",
        "title": "Matrix Factorization Techniques for Recommender Systems"
      },
      {
        "paperId": "c1aa28159e768b75d0f4637a71d20da02efe1ef2",
        "title": "Collaborative filtering with temporal dynamics"
      },
      {
        "paperId": "8490234d79b47e459824dcf87c1e288211a3c964",
        "title": "Cumulated gain-based evaluation of IR techniques"
      },
      {
        "paperId": "2d1cfc9e81fb159967c2be8446a8e3e7b50fe36b",
        "title": "An MDP-Based Recommender System"
      },
      {
        "paperId": "d82829c9fd9cfba8a44efe5ba048d3332a1671fc",
        "title": "Dynamic Programming"
      },
      {
        "paperId": "e25221b4c472c4337383341f6b2c9375e86709af",
        "title": "Matrix Factorization Techniques for Recommender Systems"
      },
      {
        "paperId": "4c915c1eecb217c123a36dc6d3ce52d12c742614",
        "title": "Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning"
      },
      {
        "paperId": "ac4af1df88e178386d782705acc159eaa0c3904a",
        "title": "Actor-Critic Algorithms"
      }
    ],
    "cited_by": [
      {
        "paperId": "3163cb4677408c97e42b547c15ee2974675cb4a4",
        "title": "Beyond Accuracy: Decision Transformers for Reward-Driven Multi-Objective Recommendations"
      },
      {
        "paperId": "1fd7057dc545c32a2e5ed93d55e4ef763d5e008d",
        "title": "KLAN: Kuaishou Landing-page Adaptive Navigator"
      },
      {
        "paperId": "7feeaeffd7adafda550e4ae637804e2f814b9c9d",
        "title": "PinRec: Outcome-Conditioned, Multi-Token Generative Retrieval for Industry-Scale Recommendation Systems"
      },
      {
        "paperId": "894b734d6d7b2434639641e98697170cd97ca904",
        "title": "Rec-R1: Bridging Generative Large Language Models and User-Centric Recommendation Systems via Reinforcement Learning"
      },
      {
        "paperId": "d335b1ab64168530c9ecdf07fcbbf7ec87bea414",
        "title": "Generative Models in Decision Making: A Survey"
      },
      {
        "paperId": "5d90e81e0583db7876494566f820b6810730526d",
        "title": "An efficient continuous control perspective for reinforcement-learning-based sequential recommendation"
      },
      {
        "paperId": "d4f70d5de33511805a01feac82d4bacd7e1b6a03",
        "title": "Value Function Decomposition in Markov Recommendation Process"
      },
      {
        "paperId": "47387abd3fe650391ee84f8189e9c655c3dd067f",
        "title": "Large Language Model driven Policy Exploration for Recommender Systems"
      },
      {
        "paperId": "814d9c2d100511e04ae5c437c3e79f3868cd03c2",
        "title": "Time-Frequency Sensitive Prompt Tuning Framework for Session-based Recommendation"
      },
      {
        "paperId": "418069287b0ce0e15bce82fd359b34b1a3c70218",
        "title": "Goal-Conditioned Supervised Learning for Multi-Objective Recommendation"
      },
      {
        "paperId": "2a919a8b5b5d2be610a2144721a69f9abde9dd2a",
        "title": "RLT4Rec: Reinforcement Learning Transformer for User Cold Start and Item Recommendation"
      },
      {
        "paperId": "21744db6c5dc131f0a83edb4b1b6f017fe7cbd94",
        "title": "Unified Parameter-Efficient Unlearning for LLMs"
      },
      {
        "paperId": "9f2791148b739a58a1b795b59d1a2fb5661f65a0",
        "title": "User Response Modeling in Recommender Systems: A Survey"
      },
      {
        "paperId": "4329f0ac454a7428b15868b272d15af131b5313d",
        "title": "Sparks of Surprise: Multi-objective Recommendations with Hierarchical Decision Transformers for Diversity, Novelty, and Serendipity"
      },
      {
        "paperId": "87fa15f550b6db7f97dabea75632b33f6419218f",
        "title": "Achieving EEG-based depression recognition using Decentralized-Centralized structure"
      },
      {
        "paperId": "d502c26bef3d887efceed982f1839a9e0900b24f",
        "title": "The Research on Intelligent News Advertisement Recommendation Algorithm Based on Prompt Learning in End-to-End Large Language Model Architecture"
      },
      {
        "paperId": "6cbbe20c5a78de6ee27cd254f6695e5daf0ce553",
        "title": "Reformulating Conversational Recommender Systems as Tri-Phase Offline Policy Learning"
      },
      {
        "paperId": "2b6f5774c159d470debe2e333072687839215fed",
        "title": "Rethinking Offline Reinforcement Learning for Sequential Recommendation from A Pair-Wise Q-Learning Perspective"
      },
      {
        "paperId": "4030f62bc5f26a6c3454ca61b444be621468b8a2",
        "title": "Understanding Language Modeling Paradigm Adaptations in Recommender Systems: Lessons Learned and Open Challenges"
      },
      {
        "paperId": "f6b373b7a4b9fb31052df0f35987edabd2f93ce6",
        "title": "Reinforcement Learning-based Recommender Systems with Large Language Models for State Reward and Action Modeling"
      },
      {
        "paperId": "09a7416c23afc6eef292c8e523f269a256e5bde2",
        "title": "Improving recommendation diversity without retraining from scratch"
      },
      {
        "paperId": "11605cb7ef185dfa65758866645440ad9a292ff8",
        "title": "PromptMM: Multi-Modal Knowledge Distillation for Recommendation with Prompt-Tuning"
      },
      {
        "paperId": "366628ddd04a281f46d91e1bf91d76cc4610db1a",
        "title": "Future Impact Decomposition in Request-level Recommendations"
      },
      {
        "paperId": "4758630b6673316c951b276d3db46380d6b6aba5",
        "title": "MCRPL: A Pretrain, Prompt, and Fine-tune Paradigm for Non-overlapping Many-to-one Cross-domain Recommendation"
      },
      {
        "paperId": "bd0d7fa39d259347f21282a30998c80ef88e43cb",
        "title": "SOAC: Supervised Off-Policy Actor-Critic for Recommender Systems"
      },
      {
        "paperId": "78486d4d1eecfac46e4a24efd02d5aefae84a9b9",
        "title": "SARDINE: Simulator for Automated Recommendation in Dynamic and Interactive Environments"
      },
      {
        "paperId": "99335b0ca039ea2f59ccbe8bd182acb3e5c705d6",
        "title": "A Social-aware Gaussian Pre-trained Model for Effective Cold-start Recommendation"
      },
      {
        "paperId": "d3db65aec6bc2e1bdb101658ed854ec972829032",
        "title": "Model-enhanced Contrastive Reinforcement Learning for Sequential Recommendation"
      },
      {
        "paperId": "e776b35d50f07998e0c4fdca6bfe1f21c364fddf",
        "title": "AutoTransfer: Instance Transfer for Cross-Domain Recommendations"
      },
      {
        "paperId": "14cbf1cfd5dd4b451bfd6ef89e2bc277799df9bd",
        "title": "Alleviating Matthew Effect of Offline Reinforcement Learning in Interactive Recommendation"
      },
      {
        "paperId": "f5de5cbb5563ed5633421d17d894573828c13b2d",
        "title": "Exploring Adapter-based Transfer Learning for Recommender Systems: Empirical Studies and Practical Insights"
      },
      {
        "paperId": "dadd173d7dde127640020938277a2f590729bff6",
        "title": "Prediction then Correction: An Abductive Prediction Correction Method for Sequential Recommendation"
      },
      {
        "paperId": "da5509599d7d5ab4336d524b65467f561d18a887",
        "title": "Automated Prompting for Non-Overlapping Cross-Domain Sequential Recommendation"
      },
      {
        "paperId": "5829b645dca5aa5818b9e8974f8d41dc59fe2268",
        "title": "Research on Intelligent Recommendation Technology for Complex Tasks"
      },
      {
        "paperId": "a66bb19c36db1a2f400b4c8f90b5da464b6c9e80",
        "title": "User Retention-oriented Recommendation with Decision Transformer"
      },
      {
        "paperId": "07a70bf4ace20e24416851d839e61a777e461890",
        "title": "Denoising and Prompt-Tuning for Multi-Behavior Recommendation"
      },
      {
        "paperId": "c589a3420ba335a05c248f525ea3c6e90215e42b",
        "title": "Pre-train, Prompt, and Recommendation: A Comprehensive Survey of Language Modeling Paradigm Adaptations in Recommender Systems"
      },
      {
        "paperId": "1cd023a46f1e22a940891641aa696c0c711fafd6",
        "title": "Recommender Systems: A Primer"
      },
      {
        "paperId": "2de92237a4e57ec060db92574b48d8b5df973844",
        "title": "Local Policy Improvement for Recommender Systems"
      },
      {
        "paperId": "fe0b2ebe2053dfaf61f78c4b7f9d715a0b3a746d",
        "title": "Personalization for Web-based Services using Offline Reinforcement Learning"
      },
      {
        "paperId": "08c8570788035485218f218c6f02760ac2ddd96e",
        "title": "On the Unexpected Effectiveness of Reinforcement Learning for Sequential Recommendation"
      }
    ],
    "score": 13.666666666666666
  },
  {
    "id": "2fd42844445ec644c2c44c093c3522c08b59cb45",
    "title": "Event-Triggered Model Predictive Control With Deep Reinforcement Learning for Autonomous Driving",
    "authors": [
      "Fengying Dang",
      "Dong Chen",
      "J. Chen",
      "Zhaojian Li"
    ],
    "year": 2022,
    "citationCount": 39,
    "abstract": "Event-triggered model predictive control (eMPC) is a popular optimal control method with an aim to alleviate the computation and/or communication burden of MPC. However, it generally requires a priori knowledge of the closed-loop system behavior along with the communication characteristics for designing the event-trigger policy. This paper attempts to solve this challenge by proposing an efficient eMPC framework and demonstrates successful implementation of this framework on the autonomous vehicle path following. First of all, a model-free reinforcement learning (RL) agent is used to learn the optimal event-trigger policy without the need for a complete dynamical system and communication knowledge in this framework. Furthermore, techniques including prioritized experience replay (PER) buffer and long short-term memory (LSTM) are employed to foster exploration and improve training efficiency. In this paper, we use the proposed framework with three deep RL algorithms, i.e., Double Q-learning (DDQN), Proximal Policy Optimization (PPO), and Soft Actor-Critic (SAC), to solve this problem. Results show that all three deep RL-based eMPC (deep-RL-eMPC) can achieve better evaluation performance than the conventional threshold-based and previous linear Q-based approach in the autonomous path following. In particular, PPO-eMPC with LSTM and DDQN-eMPC with PER and LSTM obtain a superior balance between the closed-loop control performance and event-trigger frequency.",
    "url": "https://www.semanticscholar.org/paper/2fd42844445ec644c2c44c093c3522c08b59cb45",
    "pdf_url": "https://arxiv.org/pdf/2208.10302.pdf",
    "venue": "IEEE Transactions on Intelligent Vehicles",
    "publicationDate": "2022-08-22",
    "externalIds": {
      "DBLP": "journals/tiv/DangCCL24",
      "ArXiv": "2208.10302",
      "DOI": "10.1109/TIV.2023.3329785",
      "CorpusId": 251719677
    },
    "references": [
      {
        "paperId": "82026d329710067805683adf469396120e65cad4",
        "title": "A Novel Event-Triggered Torque Vectoring Control for Improving Lateral Stability and Communication Resource Consumption of Electric Vehicles"
      },
      {
        "paperId": "e8d3133561c0e17a14c7f21c92c43e35c75f30e8",
        "title": "Experimental Validation of Event-Triggered Model Predictive Control for Autonomous Vehicle Path Tracking"
      },
      {
        "paperId": "197430de216b89282681d4dac981bf27bbca633a",
        "title": "Event-Triggered Deep Reinforcement Learning Using Parallel Control: A Case Study in Autonomous Driving"
      },
      {
        "paperId": "1c6fbf5c76aee77b539dc3f50991d7ac6c8356e8",
        "title": "Motion Planning for Autonomous Driving: The State of the Art and Future Perspectives"
      },
      {
        "paperId": "80985e5b3bdcf5dfaf88550d52a08cd08ea9e65a",
        "title": "Event-Triggered Multi-Lane Fusion Control for 2-D Vehicle Platoon Systems With Distance Constraints"
      },
      {
        "paperId": "e6640addc517891ba9de59d13d782b583d537931",
        "title": "A machine-learning-based event-triggered model predictive control for building energy management"
      },
      {
        "paperId": "9e24ca96b87c5b159f0c7df144cc4a7288fd6da4",
        "title": "Autonomous Vehicle Fleets for Public Transport: Scenarios and Comparisons"
      },
      {
        "paperId": "86c7684632d3567ae93de3cd8191583a43d059ff",
        "title": "Reinforcement Learning-based Event-Triggered Model Predictive Control for Autonomous Vehicle Path Following"
      },
      {
        "paperId": "af331e7342014e27aa5d15e6227cd3abe85f0fe8",
        "title": "Verification and Validation of Intelligent Vehicles: Objectives and Efforts From China"
      },
      {
        "paperId": "74918e3303fbe5abc3640c3ad5f5c41935160f5c",
        "title": "Event-Triggered Near-Optimal Control for Unknown Discrete-Time Nonlinear Systems Using Parallel Control"
      },
      {
        "paperId": "69e9357a393cfdef24f9e2445debc4097880c423",
        "title": "Event-Triggered Model Predictive Control for Autonomous Vehicle with Rear Steering"
      },
      {
        "paperId": "16d0ed1eabe8a6dd256486c36121cccad3fcf3a1",
        "title": "Event-Triggered Model Predictive Adaptive Dynamic Programming for Road Intersection Path Planning of Unmanned Ground Vehicle"
      },
      {
        "paperId": "b42ed5eec55a9bb05012a58ca911e5e1677ad979",
        "title": "Comparison of Event-Triggered Model Predictive Control for Autonomous Vehicle Path Tracking"
      },
      {
        "paperId": "1fde831e4ff244ec36ec3e9facc8c0ece6c2f45c",
        "title": "A Review on the Application of the MPC Technology in Wind Power Control of Wind Farms"
      },
      {
        "paperId": "1752201e00c96e33aa148643333278d70d480498",
        "title": "Real-Time Implementation of Randomized Model Predictive Control for Autonomous Driving"
      },
      {
        "paperId": "dd3d6b81b911085b0b3cd6f030d6d3107521673d",
        "title": "MPC-Based Cooperative Control Strategy of Path Planning and Trajectory Tracking for Intelligent Vehicles"
      },
      {
        "paperId": "ef7999bdd4366582b76a6b076f1155e8efdef610",
        "title": "Optimization of the Model Predictive Control Update Interval Using Reinforcement Learning"
      },
      {
        "paperId": "fdf3708c708fd96f434f85c5b665dd3a69c5532a",
        "title": "Machine Learning in Event-Triggered Control: Recent Advances and Open Issues"
      },
      {
        "paperId": "a358c7a7a0afc7b7a900525249385e6bd1056e62",
        "title": "Autonomous boat driving system using sample\u2010efficient model predictive control\u2010based reinforcement learning approach"
      },
      {
        "paperId": "517498898024107a75f5a4ea84f4129e300af603",
        "title": "Revisiting Fundamentals of Experience Replay"
      },
      {
        "paperId": "61d181cbd946447f48131b8296b6df6d68bbb4c0",
        "title": "Computationally efficient stochastic MPC: a probabilistic scaling approach"
      },
      {
        "paperId": "94d288cd8207292bf61505d5deca0999cf9a1261",
        "title": "Sample-and-computation-efficient Probabilistic Model Predictive Control with Random Features"
      },
      {
        "paperId": "a2f48a5efee42b3a630e746c6e9e43374a163e02",
        "title": "Robust Platoon Control in Mixed Traffic Flow Based on Tube Model Predictive Control"
      },
      {
        "paperId": "2bd20dc3d65ee4a81b38fc8fad18792ae4f319ba",
        "title": "Practical Reinforcement Learning For MPC: Learning from sparse objectives in under an hour on a real robot"
      },
      {
        "paperId": "cfe4f87325a02cc80f5500883fc816b7b82e5fb3",
        "title": "Robust Event-Triggered Model Predictive Control for Multiple High-Speed Trains With Switching Topologies"
      },
      {
        "paperId": "10b51cb657843287e0db148497469e2d9da0a78f",
        "title": "Reinforcement Learning Boat Autopilot: A Sample-efficient and Model Predictive Control based Approach"
      },
      {
        "paperId": "e9a7e00cfc36e4143317bd0200b13af5e23e157e",
        "title": "Comparison of Deep Reinforcement Learning and Model Predictive Control for Adaptive Cruise Control"
      },
      {
        "paperId": "0a0866ec7180bbf87b1c87ed48bd4fa00574b814",
        "title": "Soft Actor-Critic for Discrete Action Settings"
      },
      {
        "paperId": "16504bf0357d6a17bce49aa28bdaf7b8531156ed",
        "title": "Deep Value Model Predictive Control"
      },
      {
        "paperId": "3ede407693ca91ffcd021792f0baf395dfa724f6",
        "title": "Data-Driven Model Predictive Control for Trajectory Tracking With a Robotic Arm"
      },
      {
        "paperId": "77b3f96aeb3a74e7bc864695904d5f35c8dea1b9",
        "title": "Learning-Based Model Predictive Control for Autonomous Racing"
      },
      {
        "paperId": "e7584063da8d271c51a4edc9086903b82b8b1fdb",
        "title": "Event-Triggered Model Predictive Control With a Statistical Learning"
      },
      {
        "paperId": "2a9bcda7bec2ccd865e0455dce5d565ed7d3790b",
        "title": "Computationally efficient MPC for path following of underactuated marine vessels using projection neural network"
      },
      {
        "paperId": "d3daaac4053455b370b7f0ed9c45cf99129b21cc",
        "title": "Event-triggered Pulse Control with Model Learning (if Necessary)"
      },
      {
        "paperId": "d9ad92812d61709a9bf35b09078361d8bffd3f7a",
        "title": "Autonomous Tissue Manipulation via Surgical Robot Using Learning Based Model Predictive Control"
      },
      {
        "paperId": "b1914c912dea62703856d89fe3724675a6139b71",
        "title": "Microscopic Traffic Simulation using SUMO"
      },
      {
        "paperId": "f0f55458687fdc9460bc0ac4d4e588bd9d2bcabe",
        "title": "Deep Reinforcement Learning for Event-Triggered Control"
      },
      {
        "paperId": "c526bd4a573c052087f6bc769cd86c341dcd77c8",
        "title": "Model predictive control and its application in agriculture: A review"
      },
      {
        "paperId": "60540886eeede87f818e58b2aa246ad2fbdd3bdc",
        "title": "Event-Triggered Fault Detector and Controller Coordinated Design of Fuzzy Systems"
      },
      {
        "paperId": "944524bb709e83a1b33de7a3f694a13388823736",
        "title": "An Overview of Nature-Inspired, Conventional, and Hybrid Methods of Autonomous Vehicle Path Planning"
      },
      {
        "paperId": "512bd5e55ca59dcb2a6dbedf89a8b30585cf6dbf",
        "title": "Event-based Observer and MPC with Disturbance Attenuation using ERM Learning"
      },
      {
        "paperId": "06c52b2394167800f4ac49ed166b5cad5a3753ac",
        "title": "Model Predictive Control (MPC) for Enhancing Building and HVAC System Energy Efficiency: Problem Formulation, Applications and Opportunities"
      },
      {
        "paperId": "811df72e210e20de99719539505da54762a11c6d",
        "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor"
      },
      {
        "paperId": "0eb6f12c9bcfe0bf7e0e326799d86804c66382be",
        "title": "Spatial Model Predictive Control for Smooth and Accurate Steering of an Autonomous Truck"
      },
      {
        "paperId": "ebf0615fc4d98cf1dbe527c79146ce1e50dce9af",
        "title": "CARLA: An Open Urban Driving Simulator"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "09cd5d3634d84d528d16c4bcc89c6fd5b883c7cc",
        "title": "Data-Efficient Reinforcement Learning with Probabilistic Model Predictive Control"
      },
      {
        "paperId": "9fd901b232177b6bc92408ec59adce534552e5c3",
        "title": "Path planning for autonomous vehicles using model predictive control"
      },
      {
        "paperId": "b3035f6b00ff01bd582d70e30fe54737b2df5472",
        "title": "Cautious Model Predictive Control Using Gaussian Process Regression"
      },
      {
        "paperId": "d375fe2f4970ac52ac92f1c1d9a5b500004a1e3a",
        "title": "Robust Event-Triggered MPC With Guaranteed Asymptotic Bound and Average Sampling Rate"
      },
      {
        "paperId": "2a7603c7dcecc9c820053a41c6f7e1b6bb3a8a69",
        "title": "Introduction to the History of Computing: A Computing History Primer"
      },
      {
        "paperId": "306bea14e25975ba0b1fcb4d28b423b1c515ad5d",
        "title": "Learning\u2010based Nonlinear Model Predictive Control to Improve Vision\u2010based Mobile Robot Path Tracking"
      },
      {
        "paperId": "c6170fa90d3b2efede5a2e1660cb23e1c824f2ca",
        "title": "Prioritized Experience Replay"
      },
      {
        "paperId": "29d814cd7ab1107acffdf730d5fabeb917335681",
        "title": "ACIS: An Improved Actor-Critic Method for POMDPs with Internal State"
      },
      {
        "paperId": "29f37b6c93715ddf47a10b922dae4ce9cedf8547",
        "title": "Event-Based Robust Sampled-Data Model Predictive Control: A Non-Monotonic Lyapunov Function Approach"
      },
      {
        "paperId": "3c75174706613a57e26c0c50979da752ff31b564",
        "title": "Kinematic and dynamic vehicle models for autonomous driving control design"
      },
      {
        "paperId": "340f48901f72278f6bf78a04ee5b01df208cc508",
        "title": "Human-level control through deep reinforcement learning"
      },
      {
        "paperId": "449532187c94af3dd3aa55e16d2c50f7854d2199",
        "title": "Trust Region Policy Optimization"
      },
      {
        "paperId": "912e4089b86388d754ba10cfb036801b44c0c0f6",
        "title": "Event-triggered robust model predictive control of continuous-time nonlinear systems"
      },
      {
        "paperId": "2319a491378867c7049b3da055c5df60e1671158",
        "title": "Playing Atari with Deep Reinforcement Learning"
      },
      {
        "paperId": "3bb0531d31bf0addf3d4d7e02bd5e92105e2f383",
        "title": "Novel event-triggered strategies for Model Predictive Controllers"
      },
      {
        "paperId": "0e8c927d9c2c46b87816a0f8b7b8b17ed1263e9c",
        "title": "Path Planning for Autonomous Vehicles in Unknown Semi-structured Environments"
      },
      {
        "paperId": "c5a3aa8bac30543559ed2ca3377bc73b83284713",
        "title": "Model Predictive Control Tuning Methods: A Review"
      },
      {
        "paperId": "ede44602184e414834d50ccac65ef8814b40d8da",
        "title": "Event-Triggered Real-Time Scheduling of Stabilizing Control Tasks"
      },
      {
        "paperId": "5c04727659d9fd99af3799f5b9fe81884fd409c7",
        "title": "Vehicle dynamics and control"
      },
      {
        "paperId": "085fb3acabcbf80ef1bf47daec50d246475b072b",
        "title": "Infinite-Horizon Policy-Gradient Estimation"
      },
      {
        "paperId": "2e9d221c206e9503ceb452302d68d10e293f2a10",
        "title": "Long Short-Term Memory"
      },
      {
        "paperId": "9cd8193a66cf53143cbba6ccb0c7b9c2ebf2452b",
        "title": "Self-improving reactive agents based on reinforcement learning, planning and teaching"
      },
      {
        "paperId": null,
        "title": "\u201cTracking of uncertain robotic ma- nipulatorsusingevent-triggeredmodelpredictivecontrolwithlearningter-minalcost,\u201d"
      },
      {
        "paperId": null,
        "title": "\u201cCommunication-constrained active sus-pensioncontrolfornetworkedin-wheelmotor-drivenelectricvehicleswithdynamicdampers,\u201d"
      },
      {
        "paperId": "6fb5ebb04c78820a6faab670d3e076082585c481",
        "title": "Event-Triggered Reinforcement Learning; An Application to Buildings' Micro-Climate Control"
      },
      {
        "paperId": null,
        "title": "\u201cModelpredictivecontrolofnonlinearsystem withevent-triggeredparametricidenti\ufb01cation,\u201din"
      },
      {
        "paperId": null,
        "title": "American Control Conference (ACC)"
      },
      {
        "paperId": null,
        "title": "\u201cCooperativeadaptivecruisecontrol ofvehiclesusingaresource-ef\ufb01cientcommunicationmechanism,\u201d"
      },
      {
        "paperId": "28c08217ef5ab4837724660c7fdfe5581a3c580e",
        "title": "Learning-Based Robust Model Predictive Control with State-Dependent Uncertainty"
      },
      {
        "paperId": null,
        "title": "\u201cAhybridmotionplanning frameworkforautonomousdrivinginmixedtraf\ufb01c\ufb02ow,\u201d"
      },
      {
        "paperId": null,
        "title": "Michigan State University, East Lansing, MI 48824 USA"
      },
      {
        "paperId": null,
        "title": "\u201cA path planning algorithm for autonomous \ufb02ying vehicles in cross-country environments withanovelTF-RRT*method,\u201d"
      }
    ],
    "cited_by": [
      {
        "paperId": "943142ab0db23e8e5813018b9542bd6baa2b1cb8",
        "title": "Safe Reinforcement Learning for Event-Triggered Control of Automated Vehicles With Uncertainty"
      },
      {
        "paperId": "abcf239c3e3d7c0945d70fd586f41bc131775021",
        "title": "Learning Event\u2010Triggered Model Predictive Control From Data With Hierarchical Reinforcement Learning"
      },
      {
        "paperId": "652fa3bd5fd2970f08776f1f9936ea5f2abef17d",
        "title": "Self-Triggered Stochastic MPC With Adaptive Prediction Horizon for Cloud-Based Connected Vehicles Subject to Chance Constraints"
      },
      {
        "paperId": "c4eaa0cac52110a3ff6e0648ed1212527bfb84be",
        "title": "Road-Adaptive Precise Path Tracking Based on Reinforcement Learning Method"
      },
      {
        "paperId": "765421d902d91e875fc22b32cb23f9575f47d619",
        "title": "CADCO: An Adaptive Dynamic Cloud-fog Computing Offloading Method for complex dependency tasks of IoT"
      },
      {
        "paperId": "390dd2089a483386968ee77add54e613264b7617",
        "title": "Sharpening the Spear: Adaptive Expert-Guided Adversarial Attack Against DRL-based Autonomous Driving Policies"
      },
      {
        "paperId": "7daedfb1e29b90d58653916f608a6ca6b3b6b72c",
        "title": "MIND-Stack: Modular, Interpretable, End-to-End Differentiability for Autonomous Navigation"
      },
      {
        "paperId": "210c44d337547f3d9e1a8baf6b5ca4c6a21f4dd4",
        "title": "HCRMP: A LLM-Hinted Contextual Reinforcement Learning Framework for Autonomous Driving"
      },
      {
        "paperId": "c98a917a388d1e271c21d5d501a9a56b3d8f9068",
        "title": "A New Framework for Event-Triggered Consensus Control with Reinforcement Learning"
      },
      {
        "paperId": "46c2a90298cedbf12c72fe289dfa3214ce6264de",
        "title": "Uncertainty-Aware Path Planning With Multiple Constraints for Autonomous Vehicles Based on Nonlinear Reduced-Order Model Predictive Control"
      },
      {
        "paperId": "87c0eb25d3b4e089cf879322f08287ca3cc6f5ba",
        "title": "Disturbance Compensation-Based Deep Reinforcement Learning Control Strategy for Underactuated Overhead Crane Systems: Design and Experiments"
      },
      {
        "paperId": "291053dab71c6e8081286dc758130e1df9bfebae",
        "title": "Boosting Rare Scenario Perception in Autonomous Driving: An Adaptive Approach With MoEs and LoRA"
      },
      {
        "paperId": "2afb69773457755f83f81a4a24f41ed4ebd359b5",
        "title": "Separation and Rendezvous Control With Batteries Replacement for the UAV-USV Ecosystem: A Finite-Time Bipartite Method Under the MPC Structure"
      },
      {
        "paperId": "2f94d85a5b980d726c08eb10dc7ac65f2068fe68",
        "title": "Separation and Rendezvous Control of the UAV-USV Subsystem Based on Distributed SMPC With Unbounded Disturbances"
      },
      {
        "paperId": "93c88e1fab94c89151770ea9ecd28a017a03090c",
        "title": "State Compensation Model in Adaptive Event-Triggered Predictive Control: A Novel Approach to Mitigating Moving Bottlenecks"
      },
      {
        "paperId": "4ece7d25fbe2cc0760afbd9aae2e42e83e7cb2c5",
        "title": "Autonomous Vehicle Path Tracking Using Event\u2010Triggered MPC With Switching Model: Methodology and Real\u2010World Validation"
      },
      {
        "paperId": "5efc3bde2a9ecea1d093b862eda2993fe43d4b60",
        "title": "Segment-Based Trajectory Prediction and Risk Assessment for RSU-Assisted CAVs at Signalized Intersections"
      },
      {
        "paperId": "fad0294f8095fe833dfdc7d105761eb10232fc14",
        "title": "Event-Triggered Output Feedback Model Predictive Control for Path Following of Autonomous Vehicles Under False Data Injection Attacks"
      },
      {
        "paperId": "0a76d96bdf9572f7812617fdd169633736f8a306",
        "title": "Online Multi-object Tracking with YOLOv9 and DeepSORT Optimized by Optical Flow"
      },
      {
        "paperId": "4405a3199e767f03e8fb0c6e43e32888af7ede72",
        "title": "A Novel Dynamic Berthing Scheme for an USV: DPFS Guidance and Two-Dimensional Event Triggering ILC"
      },
      {
        "paperId": "9fc7e31e2e81a530c9bb7185d396ac2ba835c29b",
        "title": "Integrating Reinforcement Learning and Large Language Models for Crop Production Process Management Optimization and Control through A New Knowledge-Based Deep Learning Paradigm"
      },
      {
        "paperId": "b998dd3f7e8f4756f8bcbee874ce3f4d25b7655d",
        "title": "A Comprehensive Review on Safe Reinforcement Learning for Autonomous Vehicle Control in Dynamic Environments"
      },
      {
        "paperId": "f278752b06076d4ce17d275f2f69a587459e2644",
        "title": "Learning-Based MPC for Autonomous Motion Planning at Freeway Off-Ramp Diverging"
      },
      {
        "paperId": "d36f3813a44de37b27a6705ba730e5ab114bbd17",
        "title": "Performance Evaluation of PID, Stanley, and Hybrid (PID with Stanley) Control Algorithms on Bitum Line-Follower Road Coaster"
      },
      {
        "paperId": "0b9a413b5fca6c50999ae18c1a2d83c656c75f73",
        "title": "A Robust Autonomous UAV Control System Based on Kalman Filter,LSTM, MLP and DQN"
      },
      {
        "paperId": "44bbf849a3784172dd2328b972665de4dd8e4594",
        "title": "Enhancing Safety in Autonomous Vehicles through Advanced AI-Driven Perception and Decision-Making Systems"
      },
      {
        "paperId": "2edb4535dbda1d6ed522d5d2b74a12de809deca9",
        "title": "Enhancing Control Strategy in Switched Reluctance Motors(SRM): A Focus on Event-Triggered Control"
      },
      {
        "paperId": "c2122cb88c0991abbdfe9735a13022e68b237c83",
        "title": "A dynamics model for Self-Propelled Modular Transporter*"
      },
      {
        "paperId": "479d22795b00f9ceaea0845f56dc48d0c2361070",
        "title": "MARRGM: Learning Framework for Multi-Agent Reinforcement Learning via Reinforcement Recommendation and Group Modification"
      },
      {
        "paperId": "d3572e6cbe0051568ea93d3d0eb7dfe9e4560b24",
        "title": "Reconfigurable Model Predictive Control for Large Scale Distributed Systems"
      },
      {
        "paperId": "7ef4499d394c060bbbd9d6c69d9c6ab8ee09a846",
        "title": "Modeling Driver Lane Change Behavior Using Inverse Reinforcement Learning"
      },
      {
        "paperId": "e2070b3bb660500d0efc92b6cacc64f6a274887b",
        "title": "Reinforcement Learning-Based Event-Triggered Active-Battery-Cell-Balancing Control for Electric Vehicle Range Extension"
      },
      {
        "paperId": "3e68773182ce681be888ae27c45d80a38565a82a",
        "title": "Event-Triggered Parallel Control Using Deep Reinforcement Learning With Application to Comfortable Autonomous Driving"
      },
      {
        "paperId": "ca18ca423c304ecf67de3af44d4fb977774f94d9",
        "title": "Graph Reinforcement Learning for Multi-Aircraft Conflict Resolution"
      },
      {
        "paperId": "12d471c0a21db38d3d494c6b060b7bbfd383aa0d",
        "title": "Learning to Model Diverse Driving Behaviors in Highly Interactive Autonomous Driving Scenarios With Multiagent Reinforcement Learning"
      },
      {
        "paperId": "e8d3133561c0e17a14c7f21c92c43e35c75f30e8",
        "title": "Experimental Validation of Event-Triggered Model Predictive Control for Autonomous Vehicle Path Tracking"
      },
      {
        "paperId": "197430de216b89282681d4dac981bf27bbca633a",
        "title": "Event-Triggered Deep Reinforcement Learning Using Parallel Control: A Case Study in Autonomous Driving"
      },
      {
        "paperId": "d1eea4e17d55ec8dd66b962a048353710a020c0a",
        "title": "Learning-Based Event-Triggered MPC With Gaussian Processes Under Terminal Constraints"
      },
      {
        "paperId": "a07aad35a156122078f7090f00e2e97013fd9bd1",
        "title": "Advanced Decision Making and Motion Planning Framework for Autonomous Navigation in Unsignalized Intersections"
      }
    ],
    "score": 13.0
  },
  {
    "id": "3e0925355554e3aeb99de8165c268582a82de3bb",
    "title": "Smooth Exploration for Robotic Reinforcement Learning",
    "authors": [
      "A. Raffin",
      "Jens Kober",
      "F. Stulp"
    ],
    "year": 2020,
    "citationCount": 64,
    "abstract": "Reinforcement learning (RL) enables robots to learn skills from interactions with the real world. In practice, the unstructured step-based exploration used in Deep RL -- often very successful in simulation -- leads to jerky motion patterns on real robots. Consequences of the resulting shaky behavior are poor exploration, or even damage to the robot. We address these issues by adapting state-dependent exploration (SDE) to current Deep RL algorithms. To enable this adaptation, we propose two extensions to the original SDE, using more general features and re-sampling the noise periodically, which leads to a new exploration method generalized state-dependent exploration (gSDE). We evaluate gSDE both in simulation, on PyBullet continuous control tasks, and directly on three different real robots: a tendon-driven elastic robot, a quadruped and an RC car. The noise sampling interval of gSDE permits to have a compromise between performance and smoothness, which allows training directly on the real robots without loss of performance. The code is available at https://github.com/DLR-RM/stable-baselines3.",
    "url": "https://www.semanticscholar.org/paper/3e0925355554e3aeb99de8165c268582a82de3bb",
    "pdf_url": "https://arxiv.org/pdf/2005.05719.pdf",
    "venue": "Conference on Robot Learning",
    "publicationDate": "2020-05-12",
    "externalIds": {
      "ArXiv": "2005.05719",
      "DBLP": "conf/corl/RaffinKS21",
      "CorpusId": 235490159
    },
    "references": [
      {
        "paperId": "e969c6b365d3bcf0ee04aa08a995a50f26e367a3",
        "title": "Regularizing Action Policies for Smooth Control with Reinforcement Learning"
      },
      {
        "paperId": "f1697ce4dddb58533d7d3f937fed74807d46edb8",
        "title": "Implementation Matters in Deep RL: A Case Study on PPO and TRPO"
      },
      {
        "paperId": "003987bfff295e76946bf430376af4fe3d466cb4",
        "title": "Learning to Walk in the Real World with Minimal Human Effort"
      },
      {
        "paperId": "c6c5155c38d4e4ed3db17ca3ab9bcec3a2e7be4d",
        "title": "Continuous-Discrete Reinforcement Learning for Hybrid Control in Robotics"
      },
      {
        "paperId": "0f6d47c38f34617626812e223f03e6272656e819",
        "title": "Six-DoF Pose Estimation for a Tendon-Driven Continuum Mechanism Without a Deformation Model"
      },
      {
        "paperId": "4cdf2fad22afc865999747336c7399fe422e6e8e",
        "title": "Optuna: A Next-generation Hyperparameter Optimization Framework"
      },
      {
        "paperId": "7698ce800b7a35dc2951586c1428545ba547d0e8",
        "title": "Autoregressive Policies for Continuous Control Deep Reinforcement Learning"
      },
      {
        "paperId": "bb0ee42d406f2361fee89cf1274073185a0e9eec",
        "title": "Learning agile and dynamic motor skills for legged robots"
      },
      {
        "paperId": "12c0751b4f51ed833172a713b7e32390032ead93",
        "title": "Soft Actor-Critic Algorithms and Applications"
      },
      {
        "paperId": "2ed619fbc7902155d54f6f21da16ad6c120eac63",
        "title": "Learning to Walk via Deep Reinforcement Learning"
      },
      {
        "paperId": "d9e08e872c69e5acfed2a93ec6f6d623cfd0c680",
        "title": "CEM-RL: Combining evolutionary and gradient-based methods for policy search"
      },
      {
        "paperId": "d37a34c204a8beefcaef4dddddb7a90c16e973d4",
        "title": "Learning dexterous in-hand manipulation"
      },
      {
        "paperId": "c09a440abdc92d187c53812d8aa29ed7de29a37b",
        "title": "Learning to Drive in a Day"
      },
      {
        "paperId": "f802802b3af5a22b79ac65d033ba3cbee33da91b",
        "title": "Randomized Prior Functions for Deep Reinforcement Learning"
      },
      {
        "paperId": "1b9ce6abc0f3024b88fcd4dbd0c10cf5bcf7d38d",
        "title": "DeepMimic"
      },
      {
        "paperId": "dfb2b26f15466bf3ec34fbd72a22bb9d6ecd42f4",
        "title": "Policy Search in Continuous Action Domains: an Overview"
      },
      {
        "paperId": "4debb99c0c63bfaa97dd433bc2828e4dac81c48b",
        "title": "Addressing Function Approximation Error in Actor-Critic Methods"
      },
      {
        "paperId": "811df72e210e20de99719539505da54762a11c6d",
        "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor"
      },
      {
        "paperId": "819bcae49054e00cef3c0972d48b4e40a525f4d9",
        "title": "Deep Neuroevolution: Genetic Algorithms Are a Competitive Alternative for Training Deep Neural Networks for Reinforcement Learning"
      },
      {
        "paperId": "b542d5f3973970902eab247154f74cc5abb5cbb4",
        "title": "Time Limits in Reinforcement Learning"
      },
      {
        "paperId": "b4e58f6710c70ec0bf85a85da033b1634567420d",
        "title": "Position control of an underactuated continuum mechanism using a reduced nonlinear model"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "34fbb5765f398d5972f6bc26cad1d564651457f1",
        "title": "Generalized exploration in policy search"
      },
      {
        "paperId": "142497432fe179ddb6ffe600c64a837ec6179550",
        "title": "Parameter Space Noise for Exploration"
      },
      {
        "paperId": "cadb96f49f3c13c86dfe285b5c75c655391ad1c3",
        "title": "Towards Generalization and Simplicity in Continuous Control"
      },
      {
        "paperId": "9172cd6c253edf7c3a1568e03577db20648ad0c4",
        "title": "Reinforcement Learning with Deep Energy-Based Policies"
      },
      {
        "paperId": "2b10281297ee001a9f3f4ea1aa9bea6b638c27df",
        "title": "OpenAI Gym"
      },
      {
        "paperId": "85eb1fa0821f5087354a81bc3b47467804f66f23",
        "title": "A structurally flexible humanoid spine based on a tendon-driven elastic continuum"
      },
      {
        "paperId": "1464776f20e2bccb6182f183b5ff2e15b0ae5e56",
        "title": "Benchmarking Deep Reinforcement Learning for Continuous Control"
      },
      {
        "paperId": "4b63e34276aa98d5345efa7fe09bb06d8a9d8f52",
        "title": "Deep Exploration via Bootstrapped DQN"
      },
      {
        "paperId": "69e76e16740ed69f4dc55361a3d319ac2f1293dd",
        "title": "Asynchronous Methods for Deep Reinforcement Learning"
      },
      {
        "paperId": "024006d4c2a89f7acacc6e4438d156525b60a98f",
        "title": "Continuous control with deep reinforcement learning"
      },
      {
        "paperId": "d316c82c12cf4c45f9e85211ef3d1fa62497bff8",
        "title": "High-Dimensional Continuous Control Using Generalized Advantage Estimation"
      },
      {
        "paperId": "449532187c94af3dd3aa55e16d2c50f7854d2199",
        "title": "Trust Region Policy Optimization"
      },
      {
        "paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8",
        "title": "Adam: A Method for Stochastic Optimization"
      },
      {
        "paperId": "687d0e59d5c35f022ce4638b3e3a6142068efc94",
        "title": "Deterministic Policy Gradient Algorithms"
      },
      {
        "paperId": "2319a491378867c7049b3da055c5df60e1671158",
        "title": "Playing Atari with Deep Reinforcement Learning"
      },
      {
        "paperId": "58a7acac2a4ac240e293752be4ffd46f786e5293",
        "title": "Robot Skill Learning: From Reinforcement Learning to Evolution Strategies"
      },
      {
        "paperId": "b6bfae6efa1110a57a4d8362721d152d78aae358",
        "title": "A Survey on Policy Search for Robotics"
      },
      {
        "paperId": "2a25f57a5ac627e5f7a2e4502c24bfa51cfdc1cf",
        "title": "2010 Special Issue: Parameter-exploring policy gradients"
      },
      {
        "paperId": "8ac68370386cb90e72189dd4d0b487bb3da19665",
        "title": "Exploring parameter space in reinforcement learning"
      },
      {
        "paperId": "bc6dff14a130c57a91d5a21339c23471faf1d46f",
        "title": "Et al"
      },
      {
        "paperId": "56109a43347a1a1d5dc84c0dff88016e58ea3938",
        "title": "State-Dependent Exploration for Policy Gradient Methods"
      },
      {
        "paperId": "ffced5b53ad956474a12d73b5cbfd38355dfb70a",
        "title": "Reinforcement learning of motor skills with policy gradients"
      },
      {
        "paperId": "476ba2a1c5204d46e420506afacb4b0da6abb868",
        "title": "Shakey the Robot"
      },
      {
        "paperId": null,
        "title": "Rl baselines3 zoo"
      },
      {
        "paperId": null,
        "title": "Pybullet, a python module for physics simulation for games, robotics and machine learning"
      },
      {
        "paperId": null,
        "title": "and M"
      },
      {
        "paperId": null,
        "title": "Stable baselines"
      },
      {
        "paperId": "4c915c1eecb217c123a36dc6d3ce52d12c742614",
        "title": "Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning"
      },
      {
        "paperId": "6aeb22e31b1d808754bfca8ba2bf597d92972d06",
        "title": "On the theory of brownian motion"
      },
      {
        "paperId": "4d6a87d76ec0c0379f0afbf24f84bba848c6246e",
        "title": "Noname manuscript No. (will be inserted by the editor) Policy Search for Motor Primitives in Robotics"
      },
      {
        "paperId": null,
        "title": "iii There is only a linear dependency between the state and the exploration noise, which limits the possibilities"
      }
    ],
    "cited_by": [
      {
        "paperId": "f3174815c3061ffb8bb274f5385f2582c26af5d3",
        "title": "Real-Time Reinforcement Learning for Dynamic Tasks with a Parallel Soft Robot"
      },
      {
        "paperId": "72ba16cc208543832c22a79e5bbaaf36c006a671",
        "title": "Where to Intervene: Action Selection in Deep Reinforcement Learning"
      },
      {
        "paperId": "b02331c58d891fc9fc0a7669be9ce4e83e447344",
        "title": "Autonomous Control and Path Planning of UAV with Deep Reinforcement Learning"
      },
      {
        "paperId": "578c4c8fc8ceb1df24cba6b9a499b7a280c8bd65",
        "title": "From Perception to Action: Transformer-Enhanced Deep Reinforcement Learning for Autonomous Robot Navigation"
      },
      {
        "paperId": "af1f473f6591b59aebf97511b43aedacfe342ba5",
        "title": "Hyperparameter Optimisation with Practical Interpretability and Explanation Methods in Probabilistic Curriculum Learning"
      },
      {
        "paperId": "b0684730040e21bd093367b1ff5159d3116cfe34",
        "title": "Probabilistic Curriculum Learning for Goal-Based Reinforcement Learning"
      },
      {
        "paperId": "574ec4679c9efdaac629b8fd7f01ee07d78b741d",
        "title": "Chunking the Critic: A Transformer-based Soft Actor-Critic with N-Step Returns"
      },
      {
        "paperId": "5d8ad978583631ac646a86ee6bd7ce3f6ccc2ec4",
        "title": "EvoRL: A GPU-accelerated Framework for Evolutionary Reinforcement Learning"
      },
      {
        "paperId": "e7550a9038577db013910f78397d518656317e80",
        "title": "Adaptive Data Exploitation in Deep Reinforcement Learning"
      },
      {
        "paperId": "d22782145e2c6e10eaf1dd1e003ca58e987f37b5",
        "title": "PPO and SAC Reinforcement Learning Based Reference Compensation"
      },
      {
        "paperId": "64448dc31e478f6528623c69142ef42a2d93388a",
        "title": "Practically High Performant Neural Adaptive Video Streaming"
      },
      {
        "paperId": "f19465757a4f29e82864f6e46617f90012e836d0",
        "title": "Toward Space Exploration on Legs: ISS-to-Earth Teleoperation Experiments with a Quadruped Robot"
      },
      {
        "paperId": "a0605fff4de534d94df5d5474997127c01129a36",
        "title": "Sequence labeling via reinforcement learning with aggregate labels"
      },
      {
        "paperId": "66d07f74896a56ff65bf35ace950189c4084e8cc",
        "title": "Adaptive Trajectory Database Learning for Nonlinear Control with Hybrid Gradient Optimization"
      },
      {
        "paperId": "a72661ca3c4de880152e69c7e20383645eb20056",
        "title": "TOP-ERL: Transformer-based Off-Policy Episodic Reinforcement Learning"
      },
      {
        "paperId": "6efb032415f6e62d8d5de7a1571f35901e1c6f97",
        "title": "Overcoming Slow Decision Frequencies in Continuous Control: Model-Based Sequence Reinforcement Learning for Model-Free Control"
      },
      {
        "paperId": "f5ce634abf9c83427752c327735bdc700934285e",
        "title": "Brain-like neural dynamics for behavioral control develop through reinforcement learning"
      },
      {
        "paperId": "4129591e538ce1dbabcb92f6dd0bb10f8f4f4192",
        "title": "Latent Action Priors for Locomotion with Deep Reinforcement Learning"
      },
      {
        "paperId": "174b829cab174d0c87c40306d6e00dd0883f6bc1",
        "title": "Human-Robot Cooperative Distribution Coupling for Hamiltonian-Constrained Social Navigation"
      },
      {
        "paperId": "ead82f9d59189cf7224b561732c7348de9848435",
        "title": "Sim-to-Real Transfer for a Robotics Task: Challenges and Lessons Learned"
      },
      {
        "paperId": "d75d3490f3fde02edd54bad7394f98de76bb4489",
        "title": "Path-Following Control of Unmanned Underwater Vehicle Based on an Improved TD3 Deep Reinforcement Learning"
      },
      {
        "paperId": "5d630f5270c6de5b749bbf137b5e139d3c04acf3",
        "title": "Acquiring musculoskeletal skills with curriculum-based reinforcement learning"
      },
      {
        "paperId": "6b804919ed681ea908f541b2779a4bddc5bd75a4",
        "title": "Analyzing and Bridging the Gap between Maximizing Total Reward and Discounted Reward in Deep Reinforcement Learning"
      },
      {
        "paperId": "1a7781465495ae54ce8413aa4ddedd24f5666dc7",
        "title": "Robust Reinforcement Learning from Corrupted Human Feedback"
      },
      {
        "paperId": "105fa634ad1d7fd24448fbfff4ced961e1ef7df1",
        "title": "Regularized Parameter Uncertainty for Improving Generalization in Reinforcement Learning"
      },
      {
        "paperId": "cd6b87d6f746bd572a79c4f77cc60cf6a92fc015",
        "title": "Adaptive Preference Scaling for Reinforcement Learning with Human Feedback"
      },
      {
        "paperId": "ed23559e4307dd3cefd0b6ce3389babb00b078b3",
        "title": "A LiDAR-based approach to autonomous racing with model-free reinforcement learning"
      },
      {
        "paperId": "5b9890c6846f28b846063be6f1fb161e55827e4e",
        "title": "Efficient Reinforcement Learning of Task Planners for Robotic Palletization Through Iterative Action Masking Learning"
      },
      {
        "paperId": "99323304cfdd8526f5e9b951795c6a6840cb5860",
        "title": "Continual Domain Randomization"
      },
      {
        "paperId": "d1145612b8874daec470fddde64e73d3725c3fc9",
        "title": "Density estimation based soft actor-critic: deep reinforcement learning for static output feedback control with measurement noise"
      },
      {
        "paperId": "134acf45a016fced57cc8f8e171eeb6e0015b0b8",
        "title": "Open the Black Box: Step-based Policy Updates for Temporally-Correlated Episodic Reinforcement Learning"
      },
      {
        "paperId": "696a035eddfb2860123863d5999b74b43133d7fd",
        "title": "Continuous Control With Swarm Intelligence Based Value Function Approximation"
      },
      {
        "paperId": "e658cc60b922ff23d17e4864f4c38174f153a72b",
        "title": "Colored Noise in PPO: Improved Exploration and Performance Through Correlated Action Sampling"
      },
      {
        "paperId": "1484e502c74eee5979518fcebf795f38e6f0ff37",
        "title": "Model-Based Epistemic Variance of Values for Risk-Aware Policy Optimization"
      },
      {
        "paperId": "16b457f9700055598125f9fa3f23bdfdde948629",
        "title": "An Open-Loop Baseline for Reinforcement Learning Locomotion Tasks"
      },
      {
        "paperId": "dc96ff6357ce0fa8b231b3bb7db1197577b91aba",
        "title": "Machine Learning Meets Advanced Robotic Manipulation"
      },
      {
        "paperId": "28aa4452c63da832a3e185475285044ba1496391",
        "title": "Value-Distributional Model-Based Reinforcement Learning"
      },
      {
        "paperId": "5c3d433550303f99f3d37e000632b144d1c0bcb6",
        "title": "SAR: Generalization of Physiological Dexterity via Synergistic Action Representation"
      },
      {
        "paperId": "bca7d3dec93b9d771c6c0d28c237043fd9709090",
        "title": "SAR: Generalization of Physiological Agility and Dexterity via Synergistic Action Representation"
      },
      {
        "paperId": "54952feedd88ca743dc6b01cfbc707d9254bbdb3",
        "title": "Latent Exploration for Reinforcement Learning"
      },
      {
        "paperId": "899f28625ba1166ae000633215851605d948b9da",
        "title": "Testing of Deep Reinforcement Learning Agents with Surrogate Models"
      },
      {
        "paperId": "674945263851dc0efa9f91dd77949e8931a6871d",
        "title": "Multiple Frequency Bands Temporal State Representation for Deep Reinforcement Learning"
      },
      {
        "paperId": "490afb7238ab1088d5a30a5c2a336dc052b956d8",
        "title": "A Multiplicative Value Function for Safe and Efficient Reinforcement Learning"
      },
      {
        "paperId": "c0851164660d064ef29c291f04c931c7d1bd1bec",
        "title": "Reinforcement Learning from Simulation to Real World Autonomous Driving using Digital Twin"
      },
      {
        "paperId": "e2e864999c7de6e7ef84ed37e0e1da484d1d4a51",
        "title": "Inferring Smooth Control: Monte Carlo Posterior Policy Iteration with Gaussian Processes"
      },
      {
        "paperId": "57a6d470eacba2233caf811dfe036bba145fa292",
        "title": "DMAP: a Distributed Morphological Attention Policy for Learning to Locomote with a Changing Body"
      },
      {
        "paperId": "0adeaf0115fd5549cc9b429c5ba45bc83e04190e",
        "title": "Learning to Exploit Elastic Actuators for Quadruped Locomotion"
      },
      {
        "paperId": "7e7475d4ca367648724b569dac5c17c16513f894",
        "title": "Performance-Driven Controller Tuning via Derivative-Free Reinforcement Learning"
      },
      {
        "paperId": "104213c5ead6ed90419aa230ee2f73ce8f793a5a",
        "title": "Some Supervision Required: Incorporating Oracle Policies in Reinforcement Learning via Epistemic Uncertainty Metrics"
      },
      {
        "paperId": "3f66a70a181e9cd1cd8d0760c6c5e7a6f3ff2006",
        "title": "Action Noise in Off-Policy Deep Reinforcement Learning: Impact on Exploration and Performance"
      },
      {
        "paperId": "75331654cbd51b9e26147be9c50cac650b7370cd",
        "title": "Improvements in learning to control perched landings"
      },
      {
        "paperId": "17dbfadccae5407c9114b2cf7d26597626b1577d",
        "title": "Open Source Tendon-driven Continuum Mechanism: A Platform for Research in Soft Robotics"
      },
      {
        "paperId": "c6ab9d54ed4f80128580a68934b1139c234d7d33",
        "title": "Ultrasound-Guided Assistive Robots for Scoliosis Assessment With Optimization-Based Control and Variable Impedance"
      },
      {
        "paperId": "74ab711c15a14ebd78b3c4e64fe62ea252a59a63",
        "title": "Zeroth-Order Actor\u2013Critic: An Evolutionary Framework for Sequential Decision Problems"
      },
      {
        "paperId": "a7d58bd29778ef0d15b9e9e3eb2f37a8cf1ea70c",
        "title": "Recurrent Model-Free RL Can Be a Strong Baseline for Many POMDPs"
      },
      {
        "paperId": "96e8aea638c842a32003aa689b237c40b3cfdc5d",
        "title": "Soft Actor-Critic With Integer Actions"
      },
      {
        "paperId": "5f587b5996e43014b9f784d5b2f46d9a9d444d0b",
        "title": "Smart Electric Vehicle Charging Algorithm to Reduce the Impact on Power Grids: A Reinforcement Learning Based Methodology"
      },
      {
        "paperId": "f997668e3d4e4bd75842c516dcba74126e610809",
        "title": "Adaptive Human-Robot Collaboration using Type-Based IRL"
      },
      {
        "paperId": "d849f85bf599d44c9ca5fbfceb17a8913fc65432",
        "title": "Low-Coupling Policy Optimization Framework for Power Allocation in Ultra-Dense Small-Cell Networks"
      },
      {
        "paperId": "d2ae04688a8fd04e6db045388382b74cbc90af83",
        "title": "Exploring Trading on Decentralized Exchanges using Reinforcement Learning"
      },
      {
        "paperId": "e208e302e31a32b2bd760a5f4e2274675486460b",
        "title": "Dec-AIRL: Decentralized Adversarial IRL for Human-Robot Teaming"
      },
      {
        "paperId": "9bd59bab0eea6c56de7069da9fc4679a86671017",
        "title": "Trust Region Optimization of Optimistic Actor Critic"
      },
      {
        "paperId": "bcc15cddde53643ae16b374bd43f086cea8f93d0",
        "title": "Scaling the Number of Tasks in Continual Learning"
      },
      {
        "paperId": "5eff5cff282ea369ea8cd25bf01dd2df5fd668b4",
        "title": "Pink Noise LQR: How does Colored Noise affect the Optimal Policy in RL?"
      }
    ],
    "score": 12.8
  },
  {
    "id": "1fa2a04bffc18cfb46650a11ab0e5696d711f58f",
    "title": "An exploration strategy improves the diversity of de novo ligands using deep reinforcement learning: a case for the adenosine A2A receptor",
    "authors": [
      "Xuhan Liu",
      "K. Ye",
      "H. V. van Vlijmen",
      "A. IJzerman",
      "G. V. van Westen"
    ],
    "year": 2018,
    "citationCount": 77,
    "abstract": "Over the last 5\u00a0years deep learning has progressed tremendously in both image recognition and natural language processing. Now it is increasingly applied to other data rich fields. In drug discovery, recurrent neural networks (RNNs) have been shown to be an effective method to generate novel chemical structures in the form of SMILES. However, ligands generated by current methods have so far provided relatively low diversity and do not fully cover the whole chemical space occupied by known ligands. Here, we propose a new method (DrugEx) to discover de novo drug-like molecules. DrugEx is an RNN model (generator) trained through reinforcement learning which was integrated with a special exploration strategy. As a case study we applied our method to design ligands against the adenosine A2A receptor. From ChEMBL data, a machine learning model (predictor) was created to predict whether generated molecules are active or not. Based on this predictor as the reward function, the generator was trained by reinforcement learning without any further data. We then compared the performance of our method with two previously published methods, REINVENT and ORGANIC. We found that candidate molecules our model designed, and predicted to be active, had a larger chemical diversity and better covered the chemical space of known ligands compared to the state-of-the-art.",
    "url": "https://www.semanticscholar.org/paper/1fa2a04bffc18cfb46650a11ab0e5696d711f58f",
    "pdf_url": "https://doi.org/10.1186/s13321-019-0355-6",
    "venue": "Journal of Cheminformatics",
    "publicationDate": "2018-12-10",
    "externalIds": {
      "MAG": "2947161483",
      "DBLP": "journals/jcheminf/LiuYVIW19",
      "PubMedCentral": "6534880",
      "DOI": "10.1186/s13321-019-0355-6",
      "CorpusId": 96450357,
      "PubMed": "31127405"
    },
    "references": [
      {
        "paperId": "6296aa7cab06eaf058f7291040b320b5a83c0091",
        "title": "Generative Adversarial Networks"
      },
      {
        "paperId": "8df72c48a7ce4418c683c4dd9bb300558ac71d47",
        "title": "Deep learning for healthcare: review, opportunities and challenges"
      },
      {
        "paperId": "389c94ce65f5eda239ebb12c8ba9d080951395a6",
        "title": "Drug Discovery Maps, a Machine Learning Model That Visualizes and Predicts Kinome\u2013Inhibitor Interaction Landscapes"
      },
      {
        "paperId": "89213307256d538942fab6b4ac7ab0dee6936582",
        "title": "The rise of deep learning in drug discovery."
      },
      {
        "paperId": "7d878fe31b9b57f75071586d83cdec2e8b81e039",
        "title": "Fr\u00e9chet ChemNet Distance: A Metric for Generative Models for Molecules in Drug Discovery"
      },
      {
        "paperId": "88a99980f1f7eeac5f36be2e4601898988bdf937",
        "title": "Mol2vec: Unsupervised Machine Learning Approach with Chemical Intuition"
      },
      {
        "paperId": "308c50f6ba4cfc354f513f21dc6a5c43028edd93",
        "title": "Erratum: Generative Recurrent Networks for De Novo Drug Design."
      },
      {
        "paperId": "1cb789ab8925bda02758bcb69eb0ed1547b5f4b9",
        "title": "Generating Focused Molecule Libraries for Drug Discovery with Recurrent Neural Networks"
      },
      {
        "paperId": "517a488f06577401422cf7e03647ac6148ef4e44",
        "title": "Generative Recurrent Networks for De Novo Drug Design"
      },
      {
        "paperId": "6e3623abf3eb994a3aef3d26234081547c617057",
        "title": "Trends in GPCR drug discovery: new agents, targets and indications"
      },
      {
        "paperId": "9381d723487755bf23f24cf21513d37aeeda5042",
        "title": "ChemGAN challenge for drug discovery: can AI reproduce natural chemical diversity?"
      },
      {
        "paperId": "3b1115226eb73af88681fc2dac9bd35ff01c8991",
        "title": "druGAN: An Advanced Generative Adversarial Autoencoder Model for de Novo Generation of New Molecules with Desired Molecular Properties in Silico."
      },
      {
        "paperId": "9f275608a190daf46d0f5a5c4636792154d07993",
        "title": "Beyond the hype: deep neural networks outperform established methods using a ChEMBL bioactivity benchmark set"
      },
      {
        "paperId": "d77bc27d16a362e5e1b727904c3789355dda6062",
        "title": "Molecular de-novo design through deep reinforcement learning"
      },
      {
        "paperId": "213381a2992ba80c8483b53ef2e38539eee5720c",
        "title": "A comprehensive map of molecular drug targets"
      },
      {
        "paperId": "d71af418eeb9f5a68062929bae12af74773ffcb2",
        "title": "The ChEMBL database in 2017"
      },
      {
        "paperId": "32c4e19f4a757f6c6984416b97d69e287d1d0ecd",
        "title": "SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient"
      },
      {
        "paperId": "cbbbf3d85f4ff0cec5c9613836adc125ed811e92",
        "title": "The Next Era: Deep Learning in Pharmaceutical Research"
      },
      {
        "paperId": "c569c21f8f1a796475e196a9a66f1e898bb050cd",
        "title": "In vitro expression and analysis of the 826 human G protein-coupled receptors"
      },
      {
        "paperId": "30501640908567ef13640d82c633b6ac6b265dfc",
        "title": "Applications of Deep Learning in Biomedicine."
      },
      {
        "paperId": "846aedd869a00c09b40f1f1f35673cb22bc87490",
        "title": "Mastering the game of Go with deep neural networks and tree search"
      },
      {
        "paperId": "5972c3d8507359a6cff6ef17c4af206ec76b32bb",
        "title": "ZINC 15 \u2013 Ligand Discovery for Everyone"
      },
      {
        "paperId": "5d1bfeed240709725c78bc72ea40e55410b373dc",
        "title": "Convolutional Networks on Graphs for Learning Molecular Fingerprints"
      },
      {
        "paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8",
        "title": "Adam: A Method for Stochastic Optimization"
      },
      {
        "paperId": "ac3ee98020251797c2b401e1389461df88e52e62",
        "title": "Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling"
      },
      {
        "paperId": "454e56efab708e876271764ce0d1fb34e12793e6",
        "title": "QSAR modeling: where have you been? Where are you going to?"
      },
      {
        "paperId": "1b6d8ec190bae244daf3e11d80825ff942a6d435",
        "title": "Adenosine receptors as drug targets \u2014 what are the challenges?"
      },
      {
        "paperId": "662fb09a1750dd888a223624e5b5053513a1c035",
        "title": "Structural Basis for Allosteric Regulation of GPCRs by Sodium Ions"
      },
      {
        "paperId": "a8db50edfe26a6ae33a6787e2049de5bacd18666",
        "title": "ChEMBL: a large-scale bioactivity database for drug discovery"
      },
      {
        "paperId": "ad4fd2c149f220a62441576af92a8a669fe81246",
        "title": "Scikit-learn: Machine Learning in Python"
      },
      {
        "paperId": "23664d82acfc6ddad847b8c45ffcab7e5752fcfd",
        "title": "Adenosine receptors as drug targets."
      },
      {
        "paperId": "6420a334687d290d77c6b5ec99ca17f9d069df4a",
        "title": "Extended-Connectivity Fingerprints"
      },
      {
        "paperId": "f22e29ec623c7a3c6d097b60ee8ba4e1da559571",
        "title": "Ligand Binding and Subtype Selectivity of the Human A2A Adenosine Receptor"
      },
      {
        "paperId": "135cc653f3b47eea2566b4acbe1cfec8d7f28590",
        "title": "The 2.6 Angstrom Crystal Structure of a Human A2A Adenosine Receptor Bound to an Antagonist"
      },
      {
        "paperId": "32c52372b372684953f25b55be84ed29147ebf1f",
        "title": "When good drugs go bad"
      },
      {
        "paperId": "d61564f9d01dadf4d201178e78b471b06eb81279",
        "title": "G-protein-coupled receptors and cancer"
      },
      {
        "paperId": "2610ac14596e90d3989207fc84bec1c272f33fd2",
        "title": "The Molecule Evoluator. An Interactive Evolutionary Algorithm for the Design of Drug-Like Molecules"
      },
      {
        "paperId": "b7b157be4eae4a15a74dd47f2d387504d97ac215",
        "title": "Computer-based de novo design of drug-like molecules"
      },
      {
        "paperId": null,
        "title": "Generating Focused Molecule Libraries for 608"
      },
      {
        "paperId": "61c05d7cab4448a1585596cc7e00ab271c442c2e",
        "title": "Optimizing distributions over molecular space. An Objective-Reinforced Generative Adversarial Network for Inverse-design Chemistry (ORGANIC)"
      },
      {
        "paperId": "4f8d648c52edf74e41b0996128aa536e13cc7e82",
        "title": "Deep Learning"
      },
      {
        "paperId": "2913c2bf3f92b5ae369400a42b2d27cc5bc05ecb",
        "title": "Deep Learning"
      },
      {
        "paperId": null,
        "title": "Ligand binding and subtype 611 selectivity of the human A ( 2 A ) adenosine receptor : identification and characterization of 612 essential amino acid residues"
      },
      {
        "paperId": "1c46943103bd7b7a2c7be86859995a4144d1938b",
        "title": "Visualizing Data using t-SNE"
      },
      {
        "paperId": "34b9635d7779e219e9d60e0d3d33919ca9bc123c",
        "title": "Publisher's Note"
      },
      {
        "paperId": "45fa549eabcb14ecd501b999afc3f865ecaacb69",
        "title": "Property Distributions: Differences between Drugs, Natural Products, and Molecules from Combinatorial Chemistry"
      },
      {
        "paperId": null,
        "title": "Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations"
      },
      {
        "paperId": null,
        "title": "hvvlijme@its"
      }
    ],
    "cited_by": [
      {
        "paperId": "64671d0fc7307406ba9c2bd04c86bd0c7d814ac0",
        "title": "Supervised machine learning and molecular docking modeling to identify potential Anti-Parkinson's agents."
      },
      {
        "paperId": "08b3a8e9a1c7419f451ef443254551806d4b7d1e",
        "title": "ICVAE: Interpretable Conditional Variational Autoencoder for De Novo Molecular Design"
      },
      {
        "paperId": "9c47ea2bf1c47dc6054c05cc4d3a09e364b49f7b",
        "title": "Test-Time Training Scaling Laws for Chemical Exploration in Drug Design"
      },
      {
        "paperId": "44fced59d5a51ce65267b06c278e6c86158c0735",
        "title": "Design of the Inhibitors for Pseudorabies Virus Replication by Reinforcement Learning from HSV-1 DNA Polymerase Inhibitors"
      },
      {
        "paperId": "f1f08c281370530185b1f30b9594d375525e4478",
        "title": "3DSMILES-GPT: 3D molecular pocket-based generation with token-only large language model"
      },
      {
        "paperId": "5be9986d4fcf0f66ddba5b32a346a0a59536aa49",
        "title": "Advanced AI and ML frameworks for transforming drug discovery and optimization: With innovative insights in polypharmacology, drug repurposing, combination therapy and nanomedicine."
      },
      {
        "paperId": "029592f12872948638161d1f9baa14dd92227213",
        "title": "Structural optimization of PNIPAM-derived thermoresponsive polymers: a computational approach employing artificial neural networks and genetic algorithms"
      },
      {
        "paperId": "03959fe4cbf302d3284420986638abe2bb5fe9ea",
        "title": "Reinforcement Learning Strategies for De Novo Drug Design: an Overview"
      },
      {
        "paperId": "8fdba536e059549f5f051a0e00b41c0955c3ff77",
        "title": "De Novo Drug Design by Multi-Objective Path Consistency Learning With Beam A* Search"
      },
      {
        "paperId": "71bfc2135be18e7a1a121ffe529133f454c21d99",
        "title": "A comprehensive review of artificial intelligence for pharmacology research"
      },
      {
        "paperId": "f3066084e928e3941c066ebf9350bd847ab2424b",
        "title": "ExSelfRL: An exploration-inspired self-supervised reinforcement learning approach to molecular generation"
      },
      {
        "paperId": "b477af4faeca60d4f3dcf7240f56b0fea97431fd",
        "title": "Multi-objective molecular generation via clustered Pareto-based reinforcement learning"
      },
      {
        "paperId": "324a464fed125f2ca2f2f3b641b06646a3d43c46",
        "title": "Diverse Hits in De Novo Molecule Design: Diversity-Based Comparison of Goal-Directed Generators"
      },
      {
        "paperId": "cac24817ef527960e63d26d369c8a66528455675",
        "title": "From predicting to decision making: Reinforcement learning in biomedicine"
      },
      {
        "paperId": "2205cc7c3373c799593e93541cf2550523a44361",
        "title": "Adapt-cMolGPT: A Conditional Generative Pre-Trained Transformer with Adapter-Based Fine-Tuning for Target-Specific Molecular Generation"
      },
      {
        "paperId": "fe5d55584aba445fd6017a0de0700354dcba6a56",
        "title": "Unlocking the potential of generative AI in drug discovery."
      },
      {
        "paperId": "8ac89908f5cdb1e6cb82f4670bebadc901f43c57",
        "title": "Multi-and many-objective optimization: present and future in de novo drug design"
      },
      {
        "paperId": "a973ae62a1d210fd4dffd843b17e5611753116ed",
        "title": "Generation of focused drug molecule library using recurrent neural network"
      },
      {
        "paperId": "7062ba649c293d858aaa1b2f4d2fd677883f85e0",
        "title": "Exploring the therapeutic potential of layered double hydroxides and transition metal dichalcogenides through the convergence of rheumatology and nanotechnology using generative adversarial network."
      },
      {
        "paperId": "383d80ef1c69f91071ed3bf1277f1d0ca2b21b5b",
        "title": "The Application of Artificial Intelligence Accelerates G Protein-Coupled Receptor Ligand Discovery"
      },
      {
        "paperId": "5147e916676e1103874781dbf8daab3f4e3a0f5e",
        "title": "Artificial intelligence for natural product drug discovery"
      },
      {
        "paperId": "c4eba611920ee29f43909abec5fa0532c715c391",
        "title": "LOGICS: Learning optimal generative distribution for designing de novo chemical structures"
      },
      {
        "paperId": "d9d24516c346d5287f4826245bd0c688f4533822",
        "title": "Yin-yang in drug discovery: rethinking de novo design and development of predictive models"
      },
      {
        "paperId": "d241c14af727b380df50f693652348c2dc95f565",
        "title": "DrugEx: Deep Learning Models and Tools for Exploration of Drug-Like Chemical Space"
      },
      {
        "paperId": "9c333605e37e00ba92f8ff1a94205c293c25363b",
        "title": "Molecular Generation with Reduced Labeling through Constraint Architecture"
      },
      {
        "paperId": "83027132d8842cf6007dca21c12b7cf3c6974e74",
        "title": "MolFilterGAN: a progressively augmented generative adversarial network for triaging AI-designed molecules"
      },
      {
        "paperId": "5579b0dd263902ad6fe76f23db074dd533522f0e",
        "title": "De novo drug design based on Stack-RNN with multi-objective reward-weighted sum and reinforcement learning"
      },
      {
        "paperId": "7c88c5642f2320dee5199f7ea45ee119b7d8ea35",
        "title": "Universal Approach to De Novo Drug Design for Target Proteins Using Deep Reinforcement Learning"
      },
      {
        "paperId": "60dc19eeb94b8da364508705e1c68a3583a815f0",
        "title": "Opportunities and challenges in application of artificial intelligence in pharmacology"
      },
      {
        "paperId": "02d7b46bfd3c8eb78202947ac78696726ec8d5e4",
        "title": "Predicting chemical structure using reinforcement learning with a stack-augmented conditional variational autoencoder"
      },
      {
        "paperId": "79ed7e07f0d4bd932e7771fac33ded9e3764652a",
        "title": "Exploration of Chemical Space Guided by PixelCNN for Fragment-Based De Novo Drug Discovery"
      },
      {
        "paperId": "4043384314e2a95d1039d555b8db9d286d42e407",
        "title": "Reinforcement Learning and Graph Neural Networks for Designing Novel Drugs with Optimized Affinity: Application to SARS-CoV-2"
      },
      {
        "paperId": "c46e4883df4004edac40050cdd9556876449bfd2",
        "title": "Conditional reduction of the loss value versus reinforcement learning for biassing a de-novo drug design generator"
      },
      {
        "paperId": "b5a59a369d207386725b6b277c57e714f30fff1b",
        "title": "ChemistGA: A Chemical Synthesizable Accessible Molecular Generation Algorithm for Real-World Drug Discovery."
      },
      {
        "paperId": "97a822bb1641e1b6a9d73d6947a4fcb175cfe7ba",
        "title": "Designing optimized drug candidates with Generative Adversarial Network"
      },
      {
        "paperId": "258e3ec609152e8832fcd5a7ed8fc72091f0f2f4",
        "title": "iFeatureOmega: an integrative platform for engineering, visualization and analysis of features from molecular sequences, structural and ligand data sets"
      },
      {
        "paperId": "e19e18e7269598130f42d4a3424e035935b59d56",
        "title": "Reinforcement learning for systems pharmacology-oriented and personalized drug design"
      },
      {
        "paperId": "c74c54bf809be3200f5677a13eeff53efa1d28b1",
        "title": "Synergy between machine learning and natural products cheminformatics: Application to the lead discovery of anthraquinone derivatives"
      },
      {
        "paperId": "89069fb582119a41280078ad8706272f53dd3dd3",
        "title": "Molecular Simulations and Drug Discovery of Adenosine Receptors"
      },
      {
        "paperId": "c363a4291f8af7b1a12cdcf1f92dcc05f4aa9a34",
        "title": "Generative machine learning for de novo drug discovery: A systematic review"
      },
      {
        "paperId": "e4241619e9af5b41150b9a4d0982ac02886899de",
        "title": "Reinforcement Learning for Personalized Drug Discovery and Design for Complex Diseases: A Systems Pharmacology Perspective"
      },
      {
        "paperId": "2d3c78f7be72deb7ad0d8b0ede1b20122286e915",
        "title": "DrugEx v3: scaffold-constrained drug design with graph transformer-based reinforcement learning"
      },
      {
        "paperId": "250ac5f01aa125bd2a3188d183c68eca185f2054",
        "title": "Deep learning approaches for de novo drug design: An overview."
      },
      {
        "paperId": "985d43b430fbfc2894347f02f9cd6dbfcebc467b",
        "title": "Molecular generation by Fast Assembly of (Deep)SMILES fragments"
      },
      {
        "paperId": "a15fa0bcf183a2e2278d01d8a82aa275bbecbb02",
        "title": "DrugEx v2: de novo design of drug molecules by Pareto-based multi-objective reinforcement learning in polypharmacology"
      },
      {
        "paperId": "afd918ae6f9dbf38b658057120dbb0a78e92c2a6",
        "title": "Deep Learning Applied to Ligand-Based De Novo Drug Design."
      },
      {
        "paperId": "574ce5427964f704bee222e639e78f6a554637e5",
        "title": "Artificial Intelligence in Compound Design."
      },
      {
        "paperId": "f75d1150dc1936b9c7c995464ac698a70f811e2b",
        "title": "Design of SARS-CoV-2 Mpro, PLpro dual-target inhibitors based on deep reinforcement learning and virtual screening"
      },
      {
        "paperId": "5d52075350f98416ad87f459e78912264b664177",
        "title": "The roles of computer-aided drug synthesis in drug development"
      },
      {
        "paperId": "f54a53ca1e1aab8bbf640695936b62b79115cbd7",
        "title": "Multi-constraint molecular generation based on conditional transformer, knowledge distillation and reinforcement learning"
      },
      {
        "paperId": "f50b4fb267cf8e82386ef29506e93cca353965f5",
        "title": "GenUI: interactive and extensible open source software platform for de novo molecular generation and cheminformatics"
      },
      {
        "paperId": "305c1173a2a10b3f90e4c65d64d19d19c73c7a77",
        "title": "Chemical language models enable navigation in sparsely populated chemical space"
      },
      {
        "paperId": "2867f915f7419915909e60b3f9f8a29ada9f10d2",
        "title": "Optimizing blood\u2013brain barrier permeation through deep reinforcement learning for de novo drug design"
      },
      {
        "paperId": "653408b8836fe8686fb031fba81d39aab5411bfa",
        "title": "De novo design with deep generative models based on 3D similarity scoring."
      },
      {
        "paperId": "b51002056dbca34f7617a7574289b0abd2322369",
        "title": "Trends in application of advancing computational approaches in GPCR ligand discovery"
      },
      {
        "paperId": "8214d2934b7ab7cb682566c01344bb1e24320b49",
        "title": "Advances in De Novo Drug Design: From Conventional to Machine Learning Methods"
      },
      {
        "paperId": "7cf8278aed93bedde4cd98074fa1e1e46245d01e",
        "title": "Deep Generative Models Enable Navigation in Sparsely Populated Chemical Space"
      },
      {
        "paperId": "ca7e8bc95864805c15e667ad016dfff98865fd51",
        "title": "Artificial Intelligence in Drug Discovery: A Comprehensive Review of Data-driven and Machine Learning Approaches"
      },
      {
        "paperId": "8db73aaa4451df27b6e3fc16e41c1835022bc4ff",
        "title": "Diversity oriented Deep Reinforcement Learning for targeted molecule generation"
      },
      {
        "paperId": "38dee061369215ee29b65b44f3ce9ea7c702ca6c",
        "title": "Exploration and augmentation of pharmacological space via adversarial auto-encoder model for facilitating kinase-centric drug development"
      },
      {
        "paperId": "685d7b452431904c650cf5e00355f6882ea05e69",
        "title": "REINVENT 2.0: An AI Tool for De Novo Drug Design"
      },
      {
        "paperId": "bfb8d3300c67ee13585fd3ec67fd3fd445489a8e",
        "title": "Memory-assisted reinforcement learning for diverse molecular de novo design"
      },
      {
        "paperId": "400705d0a6ae302e26f48ef4b014750af7446124",
        "title": "Machine learning for synthetic biology: Methods and applications"
      },
      {
        "paperId": "3b76c0b197e8bac9f0672b388e2c661b05814b67",
        "title": "Navigating Chemical Space By Interfacing Generative Artificial Intelligence and Molecular Docking"
      },
      {
        "paperId": "b20f13ddf177b09a0a81fe05ab5a9bf29fb576b5",
        "title": "REINVENT 2.0 \u2013 an AI Tool for De Novo Drug Design"
      },
      {
        "paperId": "87c80904579108e9ebecf05d78e83497e78030a1",
        "title": "Machine learning and AI-based approaches for bioactive ligand discovery and GPCR-ligand recognition"
      },
      {
        "paperId": "ff98cb1eb1d642fbba1d76c88fcc939bf8685ed4",
        "title": "Machine and Deep Learning Approaches for Cancer Drug Repurposing."
      },
      {
        "paperId": "8abe4a49c00c45c1afe10428f2ed24efc6dc4086",
        "title": "On failure modes in molecule generation and optimization."
      },
      {
        "paperId": "f8b012720a2322dcf4ed9ac4d61d6be11d9ebd10",
        "title": "Concepts of Artificial Intelligence for Computer-Assisted Drug Discovery."
      },
      {
        "paperId": "14691a417b8eba85e8b0129fc2651606ee769c06",
        "title": "Journal of Cheminformatics, ORCID, and GitHub"
      },
      {
        "paperId": "80977e9f67e9089014057f72ed0b14f4bf765eea",
        "title": "Designing Diverse and Synthesizable Molecules using Multi-Objective Generative Flow Networks guided by Retrosynthetic Accessibility"
      },
      {
        "paperId": "6bc27d6e1f33efc6aa5c4768e391d39fc2838402",
        "title": "Application of Deep Learning in Materials Design: Extraction of Process-Structure-Property Relationship"
      },
      {
        "paperId": "da985c726a48be0d480615da00906b98e139ac4f",
        "title": "Molecular Property Predictors for Downstream De Novo Generation"
      },
      {
        "paperId": "aab5fe6c74a08e0388074f8b44de09e9d4d1f43c",
        "title": "Computational Approaches for De Novo Drug Design: Past, Present, and Future"
      },
      {
        "paperId": "671f2dda7066f450891b670401410da7c0810420",
        "title": "Master Computer Science Application of deep learning in lead optimization for improving activity of drug molecules"
      },
      {
        "paperId": "463e3398310f9495a4dd7beb1185a858f1fe0d25",
        "title": "To explore drug space smarter: artificial intelligence in drug design for G protein-coupled receptors"
      },
      {
        "paperId": "8d3a2cb65b97c39bbf7b102feb7c095ee36f05b2",
        "title": "To explore drug space smarter: artificial intelligence in drug design for G protein-coupled receptors"
      }
    ],
    "score": 11.0
  },
  {
    "id": "442e9f1e8f6218e68f944fd3028c5385691d4112",
    "title": "Aggressive Quadrotor Flight Using Curiosity-Driven Reinforcement Learning",
    "authors": [
      "Qiyu Sun",
      "Jinbao Fang",
      "Weixing Zheng",
      "Yang Tang"
    ],
    "year": 2022,
    "citationCount": 32,
    "abstract": "The ability to perform aggressive movements,which are called aggressive flights, is important for quadrotors during navigation. However, aggressive quadrotor flights are still a great challenge to practical applications. The existing solutions to aggressive flights heavily rely on a predefined trajectory, which is a time-consuming preprocessing step. To avoid such path planning, we propose a curiosity-driven reinforcement learning method for aggressive flight missions and a similarity-based curiosity module is introduced to speed up the training procedure. A branch structure exploration strategy is also applied to guarantee the robustness of the policy and to ensure the policy trained in simulations can be performed in real-world experiments directly. The experimental results in simulations demonstrate that our reinforcement learning algorithm performs well in aggressive flight tasks, speeds up the convergence process and improves the robustness of the policy. Besides, our algorithm shows a satisfactory simulated to real transferability and performs well in real-world experiments.",
    "url": "https://www.semanticscholar.org/paper/442e9f1e8f6218e68f944fd3028c5385691d4112",
    "pdf_url": "https://arxiv.org/pdf/2203.14033.pdf",
    "venue": "IEEE transactions on industrial electronics (1982. Print)",
    "publicationDate": "2022-03-26",
    "externalIds": {
      "DBLP": "journals/corr/abs-2203-14033",
      "ArXiv": "2203.14033",
      "DOI": "10.1109/tie.2022.3144586",
      "CorpusId": 246329703
    },
    "references": [
      {
        "paperId": "3a5ac09e759f3223ee78b995ae2b519efc0f9292",
        "title": "Introduction to Reinforcement Learning"
      },
      {
        "paperId": "d02fc1e51604e180f6d098bb65d7fc7782c7cac8",
        "title": "An Online Trajectory Planning Approach for a Quadrotor UAV With a Slung Payload"
      },
      {
        "paperId": "2f45d8ce0781819ff3b364886f238a3f3e6d1c04",
        "title": "Deep Drone Acrobatics"
      },
      {
        "paperId": "44cc101931e03a991f0016146306d66179b9075e",
        "title": "DMP-Based Motion Generation for a Walking Exoskeleton Robot Using Reinforcement Learning"
      },
      {
        "paperId": "31d21edc3ed187c7f7bfa0aa16d185a9d45399c0",
        "title": "Dynamic obstacle avoidance for quadrotors with event cameras"
      },
      {
        "paperId": "771c35ccf70691636b527000abd8531e062a709b",
        "title": "Efficient Trajectory Planning for High Speed Flight in Unknown Environments"
      },
      {
        "paperId": "97a0e6974d6c610c48e65c94ce5b3364cd4cc538",
        "title": "FASTER: Fast and Safe Trajectory Planner for Flights in Unknown Environments"
      },
      {
        "paperId": "75fca92da207b950a83061536b8d8cb7ad1a2d33",
        "title": "Generalization through Simulation: Integrating Simulated and Real Data into Deep Reinforcement Learning for Vision-Based Autonomous Flight"
      },
      {
        "paperId": "a2bac02786e09a25a80732f738aaaee088d13e43",
        "title": "Low-Level Control of a Quadrotor With Deep Model-Based Reinforcement Learning"
      },
      {
        "paperId": "aee808d86105655b9106cc3d11ef2116504aaf11",
        "title": "Real-Time Planning with Multi-Fidelity Models for Agile Flights in Unknown Environments"
      },
      {
        "paperId": "fdfeeb14bbde2ab31b18e56b92d362dcd1b14f71",
        "title": "Episodic Curiosity through Reachability"
      },
      {
        "paperId": "beed8e2ca54e4421726b34888b22213a58f4b08d",
        "title": "Optimal Output Regulation for Model-Free Quanser Helicopter With Multistep Q-Learning"
      },
      {
        "paperId": "56136aa0b2c347cbcf3d50821f310c4253155026",
        "title": "Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models"
      },
      {
        "paperId": "4debb99c0c63bfaa97dd433bc2828e4dac81c48b",
        "title": "Addressing Function Approximation Error in Actor-Critic Methods"
      },
      {
        "paperId": "811df72e210e20de99719539505da54762a11c6d",
        "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor"
      },
      {
        "paperId": "1f8b463196b7b994604d02d9b7b2802cf2256ad9",
        "title": "Search-Based Motion Planning for Aggressive Flight in SE(3)"
      },
      {
        "paperId": "25da67d09fe34e9e3490215890c7bde9f7a1e316",
        "title": "Safe Local Exploration for Replanning in Cluttered Unknown Environments for Microaerial Vehicles"
      },
      {
        "paperId": "2bad0c28a95e17d1440d6618067b1fd70fd22bf1",
        "title": "Hawkeye: Open source framework for field surveillance"
      },
      {
        "paperId": "9e09cff6cd4c0cfbe9675cd1f0ae67c9c75a8ac9",
        "title": "Control of a Quadrotor With Reinforcement Learning"
      },
      {
        "paperId": "225ab689f41cef1dc18237ef5dab059a49950abf",
        "title": "Curiosity-Driven Exploration by Self-Supervised Prediction"
      },
      {
        "paperId": "06a78e6e6520befbbc1621ab058c1beb08f89f13",
        "title": "Data-Driven Optimal Consensus Control for Discrete-Time Multi-Agent Systems With Unknown Dynamics Using Reinforcement Learning Method"
      },
      {
        "paperId": "d23e8ccc64cdc763be11a32794666700dc79762b",
        "title": "Adaptive Impedance Control of Human\u2013Robot Cooperation Using Reinforcement Learning"
      },
      {
        "paperId": "32286b4d5e76ed37abf714e5024756577259d05b",
        "title": "Estimation, Control, and Planning for Aggressive Flight With a Small Quadrotor With a Single Camera and IMU"
      },
      {
        "paperId": "9b15ccd3bd2a636b31a03007c3a160c819434d9d",
        "title": "Robust Attitude Stabilization for Nonlinear Quadrotor Systems With Uncertainties and Delays"
      },
      {
        "paperId": "f2c20cb6ebd2ad704c5bcae4eb8b942d3c62f8e0",
        "title": "Uncertainty-Aware Reinforcement Learning for Collision Avoidance"
      },
      {
        "paperId": "edc2d374553d5832708f61a1024f7e84113a84f2",
        "title": "Aggressive quadrotor flight through narrow gaps with onboard sensing and computing using active vision"
      },
      {
        "paperId": "2b781df57bf0dc1c10406f842da5459564de1fce",
        "title": "Learning quadrotor dynamics using neural network for flight control"
      },
      {
        "paperId": "dc89af07733ac65b9f61ffcac846f9851065de35",
        "title": "Aggressive quadrotor flight using dense visual-inertial fusion"
      },
      {
        "paperId": "10b9283146b71ee5d38238eb3b30732a3d2bdbb2",
        "title": "Learning deep control policies for autonomous aerial vehicles with MPC-guided policy search"
      },
      {
        "paperId": "024006d4c2a89f7acacc6e4438d156525b60a98f",
        "title": "Continuous control with deep reinforcement learning"
      },
      {
        "paperId": "340f48901f72278f6bf78a04ee5b01df208cc508",
        "title": "Human-level control through deep reinforcement learning"
      },
      {
        "paperId": "449532187c94af3dd3aa55e16d2c50f7854d2199",
        "title": "Trust Region Policy Optimization"
      },
      {
        "paperId": "d2c72dd7df5d349b7daf6d10cd25de5983f4020e",
        "title": "Toward a Fully Autonomous UAV: Research Platform for Indoor and Outdoor Urban Search and Rescue"
      },
      {
        "paperId": "fc922cabc43fafea6669c94a403ff4e7892c5328",
        "title": "Collaborative mapping of an earthquake\u2010damaged building via ground and aerial robots"
      },
      {
        "paperId": "40ece6c806de825dc6e860efef062f35e65083e4",
        "title": "Nonlinear robust tracking control of a quadrotor UAV on SE(3)"
      },
      {
        "paperId": "d9c54dd53a5cdf8aa41cc9f4e2d8b0aa002d5523",
        "title": "A simple learning strategy for high-speed quadrocopter multi-flips"
      },
      {
        "paperId": "7fb7364cf5f9bd48a9dd3a92a5243aae128afc24",
        "title": "Exact indexing of dynamic time warping"
      },
      {
        "paperId": "bff20fb30adad8d1c173963089df5fc9664304f0",
        "title": "A Markovian Decision Process"
      },
      {
        "paperId": "6ebdf55cade577979515dc5d09620204a07e7c92",
        "title": "Using Simulation and Domain Adaptation to Improve Efficiency of Deep Robotic Grasping"
      },
      {
        "paperId": "26b8747eb4d7fb4d4fc45707606d5e969b9afb0c",
        "title": "Issues in Using Function Approximation for Reinforcement Learning"
      }
    ],
    "cited_by": [
      {
        "paperId": "f4f8af25db4f404957085df071972fe0ef2bad30",
        "title": "JuggleRL: Mastering Ball Juggling with a Quadrotor via Deep Reinforcement Learning"
      },
      {
        "paperId": "aa9610ee0642942f23fbf873ac2e9a8a3ee7c077",
        "title": "Continuous reinforcement learning via advantage value difference reward shaping: A proximal policy optimization perspective"
      },
      {
        "paperId": "8100df512df5ef11394f76630a9975bd4f6771f8",
        "title": "Reinforcement-learning empowered adaptive piezoelectric metamaterial for variable-frequency vibration attenuation"
      },
      {
        "paperId": "a10c3839a9f8c4a754f5b222e437f978f8996087",
        "title": "Transformer-based aerial robot tracking system in environments with wind disturbances"
      },
      {
        "paperId": "70762c699b32ad15212c9607f72dc049e050003c",
        "title": "FM-Planner: Foundation Model Guided Path Planning for Autonomous Drone Navigation"
      },
      {
        "paperId": "ef0e41b9bf9fcab08553d7fe73cb38c50b382772",
        "title": "Hierarchical Optimization Design for Autonomous Flight of Vision-Based Quadrotor Using Reinforcement Learning"
      },
      {
        "paperId": "cc744c82156d686966aed5560f68751eebbc4387",
        "title": "Finite memory output sliding mode control under the round-robin protocol: variable scheduling frequency"
      },
      {
        "paperId": "88a6c084166d944986d47e225a33a131741cba86",
        "title": "Sim-to-Real Transfer in Reinforcement Learning for Maneuver Control of a Variable-Pitch MAV"
      },
      {
        "paperId": "6f41f72a27ec9336c5b588416fa2c919f944d65c",
        "title": "When Embodied AI Meets Industry 5.0: Human-Centered Smart Manufacturing"
      },
      {
        "paperId": "7efb9cdc4045aa014288e9d61ac4ff43c533c9fa",
        "title": "VolleyBots: A Testbed for Multi-Drone Volleyball Game Combining Motion Control and Strategic Play"
      },
      {
        "paperId": "d947218a0a4eb62c121efca219360554bf9d6b2b",
        "title": "What Matters in Learning a Zero-Shot Sim-to-Real RL Policy for Quadrotor Control? A Comprehensive Study"
      },
      {
        "paperId": "702947f655ea506f4cf8f83f3c3bc6675f272d3f",
        "title": "Path Planning Based on Deep Reinforcement Learning in Unstructured Environment"
      },
      {
        "paperId": "7cd32c93b4364d8b590a55c77474c322c48d12c0",
        "title": "Knowledge guided fuzzy deep reinforcement learning"
      },
      {
        "paperId": "a51383f1daa80d07e87889d02e7fef4cce474d42",
        "title": "MAPPO and Hungarian Algorithm Based Collaborative Decision-Making for UAV Air Combat"
      },
      {
        "paperId": "ffcd84525b8f32c9eca8d115b9e542296f979158",
        "title": "Motion Configuration Planning Method of Morphing Quadrotor"
      },
      {
        "paperId": "40861c51301578a4ffd457266a28d9e191f14d32",
        "title": "Tracking Control with Uncertainty Smoothing Estimation under Aggressive Maneuvers of Aerial Vehicles"
      },
      {
        "paperId": "e1edaf090520f69133b951b0a5e78bbdc352552f",
        "title": "The Power of Input: Benchmarking Zero-Shot Sim-to-Real Transfer of Reinforcement Learning Control Policies for Quadrotor Control"
      },
      {
        "paperId": "da9c1d55874c80d971f5026d7383f947fdf432d2",
        "title": "Learning-Based Navigation and Collision Avoidance Through Reinforcement for UAVs"
      },
      {
        "paperId": "609dc7461e881b9c90d71203c4440886776fc39a",
        "title": "FedSwarm: An Adaptive Federated Learning Framework for Scalable AIoT"
      },
      {
        "paperId": "e1c97078e23547b8f44eb711be7384c995e7f197",
        "title": "A Hybrid Controller for Musculoskeletal Robots Targeting Lifting Tasks in Industrial Metaverse"
      },
      {
        "paperId": "476c6d75f9d7d0c19f1053d98be7e36ef310f0dd",
        "title": "Observer-Based Event-Triggered Iterative Learning Consensus for Locally Lipschitz Nonlinear MASs"
      },
      {
        "paperId": "ee4947406e067ca5a9df9d1106855a0e6fc439f6",
        "title": "Motion Planning and Control of a Morphing Quadrotor in Restricted Scenarios"
      },
      {
        "paperId": "25b1fb69a74efa70d8f0b44144f7410cd66d62ba",
        "title": "Exploring Deep Reinforcement Learning for Robust Target Tracking Using Micro Aerial Vehicles"
      },
      {
        "paperId": "3c2835ed8349d490e987bf11665e13829564ce94",
        "title": "Alternating-direction-method-of-multipliers-based fast model predictive control for an aerial trees-pruning robot"
      },
      {
        "paperId": "c73373af557ddeca8f56d3c6e4597fcb685716f6",
        "title": "Curiosity-Driven Reinforcement Learning based Low-Level Flight Control"
      },
      {
        "paperId": "e5ebea7de6ca2562fce5cd8b418c152593474406",
        "title": "Target Tracking Control of UAV Through Deep Reinforcement Learning"
      },
      {
        "paperId": "ed3b0575f5080700b8ed0f33d25bc909a4c759f4",
        "title": "Learning Real-Time Dynamic Responsive Gap-Traversing Policy for Quadrotors With Safety-Aware Exploration"
      },
      {
        "paperId": "a58a743f7e5d986ffc543a8d4ad8167948877aea",
        "title": "Learning Agile Flights Through Narrow Gaps with Varying Angles Using Onboard Sensing"
      },
      {
        "paperId": "d79c5e6d363c64c63be93a5e0520e35532560dbb",
        "title": "Event\u2010triggered prescribed performance robust collision\u2010free capturing control for drag\u2010free spacecraft system"
      },
      {
        "paperId": "cce1245ba1ec154120b3b256faf7bf28f769b505",
        "title": "A Novel Guided Deep Reinforcement Learning Tracking Control Strategy for Multirotors"
      },
      {
        "paperId": "c5b8cc4a05701a6c30d20e193a3bfe6e1c71d757",
        "title": "Digital Twins and Control Theory: A Critical Review on Revolutionizing Quadrotor UAVs"
      },
      {
        "paperId": "14e2e05c68ecd10c9c81d713114a272ce0bac8c1",
        "title": "Quadrotor navigation considering attitude: A deep reinforcement learning method using tangent path rewards"
      }
    ],
    "score": 10.666666666666666
  },
  {
    "id": "a25b645c3d24f91164230a0ac5bb2d4ec88c1538",
    "title": "Information-Directed Exploration for Deep Reinforcement Learning",
    "authors": [
      "Nikolay Nikolov",
      "Johannes Kirschner",
      "Felix Berkenkamp",
      "Andreas Krause"
    ],
    "year": 2018,
    "citationCount": 74,
    "abstract": "Efficient exploration remains a major challenge for reinforcement learning. One reason is that the variability of the returns often depends on the current state and action, and is therefore heteroscedastic. Classical exploration strategies such as upper confidence bound algorithms and Thompson sampling fail to appropriately account for heteroscedasticity, even in the bandit setting. Motivated by recent findings that address this issue in bandits, we propose to use Information-Directed Sampling (IDS) for exploration in reinforcement learning. As our main contribution, we build on recent advances in distributional reinforcement learning and propose a novel, tractable approximation of IDS for deep Q-learning. The resulting exploration strategy explicitly accounts for both parametric uncertainty and heteroscedastic observation noise. We evaluate our method on Atari games and demonstrate a significant improvement over alternative approaches.",
    "url": "https://www.semanticscholar.org/paper/a25b645c3d24f91164230a0ac5bb2d4ec88c1538",
    "pdf_url": "https://arxiv.org/pdf/1812.07544.pdf",
    "venue": "International Conference on Learning Representations",
    "publicationDate": "2018-09-27",
    "externalIds": {
      "ArXiv": "1812.07544",
      "DBLP": "conf/iclr/NikolovKBK19",
      "MAG": "2950506635",
      "CorpusId": 56177829
    },
    "references": [
      {
        "paperId": "3a5ac09e759f3223ee78b995ae2b519efc0f9292",
        "title": "Introduction to Reinforcement Learning"
      },
      {
        "paperId": "57ffba5ea5e602b5365752e19a6f85a716db1d8b",
        "title": "Bandit Algorithms"
      },
      {
        "paperId": "5d7ffe235578dcbdce50f4447b8f436aef308f7e",
        "title": "Deep Reinforcement Learning with Risk-Seeking Exploration"
      },
      {
        "paperId": "d85623ffae865f9ef386644dd02d0ea2d6a8c8de",
        "title": "Implicit Quantile Networks for Distributional Reinforcement Learning"
      },
      {
        "paperId": "85d593d5ad0f1b864c47e59309ae313ce0434a72",
        "title": "The Potential of the Return Distribution for Exploration in RL"
      },
      {
        "paperId": "3995b02fb3d4be47de8cc1a178fe577a445ac5e7",
        "title": "Exploration by Distributional Reinforcement Learning"
      },
      {
        "paperId": "93a636f0b6d450217fda5aaa26c74bb6b232f498",
        "title": "Deep Bayesian Bandits Showdown: An Empirical Comparison of Bayesian Deep Networks for Thompson Sampling"
      },
      {
        "paperId": "3f5eefd759da6e85feb55134f5ad7b1f4af8ee3d",
        "title": "Efficient Exploration Through Bayesian Deep Q-Networks"
      },
      {
        "paperId": "4e1cff5dee4671db7138a938469a2d062cb5f025",
        "title": "Information Directed Sampling and Bandits with Heteroscedastic Noise"
      },
      {
        "paperId": "6f40b8d2e73ec0bcc4d12ec2c7c8387d53c7727b",
        "title": "Efficient exploration with Double Uncertain Value Networks"
      },
      {
        "paperId": "fe3e91e40a950c6b6601b8f0a641884774d949ae",
        "title": "Distributional Reinforcement Learning with Quantile Regression"
      },
      {
        "paperId": "bdf6572b67a6c5d8aacd39e1826db2c5c8f85716",
        "title": "The Uncertainty Bellman Equation and Exploration"
      },
      {
        "paperId": "c1f4ef741242d629d1f56e442a09a7ba29595a0e",
        "title": "A Distributional Perspective on Reinforcement Learning"
      },
      {
        "paperId": "3fab4d28be5ff73f42c8b307117aa8d680345ad9",
        "title": "Noisy Networks for Exploration"
      },
      {
        "paperId": "142497432fe179ddb6ffe600c64a837ec6179550",
        "title": "Parameter Space Noise for Exploration"
      },
      {
        "paperId": "0fcb2034e31a2bc2f12a2b1363d0d77baf445fdf",
        "title": "#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning"
      },
      {
        "paperId": "6e90fd78e8a3b98af3954aae5209703aa966603e",
        "title": "Unifying Count-Based Exploration and Intrinsic Motivation"
      },
      {
        "paperId": "317cd4522b1f4a6f889743578143bb8823623f8b",
        "title": "VIME: Variational Information Maximizing Exploration"
      },
      {
        "paperId": "4b63e34276aa98d5345efa7fe09bb06d8a9d8f52",
        "title": "Deep Exploration via Bootstrapped DQN"
      },
      {
        "paperId": "2a91c778288f67c856041447f350a1a022fc6554",
        "title": "A Variational Analysis of Stochastic Gradient Algorithms"
      },
      {
        "paperId": "4c05d7caa357148f0bbd61720bdd35f0bc05eb81",
        "title": "Dueling Network Architectures for Deep Reinforcement Learning"
      },
      {
        "paperId": "c6170fa90d3b2efede5a2e1660cb23e1c824f2ca",
        "title": "Prioritized Experience Replay"
      },
      {
        "paperId": "3b9732bb07dc99bde5e1f9f75251c6ea5039373e",
        "title": "Deep Reinforcement Learning with Double Q-Learning"
      },
      {
        "paperId": "024006d4c2a89f7acacc6e4438d156525b60a98f",
        "title": "Continuous control with deep reinforcement learning"
      },
      {
        "paperId": "d5ed07113ddcd038062525a5a54550c012ac9a74",
        "title": "Massively Parallel Methods for Deep Reinforcement Learning"
      },
      {
        "paperId": "62adfea3cc1cd9eb6b53e0e8a40be5dfda2adf8d",
        "title": "Weight Uncertainty in Neural Network"
      },
      {
        "paperId": "2470fcf0f89082de874ac9133ccb3a8667dd89a8",
        "title": "Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models"
      },
      {
        "paperId": "f35de4f9b1a7c4d3fa96a0d2ab1bf8937671f6b6",
        "title": "Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning"
      },
      {
        "paperId": "da6057368920585bcf2443295b98418840f1fc80",
        "title": "Weight Uncertainty in Neural Networks"
      },
      {
        "paperId": "340f48901f72278f6bf78a04ee5b01df208cc508",
        "title": "Human-level control through deep reinforcement learning"
      },
      {
        "paperId": "93bc65d2842b8cc5f3cf72ebc5b8f75daeacea35",
        "title": "Scalable Bayesian Optimization Using Deep Neural Networks"
      },
      {
        "paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8",
        "title": "Adam: A Method for Stochastic Optimization"
      },
      {
        "paperId": "f770614e497f456cfbe310bf7fd5223a4c28edd7",
        "title": "Learning to Optimize via Information-Directed Sampling"
      },
      {
        "paperId": "1389772b8a0f9c7fc43057f9da41a7d0ebf0308b",
        "title": "Generalization and Exploration via Randomized Value Functions"
      },
      {
        "paperId": "f82e4ff4f003581330338aaae71f60316e58dd26",
        "title": "The Arcade Learning Environment: An Evaluation Platform for General Agents"
      },
      {
        "paperId": "ab867c140d2947511979c87e7ae580d9d3f0aeab",
        "title": "An Empirical Evaluation of Thompson Sampling"
      },
      {
        "paperId": "aeed631d6a84100b5e9a021ec1914095c66de415",
        "title": "Bayesian Learning via Stochastic Gradient Langevin Dynamics"
      },
      {
        "paperId": "33224ad0cdf6e2dc4893194dd587309c7887f0ba",
        "title": "Formal Theory of Creativity, Fun, and Intrinsic Motivation (1990\u20132010)"
      },
      {
        "paperId": "714c6edbebe8a1bc6a85bf2b30603ab8196005b1",
        "title": "Bandit Problems"
      },
      {
        "paperId": "e41ba5dc12c79a64dfa905c0328f95976252ffe0",
        "title": "The Elements of Statistical Learning"
      },
      {
        "paperId": "1e1d35136b1bf3b13ef6b53f6039f39d9ee820e3",
        "title": "Finite-time Analysis of the Multiarmed Bandit Problem"
      },
      {
        "paperId": "59b50a775542e87f078db35b868ac10ab43d4c75",
        "title": "Learning from delayed rewards"
      },
      {
        "paperId": "4c9e36a0dbdc3e7b20e38bd288a804c05c77e9fa",
        "title": "Asymptotically efficient adaptive allocation rules"
      },
      {
        "paperId": "ee2cd1d17f833d3c157a1016a778c7c22af555a2",
        "title": "ON THE LIKELIHOOD THAT ONE UNKNOWN PROBABILITY EXCEEDS ANOTHER IN VIEW OF THE EVIDENCE OF TWO SAMPLES"
      },
      {
        "paperId": "a428976111d7d0404f00b90809532d1737ed9c54",
        "title": "Information Directed reinforcement learning"
      },
      {
        "paperId": null,
        "title": "UCB and infogain exploration via q-ensembles"
      },
      {
        "paperId": "360ca02e6f5a5e1af3dce4866a257aafc2d6d6f5",
        "title": "Machine learning - a probabilistic perspective"
      },
      {
        "paperId": null,
        "title": "Gaussian process optimization in the bandit setting: No regret and experimental design"
      },
      {
        "paperId": "ea825c57ffebc7065f5279165e6fc261ac317691",
        "title": "Bootstrap Methods: Another Look at the Jackknife"
      },
      {
        "paperId": "610483cd70574d41979c4f1d6f53c29b8e0b5944",
        "title": "A Survey of Partially Observable Markov Decision Processes: Theory, Models, and Algorithms"
      },
      {
        "paperId": "a22b36cf5dba3e85eb064220be7ef03be4efba48",
        "title": "Bayesian learning for neural networks"
      },
      {
        "paperId": null,
        "title": "Double Dunk -18"
      },
      {
        "paperId": null,
        "title": "Name This Game 2"
      }
    ],
    "cited_by": [
      {
        "paperId": "22b711eb36f408251070c31ae5107fa79375ef82",
        "title": "Uncertainty-driven Adaptive Exploration"
      },
      {
        "paperId": "845e7b768dd3b846be8e2aa5a001d57fcc635a90",
        "title": "Uncertainty Prioritized Experience Replay"
      },
      {
        "paperId": "2b99f1383a719b8ffc6c4a8b5b2b7cba12a6ec32",
        "title": "Universal Value-Function Uncertainties"
      },
      {
        "paperId": "fa16a98a58bf4926ccdf3e1981982bb727f613f2",
        "title": "Contextual Similarity Distillation: Ensemble Uncertainties with a Single Model"
      },
      {
        "paperId": "55a7e9d38c65e89033da3f10a2f39e0f013803a9",
        "title": "Empirical Bound Information-Directed Sampling for Norm-Agnostic Bandits"
      },
      {
        "paperId": "3a0a898a7205663fd19e37700fcc2c1c698edaf5",
        "title": "Optimal Transport-Guided Safety in Temporal Difference Reinforcement Learning"
      },
      {
        "paperId": "e4fef8d5864c5468100ca167639ef3fa374c0442",
        "title": "MaxInfoRL: Boosting exploration in reinforcement learning through information gain maximization"
      },
      {
        "paperId": "25099f9290c018801c1f379c47c92cb8d7ace443",
        "title": "Learning to Assist Humans without Inferring Rewards"
      },
      {
        "paperId": "37dff301fbb396d4610eb396a9ebf0bab12ab0b9",
        "title": "Information-directed policy sampling for episodic Bayesian Markov decision processes"
      },
      {
        "paperId": "46b6a615baf77306611f77ecf6dfe35d611eab59",
        "title": "Directed Exploration in Reinforcement Learning from Linear Temporal Logic"
      },
      {
        "paperId": "db9983fe995c19bdb4603f1dd349fad17f29df46",
        "title": "Model-Free Active Exploration in Reinforcement Learning"
      },
      {
        "paperId": "1a83ba3e17da7c18058c78b9c96c489cc7f62e09",
        "title": "Reinforcement Learning with Intrinsically Motivated Feedback Graph for Lost-sales Inventory Control"
      },
      {
        "paperId": "203e515826b759dc9c81fe83735ab728b9015937",
        "title": "Provably Efficient Information-Directed Sampling Algorithms for Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "85e3fbeb7b90ac71a2912870273c4928c457bdb1",
        "title": "Pessimistic value iteration for multi-task data sharing in Offline Reinforcement Learning"
      },
      {
        "paperId": "91edf9d897ddfa4c39f013d6acc0f2a0905ef789",
        "title": "OVD-Explorer: Optimism Should Not Be the Sole Pursuit of Exploration in Noisy Environments"
      },
      {
        "paperId": "4ee251bb48d0a0482ad08fe5aff2624bdee0b931",
        "title": "Closed-Loop Uncertainty: The Evaluation and Calibration of Uncertainty for Human\u2013Machine Teams under Data Drift"
      },
      {
        "paperId": "01df7c46ec1dc84da5e3c525b2a992f502d31364",
        "title": "Thompson sampling for improved exploration in GFlowNets"
      },
      {
        "paperId": "a5187bbf4bf8bd905dbbc70a60d606ca54408d51",
        "title": "Diverse Projection Ensembles for Distributional Reinforcement Learning"
      },
      {
        "paperId": "433eb27e9853d276bf119c70b027b4c7fbc7dafd",
        "title": "Seizing Serendipity: Exploiting the Value of Past Success in Off-Policy Actor-Critic"
      },
      {
        "paperId": "fd3f34a37e3963d3c28b6d389ce4448ff9fd0c33",
        "title": "A Unified Uncertainty-Aware Exploration: Combining Epistemic and Aleatory Uncertainty"
      },
      {
        "paperId": "cdbe0964ec7d90555c9f08d8d4f0c5b6688293a5",
        "title": "A Unified Framework for Factorizing Distributional Value Functions for Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "c99db0656d62e185486da6bcba6485f182307811",
        "title": "ReLU to the Rescue: Improve Your On-Policy Actor-Critic with Positive Advantages"
      },
      {
        "paperId": "54952feedd88ca743dc6b01cfbc707d9254bbdb3",
        "title": "Latent Exploration for Reinforcement Learning"
      },
      {
        "paperId": "fea10e129e19eb3afe16a9040164535ce2c4895a",
        "title": "AID-RL: Active information-directed reinforcement learning for autonomous source seeking and estimation"
      },
      {
        "paperId": "0670b6552f53d2ed8b56a712bb0db4783138123a",
        "title": "Exploration via Epistemic Value Estimation"
      },
      {
        "paperId": "7e6b0600c15c8894117df4dab96a3f57a8605e54",
        "title": "Toward Risk-based Optimistic Exploration for Cooperative Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "296f99cbf0425b770cd3f32119e2a9355a937b19",
        "title": "Linear Partial Monitoring for Sequential Decision-Making: Algorithms, Regret Bounds and Applications"
      },
      {
        "paperId": "4da360c81092ecd90ef87d63cc899bc59e6094c4",
        "title": "Multi-Robot Learning Dynamic Obstacle Avoidance in Formation With Information-Directed Exploration"
      },
      {
        "paperId": "0974ff20cf247785819d2d77d422c78b24bdc694",
        "title": "MEET: A Monte Carlo Exploration-Exploitation Trade-Off for Buffer Sampling"
      },
      {
        "paperId": "3aa3b2aa47f4eeb9373aef0e6b2b1a545d0e94b8",
        "title": "Exploration via Planning for Information about the Optimal Trajectory"
      },
      {
        "paperId": "7300ef17cf35c5d2c830729dab6efc8c302b5d99",
        "title": "Planning lunar In-Situ Resource Utilisation with a reinforcement learning agent"
      },
      {
        "paperId": "104213c5ead6ed90419aa230ee2f73ce8f793a5a",
        "title": "Some Supervision Required: Incorporating Oracle Policies in Reinforcement Learning via Epistemic Uncertainty Metrics"
      },
      {
        "paperId": "4b1489711d6fbbd16c25ec5d6ac4d34df4003464",
        "title": "Distributional Actor-Critic Ensemble for Uncertainty-Aware Continuous Control"
      },
      {
        "paperId": "874349d487c3ebe51fb139c0e22bfa56a4b1f9eb",
        "title": "Regret Bounds for Information-Directed Reinforcement Learning"
      },
      {
        "paperId": "4bceb9f2162d6931ccf704002d71fcc813d36689",
        "title": "Disentangling Epistemic and Aleatoric Uncertainty in Reinforcement Learning"
      },
      {
        "paperId": "8e25eaa421da5042884105b51a0b6009c152d7f3",
        "title": "Uncertainty-Aware Portfolio Management With Risk-Sensitive Multiagent Network"
      },
      {
        "paperId": "85cd1064f7b82d52c1864d35223d5f45144721ff",
        "title": "From Dirichlet to Rubin: Optimistic Exploration in RL without Bonuses"
      },
      {
        "paperId": "bfcccf1f9f709884e8b0d2c119582171915665cb",
        "title": "Nonstationary Bandit Learning via Predictive Sampling"
      },
      {
        "paperId": "da1e404ae194dcbcce198f5335f49f090403bbd7",
        "title": "Exploration in Deep Reinforcement Learning: A Survey"
      },
      {
        "paperId": "23abe79046d7f7b430d5d21b6a93598d0aa1b9c2",
        "title": "Pessimistic Bootstrapping for Uncertainty-Driven Offline Reinforcement Learning"
      },
      {
        "paperId": "6e0c6f6b5355700e53e239655387e6fc6c029972",
        "title": "Occupancy Information Ratio: Infinite-Horizon, Information-Directed, Parameterized Policy Search"
      },
      {
        "paperId": "98745c73afc4c1cd173522f05a209e62fb4449a4",
        "title": "Gaussian Imagination in Bandit Learning"
      },
      {
        "paperId": "ca6ff8ad01d37a223b49b2db0eb692c4f42ffa51",
        "title": "E xploration E xploitation Problem in Policy Based Deep Reinforcement Learning for Episodic and Continuous Environments"
      },
      {
        "paperId": "2bd2080efe63afd1fd0c17e04b80b79166ec5aa7",
        "title": "An Experimental Design Perspective on Model-Based Reinforcement Learning"
      },
      {
        "paperId": "3f9f9f9fa1a0d8fb2d169aae2d81ed51ea0b0a1a",
        "title": "Estimating the Variance of Return Sequences for Exploration"
      },
      {
        "paperId": "82419167f0d6ed66f2137b6c653e88c872c8756d",
        "title": "The Value of Information When Deciding What to Learn"
      },
      {
        "paperId": "12075ea34f5fbe32ec5582786761ab34d401209b",
        "title": "Exploration in Deep Reinforcement Learning: From Single-Agent to Multiagent Domain"
      },
      {
        "paperId": "7c742e1c8f28c949631d5d863019470844478912",
        "title": "Meta-Reinforcement Learning With Dynamic Adaptiveness Distillation"
      },
      {
        "paperId": "00a6cb7d4ede92cd4da68243b4c212faf58c3e4b",
        "title": "Disentangling What and Where for 3D Object-Centric Representations Through Active Inference"
      },
      {
        "paperId": "91af6093fff7dda4a2b5621eed8957cf441424b6",
        "title": "Information-Directed Exploration via Distributional Deep Reinforcement Learning"
      },
      {
        "paperId": "10ac0ad7595e1e0299e052b24d3fc75bc1bc340c",
        "title": "GMAC: A Distributional Perspective on Actor-Critic Framework"
      },
      {
        "paperId": "31d1cc6aa3a04bc524b40b089087c8ccffab35df",
        "title": "Sequential Generative Exploration Model for Partially Observable Reinforcement Learning"
      },
      {
        "paperId": "6d2f554fbd24715c2268d64f90b53a5f19044774",
        "title": "Principled Exploration via Optimistic Bootstrapping and Backward Induction"
      },
      {
        "paperId": "da45e961f285fdba9aefb3f4d4270620044eccb3",
        "title": "Reinforcement Learning, Bit by Bit"
      },
      {
        "paperId": "3098e236a15908743cea2e83ca07d1992e7c0f6c",
        "title": "DFAC Framework: Factorizing the Value Function via Quantile Mixture for Multi-Agent Distributional Q-Learning"
      },
      {
        "paperId": "ec05bd6725ac6a5217021881cac8553581b3e313",
        "title": "Measuring Progress in Deep Reinforcement Learning Sample Efficiency"
      },
      {
        "paperId": "6b5adfa3cd172198c48997635d633579a6123eea",
        "title": "Leveraging the Variance of Return Sequences for Exploration Policy"
      },
      {
        "paperId": "e2cac0f861832a988e52506f1e973a14f90d9647",
        "title": "Bridging Imagination and Reality for Model-Based Deep Reinforcement Learning"
      },
      {
        "paperId": "f4595217d578036d18379478855fd4350ed43e53",
        "title": "On the Sample Complexity of Reinforcement Learning with Policy Space Generalization"
      },
      {
        "paperId": "69efbf9ddee489b3bdc10a278d3cd375abbb0d54",
        "title": "Hypermodels for Exploration"
      },
      {
        "paperId": "0d6fcc24621f2f3be1785c4b864d018e25126f1f",
        "title": "Segregation dynamics with reinforcement learning and agent based modeling"
      },
      {
        "paperId": "6379c473d44c63a69f0f980266376d8cd96b3ba9",
        "title": "Dueling Posterior Sampling for Preference-Based Reinforcement Learning"
      },
      {
        "paperId": "81768cdace11e14982d3aba9060059e1133d83fc",
        "title": "Learning Efficient and Effective Exploration Policies with Counterfactual Meta Policy"
      },
      {
        "paperId": "d6285ff3dcb15c1da84dcbc98141be10ef0e8dd1",
        "title": "Estimating Risk and Uncertainty in Deep Reinforcement Learning"
      },
      {
        "paperId": "95eae6bee4af291a572c40517e1aa7c23c5d6d63",
        "title": "Sample-Efficient Model-Free Reinforcement Learning with Off-Policy Critics"
      },
      {
        "paperId": "72a75aeab2c918394dc9af2408fd8e1076ae39ac",
        "title": "Successor Uncertainties: exploration and uncertainty in temporal difference learning"
      },
      {
        "paperId": "0fc3b453f796fedc93f4c667e71e73838b134505",
        "title": "Model-based Policy Optimization under Approximate Bayesian Inference"
      },
      {
        "paperId": "941e31df751884e8319d275ecaa5697407001347",
        "title": "Uncertainty-Aware Rank-One MIMO Q Network Framework for Accelerated Offline Reinforcement Learning"
      },
      {
        "paperId": "7db22589c2151d3f4c5c24b038154d00c51308f8",
        "title": "Exploration and Communication for Partially Observable Collaborative Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "2e5dc4b028e670d1b9ac0b4e15580cdb7197cb68",
        "title": "Exploration in Deep Reinforcement Learning: A Comprehensive Survey"
      },
      {
        "paperId": "46b4cf516bb32f22b8061fdd750dee84969aaeab",
        "title": "Optimistic Exploration with Backward Bootstrapped Bonus for Deep Reinforcement Learning"
      },
      {
        "paperId": "cf28c18a733810bb7f50a2fc43e0e57867da7656",
        "title": "A Distributional Perspective on Value Function Factorization Methods for Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "2e0a33fe4c04f7f57023e8afaa09cc6261431bcb",
        "title": "Cost-Aware Bayesian Optimization via Information Directed Sampling"
      },
      {
        "paperId": "e2adb8231150968a8a41216f662f852eeb1685d7",
        "title": "SEQUENCE-LEVEL INTRINSIC EXPLORATION MODEL"
      }
    ],
    "score": 10.571428571428571
  },
  {
    "id": "2fc993c78cd84dafe4c36d2ac1cc25a6256aaa9d",
    "title": "Sample-Efficient Reinforcement Learning with loglog(T) Switching Cost",
    "authors": [
      "Dan Qiao",
      "Ming Yin",
      "Ming Min",
      "Yu-Xiang Wang"
    ],
    "year": 2022,
    "citationCount": 31,
    "abstract": "We study the problem of reinforcement learning (RL) with low (policy) switching cost - a problem well-motivated by real-life RL applications in which deployments of new policies are costly and the number of policy updates must be low. In this paper, we propose a new algorithm based on stage-wise exploration and adaptive policy elimination that achieves a regret of $\\widetilde{O}(\\sqrt{H^4S^2AT})$ while requiring a switching cost of $O(HSA \\log\\log T)$. This is an exponential improvement over the best-known switching cost $O(H^2SA\\log T)$ among existing methods with $\\widetilde{O}(\\mathrm{poly}(H,S,A)\\sqrt{T})$ regret. In the above, $S,A$ denotes the number of states and actions in an $H$-horizon episodic Markov Decision Process model with unknown transitions, and $T$ is the number of steps. As a byproduct of our new techniques, we also derive a reward-free exploration algorithm with a switching cost of $O(HSA)$. Furthermore, we prove a pair of information-theoretical lower bounds which say that (1) Any no-regret algorithm must have a switching cost of $\\Omega(HSA)$; (2) Any $\\widetilde{O}(\\sqrt{T})$ regret algorithm must incur a switching cost of $\\Omega(HSA\\log\\log T)$. Both our algorithms are thus optimal in their switching costs.",
    "url": "https://www.semanticscholar.org/paper/2fc993c78cd84dafe4c36d2ac1cc25a6256aaa9d",
    "pdf_url": "https://arxiv.org/pdf/2202.06385.pdf",
    "venue": "International Conference on Machine Learning",
    "publicationDate": "2022-02-13",
    "externalIds": {
      "DBLP": "conf/icml/QiaoYM022",
      "ArXiv": "2202.06385",
      "CorpusId": 246823870
    },
    "references": [
      {
        "paperId": "ff5abca5cb5c193a19c2cebad8f99e3d9cb928d1",
        "title": "Towards Deployment-Efficient Reinforcement Learning: Lower Bound and Optimality"
      },
      {
        "paperId": "5685abf9e7bb2c16449ae1eb181051e503602a55",
        "title": "Reinforcement Learning based Recommender Systems: A Survey"
      },
      {
        "paperId": "157c830c85fc7ee4aa360c72fa7bb9426de5f5b2",
        "title": "Provably Efficient Reinforcement Learning with Linear Function Approximation Under Adaptivity Constraints"
      },
      {
        "paperId": "922a77d3779b1780d5a7b1f18405ddbfc7397c49",
        "title": "A Provably Efficient Algorithm for Linear Markov Decision Process with Low Switching Cost"
      },
      {
        "paperId": "c622997e1ea2bd7128d6ec6d78eea57bc49e78a9",
        "title": "Nearly Minimax Optimal Reward-free Reinforcement Learning"
      },
      {
        "paperId": "561919be6df0a3aa40104773eea19a5b12e47e50",
        "title": "Provably Efficient Reward-Agnostic Navigation with Linear Value Iteration"
      },
      {
        "paperId": "576c7b003427d6906e3b58a0139b4d83301909fd",
        "title": "Fast active learning for pure exploration in reinforcement learning"
      },
      {
        "paperId": "3eebdbffbc05d6b2987010c4d594cfc11b9b5071",
        "title": "On Reward-Free Reinforcement Learning with Linear Function Approximation"
      },
      {
        "paperId": "4a4ef182464b5ed53561d6eed0c173935c054f88",
        "title": "Task-agnostic Exploration in Reinforcement Learning"
      },
      {
        "paperId": "669f07843065e471974080608a3cbee876d13cef",
        "title": "Adaptive Reward-Free Exploration"
      },
      {
        "paperId": "79ebde314ab90d066cee3b82193ef05666323394",
        "title": "Deployment-Efficient Reinforcement Learning via Model-Based Offline Optimization"
      },
      {
        "paperId": "8046dbd4ebc3fda0fcc43d9110c0d2d940052980",
        "title": "Almost Optimal Model-Free Reinforcement Learning via Reference-Advantage Decomposition"
      },
      {
        "paperId": "90f9ef793c1e4000ee523e1f8cea501fb8806fa0",
        "title": "Reward-Free Exploration for Reinforcement Learning"
      },
      {
        "paperId": "2c9c8df8e097ff06ae5741ad35f520110f264549",
        "title": "Regret Bounds for Batched Bandits"
      },
      {
        "paperId": "222baa4e9e7ce691fdfddbc826a70e027daed70d",
        "title": "Reinforcement Learning in Healthcare: A Survey"
      },
      {
        "paperId": "d423fa6cd0f4d088941d9fe4bebd834d0137c9b9",
        "title": "Provably Efficient Reinforcement Learning with Linear Function Approximation"
      },
      {
        "paperId": "6101d16008e536740adb97588616af51fe392950",
        "title": "Provably Efficient Q-Learning with Low Switching Cost"
      },
      {
        "paperId": "f3d64e57043f855f6b7b5bf83358ffe77b6ab4e7",
        "title": "Batched Multi-armed Bandits Problem"
      },
      {
        "paperId": "f14ea2243fe74cbf1a4ea86cebed1fb9547c3a7c",
        "title": "Tighter Problem-Dependent Regret Bounds in Reinforcement Learning without Domain Knowledge using Value Function Bounds"
      },
      {
        "paperId": "cd4ee7825ea974f0fbb69445d403c9dde54f6a04",
        "title": "Policy Certificates: Towards Accountable Reinforcement Learning"
      },
      {
        "paperId": "ba7bf6e857da202314176895ec2bc47d330256e2",
        "title": "Learning to Optimize Join Queries With Deep Reinforcement Learning"
      },
      {
        "paperId": "03cc81e98942bdafd994af7a1d1e62a68ff8b682",
        "title": "Is Q-learning Provably Efficient?"
      },
      {
        "paperId": "ec1d30b1fba63149f0ac63d3094585172caeb864",
        "title": "Experience-driven Networking: A Deep Reinforcement Learning based Approach"
      },
      {
        "paperId": "7586bdad167178c32271000b5914944bae9a6dc0",
        "title": "Posterior sampling for reinforcement learning: worst-case regret bounds"
      },
      {
        "paperId": "67d10cea808937089d6492acff14ad9ef156e3c5",
        "title": "Minimax Regret Bounds for Reinforcement Learning"
      },
      {
        "paperId": "5dcc07acb63cc909c5be701c1c88fef3718ba326",
        "title": "Unifying PAC and Regret: Uniform PAC Bounds for Episodic Reinforcement Learning"
      },
      {
        "paperId": "765d93759b7888fa1f7b2f3576809ad558c60caf",
        "title": "Machine-learning-assisted materials discovery using failed experiments"
      },
      {
        "paperId": "8f9259dcd94192b830b6d24f22eed710d01e1fbf",
        "title": "Batched Bandit Problems"
      },
      {
        "paperId": "789783016fb708abbc061790612ebe91273c05d3",
        "title": "(More) Efficient Reinforcement Learning via Posterior Sampling"
      },
      {
        "paperId": "8a1d0d8f2cce1a180c6b41c733262fe81ce35a9c",
        "title": "Online Learning with Switching Costs and Other Adaptive Adversaries"
      },
      {
        "paperId": "6e71a24fc0bba4a712b89dd9ff87a452230c2c4b",
        "title": "Empirical Bernstein Bounds and Sample-Variance Penalization"
      },
      {
        "paperId": "0cafe2903b097fc042782c359cb231ea34ef7ed3",
        "title": "Near-optimal Regret Bounds for Reinforcement Learning"
      },
      {
        "paperId": "dc649486b881e672eea6546da48c46e1f98daf32",
        "title": "Near-Optimal Reinforcement Learning in Polynomial Time"
      },
      {
        "paperId": "1e1d35136b1bf3b13ef6b53f6039f39d9ee820e3",
        "title": "Finite-time Analysis of the Multiarmed Bandit Problem"
      },
      {
        "paperId": null,
        "title": "\u00d5((S2AH2/\u03b52) sample complexity under traditional setting where rh \u2208 [0, 1], this result matches the lower bound"
      },
      {
        "paperId": null,
        "title": "2020a) designed an algorithm: UCB-Zero that finds \u03b5-optimal policies for N arbitrary tasks after at most \u00d5(H5SA logN/\u03b52) exploration episodes. A concurrent work"
      },
      {
        "paperId": null,
        "title": "2020b] considered a more general setting with stationary transition"
      },
      {
        "paperId": null,
        "title": "g,G\u2032) and Y , Y i related to this tuple, we have that Y i\u2019s are i.i.d samples from the distribution of Y . Same to the proof in [Jin et"
      },
      {
        "paperId": null,
        "title": "2020a] first studied the problem of reward-free exploration"
      },
      {
        "paperId": null,
        "title": "2020] designed an algorithm RF-UCRL by building upper confidence bound for any reward function and any policy, their algorithm needs of order \u00d5((S2AH4/ 2) episodes"
      },
      {
        "paperId": "93c3a7e718d836002dd092f96269e48cef218cf7",
        "title": "L G ] 2 6 M ay 2 01 9 Phase Transitions and Cyclic Phenomena in Bandits with Switching Constraints"
      },
      {
        "paperId": null,
        "title": "The result is generalized to K-armed bandit"
      },
      {
        "paperId": null,
        "title": "2019) proved that for any algorithm with \u00d5"
      },
      {
        "paperId": "c7dbf5ed7e9b63e104adb4e18bfbc98f5d6afdae",
        "title": "R-MAX - A General Polynomial Time Algorithm for Near-Optimal Reinforcement Learning"
      },
      {
        "paperId": null,
        "title": "Step 2. Fine exploration Explore each h, s, a with the crude estimate of the absorbing MDP. Construct a more re\ufb01ned estimate ( (cid:98) P ) of the absorbing MDP\u2019s (cid:101)"
      }
    ],
    "cited_by": [
      {
        "paperId": "9610a326a4c48bc8e7099ded8c853d86834e72de",
        "title": "A Tutorial: An Intuitive Explanation of Offline Reinforcement Learning Theory"
      },
      {
        "paperId": "95148ebccb6062606261ce17199b3326fa601620",
        "title": "Regret-Optimal Q-Learning with Low Cost for Single-Agent and Federated Reinforcement Learning"
      },
      {
        "paperId": "dab91fc8eddb34879b1014b69945dc6575ba1742",
        "title": "Gap-Dependent Bounds for Federated Q-learning"
      },
      {
        "paperId": "3484948573f0a1d5627036a7691842447eaef14d",
        "title": "Secure Reinforcement Learning via Shuffle Privacy Model"
      },
      {
        "paperId": "b292198873f1fbec82f97306692e18308d706c8e",
        "title": "Learning in Markov Games with Adaptive Adversaries: Policy Regret, Fundamental Barriers, and Efficient Algorithms"
      },
      {
        "paperId": "0b94bb75aacb78afb24915bdc5943cdaad349c09",
        "title": "Federated UCBVI: Communication-Efficient Federated Regret Minimization with Heterogeneous Agents"
      },
      {
        "paperId": "cf719230aea682499c43580a40f8fbb6c4181d97",
        "title": "Gap-Dependent Bounds for Q-Learning using Reference-Advantage Decomposition"
      },
      {
        "paperId": "ab954896c78f4c1136e3178ef08a3316d996d316",
        "title": "To Switch or Not to Switch? Balanced Policy Switching in Offline Reinforcement Learning"
      },
      {
        "paperId": "48b3231c3021ad52874bb7b3dc5717402a135b08",
        "title": "Stable Minima Cannot Overfit in Univariate ReLU Networks: Generalization by Large Step Sizes"
      },
      {
        "paperId": "8bd8bc8073f41f82317c67c6c6db8694b0310c6e",
        "title": "Differentially Private Reinforcement Learning with Self-Play"
      },
      {
        "paperId": "895108dcb9cc7023d584a66df568a66e0edfd304",
        "title": "Batched Nonparametric Contextual Bandits"
      },
      {
        "paperId": "96db4df948ec3dcfe5a8fa0bcc11325967796a53",
        "title": "Near-Optimal Reinforcement Learning with Self-Play under Adaptivity Constraints"
      },
      {
        "paperId": "95cf4e267592b32503aa1afa87ae0ef969150253",
        "title": "KUALITAS PENGIRIMAN ONLINE FOOD DELIVERY TERHADAP CUSTOMER LOYALTY DIIMPLEMENTASIKAN OLEH SWITCHING COST"
      },
      {
        "paperId": "8364ae9b3f7bd8533af069b854498b7e389176b8",
        "title": "Sample Efficient Reinforcement Learning with Partial Dynamics Knowledge"
      },
      {
        "paperId": "3e18a6871dd5c2a2c8d7a3c18d8acbf97862be26",
        "title": "Sample-Efficiency in Multi-Batch Reinforcement Learning: The Need for Dimension-Dependent Adaptivity"
      },
      {
        "paperId": "9e6e02bb331973dcf469bb1955a6202b3f9e574b",
        "title": "Policy Finetuning in Reinforcement Learning via Design of Experiments using Offline Data"
      },
      {
        "paperId": "5f2a8d7800857355cece73fb46e3cc58baf8f895",
        "title": "Offline Policy Evaluation for Reinforcement Learning with Adaptively Collected Data"
      },
      {
        "paperId": "9a09099b53349214fd3301f3e4dacf9a71716452",
        "title": "Regret-Optimal Model-Free Reinforcement Learning for Discounted MDPs with Short Burn-In Time"
      },
      {
        "paperId": "5071dda43602ad4dfbba90aced6122cfe2b16abf",
        "title": "Minimax-Optimal Reward-Agnostic Exploration in Reinforcement Learning"
      },
      {
        "paperId": "22f49494ec67ad28be2be6418bb37d2fa180ca85",
        "title": "Logarithmic Switching Cost in Reinforcement Learning beyond Linear MDPs"
      },
      {
        "paperId": "272ec7c5cd1b1c1f550a55cdbbea1a21096f8035",
        "title": "Near-Optimal Adversarial Reinforcement Learning with Switching Costs"
      },
      {
        "paperId": "f5e4bd8df3e07672f7b2246364059deb50b86774",
        "title": "A Reduction-based Framework for Sequential Decision Making with Delayed Feedback"
      },
      {
        "paperId": "df0fc3fd23fd69c93ccde8745c3c51979057fa43",
        "title": "Near-Optimal Differentially Private Reinforcement Learning"
      },
      {
        "paperId": "dbb104780567e5b2d5975da8f9d731725c64aadc",
        "title": "Near-Optimal Regret Bounds for Multi-batch Reinforcement Learning"
      },
      {
        "paperId": "88298748d970690e09fe6043d68125b93ccce89d",
        "title": "Near-Optimal Deployment Efficiency in Reward-Free Reinforcement Learning with Linear Function Approximation"
      },
      {
        "paperId": "550d9aadc6322adaccef9e1fe7dbc1b623962685",
        "title": "Offline Reinforcement Learning with Differentiable Function Approximation is Provably Efficient"
      },
      {
        "paperId": "d6b2b76e1c040a46a34c81d4fffc987aca79fbcb",
        "title": "Doubly Fair Dynamic Pricing"
      },
      {
        "paperId": "2674f653b9cf3b84bbe2604238dff7c343be3d24",
        "title": "Offline Reinforcement Learning with Differential Privacy"
      },
      {
        "paperId": "0a19d07533622bb1dfb11827a6a2153c73e586eb",
        "title": "Differentially Private Linear Sketches: Efficient Implementations and Applications"
      },
      {
        "paperId": "668d882897d1dfd3cc4b59823388138c0764a46f",
        "title": "Online Sub-Sampling for Reinforcement Learning with General Function Approximation"
      },
      {
        "paperId": "81e78a7bdbd578a838cf0cc5f128b1f168500f03",
        "title": "Beyond Information Gain: An Empirical Benchmark for Low-Switching-Cost Reinforcement Learning"
      }
    ],
    "score": 10.333333333333332
  },
  {
    "id": "0ba7f2e592dda172bc3d07f88cdcf2a57830deec",
    "title": "Towards Safe Reinforcement Learning with a Safety Editor Policy",
    "authors": [
      "Haonan Yu",
      "Wei Xu",
      "Haichao Zhang"
    ],
    "year": 2022,
    "citationCount": 31,
    "abstract": "We consider the safe reinforcement learning (RL) problem of maximizing utility with extremely low constraint violation rates. Assuming no prior knowledge or pre-training of the environment safety model given a task, an agent has to learn, via exploration, which states and actions are safe. A popular approach in this line of research is to combine a model-free RL algorithm with the Lagrangian method to adjust the weight of the constraint reward relative to the utility reward dynamically. It relies on a single policy to handle the conflict between utility and constraint rewards, which is often challenging. We present SEditor, a two-policy approach that learns a safety editor policy transforming potentially unsafe actions proposed by a utility maximizer policy into safe ones. The safety editor is trained to maximize the constraint reward while minimizing a hinge loss of the utility state-action values before and after an action is edited. SEditor extends existing safety layer designs that assume simplified safety models, to general safe RL scenarios where the safety model can in theory be arbitrarily complex. As a first-order method, it is easy to implement and efficient for both inference and training. On 12 Safety Gym tasks and 2 safe racing tasks, SEditor obtains much a higher overall safety-weighted-utility (SWU) score than the baselines, and demonstrates outstanding utility performance with constraint violation rates as low as once per 2k time steps, even in obstacle-dense environments. On some tasks, this low violation rate is up to 200 times lower than that of an unconstrained RL method with similar utility performance. Code is available at https://github.com/hnyu/seditor.",
    "url": "https://www.semanticscholar.org/paper/0ba7f2e592dda172bc3d07f88cdcf2a57830deec",
    "pdf_url": "https://arxiv.org/pdf/2201.12427.pdf",
    "venue": "Neural Information Processing Systems",
    "publicationDate": "2022-01-28",
    "externalIds": {
      "DBLP": "conf/nips/Yu0Z22",
      "ArXiv": "2201.12427",
      "CorpusId": 246430493
    },
    "references": [
      {
        "paperId": "81bbf1bccd16e145ae3933e829aa41d2c8f7faa8",
        "title": "SAAC: Safe Reinforcement Learning as an Adversarial Game of Actor-Critics"
      },
      {
        "paperId": "d8da5d28807fe5f8af2e6942c67738bd9f12a1ec",
        "title": "Safe Reinforcement Learning by Imagining the Near Future"
      },
      {
        "paperId": "53ae8b06d3f1e93b211724204cfe713fa965659f",
        "title": "Do You Need the Entropy Reward (in Practice)?"
      },
      {
        "paperId": "4b89f78987e5d0dba67a4f533945a838a5428ed9",
        "title": "Constrained Variational Policy Optimization for Safe Reinforcement Learning"
      },
      {
        "paperId": "f1e48bfb4464fedb94ced2d85b74991efcfe2856",
        "title": "Constrained Policy Optimization via Bayesian World Models"
      },
      {
        "paperId": "676b8462708f3851effee13c1b029ed0b79e82e9",
        "title": "DESTA: A Framework for Safe Reinforcement Learning with Markov Games of Intervention"
      },
      {
        "paperId": "aa6c2814ff94ca098d90f188f95126b5b06ebb69",
        "title": "Nonlinear Programming"
      },
      {
        "paperId": "36d55c909e8c1be84f3a4f2631e3303ef5392fb0",
        "title": "Learning Barrier Certificates: Towards Safe Reinforcement Learning with Zero Training-time Violations"
      },
      {
        "paperId": "9333138ad1d0ef3778bae9ae517aaf396de1b287",
        "title": "Density Constrained Reinforcement Learning"
      },
      {
        "paperId": "da5e6083a846c82762487d5cb1f545ad5f16f08a",
        "title": "Debiasing a First-order Heuristic for Approximate Bi-level Optimization"
      },
      {
        "paperId": "a853385c32d39aff6ff316ca32ad0e21f7740135",
        "title": "Safe Reinforcement Learning Using Robust Action Governor"
      },
      {
        "paperId": "81d612d385aec3839ab53babfa83081221de22b4",
        "title": "How RL Agents Behave When Their Actions Are Modified"
      },
      {
        "paperId": "431dc05ac25510de6264084434254cca877f9ab3",
        "title": "Recovery RL: Safe Reinforcement Learning With Learned Recovery Zones"
      },
      {
        "paperId": "19e4e04e9c48bc86ddd849abdda2ec305c060694",
        "title": "First Order Constrained Optimization in Policy Space"
      },
      {
        "paperId": "b07b03a5170c46a7cb57eea4f65e47d23ba1e38d",
        "title": "Safety Aware Reinforcement Learning (SARL)"
      },
      {
        "paperId": "5a1b92aa50797a7c1e99b8840ff01aad66038596",
        "title": "Sim-to-Real Transfer in Deep Reinforcement Learning for Robotics: a Survey"
      },
      {
        "paperId": "629d0ce250581471f07083bbab95f23623b00201",
        "title": "Responsive Safety in Reinforcement Learning by PID Lagrangian Methods"
      },
      {
        "paperId": "d75cf795676f10168853e443d1ca7a3c663e4e9e",
        "title": "Safe Reinforcement Learning via Curriculum Induction"
      },
      {
        "paperId": "8ba600c169f0d2422625822223976bce562eabe1",
        "title": "dm_control: Software and Tasks for Continuous Control"
      },
      {
        "paperId": "9ec0bd60df62b997bb371e65d254434f8851ea9b",
        "title": "Projection-Based Constrained Policy Optimization"
      },
      {
        "paperId": "b19729b27a1b4c24b52f87308c907653300afa7f",
        "title": "Dota 2 with Large Scale Deep Reinforcement Learning"
      },
      {
        "paperId": "361c00b22e29d0816ca896513d2c165e26399821",
        "title": "Grandmaster level in StarCraft II using multi-agent reinforcement learning"
      },
      {
        "paperId": "320b227027030fc291de2896fc3c6da49d7614be",
        "title": "Solving Rubik's Cube with a Robot Hand"
      },
      {
        "paperId": "adcd4bfd88213da0c33d3cb7057411dd15b72c7a",
        "title": "End-to-End Safe Reinforcement Learning through Barrier Functions for Safety-Critical Continuous Control Tasks"
      },
      {
        "paperId": "2c18f0c1f00bf079d536c66df6ab614a0147e5ea",
        "title": "Value constrained model-free continuous control"
      },
      {
        "paperId": "3fa50569925cfecc66fed5ec616682ecf3794ad7",
        "title": "Lyapunov-based Safe Policy Optimization for Continuous Control"
      },
      {
        "paperId": "12c0751b4f51ed833172a713b7e32390032ead93",
        "title": "Soft Actor-Critic Algorithms and Applications"
      },
      {
        "paperId": "cb7c479a36520da1caeeec67db10772351a390c6",
        "title": "Reward Constrained Policy Optimization"
      },
      {
        "paperId": "65fb1b37c41902793ac65db3532a6e51631a9aff",
        "title": "A Lyapunov-based Approach to Safe Reinforcement Learning"
      },
      {
        "paperId": "7f567df97dc7e099d96e6c590ddf5aef8c5b11c4",
        "title": "Safe Exploration in Continuous Action Spaces"
      },
      {
        "paperId": "70760635a1e117c0ae10f879075021b01b456b04",
        "title": "OptLayer - Practical Constrained Optimization for Deep Reinforcement Learning in the Real World"
      },
      {
        "paperId": "deb421b829dfd30ee1105b9fc9542084269dcbaf",
        "title": "Improving Stochastic Policy Gradients in Continuous Control with Deep Reinforcement Learning using the Beta Distribution"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "7a4193d0b042643a8bb9ec262ed7f9d509bdb12e",
        "title": "Constrained Policy Optimization"
      },
      {
        "paperId": "88880d88073a99107bbc009c9f4a4197562e1e44",
        "title": "Safe Model-based Reinforcement Learning with Stability Guarantees"
      },
      {
        "paperId": "0076b232181e4e5be58dce8354a813ad2bbf663a",
        "title": "OptNet: Differentiable Optimization as a Layer in Neural Networks"
      },
      {
        "paperId": "2b10281297ee001a9f3f4ea1aa9bea6b638c27df",
        "title": "OpenAI Gym"
      },
      {
        "paperId": "494e2d5b40dcebde349f9872c7317e5003f9c5d2",
        "title": "Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection"
      },
      {
        "paperId": "846aedd869a00c09b40f1f1f35673cb22bc87490",
        "title": "Mastering the game of Go with deep neural networks and tree search"
      },
      {
        "paperId": "449532187c94af3dd3aa55e16d2c50f7854d2199",
        "title": "Trust Region Policy Optimization"
      },
      {
        "paperId": "a82795a372cec5266e7247054e9aace8f3b6a4f3",
        "title": "A Survey of Multi-Objective Sequential Decision-Making"
      },
      {
        "paperId": "f51265b88ce01e7e3c12fa9b8dc84dfd0a73975c",
        "title": "Scalarized multi-objective reinforcement learning: Novel design techniques"
      },
      {
        "paperId": "f332ecd5d54adf0530a39dae189cf6b160ad5c0e",
        "title": "An Online Actor\u2013Critic Algorithm with Function Approximation for Constrained Markov Decision Processes"
      },
      {
        "paperId": "13e2d5c4d39bc58b42f9004e5b03905f847dfa0f",
        "title": "Autonomous Helicopter Flight via Reinforcement Learning"
      },
      {
        "paperId": null,
        "title": "14: Key frames of several episodes where the robot violated constraints or only learned a sub-optimal policy"
      },
      {
        "paperId": "4d0f6a6ffcd6ab04732ff76420fd9f8a7bb649c3",
        "title": "Benchmarking Safe Exploration in Deep Reinforcement Learning"
      },
      {
        "paperId": null,
        "title": "d) We propose the safety-weighted-utility (SWU) score for quantitatively evaluating a safe RL method. The score is a soft indicator of the dominance de\ufb01ned by Ray et al."
      },
      {
        "paperId": "ec85eeb27f71f389416ebbf6e13c725ddef78bea",
        "title": "Multi-objective reinforcement learning using sets of pareto dominating policies"
      },
      {
        "paperId": null,
        "title": "a) Did you include the full text of instructions given to participants and screenshots, if applicable?"
      },
      {
        "paperId": null,
        "title": "c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation?"
      },
      {
        "paperId": null,
        "title": "We present SEditor, a \ufb01rst-order, easy-to-implement approach that is trained by SGD like most model-free RL methods"
      },
      {
        "paperId": null,
        "title": "Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable?"
      },
      {
        "paperId": null,
        "title": "c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [Yes] For"
      }
    ],
    "cited_by": [
      {
        "paperId": "7fc4284b11bb5e24315b0eeff8834d044e6f4b96",
        "title": "Efficient Action-Constrained Reinforcement Learning via Acceptance-Rejection Method and Augmented MDPs"
      },
      {
        "paperId": "7f2bd1e799aedcf29f148aa82cb275b360fa6596",
        "title": "Off-Policy Conservative Distributional Reinforcement Learning With Safety Constraints"
      },
      {
        "paperId": "74cde73aeaa438d9d57d586057e39585d2bae3cf",
        "title": "Risk-Conscious Mutations in Jump-Start Reinforcement Learning for Autonomous Racing Policy"
      },
      {
        "paperId": "dcd89dd8b926ffcbe22fc5bbb06ce03aa2fdd93e",
        "title": "Repairing Neural Networks for Safety in Robotic Systems using Predictive Models"
      },
      {
        "paperId": "1e019de605bd8547167462c1598b372b7b00ae96",
        "title": "Safe Reinforcement Learning Filter for Multicopter Collision-Free Tracking under disturbances"
      },
      {
        "paperId": "815a86be3d83f950cfa76480b2b49cb5d0c5e46f",
        "title": "A Safety Modulator Actor-Critic Method in Model-Free Safe Reinforcement Learning and Application in UAV Hovering"
      },
      {
        "paperId": "ee7989bdeb3a0c15aa5ff14a30e50be2ff3922c0",
        "title": "SMLE: Safe Machine Learning via Embedded Overapproximation"
      },
      {
        "paperId": "17a4e9ee072cf54a56bd68c7201eb29ac4dfc441",
        "title": "Optimization of Urban Target Area Accessibility for Multi-UAV Data Gathering Based on Deep Reinforcement Learning"
      },
      {
        "paperId": "6a70838c765aa25fe7654eb14678ad92523dbb94",
        "title": "SoNIC: Safe Social Navigation with Adaptive Conformal Inference and Constrained Reinforcement Learning"
      },
      {
        "paperId": "a234389fd1e14e17b2d59f5fc02c85c99ec242de",
        "title": "Exterior Penalty Policy Optimization with Penalty Metric Network under Constraints"
      },
      {
        "paperId": "a6af2113ca7a98f299433b17dc7a3365a57e384d",
        "title": "Reinforcement Learning with Adaptive Regularization for Safe Control of Critical Systems"
      },
      {
        "paperId": "10caaa31ba9997c25174cde009cca38ff9b9115e",
        "title": "Efficient Multi-Task Reinforcement Learning via Task-Specific Action Correction"
      },
      {
        "paperId": "e63f0dc1963db342f8e87be1b565f4273404db5b",
        "title": "Safety Optimized Reinforcement Learning via Multi-Objective Policy Optimization"
      },
      {
        "paperId": "d1f30df851e2e2d883988f2a4910889002e9e45c",
        "title": "Uniformly Safe RL with Objective Suppression for Multi-Constraint Safety-Critical Applications"
      },
      {
        "paperId": "1f356f5271c60794dc3b7448954b4bdc14ed4cac",
        "title": "A Survey of Constraint Formulations in Safe Reinforcement Learning"
      },
      {
        "paperId": "f569ff7165f69798a34fc9e2bb570a1cbfab83fa",
        "title": "Enhancing Badminton Player Performance via a Closed-Loop AI Approach: Imitation, Simulation, Optimization, and Execution"
      },
      {
        "paperId": "1f04934cf96dda1ef0651c8eb2a1f4de8e04b0f7",
        "title": "Reinforcement Learning in a Safety-Embedded MDP with Trajectory Optimization"
      },
      {
        "paperId": "b9dff780b3cbe81adca8f063a1fdcc699db0f79f",
        "title": "Constraint-Conditioned Policy Optimization for Versatile Safe Reinforcement Learning"
      },
      {
        "paperId": "bdaf3d86248d4d4897a1047d0a4a9fb7eab223e0",
        "title": "Robust Perception-Based Navigation using PAC-NMPC with a Learned Value Function"
      },
      {
        "paperId": "73ee1392b2cf8a8ed5eb0d93c15ffbb6e61ffeab",
        "title": "Learning to Recover for Safe Reinforcement Learning"
      },
      {
        "paperId": "8f65c327891cd28790b23f139843834828cf6724",
        "title": "Safe Reinforcement Learning for Strategic Bidding of Virtual Power Plants in Day-Ahead Markets"
      },
      {
        "paperId": "e25a311f855ace4ef2510cbf80f17426723787fc",
        "title": "Safe Online Integral Reinforcement Learning for Control Systems via Controller Decomposition"
      },
      {
        "paperId": "059fb62256b46a6020db103212124025a49804a6",
        "title": "Safety-Aware Unsupervised Skill Discovery"
      },
      {
        "paperId": "3b8a8bf747d2ef81bb2cbdd5498cb617ed0292ce",
        "title": "Handling Long and Richly Constrained Tasks through Constrained Hierarchical Reinforcement Learning"
      },
      {
        "paperId": "308a801dafad411f092a2b0303d5802c3c75169b",
        "title": "Trustworthy Reinforcement Learning Against Intrinsic Vulnerabilities: Robustness, Safety, and Generalizability"
      },
      {
        "paperId": "274f0cee47c8a5bbc38c32de6787adee6a08f69e",
        "title": "On the Robustness of Safe Reinforcement Learning under Observational Perturbations"
      },
      {
        "paperId": "53ae8b06d3f1e93b211724204cfe713fa965659f",
        "title": "Do You Need the Entropy Reward (in Practice)?"
      },
      {
        "paperId": "12075ea34f5fbe32ec5582786761ab34d401209b",
        "title": "Exploration in Deep Reinforcement Learning: From Single-Agent to Multiagent Domain"
      },
      {
        "paperId": "f7c1fa4f4aca994aa8e2c258c3f9724e04977abb",
        "title": "On the Benefits of Inducing Local Lipschitzness for Robust Generative Adversarial Imitation Learning"
      },
      {
        "paperId": "8bda28c09d051d1f80201dd921509452cd65e53f",
        "title": "Risk-Aware Constrained Reinforcement Learning with Non-Stationary Policies"
      },
      {
        "paperId": "ba6f6213b524f97587dfc64f7a9c45f0635a835b",
        "title": "Multi-Agent Reinforcement Learning with Safety Layer for Active Voltage Control"
      }
    ],
    "score": 10.333333333333332
  },
  {
    "id": "09da56cd3bf72b632c43969be97874fa14a3765c",
    "title": "The Challenges of Exploration for Offline Reinforcement Learning",
    "authors": [
      "Nathan Lambert",
      "Markus Wulfmeier",
      "William F. Whitney",
      "Arunkumar Byravan",
      "Michael Bloesch",
      "Vibhavari Dasagi",
      "Tim Hertweck",
      "Martin A. Riedmiller"
    ],
    "year": 2022,
    "citationCount": 30,
    "abstract": "Offline Reinforcement Learning (ORL) enablesus to separately study the two interlinked processes of reinforcement learning: collecting informative experience and inferring optimal behaviour. The second step has been widely studied in the offline setting, but just as critical to data-efficient RL is the collection of informative data. The task-agnostic setting for data collection, where the task is not known a priori, is of particular interest due to the possibility of collecting a single dataset and using it to solve several downstream tasks as they arise. We investigate this setting via curiosity-based intrinsic motivation, a family of exploration methods which encourage the agent to explore those states or transitions it has not yet learned to model. With Explore2Offline, we propose to evaluate the quality of collected data by transferring the collected data and inferring policies with reward relabelling and standard offline RL algorithms. We evaluate a wide variety of data collection strategies, including a new exploration agent, Intrinsic Model Predictive Control (IMPC), using this scheme and demonstrate their performance on various tasks. We use this decoupled framework to strengthen intuitions about exploration and the data prerequisites for effective offline RL.",
    "url": "https://www.semanticscholar.org/paper/09da56cd3bf72b632c43969be97874fa14a3765c",
    "pdf_url": "https://arxiv.org/pdf/2201.11861.pdf",
    "venue": "arXiv.org",
    "publicationDate": "2022-01-27",
    "externalIds": {
      "ArXiv": "2201.11861",
      "DBLP": "journals/corr/abs-2201-11861",
      "CorpusId": 246411688
    },
    "references": [
      {
        "paperId": "56a35ffb3ca0d820155e5655b527a74bf8e7b13a",
        "title": "Don't Change the Algorithm, Change the Data: Exploratory Data for Offline Reinforcement Learning"
      },
      {
        "paperId": "39d2e380967c91b447d47377840fa541e276b479",
        "title": "Interesting Object, Curious Agent: Learning Task-Agnostic Exploration"
      },
      {
        "paperId": "9317736e9bb1b25c9d8e7325b7b364b7fbae0f3f",
        "title": "URLB: Unsupervised Reinforcement Learning Benchmark"
      },
      {
        "paperId": "90ab9f71262e03ba7429b3fc4b631aa8ab1ddd28",
        "title": "Evaluating model-based planning and planner amortization for continuous control"
      },
      {
        "paperId": "dbabe6ce982b0d8b3f3a842ec85ddc088733385e",
        "title": "Is Curiosity All You Need? On the Utility of Emergent Behaviours from Curious Exploration"
      },
      {
        "paperId": "6481e73b66577788be2d90619e0de55e78516e51",
        "title": "Collect & Infer - a fresh look at data-efficient Reinforcement Learning"
      },
      {
        "paperId": "c879b25308026d6538e52b27bcf4fd3cb60855f3",
        "title": "A Minimalist Approach to Offline Reinforcement Learning"
      },
      {
        "paperId": "8923ac90317d7bc4f7c7591c802cf954ab382662",
        "title": "Efficient Self-Supervised Data Collection for Offline Robot Learning"
      },
      {
        "paperId": "6f812978ce061b6c650ffd70ad3cf7dc3eef6003",
        "title": "DisCo RL: Distribution-Conditioned Reinforcement Learning for General-Purpose Policies"
      },
      {
        "paperId": "677b103eecc4d34e378502d60147456875e8741b",
        "title": "Actionable Models: Unsupervised Offline Reinforcement Learning of Robotic Skills"
      },
      {
        "paperId": "0295df1b9d11e6e49b20119410fd755a4d7781af",
        "title": "Behavior From the Void: Unsupervised Active Pre-Training"
      },
      {
        "paperId": "a50f7bcf8a998f3de11bc085b0f4dea32be19783",
        "title": "Reinforcement Learning with Prototypical Representations"
      },
      {
        "paperId": "9e5fe2ba652774ba3b1127f626c192668a907132",
        "title": "Decoupled Exploration and Exploitation Policies for Sample-Efficient Reinforcement Learning"
      },
      {
        "paperId": "b62f094179aa79015c57690c01e76e7d7d75f78c",
        "title": "Representation Matters: Improving Perception and Exploration for Robotics"
      },
      {
        "paperId": "7acbdb961f67d50fef359066f2a1d7755cf16ee2",
        "title": "Critic Regularized Regression"
      },
      {
        "paperId": "de46f4e4613364792bbd13f185c381ab656a27ef",
        "title": "RL Unplugged: Benchmarks for Offline Reinforcement Learning"
      },
      {
        "paperId": "669f07843065e471974080608a3cbee876d13cef",
        "title": "Adaptive Reward-Free Exploration"
      },
      {
        "paperId": "28db20a81eec74a50204686c3cf796c42a020d2e",
        "title": "Conservative Q-Learning for Offline Reinforcement Learning"
      },
      {
        "paperId": "3a71c306eb6232658c9e5fd48aed1ef3befe5fbe",
        "title": "Planning to Explore via Self-Supervised World Models"
      },
      {
        "paperId": "5e7bc93622416f14e6948a500278bfbe58cd3890",
        "title": "Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems"
      },
      {
        "paperId": "a326d9f2d2d351001fece788165dbcbb524da2e4",
        "title": "D4RL: Datasets for Deep Data-Driven Reinforcement Learning"
      },
      {
        "paperId": "55999400a3eed52ea9dd2f4b9f1b71ccb5c51238",
        "title": "Rewriting History with Inverse RL: Hindsight Inference for Policy Improvement"
      },
      {
        "paperId": "90f9ef793c1e4000ee523e1f8cea501fb8806fa0",
        "title": "Reward-Free Exploration for Reinforcement Learning"
      },
      {
        "paperId": "7b0871c783e721bfbf9b5d16e575130a07a672cd",
        "title": "Generalized Hindsight for Reinforcement Learning"
      },
      {
        "paperId": "ce9b93c52114e065a3f9175341fcc1df964d48e8",
        "title": "Evaluating task-agnostic exploration for fixed-batch learning of arbitrary future tasks"
      },
      {
        "paperId": "ad14227e4f51276892ffc37aa43fd8750bb5eba8",
        "title": "Advantage-Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning"
      },
      {
        "paperId": "cac78ad6696d6b6370942679f7d0e4425ef4b3e7",
        "title": "A Framework for Data-Driven Robotics"
      },
      {
        "paperId": "4012d4ab621f3f5f04b0f91849a60c6eaabe64b4",
        "title": "An Optimistic Perspective on Offline Reinforcement Learning"
      },
      {
        "paperId": "49172458767567434abf60970a45e07948c30c66",
        "title": "Compositional Transfer in Hierarchical Reinforcement Learning"
      },
      {
        "paperId": "55203cd25f9c03d7c6b691ba84e95bb82df0bc6f",
        "title": "Fast Task Inference with Variational Intrinsic Successor Features"
      },
      {
        "paperId": "87a40a2d506fec06d67dc8f14ce2c708a789a2b4",
        "title": "Self-Supervised Exploration via Disagreement"
      },
      {
        "paperId": "a2bac02786e09a25a80732f738aaaee088d13e43",
        "title": "Low-Level Control of a Quadrotor With Deep Model-Based Reinforcement Learning"
      },
      {
        "paperId": "6a9013a8cdd84e423223f76a903028011c84c4ab",
        "title": "Plan Online, Learn Offline: Efficient Learning and Exploration via Model-Based Control"
      },
      {
        "paperId": "4cb3fd057949624aa4f0bbe7a6dcc8777ff04758",
        "title": "Exploration by Random Network Distillation"
      },
      {
        "paperId": "ca14dce53be20d3d23d4f0db844a8389ab619db3",
        "title": "Large-Scale Study of Curiosity-Driven Learning"
      },
      {
        "paperId": "56136aa0b2c347cbcf3d50821f310c4253155026",
        "title": "Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models"
      },
      {
        "paperId": "5b01eaef54a653ba03ddd5a978690380fbc19bfc",
        "title": "Diversity is All You Need: Learning Skills without a Reward Function"
      },
      {
        "paperId": "a8ef08940341381390d9a5672546354d0ce51328",
        "title": "Maximum a Posteriori Policy Optimisation"
      },
      {
        "paperId": "a9a3ed69c94a3e1c08ef1f833d9199f57736238b",
        "title": "DeepMind Control Suite"
      },
      {
        "paperId": "c1f4ef741242d629d1f56e442a09a7ba29595a0e",
        "title": "A Distributional Perspective on Reinforcement Learning"
      },
      {
        "paperId": "429ed4c9845d0abd1f8204e1d7705919559bc2a2",
        "title": "Hindsight Experience Replay"
      },
      {
        "paperId": "225ab689f41cef1dc18237ef5dab059a49950abf",
        "title": "Curiosity-Driven Exploration by Self-Supervised Prediction"
      },
      {
        "paperId": "869f4fc59d74a96098ed46935cb6fd1a537c38ce",
        "title": "Information theoretic MPC for model-based reinforcement learning"
      },
      {
        "paperId": "dc3e905bfb27d21675ee1720413e007b014b37d3",
        "title": "Safe and Efficient Off-Policy Reinforcement Learning"
      },
      {
        "paperId": "6b6b078ee9aabf3c17220a2da1e3f0dd822956b7",
        "title": "Chapter 3 \u2013 The Cross-Entropy Method for Optimization"
      },
      {
        "paperId": "3c3fbadbcfe98e3dd096865948c486bb3b3ac386",
        "title": "Model Predictive Control"
      },
      {
        "paperId": "e8077729ef723d71a0b7492f2f271ae413b5a4d5",
        "title": "What is Intrinsic Motivation? A Typology of Computational Approaches"
      },
      {
        "paperId": "d735b0ba3a716704d07f00e7b1b00c3d7cdcf0cf",
        "title": "Trajectory Free Linear Model Predictive Control for Stable Walking in the Presence of Strong Perturbations"
      },
      {
        "paperId": "6a592b571b1a5ce2b580b0f82db5973ce7efdde5",
        "title": "Understanding the Effects of Dataset Characteristics on Offline Reinforcement Learning"
      },
      {
        "paperId": "ebfa3fb595ca0299ab5249c69374c98b74bb62b3",
        "title": "A Tutorial on the Cross-Entropy Method"
      }
    ],
    "cited_by": [
      {
        "paperId": "96d3dc5b0e3282d1fbc4f86b66631bdea4e8a261",
        "title": "Exploiting Policy Idling for Dexterous Manipulation"
      },
      {
        "paperId": "4f8e6c36ff27dfc3d116d2eed48d79a8d0609622",
        "title": "Sparse-Reg: Improving Sample Complexity in Offline Reinforcement Learning using Sparsity"
      },
      {
        "paperId": "4040099ed20718f418733cd201709cd950f11def",
        "title": "Online Self-Preferring Language Models"
      },
      {
        "paperId": "449896dafde44a950fdf1bb1cc91a29a7979726e",
        "title": "Learning Robot Soccer from Egocentric Vision with Deep Reinforcement Learning"
      },
      {
        "paperId": "ea65fce1f9a16681ec05138b6883b1ece6be2dc8",
        "title": "Uniformly Distributed Data Effects in Offline RL: A Case Study in Gridworld Setting"
      },
      {
        "paperId": "c39b3a7c12a1fad1d6fe8923bb9ea5e40afe4982",
        "title": "Real-World Fluid Directed Rigid Body Control via Deep Reinforcement Learning"
      },
      {
        "paperId": "b46d05bcf42295b872f3cebf875643d2e66496a4",
        "title": "Direct Language Model Alignment from Online AI Feedback"
      },
      {
        "paperId": "bdec194bfc9bfbc1016590b06f9790c9f1b9e06c",
        "title": "Mastering Stacking of Diverse Shapes with Large-Scale Iterative Reinforcement Learning on Real Robots"
      },
      {
        "paperId": "842971bbd2668c9e4d0b3bc983383227fbfe2aa1",
        "title": "The Generalization Gap in Offline Reinforcement Learning"
      },
      {
        "paperId": "06cb176ff32fbf498f0df93016a46dbbb333d533",
        "title": "Goal-conditioned Offline Planning from Curious Exploration"
      },
      {
        "paperId": "65731c720d2575ced7fa5134d408cffdbfce37ca",
        "title": "Replay across Experiments: A Natural Extension of Off-Policy RL"
      },
      {
        "paperId": "45357a581f9a29b2127a150d834a88b175be04d7",
        "title": "Optimizing Energy Consumption and Provisioning for Wireless Charging and Data Collection in Large-Scale WRSNs With Mobile Elements"
      },
      {
        "paperId": "65ab05a440e3a68bf1745c7bd680f16d7bcea43e",
        "title": "Towards Robust Offline-to-Online Reinforcement Learning via Uncertainty and Smoothness"
      },
      {
        "paperId": "c3e2bec83b9105b7925aa76c0f38b88d2e337b31",
        "title": "Motif: Intrinsic Motivation from Artificial Intelligence Feedback"
      },
      {
        "paperId": "8f9ea5ed79c7996be594b08a0fc8d5471ecdf0f5",
        "title": "Offline Reinforcement Learning with Imbalanced Datasets"
      },
      {
        "paperId": "5f2a8d7800857355cece73fb46e3cc58baf8f895",
        "title": "Offline Policy Evaluation for Reinforcement Learning with Adaptively Collected Data"
      },
      {
        "paperId": "339cde1b2b272da377c89fd40956f95d21e38690",
        "title": "Think Before You Act: Unified Policy for Interleaving Language Reasoning with Actions"
      },
      {
        "paperId": "924501c0205218280cb0251c89bda88c5a142b3e",
        "title": "Investigating the role of model-based learning in exploration and transfer"
      },
      {
        "paperId": "b0cdf3658d6a85fda507674e30cd3dce2808493c",
        "title": "Learning Goal-Conditioned Policies Offline with Self-Supervised Reward Shaping"
      },
      {
        "paperId": "0a9f6718ecdbbb8cf4b200a3d14695e1e350cf49",
        "title": "Exploring through Random Curiosity with General Value Functions"
      },
      {
        "paperId": "c90a33f1f0049d524e9b5b3174d35611fd9a8096",
        "title": "Pretraining in Deep Reinforcement Learning: A Survey"
      },
      {
        "paperId": "9e8b29d025cd2718ff61b363ce1c1f422d612303",
        "title": "Learning General World Models in a Handful of Reward-Free Deployments"
      },
      {
        "paperId": "c4df1a5633615a5e23c16cc6f2d5998f6a2fc3de",
        "title": "Epistemic Monte Carlo Tree Search"
      },
      {
        "paperId": "bef625611fceceb93128e510a6ae76b16d76541b",
        "title": "Reliable Conditioning of Behavioral Cloning for Offline Reinforcement Learning"
      },
      {
        "paperId": "cbf56ae8cd6f4641d6af592f85971282a38b459b",
        "title": "Curious Exploration via Structured World Models Yields Zero-Shot Object Manipulation"
      },
      {
        "paperId": "56a35ffb3ca0d820155e5655b527a74bf8e7b13a",
        "title": "Don't Change the Algorithm, Change the Data: Exploratory Data for Offline Reinforcement Learning"
      },
      {
        "paperId": "5772e99161f0b420ca3628b7b890f07ec20e1e95",
        "title": "Efficient Multi-agent Offline Coordination via Diffusion-based Trajectory Stitching"
      },
      {
        "paperId": "5b9ed6108e8655f3dc1569759f9f8ed3fd028f97",
        "title": "Cross-Domain Generalization with Reverse Dynamics Models in Offline Model-Based Reinforcement Learning"
      },
      {
        "paperId": "a9f093ce82f8b00df00dadf7309938eb92f62398",
        "title": "ConserWeightive Behavioral Cloning for Reliable Offline Reinforcement Learning"
      },
      {
        "paperId": "42da79f3516623ab00416e79ac0e23ebda544538",
        "title": "A Study of Generalization in Offline Reinforcement Learning"
      }
    ],
    "score": 10.0
  },
  {
    "id": "fc3b5dc5528e24dbbc8f4ec273e622ce40eec855",
    "title": "Disentangling Transfer in Continual Reinforcement Learning",
    "authors": [
      "Maciej Wo\u0142czyk",
      "Michal Zajkac",
      "Razvan Pascanu",
      "Lukasz Kuci'nski",
      "Piotr Milo's"
    ],
    "year": 2022,
    "citationCount": 30,
    "abstract": "The ability of continual learning systems to transfer knowledge from previously seen tasks in order to maximize performance on new tasks is a significant challenge for the field, limiting the applicability of continual learning solutions to realistic scenarios. Consequently, this study aims to broaden our understanding of transfer and its driving forces in the specific case of continual reinforcement learning. We adopt SAC as the underlying RL algorithm and Continual World as a suite of continuous control tasks. We systematically study how different components of SAC (the actor and the critic, exploration, and data) affect transfer efficacy, and we provide recommendations regarding various modeling options. The best set of choices, dubbed ClonEx-SAC, is evaluated on the recent Continual World benchmark. ClonEx-SAC achieves 87% final success rate compared to 80% of PackNet, the best method in the benchmark. Moreover, the transfer grows from 0.18 to 0.54 according to the metric provided by Continual World.",
    "url": "https://www.semanticscholar.org/paper/fc3b5dc5528e24dbbc8f4ec273e622ce40eec855",
    "pdf_url": "https://arxiv.org/pdf/2209.13900.pdf",
    "venue": "Neural Information Processing Systems",
    "publicationDate": "2022-09-28",
    "externalIds": {
      "DBLP": "journals/corr/abs-2209-13900",
      "ArXiv": "2209.13900",
      "DOI": "10.48550/arXiv.2209.13900",
      "CorpusId": 252567887
    },
    "references": [
      {
        "paperId": "4247f45a5730e3bda5836e2bc7941e30f5b91cb7",
        "title": "Board"
      },
      {
        "paperId": "266cc7ff4856b6a2ce9cc0a3e5f6c155ecc448a2",
        "title": "Can Wikipedia Help Offline Reinforcement Learning?"
      },
      {
        "paperId": "f7fa5e6899abbe0108277e54e1c4141f740767a0",
        "title": "Why Should I Trust You, Bellman? The Bellman Error is a Poor Replacement for Value Error"
      },
      {
        "paperId": "94cefa04e0f834272d85cc425e0adfb27fd17e08",
        "title": "Same State, Different Task: Continual Reinforcement Learning without Interference"
      },
      {
        "paperId": "090273ad6b3720027e34f9183576dd2812bb4454",
        "title": "Continual World: A Robotic Benchmark For Continual Reinforcement Learning"
      },
      {
        "paperId": "3544650f12a05cf4ed3bf2f7e22fc5c02fcabf50",
        "title": "Pretrained Transformers as Universal Computation Engines"
      },
      {
        "paperId": "9faecf3e18a833f2d49b030d591cc2ded0b54336",
        "title": "Towards Continual Reinforcement Learning: A Review and Perspectives"
      },
      {
        "paperId": "34b6871b40d3389f1d5c2a89fc75664d8619490c",
        "title": "Embracing Change: Continual Learning in Deep Neural Networks"
      },
      {
        "paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a",
        "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
      },
      {
        "paperId": "e10682f8999bd5f05d995732cb418df7ae45d8a7",
        "title": "Continual Model-Based Reinforcement Learning with Hypernetworks"
      },
      {
        "paperId": "f8492a321d66c381637b693a24af994af41b3cdf",
        "title": "Transfer Learning in Deep Reinforcement Learning: A Survey"
      },
      {
        "paperId": "5baa3e00d66bc42db7e3908f0b70875cff9d0193",
        "title": "What is being transferred in transfer learning?"
      },
      {
        "paperId": "6ec8797952213227eea2e63620f4d7c060d598d5",
        "title": "Hierarchical reinforcement learning for efficient exploration and transfer"
      },
      {
        "paperId": "449c5660d637741f7aa7ff42549c32b43c9968bf",
        "title": "Gradient Surgery for Multi-Task Learning"
      },
      {
        "paperId": "a2e43270a9b1421e452c2975e5163e2a216abeac",
        "title": "A Survey of Deep Reinforcement Learning in Video Games"
      },
      {
        "paperId": "18d026ec5d0eebd17ee2c762da89540c0b3d7bde",
        "title": "A Comprehensive Survey on Transfer Learning"
      },
      {
        "paperId": "0bc855f84668b35cb65618d996d09f6e434d28c9",
        "title": "Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning"
      },
      {
        "paperId": "d970b2265d4c896019c52d3f04b954ea0271b60f",
        "title": "The Visual Task Adaptation Benchmark"
      },
      {
        "paperId": "abf5478c24664a1380b7e213a3ab1c4af54775d0",
        "title": "Rapid Learning or Feature Reuse? Towards Understanding the Effectiveness of MAML"
      },
      {
        "paperId": "90e04f3ae23ca7df5f59b11453341e3db943b6f4",
        "title": "A Continual Learning Survey: Defying Forgetting in Classification Tasks"
      },
      {
        "paperId": "33a2c0f3b9a0adc452a13d53f950c0c3c4abb11b",
        "title": "Emergent Tool Use From Multi-Agent Autocurricula"
      },
      {
        "paperId": "09241a07ddf6aed5070999f5e41e25e7953c093a",
        "title": "DisCoRL: Continual Reinforcement Learning via Policy Distillation"
      },
      {
        "paperId": "853a5a6950827c3000b421702ae986ccef364ff6",
        "title": "Continual Learning with Tiny Episodic Memories"
      },
      {
        "paperId": "12c0751b4f51ed833172a713b7e32390032ead93",
        "title": "Soft Actor-Critic Algorithms and Applications"
      },
      {
        "paperId": "d9ff7a9344dd5d6653bd7a02bfd704422bb29951",
        "title": "Experience Replay for Continual Learning"
      },
      {
        "paperId": "4a954b3e72a61968ab235076bcc242aca3a05520",
        "title": "Efficient Lifelong Learning with A-GEM"
      },
      {
        "paperId": "9235d511dea04aa563a577ab236506b8eb8242ff",
        "title": "A Survey on Deep Transfer Learning"
      },
      {
        "paperId": "4a94e94fd32241cca633b9f35a67562ea0d087a2",
        "title": "Towards Robust Evaluations of Continual Learning"
      },
      {
        "paperId": "9ea50b3408f993853f1c5e374690e5fbe73c2a3c",
        "title": "Continual Lifelong Learning with Neural Networks: A Review"
      },
      {
        "paperId": "4b0e8a4df3605d5e22c9eacc3cb360ff08eb8c4e",
        "title": "Continual Reinforcement Learning with Complex Synapses"
      },
      {
        "paperId": "811df72e210e20de99719539505da54762a11c6d",
        "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor"
      },
      {
        "paperId": "713b0d9005944f80af00addc81b162ca74ea4b14",
        "title": "Memory Aware Synapses: Learning what (not) to forget"
      },
      {
        "paperId": "47bc048efb90e7b8bae5c1fcc979a78b65763fe9",
        "title": "PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning"
      },
      {
        "paperId": "d475f695dedd94e96771fdaa1e5c075fd01d11cf",
        "title": "Variational Continual Learning"
      },
      {
        "paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
        "title": "Attention is All you Need"
      },
      {
        "paperId": "321f1877bc570ff9b318e909cefb7c27138458df",
        "title": "PathNet: Evolution Channels Gradient Descent in Super Neural Networks"
      },
      {
        "paperId": "2e55ba6c97ce5eb55abd959909403fe8da7e9fe9",
        "title": "Overcoming catastrophic forgetting in neural networks"
      },
      {
        "paperId": "3a13f7c43b767b1fb72ef107ef62a4ddd48dd2a7",
        "title": "Modular Multitask Reinforcement Learning with Policy Sketches"
      },
      {
        "paperId": "8fab7d7dfd233fd5d19bc2641b4c1ca74fc7bc6a",
        "title": "Learning modular neural network policies for multi-task and multi-robot transfer"
      },
      {
        "paperId": "53c9443e4e667170acc60ca1b31a0ec7151fe753",
        "title": "Progressive Neural Networks"
      },
      {
        "paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d",
        "title": "Deep Residual Learning for Image Recognition"
      },
      {
        "paperId": "235585741d5d2eddb85949226ddf4f581fd79dc1",
        "title": "Policy Transfer using Reward Shaping"
      },
      {
        "paperId": "abd1c342495432171beb7ca8fd9551ef13cbd0ff",
        "title": "ImageNet classification with deep convolutional neural networks"
      },
      {
        "paperId": "467568f1777bc51a15a5100516cd4fe8de62b9ab",
        "title": "Transfer Learning for Reinforcement Learning Domains: A Survey"
      },
      {
        "paperId": "36fcc32aac99de1f67f83da33f134721258ea943",
        "title": "Transfer in variable-reward hierarchical reinforcement learning"
      },
      {
        "paperId": "b3412ded0375f8fe7336e82dc534eed994cac088",
        "title": "Transfer Learning via Inter-Task Mappings for Temporal Difference Learning"
      },
      {
        "paperId": "65873acfc47fdc16a29a7415ed96f8983eee050d",
        "title": "An Experts Algorithm for Transfer Learning"
      },
      {
        "paperId": "94066dc12fe31e96af7557838159bde598cb4f10",
        "title": "Policy Invariance Under Reward Transformations: Theory and Application to Reward Shaping"
      },
      {
        "paperId": "e75fb417b54a6eae589ff382874de09d7f58a3de",
        "title": "Open-Ended Learning Leads to Generally Capable Agents"
      },
      {
        "paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992",
        "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
      },
      {
        "paperId": null,
        "title": "Did you discuss whether the data you are using/curating contains personally identi\ufb01able information or offensive content?"
      },
      {
        "paperId": null,
        "title": "Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation"
      },
      {
        "paperId": null,
        "title": "Did you discuss whether and how consent was obtained from people whose data you\u2019re using/curating?"
      },
      {
        "paperId": null,
        "title": "If you used crowdsourcing or conducted research with human subjects"
      },
      {
        "paperId": null,
        "title": "code, data, models) or curating/releasing new assets... (a) If your work uses existing assets, did you cite the creators?"
      },
      {
        "paperId": null,
        "title": "Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable?"
      },
      {
        "paperId": null,
        "title": "Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes]"
      },
      {
        "paperId": null,
        "title": "c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [Yes] We conduct each experiment with multiple seeds"
      }
    ],
    "cited_by": [
      {
        "paperId": "7a219e065b1099f5c482b345635f7ab0a56a013d",
        "title": "Efficient Multi-task Reinforcement Learning with Cross-Task Policy Guidance"
      },
      {
        "paperId": "da5e3e299854419ea47e19a2b82800ee71e927bd",
        "title": "A Survey of Continual Reinforcement Learning"
      },
      {
        "paperId": "33bfced3ccc883ecac60da7888028a9a3c4d7f43",
        "title": "Policy Search, Retrieval, and Composition via Task Similarity in Collaborative Agentic Systems"
      },
      {
        "paperId": "49a5bc9335768d85111fc1f1b5c63cef102cf443",
        "title": "Self-Composing Policies for Scalable Continual Reinforcement Learning"
      },
      {
        "paperId": "014fa374a72d63da27fa13e34fcb1f0a5c431c49",
        "title": "Pay Attention to What and Where? Interpretable Feature Extractor in Vision-based Deep Reinforcement Learning"
      },
      {
        "paperId": "040b16f021b9da9d7422253bd6d93f0ad040ccdd",
        "title": "Neuron-level Balance between Stability and Plasticity in Deep Reinforcement Learning"
      },
      {
        "paperId": "5436b4b69a5ba8f8649cef184017e37a7814de5c",
        "title": "Mastering Continual Reinforcement Learning through Fine-Grained Sparse Network Allocation and Dormant Neuron Exploration"
      },
      {
        "paperId": "7fbfae2cd00f4133654e368dac3cbc8891a3bd5b",
        "title": "Continual Task Learning through Adaptive Policy Self-Composition"
      },
      {
        "paperId": "4145de15c870d44c6d7e9f406c41120efc8f7a60",
        "title": "LoopSR: Looping Sim-and-Real for Lifelong Policy Adaptation of Legged Robots"
      },
      {
        "paperId": "888b262f485551a92c7236fb7c1bbcf870897c93",
        "title": "CaDeT: A Causal Disentanglement Approach for Robust Trajectory Prediction in Autonomous Driving"
      },
      {
        "paperId": "8c2a8b876d7d2ed3df7b520d4310287303fafc1e",
        "title": "Continual Offline Reinforcement Learning via Diffusion-based Dual Generative Replay"
      },
      {
        "paperId": "77a58e445e34f1a6655b83233b444037bfc9e30a",
        "title": "Multi-world Model in Continual Reinforcement Learning"
      },
      {
        "paperId": "6e3e807b79f30e8ac749a62a37aaafb91b2c0caf",
        "title": "Reset & Distill: A Recipe for Overcoming Negative Transfer in Continual Reinforcement Learning"
      },
      {
        "paperId": "40c52f7a1df09143f7cc4d686e3d78e9e0f77f21",
        "title": "Fine-tuning Reinforcement Learning Models is Secretly a Forgetting Mitigation Problem"
      },
      {
        "paperId": "efbb395b0b367b737085f91a74835e08d5811df9",
        "title": "DGTRL: Deep graph transfer reinforcement learning method based on fusion of knowledge and data"
      },
      {
        "paperId": "bd2524bbe950ddcf69d1c7b7882a0ce522f08f9b",
        "title": "Multi-granularity Knowledge Transfer for Continual Reinforcement Learning"
      },
      {
        "paperId": "3d7e5485fae2965ddf081dc64be6ab52f5834cf8",
        "title": "Continual Learning: Applications and the Road Forward"
      },
      {
        "paperId": "f5bbd862df67bc681957e8c1bc64159c5ac38020",
        "title": "Replay-enhanced Continual Reinforcement Learning"
      },
      {
        "paperId": "a590ee835a3bdaccfd436c2e86f6b72db54c2c1d",
        "title": "A Survey: Navigating the Landscape of Incremental Learning Techniques and Trends"
      },
      {
        "paperId": "80543d5e827605d5d42f0af7ea42697e2dd7f62a",
        "title": "Learning to Modulate pre-trained Models in RL"
      },
      {
        "paperId": "98cfd7b1b29453c4e82536f5afdc6ddc58bbb1b3",
        "title": "LIBERO: Benchmarking Knowledge Transfer for Lifelong Robot Learning"
      },
      {
        "paperId": "ce588d27c10a475ef766ec4106cc0fd70e605349",
        "title": "Continual Task Allocation in Meta-Policy Network via Sparse Prompting"
      },
      {
        "paperId": "09bc7b16e793369f673368eccc7cd7e9e0467a0b",
        "title": "On the Value of Myopic Behavior in Policy Reuse"
      },
      {
        "paperId": "9ced2505d3c720a05b490c1a887af9f0e9110c07",
        "title": "Offline Experience Replay for Continual Offline Reinforcement Learning"
      },
      {
        "paperId": "9348656b761f7b76fb65cfe6fac55386b04a3a8a",
        "title": "A Comprehensive Survey of Continual Learning: Theory, Method and Application"
      },
      {
        "paperId": "12c2d8a5e1d65c857036580b7e279dc772cffd15",
        "title": "Task-Agnostic Continual Reinforcement Learning: Gaining Insights and Overcoming Challenges"
      },
      {
        "paperId": "7eff8e646fe0d76f196e637d8e86ee0ea64561af",
        "title": "Hi-Core: Hierarchical Knowledge Transfer for Continual Reinforcement Learning"
      },
      {
        "paperId": "1dbbcc84f8bacbe0166b411c6b44077102bf87b6",
        "title": "Curriculum Reinforcement Learning for Tokamak Control"
      },
      {
        "paperId": "04f1feccbea7d93037bd5e507d3f8e199b9acb09",
        "title": "O N T HE R OLE OF F ORGETTING IN F INE -T UNING R E - INFORCEMENT L EARNING M ODELS"
      },
      {
        "paperId": "1ec78f256f6a46ed38b184841484f62ca3591795",
        "title": "COOM: A Game Benchmark for Continual Reinforcement Learning"
      }
    ],
    "score": 10.0
  },
  {
    "id": "46eb68c585bdb8a1051dfda98b4b35610301264f",
    "title": "Spacecraft Proximity Maneuvering and Rendezvous With Collision Avoidance Based on Reinforcement Learning",
    "authors": [
      "Qingyu Qu",
      "Kexin Liu",
      "Wei Wang",
      "Jinhu Lu"
    ],
    "year": 2022,
    "citationCount": 29,
    "abstract": "The rapid development of the aerospace industry puts forward the urgent need for the evolution of autonomous spacecraft rendezvous technology, which has gained significant attention recently due to increased applications in various space missions. This article studies the relative position tracking problem of the autonomous spacecraft rendezvous under the requirement of collision avoidance. An exploration-adaptive deep deterministic policy gradient (DDPG) algorithm is proposed to train a definite control strategy for this mission. Similar to the DDPG algorithm, four neural networks are used in this method, where two of them are used to generate the deterministic policy, whereas the other two are used to score the obtained policy. Differently, adaptive noise is introduced to reduce the possibility of oscillations and divergences and to cut down the unnecessary computation by weakening the exploration of stabilization problems. In addition, in order to effectively and quickly adapt to some other similar scenarios, a metalearning-based idea is introduced by fine-tuning the prior strategy. Finally, two numerical simulations show that the trained control strategy can effectively avoid the oscillation phenomenon caused by the artificial potential function. Benefiting from this, the trained control strategy based on deep reinforcement learning technology can decrease the energy consumption by 16.44% during the close proximity phase, compared with the traditional artificial potential function method. Besides, after introducing the metalearning-based idea, a strategy available for some other perturbed scenarios can be trained in a relatively short period of time, which illustrates its adaptability.",
    "url": "https://www.semanticscholar.org/paper/46eb68c585bdb8a1051dfda98b4b35610301264f",
    "pdf_url": "https://doi.org/10.1109/TAES.2022.3180271",
    "venue": "IEEE Transactions on Aerospace and Electronic Systems",
    "publicationDate": "2022-12-01",
    "externalIds": {
      "DBLP": "journals/taes/QuL0L22",
      "DOI": "10.1109/TAES.2022.3180271",
      "CorpusId": 249593810
    },
    "references": [],
    "cited_by": [
      {
        "paperId": "57d4bb32b4b793e2d2d58e33c5c2f5b819491734",
        "title": "Machine Learning-Aided Adaptive Control of Spacecraft Formation Path Planning and Collision Avoidance"
      },
      {
        "paperId": "5552e8a154791e585c6fe48adc4812dad2ec5ebb",
        "title": "YOLO-GRBI: An Enhanced Lightweight Detector for Non-Cooperative Spatial Target in Complex Orbital Environments"
      },
      {
        "paperId": "d82408b38904205bf166e84490b1147153e3cd12",
        "title": "Review of data-driven computational guidance for unmanned aerospace vehicles"
      },
      {
        "paperId": "9d054156f6c2413124a587a1f71a636a833574e3",
        "title": "Satellite Navigation and Control using Discrete-APF and Fixed-Time Sliding Mode Controller"
      },
      {
        "paperId": "2da8184e3f4a8cfc5de244bde29959a7b371dee6",
        "title": "Low-orbit Space Debris Warning and Autonomous Collision Avoidance for Space Environment Governance"
      },
      {
        "paperId": "05d0662d1e0c6e9343590a6f54be583b5ed0d463",
        "title": "Onboard Mission Planning for Autonomous Avoidance of Spacecraft Subject to Various Orbital Threats: An SMT-Based Approach"
      },
      {
        "paperId": "9eebeb03dfb4ccca125172d4175d862fd4ac4c45",
        "title": "Hierarchical Guidance for Spacecraft Proximity via Iterative State Transitions"
      },
      {
        "paperId": "aca201f2d4a635b1e518fbb959c2c9c327320072",
        "title": "Optimal Performance Guaranteed Motion Control for Libration Point Orbit Rendezvous: A Semianalytical Predictive Approach"
      },
      {
        "paperId": "1cedbcb2c88426b4b4700e617518e4d3f18f6f98",
        "title": "Adaptive Integral Sliding-Mode Control for a Class of Nonlinear Spacecraft Proximity Systems Under Multi-Source Disturbances and Unmodeled Dynamics"
      },
      {
        "paperId": "7f4974c4c165574a8bc7fa2cacc61141d2f30d31",
        "title": "Intelligent Decision-Making Approach for Contingency Return Trajectory Based on Production Rule Base and Deep Learning"
      },
      {
        "paperId": "69830d7484a1e8d5efaf2fa6d7db2fa4ca881554",
        "title": "A Factor-Ignored Module for Easy Detection of Uncooperative Spacecraft Using Small Training Samples"
      },
      {
        "paperId": "e401ba782c2da93959582295089d3f04a051d6c1",
        "title": "Reinforcement Learning and Sim-to-Real Transfer of Reorientation and Landing Control for Quadruped Robots on Asteroids"
      },
      {
        "paperId": "7ebc3d71f513c3fdebf96285ed0e60d68777e905",
        "title": "Bayesian quadrature policy optimization for spacecraft proximity maneuvers and docking"
      },
      {
        "paperId": "ac99b40fac7dd9a7d8d86b303edcc00c6cf406e1",
        "title": "Mission Planning on Autonomous Avoidance for Spacecraft Confronting Orbital Debris"
      },
      {
        "paperId": "496cbfa0d68bb69a33949949958050a061cc882b",
        "title": "Dynamic Obstacle Avoidance of Fixed-Wing Aircraft in Final Phase via Reinforcement Learning"
      },
      {
        "paperId": "1c6f9c25d41535a02d1287a75632bf5a70a3492d",
        "title": "Adaptive Neural Stochastic Control With Lipschitz Constant Optimization"
      },
      {
        "paperId": "3c0e5aae15c89c954309ff74b011223ca0c9d1a5",
        "title": "Artificial Intelligence for Trusted Autonomous Satellite Operations"
      },
      {
        "paperId": "12b12de7e21333039defb7f840aefd862fa63ecc",
        "title": "Research Advancements in Artificial Intelligence for Space Situational Awareness"
      },
      {
        "paperId": "84cb9d5bc4c2d5cdd39b4f63d5315c6c0c174b1f",
        "title": "Advancing spacecraft rendezvous and docking through safety reinforcement learning and ubiquitous learning principles"
      },
      {
        "paperId": "ba00aa27e5c61c16e5af599a43cefd4018a24862",
        "title": "A Fast Approach to Satellite Range Rescheduling Using Deep Reinforcement Learning"
      },
      {
        "paperId": "927fde98aad557340799c9d41cc3a406e9d5a62b",
        "title": "Safely Learn to Fly Aircraft From Human: An Offline\u2013Online Reinforcement Learning Strategy and Its Application to Aircraft Stall Recovery"
      },
      {
        "paperId": "ad561d32b077e651e79758bce53f8985d2ccaffd",
        "title": "Cooperative Spacecraft Formation Flying Based on Reinforcement Learning"
      },
      {
        "paperId": "4957520377b7e96be348f2699ef3d92a7490ae9f",
        "title": "Ground Experiment of Safe Proximity Control for Complex-Shaped Spacecraft"
      },
      {
        "paperId": "29a1e1b166e212a2b65d2de18eccb1a43d8386f5",
        "title": "Spacecraft Autonomous Decision-Planning for Collision Avoidance: a Reinforcement Learning Approach"
      },
      {
        "paperId": "0c8d131dc69af0f26d1afe091c2aaaad2d581314",
        "title": "Autonomous Spacecraft Collision Avoidance with Multiple Space Debris Based on Reinforcement Learning"
      },
      {
        "paperId": "67f69ab0807f730c5abbbe9d98fec28e7e31954e",
        "title": "PRD-MADDPG: An Efficient Learning-Based Algorithm for Orbital Pursuit-Evasion Game with Impulsive Maneuvers"
      },
      {
        "paperId": "dda5e98313fabc09fdf5a7c8db9126a1d1267d99",
        "title": "Learning Reference Governor for Constrained Spacecraft Rendezvous and Proximity Maneuvering"
      },
      {
        "paperId": "8c24bcc625d569239b901c64adac96fd370c4075",
        "title": "Reframe the Field of Aerospace Engineering Via Machine Learning: Application and Comparison"
      },
      {
        "paperId": "70a13d6ffe16f32d2f84929b7f37dc1e5c85a0ce",
        "title": "Observer-Based Deep Reinforcement Learning for Robust Missile Guidance and Control"
      }
    ],
    "score": 9.666666666666666
  },
  {
    "id": "04efc9768a8e0c5f23b8c8504fb6db8803ffc071",
    "title": "Generative Design by Using Exploration Approaches of Reinforcement Learning in Density-Based Structural Topology Optimization",
    "authors": [
      "H. Sun",
      "Ling Ma"
    ],
    "year": 2020,
    "citationCount": 48,
    "abstract": "A central challenge in generative design is the exploration of vast number of solutions. In this work, we extend two major density-based structural topology optimization (STO) methods based on four classes of exploration algorithms of reinforcement learning (RL) to STO problems, which approaches generative design in a new way. The four methods are: first, using \u03b5 -greedy policy to disturb the search direction; second, using upper confidence bound (UCB) to add a bonus on sensitivity; last, using Thompson sampling (TS) as well as information-directed sampling (IDS) to direct the search, where the posterior function of reward is fitted by Beta distribution or Gaussian distribution. Those combined methods are evaluated on some structure compliance minimization tasks from 2D to 3D, including the variable thickness design problem of an atmospheric diving suit (ADS). We show that all methods can generate various acceptable design options by varying one or two parameters simply, except that IDS fails to reach the convergence for complex structures due to the limitation of computation ability. We also show that both Beta distribution and Gaussian distribution work well to describe the posterior probability.",
    "url": "https://www.semanticscholar.org/paper/04efc9768a8e0c5f23b8c8504fb6db8803ffc071",
    "pdf_url": "https://doi.org/10.3390/designs4020010",
    "venue": "Designs",
    "publicationDate": "2020-05-01",
    "externalIds": {
      "MAG": "3021658836",
      "DOI": "10.3390/designs4020010",
      "CorpusId": 219052256
    },
    "references": [
      {
        "paperId": "361c00b22e29d0816ca896513d2c165e26399821",
        "title": "Grandmaster level in StarCraft II using multi-agent reinforcement learning"
      },
      {
        "paperId": "6bebda36c1b72922045dedded6b10af7ee32c4af",
        "title": "Application of Adversarial Networks for 3D Structural Topology Optimization"
      },
      {
        "paperId": "1f5d4eeb0582941c5ccf5f0e219b301758528b08",
        "title": "Deep Generative Design: Integration of Topology Optimization and Generative Models"
      },
      {
        "paperId": "9704964e32ca7f2332c7a7a59a57a8a59c064b45",
        "title": "Dream Lens: Exploration and Visualization of Large-Scale Generative Design Datasets"
      },
      {
        "paperId": "2cb43c686233e8fca415f234e91b662319b67247",
        "title": "Deep learning for determining a near-optimal topological design without any iteration"
      },
      {
        "paperId": "c27db32efa8137cbf654902f8f728f338e55cd1c",
        "title": "Mastering the game of Go without human knowledge"
      },
      {
        "paperId": "6210aeb142a7075bee11c097579cbbf03d1f8119",
        "title": "Neural networks for topology optimization"
      },
      {
        "paperId": "14ac2c83f2e33e0b945de65e8888cd909ecb019d",
        "title": "Generative Design: What it is? How is it being used? Why it\u2019s a game changer"
      },
      {
        "paperId": "340f48901f72278f6bf78a04ee5b01df208cc508",
        "title": "Human-level control through deep reinforcement learning"
      },
      {
        "paperId": "4dc7ff778b073201c023169056e1ca21fd537edc",
        "title": "Neuro-evolutionary topology optimization of structures by utilizing local state features"
      },
      {
        "paperId": "f770614e497f456cfbe310bf7fd5223a4c28edd7",
        "title": "Learning to Optimize via Information-Directed Sampling"
      },
      {
        "paperId": "c6781f25d706093363a7e42c6032fa615e29474f",
        "title": "Topology optimization approaches"
      },
      {
        "paperId": "9805dcb5d37630af0f9dece81acdfc9e9b35200f",
        "title": "Topology Optimization: \"Theory, Methods, And Applications\""
      },
      {
        "paperId": "5a4f795dc8ba1b9dce4fb54bbecbfd8ef88180dd",
        "title": "A binary particle swarm optimization for continuum structural topology optimization"
      },
      {
        "paperId": "7c55729277512f14b5b34f23729f9533798174c0",
        "title": "Structural topology optimization using ant colony optimization algorithm"
      },
      {
        "paperId": "a51fdffbc69bb3f8c6c1e30605e6fe8f58d20e0f",
        "title": "Combining genetic algorithms with BESO for topology optimization"
      },
      {
        "paperId": "86985edef646a1a0c37d5ea48ac2f54ed72f8624",
        "title": "Structural topology optimization using ant colony methodology"
      },
      {
        "paperId": "37054142f2280f09b4ae7e991bbbbc2549da5108",
        "title": "Genetic evolutionary structural optimization"
      },
      {
        "paperId": "e4d4ab618e959012b9203db07cc7f1caa7e5b49e",
        "title": "Convergent and mesh-independent solutions for the bi-directional evolutionary structural optimization method"
      },
      {
        "paperId": "2603ce24086ffa48a2639b5bc0d1910c18876dc2",
        "title": "Towards integrated performance-driven generative design tools"
      },
      {
        "paperId": "1e1d35136b1bf3b13ef6b53f6039f39d9ee820e3",
        "title": "Finite-time Analysis of the Multiarmed Bandit Problem"
      },
      {
        "paperId": "55c228b22f2df912e7f94f5b912d2cbfe9cde17f",
        "title": "3D and multiple load case bi-directional evolutionary structural optimization (BESO)"
      },
      {
        "paperId": "f3f6374c0e8e333451a75d6a00bd36a946e1bdea",
        "title": "Evolutionary structural optimization for problems with stiffness constraints"
      },
      {
        "paperId": "51985a7551a99f838a1cc2b642c2ea371ce2d686",
        "title": "Optimal shape design as a material distribution problem"
      },
      {
        "paperId": "4c9e36a0dbdc3e7b20e38bd288a804c05c77e9fa",
        "title": "Asymptotically efficient adaptive allocation rules"
      },
      {
        "paperId": "ee2cd1d17f833d3c157a1016a778c7c22af555a2",
        "title": "ON THE LIKELIHOOD THAT ONE UNKNOWN PROBABILITY EXCEEDS ANOTHER IN VIEW OF THE EVIDENCE OF TWO SAMPLES"
      },
      {
        "paperId": "1e18bacaf43d4944c77e9ebf7ec9238f95c650f3",
        "title": "Generative Design"
      },
      {
        "paperId": "760e488863e62053dd98a2ad45f23a654f20e748",
        "title": "TOPOLOGY OPTIMIZATION BY PREDICTING SENSITIVITIES BASED ON LOCAL STATE FEATURES"
      },
      {
        "paperId": "5384fa882e11ad927e178fee670c34fbdd702449",
        "title": "Multidomain Demand Modeling in Design for Market Systems"
      },
      {
        "paperId": "56bcede9a7485e174a9b063b97494ae35ad73259",
        "title": "A practical generative design method"
      },
      {
        "paperId": "8851953ef486615fce803bda2e40aec97cbb5547",
        "title": "Multi-agent Reinforcement Learning: An Overview"
      },
      {
        "paperId": "8eaf6f89f21cb0df455d401419ca8e90ffd0fc92",
        "title": "Bidirectional Evolutionary Topology Optimization for Structures with Geometrical and Material Nonlinearities"
      },
      {
        "paperId": "97efafdb4a3942ab3efba53ded7413199f79c054",
        "title": "Reinforcement Learning: An Introduction"
      },
      {
        "paperId": null,
        "title": "This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license"
      }
    ],
    "cited_by": [
      {
        "paperId": "2698bc7bb11a3ef9461fbae8dbfd8fac9a109ef6",
        "title": "Redesigning UAV Vertical Tail Structures with Lightweight Eco-Friendly Carbon-Reinforced Polymers: A Generative Design-Based Approach"
      },
      {
        "paperId": "bd2b4059da96a09f722d60b8eea3e4193c7b0780",
        "title": "Generative Design, Simulation, and 3D Printing of the Quadcopter Drone Frame"
      },
      {
        "paperId": "b5445878165c592c815b1d7542cbccb53b92946b",
        "title": "Reinforcement learning framework for the mechanical design of microelectronic components under multiphysics constraints"
      },
      {
        "paperId": "f201c4f7471f728a82fb1c1e729dd3a7a4592738",
        "title": "Additive Manufacturing and Chemical Engineering: Looking for Synergies from a Bibliometric Study"
      },
      {
        "paperId": "62ac042b256487c8bbc7d73d5c9865c03939cb48",
        "title": "Flow and heat transfer improvement in microfluidic thermal camouflage film by topology optimization"
      },
      {
        "paperId": "ddb8b99029f76cbba41e93e830ffc020c806ced2",
        "title": "Conditional diffusion models for the inverse design of lattice structures"
      },
      {
        "paperId": "4864a4774173e3942a87266fff13d95f5cdc2b17",
        "title": "Flexural analysis of 3D-printed sandwich beams with chiral cores using deep neural networks and response surface methodology"
      },
      {
        "paperId": "72597dbcef8bbe69e946cc8538cffd3b4d1a428a",
        "title": "Topology optimization of microchannel structures for enhanced heat flow in liquid cooling garments"
      },
      {
        "paperId": "843bc25c558fb8c5c5631056c52bb476216aeb5f",
        "title": "Trajectory of building and structural design automation from generative design towards the integration of deep generative models and optimization: A review"
      },
      {
        "paperId": "103a1eaf039f2b6492511d828c0143d431f296bf",
        "title": "Deep Concept Identification for Generative Design"
      },
      {
        "paperId": "a7fb0d2b72c8cd372ff178f401b330b90d9c1ecb",
        "title": "Generative design framework for RC deep beams using topology optimization and generative tie method: Experimental and numerical investigation"
      },
      {
        "paperId": "146c005868d06591aa3166474c49d189aa44fb0c",
        "title": "H\u0130BR\u0130T \u0130MALATTA YAPAY ZEK\u00c2 VE VER\u0130 ANAL\u0130T\u0130\u011e\u0130N\u0130N ROL\u00dcN\u00dcN ARA\u015eTIRILMASI"
      },
      {
        "paperId": "6023d272fc4218780b1ae909cdbea852529ffa53",
        "title": "Towards Domain-Adaptive, Resolution-Free 3D Topology Optimization With Neural Implicit Fields"
      },
      {
        "paperId": "716f70ccc8cba3faa86fdcd3027b96e6a267861d",
        "title": "Optimization study of a probe chuck for semiconductor wafers using genetic algorithm and deep reinforcement learnings"
      },
      {
        "paperId": "831bd9aeed0f17ce5137854c94ab10852ed6afbb",
        "title": "Exploring Deep Generative Models in Building Design"
      },
      {
        "paperId": "89316e466108fdd53bf3cb39adea18ccc0a66182",
        "title": "Form innovation: investigating the use of generative design tools to encourage creativity in product design"
      },
      {
        "paperId": "b2fb92eb6050b79998d4f4f6fdf3c95594d86bb7",
        "title": "A Review of Generative Models for 3D Vehicle Wheel Generation and Synthesis"
      },
      {
        "paperId": "a8cc49d965818650b77cbf33fd72b9a8afc52319",
        "title": "NITO: Neural Implicit Fields for Resolution-free Topology Optimization"
      },
      {
        "paperId": "d622272db2907e84bfdfff6eb88834302ea016e1",
        "title": "Deep learning in computational mechanics: a review"
      },
      {
        "paperId": "b19dc16f5a3218d01f1246aa8068fc245880a10b",
        "title": "Generative AI design for building structures"
      },
      {
        "paperId": "7a91ec4083716b7c1332eea8a86427de5e6bbf39",
        "title": "Establish an Algorithm for the Design Automation of Truss Structures using Generative Design Techniques"
      },
      {
        "paperId": "f0786790018e013e671086be01b642126ab7c2aa",
        "title": "Application of Advanced Design Methods of \u201cDesign for Additive Manufacturing\u201d (DfAM) to the Process of Development of Components for Mobile Machines"
      },
      {
        "paperId": "89e2413c3b5db4d48431e7d116c61ccff5e8d03e",
        "title": "Deep Learning in Deterministic Computational Mechanics"
      },
      {
        "paperId": "23409a3dcae3ab10970012e6e14bbc46576a80c5",
        "title": "Fairness- and uncertainty-aware data generation for data-driven design"
      },
      {
        "paperId": "9430e48600a14e40e1faf036fdccc01858252680",
        "title": "Structural and thermal generative design using reinforcement learning-based search strategy for additive manufacturing"
      },
      {
        "paperId": "e567e89bde6a64401cfb628e13803ecfbca82300",
        "title": "A critical review on applications of artificial intelligence in manufacturing"
      },
      {
        "paperId": "f1f8318137e438173e34071cf2963b59bdd3021e",
        "title": "A multi-point synergistic gradient evolution method for topology optimization leveraging neural network with applications in converged and diverse designs"
      },
      {
        "paperId": "ad5fb88450fef465332327541351e9e5fae03210",
        "title": "Aligning Optimization Trajectories with Diffusion Models for Constrained Design Generation"
      },
      {
        "paperId": "de170ed5c35659dc6bfcff475821da25c4bcef9b",
        "title": "Experimental Approach of a Curved-Crease Folding Process with Multiple Folding Lines Applied on a Composite Material"
      },
      {
        "paperId": "65ca4c3768bc98ccf6a4676467fc443bad7ce7bf",
        "title": "Designing mechanically tough graphene oxide materials using deep reinforcement learning"
      },
      {
        "paperId": "95726fc3f28e217c081d605be383175968bc4b83",
        "title": "Topology Optimization via Machine Learning and Deep Learning: A Review"
      },
      {
        "paperId": "aa397249008100aaa6d6e777608066aff8b46bd4",
        "title": "A survey of machine learning techniques in structural and multidisciplinary optimization"
      },
      {
        "paperId": "651c19e2623f5bc2ef46f9b6b85c5126bce1c35e",
        "title": "On the use of artificial neural networks in topology optimisation"
      },
      {
        "paperId": "eabd44e79fbcdf557ee1bc9a85e7ffdd78ac1547",
        "title": "Improving the diversity of topology-optimized designs by swarm intelligence"
      },
      {
        "paperId": "4de30c2057aa1ae03de41028f23b2180533d9eb2",
        "title": "Generative design for self-balancing unicycle robot in additive manufacturing"
      },
      {
        "paperId": "8a1933ee060ef996c61a9b6b5543b22b2ec7f857",
        "title": "Reinforcement Learning for Engineering Design Automation"
      },
      {
        "paperId": "0a2fb4f5eefa6bc03e3cd38b897817a72534b487",
        "title": "Deep learning-based inverse design for engineering systems: multidisciplinary design optimization of automotive brakes"
      },
      {
        "paperId": "a64df6e3eee26162cc957f2c09b7b1d90aacf342",
        "title": "Performance-Driven Engineering Design Approaches Based on Generative Design and Topology Optimization Tools: A Comparative Study"
      },
      {
        "paperId": "2bf48dfc98f3129bbb71a29e841ea8fc8cdba948",
        "title": "GENERATIVE DESIGN OF STRUCTURAL STEEL JOINTS"
      },
      {
        "paperId": "d7aa254469699df9e4357361e0e80d6a28513bd8",
        "title": "Enhancing Design for Additive Manufacturing Workflow: Optimization, Design and Simulation Tools"
      },
      {
        "paperId": "68daefeba9455f140507861adcab42668a0114a9",
        "title": "Optimization of quadcopter frame using generative design and comparison with DJI F450 drone frame"
      },
      {
        "paperId": "45e4b8fb9a6290a156e981435b773d3aaa51c3ad",
        "title": "Generative Design by Reinforcement Learning: Enhancing the Diversity of Topology Optimization Designs"
      },
      {
        "paperId": "ab672fbe31364de0ca95af4a576b717289adf6a6",
        "title": "Generative Design by Reinforcement Learning: Maximizing Diversity of Topology Optimized Designs"
      },
      {
        "paperId": "b5dd26708e0094f4d0d131bd9fc5714be64ee70c",
        "title": "A Shape Optimization Method for Part Design Derived from the Buildability Restrictions of the Directed Energy Deposition Additive Manufacturing Process"
      },
      {
        "paperId": "bd4fee2eee2ab9c4c73cc3e1ab277cda8e645413",
        "title": "Integrating deep learning into CAD/CAE system: generative design and evaluation of 3D conceptual wheel"
      },
      {
        "paperId": "19f0ddec04b6ab230b073f886c2a9af018654e39",
        "title": "A review & critique of optimal fail-safe structural design"
      },
      {
        "paperId": "cc58ec5697ae79bcfe5a22fcf07ad252c94aa7dd",
        "title": "APPLICATION OF GENERATIVE DESIGN APPROACH FOR OPTIMIZATION AND ADDITIVE MANUFACTURING OF UAV\u2019s FRAME STRUCTURE"
      },
      {
        "paperId": "5f321330e472a83880f6b2e41d00fc52e1503cdc",
        "title": "Design Synthesis Through a Markov Decision Process and Reinforcement Learning Framework"
      }
    ],
    "score": 9.600000000000001
  },
  {
    "id": "4df0238d5bfe0ab4cb8eaba67f4388c4e02dc2c8",
    "title": "A Human-Machine Reinforcement Learning Method for Cooperative Energy Management",
    "authors": [
      "Yuechuan Tao",
      "Jing Qiu",
      "Shuying Lai",
      "Xian Zhang",
      "Yunqi Wang",
      "Guibin Wang"
    ],
    "year": 2022,
    "citationCount": 28,
    "abstract": "The increasing penetration of distributed energy resources and a large volume of unprecedented data from smart metering infrastructure can help consumers transit to an active role in the smart grid. In this article, we propose a human-machine reinforcement learning (RL) framework in the smart grid context to formulate an energy management strategy for electric vehicles and thermostatically controlled loads aggregators. The proposed model-free method accelerates the decision-making speed by substituting the conventional optimization process, and it is more capable of coping with the diverse system environment via online learning. The human intervention is coordinated with machine learning to: 1) prevent the huge loss during the learning process; 2) realize emergency control; and 3) find preferable control policy. The performance of the proposed human-machine RL framework is verified in case studies. It can be concluded that our proposed method performs better than the conventional deep Q-learning and deep deterministic policy gradient in terms of convergence capability and preferable result exploration. Besides, the proposed method can better deal with emergent events, such as a sudden drop of photovoltaic (PV) output. Compared with the conventional model-based method, there are slight deviations between our method and the optimal solution, but the decision-making time is significantly reduced.",
    "url": "https://www.semanticscholar.org/paper/4df0238d5bfe0ab4cb8eaba67f4388c4e02dc2c8",
    "pdf_url": "https://doi.org/10.1109/tii.2021.3105115",
    "venue": "IEEE Transactions on Industrial Informatics",
    "publicationDate": "2022-05-01",
    "externalIds": {
      "MAG": "3194784047",
      "DBLP": "journals/tii/TaoQLZWW22",
      "DOI": "10.1109/tii.2021.3105115",
      "CorpusId": 238874316
    },
    "references": [],
    "cited_by": [
      {
        "paperId": "49a19b1bfebdd6e0deac1098039b56c37d36e5b4",
        "title": "A human-machine collaborative approach for high-resolution monitoring of suspended sediment dynamics in data-scarce and optically complex waters"
      },
      {
        "paperId": "338b99f3ac4bf1be86a5edc78a5409b1c47d4f67",
        "title": "Two-Stage TSO-DSO Services Provision Framework for Electric Vehicle Coordination"
      },
      {
        "paperId": "8c68cbc653d306d43098d76e0180ace18bce73c5",
        "title": "A Comprehensive review on AIoT applications for intelligent EV charging/discharging ecosystem"
      },
      {
        "paperId": "be363a82b3b385fd67d5525ccadd9ac977abd897",
        "title": "Model-Based Safe Reinforcement Learning for Active Distribution Network Scheduling"
      },
      {
        "paperId": "76ed5c107ada9d8a252665a1c0c77becd9a10dcf",
        "title": "Reinforcement learning for vehicle-to-grid: A review"
      },
      {
        "paperId": "caf19441964464aebae27d67c29124739f16082c",
        "title": "Powering Future Advancements and Applications of Battery Energy Storage Systems Across Different Scales"
      },
      {
        "paperId": "ca18feae27e3392ed7da0a212f1e71fbd6cadad9",
        "title": "Performance analysis of machine learning algorithms for estimation of EV penetration"
      },
      {
        "paperId": "21a37cdfb03314fccb8094a08abfd95a00db861e",
        "title": "Peak shaving and self-consumption maximization in home energy management systems: A combined integer programming and reinforcement learning approach"
      },
      {
        "paperId": "84cf5e84e0609101984b71119dd89ad9af3c43ca",
        "title": "Stochastic Dynamic Power Dispatch With Human Knowledge Transfer Using Graph-GAN Assisted Inverse Reinforcement Learning"
      },
      {
        "paperId": "9450ba1ff311d2311684db2f88c17fe1f153325a",
        "title": "A self-learning human-machine cooperative control method based on driver intention recognition"
      },
      {
        "paperId": "b8f6c79b7ee4487b36b9cd6a120ccfd3e53b08f8",
        "title": "Human\u2013Machine Collaborative Reinforcement Learning for Power Line Flow Regulation"
      },
      {
        "paperId": "ad1a7f81f7a6862cbfeb4512e0415a35eccb6c3d",
        "title": "Reviewing 40 years of artificial intelligence applied to power systems \u2013 a taxonomic perspective"
      },
      {
        "paperId": "f84addd8654b657e0286a44ba7c972f3cac0007f",
        "title": "Physics-Informed Machine Learning for Data Anomaly Detection, Classification, Localization, and Mitigation: A Review, Challenges, and Path Forward"
      },
      {
        "paperId": "0d7eb7613092c63dce55aebfcca7649cd1c85d56",
        "title": "Reinforcement learning for electric vehicle applications in power systems:A critical review"
      },
      {
        "paperId": "0fba53d3ce7d7929a265d7fbdac88d01e584d5e6",
        "title": "Coordinated Electric Vehicle Active and Reactive Power Control for Active Distribution Networks"
      },
      {
        "paperId": "5fc22ffe36d21e585c9044a5bfbe6e7ec6b6f251",
        "title": "Research on human-machine interaction method of smart grid based on meta-learning"
      },
      {
        "paperId": "8ed5cebcd42b8b1a54d0d803fb2d07d2ffba5b0c",
        "title": "Deep Reinforcement Learning for Charging Electric Vehicles With Uncertain Photovoltaic Power"
      },
      {
        "paperId": "cf9918ccaef7747546b1b86d1103c25c9524bdf0",
        "title": "Artificial intelligence and digital twins in power systems: Trends, synergies and opportunities"
      },
      {
        "paperId": "9dee21cc1e5f934f9512d0c3e4c453845d006c51",
        "title": "Data-driven Energy Management in Residential Areas Leveraging Demand Response"
      },
      {
        "paperId": "72c4f2195a2519fe6b8358287589f0b89f752acf",
        "title": "Deep Learning-based Energy Optimization for Electric Vehicles Integrated Smart Micro Grid"
      },
      {
        "paperId": "209cb9edd2b0cbc80f7e156de3c9e509e809f76a",
        "title": "SV2G-ET: A Secure Vehicle-to-Grid Energy Trading Scheme Using Deep Reinforcement Learning"
      },
      {
        "paperId": "79ffafec2133a6216e367fa52b28ee4bd4f685bf",
        "title": "A Data-Driven Management Strategy of Electric Vehicles and Thermostatically Controlled Loads Based on Modified Generative Adversarial Network"
      },
      {
        "paperId": "f64b646db23c8ba97975e4c97f1a152445fb0f5f",
        "title": "Coordinated electric vehicles dispatch for multi-service provisions: A comprehensive review of modelling and coordination approaches"
      },
      {
        "paperId": "2a6b68c4a106ade32d241781dce9017d21dee625",
        "title": "Rule-based shields embedded safe reinforcement learning approach for electric vehicle charging control"
      },
      {
        "paperId": "58096ba2612ac314236294d0648e58817085a134",
        "title": "Residential customers-oriented customized electricity retail pricing design"
      },
      {
        "paperId": "6772a11146d119e2d024e8a515c6e5d992234e50",
        "title": "A scalable graph reinforcement learning algorithm based stochastic dynamic dispatch of power system under high penetration of renewable energy"
      },
      {
        "paperId": "90842c9ea04f8649d5c22c2598932f15fb0456be",
        "title": "An efficient and privacy-preserving algorithm for multiple energy hubs scheduling with federated and matching deep reinforcement learning"
      },
      {
        "paperId": "0f094226bc9b4b0d9decd76393d334cf31927737",
        "title": "devel-Challenges and Pathways of Low-carbon Oriented Energy Transition and Power System Planning Strategy: A Review"
      }
    ],
    "score": 9.333333333333332
  },
  {
    "id": "fb3c6456708b0e143f545d77dc8ec804eb947395",
    "title": "Curiosity-driven Exploration in Deep Reinforcement Learning via Bayesian Neural Networks",
    "authors": [
      "Rein Houthooft",
      "Xi Chen",
      "Yan Duan",
      "John Schulman",
      "F. Turck",
      "P. Abbeel"
    ],
    "year": 2016,
    "citationCount": 79,
    "abstract": "Scalable and effective exploration remains a key challenge in reinforcement learning (RL). While there are methods with optimality guarantees in the setting of discrete state and action spaces, these methods cannot be applied in high-dimensional deep RL scenarios. As such, most contemporary RL relies on simple heuristics such as epsilon-greedy exploration or adding Gaussian noise to the controls. This paper introduces Variational Information Maximizing Exploration (VIME), an exploration strategy based on maximization of information gain about the agent's belief of environment dynamics. We propose a practical implementation, using variational inference in Bayesian neural networks which efficiently handles continuous state and action spaces. VIME modifies the MDP reward function, and can be applied with several different underlying RL algorithms. We demonstrate that VIME achieves significantly better performance compared to heuristic exploration methods across a variety of continuous control tasks and algorithms, including tasks with very sparse rewards.",
    "url": "https://www.semanticscholar.org/paper/fb3c6456708b0e143f545d77dc8ec804eb947395",
    "pdf_url": "https://arxiv.org/pdf/1605.09674.pdf",
    "venue": "arXiv.org",
    "publicationDate": "2016-05-31",
    "externalIds": {
      "MAG": "2514775068",
      "DBLP": "journals/corr/HouthooftCDSTA16",
      "ArXiv": "1605.09674",
      "CorpusId": 17141244
    },
    "references": [
      {
        "paperId": "1f8c9c98fbeed5d1b8893119ff02384f79c29e90",
        "title": "Intrinsically motivated model learning for developing curious robots"
      },
      {
        "paperId": "281fe5aa28e787596416883efcbadafa5caaaa6b",
        "title": "Information Theoretically Aided Reinforcement Learning for Embodied Agents"
      },
      {
        "paperId": "dc0adbb27267f1ae88dc5c9982ec7acb5124b95f",
        "title": "Exploration from Demonstration for Interactive Reinforcement Learning"
      },
      {
        "paperId": "1464776f20e2bccb6182f183b5ff2e15b0ae5e56",
        "title": "Benchmarking Deep Reinforcement Learning for Continuous Control"
      },
      {
        "paperId": "4b63e34276aa98d5345efa7fe09bb06d8a9d8f52",
        "title": "Deep Exploration via Bootstrapped DQN"
      },
      {
        "paperId": "b32d99c853ca991dc0230facd8359505699e440d",
        "title": "Information-theoretic neuro-correlates boost evolution of cognitive systems"
      },
      {
        "paperId": "ea6d73a33ac2109ed095963e25b857a9350449bd",
        "title": "Bayesian Reinforcement Learning: A Survey"
      },
      {
        "paperId": "ab68ddbdd8d0b61d9f9c8fa500a4c13d06158060",
        "title": "Variational Information Maximisation for Intrinsically Motivated Reinforcement Learning"
      },
      {
        "paperId": "e4257bc131c36504a04382290cbc27ca8bb27813",
        "title": "Action-Conditional Video Prediction using Deep Networks in Atari Games"
      },
      {
        "paperId": "2470fcf0f89082de874ac9133ccb3a8667dd89a8",
        "title": "Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models"
      },
      {
        "paperId": "4443e2e5bfd112562ecef578f772cee3c692f19f",
        "title": "Variational Dropout and the Local Reparameterization Trick"
      },
      {
        "paperId": "da6057368920585bcf2443295b98418840f1fc80",
        "title": "Weight Uncertainty in Neural Networks"
      },
      {
        "paperId": "340f48901f72278f6bf78a04ee5b01df208cc508",
        "title": "Human-level control through deep reinforcement learning"
      },
      {
        "paperId": "449532187c94af3dd3aa55e16d2c50f7854d2199",
        "title": "Trust Region Policy Optimization"
      },
      {
        "paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8",
        "title": "Adam: A Method for Stochastic Optimization"
      },
      {
        "paperId": "19cb18335c0ef439882d9e72ffa237c52886801c",
        "title": "Bayes-Adaptive Simulation-based Search with Value Function Approximation"
      },
      {
        "paperId": "1389772b8a0f9c7fc43057f9da41a7d0ebf0308b",
        "title": "Generalization and Exploration via Randomized Value Functions"
      },
      {
        "paperId": "b16382146fb017fe5b6dbf6f8bd295a771647e78",
        "title": "Guided Self-Organization: Inception"
      },
      {
        "paperId": "94d7d3569911dc79a7dba9da01912e8c8f9c85e2",
        "title": "Linear combination of one-step predictive information with an external reward in an episodic policy gradient setting: a critical analysis"
      },
      {
        "paperId": "d9eeb1277a4cea35a2c1a147dfcd4b41b83ecacd",
        "title": "PAC Optimal Exploration in Continuous Space Markov Decision Processes"
      },
      {
        "paperId": "c1263c9d850c4012dd62e5513621cf40f4a3ff98",
        "title": "Learning and exploration in action-perception loops"
      },
      {
        "paperId": "fe859f7e498ad093721c795b70d2e8380f980ce3",
        "title": "Exploration in Model-based Reinforcement Learning by Empirically Estimating Learning Progress"
      },
      {
        "paperId": "66b35f2a58d9cf2804828185c33857a7d46e6424",
        "title": "An information-theoretic approach to curiosity-driven reinforcement learning"
      },
      {
        "paperId": "5a9ef216bf11f222438fff130c778267d39a9564",
        "title": "Practical Variational Inference for Neural Networks"
      },
      {
        "paperId": "5df39cc393907ea78fddf461b494b4c5b1b5a2e4",
        "title": "Planning to Be Surprised: Optimal Bayesian Exploration in Dynamic Environments"
      },
      {
        "paperId": "33224ad0cdf6e2dc4893194dd587309c7887f0ba",
        "title": "Formal Theory of Creativity, Fun, and Intrinsic Motivation (1990\u20132010)"
      },
      {
        "paperId": "4282669947ca340de9f93a9a2a38007fe542195d",
        "title": "Near-Bayesian exploration in polynomial time"
      },
      {
        "paperId": "0cafe2903b097fc042782c359cb231ea34ef7ed3",
        "title": "Near-optimal Regret Bounds for Reinforcement Learning"
      },
      {
        "paperId": "e8077729ef723d71a0b7492f2f271ae413b5a4d5",
        "title": "What is Intrinsic Motivation? A Typology of Computational Approaches"
      },
      {
        "paperId": "2d01e2f7be589f2674c924032e676ee1d349fc52",
        "title": "Simple Algorithmic Principles of Discovery, Subjective Beauty, Selective Attention, Curiosity & Creativity"
      },
      {
        "paperId": "1f869232f148ec52066fab06a49855937f84098b",
        "title": "Reinforcement learning by reward-weighted regression for operational space control"
      },
      {
        "paperId": "aebd8bab5cff769fed204dba35112e364a47e504",
        "title": "Bayesian surprise attracts human attention"
      },
      {
        "paperId": "6bddcc4ff63e80fe576d379776284ede0be0a80c",
        "title": "Exploration in Metric State Spaces"
      },
      {
        "paperId": "103f6fe35033f9327611ddafde74a2b544072980",
        "title": "Using Confidence Bounds for Exploitation-Exploration Trade-offs"
      },
      {
        "paperId": "dc649486b881e672eea6546da48c46e1f98daf32",
        "title": "Near-Optimal Reinforcement Learning in Polynomial Time"
      },
      {
        "paperId": "25c9f33aceac6dcff357727cbe2faf145b01d13c",
        "title": "Keeping the neural networks simple by minimizing the description length of the weights"
      },
      {
        "paperId": "94db34f4b68189bfcba22beab33ee3b54f10b876",
        "title": "Curious model-building control systems"
      },
      {
        "paperId": "af64497a418cf62016251360d9ba537604f7622a",
        "title": "Efficient Exploration in Reinforcement Learning"
      },
      {
        "paperId": "9fba02c6068aa2c4af7f90ba09a1c0ff0c6b531a",
        "title": "Efficient Exploration in Reinforcement Learning"
      },
      {
        "paperId": "4c915c1eecb217c123a36dc6d3ce52d12c742614",
        "title": "Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning"
      },
      {
        "paperId": "c7dbf5ed7e9b63e104adb4e18bfbc98f5d6afdae",
        "title": "R-MAX - A General Polynomial Time Algorithm for Near-Optimal Reinforcement Learning"
      },
      {
        "paperId": "2547be25e1e07728aa0966a0354e90664816d15e",
        "title": "REINFORCEMENT DRIVEN INFORMATION ACQUISITION IN NON-DETERMINISTIC ENVIRONMENTS"
      },
      {
        "paperId": "4d6a87d76ec0c0379f0afbf24f84bba848c6246e",
        "title": "Noname manuscript No. (will be inserted by the editor) Policy Search for Motor Primitives in Robotics"
      }
    ],
    "cited_by": [
      {
        "paperId": "2e5bb7e994eb8896ad64771184fa9e029a241440",
        "title": "Bounded Active Exploration for Model-Based Reinforcement Learning"
      },
      {
        "paperId": "fa70bc2fda70a1741129fef5774151943122524f",
        "title": "Improving Reinforcement Learning Exploration by Autoencoders"
      },
      {
        "paperId": "f86367bd4e1eca13271068ecefd149cdc6d57b3d",
        "title": "Autoencoder Reconstruction Model for Long-Horizon Exploration"
      },
      {
        "paperId": "2be4f6ae240a1046d03d8e92d55824e1b9a913e8",
        "title": "BPMB: BayesCNNs with perturbed multi-branch structure for robust facial expression recognition"
      },
      {
        "paperId": "9f54952558f02a7947f91a9feb87735faaa18b58",
        "title": "Data-Efficient Task Generalization via Probabilistic Model-Based Meta Reinforcement Learning"
      },
      {
        "paperId": "e969ccd2d70953dc583dec1c8b6a1288b15c1e86",
        "title": "Using Curiosity for an Even Representation of Tasks in Continual Offline Reinforcement Learning"
      },
      {
        "paperId": "4a2af705f93a57a39d19dd75eb13d392c72dec60",
        "title": "KL Convergence Guarantees for Score Diffusion Models under Minimal Data Assumptions"
      },
      {
        "paperId": "c73373af557ddeca8f56d3c6e4597fcb685716f6",
        "title": "Curiosity-Driven Reinforcement Learning based Low-Level Flight Control"
      },
      {
        "paperId": "193af00290b5ee975226e5d8c9fbbdca2c9eeb36",
        "title": "Active sensing with predictive coding and uncertainty minimization"
      },
      {
        "paperId": "8a84ee6294bc7f88d0343c8615a12f1c763209bd",
        "title": "Off-Policy RL Algorithms Can be Sample-Efficient for Continuous Control via Sample Multiple Reuse"
      },
      {
        "paperId": "a7d52c02213d5de2aa79134a5dd9b24c5619155c",
        "title": "Strangeness-driven Exploration in Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "fe99f7ccef45bdddeee1e602db718e8349edf31e",
        "title": "Diverse Effective Relationship Exploration for Cooperative Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "c28cca3538c949b223194a00e14c3575b323ef2f",
        "title": "Normality-Guided Distributional Reinforcement Learning for Continuous Control"
      },
      {
        "paperId": "89ae946f74e75d1e7a9ca5d9d44c0c610e70d41b",
        "title": "Self-Supervised Exploration via Temporal Inconsistency in Reinforcement Learning"
      },
      {
        "paperId": "9a1bdf31bfecac774e19c104515486b7eb2f4f73",
        "title": "MatFormer"
      },
      {
        "paperId": "538f858732047960f5a390def882066b0c55fbe8",
        "title": "k-Means Maximum Entropy Exploration"
      },
      {
        "paperId": "ba107fdb842ee0218dffa34d7d7da7a20289a900",
        "title": "Deep reinforcement learning-based rehabilitation robot trajectory planning with optimized reward functions"
      },
      {
        "paperId": "8a913111f23fbded7f2e9d2d6c9c4278e7c682c9",
        "title": "APS: Active Pretraining with Successor Features"
      },
      {
        "paperId": "1f80036ebcad2190c1041a6618ce758e25fd7b48",
        "title": "Co-GAIL: Learning Diverse Strategies for Human-Robot Collaboration"
      },
      {
        "paperId": "491088fecea387f9d1315ba2e3be68dc01970924",
        "title": "A Survey of Brain-Inspired Intelligent Robots: Integration of Vision, Decision, Motion Control, and Musculoskeletal Systems"
      },
      {
        "paperId": "76907582d260103c90058d3fc4b9f9969fb75922",
        "title": "Touch-based Curiosity for Sparse-Reward Tasks"
      },
      {
        "paperId": "e340cc8c6952fc56ad2a6ce202c5f6bccd38a09d",
        "title": "Action Guidance: Getting the Best of Sparse Rewards and Shaped Rewards for Real-time Strategy Games"
      },
      {
        "paperId": "48e15d9d875b8cb74a1261a61e2d64d225078f7b",
        "title": "See, Hear, Explore: Curiosity via Audio-Visual Association"
      },
      {
        "paperId": "6f5b05f91a622385165983f18c72ddacaf8bdab3",
        "title": "Motion Planning of Six-DOF Arm Robot Based on Improved DDPG Algorithm"
      },
      {
        "paperId": "0f6d74ebfbf9265a72bafaab7eef6bac3b50e33f",
        "title": "From proprioception to long-horizon planning in novel environments: A hierarchical RL model"
      },
      {
        "paperId": "ac98fe10cb47c3546b707cac52494bc3d0999bbc",
        "title": "PQA-CNN: Towards Perceptual Quality Assured Single-Image Super-Resolution in Remote Sensing"
      },
      {
        "paperId": "6d91886bb07d3af29244dd8a186e6dda2315fd9e",
        "title": "LEAF: Latent Exploration Along the Frontier"
      },
      {
        "paperId": "4c0a4e69298dc28eb16cb48374d6dd9469ccbe3f",
        "title": "Dynamics-Aware Latent Space Reachability for Exploration in Temporally-Extended Tasks"
      },
      {
        "paperId": "ae3b2768b0a3c73410bce0d2ae03feaf01f6f864",
        "title": "Dynamics-Aware Unsupervised Skill Discovery"
      },
      {
        "paperId": "a4803df5337676175fdc9f5a2ab781e9b6b83af7",
        "title": "Multi-modal Visual-Thermal Saliency-based Object Detection in Visually-degraded Environments"
      },
      {
        "paperId": "d17c6cb9c0d8ec64ce8d1eff653ac72b15df683a",
        "title": "Reinforcement Learning through Active Inference"
      },
      {
        "paperId": "46ef96b178f0393f5d01998d7179f347d2e3bbfd",
        "title": "Scalable Quantitative Verification for Deep Neural Networks"
      },
      {
        "paperId": "7bbeb74513b34ab148ed79c8d824a582959850c2",
        "title": "Distributed Deep Reinforcement Learning for Drone Swarm Control"
      },
      {
        "paperId": "5b925493d2812a5077c4e135cd07bf6647ba59df",
        "title": "Efficient Exploration in Side-Scrolling Video Games with Trajectory Replay"
      },
      {
        "paperId": "22005795db3a0e85c9091a855427a39b5f8bb33a",
        "title": "Neural Embedding for Physical Manipulations"
      },
      {
        "paperId": "def8207fb6457f3c2656bac9f2bbf954de1d7de1",
        "title": "Deep Active Inference as Variational Policy Gradients"
      },
      {
        "paperId": "ffb3886a253ff927bcc46b78e00409893865a68e",
        "title": "Dynamics-Aware Unsupervised Discovery of Skills"
      },
      {
        "paperId": "39e299cde9053346284b074d8ca58e083ad85cbd",
        "title": "Learning-Driven Exploration for Reinforcement Learning"
      },
      {
        "paperId": "28348ca34d609bd52ff10705bf396cc1da3e85ab",
        "title": "Adaptive Variance for Changing Sparse-Reward Environments"
      },
      {
        "paperId": "1430ff8b67a5a221dd719eff6e1c9dee6adbc595",
        "title": "A Strongly Asymptotically Optimal Agent in General Environments"
      },
      {
        "paperId": "a7a410510ce9f73ab1a36f136eae852c8f159eb9",
        "title": "A Bandit Framework for Optimal Selection of Reinforcement Learning Agents"
      },
      {
        "paperId": "b8c0071c74e04ea1598ed2a208cbc255656f50b0",
        "title": "A Meta-MDP Approach to Exploration for Lifelong Reinforcement Learning"
      },
      {
        "paperId": "a962b134cc85fc15af2aca7c3cf8655b2ca70d94",
        "title": "A Comprehensive guide to Bayesian Convolutional Neural Network with Variational Inference"
      },
      {
        "paperId": "6b0f75480a88d6b392a9f44d89b2cea23415a4c7",
        "title": "Using State Predictions for Value Regularization in Curiosity Driven Deep Reinforcement Learning"
      },
      {
        "paperId": "c46d80f83813fba0e8363a0ab36a19fba062540e",
        "title": "Learning Actionable Representations with Goal-Conditioned Policies"
      },
      {
        "paperId": "787c563657c34041357b5a92354a69b05c435032",
        "title": "Curiosity Driven Exploration of Learned Disentangled Goal Spaces"
      },
      {
        "paperId": "1f40de93f0ad85809bd82f5923ee741832874353",
        "title": "Diffusion-Based Approximate Value Functions"
      },
      {
        "paperId": "c7126ac7b4edc84f1d363e6e478ac2e18901fd41",
        "title": "Bayesian Convolutional Neural Networks with Variational Inference"
      },
      {
        "paperId": "9a26c34cc2f56ed981e8bddd374b17076d6da7f2",
        "title": "Uncertainty Estimations by Softplus normalization in Bayesian Convolutional Neural Networks with Variational Inference"
      },
      {
        "paperId": "25fc42760413478a1d6c8396a0f3327d1e97f59b",
        "title": "Bayesian Convolutional Neural Networks"
      },
      {
        "paperId": "43879cf527f4918955fd55128baa6745174d8555",
        "title": "Graph networks as learnable physics engines for inference and control"
      },
      {
        "paperId": "92027f3111b968cde0d2d15e2c7ca087b161d53d",
        "title": "Review of Intrinsic Motivation in Simulation-based Game Testing"
      },
      {
        "paperId": "ed07da98cef78a92e0f0f7e9cd0475a0e26794b8",
        "title": "BLS: A learning search algorithm with Bayesian learning"
      },
      {
        "paperId": "1b9ce6abc0f3024b88fcd4dbd0c10cf5bcf7d38d",
        "title": "DeepMimic"
      },
      {
        "paperId": "485445ad19e79c9f81c2236105f47b1a7f85863c",
        "title": "Model-Based Action Exploration for Learning Dynamic Motion Skills"
      },
      {
        "paperId": "468c522d7ee3fad66cdfd86ea95bc90351a87600",
        "title": "Model-Based Action Exploration"
      },
      {
        "paperId": "e437d877d141cfff1e1becc02726854060d26136",
        "title": "A Developmental Approach to Machine Learning?"
      },
      {
        "paperId": "1fdf0319b4a1db62c611980351e5f4c2f08958cd",
        "title": "GEP-PG: Decoupling Exploration and Exploitation in Deep Reinforcement Learning Algorithms"
      },
      {
        "paperId": "da981e00c5f4a2aa931d02ae2a3211b0a0ac1b63",
        "title": "Emotion in reinforcement learning agents and robots: a survey"
      },
      {
        "paperId": "8499a250422a3c66357367c8d5fa504de5424c59",
        "title": "Intrinsic Motivation and Automatic Curricula via Asymmetric Self-Play"
      },
      {
        "paperId": "4aa5334b7683019ec60794a483a18b48557a22c2",
        "title": "Transfer Reinforcement Learning with Shared Dynamics"
      },
      {
        "paperId": "c930a04f34ab31ace3d4e2f7683b4ce392329335",
        "title": "Exploration Potential"
      },
      {
        "paperId": "f826381aea632791b6007e427a9587c11b239b6a",
        "title": "Efficient Exploration for Dialog Policy Learning with Deep BBQ Networks \\& Replay Buffer Spiking"
      },
      {
        "paperId": "10b85d9ee5c3188d6bddb1cd674d9c6f9566afd7",
        "title": "Efficient Dialogue Policy Learning with BBQ-Networks"
      },
      {
        "paperId": "885fe11ed7ab81c8609ccddb3e10f62577c04ab9",
        "title": "BBQ-Networks: Efficient Exploration in Deep Reinforcement Learning for Task-Oriented Dialogue Systems"
      },
      {
        "paperId": "ef32487c463d918ddd232e6a693861d2b48e4b85",
        "title": "Haptics-based Curiosity for Sparse-reward Tasks"
      },
      {
        "paperId": "e46ad66a25a0155e9d61b4cbd1b1251a375ddde8",
        "title": "A Survey On Model-Free Deep Reinforcement Learning"
      },
      {
        "paperId": "d1923bd80314d8dea79f9643384870e33bb5663e",
        "title": "Residual Policy Learning"
      },
      {
        "paperId": "6c93fd327df5a0a95053929a1fad1881ae623ca2",
        "title": "THE PURDUE UNIVERSITY GRADUATE SCHOOL STATEMENT OF THESIS APPROVAL"
      },
      {
        "paperId": "5afdf86364d79a9f7173dab39ae5c20674f4d3de",
        "title": "Fast online model learning for controlling complex real-world robots"
      },
      {
        "paperId": "a1a536d92551b6690761e36f413f53e8ce923d33",
        "title": "Sense, Think, Grasp: A study on visual and tactile information processing for autonomous manipulation"
      },
      {
        "paperId": "4c5dca886f1d5cc4213ece9cfb7895c7f48fd1be",
        "title": "Scalable deep reinforcement learning for physics-based motion control"
      },
      {
        "paperId": "e610df55e71a666381e3c24cecaa3f49a39e29da",
        "title": "Elements of Intelligence: Memory, Communication and Intrinsic Motivation"
      },
      {
        "paperId": "97c634c3b85ef4d78451cca6efb8811617f4ff2f",
        "title": "Learning Goal-Directed Behaviour"
      },
      {
        "paperId": "e31692a74427b58b6154e37da7535e142ceceb4b",
        "title": "Optimizing Expectations: From Deep Reinforcement Learning to Stochastic Computation Graphs"
      },
      {
        "paperId": "3e59b3e1e3ef65f9574a0fe30f18ba7a815ea0af",
        "title": "Ef\ufb01cient Exploration for Dialogue Policy Learning with BBQ Networks & Replay Buffer Spiking"
      },
      {
        "paperId": null,
        "title": "Task Episode ( supervision from external reward ) Alice \u2019 s turn Bob \u2019 s turn"
      },
      {
        "paperId": "8c513d52a5fcb28e4c310860d14418642f1e220d",
        "title": "Active Vision with Predictive Coding and Uncertainty Minimization"
      },
      {
        "paperId": "b3fbaae23c21248a549ac487f5d133b312200ecf",
        "title": "Toward Reliable Robot Navigation Using Deep Reinforcement Learning"
      }
    ],
    "score": 8.777777777777777
  },
  {
    "id": "248a25d697fe0132840e9d03c00aefadf03408d8",
    "title": "Offline Reinforcement Learning for Autonomous Driving with Safety and Exploration Enhancement",
    "authors": [
      "Tianyu Shi",
      "Dong Chen",
      "Kaian Chen",
      "Zhaojian Li"
    ],
    "year": 2021,
    "citationCount": 35,
    "abstract": "Reinforcement learning (RL) is a powerful data-driven control method that has been largely explored in autonomous driving tasks. However, conventional RL approaches learn control policies through trial-and-error interactions with the environment and therefore may cause disastrous consequences such as collisions when testing in real-world traffic. Offline RL has recently emerged as a promising framework to learn effective policies from previously-collected, static datasets without the requirement of active interactions, making it especially appealing for autonomous driving applications. Despite promising, existing offline RL algorithms such as Batch-Constrained deep Q-learning (BCQ) generally lead to rather conservative policies with limited exploration efficiency. To address such issues, this paper presents an enhanced BCQ algorithm by employing a learnable parameter noise scheme in the perturbation model to increase the diversity of observed actions. In addition, a Lyapunov-based safety enhancement strategy is incorporated to constrain the explorable state space within a safe region. Experimental results in highway and parking traffic scenarios show that our approach outperforms the conventional RL method, as well as state-of-the-art offline RL algorithms.",
    "url": "https://www.semanticscholar.org/paper/248a25d697fe0132840e9d03c00aefadf03408d8",
    "pdf_url": "https://arxiv.org/pdf/2110.07067.pdf",
    "venue": "arXiv.org",
    "publicationDate": "2021-10-13",
    "externalIds": {
      "ArXiv": "2110.07067",
      "DBLP": "journals/corr/abs-2110-07067",
      "CorpusId": 238857298
    },
    "references": [
      {
        "paperId": "bec834d11bc738eab0d380956cc8673d634821eb",
        "title": "Human-in-the-Loop Deep Reinforcement Learning with Application to Autonomous Driving"
      },
      {
        "paperId": "5e7bc93622416f14e6948a500278bfbe58cd3890",
        "title": "Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems"
      },
      {
        "paperId": "4350f9b0fb5c2fbe6791cab4990d42908358bed7",
        "title": "Neural Lyapunov Control"
      },
      {
        "paperId": "c66b647faa4f2970efb0aea2f961a8789cd97db3",
        "title": "Interpretable End-to-End Urban Autonomous Driving With Latent Deep Reinforcement Learning"
      },
      {
        "paperId": "65438016ef8e37c202b22669ccf001d5b0b37027",
        "title": "Learning Stable Deep Dynamics Models"
      },
      {
        "paperId": "48f18922208ed1bfb5d3d4e691f2b3ad29adf0cb",
        "title": "Autonomous Driving using Safe Reinforcement Learning by Incorporating a Regret-based Human Lane-Changing Decision Model"
      },
      {
        "paperId": "1534d860ab98b64d23ffa741d0ae52b6cadbf503",
        "title": "Benchmarking Batch Deep Reinforcement Learning Algorithms"
      },
      {
        "paperId": "9be492858863c8c7c24be1ecb75724de5086bd8e",
        "title": "Behavior Regularized Offline Reinforcement Learning"
      },
      {
        "paperId": "17f0525751da4440d6e5561a63cc709017ff577a",
        "title": "Driving Decision and Control for Autonomous Lane Change based on Deep Reinforcement Learning"
      },
      {
        "paperId": "5285cb8faada5de8a92a47622950f6cfd476ac1d",
        "title": "Off-Policy Deep Reinforcement Learning without Exploration"
      },
      {
        "paperId": "3f19ea8cbcbdcc66ce12af138f181a35d703cca4",
        "title": "Automated Driving Maneuvers under Interactive Environment based on Deep Reinforcement Learning"
      },
      {
        "paperId": "3fab4d28be5ff73f42c8b307117aa8d680345ad9",
        "title": "Noisy Networks for Exploration"
      },
      {
        "paperId": "d02a971f787c51adcd379278de8c4a3351bfe587",
        "title": "The kinematic bicycle model: A consistent model for planning feasible trajectories for autonomous vehicles?"
      },
      {
        "paperId": "142497432fe179ddb6ffe600c64a837ec6179550",
        "title": "Parameter Space Noise for Exploration"
      },
      {
        "paperId": "8db9df2eadea654f128c1887722c677c708e8a47",
        "title": "Deep Reinforcement Learning framework for Autonomous Driving"
      },
      {
        "paperId": "ee9aae0190c3ff177e7d1c9cdc977ce7d971bcc1",
        "title": "Input Convex Neural Networks"
      },
      {
        "paperId": "024006d4c2a89f7acacc6e4438d156525b60a98f",
        "title": "Continuous control with deep reinforcement learning"
      },
      {
        "paperId": "5f5dc5b9a2ba710937e2c413b37b053cd673df02",
        "title": "Auto-Encoding Variational Bayes"
      },
      {
        "paperId": "2319a491378867c7049b3da055c5df60e1671158",
        "title": "Playing Atari with Deep Reinforcement Learning"
      },
      {
        "paperId": "9cd8193a66cf53143cbba6ccb0c7b9c2ebf2452b",
        "title": "Self-improving reactive agents based on reinforcement learning, planning and teaching"
      },
      {
        "paperId": "e84b3f8b7bee67c44315cbae7639a7ffa56ea56f",
        "title": "Extracting Traf\ufb01c Smoothing Controllers Directly From Driving Data using Of\ufb02ine RL"
      },
      {
        "paperId": "40df15ce1de6eb0366179f4726aa55a3e50141fc",
        "title": "Learning to Explore via Meta-Policy Gradient"
      },
      {
        "paperId": null,
        "title": "An environment for autonomous driving decision-making"
      },
      {
        "paperId": "a726f99e6152a39bd8743ccec7986e974be9c921",
        "title": "Nonlinear system I"
      },
      {
        "paperId": "97efafdb4a3942ab3efba53ded7413199f79c054",
        "title": "Reinforcement Learning: An Introduction"
      }
    ],
    "cited_by": [
      {
        "paperId": "9685caad09612ae3b645cb78f36cd05f0afbab3d",
        "title": "Wavelet Fourier Diffuser: Frequency-Aware Diffusion Model for Reinforcement Learning"
      },
      {
        "paperId": "7f24f4435120e586bdb62c36b36aad54171cfd6b",
        "title": "RAD: Retrieval High-quality Demonstrations to Enhance Decision-making"
      },
      {
        "paperId": "1227cbf57898da69faff7da0adc7e1b4241ec6ce",
        "title": "Advancing Narrow Space Parking with Latent Memory-Based Reinforcement Learning"
      },
      {
        "paperId": "07e5a5b7aea02110ac9ef295174595ae21514667",
        "title": "Offline Reinforcement Learning using Human-Aligned Reward Labeling for Autonomous Emergency Braking in Occluded Pedestrian Crossing"
      },
      {
        "paperId": "af66187d02889719bd1dd410794413c6db9c2f3a",
        "title": "Don't Trade Off Safety: Diffusion Regularization for Constrained Offline RL"
      },
      {
        "paperId": "53908044d5ffc9ccd86fd65bf950d4d811d99d06",
        "title": "Primal-Dual Spectral Representation for Off-policy Evaluation"
      },
      {
        "paperId": "7c6a710228787e1e109bee0cb852506038c1ee85",
        "title": "PoRank: A Practical Framework for Learning to Rank Policies"
      },
      {
        "paperId": "777c5e1df416c32b4ee12eb33eead320d671a0af",
        "title": "Enhanced Safety in Autonomous Driving: Integrating a Latent State Diffusion Model for End-to-End Navigation"
      },
      {
        "paperId": "f4a2f4c44a05063777099450a6afcaaf70319654",
        "title": "Leaky PPO: A Simple and Efficient RL Algorithm for Autonomous Vehicles"
      },
      {
        "paperId": "0778ce4d368b39a68fbfeb9d5f70989c89b4a02a",
        "title": "Urban-Focused Multi-Task Offline Reinforcement Learning with Contrastive Data Sharing"
      },
      {
        "paperId": "2abdd73d48fc5bbf7cfe0240eed17eeb9471266e",
        "title": "Real-Data-Driven Offline Reinforcement Learning for Autonomous Vehicle Speed Decision Making"
      },
      {
        "paperId": "699d6ba5a173b30df697bc7514b8b1b03d35d762",
        "title": "State-Constrained Offline Reinforcement Learning"
      },
      {
        "paperId": "b708bcc30c3387c19d71ab1b60c7c8b51569abc6",
        "title": "Offline Reinforcement Learning with Domain-Unlabeled Data"
      },
      {
        "paperId": "6598fdfcf73078ad7a09cdc879ee2041985fc7cc",
        "title": "AD4RL: Autonomous Driving Benchmarks for Offline Reinforcement Learning with Value-based Dataset"
      },
      {
        "paperId": "eb19304efccd3398ab2c78e8724a5589b92bb100",
        "title": "Context-aware LLM-based Safe Control Against Latent Risks"
      },
      {
        "paperId": "a7c9db42d9f07c7ed044d5efa9d6b30fc23baac9",
        "title": "Learning Goal-Conditioned Policies from Sub-Optimal Offline Data via Metric Learning"
      },
      {
        "paperId": "1708226d2ca2c9f03ccece4e25def9cf6918ef42",
        "title": "Contrastive Diffuser: Planning Towards High Return States via Contrastive Learning"
      },
      {
        "paperId": "8b79b3615eb2f2ee8ce6e988313398b8f42305f4",
        "title": "DiffStitch: Boosting Offline Reinforcement Learning with Diffusion-based Trajectory Stitching"
      },
      {
        "paperId": "3008a19d035059538ad439b47b00ef8a5778211a",
        "title": "CAC: Enabling Customer-Centered Passenger-Seeking for Self-Driving Ride Service with Conservative Actor-Critic"
      },
      {
        "paperId": "82ff15c65a7ab93f1c43cc7f383ba8fd9415923d",
        "title": "Uncertainty-Aware Decision Transformer for Stochastic Driving Environments"
      },
      {
        "paperId": "341975e953eae14932b708d7f80e378d11013a57",
        "title": "Boosting Offline Reinforcement Learning for Autonomous Driving with Hierarchical Latent Skills"
      },
      {
        "paperId": "4874a7e4673f27e3eceb8afa34339c364fe7cc8f",
        "title": "Optimizing Traffic Control with Model-Based Learning: A Pessimistic Approach to Data-Efficient Policy Inference"
      },
      {
        "paperId": "99b0c3a18050889c591e1db6d51ca01298638437",
        "title": "Transformers in Reinforcement Learning: A Survey"
      },
      {
        "paperId": "cb596f8c310a6ed9e7bcd74017a4b1418ed99518",
        "title": "Rethinking Closed-Loop Training for Autonomous Driving"
      },
      {
        "paperId": "872bd103f8d7ec9954e8c4366e03f583753a0d39",
        "title": "Lyapunov Stability Regulation of Deep Reinforcement Learning Control with Application to Automated Driving"
      },
      {
        "paperId": "44ba91854390e6795681e022adbeabfc51359bbc",
        "title": "A Model-Based Solution to the Offline Multi-Agent Reinforcement Learning Coordination Problem"
      },
      {
        "paperId": "c5fc646d2c7466affa9eae7c69ac2d0b44d888ba",
        "title": "Learning to Control Autonomous Fleets from Observation via Offline Reinforcement Learning"
      },
      {
        "paperId": "1bbf1c6e56c26d829e6fc8032168e3b1ad1d7c10",
        "title": "Distributionally Robust Offline Reinforcement Learning with Linear Function Approximation"
      },
      {
        "paperId": "8856b6e9a0d79d674d772c175e95fb03e9e4a5f5",
        "title": "An Empirical Study of Implicit Regularization in Deep Offline RL"
      },
      {
        "paperId": "634b6aa1f7f7296faf0b42cfdcc14f922082780c",
        "title": "Offline Reinforcement Learning for Road Traffic Control"
      },
      {
        "paperId": "b90dbe327bde8ce4d4055231d08fc64de318e01c",
        "title": "ContraDiff: Planning Towards High Return States via Contrastive Learning"
      },
      {
        "paperId": "18fe404812bbbe4964173befc4f0e86c20e2539e",
        "title": "Goal-Conditioned Offline Reinforcement Learning via Metric Learning"
      },
      {
        "paperId": "7c12e0b3495fe2fff960c3ee0ac350c72fa7553a",
        "title": "Multi-Agent Reinforcement Learning from Human Feedback: Data Coverage and Algorithmic Techniques"
      },
      {
        "paperId": "846661f07ac35af98674a57ccf524255fec6ed17",
        "title": "Provably Efficient Offline Reinforcement Learning for Partially Observable Markov Decision Processes"
      },
      {
        "paperId": "7e099edd2827c6fabeb92ff9fae733910f5038f5",
        "title": "NondBREM: Nondeterministic Of\ufb02ine Reinforcement Learning for Large-Scale Order Dispatching"
      }
    ],
    "score": 8.75
  },
  {
    "id": "7d1722451bff3b95e1d082864bb9b8437a0ddf41",
    "title": "UAV Networks Against Multiple Maneuvering Smart Jamming With Knowledge-Based Reinforcement Learning",
    "authors": [
      "Zhiwei Li",
      "Yu Lu",
      "Xi Li",
      "Zeng-Guang Wang",
      "Wenxin Qiao",
      "Yicen Liu"
    ],
    "year": 2021,
    "citationCount": 35,
    "abstract": "The unmanned aerial vehicles (UAVs) networks are very vulnerable to smart jammers that can choose their jamming strategy based on the ongoing channel state accordingly. Although reinforcement learning (RL) algorithms can give UAV networks the ability to make intelligent decisions, the high-dimensional state space makes it difficult for algorithms to converge quickly. This article proposes a knowledge-based RL method, which uses domain knowledge to compress the state space that the agent needs to explore and then improve the algorithm convergence speed. Specifically, we use the inertial law of the aircraft and the law of signal attenuation in free space to guide the highly efficient exploration of the UAVs in the state space. We incorporate the performance indicators of the receiver and the subjective value of the task into the design of the reward function, and build a virtual environment for pretraining to accelerate the convergence of anti-jamming decisions. In addition, the algorithm proposed is completely based on observable data, which is more realistic than those studies that assume the position or the channel strategy of the jammer. The simulation shows that the proposed algorithm can outperform the benchmarks of model-free RL algorithm in terms of converge speed and averaged reward.",
    "url": "https://www.semanticscholar.org/paper/7d1722451bff3b95e1d082864bb9b8437a0ddf41",
    "pdf_url": "https://doi.org/10.1109/JIOT.2021.3062659",
    "venue": "IEEE Internet of Things Journal",
    "publicationDate": "2021-08-01",
    "externalIds": {
      "DBLP": "journals/iotj/LiLLWQL21",
      "MAG": "3134853632",
      "DOI": "10.1109/JIOT.2021.3062659",
      "CorpusId": 234307584
    },
    "references": [],
    "cited_by": [
      {
        "paperId": "cfd99eb3f0e5cea92307df2ec9e5862840628086",
        "title": "Resource Allocation for UAV Swarm-Assisted Green ISAC Networks via Multi-Agent RL"
      },
      {
        "paperId": "adb59391ca968dfc2834746790bcaf3d964e8a29",
        "title": "Agent-Based Anti-Jamming Techniques for UAV Communications in Adversarial Environments: A Comprehensive Survey"
      },
      {
        "paperId": "52c54d3d8a0f392d4b245f79c7cd21c7f33c1a1d",
        "title": "Structured Cross-Task Transfer Reinforcement Learning for Mobility Control based UAV Anti-Jamming Communications"
      },
      {
        "paperId": "3fa30b6bb700b6acf0e63d3b7a7bce7c66392b40",
        "title": "Frequency Point Game Environment for UAVs via Expert Knowledge and Large Language Model"
      },
      {
        "paperId": "823013b7545b86acfdb2b290534288471ac61dad",
        "title": "Game Theory and Reinforcement Learning for Anti-Jamming Defense in Wireless Communications: Current Research, Challenges, and Solutions"
      },
      {
        "paperId": "603dc8c45061c1fc46d493209bc53d542f61d747",
        "title": "Defending against jamming and interference for Internet of UAVs using cooperative multi-agent reinforcement learning with mutual information"
      },
      {
        "paperId": "62cab588ced883447b5c31d2dff24c5dd3c1f413",
        "title": "Robust Spectrum Access Scheme Against Diverse Jamming Policies: A Prioritized Fictitious Rival-Play-Based Approach"
      },
      {
        "paperId": "9721bfb328b408386dd918a509e20884f6979137",
        "title": "UAV Communication Against Intelligent Jamming: A Stackelberg Game Approach With Federated Reinforcement Learning"
      },
      {
        "paperId": "36d7d930fccb6d26f8d2c5d4e2cba3a87e548017",
        "title": "Game Theory and Multi\u2013Agent DRL Based Anti-Jamming Transmission for Integrated Air-Ground Network"
      },
      {
        "paperId": "8216bd1f99d39b07b52980f543ae2a364b09e07d",
        "title": "Capacity Enhancement Frequency Hopping System Under Multi Input Multi Output Jamming Channel"
      },
      {
        "paperId": "8a24eda5d0a5f6d218a1fc84e3c0c736b454092b",
        "title": "A Survey on Security of UAV Swarm Networks: Attacks and Countermeasures"
      },
      {
        "paperId": "e34b3c18d89da8350600e7c63fb610324be8f9a4",
        "title": "A survey on security of UAV and deep reinforcement learning"
      },
      {
        "paperId": "3fead837b87489867ba29ce9ceefc35bf9b777b2",
        "title": "Dynamic Spectrum Anti-Jamming for Cognitive UAV Networks Against Reactive Jamming"
      },
      {
        "paperId": "251d18116016e5d01722f82afa33c2a9687364ad",
        "title": "Meta-Reinforcement Learning in Time-Varying UAV Communications: Adaptive Anti-Jamming Channel Selection"
      },
      {
        "paperId": "ac86a0e98c0e5be2f3d74ccb7f28c43c1ddc84a9",
        "title": "An Intelligent Strategy Decision Method for Collaborative Jamming Based on Hierarchical Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "b73b8f1567f91e48863d7301dd223aa5eb23055b",
        "title": "Holistic Review of UAV-Centric Situational Awareness: Applications, Limitations, and Algorithmic Challenges"
      },
      {
        "paperId": "89c263f07ce0e16c2b18fbded43532e0cb3c0a5d",
        "title": "Task Assignment in Multi-Agent Games via Reinforcement Learning and Expert Knowledge"
      },
      {
        "paperId": "71fead0a33bbe8639578cc66a3d297aed8f18032",
        "title": "Multidimensional Resource Management for Distributed MEC Networks in Jamming Environment: A Hierarchical DRL Approach"
      },
      {
        "paperId": "4908b1d33597242e7742b4b1425db218974dcab6",
        "title": "Improving Physical Layer Security for multi-UAV Systems Against Hybrid Wireless Attacks"
      },
      {
        "paperId": "e17d666eaecd0717cc3c58b480d5cabe901fd863",
        "title": "Deep Learning for Secure UAV-Assisted RIS Communication Networks"
      },
      {
        "paperId": "1f8f762d8f64ad73de6703b1b8c9f3f3e62b9108",
        "title": "Optimization-Driven DRL-Based Joint Beamformer Design for IRS-Aided ITSN Against Smart Jamming Attacks"
      },
      {
        "paperId": "6df8dd21572a2dadfc604b001bef54d8c100c617",
        "title": "Resilient Path Planning for UAVs in Data Collection Under Adversarial Attacks"
      },
      {
        "paperId": "7939a707d44c0a26c933451fc47b958c113bc083",
        "title": "GAN-powered heterogeneous multi-agent reinforcement learning for UAV-assisted task offloading"
      },
      {
        "paperId": "26a659de4a2dc9bf1de52f8ea4512a31e09d51ea",
        "title": "Task Assignment with Minimum Cost for Multi-UAV System via Reinforcement Learning*"
      },
      {
        "paperId": "d86e73a18c008722e931976b62a9b0564fde83e0",
        "title": "UAV Anti-Jamming Communications With Power and Mobility Control"
      },
      {
        "paperId": "b6dd378d2c0ee155eec1928c95dcf5248505ede5",
        "title": "Aerial Reconfigurable Intelligent Surface-Aided Wireless Communications Against the Moving Jamming"
      },
      {
        "paperId": "48e42f72d4ac81086a76d4890c2abb10985b850f",
        "title": "RIS-Assisted Jamming Rejection and Path Planning for UAV-Borne IoT Platform: A New Deep Reinforcement Learning Framework"
      },
      {
        "paperId": "529db11d055b0e5368423633eaaee95e640721fc",
        "title": "5G Physical Layer Resiliency Enhancements with NB-IoT Use Case Study"
      },
      {
        "paperId": "4070325414f3ceedeb45461c21039dcf8c1caca4",
        "title": "Recurrent-Neural-Network-Based Anti-Jamming Framework for Defense Against Multiple Jamming Policies"
      },
      {
        "paperId": "cdd83966c8d9e7ca849a6fea7efaf82cc0593636",
        "title": "Resilient UAV Path Planning for Data Collection under Adversarial Attacks"
      },
      {
        "paperId": "f6f146c005c4a28b0810494e7e44d5efa50e3d13",
        "title": "\u201cLure the Enemy in Deep\u201d: Confronting Rogue UAV Through Diverse Hybrid Jamming"
      },
      {
        "paperId": "bb435bfaafab13c96c35a6003809895c630c02ad",
        "title": "RL-Based Adaptive UAV Swarm Formation and Clustering for Secure 6G Wireless Communications in Dynamic Dense Environments"
      },
      {
        "paperId": "25af6b8af9c7031e485115f1b13dd5c605fa164b",
        "title": "Reinforcement Learning-Based Physical Cross-Layer Security and Privacy in 6G"
      },
      {
        "paperId": "55511a0e3a9aea5dde042952d38a081fab9dc76f",
        "title": "Toward Autonomous Multi-UAV Wireless Network: A Survey of Reinforcement Learning-Based Approaches"
      },
      {
        "paperId": "eee4b2249c363124f85f25c1780e8cef67505dab",
        "title": "Multi-agent collaboration based UAV clusters multi-domain energy-saving anti-jamming communication"
      }
    ],
    "score": 8.75
  },
  {
    "id": "b0d376434a528ee69d98174d75b4a571c53247ae",
    "title": "Feature Augmentation with Reinforcement Learning",
    "authors": [
      "Jiabin Liu",
      "Chengliang Chai",
      "Yuyu Luo",
      "Yin Lou",
      "Jianhua Feng",
      "Nan Tang"
    ],
    "year": 2022,
    "citationCount": 26,
    "abstract": "Sufficient good features are indispensable to train well-performed machine learning models. However, it is com-mon that good features are not always enough, where feature augmentation is necessary to enrich high-quality features by joining with other tables. There are two main challenges for the problem. Given a set of tables where we can augment features from, the first challenge is that there are a lot of ways of joining multiple tables and deciding which features (or attributes) to use - selecting the best set of features to augment is hard. Moreover, we may need to materialize the join results for different join options, doing full materialization might be time consuming - efficient but approximate methods are needed. In this paper, we first introduce the design space of the feature augmentation problem. Then, to address the above challenges, we propose a reinforcement learning based framework, namely AutoFeature, to augment the features following an exploration-exploitation strategy. AutoFeature keeps exploring the features in tables that have led to performance improvement. At the same time, AutoFeature also exploits the tables (features) that are rarely selected. In this way, the search space of tables (features) to be augmented can be well explored and a subset of good features can be selected. AutoFeature utilizes sampling techniques to achieve high efficiency. We implement two algorithms, one with multi-arm bandit and the other with branch Deep Q Networks (branch DQN), to realize the framework of AutoFeature. We conducted experiments on three real-world datasets School/XuetangE/Air using 16/23/34 candidate tables with 695/204/338 candidate features. Extensive results show that AutoFeature outperforms other methods by 12.4% and 9.8% on AUC values on two classification datasets (School and XuetangE) and by 0.113 on the MSE value on Air in terms of the model performance.",
    "url": "https://www.semanticscholar.org/paper/b0d376434a528ee69d98174d75b4a571c53247ae",
    "pdf_url": "https://doi.org/10.1109/icde53745.2022.00317",
    "venue": "IEEE International Conference on Data Engineering",
    "publicationDate": "2022-05-01",
    "externalIds": {
      "DBLP": "conf/icde/LiuCLLFT22",
      "DOI": "10.1109/icde53745.2022.00317",
      "CorpusId": 251303310
    },
    "references": [],
    "cited_by": [
      {
        "paperId": "bce89d255a23873e1f371601562feb7d02aa6800",
        "title": "Toast: Task-Oriented Multi-dimensional Augmentation for Spatio-Temporal Trajectory Data"
      },
      {
        "paperId": "feef8c62aa4bec0c01ad0392409fdb79d13db398",
        "title": "Featpilot: Automatic Feature Augmentation on Tabular Data"
      },
      {
        "paperId": "6c5c0b8ef3cf232850d88e00e9881dacd7c0cbe5",
        "title": "FeatInsight: An Online ML Feature Management System on 4Paradigm Sage-Studio Platform"
      },
      {
        "paperId": "36f7876f5d622f3b76ec3424434e7b5e5855e55a",
        "title": "Game Theory Meets Data Augmentation"
      },
      {
        "paperId": "69df5c3726682c28689d8f21782c5d597c2dfe17",
        "title": "Human-inspired Perspectives: A Survey on AI Long-term Memory"
      },
      {
        "paperId": "c9b515315cd58fdf3e5b440bd0dc9a1d52fd0f3d",
        "title": "AutoFeat: Transitive Feature Discovery over Join Paths"
      },
      {
        "paperId": "4c39eafbf8dc8e031d7eab7c8a0dbdcedf22d432",
        "title": "Mitigating Data Scarcity in Supervised Machine Learning Through Reinforcement Learning Guided Data Generation"
      },
      {
        "paperId": "d4eb53da62a6c28ede9804ec12812334a6e90e00",
        "title": "Spatial Transfer Learning for Estimating PM2.5 in Data-Poor Regions"
      },
      {
        "paperId": "e5f36c50df96be4a80b959fa1a5e417c57c5830c",
        "title": "Efficiently Estimating Mutual Information Between Attributes Across Tables"
      },
      {
        "paperId": "bb4fd59430d0592630b87fba8adbd93f73d92719",
        "title": "FeatAug: Automatic Feature Augmentation From One-to-Many Relationship Tables"
      },
      {
        "paperId": "33c4cea9159564a7bc4239f183c7b0bf6ecc4fff",
        "title": "Retrieve, Merge, Predict: Augmenting Tables with Data Lakes"
      },
      {
        "paperId": "04d131409509b40b183bbe244037ae57f1eb8d21",
        "title": "HOFD: An Outdated Fact Detector for Knowledge Bases"
      },
      {
        "paperId": "1e00abdb38c77e82a69e5bf2ebd34c8e94a7f56e",
        "title": "Sampling Methods for Inner Product Sketching"
      },
      {
        "paperId": "86ab1b3c7af0bf58196685a76833c1eb6d9359c8",
        "title": "GoodCore: Data-effective and Data-efficient Machine Learning through Coreset Selection over Incomplete Data"
      },
      {
        "paperId": "b2da80ff14600017f597d32c0bf7609568fe30fd",
        "title": "Demystifying Artificial Intelligence for Data Preparation"
      },
      {
        "paperId": "f0166047e2c0f718b132dfa4ae4d9073146c0848",
        "title": "Learned Data-aware Image Representations of Line Charts for Similarity Search"
      },
      {
        "paperId": "d590ad6e83c9aa5d4fcdff11d514f38f5c560107",
        "title": "An Adaptive Elastic Multi-model Big Data Analysis and Information Extraction System"
      },
      {
        "paperId": "6d59a3d559252cdb8d2303f52335ffbf1158520c",
        "title": "Efficient Partitioning Method for Optimizing the Compression on Array Data"
      },
      {
        "paperId": "0d1725a31eb75c4ea7d58afe7fc2a9e53ade1302",
        "title": "GAM: A GPU-Accelerated Algorithm for MaxRS Queries in Road Networks"
      },
      {
        "paperId": "7e3442349f280a52904bab71c97776d6609ed6bc",
        "title": "Incremental User Identification Across Social Networks Based on User-Guider Similarity Index"
      },
      {
        "paperId": "dc09ce51a7c4a805bf9f7031190648b20261098d",
        "title": "An Exercise Collection Auto-Assembling Framework with Knowledge Tracing and Reinforcement Learning"
      },
      {
        "paperId": "597b7a7b27928c940e1895b4b489d0993683a30d",
        "title": "Coresets over Multiple Tables for Feature-rich and Data-efficient Machine Learning"
      },
      {
        "paperId": "d5aa0efc065ac3073404aebbcc3a2ca3eb63db4b",
        "title": "Cost-based or Learning-based? A Hybrid Query Optimizer for Query Plan Selection"
      },
      {
        "paperId": "f3dc55c3ab1af01bfc365dd8a9f811e9deaca56c",
        "title": "Large Language Models for Data Discovery and Integration: Challenges and Opportunities"
      },
      {
        "paperId": "70f805ad4abb150f8a4cb6a7ccebb18cd541344e",
        "title": "Transfer Learning via Latent Dependency Factor for Estimating PM 2.5"
      },
      {
        "paperId": "4377c5f9730f82894a9f3da95f014301adebd01f",
        "title": "4DBInfer: A 4D Benchmarking Toolbox for Graph-Centric Predictive Modeling on RDBs"
      }
    ],
    "score": 8.666666666666666
  },
  {
    "id": "2822e79cb293f54e05fd8a7cd2844cfcc683f5d8",
    "title": "Plume Tracing via Model-Free Reinforcement Learning Method",
    "authors": [
      "Hangkai Hu",
      "Shiji Song",
      "C. L. Phillip Chen"
    ],
    "year": 2019,
    "citationCount": 50,
    "abstract": "This paper studies the plume-tracing strategy for an autonomous underwater vehicle (AUV) in the deep-sea turbulent environment. The tracing problem is modeled as a partially observable Markov decision process with continuous state space and action space due to the spatio-temporal changes of environment. An long short-term memory-based reinforcement learning framework with full use of history information is proposed to generate a smooth strategy while the AUV interacting with the environment. Continuous temporal difference and deterministic policy gradient methods are employed to improve the strategy. To promote the performance of the algorithm, a supervised strategy generated by dynamic programming methods is utilized as transcendental knowledge of the agent. Historical searching trajectory\u2019s form and the exploration technology are specially designed to fit the algorithm. Simulation environments are established based on Reynolds-averaged Navier\u2013Stokes equations and the effectiveness of the learned plume-tracing strategy is validated with simulation experiments.",
    "url": "https://www.semanticscholar.org/paper/2822e79cb293f54e05fd8a7cd2844cfcc683f5d8",
    "pdf_url": "https://doi.org/10.1109/TNNLS.2018.2885374",
    "venue": "IEEE Transactions on Neural Networks and Learning Systems",
    "publicationDate": "2019-08-01",
    "externalIds": {
      "DBLP": "journals/tnn/HuSC19",
      "MAG": "2906829782",
      "DOI": "10.1109/TNNLS.2018.2885374",
      "CorpusId": 58623810,
      "PubMed": "30605109"
    },
    "references": [],
    "cited_by": [
      {
        "paperId": "2d0dd8475e2fd1babf7c918ca33b35112862d0e7",
        "title": "Multi-source plume tracing via multi-agent reinforcement learning under common UAV-faults"
      },
      {
        "paperId": "811434cb88fb754e57b2c3b936ad3cf4326eb78f",
        "title": "Distributed Data\u2010Driven Formation Control for Unmanned Vehicle Networks Under Switching Topologies"
      },
      {
        "paperId": "dc932050a575a1115a1e2ff95bd5aa0f0addd7bc",
        "title": "Multi-source Plume Tracing via Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "da4bcfce7969fe8d798f9f12c24da6f71a45c7ec",
        "title": "Decoding olfactory mechanisms: molecular functions and robotic applications for odor source localization"
      },
      {
        "paperId": "f770d1ac319f861551d87974c5bff2f8001fd473",
        "title": "Multi-AUV sediment plume estimation using Bayesian optimization"
      },
      {
        "paperId": "df861b88cab058e3ec59726d8730d87db1df8bd9",
        "title": "Integrating Vision and Olfaction via Multi-Modal LLM for Robotic Odor Source Localization"
      },
      {
        "paperId": "8dcdbef8a0694baeed0dd61408c0961ace783157",
        "title": "Adaptive meta-reinforcement learning for AUVs 3D guidance and control under unknown ocean currents"
      },
      {
        "paperId": "ae9c81ebdfd1cf2bac3eb7543c77cb9be3b6cf1a",
        "title": "Adaptive Path Planning for Subsurface Plume Tracing with an Autonomous Underwater Vehicle"
      },
      {
        "paperId": "8b48ad2206f9fa899372244304f4183a6bf0c084",
        "title": "Search Agents Assisted by Wireless Sensor Networks: Cooperation Algorithms and Performance"
      },
      {
        "paperId": "5466809ad1281bab223f70cfcca52de858a516b5",
        "title": "GSL-Bench: High Fidelity Gas Source Localization Benchmarking Tool"
      },
      {
        "paperId": "f05de0deee1fe85e70415459beb6d48eb9d1b811",
        "title": "Olfactory search"
      },
      {
        "paperId": "6f6dd82db97ade72ff761f2381248c39273c6620",
        "title": "An Active Olfaction Approach Using Deep Reinforcement Learning for Indoor Attenuation Odor Source Localization"
      },
      {
        "paperId": "c05b5a83304acef25843a44b709fcf98d9bec3f1",
        "title": "Gas source localization using Dueling Deep Q-Network with an olfactory quadruped robot"
      },
      {
        "paperId": "208a41c604c2eaaa6e7ee700b01fc99186f56556",
        "title": "Pontryagin\u2019s Minimum Principle-Guided RL for Minimum-Time Exploration of Spatiotemporal Fields"
      },
      {
        "paperId": "c7f2a7929fbd2c0a541265d9743c55b975a94494",
        "title": "Robotic Odor Source Localization via Vision and Olfaction Fusion Navigation Algorithm"
      },
      {
        "paperId": "90632d9d29017274722d130c94d193424928c681",
        "title": "Reinforcement Learning for Source Location Estimation: A Multi-Step Approach"
      },
      {
        "paperId": "26f233512687af1a51b52c5fd66b7cfb4e6391e8",
        "title": "Robotic Odor Source Localization via End-to-End Recurrent Deep Reinforcement Learning"
      },
      {
        "paperId": "93078a56b5049e87e613cc9b04d8a4e06fe1310a",
        "title": "Deep Learning-Based Trajectory Planning and Control for Autonomous Ground Vehicle Parking Maneuver"
      },
      {
        "paperId": "3c29f3c954766b9a6583cc3e98eaf553f1815601",
        "title": "Error scaling\u2013based adaptive region tracking control for autonomous underwater vehicles"
      },
      {
        "paperId": "89b7ff3e7eaeffdfc8012b0126dd6ed10b77e45f",
        "title": "Learn to Trace Odors: Robotic Odor Source Localization via Deep Learning Methods with Real-world Experiments"
      },
      {
        "paperId": "c2e96bdbcb6075bf551012a043c273844e25fee8",
        "title": "Uncertainty-aware transfer across tasks using hybrid model-based successor feature reinforcement learning\u2606"
      },
      {
        "paperId": "3d50ce966c6cfe653c0153a1e6eec2eb3b2d6fa0",
        "title": "Odor Source Localization in Obstacle Regions Using Switching Planning Algorithms with a Switching Framework"
      },
      {
        "paperId": "a20389b7b65e8e2308613b4ef0c514b812fdd371",
        "title": "Design and Experimental Validation of Deep Reinforcement Learning-Based Fast Trajectory Planning and Control for Mobile Robot in Unknown Environment"
      },
      {
        "paperId": "45325be677a2e506cdb8423b5e75a63150ea3185",
        "title": "Practical Thinking of Neo-Confucianism in Qing Dynasty from the Scope of Practice Based on Deep Learning"
      },
      {
        "paperId": "44ab920db567a6ff07394287d5f3ab840ec42161",
        "title": "A reinforcement learning fuzzy system for continuous control in robotic odor plume tracking"
      },
      {
        "paperId": "a8a4904195df72e5af4c4f57e0225e0ac6142266",
        "title": "Abnormality Detection and Localization Schemes Using Molecular Communication Systems: A Survey"
      },
      {
        "paperId": "cdff7d57157abc21260ff2439f3c898bcd624c13",
        "title": "Asynchronous Multithreading Reinforcement-Learning-Based Path Planning and Tracking for Unmanned Underwater Vehicle"
      },
      {
        "paperId": "0c7a55e5ce0869ae71eed975d16813cc5924999a",
        "title": "AUV position tracking and trajectory control based on fast-deployed deep reinforcement learning method"
      },
      {
        "paperId": "fe609b4a2ab1ee99c50602f7ac1a2df8496356b5",
        "title": "Reinforcement Learning for Mobile Robotics Exploration: A Survey"
      },
      {
        "paperId": "3862d41354417d0bbcb74fcee7bafb7a06247df3",
        "title": "Robotic odor source localization via adaptive bio-inspired navigation using fuzzy inference methods"
      },
      {
        "paperId": "34f45b76d61fb0128e7e6daa1ab099450706a2a5",
        "title": "AKF-SR: Adaptive Kalman filtering-based successor representation"
      },
      {
        "paperId": "2061e0507391ba8d27653dd323f20a006e5596ae",
        "title": "A Deep Q-Network for robotic odor/gas source localization: Modeling, measurement and comparative study"
      },
      {
        "paperId": "cfc04f88acbafbe0add30bb59d9201fe9dbf1da2",
        "title": "Recent Progress and Trend of Robot Odor Source Localization"
      },
      {
        "paperId": "614312414d012affdba21a7630f576255787746d",
        "title": "Makf-Sr: Multi-Agent Adaptive Kalman Filtering-Based Successor Representations"
      },
      {
        "paperId": "cd88334ce75e08ad9298e1ac4c3437021cda2047",
        "title": "Attention-Based Meta-Reinforcement Learning for Tracking Control of AUV With Time-Varying Dynamics"
      },
      {
        "paperId": "e0642272d01afd867c090c7beddd37218616fcfd",
        "title": "Reinforcement learning in robotic applications: a comprehensive survey"
      },
      {
        "paperId": "56ef2bd0292b3fc8fc4fa4e1d5d84e32a3ab19d7",
        "title": "An Integrated Reinforcement Learning and Centralized Programming Approach for Online Taxi Dispatching"
      },
      {
        "paperId": "bdbaf20d667d81a848ffc37f665d90d93dfa7bef",
        "title": "Event-Triggered Control of Nonlinear Discrete-Time System With Unknown Dynamics Based on HDP(\u03bb)"
      },
      {
        "paperId": "9588ad817f1769c6fd8a52b7c3ce8b82cdc2c2fe",
        "title": "Robust Self-Adjustable Path-Tracking Control for Autonomous Underwater Vehicle"
      },
      {
        "paperId": "3cb6c06f57c6539e7b573aa01aa13ed6a4f9d0f4",
        "title": "Robust Self-Adjustable Path-Tracking Control for Autonomous Underwater Vehicle"
      },
      {
        "paperId": "ca5950edb6c3d2b82428d89a0035b173a9e48a26",
        "title": "3-Dimensional Hydrothermal Vent Localization Based on Chemical Plume Tracing"
      },
      {
        "paperId": "1a63a6cd291dd9b10f7ae03a01fec6d3a416cc0b",
        "title": "Function Approximation Technique Based Immersion and Invariance Control for Unknown Nonlinear Systems"
      },
      {
        "paperId": "0a56bb37ab28a39c31fe2572595368398d7a2e75",
        "title": "Approximate Policy-Based Accelerated Deep Reinforcement Learning"
      },
      {
        "paperId": "4cc3ca21e2d7a0ef076cf5556db19b5f26313e9d",
        "title": "Multilayer neural networks-based control of underwater vehicles with uncertain dynamics and disturbances"
      },
      {
        "paperId": "e77f742ec98820aa2d6b073601111c38cffd4dbe",
        "title": "STDPG: A Spatio-Temporal Deterministic Policy Gradient Agent for Dynamic Routing in SDN"
      },
      {
        "paperId": "b63cb9ca048343615a566f656d2ccf97dbb3843b",
        "title": "Fuzzy observer\u2010based tracking control of an underactuated underwater vehicle with linear velocity estimation"
      },
      {
        "paperId": "091296604a7a600eb5057f3b4d1167d35f9d4435",
        "title": "A selected review on reinforcement learning based control for autonomous underwater vehicles"
      },
      {
        "paperId": "48c984e6008dffbbd9a8d5c10c3f601c8f1689f0",
        "title": "End-to-end sensorimotor control problems of AUVs with deep reinforcement learning"
      },
      {
        "paperId": "7bd6b09775dcda18e61f855a0ee05c1358d617b7",
        "title": "Enhanced Reward Function Design for Source Term Estimation Based on Deep Reinforcement Learning"
      },
      {
        "paperId": "888caad9cd70a5c184cd1640243ff9a705227e2a",
        "title": "Chemical plume tracking using an AUV with UKF based extremum seeking"
      }
    ],
    "score": 8.333333333333332
  },
  {
    "id": "fed0701afdfa6896057f7d04bd30ab1328eff110",
    "title": "Evolutionary Diversity Optimization with Clustering-based Selection for Reinforcement Learning",
    "authors": [
      "Yutong Wang",
      "Ke Xue",
      "Chaojun Qian"
    ],
    "year": 2022,
    "citationCount": 25,
    "abstract": "",
    "url": "https://www.semanticscholar.org/paper/fed0701afdfa6896057f7d04bd30ab1328eff110",
    "pdf_url": null,
    "venue": "International Conference on Learning Representations",
    "publicationDate": "",
    "externalIds": {
      "DBLP": "conf/iclr/WangXQ22",
      "CorpusId": 251648121
    },
    "references": [],
    "cited_by": [
      {
        "paperId": "4be4c9497827df6787bc9931113d9374412796b0",
        "title": "An Improved Genetic Algorithm for Causal Discovery"
      },
      {
        "paperId": "3cda2d036ad083338164bb818c9130473c8e61f0",
        "title": "Bayesian Optimization for Quality Diversity Search With Coupled Descriptor Functions"
      },
      {
        "paperId": "a586e8a1295946d3eb5c133f11883ab01d5ecf3d",
        "title": "MUSS: Multilevel Subset Selection for Relevance and Diversity"
      },
      {
        "paperId": "d0e0e12873658be31bb9d6391cc3c473c0a6606d",
        "title": "Hardening Active Directory Graphs via Evolutionary Diversity Optimization based Policies"
      },
      {
        "paperId": "7a0e3b62c34e412e521bca814eadfe5a96cbd1ab",
        "title": "Quality-Diversity with Limited Resources"
      },
      {
        "paperId": "5b886f09c89f8221106b6198cbdbd1e717f59166",
        "title": "REACT: Revealing Evolutionary Action Consequence Trajectories for Interpretable Reinforcement Learning"
      },
      {
        "paperId": "d5c901576caf85141fb3b88f1a2c5178fa87a9a2",
        "title": "Phasic Diversity Optimization for Population-Based Reinforcement Learning"
      },
      {
        "paperId": "43d72582f68133ff7316f4fcbd6eeac41845f57a",
        "title": "Evolutionary Reinforcement Learning: A Systematic Review and Future Directions"
      },
      {
        "paperId": "25523c3e70daedcd0f1172844fcbafe30b8933f1",
        "title": "Bridging Evolutionary Algorithms and Reinforcement Learning: A Comprehensive Survey"
      },
      {
        "paperId": "a519a5477ba9696aefb2aaad5bf547ec422f3763",
        "title": "Quality-Diversity Algorithms Can Provably Be Helpful for Optimization"
      },
      {
        "paperId": "0de72952fa31b595f8dfd6f370dfe26463cf18f1",
        "title": "Diversity from Human Feedback"
      },
      {
        "paperId": "17a7b04f75c9890583cb29d8de23ba6b43ba37d1",
        "title": "Multi-objective Optimization-based Selection for Quality-Diversity by Non-surrounded-dominated Sorting"
      },
      {
        "paperId": "223066f0c1f0d834848bf41f96a2136ef40467fb",
        "title": "Robust multi-agent coordination via evolutionary generation of auxiliary adversarial attackers"
      },
      {
        "paperId": "3e91058c14d6254c235a49b0939210090f9655c6",
        "title": "Communication-robust multi-agent learning by adaptable auxiliary multi-agent adversary generation"
      },
      {
        "paperId": "c8eebfd18624b4579ea472d4a3407a3d05524cbb",
        "title": "Reducing idleness in financial cloud services via multi-objective evolutionary reinforcement learning based load balancer"
      },
      {
        "paperId": "4bb9f478cb464049100abafc77b6491b6e5b731a",
        "title": "Evolutionary Reinforcement Learning: A Survey"
      },
      {
        "paperId": "88eb6cf17a61534462757b9d1826c05da9eaf635",
        "title": "Ensemble Reinforcement Learning: A Survey"
      },
      {
        "paperId": "ef9e1910dafebf2a26a2c800a1d5099abd17ed68",
        "title": "ERL-Re2: Efficient Evolutionary Reinforcement Learning with Shared State Representation and Individual Policy Representation"
      },
      {
        "paperId": "59714a5c3af2778fdf888760a20ca87b5e7f6358",
        "title": "Heterogeneous Multi-agent Zero-Shot Coordination by Coevolution"
      },
      {
        "paperId": "ba42f26f0575653c01451b6cf727d623af2ae312",
        "title": "Towards Theoretically Grounded Evolutionary Learning"
      },
      {
        "paperId": "e5c5b15ff7b523ef7160968331dc5bfe057b62fe",
        "title": "Accelerated Quality-Diversity through Massive Parallelism"
      },
      {
        "paperId": "d9b45079fb7d7b1031837197fe784756b465c011",
        "title": "Sample-Efficient Quality-Diversity by Cooperative Coevolution"
      },
      {
        "paperId": "82e05cfe23295ef88d16bea4aa742da625de2e21",
        "title": "MAP-Elites with Cosine-Similarity for Evolutionary Ensemble Learning"
      },
      {
        "paperId": "98abf6219d648a18292686670668aef7177dc64a",
        "title": "Quality-Similar Diversity via Population Based Reinforcement Learning"
      },
      {
        "paperId": "d944aba6136f0c8548d9189ad7e8723cb146c5ed",
        "title": "Reducing idleness in \ufb01nancial cloud services via multi-objective evolutionary reinforcement learning based load balancer"
      }
    ],
    "score": 8.333333333333332
  },
  {
    "id": "813f6e34feb3dc0346b6392d061af12ff186ba7e",
    "title": "Learning Efficient Multi-Agent Cooperative Visual Exploration",
    "authors": [
      "Chao Yu",
      "Xinyi Yang",
      "Jiaxuan Gao",
      "Huazhong Yang",
      "Yu Wang",
      "Yi Wu"
    ],
    "year": 2021,
    "citationCount": 32,
    "abstract": "We tackle the problem of cooperative visual exploration where multiple agents need to jointly explore unseen regions as fast as possible based on visual signals. Classical planning-based methods often suffer from expensive computation overhead at each step and a limited expressiveness of complex cooperation strategy. By contrast, reinforcement learning (RL) has recently become a popular paradigm for tackling this challenge due to its modeling capability of arbitrarily complex strategies and minimal inference overhead. In this paper, we extend the state-of-the-art single-agent visual navigation method, Active Neural SLAM (ANS), to the multi-agent setting by introducing a novel RL-based planning module, Multi-agent Spatial Planner (MSP).MSP leverages a transformer-based architecture, Spatial-TeamFormer, which effectively captures spatial relations and intra-agent interactions via hierarchical spatial self-attentions. In addition, we also implement a few multi-agent enhancements to process local information from each agent for an aligned spatial representation and more precise planning. Finally, we perform policy distillation to extract a meta policy to significantly improve the generalization capability of final policy. We call this overall solution, Multi-Agent Active Neural SLAM (MAANS). MAANS substantially outperforms classical planning-based baselines for the first time in a photo-realistic 3D simulator, Habitat. Code and videos can be found at https://sites.google.com/view/maans.",
    "url": "https://www.semanticscholar.org/paper/813f6e34feb3dc0346b6392d061af12ff186ba7e",
    "pdf_url": "https://arxiv.org/pdf/2110.05734.pdf",
    "venue": "European Conference on Computer Vision",
    "publicationDate": "2021-10-12",
    "externalIds": {
      "DBLP": "journals/corr/abs-2110-05734",
      "ArXiv": "2110.05734",
      "DOI": "10.1007/978-3-031-19842-7_29",
      "CorpusId": 238634378
    },
    "references": [
      {
        "paperId": "aea5f8adff2fa7a0ef4c77822398243f3632907f",
        "title": "Multi-Agent Embodied Visual Semantic Navigation With Scene Prior Knowledge"
      },
      {
        "paperId": "feb7f993c402e2663d20bbafa83c11e6db3dfe6b",
        "title": "Cooperative Exploration for Multi-Agent Deep Reinforcement Learning"
      },
      {
        "paperId": "9c4f019756d54397000f1d377e0104e02c6394fa",
        "title": "Collaborative Visual Navigation"
      },
      {
        "paperId": "2195a25a8741c4c8fa229131744032666f3028d1",
        "title": "SMMR-Explore: SubMap-based Multi-Robot Exploration System with Multi-robot Multi-target Potential Field Exploration Method"
      },
      {
        "paperId": "ecd6a50c863b45534570c4e78b37b32af716fe24",
        "title": "Multi-Agent Ergodic Coverage in Urban Environments"
      },
      {
        "paperId": "3a315c81a98851f0614c09fef6a14c30d6a1e63c",
        "title": "The Surprising Effectiveness of PPO in Cooperative Multi-Agent Games"
      },
      {
        "paperId": "f8d8e192979ab5d8d593e76a0c4e2c7581778732",
        "title": "Voronoi-Based Multi-Robot Autonomous Exploration in Unknown Environments via Deep Reinforcement Learning"
      },
      {
        "paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a",
        "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
      },
      {
        "paperId": "941e2cb3a575f6a3c7855a21a30359be6d7a6f8a",
        "title": "Heterogeneous Multi-Agent Reinforcement Learning for Unknown Environment Mapping"
      },
      {
        "paperId": "54a86d25e0a98c5b719e4401b1aa66f2c84593f4",
        "title": "Exploration and Mapping with Groups of Robots: Recent Trends"
      },
      {
        "paperId": "7a9efbe9127eed39284e2d43af405d06c59db92f",
        "title": "Occupancy Anticipation for Efficient Exploration and Navigation"
      },
      {
        "paperId": "62516303058a1322450b58e4cd778ab873b5e531",
        "title": "Object Goal Navigation using Goal-Oriented Semantic Exploration"
      },
      {
        "paperId": "6f3d3c8b53a400a233b0332520b7af5ea5f4b321",
        "title": "Parameter Sharing is Surprisingly Useful for Multi-Agent Deep Reinforcement Learning."
      },
      {
        "paperId": "5c378ca2e4699eaf763de9f8ec02ca89860bb1cf",
        "title": "Neural Topological SLAM for Visual Navigation"
      },
      {
        "paperId": "6c5f199f7e2cc1fd93240a21719498a3f540dcbe",
        "title": "Learning to Explore using Active Neural SLAM"
      },
      {
        "paperId": "707eb919d3daa087e63d48930c8630b06c43d24f",
        "title": "Evolutionary Population Curriculum for Scaling Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "7695d00e9dd469f24fdad994daf96dcb2b973d7d",
        "title": "Shapeshifter: A Multi-Agent, Multi-Modal Robotic Platform for Exploration of Titan"
      },
      {
        "paperId": "14c88059503461afdab6d59c3e56914b6690eb02",
        "title": "An Exploration of Embodied Visual Exploration"
      },
      {
        "paperId": "46106fcd08223540d080f674b26770c1fa8a52ff",
        "title": "Influence-Based Multi-Agent Exploration"
      },
      {
        "paperId": "4f6a187dea2d9aad945d76332aa73e7060a28702",
        "title": "Multi-Agent Actor-Critic with Hierarchical Graph Attention Network"
      },
      {
        "paperId": "ac0a9ced9c704649326fcb68cba2f904e2c6fb1d",
        "title": "Bayesian Relational Memory for Semantic Visual Navigation"
      },
      {
        "paperId": "3b88d9522afe6ce2badd7de6d493da9bb2a46713",
        "title": "From Few to More: Large-scale Dynamic Multiagent Curriculum Learning"
      },
      {
        "paperId": "e3d662bbd0e5539fe22a85f3518f960595b9914e",
        "title": "Heterogeneous Graph Neural Network"
      },
      {
        "paperId": "c91907248cd832ba4f766394cd86a27946047a9e",
        "title": "Deep Set Prediction Networks"
      },
      {
        "paperId": "c01dd6bc93ed01124079938cae19aba040273fc6",
        "title": "Coordinated Exploration via Intrinsic Rewards for Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "7cec763510b951fcf8bb93b508e7a49ec5162875",
        "title": "Two Body Problem: Collaborative Visual Task Completion"
      },
      {
        "paperId": "b4a35e548de27b6924e5f2ee41d37238a5c4a1d5",
        "title": "Habitat: A Platform for Embodied AI Research"
      },
      {
        "paperId": "505e3315b7c5d4094c3160d00956a7d1596c0956",
        "title": "Scene Memory Transformer for Embodied Agents in Long-Horizon Tasks"
      },
      {
        "paperId": "48182d7620a9278d7e9cd880a961fa14d22a0281",
        "title": "Learning Exploration Policies for Navigation"
      },
      {
        "paperId": "86328e0c84bf97ae9b0680c06c4886bc04fb63c1",
        "title": "A review of mobile robots: Concepts, methods, theoretical framework, and applications"
      },
      {
        "paperId": "28e66d188efbd0bbb64242b611d96769be910c15",
        "title": "Deep Reinforcement Learning for Multiagent Systems: A Review of Challenges, Solutions, and Applications"
      },
      {
        "paperId": "3a90c02e95fde1a68f83f87b19712d4d4321a246",
        "title": "Deep Multi-Agent Reinforcement Learning with Relevance Graphs"
      },
      {
        "paperId": "1796f6f928079f935748dad59f6ed6fdbba960c8",
        "title": "Graph Convolutional Reinforcement Learning"
      },
      {
        "paperId": "fdfeeb14bbde2ab31b18e56b92d362dcd1b14f71",
        "title": "Episodic Curiosity through Reachability"
      },
      {
        "paperId": "c7aea4b653d4e12cb47438960f5689f5f835e073",
        "title": "Visual Semantic Navigation using Scene Priors"
      },
      {
        "paperId": "929bef0066bad871ba971b673c053112d055d29f",
        "title": "Actor-Attention-Critic for Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "cf4a1eadf2e1a8e9144446790e563c8a3097f9c3",
        "title": "CM3: Cooperative Multi-goal Multi-stage Multi-agent Reinforcement Learning"
      },
      {
        "paperId": "65769b53e71ea7c52b3a07ad32bd4fdade6a0173",
        "title": "Multi-task Deep Reinforcement Learning with PopArt"
      },
      {
        "paperId": "c4e824a574d396803cf4677b7d0ad4e28ad54804",
        "title": "Value-Decomposition Networks For Cooperative Multi-Agent Learning Based On Team Reward"
      },
      {
        "paperId": "5e49c80f8b12a100c5f4518897c4cbf72710c252",
        "title": "Relational Deep Reinforcement Learning"
      },
      {
        "paperId": "795783cf1b7f267fa59cb6901a238762fe3ee608",
        "title": "MapNet: An Allocentric Spatial Memory for Mapping Environments"
      },
      {
        "paperId": "e89a4fe6e8286eccedd702216153f0f248adb151",
        "title": "Gibson Env: Real-World Perception for Embodied Agents"
      },
      {
        "paperId": "72c85ffe4491abd6f9e537d7dadf8f23c314849e",
        "title": "Learning Attentional Communication for Multi-Agent Cooperation"
      },
      {
        "paperId": "005bcdb7e3f893e2a6e1e27660595e0e7f3d3eb1",
        "title": "Visual Representations for Semantic Target Driven Navigation"
      },
      {
        "paperId": "ffc211476f2e40e79466ffc198c919a97da3bb76",
        "title": "QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "c8efcc854d97dfc2a42b83316a2109f9d166e43f",
        "title": "Self-Attention with Relative Position Representations"
      },
      {
        "paperId": "c0d96d1ea69855a5a3abf614f17095c29b3339a4",
        "title": "Semi-parametric Topological Memory for Navigation"
      },
      {
        "paperId": "8899094797e82c5c185a0893896320ef77f60e64",
        "title": "Non-local Neural Networks"
      },
      {
        "paperId": "b36a5bb1707bb9c70025294b3a310138aae8327a",
        "title": "Automatic differentiation in PyTorch"
      },
      {
        "paperId": "59455ef07a99d8c1b4110c2153e9c26d0faae437",
        "title": "Parameter Sharing Deep Deterministic Policy Gradient for Cooperative Multi-agent Reinforcement Learning"
      },
      {
        "paperId": "db31ea6597cb76b4294767982709e5e17a6744e2",
        "title": "Simultaneous Localization and Mapping: A Survey of Current Trends in Autonomous Driving"
      },
      {
        "paperId": "4b952acda89f478c481735e36d0e10362b52fea7",
        "title": "Autonomous robotic exploration based on multiple rapidly-exploring randomized trees"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "cf90552b5d2e992e93ab838fd615e1c36618e31c",
        "title": "Distral: Robust multitask reinforcement learning"
      },
      {
        "paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
        "title": "Attention is All you Need"
      },
      {
        "paperId": "94e10392b982b9ea8dad258cd331c6b145a7ef4d",
        "title": "Multiagent Bidirectionally-Coordinated Nets for Learning to Play StarCraft Combat Games"
      },
      {
        "paperId": "5c57bb5630835a05eb1c3d0df3e12d6180d75de2",
        "title": "One-Shot Imitation Learning"
      },
      {
        "paperId": "3ee01ec27e4e66e089b72a9989724be611c2ad90",
        "title": "Neural Map: Structured Memory for Deep Reinforcement Learning"
      },
      {
        "paperId": "16e1f2669b9a9ecb71ea4e0ed581e2c24cb55f79",
        "title": "Playing Doom with SLAM-Augmented Deep Reinforcement Learning"
      },
      {
        "paperId": "d35b05f440b5ba00d9429139edef7182bf9f7ce7",
        "title": "Learning to Navigate in Complex Environments"
      },
      {
        "paperId": "2f28ab0f448d995cb53910e634459d00361301a4",
        "title": "From monocular SLAM to autonomous drone exploration"
      },
      {
        "paperId": "7af7f2f539cd3479faae4c66bbef49b0f66202fa",
        "title": "Target-driven visual navigation in indoor scenes using deep reinforcement learning"
      },
      {
        "paperId": "50295c19e177480ba3599300de1ab837cc62b08c",
        "title": "Learning Multiagent Communication with Backpropagation"
      },
      {
        "paperId": "0772905d40b9afa3dc087a88184f09f3b3e1464f",
        "title": "Learning to Communicate with Deep Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "ac61e341b80cc3052b37a51bfc339416e48e562a",
        "title": "An information gain formulation for active volumetric 3D reconstruction"
      },
      {
        "paperId": "024006d4c2a89f7acacc6e4438d156525b60a98f",
        "title": "Continuous control with deep reinforcement learning"
      },
      {
        "paperId": "2319a491378867c7049b3da055c5df60e1671158",
        "title": "Playing Atari with Deep Reinforcement Learning"
      },
      {
        "paperId": "f4ca8a90da89d4064d6e49787c9345ed47fa0588",
        "title": "Multi-agent RRT: sampling-based cooperative pathfinding"
      },
      {
        "paperId": "18836d463407274ad8a0a8b0c9f6a15d506b61a6",
        "title": "Visual simultaneous localization and mapping: a survey"
      },
      {
        "paperId": "426a3b8fb728cfc9281a2a54011e84137edcbd79",
        "title": "A comparison of path planning strategies for autonomous exploration and mapping of unknown environments"
      },
      {
        "paperId": "abe6ea031ec1d1caed75bf3978731e9c0ad94d9a",
        "title": "Decentralized path planning for multi-agent teams in complex environments using rapidly-exploring random trees"
      },
      {
        "paperId": "79ab3c49903ec8cb339437ccf5cf998607fc313e",
        "title": "A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning"
      },
      {
        "paperId": "7216ed68677549dca19164b142d8570c19c2d5df",
        "title": "Evaluating the Efficiency of Frontier-based Exploration Strategies"
      },
      {
        "paperId": "d4f81b7a3a35ead6e9598cb3c5f4eaefee1815aa",
        "title": "Google Street View: Capturing the World at Street Level"
      },
      {
        "paperId": "2f755a9de4c9ca23a7e412e327a4e42c807f16a6",
        "title": "Coordinated multi-robot exploration using a segmentation of the environment"
      },
      {
        "paperId": "adf40e0bc0d6377084a1b7f1e63c4f9f0dfef19e",
        "title": "RFID Technology-based Exploration and SLAM for Search And Rescue"
      },
      {
        "paperId": "ce81a0b905dcfc206d5122dd93785be3d1f40bc7",
        "title": "Coordinated multi-robot exploration"
      },
      {
        "paperId": "a1875055e9c526cbdc7abb161959d76d14b58266",
        "title": "A frontier-based approach for autonomous exploration"
      },
      {
        "paperId": "8cb74fe4f598699c9c24d88acd4906e2489267af",
        "title": "Adaptive mapping and navigation by teams of simple robots"
      },
      {
        "paperId": "7e81dac6260c4768af3a28ac21c78c5a38a5f7d0",
        "title": "Probabilistic roadmaps for path planning in high-dimensional configuration spaces"
      },
      {
        "paperId": "e5a9fb3d49ef54f049509231e5883a14bef070f0",
        "title": "A fast marching level set method for monotonically advancing fronts."
      },
      {
        "paperId": null,
        "title": "Main: A multi-agent indoor navigation benchmark for cooperative learning"
      },
      {
        "paperId": null,
        "title": "Distilling policy distillation. CoRR, abs/1902.02186"
      },
      {
        "paperId": null,
        "title": "Neural slam: Learning to explore with external memory"
      },
      {
        "paperId": "9819b600a828a57e1cde047bbe710d3446b30da5",
        "title": "Recurrent neural network based language model"
      },
      {
        "paperId": "2ea0dae952ecb0e3eba3f0f7509fc209a5d7cfbd",
        "title": "Multi-Agent Area Coverage Using a Single Query Roadmap: A Swarm Intelligence Approach"
      },
      {
        "paperId": "3f8d7bdfc3ed0ff793f1236730486b3d5cf946aa",
        "title": "Probabilistic robotics"
      },
      {
        "paperId": "04bf89a4b8c8894d878c740794cfdef6b9508723",
        "title": "Rapidly-Exploring Random Trees : Progress and Prospe tsSteven"
      },
      {
        "paperId": "2646cb5dba98b50862d2a07354a5a41cf76f09f7",
        "title": "http://urn.kb.se/resolve?urn=urn:nbn:se:liu:diva-72517 A Frontier-Void-Based Approach for Autonomous Exploration in 3D"
      },
      {
        "paperId": null,
        "title": "Accepted by ECCV"
      },
      {
        "paperId": null,
        "title": "Overlap Ratio Figure 10: Ablation Studies on SCP vs. SCP w.o. RE. on 2 representative training maps"
      },
      {
        "paperId": null,
        "title": "We propose the \ufb01rst multi-agent cooperative exploration framework, Multi-Agent Active Neural SLAM (MAANS) that outperforms planning-based competitors in a photo-realist physical environment"
      }
    ],
    "cited_by": [
      {
        "paperId": "eb7db9262a189fa8bb1da878736b030d2a564932",
        "title": "Deep Reinforcement Learning of Mobile Robot Navigation in Dynamic Environment: A Review"
      },
      {
        "paperId": "69f2a7572e8b2a5895ee37c341b50feac147e329",
        "title": "SayCoNav: Utilizing Large Language Models for Adaptive Collaboration in Decentralized Multi-Robot Navigation"
      },
      {
        "paperId": "9b38f0c9bb53005de9cb0e4ab618847d95257a8f",
        "title": "Multi-Robot System for Cooperative Exploration in Unknown Environments: A Survey"
      },
      {
        "paperId": "cd510e5074975446aac6b9548b68a7cbfbf97ef2",
        "title": "Cooperative Motion Planning in Divided Environments via Congestion-Aware Deep Reinforcement Learning"
      },
      {
        "paperId": "74128804d80f6b543775ea39035f6e06ce2b5469",
        "title": "MARVEL: Multi-Agent Reinforcement Learning for Constrained Field-of-View Multi-Robot Exploration in Large-Scale Environments"
      },
      {
        "paperId": "d95c602a2e493865f2686a1da09dee346ae2952c",
        "title": "RAMEN: Real-time Asynchronous Multi-agent Neural Implicit Mapping"
      },
      {
        "paperId": "8c0521295ead6eaf6c84326025b141d9dd83a1cc",
        "title": "Enhancing Multi-Robot Semantic Navigation Through Multimodal Chain-of-Thought Score Collaboration"
      },
      {
        "paperId": "00ef2f9a0c2564c5c7e915d02245014a3b015b87",
        "title": "Distributed NeRF Learning for Collaborative Multi-Robot Perception"
      },
      {
        "paperId": "174b829cab174d0c87c40306d6e00dd0883f6bc1",
        "title": "Human-Robot Cooperative Distribution Coupling for Hamiltonian-Constrained Social Navigation"
      },
      {
        "paperId": "c6045936879d7e31d36ca715a175d6547801d133",
        "title": "Leveraging Large Language Model for Heterogeneous Ad Hoc Teamwork Collaboration"
      },
      {
        "paperId": "ab08f2e7cb41aa0a290454cc93eaf47a665c668e",
        "title": "MAM<mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" altimg=\"si23.svg\" display=\"inline\" id=\"d1e1825\"><mml:msup><mml:mrow /><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:math>SLAM: Towards underwater-robust multi-agent visual SLAM"
      },
      {
        "paperId": "8ffaed7ab7f0d234b1573524592701a975a23cb2",
        "title": "Multi-Robot Environmental Coverage With a Two-Stage Coordination Strategy via Deep Reinforcement Learning"
      },
      {
        "paperId": "6257fcfd5cf2eacbae1bb8b97de57160ae864a12",
        "title": "Asymmetric Information Enhanced Mapping Framework for Multirobot Exploration based on Deep Reinforcement Learning"
      },
      {
        "paperId": "e9e203948399c2c36b6a46a86a07e3ca7caa3bd8",
        "title": "Visual Hindsight Self-Imitation Learning for Interactive Navigation"
      },
      {
        "paperId": "240c0a790c304e80617e969446df51bcefa688bf",
        "title": "MASP: Scalable GNN-based Planning for Multi-Agent Navigation"
      },
      {
        "paperId": "7450c20d844ca05e643ece2461ff7aa2f381e22a",
        "title": "Towards Top-Down Reasoning: An Explainable Multi-Agent Approach for Visual Question Answering"
      },
      {
        "paperId": "5d3bf101809b7782f6b89649f8f14c60a84be0c7",
        "title": "Active Neural Topological Mapping for Multi-Agent Exploration"
      },
      {
        "paperId": "53daed7461dde3e321ca7fa59860810905e657b5",
        "title": "Co-NavGPT: Multi-Robot Cooperative Visual Semantic Navigation Using Vision Language Models"
      },
      {
        "paperId": "9468b6b296629e1226459d73b11717345f9ea938",
        "title": "Collaborative Visual Place Recognition"
      },
      {
        "paperId": "e9f6b92c13a9bf809e9498081ef7aec912ce5a9b",
        "title": "An Expert Data Generation Method for Multi-Agent Cooperative Planning Method"
      },
      {
        "paperId": "ccfea7f110e3597b20e38b841daa0b6354ab2d3f",
        "title": "Diverse Policies Converge in Reward-free Markov Decision Processe"
      },
      {
        "paperId": "4dc0aa59c3824435bce46ddf714652beb5cdada8",
        "title": "Heterogeneous Embodied Multi-Agent Collaboration"
      },
      {
        "paperId": "99b0c3a18050889c591e1db6d51ca01298638437",
        "title": "Transformers in Reinforcement Learning: A Survey"
      },
      {
        "paperId": "69da716ca399dbac414ee146cac1ab8213d71c35",
        "title": "Multiagent Reinforcement Learning: Methods, Trustworthiness, Applications in Intelligent Vehicles, and Challenges"
      },
      {
        "paperId": "c368238f147a547a57f6e603ac7d6aadbb8ab1b2",
        "title": "Learning Graph-Enhanced Commander-Executor for Multi-Agent Navigation"
      },
      {
        "paperId": "2f3c3638a065d22525781bc9365aaaca07353e55",
        "title": "Asynchronous Multi-Agent Reinforcement Learning for Efficient Real-Time Multi-Robot Cooperative Exploration"
      },
      {
        "paperId": "af170b7e21b0f7b10da6b52710f33dac5ddb435a",
        "title": "Multi-Agent Exploration of an Unknown Sparse Landmark Complex via Deep Reinforcement Learning"
      },
      {
        "paperId": "09093b425cd2315a874cfd57053897b1a1065a6b",
        "title": "DGPO: Discovering Multiple Strategies with Diversity-Guided Policy Optimization"
      },
      {
        "paperId": "a2b2ff265ebe112626892703a233203348b3936e",
        "title": "Embodied Multi-Agent Task Planning from Ambiguous Instruction"
      },
      {
        "paperId": "cd743889c9c9c0c202ee07adcb946b765db4cf19",
        "title": "Multi-Robot Active Mapping via Neural Bipartite Graph Matching"
      },
      {
        "paperId": "16ecaa7cf142605331fc21c9be73c7b13e8c1acd",
        "title": "Co-NavGPT: Multi-Robot Cooperative Visual Semantic Navigation using Large Language Models"
      },
      {
        "paperId": "c6294f307d818dab1d35f2096087bc12bc4fef28",
        "title": "MASP: Scalable GNN-based Planning for Multi-Agent Navigation"
      }
    ],
    "score": 8.0
  },
  {
    "id": "21828b3f05acffbfdf7de7259ad8b8ebd57fa351",
    "title": "Data-efficient deep reinforcement learning with expert demonstration for active flow control",
    "authors": [
      "Changdong Zheng",
      "Fangfang Xie",
      "Tingwei Ji",
      "Xinshuai Zhang",
      "Yufeng Lu",
      "Hongjie Zhou",
      "Yao Zheng"
    ],
    "year": 2022,
    "citationCount": 24,
    "abstract": "Deep reinforcement learning (RL) is capable of identifying and modifying strategies for active flow control. However, the classic active formulation of deep RL requires lengthy active exploration. This paper describes the introduction of expert demonstration into a classic off-policy RL algorithm, the soft actor-critic algorithm, for application to vortex-induced vibration problems. This combined online-learning framework is applied to an oscillator wake environment and a Navier--Stokes environment, with expert demonstration obtained from the pole-placement method and surrogate model optimization. The results show that the soft actor--critic framework combined with expert demonstration enables rapid learning of active flow control strategies through a combination of prior demonstration data and online experience. The present study develops a new data-efficient RL approach for discovering active flow control strategies for vortex-induced vibration, providing a more practical methodology for industrial applications.",
    "url": "https://www.semanticscholar.org/paper/21828b3f05acffbfdf7de7259ad8b8ebd57fa351",
    "pdf_url": "https://doi.org/10.1063/5.0120285",
    "venue": "The Physics of Fluids",
    "publicationDate": "2022-10-08",
    "externalIds": {
      "DOI": "10.1063/5.0120285",
      "CorpusId": 252770641
    },
    "references": [],
    "cited_by": [
      {
        "paperId": "5103e50590a11bb57812dae0d2d6017b803d6c4e",
        "title": "Strategy tuning of synthetic jet control over supercritical airfoil based on deep reinforcement learning"
      },
      {
        "paperId": "693f0af66897b02b18fd1746bcf4d1010fd8042c",
        "title": "From black box to physically interpretable: Trustworthy computing for AI-driven decision-making and control"
      },
      {
        "paperId": "c7cf2762bb23f4c0369a919fcc9aec9563defc4b",
        "title": "Jet control of tandem cylinders with variable radius using deep reinforcement learning"
      },
      {
        "paperId": "f1ca5e3f4b4b87d96b0101aa426499a2c87f71b0",
        "title": "Deep reinforcement learning for active flow control in bluff bodies: A state-of-the-art review"
      },
      {
        "paperId": "cb73a054d921c02c1fed1aafbad65d4a79407d29",
        "title": "Study on Liutex-driven reward for intelligent flow control by dynamic feature-based deep reinforcement learning"
      },
      {
        "paperId": "2d56367d370f43b066a2e7772c87091db9107f6c",
        "title": "Closed-loop control for suppressing the vortex-induced vibration of a pipeline by the eigensystem realization algorithm"
      },
      {
        "paperId": "b940c9ca970a0c78d437d4e86d57cf4ab8afdadf",
        "title": "Comparative study of reinforcement learning and reduced-order model-based control for mitigating vortex-induced vibration"
      },
      {
        "paperId": "18c8da7705fda0ea1e919c5eb937fcff4dc960b7",
        "title": "Approach for Task Transfer and Demonstration Learning Optimization in Industrial Robotic Manipulators"
      },
      {
        "paperId": "648e358410c8090d7069f00b60fb38e4b8f1383c",
        "title": "Transformer-based in-context policy learning for efficient active flow control across various airfoils"
      },
      {
        "paperId": "e24a70e8104863d60084c6b8b4ba85e6cabeec62",
        "title": "Model-Free Closed-Loop Control of Flow Past a Bluff Body: Methods, Applications, and Emerging Trends"
      },
      {
        "paperId": "3ada91fa94649890ed847831ce7446f4293b4d09",
        "title": "Mitigating the lift of a circular cylinder in wake flow using deep reinforcement learning guided self-rotation"
      },
      {
        "paperId": "c654e80de331bbf3751d7ddabb9b6c4e876b4f46",
        "title": "Balanced proper-orthogonal-decomposition-based feedback control of vortex-induced vibration"
      },
      {
        "paperId": "2d2e8329afb6052cafb4a85579ae4092ee325673",
        "title": "Machine Learning for Bridge Wind Engineering"
      },
      {
        "paperId": "7cceb8b89d8651e8f91423d3d00c75c94607ded1",
        "title": "Leveraging deep reinforcement learning for design space exploration with multi-fidelity surrogate model"
      },
      {
        "paperId": "52ccf9350a0807485f7543b06f258a6cc2bf9076",
        "title": "Continuous control of structural vibrations using hybrid deep reinforcement learning policy"
      },
      {
        "paperId": "6cced8d7345edfa2b61003fb866575366d66eb75",
        "title": "An Autoencoder\u2010Based Deep\u2010Learning Method for Augmenting the Sensing Capability of Piezoelectric Microelectromechanical System Sensors in a Fluid\u2010Dynamic System"
      },
      {
        "paperId": "906ecbc5a44416d68c14b5479fa31059c9fc2eb4",
        "title": "Reinforcement learning-based active flow control of oscillating cylinder for drag reduction"
      },
      {
        "paperId": "8c5e3efff61cdc1fddfdb0eb423fcdc97e68c2ac",
        "title": "Surrogate model-based deep reinforcement learning for experimental study of active flow control of circular cylinder"
      },
      {
        "paperId": "4873875dff1437dd443c5c7f78083bb402916018",
        "title": "DRLFluent: A distributed co-simulation framework coupling deep reinforcement learning with Ansys-Fluent on high-performance computing systems"
      },
      {
        "paperId": "c16aab5622e5d59d2e220e5f5351c41047a9aeeb",
        "title": "Deep reinforcement learning for propulsive performance of a flapping foil"
      },
      {
        "paperId": "b158d6c52c7cb5042734078cea83a04942eecec1",
        "title": "Applying reinforcement learning to mitigate wake-induced lift fluctuation of a wall-confined circular cylinder in tandem configuration"
      },
      {
        "paperId": "ce91e1e137459316b02e9bb3677b181ea66aa2d6",
        "title": "How to Control Hydrodynamic Force on Fluidic Pinball via Deep Reinforcement Learning"
      },
      {
        "paperId": "81a8b8ec36fc553999068e1db322754e2e84acd7",
        "title": "Deep Reinforcement Learning: A New Beacon for Intelligent Active Flow Control"
      },
      {
        "paperId": "3c66bdfcddbcde02854582a8f97ca8d302924ad8",
        "title": "Vid2Act: Activate Offline Videos for Visual RL"
      }
    ],
    "score": 8.0
  },
  {
    "id": "714b7c61d81687d093fd8b7d6d6737d582a4c9b7",
    "title": "Towards Applicable Reinforcement Learning: Improving the Generalization and Sample Efficiency with Policy Ensemble",
    "authors": [
      "Zhengyu Yang",
      "Kan Ren",
      "Xufang Luo",
      "Minghuan Liu",
      "Weiqing Liu",
      "J. Bian",
      "Weinan Zhang",
      "Dongsheng Li"
    ],
    "year": 2022,
    "citationCount": 23,
    "abstract": "It is challenging for reinforcement learning (RL) algorithms to succeed in real-world applications. Take financial trading as an example, the market information is noisy yet imperfect and the macroeconomic regulation or other factors may shift between training and evaluation, thus it requires both generalization and high sample efficiency for resolving the task. However, directly applying typical RL algorithms can lead to poor performance in such scenarios. To derive a robust and applicable RL algorithm, in this work, we design a simple but effective method named Ensemble Proximal Policy Optimization (EPPO), which learns ensemble policies in an end-to-end manner. Notably, EPPO combines each policy and the policy ensemble organically and optimizes both simultaneously. In addition, EPPO adopts a diversity enhancement regularization over the policy space which helps to generalize to unseen states and promotes exploration. We theoretically prove that EPPO can increase exploration efficacy, and through comprehensive experimental evaluations on various tasks, we demonstrate that EPPO achieves higher efficiency and is robust for real-world applications compared with vanilla policy optimization algorithms and other ensemble methods. Code and supplemental materials are available at https://seqml.github.io/eppo.",
    "url": "https://www.semanticscholar.org/paper/714b7c61d81687d093fd8b7d6d6737d582a4c9b7",
    "pdf_url": "https://arxiv.org/pdf/2205.09284.pdf",
    "venue": "International Joint Conference on Artificial Intelligence",
    "publicationDate": "2022-05-19",
    "externalIds": {
      "DBLP": "journals/corr/abs-2205-09284",
      "ArXiv": "2205.09284",
      "DOI": "10.48550/arXiv.2205.09284",
      "CorpusId": 248887230
    },
    "references": [
      {
        "paperId": "e06afabc93bb10873d65d983c6da326f88f8c2fc",
        "title": "PerfectDou: Dominating DouDizhu with Perfect Information Distillation"
      },
      {
        "paperId": "2c23085488337c4c1b5673b8d0f4ac95bda73529",
        "title": "Uncertainty Weighted Actor-Critic for Offline Reinforcement Learning"
      },
      {
        "paperId": "e7c33544f157974083e9b106605f417722777352",
        "title": "Probabilistic Mixture-of-Experts for Efficient Deep Reinforcement Learning"
      },
      {
        "paperId": "daa92545b0363e1d8ea83e74b47d7f7e9790fa80",
        "title": "Universal Trading for Order Execution with Oracle Policy Distillation"
      },
      {
        "paperId": "324effa5a7c737257b675f85bd93d777fc486878",
        "title": "SUNRISE: A Simple Unified Framework for Ensemble Learning in Deep Reinforcement Learning"
      },
      {
        "paperId": "b30c663690c3a096c7d92f307ba7d17bdfd48553",
        "title": "Suphx: Mastering Mahjong with Deep Reinforcement Learning"
      },
      {
        "paperId": "0c62a64c6e758d4cd2d968ca39d41f0f6579c14f",
        "title": "Effective Diversity in Population-Based Reinforcement Learning"
      },
      {
        "paperId": "292bbd1287a0674cd9e3e79224e768ca557dcf81",
        "title": "Mastering Complex Control in MOBA Games with Deep Reinforcement Learning"
      },
      {
        "paperId": "e8fd8928472a82c7e73218f2ec2ae97409b80415",
        "title": "SEERL: Sample Efficient Ensemble Reinforcement Learning"
      },
      {
        "paperId": "103cb5d78e89e63f0fbb8d9a5b2afbfc43cedd64",
        "title": "Reinforcement Learning with Competitive Ensembles of Information-Constrained Primitives"
      },
      {
        "paperId": "0afedce86320fbb798b161e88584c93caaf6d5a8",
        "title": "Diversity-Inducing Policy Gradient: Using Maximum Mean Discrepancy to Find a Set of Diverse Policies"
      },
      {
        "paperId": "56136aa0b2c347cbcf3d50821f310c4253155026",
        "title": "Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models"
      },
      {
        "paperId": "27dfecb6bb0308c7484e13dcaefd5eeebba677d3",
        "title": "Model-Ensemble Trust-Region Policy Optimization"
      },
      {
        "paperId": "3c3093f5f8e9601637612fcfb8f160f116fa30e4",
        "title": "Diversity-Driven Exploration Strategy for Deep Reinforcement Learning"
      },
      {
        "paperId": "904307cb58795241b22cfaa34f560e610997f5c1",
        "title": "Divide-and-Conquer Reinforcement Learning"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "6ecb8a743f92db6c6b8691ab8e8aebbb06fb1b48",
        "title": "Averaged-DQN: Variance Reduction and Stabilization for Deep Reinforcement Learning"
      },
      {
        "paperId": "d316c82c12cf4c45f9e85211ef3d1fa62497bff8",
        "title": "High-Dimensional Continuous Control Using Generalized Advantage Estimation"
      },
      {
        "paperId": "3fb7fc198443ca76c3d0d4ab04e7825b00818b1f",
        "title": "Neural Network Ensembles in Reinforcement Learning"
      },
      {
        "paperId": "0b3c2a32ceee0359a199298e2e7c80bc7cebbc31",
        "title": "Diversity Regularized Ensemble Pruning"
      },
      {
        "paperId": "f82e4ff4f003581330338aaae71f60316e58dd26",
        "title": "The Arcade Learning Environment: An Evaluation Platform for General Agents"
      },
      {
        "paperId": "bc6dff14a130c57a91d5a21339c23471faf1d46f",
        "title": "Et al"
      },
      {
        "paperId": "6d23073dbb68d353f30bb97f4803cfbd66546444",
        "title": "Ensemble Algorithms in Reinforcement Learning"
      },
      {
        "paperId": "be4fd29e3225d8451b02683f1a32baef0483266a",
        "title": "Ensembling neural networks: Many could be better than all"
      },
      {
        "paperId": "c98835b9a6e1fd475b6975b27ae6333b91980912",
        "title": "Neural processing letters"
      },
      {
        "paperId": "c8d90974c3f3b40fa05e322df2905fc16204aa56",
        "title": "Adaptive Mixtures of Local Experts"
      },
      {
        "paperId": "c8e607329a4e59dad1b64ea75d3cd8e48be04cae",
        "title": "The Diversified Ensemble Neural Network"
      },
      {
        "paperId": null,
        "title": "NeurIPS"
      },
      {
        "paperId": null,
        "title": "Minimalistic gridworld environment for openai gym"
      },
      {
        "paperId": "ebbac18c556ba9ac2b68854a0124ff1c93055023",
        "title": "Diverse Ensemble Evolution: Curriculum Data-Model Marriage"
      },
      {
        "paperId": "f617db6887b25c5979775909c27d14dc09716377",
        "title": "Ensembles for Continuous Actions in Reinforcement Learning"
      },
      {
        "paperId": null,
        "title": "In ESANN"
      },
      {
        "paperId": null,
        "title": "pages 330\u2013345"
      },
      {
        "paperId": null,
        "title": "Part B (Cybernetics)"
      },
      {
        "paperId": "e1082fee59f00b1096b2206f3dc434e68f7ffac6",
        "title": "Neural Computation"
      }
    ],
    "cited_by": [
      {
        "paperId": "5fb99fd13a583f56ea37d283845fb49573bbb4d3",
        "title": "Scaling DRL for Decision Making: A Survey on Data, Network, and Training Budget Strategies"
      },
      {
        "paperId": "3d74b73bbaafd89d8b79141e1b97f2f6d316a580",
        "title": "Inductive biased-deep reinforcement learning methods for flow control: Group-invariant and positional-encoding networks improve learning reproducibility and quality"
      },
      {
        "paperId": "1452775b071565b0d74df8753578ca795f474191",
        "title": "Adaptive Missile Avoidance Algorithm for UAV Based on Multi-Head Attention Mechanism and Dual Population Confrontation Game"
      },
      {
        "paperId": "c403842a80537df1908933f0f45eb021bdfd93e4",
        "title": "Multi-CALF: A Policy Combination Approach with Statistical Guarantees"
      },
      {
        "paperId": "4fd8a6a8924ef8cb4becd74d5db27c2a15a482f5",
        "title": "A Memory Consolidation Model Based on Neuromodulation Mechanisms in the Human Brain"
      },
      {
        "paperId": "bd45a1e4a04fc44de17dd90f740cc01770ec8b95",
        "title": "A Comprehensive Survey of Mixture-of-Experts: Algorithms, Theory, and Applications"
      },
      {
        "paperId": "ecba1123b4314f23207da6da9b347738e16542b8",
        "title": "Optimizing capacitive deionization operation using dynamic modeling and reinforcement learning"
      },
      {
        "paperId": "76e6a9d7e4481f76b4404ab9e3b2d5af71f9a89d",
        "title": "Swarm Behavior Cloning"
      },
      {
        "paperId": "070a482e21c1f95dcd8d037f561ce7cf01d1e746",
        "title": "Enabling Ensemble Reinforcement Learning with ICN in Mobile Edge Computing Environments"
      },
      {
        "paperId": "8a8e5af38cfa500cc7e3cac76cd5a1d59aeecdfc",
        "title": "Advanced deep-reinforcement-learning methods for flow control: group-invariant and positional-encoding networks improve learning speed and quality"
      },
      {
        "paperId": "06ca5fab665b49f44ed697b003691552d25270cd",
        "title": "Skill enhancement learning with knowledge distillation"
      },
      {
        "paperId": "a932e233bca84a84329afa604f59c8e861886c57",
        "title": "Towards Domain-Aware Stable Meta Learning for Out-of-Distribution Generalization"
      },
      {
        "paperId": "d9be146c80078ced12602b9d6a9c0c8ecb44d941",
        "title": "I Know How: Combining Prior Policies to Solve New Tasks"
      },
      {
        "paperId": "f7cd019af29a9f860365ca57e099dafa40718798",
        "title": "An Improved Prioritized DDPG Based on Fractional-Order Learning Scheme"
      },
      {
        "paperId": "c44471e846846bde281779405a3b5c132fd60b00",
        "title": "Survey on Large Language Model-Enhanced Reinforcement Learning: Concept, Taxonomy, and Methods"
      },
      {
        "paperId": "5a8dd0e310ef3465ac286a0aab776b2af60b99b6",
        "title": "Sample Efficient Myopic Exploration Through Multitask Reinforcement Learning with Diverse Tasks"
      },
      {
        "paperId": "279c8d34b28e2d91e0465db367a4e53506617487",
        "title": "Active reinforcement learning versus action bias and hysteresis: control with a mixture of experts and nonexperts"
      },
      {
        "paperId": "951542a0719aea376d6b0279603df142ab20826b",
        "title": "One is More: Diverse Perspectives within a Single Network for Efficient DRL"
      },
      {
        "paperId": "f817e4d16a2300f5dc8d408c50442b999f0db890",
        "title": "RL-ViGen: A Reinforcement Learning Benchmark for Visual Generalization"
      },
      {
        "paperId": "256d20b96fa0ec65a373bfe64f128eb56b4ea508",
        "title": "Instruction Tuned Models are Quick Learners"
      },
      {
        "paperId": "88eb6cf17a61534462757b9d1826c05da9eaf635",
        "title": "Ensemble Reinforcement Learning: A Survey"
      },
      {
        "paperId": "8e9931c561dbba7227baba58f7b487a5e2b5c676",
        "title": "Towards Inference Efficient Deep Ensemble Learning"
      },
      {
        "paperId": "7e287a03b8d4073ec05537eb100dee91ec292195",
        "title": "PECAN: Leveraging Policy Ensemble for Context-Aware Zero-Shot Human-AI Coordination"
      }
    ],
    "score": 7.666666666666666
  },
  {
    "id": "1e650cb12aaad371a49b0c8c4514e1b988a5178c",
    "title": "Pareto Policy Pool for Model-based Offline Reinforcement Learning",
    "authors": [
      "Yijun Yang",
      "J. Jiang",
      "Tianyi Zhou",
      "Jie Ma",
      "Yuhui Shi"
    ],
    "year": 2022,
    "citationCount": 22,
    "abstract": "",
    "url": "https://www.semanticscholar.org/paper/1e650cb12aaad371a49b0c8c4514e1b988a5178c",
    "pdf_url": null,
    "venue": "International Conference on Learning Representations",
    "publicationDate": "",
    "externalIds": {
      "DBLP": "conf/iclr/YangJZMS22",
      "CorpusId": 251649208
    },
    "references": [],
    "cited_by": [
      {
        "paperId": "520433c3777514cc084b9a052fbe7666c7a8b4bc",
        "title": "Controllable Pareto Trade-off between Fairness and Accuracy"
      },
      {
        "paperId": "05193b2e1411189505da039e0b595c3f593403f5",
        "title": "STIMULUS: Achieving Fast Convergence and Low Sample Complexity in Stochastic Multi-Objective Learning"
      },
      {
        "paperId": "69dcde212b9cbfd0d6354184747e1d2d5f4443da",
        "title": "Model-Based Offline Reinforcement Learning with Reliability-Guaranteed Sequence Modeling"
      },
      {
        "paperId": "cbe901f436440d0e196bb02e273b57044af228e6",
        "title": "FERERO: A Flexible Framework for Preference-Guided Multi-Objective Learning"
      },
      {
        "paperId": "09b7e4b38736f94c032bc0e12d274aa5d13f8cd9",
        "title": "POCE: Primal Policy Optimization with Conservative Estimation for Multi-constraint Offline Reinforcement Learning"
      },
      {
        "paperId": "b44b0e8222021b51783c62b0bce071ef6fb3f12c",
        "title": "OCEAN-MBRL: Offline Conservative Exploration for Model-Based Offline Reinforcement Learning"
      },
      {
        "paperId": "425f1edd88fe3539c40ddd93c3e07c95de67ba00",
        "title": "Embodied Multi-Modal Agent trained by an LLM from a Parallel TextWorld"
      },
      {
        "paperId": "a8cc4a6a37509909b8ea688514f68248deb8f502",
        "title": "Federated Multi-Objective Learning"
      },
      {
        "paperId": "d293a75f529d7b1abb161480a95122c9a3ed6376",
        "title": "Causal Reinforcement Learning: A Survey"
      },
      {
        "paperId": "ba1c8e1d51dc8a2685d2f0ddc146c54ef7d61d23",
        "title": "Three-Way Trade-Off in Multi-Objective Learning: Optimization, Generalization and Conflict-Avoidance"
      },
      {
        "paperId": "ce588d27c10a475ef766ec4106cc0fd70e605349",
        "title": "Continual Task Allocation in Meta-Policy Network via Sparse Prompting"
      },
      {
        "paperId": "bfd9d1b5d0b4af68d393ee4865aaa2eeebd97ae6",
        "title": "Uncertainty-Driven Trajectory Truncation for Data Augmentation in Offline Reinforcement Learning"
      },
      {
        "paperId": "2f130f118ee696d6a97cf22b2e02738d550c52e8",
        "title": "One Risk to Rule Them All: A Risk-Sensitive Perspective on Model-Based Offline Reinforcement Learning"
      },
      {
        "paperId": "da91a4d25131c442cff6119bc770683d5231ff43",
        "title": "Mitigating Gradient Bias in Multi-objective Learning: A Provably Convergent Stochastic Approach"
      },
      {
        "paperId": "41479416d858bacfaa01bfc068ceb86466d542e9",
        "title": "User-Interactive Offline Reinforcement Learning"
      },
      {
        "paperId": "ac524357a12469e634ef9b003832aeb93c21781d",
        "title": "RAMBO-RL: Robust Adversarial Model-Based Offline Reinforcement Learning"
      },
      {
        "paperId": "ef04516658ecc9e42ec978ccf9586c06ecbef4b0",
        "title": "SemiNLL: A Framework of Noisy-Label Learning by Semi-Supervised Learning"
      },
      {
        "paperId": "2995fdd2073562effa7bcfed17c2928527f575c2",
        "title": "Neural Stochastic Differential Equations for Uncertainty-Aware Offline RL"
      },
      {
        "paperId": "3ed097d7351c32bb971a00d981281b5a9c28facd",
        "title": "Uncertainty-driven Trajectory Truncation for Model-based Offline Reinforcement Learning"
      },
      {
        "paperId": "3b1dcf6d1a0e3b37641d751471bef5398f2ee3ac",
        "title": "Mitigating Gradient Bias in Multi-objective Learning: A Provably Convergent Approach"
      },
      {
        "paperId": "a7f1be908de231bb72928fa984289501c0796823",
        "title": "One Risk to Rule Them All: Addressing Distributional Shift in Offline Reinforcement Learning via Risk-Aversion"
      },
      {
        "paperId": "449381674c67d6fd178776dfcfe1769da795b105",
        "title": "EAT-C: Environment-Adversarial sub-Task Curriculum for Efficient Reinforcement Learning"
      }
    ],
    "score": 7.333333333333333
  },
  {
    "id": "1d2acae1d631e932c7ebe96fad3c3b04909e5cee",
    "title": "A Deep Reinforcement Learning Strategy Combining Expert Experience Guidance for a Fruit-Picking Manipulator",
    "authors": [
      "Yuqi Liu",
      "Po Gao",
      "Change Zheng",
      "Lijing Tian",
      "Ye Tian"
    ],
    "year": 2022,
    "citationCount": 22,
    "abstract": "When using deep reinforcement learning algorithms for path planning of a multi-DOF fruit-picking manipulator in unstructured environments, it is much too difficult for the multi-DOF manipulator to obtain high-value samples at the beginning of training, resulting in low learning and convergence efficiency. Aiming to reduce the inefficient exploration in unstructured environments, a reinforcement learning strategy combining expert experience guidance was first proposed in this paper. The ratios of expert experience to newly generated samples and the frequency of return visits to expert experience were studied by the simulation experiments. Some conclusions were that the ratio of expert experience, which declined from 0.45 to 0.35, was more effective in improving learning efficiency of the model than the constant ratio. Compared to an expert experience ratio of 0.35, the success rate increased by 1.26%, and compared to an expert experience ratio of 0.45, the success rate increased by 20.37%. The highest success rate was achieved when the frequency of return visits was 15 in 50 episodes, an improvement of 31.77%. The results showed that the proposed method can effectively improve the model performance and enhance the learning efficiency at the beginning of training in unstructured environments. This training method has implications for the training process of reinforcement learning in other domains.",
    "url": "https://www.semanticscholar.org/paper/1d2acae1d631e932c7ebe96fad3c3b04909e5cee",
    "pdf_url": "https://doi.org/10.3390/electronics11030311",
    "venue": "Electronics",
    "publicationDate": "2022-01-19",
    "externalIds": {
      "DOI": "10.3390/electronics11030311",
      "CorpusId": 246085198
    },
    "references": [
      {
        "paperId": "adadeb0ff410f8e1700cc971c7ebc335db2f3302",
        "title": "Scheduling of AGVs in Automated Container Terminal Based on the Deep Deterministic Policy Gradient (DDPG) Using the Convolutional Neural Network (CNN)"
      },
      {
        "paperId": "47eb4d0d489424be244e369653bd2ef1f70b09dc",
        "title": "PMSM Speed Control Based on Particle Swarm Optimization and Deep Deterministic Policy Gradient under Load Disturbance"
      },
      {
        "paperId": "da9e3eba7b997d84d77e978bbae995477aa4784f",
        "title": "Automatic Path Planning Offloading Mechanism in Edge-Enabled Environments"
      },
      {
        "paperId": "e4ae547b2185cd60cc93637b77024626b65565d9",
        "title": "Deep Reinforcement Learning-Based Accurate Control of Planetary Soft Landing"
      },
      {
        "paperId": "abb98a07ee4dcefeead57190ac9750bd11ee95d0",
        "title": "Collision-Free Motion Planning of a Six-Link Manipulator Used in a Citrus Picking Robot"
      },
      {
        "paperId": "34672ec9b9a8ddf4ff9c94c8949197aeda603d76",
        "title": "A Multi-Objective Coverage Path Planning Algorithm for UAVs to Cover Spatially Distributed Regions in Urban Environments"
      },
      {
        "paperId": "449eea527078ebbc78ea8fb47e8964a15c2f00dd",
        "title": "Efficient Local Path Planning Algorithm Using Artificial Potential Field Supported by Augmented Reality"
      },
      {
        "paperId": "1291e5781c55283271bb6f84f1085cdadd981ce3",
        "title": "A Study on an Enhanced Autonomous Driving Simulation Model Based on Reinforcement Learning Using a Collision Prevention Model"
      },
      {
        "paperId": "b6add19db0a40b4d9d974608dc3f29eda747f5f2",
        "title": "Deep Reinforcement Learning-Based Path Planning for Multi-Arm Manipulators with Periodically Moving Obstacles"
      },
      {
        "paperId": "49eba739320af41b21ad7afcfcfe2b282b59e3fc",
        "title": "Deep reinforcement learning based moving object grasping"
      },
      {
        "paperId": "e8f1b6017bf4714dad15bf26b59a9e2c692b777e",
        "title": "Learning-Based End-to-End Path Planning for Lunar Rovers with Safety Constraints"
      },
      {
        "paperId": "eca65630a26e57ea126ae0e3cba176c4c8171c24",
        "title": "Research and Implementation of Intelligent Decision Based on a Priori Knowledge and DQN Algorithms in Wargame Environment"
      },
      {
        "paperId": "1b259867d144c6009995466d8d9eeec60b4aa3fb",
        "title": "Motion Planning of Robot Manipulators for a Smoother Path Using a Twin Delayed Deep Deterministic Policy Gradient with Hindsight Experience Replay"
      },
      {
        "paperId": "9f4bd3ef9435b75e21105265c6415310d62776b2",
        "title": "RRT-based path planning for an intelligent litchi-picking manipulator"
      },
      {
        "paperId": "55fb5803d6b0c797e0a166f4152663fb692b3130",
        "title": "Path Planning of Humanoid Arm Based on Deep Deterministic Policy Gradient"
      },
      {
        "paperId": "e450b83add854ab04c4becc1697f6f22e0962540",
        "title": "Ant Colony Optimization with Improved Potential Field Heuristic for Robot Path Planning"
      },
      {
        "paperId": "027d002d205e49989d734603ff0c2f7cbfa6b6dd",
        "title": "A novel DDPG method with prioritized experience replay"
      },
      {
        "paperId": "e3b0ea7209731c47b582215c6c67f9c691ad9863",
        "title": "Deep Q-learning From Demonstrations"
      },
      {
        "paperId": "e37b999f0c96d7136db07b0185b837d5decd599a",
        "title": "Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates"
      },
      {
        "paperId": "c6170fa90d3b2efede5a2e1660cb23e1c824f2ca",
        "title": "Prioritized Experience Replay"
      },
      {
        "paperId": "024006d4c2a89f7acacc6e4438d156525b60a98f",
        "title": "Continuous control with deep reinforcement learning"
      },
      {
        "paperId": "e0d280c1b0cf91e71495fe29c4e6daff4ade8d8e",
        "title": "Task and Motion Planning for Apple Harvesting Robot"
      },
      {
        "paperId": "33a3defface2cb3295811d8717945cd3ebb7f5eb",
        "title": "Motion Planning for Mobile Robots\u2014Focusing on Deep Reinforcement Learning: A Systematic Review"
      },
      {
        "paperId": "05550696a0a1f2ca9c444aada0011019d9ce84f9",
        "title": "Deep Reinforcement Learning With Optimized Reward Functions for Robotic Trajectory Planning"
      },
      {
        "paperId": "a693f721b3cf1ef292f48333fdd92f9e6345a677",
        "title": "Obstacle-avoidance Path Planning of Robot Arm for Tomato-picking Robot"
      },
      {
        "paperId": "2593b90ea7a2412b8e05427d997d60cddc47b470",
        "title": "Path Planning for Space Manipulator to Avoid Obstacle Based on A~* Algorithm"
      },
      {
        "paperId": "4b854ab0268c9b1a63a53594e25059b3e15831dc",
        "title": "Algorithm for optimization of apple harvesting path and simulation."
      },
      {
        "paperId": null,
        "title": "Fruits segmentation method based on super pixel features for apple harvesting"
      },
      {
        "paperId": null,
        "title": "Path planning of fruits harvesting"
      },
      {
        "paperId": null,
        "title": "Path planning of fruits harvesting robot"
      },
      {
        "paperId": null,
        "title": "3 D path planning approach based on gravitational search algorithm for sprayer UAV"
      },
      {
        "paperId": null,
        "title": "Depth-sphere transversal method for on-branch citrus fruit recognition"
      },
      {
        "paperId": null,
        "title": "Simulation for Manipulator Trajectory Planning Based on Deep Reinforcement Learning"
      },
      {
        "paperId": null,
        "title": "Trajectory planning method for apple picking manipulator based on stepwise migration strategy"
      },
      {
        "paperId": null,
        "title": "3D path planning approach based on gravitational search algorithm for sprayer"
      },
      {
        "paperId": null,
        "title": "Fruits segmentation method based on super pixel features for apple harvesting robot"
      },
      {
        "paperId": null,
        "title": "Simulation for Manipulator Trajectory Planning Based on Deep Reinforcement Learning. Master\u2019s Thesis, University of Electronic Science and Technology of China, Chengdu, China, 2020"
      }
    ],
    "cited_by": [
      {
        "paperId": "313482674077eabac567da95b78dc243ee7467b5",
        "title": "Dynamic obstacle avoidance and grasping planning for mobile robotic arm in complex environment based on improved TD3"
      },
      {
        "paperId": "2440d980b9294242104b0d2c3a54a74cf0ce0d28",
        "title": "Applications of Multi-Robotic Arms to Assist Agricultural Production: A Review"
      },
      {
        "paperId": "0ef412f8367460c38cf675ce46b0d917f54cfb00",
        "title": "A Review of Research on Fruit and Vegetable Picking Robots Based on Deep Learning"
      },
      {
        "paperId": "a82dfbab0a33ba72d8a16cbdd7d124e3514affea",
        "title": "PEMFC Thermal Management Control Strategy Based on Dual Deep Deterministic Policy Gradient"
      },
      {
        "paperId": "5d4e1169a52f16a86e81760f860b991a41cbe3ee",
        "title": "Compliant Motion Planning Integrating Human Skill for Robotic Arm Collecting Tomato Bunch Based on Improved DDPG"
      },
      {
        "paperId": "76f4fc9c695b7d19c701bae5215474ad50a9fe7a",
        "title": "Research on kiwifruit harvesting robot worldwide: A solution for sustainable development of kiwifruit industry"
      },
      {
        "paperId": "124def719f2b6840f88980b37c376146b9a4147a",
        "title": "Deep Reinforcement Learning for Robotic Arm Path Planning in Multi-Obstacle Environments"
      },
      {
        "paperId": "c0342f24f4596d6d301966a9e37596d5008ae386",
        "title": "Peduncle collision-free grasping based on deep reinforcement learning for tomato harvesting robot"
      },
      {
        "paperId": "f5bcd7551c17b2356b77530b6e2d077e33ed60f6",
        "title": "Review on Motion Planning of Robotic Manipulator in Dynamic Environments"
      },
      {
        "paperId": "85726e40a76fc2a51849e42192921a40f3d14976",
        "title": "Deep Reinforcement Learning with Inverse Jacobian based Model-Free Path Planning for Deburring in Complex Industrial Environment"
      },
      {
        "paperId": "160c746040c6b5f94ea312a501f19bea3df88115",
        "title": "Robust Control Approaches and Trajectory Planning Strategies for Industrial Robotic Manipulators in the Era of Industry 4.0: A Comprehensive Review"
      },
      {
        "paperId": "529e32fc4e229b90839a9c26f049094380dc828a",
        "title": "Trajectory Planning and Control of Serially Linked Robotic Arm for Fruit Picking Using Reinforcement Learning"
      },
      {
        "paperId": "f37a982a5cea15e7a4580f930eddb1b64c8e3fc6",
        "title": "EasyDAM_V3: Automatic Fruit Labeling Based on Optimal Source Domain Selection and Data Synthesis via a Knowledge Graph"
      },
      {
        "paperId": "3f236495d0dbec2ba8ee01dafd6a23ef9acabf95",
        "title": "Three-dimensional continuous picking path planning based on ant colony optimization algorithm"
      },
      {
        "paperId": "c8a493183be7be125c1ea8a19da7640fb41535d9",
        "title": "Deep reinforcement learning based energy management strategy for range extend fuel cell hybrid electric vehicle"
      },
      {
        "paperId": "2cbd4577688bf19f4217d09811b4e719cdfc6d9b",
        "title": "Modeling and Control of Robotic Manipulators Based on Artificial Neural Networks: A Review"
      },
      {
        "paperId": "07641a0b4f052e5c5baf6bbc74d00a95390cf94c",
        "title": "Prioritized Hindsight with Dual Buffer for Meta-Reinforcement Learning"
      },
      {
        "paperId": "448131c0ec4aec2c9f7c6723e4db696c61ac622f",
        "title": "Autoencoder-based OFDM for Agricultural Image Transmission"
      },
      {
        "paperId": "807bd14ab066fac3d61ca8b4cc088e3febfd5b1f",
        "title": "Optimal scheduling for palletizing task using robotic arm and artificial bee colony algorithm"
      },
      {
        "paperId": "098f1fa72aed7edc77946ea1b267b70597dde16a",
        "title": "Improved Rapidly Exploring Random Tree with Bacterial Mutation and Node Deletion for Offline Path Planning of Mobile Robot"
      },
      {
        "paperId": "2d4b2923e337637329cb2f14b29fb60be9351d57",
        "title": "Design of thick panels origami-inspired flexible grasper with anti-interference ability"
      },
      {
        "paperId": "3d02964e2d5de46899c7113b5389d8a865966874",
        "title": "A Two-Stage Support Vector Machine and SqueezeNet System for Range-Angle and Range-Speed Estimation in a Cluttered Environment of Automotive MIMO Radar Systems"
      }
    ],
    "score": 7.333333333333333
  },
  {
    "id": "fe7382db243694c67c667cf2ec80072577d2372b",
    "title": "Rule-Aware Reinforcement Learning for Knowledge Graph Reasoning",
    "authors": [
      "Zhongni Hou",
      "Xiaolong Jin",
      "Zixuan Li",
      "Long Bai"
    ],
    "year": 2021,
    "citationCount": 29,
    "abstract": "Multi-hop reasoning is an effective and ex-plainable approach to predicting missing facts in Knowledge Graphs (KGs). It usually adopts the Reinforcement Learning (RL) framework and searches over the KG to \ufb01nd an evidential path. However, due to the large exploration space, the RL-based model struggles with the serious sparse reward problem and needs to make a lot of trials. Moreover, its exploration can be biased towards spurious paths that coin-cidentally lead to correct answers. To solve both problems, we propose a simple but effective RL-based method called RARL (Rule-Aware RL). It injects high quality symbolic rules into the model\u2019s reasoning process and employs partially random beam search, which can not only increase the probability of paths getting rewards, but also alleviate the impact of spurious paths. Experimental results show that it outperforms existing multi-hop methods in terms of Hit@1 and MRR.",
    "url": "https://www.semanticscholar.org/paper/fe7382db243694c67c667cf2ec80072577d2372b",
    "pdf_url": "https://doi.org/10.18653/v1/2021.findings-acl.412",
    "venue": "Findings",
    "publicationDate": "2021-08-01",
    "externalIds": {
      "DBLP": "conf/acl/HouJLB21",
      "MAG": "3174267585",
      "ACL": "2021.findings-acl.412",
      "DOI": "10.18653/v1/2021.findings-acl.412",
      "CorpusId": 236477903
    },
    "references": [
      {
        "paperId": "065691db44b3b1ae48ffb64559e7a98e5b80306f",
        "title": "Query Graph Generation for Answering Multi-hop Complex Questions from Knowledge Bases"
      },
      {
        "paperId": "4e36251b223b86066aa280d6ea8a277c51775281",
        "title": "Differentiable learning of numerical rules in knowledge graphs"
      },
      {
        "paperId": "b473c512b9e75b9b3dcd8dbbd65c19b51c4e423f",
        "title": "Contextual Parameter Generation for Knowledge Graph Link Prediction"
      },
      {
        "paperId": "845b4941d8c016aa5f8967da2f86d38ef6c18fa3",
        "title": "A Survey on Knowledge Graphs: Representation, Acquisition, and Applications"
      },
      {
        "paperId": "cd1f7724a1b3d2bdacc1a87e23ca52d0244d9345",
        "title": "Reasoning on Knowledge Graphs with Debate Dynamics"
      },
      {
        "paperId": "ebcaee68bdbad2758fcf498b23ded232c1e3df57",
        "title": "Rule-Guided Compositional Representation Learning on Knowledge Graphs"
      },
      {
        "paperId": "132d602fd1276521f7b08577f5d4dbec2adc3c03",
        "title": "Dealing with Sparse Rewards in Reinforcement Learning"
      },
      {
        "paperId": "7445966ded8f38cac487314e8c6b71cdf327b047",
        "title": "DRUM: End-To-End Differentiable Rule Mining On Knowledge Graphs"
      },
      {
        "paperId": "97a7930625b27e286dc1c24165a79de24f3e4561",
        "title": "Collaborative Policy Learning for Open Knowledge Graph Reasoning"
      },
      {
        "paperId": "de02ec03f6a71246e505862a7195894601fbab99",
        "title": "KGAT: Knowledge Graph Attention Network for Recommendation"
      },
      {
        "paperId": "8b874d6d1e86567128b4b7623f7e59bbbfe8bf22",
        "title": "Iteratively Learning Embeddings and Rules for Knowledge Graph Reasoning"
      },
      {
        "paperId": "05dc5fb3a3bdefdf181aafcc42cd80ff6b7704e7",
        "title": "TuckER: Tensor Factorization for Knowledge Graph Completion"
      },
      {
        "paperId": "7a941148d8c5865749801b2f9f67f9ad1fba1d25",
        "title": "Multi-Hop Knowledge Graph Reasoning with Reward Shaping"
      },
      {
        "paperId": "e9f32b34fde7d0034a147d5996470634c9e19002",
        "title": "Go for a Walk and Arrive at the Answer: Reasoning Over Paths in Knowledge Bases using Reinforcement Learning"
      },
      {
        "paperId": "c28ec2a40a2c77e20d64cf1c85dc931106df8e83",
        "title": "Overcoming Exploration in Reinforcement Learning with Demonstrations"
      },
      {
        "paperId": "857176d022369e963d3ff1be2cb9e1ca2f674520",
        "title": "DeepPath: A Reinforcement Learning Method for Knowledge Graph Reasoning"
      },
      {
        "paperId": "9697d32ed0a16da167f2bdba05ef96d0da066eb5",
        "title": "Convolutional 2D Knowledge Graph Embeddings"
      },
      {
        "paperId": "5889e9afbcc3935867f9ae16fe46c71b9f2b071f",
        "title": "End-to-end Differentiable Proving"
      },
      {
        "paperId": "1ae84940e212e2e3ca132c7aa0878baf0fda06cd",
        "title": "From Language to Programs: Bridging Reinforcement Learning and Maximum Marginal Likelihood"
      },
      {
        "paperId": "fcbedb12dcc3618df7a76012944c64fd62a18286",
        "title": "Learning Symmetric Collaborative Dialogue Agents with Dynamic Knowledge Graph Embeddings"
      },
      {
        "paperId": "23e42bc79f10234bdceef31441be39a2d9d2a9a0",
        "title": "Differentiable Learning of Logical Rules for Knowledge Base Reasoning"
      },
      {
        "paperId": "2218e2e1df2c3adfb70e0def2e326a39928aacfc",
        "title": "Complex Embeddings for Simple Link Prediction"
      },
      {
        "paperId": "619ccea2ae6bc6c1b02731a578e184a300d6a966",
        "title": "Fast rule mining in ontological knowledge bases with AMIE+\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$+$$\\end{docu"
      },
      {
        "paperId": "86412306b777ee35aba71d4795b02915cb8a04c3",
        "title": "Embedding Entities and Relations for Learning and Inference in Knowledge Bases"
      },
      {
        "paperId": "cea967b59209c6be22829699f05b8b1ac4dc092d",
        "title": "Sequence to Sequence Learning with Neural Networks"
      },
      {
        "paperId": "2582ab7c70c9e7fcb84545944eba8f3a7f253248",
        "title": "Translating Embeddings for Modeling Multi-relational Data"
      },
      {
        "paperId": "974d1048fa45227a3ef9f71efe5501f79683dfdf",
        "title": "Improving Learning and Inference in a Large Knowledge-Base using Latent Syntactic Cues"
      },
      {
        "paperId": "b6ce4ec0d28c050b99ec647a16e47116c939473c",
        "title": "Statistical predicate invention"
      },
      {
        "paperId": "7e928ef936c2815d7522c5176163d6ab7309a8b7",
        "title": "Representing Text for Joint Embedding of Text and Knowledge Bases"
      },
      {
        "paperId": "4c915c1eecb217c123a36dc6d3ce52d12c742614",
        "title": "Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning"
      }
    ],
    "cited_by": [
      {
        "paperId": "f5241f25b535dc9c1e53478695f1c12e4ab19a7f",
        "title": "Graph Self-attention Mechanism for Interpretable Multi-hop Knowledge Graph Link Prediction"
      },
      {
        "paperId": "a60a61466e5bee16666c64e6c5e19c6d95cef99b",
        "title": "SCR: A completion-then-reasoning framework for multi-hop question answering over incomplete knowledge graph"
      },
      {
        "paperId": "fd7a603630b43bf7a0f2a43191241e87483ff33a",
        "title": "Look one step ahead through first-order aggregation in reinforcement learning-based knowledge graph reasoning"
      },
      {
        "paperId": "e242fac031a0fc05fca2bcb15683805364922111",
        "title": "Multi-hop reasoning over sparse knowledge graphs with deep reinforcement learning"
      },
      {
        "paperId": "5fa9bcf38bcf71d9b0dd27b7c84023f1aa8b0f7e",
        "title": "A Survey of Task-Oriented Knowledge Graph Reasoning: Status, Applications, and Prospects"
      },
      {
        "paperId": "035fc8ecad9f161321e5480a00d069da9865e153",
        "title": "PMHR: Path-Based Multi-Hop Reasoning Incorporating Rule-Enhanced Reinforcement Learning and KG Embeddings"
      },
      {
        "paperId": "779db90483d8e07be96e9be4587826e2b579cad9",
        "title": "ARL: analogical reinforcement learning for knowledge graph reasoning"
      },
      {
        "paperId": "2eeb0baafd529c033d41918cca2040d5dbec23cc",
        "title": "Relational reasoning based on deep reinforcement learning"
      },
      {
        "paperId": "a2770606807631a930ce9902f02785e098dd9b46",
        "title": "Dual-Agent Multi-Hop Reasoning Based on Reward Shaping and Action Dropout"
      },
      {
        "paperId": "a34c592165ee10fbd05d4fae71f7e522b6ed3eed",
        "title": "Hierarchical Knowledge-Enhancement Framework for multi-hop knowledge graph reasoning"
      },
      {
        "paperId": "6a66b459955959c4b8a67bd298ed291506923b7a",
        "title": "A collaborative learning framework for knowledge graph embedding and reasoning"
      },
      {
        "paperId": "4ef2aa246044d1244747783b1647c39e7f24f355",
        "title": "Machine Learning for Refining Knowledge Graphs: A Survey"
      },
      {
        "paperId": "a0d9f3672d3190bdeae3588caad1c16ae83c2bb0",
        "title": "Attention-based exploitation and exploration strategy for multi-hop knowledge graph reasoning"
      },
      {
        "paperId": "18664b47516ba5424ba5efa79d3f816224245325",
        "title": "ChatRule: Mining Logical Rules with Large Language Models for Knowledge Graph Reasoning"
      },
      {
        "paperId": "fff48be5e6ef0cab7e45198a768fc62b9e626be2",
        "title": "Comprehensible Artificial Intelligence on Knowledge Graphs: A survey"
      },
      {
        "paperId": "aec8eed33a7f9195ba0925c798e6431702d22e1a",
        "title": "End-to-End Entity Linking with Hierarchical Reinforcement Learning"
      },
      {
        "paperId": "3f1473dd6c0f159a255b16f44998663e605b4878",
        "title": "Do as I Can, Not as I Get: Topology-Aware Multi-Hop Reasoning on Multi-Modal Knowledge Graphs"
      },
      {
        "paperId": "aa35bf542164d0467577ff68fc7d1d554aba58ab",
        "title": "Reinforcement learning with dynamic completion for answering multi-hop questions over incomplete knowledge graph"
      },
      {
        "paperId": "a63eb28869a9ab461bf93199e6f91791ad80f245",
        "title": "Effects of Locality and Rule Language on Explanations for Knowledge Graph Embeddings"
      },
      {
        "paperId": "730f037ee3652cffbe0d9200582baa2b6afdbdde",
        "title": "A causal-based symbolic reasoning framework for uncertain knowledge graphs"
      },
      {
        "paperId": "4851fc99dd95b568a38e52546a0c23cf7d0ef48e",
        "title": "Reason more like human: Incorporating meta information into hierarchical reinforcement learning for knowledge graph reasoning"
      },
      {
        "paperId": "d783f5d4421d680542ff7684e12e2e4785897c9f",
        "title": "EG-KGR: A Knowledge Graph Reasoning Model Based on Enhanced Graph Sample and Aggregate Inductive Learning Algorithm"
      },
      {
        "paperId": "3486dc8a89e1d45880a08b5fc98ffaa5c6592336",
        "title": "ARL: An adaptive reinforcement learning framework for complex question answering over knowledge base"
      },
      {
        "paperId": "b7818f5b47b3ea9c75e8ecc71b57ec3e85356c5a",
        "title": "Rule Mining over Knowledge Graphs via Reinforcement Learning"
      },
      {
        "paperId": "412bc1caf92614838749dcfce104776a36fdb7d7",
        "title": "SQUIRE: A Sequence-to-sequence Framework for Multi-hop Knowledge Graph Reasoning"
      },
      {
        "paperId": "8e29b969cbe3ec2bd6d26195052f9ebe66649a5a",
        "title": "Path Spuriousness-aware Reinforcement Learning for Multi-Hop Knowledge Graph Reasoning"
      },
      {
        "paperId": "df4a09f829ffed401930d20fc208a91e9c133bc6",
        "title": "AInvR: Adaptive Learning Rewards for Knowledge Graph Reasoning Using Agent Trajectories"
      },
      {
        "paperId": "78d587e730b55add3e994eed3b556402699d71b3",
        "title": "Rule Learning over Knowledge Graphs: A Review"
      },
      {
        "paperId": "9c51f896ed229e17163f75269cac44cab7f574e7",
        "title": "Construction and Evolution of Fault Diagnosis Knowledge Graph in Industrial Process"
      }
    ],
    "score": 7.25
  },
  {
    "id": "ba0e111b711e9b577932a02c9e40bc44b56cb88d",
    "title": "Reinforcement Learning with Fast Stabilization in Linear Dynamical Systems",
    "authors": [
      "Sahin Lale",
      "K. Azizzadenesheli",
      "B. Hassibi",
      "Anima Anandkumar"
    ],
    "year": 2020,
    "citationCount": 36,
    "abstract": "In this work, we study model-based reinforcement learning (RL) in unknown stabilizable linear dynamical systems. When learning a dynamical system, one needs to stabilize the unknown dynamics in order to avoid system blow-ups. We propose an algorithm that certifies fast stabilization of the underlying system by effectively exploring the environment with an improved exploration strategy. We show that the proposed algorithm attains $\\tilde{\\mathcal{O}}(\\sqrt{T})$ regret after $T$ time steps of agent-environment interaction. We also show that the regret of the proposed algorithm has only a polynomial dependence in the problem dimensions, which gives an exponential improvement over the prior methods. Our improved exploration method is simple, yet efficient, and it combines a sophisticated exploration policy in RL with an isotropic exploration strategy to achieve fast stabilization and improved regret. We empirically demonstrate that the proposed algorithm outperforms other popular methods in several adaptive control tasks.",
    "url": "https://www.semanticscholar.org/paper/ba0e111b711e9b577932a02c9e40bc44b56cb88d",
    "pdf_url": "https://arxiv.org/pdf/2007.12291.pdf",
    "venue": "International Conference on Artificial Intelligence and Statistics",
    "publicationDate": "2020-07-23",
    "externalIds": {
      "DBLP": "conf/aistats/LaleAHA22",
      "ArXiv": "2007.12291",
      "CorpusId": 248923900
    },
    "references": [
      {
        "paperId": "f543b5c3cab53bfa2ade599842090f0a33d7d135",
        "title": "Infinite-horizon Risk-constrained Linear Quadratic Regulator with Average Cost"
      },
      {
        "paperId": "51301b9c3bfd01c840579bf120ee92207bc3907a",
        "title": "Black-Box Control for Linear Dynamical Systems"
      },
      {
        "paperId": "171a46ea0c20631327f0347fecb8224c874d6c4d",
        "title": "Efficient Optimistic Exploration in Linear-Quadratic Regulators via Lagrangian Relaxation"
      },
      {
        "paperId": "0476d87ae13ec7ee4ef71ccee35dbe4d0dad611b",
        "title": "Logarithmic Regret Bound in Partially Observable Linear Dynamical Systems"
      },
      {
        "paperId": "4fe5cc0e6e38a958a5a4b93c9301b86389532837",
        "title": "Regret Bound of Adaptive Control in Linear Quadratic Gaussian (LQG) Systems"
      },
      {
        "paperId": "78ac4fbf318818481e471fd2901627b66cb2ecd4",
        "title": "Logarithmic Regret for Learning Linear Quadratic Regulators Efficiently"
      },
      {
        "paperId": "e1b0987ecc4629590613aeb4a1384505d4a8db39",
        "title": "Regret Minimization in Partially Observable Linear Quadratic Control"
      },
      {
        "paperId": "c93632b6b0ad4742b3ad838732e2ce2451588a3e",
        "title": "Naive Exploration is Optimal for Online LQR"
      },
      {
        "paperId": "9594ca0eeebf14dc81307f77873948d89eeec6ef",
        "title": "Improper Learning for Non-Stochastic Control"
      },
      {
        "paperId": "07c278c13f26f8e339c5ed9f4d8f320a6d480a21",
        "title": "The Nonstochastic Control Problem"
      },
      {
        "paperId": "4d2116795ffc5bdf2eba56ddca00aafb5a4e3c95",
        "title": "Randomized Algorithms for Data-Driven Stabilization of Stochastic Linear Systems"
      },
      {
        "paperId": "2c6f55132fe4b6dea414a300cc599c8d79c74a97",
        "title": "Certainty Equivalent Control of LQR is Efficient"
      },
      {
        "paperId": "032341c6e7de3e2180be6e433cc721cebfc0940e",
        "title": "Input Perturbations for Adaptive Regulation and Learning"
      },
      {
        "paperId": "26dc11f12e36c15911a2a26887852cbb84655680",
        "title": "Linear Estimation"
      },
      {
        "paperId": "28826c725e6d7dc88c7ab84e47805a0cf9286e61",
        "title": "Finite Time Adaptive Stabilization of LQ Systems"
      },
      {
        "paperId": "79d519d69ed9a3d4e06e9eab098c8f24eff25cf0",
        "title": "Improved Regret Bounds for Thompson Sampling in Linear Quadratic Control Problems"
      },
      {
        "paperId": "bab542377d2a73053d842232f2d717d9c84569c4",
        "title": "On adaptive Linear-Quadratic regulators"
      },
      {
        "paperId": "fb362cae5ec5b24fb5ec502b92b23b979ee724e5",
        "title": "Online Linear Quadratic Control"
      },
      {
        "paperId": "a87518fff4109d857ba624fb95296fe3b86bbf77",
        "title": "Regret Bounds for Robust Adaptive Control of the Linear Quadratic Regulator"
      },
      {
        "paperId": "ff0c75b740c5ab92e9c087c2e6575e8c4d802aa8",
        "title": "Regret Bounds for Model-Free Linear Quadratic Control"
      },
      {
        "paperId": "dcbf134deb9cc31781726ffbabd5685d6f6dfe24",
        "title": "Least-Squares Temporal Difference Learning for the Linear Quadratic Regulator"
      },
      {
        "paperId": "e53eaf5a509763e376f9e5fb2b278318430d75e0",
        "title": "On the Sample Complexity of the Linear Quadratic Regulator"
      },
      {
        "paperId": "a261dd87e8e84996b096ae2628354ee0ddc51fd5",
        "title": "Thompson Sampling for Linear-Quadratic Control Problems"
      },
      {
        "paperId": "6bea71fa6deb19c67e9586428f8f240e789fb3df",
        "title": "Improved Algorithms for Linear Stochastic Bandits"
      },
      {
        "paperId": "3f99769ca3f0e3f41a239a1b58adf26bb930b9b2",
        "title": "Off-policy Learning with Options and Recognizers"
      },
      {
        "paperId": "3665568eeab86eba575a3a4ae27ca88f77842059",
        "title": "A design of discrete-time integral controllers with computation delays via loop transfer recovery"
      },
      {
        "paperId": "0c10fce56af2d3816b7e280e208f211066187b7f",
        "title": "Convergence of adaptive control schemes using least-squares parameter estimates"
      },
      {
        "paperId": "4b48c17e8fc8a35617a336909e4a82c06e223876",
        "title": "Adaptive control with the stochastic approximation algorithm: Geometry and convergence"
      },
      {
        "paperId": "4c9e36a0dbdc3e7b20e38bd288a804c05c77e9fa",
        "title": "Asymptotically efficient adaptive allocation rules"
      },
      {
        "paperId": "0aa1446c1fe6d2cb74508487cb27e48c9e8c36fb",
        "title": "Least Squares Estimates in Stochastic Regression Models with Applications to Identification and Control of Dynamic Systems"
      },
      {
        "paperId": null,
        "title": "\u0398\u2217)A\u2217 produces stable closed-loop system, \u03c1(A\u2217 + B\u2217K(\u0398\u2217)) < 1. This result shows that, for we get unique positive definite solution to DARE for stabilizable systems"
      },
      {
        "paperId": null,
        "title": "C0\u03b52. This result shows that there exists a \u03b5-neighborhood around the system parameters that stabilizes the system. This result further extended to quantify the stability in Cassel et al. (2020)"
      },
      {
        "paperId": null,
        "title": "H\u2016\u2016H\u22121\u2016 \u2264 \u03ba, and \u2016L"
      },
      {
        "paperId": null,
        "title": "Similar to StabL, we add an additional minimum policy duration constraint to the general"
      },
      {
        "paperId": null,
        "title": "Abbasi-Yadkori and Szepesv\u00e1ri (2011))"
      },
      {
        "paperId": "572fc4c1e4bd3798876d23526840704417e97c97",
        "title": "ADAPTIVE CONTROL OF LINEAR TIME INVARIANT SYSTEMS: THE \"BET ON THE BEST\" PRINCIPLE \u2217"
      },
      {
        "paperId": "c4179bf70b2be0f31762db4bed59ba2e034d7534",
        "title": "Boekbespreking van D.P. Bertsekas (ed.), Dynamic programming and optimal control - volume 2"
      },
      {
        "paperId": null,
        "title": "Stabilizing Neighborhood Around the System Parameters Theorem A.1 (Unique Positive Definite Solution to DARE, (Bertsekas, 1995))"
      },
      {
        "paperId": null,
        "title": "This problem is the longitudinal flight control of Boeing 747 with linearized dynamics and introduced in (Ishihara et"
      },
      {
        "paperId": "cd6dc2fe6c71c0845f42d06ee64764851d4d96ac",
        "title": "Control systems design: An introduction to state-space methods : Bernard Friedland"
      },
      {
        "paperId": "572379c1d56e7e19422ae38218ee228c61aefb2f",
        "title": "Asymptotically Efficient Adaptive Allocation Rules"
      }
    ],
    "cited_by": [
      {
        "paperId": "30aa89ced91af2eb011470bf45d18737cbf2a228",
        "title": "Finite-time regret minimization for linear quadratic adaptive controllers: An experiment design approach"
      },
      {
        "paperId": "7861139c81e4ab501453c286f3c96b99783cfb5a",
        "title": "System Identification Under Bounded Noise: Optimal Rates Beyond Least Squares"
      },
      {
        "paperId": "423fbb21823f0e52b370ba802341e6882b418372",
        "title": "Neural Operator based Reinforcement Learning for Control of first-order PDEs with Spatially-Varying State Delay"
      },
      {
        "paperId": "234b0a44c5b57949b4c6941591e01f31eede5ece",
        "title": "Occupancy Information Ratio: Infinite-Horizon, Information-Directed, Parameterized Policy Search"
      },
      {
        "paperId": "6decacac529e3c0f4124d4de7e19984f8b0f0175",
        "title": "Online Policy Optimization in Unknown Nonlinear Systems"
      },
      {
        "paperId": "fc8c9c05dcd68c4403391b0f0eecb99602ea6c37",
        "title": "Nonasymptotic Regret Analysis of Adaptive Linear Quadratic Control with Model Misspecification"
      },
      {
        "paperId": "8220d3ef21490504ae838bc374f8b73489171d2b",
        "title": "Controlgym: Large-scale control environments for benchmarking reinforcement learning algorithms"
      },
      {
        "paperId": "a697bdad87199c9697edb31e0e20b5d75a10fd46",
        "title": "On the Hardness of Learning to Stabilize Linear Systems"
      },
      {
        "paperId": "c75abc549b852d3691c4a6df539c36b89ebb9d96",
        "title": "Stabilizing reinforcement learning control: A modular framework for optimizing over all stable behavior"
      },
      {
        "paperId": "c6dc82de3b777fd8874d472f90fd44b3bb009099",
        "title": "Regret Analysis of Distributed Online Control for LTI Systems with Adversarial Disturbances"
      },
      {
        "paperId": "14dd8e7a4fdd3a18e368762b033408245b5dbc48",
        "title": "Learning the Uncertainty Sets of Linear Control Systems via Set Membership: A Non-asymptotic Analysis"
      },
      {
        "paperId": "d51e5a77019ed45350431e3ad51e08157e95827f",
        "title": "Thompson Sampling for Partially Observable Linear-Quadratic Control"
      },
      {
        "paperId": "e86bc273057c182761d151c40b909b2c341502e2",
        "title": "Finite Time Regret Bounds for Minimum Variance Control of Autoregressive Systems with Exogenous Inputs"
      },
      {
        "paperId": "d864dd104ac687fcdfdf380fd8f83709193333d8",
        "title": "Regret bounds for online-learning-based linear quadratic control under database attacks"
      },
      {
        "paperId": "f0178e1edf8a410dbd6b93a34eb0cf2c2e7c2fa7",
        "title": "Online Adversarial Stabilization of Unknown Linear Time-Varying Systems"
      },
      {
        "paperId": "877dffdf100b8e59a6b10dc31d0fa0e2b26a43ba",
        "title": "Neural Operators of Backstepping Controller and Observer Gain Functions for Reaction-Diffusion PDEs"
      },
      {
        "paperId": "c9c3ca6e5cb1bd07434cdf17559322a25544c37d",
        "title": "Neural Operators for Bypassing Gain and Control Computations in PDE Backstepping"
      },
      {
        "paperId": "d04d1eeb6de9e756b777a7082a92739d03f8d323",
        "title": "Optimal exploration strategies for finite horizon regret minimization in some adaptive control problems"
      },
      {
        "paperId": "cbb5c5edf310da22134c7ec93237285f9b34a991",
        "title": "Provable Sim-to-real Transfer in Continuous Domain with Partial Observations"
      },
      {
        "paperId": "f78eed4e9742b2f063e53e5db2ce49d862e8eee7",
        "title": "Safety-Aware Learning-Based Control of Systems with Uncertainty Dependent Constraints"
      },
      {
        "paperId": "4e41a44dc16ce2ae445786f1cc316818eb92e71d",
        "title": "Learning-Based Adaptive Control for Stochastic Linear Systems With Input Constraints"
      },
      {
        "paperId": "0b5d90a629b5efa4ce237deadcb4d0d768be4a64",
        "title": "Fast identification and stabilization of unknown linear systems"
      },
      {
        "paperId": "2fc9caffd54a24631f0945a6f77c5fb56d2c9af1",
        "title": "Thompson Sampling Achieves O\u0303(\u221aT) Regret in Linear Quadratic Control"
      },
      {
        "paperId": "f0c6ad84aeb81e2c9d44b0afe8077d7dec8cb991",
        "title": "KCRL: Krasovskii-Constrained Reinforcement Learning with Guaranteed Stability in Nonlinear Dynamical Systems"
      },
      {
        "paperId": "711ac0845d42fdf58a4cc0f9ddc018913c027c02",
        "title": "Optimal Competitive-Ratio Control"
      },
      {
        "paperId": "6fafaae137343ce8a99412582092b15467e16efc",
        "title": "Online Adversarial Stabilization of Unknown Networked Systems"
      },
      {
        "paperId": "cf6d125e48ff0813d8b3a35dea6d573de49b5d70",
        "title": "Online Actuator Selection and Controller Design for Linear Quadratic Regulation With an Unknown System Model"
      },
      {
        "paperId": "ca2eca76c734c530a0a1a2c4d0a3388bc45a33ae",
        "title": "Augmented RBMLE-UCB Approach for Adaptive Control of Linear Quadratic Systems"
      },
      {
        "paperId": "7d8b932624e00da64f653e40285459c81fb16e7f",
        "title": "Integration of Adaptive Control and Reinforcement Learning for Real-Time Control and Learning"
      },
      {
        "paperId": "e896882feb3c3d050f1e69abe52c58f4bfa214b6",
        "title": "Regret Bounds for LQ Adaptive Control Under Database Attacks (Extended Version)"
      },
      {
        "paperId": "34299a854ce77fe6722d3444bd906333d6f96d4b",
        "title": "Online Nonstochastic Control Versus Retrospective Cost Adaptive Control"
      },
      {
        "paperId": "b35b6c34785dfb2ae95ff3f6f9037e982e363099",
        "title": "Learning the Uncertainty Sets for Control Dynamics via Set Membership: A Non-Asymptotic Analysis"
      },
      {
        "paperId": "87f9a90a6e124992cb183cc51372040aff478654",
        "title": "Controlgym: Large-Scale Safety-Critical Control Environments for Benchmarking Reinforcement Learning Algorithms"
      },
      {
        "paperId": "56a28863620e2a9e41e62336de0f7d3a1ccea1fa",
        "title": "Online Algorithms and Policies Using Adaptive and Machine Learning Approaches"
      },
      {
        "paperId": "9bc25deea3427bb3a24d3adba4a4fabcf6776444",
        "title": "Bayesian Online Non-Stationary Detection for Robust Reinforcement Learning"
      },
      {
        "paperId": "db42ceedc831ff27794c8989182bb800f0cc7ba5",
        "title": "Transfer Learning, Reinforcement Learning for Adaptive Control Optimization under Distribution Shift"
      }
    ],
    "score": 7.2
  },
  {
    "id": "cf628a42ee56c8f1b858790822a2bc0a61a49110",
    "title": "Deep Black-Box Reinforcement Learning with Movement Primitives",
    "authors": [
      "Fabian Otto",
      "Onur \u00c7elik",
      "Hongyi Zhou",
      "Hanna Ziesche",
      "Ngo Anh Vien",
      "G. Neumann"
    ],
    "year": 2022,
    "citationCount": 21,
    "abstract": "\\Episode-based reinforcement learning (ERL) algorithms treat reinforcement learning (RL) as a black-box optimization problem where we learn to select a parameter vector of a controller, often represented as a movement primitive, for a given task descriptor called a context. ERL offers several distinct benefits in comparison to step-based RL. It generates smooth control trajectories, can handle non-Markovian reward definitions, and the resulting exploration in parameter space is well suited for solving sparse reward settings. Yet, the high dimensionality of the movement primitive parameters has so far hampered the effective use of deep RL methods. In this paper, we present a new algorithm for deep ERL. It is based on differentiable trust region layers, a successful on-policy deep RL algorithm. These layers allow us to specify trust regions for the policy update that are solved exactly for each state using convex optimization, which enables policies learning with the high precision required for the ERL. We compare our ERL algorithm to state-of-the-art step-based algorithms in many complex simulated robotic control tasks. In doing so, we investigate different reward formulations - dense, sparse, and non-Markovian. While step-based algorithms perform well only on dense rewards, ERL performs favorably on sparse and non-Markovian rewards. Moreover, our results show that the sparse and the non-Markovian rewards are also often better suited to define the desired behavior, allowing us to obtain considerably higher quality policies compared to step-based RL.",
    "url": "https://www.semanticscholar.org/paper/cf628a42ee56c8f1b858790822a2bc0a61a49110",
    "pdf_url": "https://arxiv.org/pdf/2210.09622.pdf",
    "venue": "Conference on Robot Learning",
    "publicationDate": "2022-10-18",
    "externalIds": {
      "DBLP": "journals/corr/abs-2210-09622",
      "ArXiv": "2210.09622",
      "DOI": "10.48550/arXiv.2210.09622",
      "CorpusId": 252967797
    },
    "references": [
      {
        "paperId": "a2e6cb2bcb7f89ded3b097005e5077e01004b33a",
        "title": "Specializing Versatile Skill Libraries using Local Mixture of Experts"
      },
      {
        "paperId": "558ca2e8c7eb56edd77a52b084e6cc24dffe5bcd",
        "title": "Deep Reinforcement Learning at the Edge of the Statistical Precipice"
      },
      {
        "paperId": "0efa2efa49a1923954d69eaa9f898af22f63a983",
        "title": "Differentiable Trust Region Layers for Deep Reinforcement Learning"
      },
      {
        "paperId": "5fa8b76256a2125c7a72db372b6e0d6be90d3a54",
        "title": "Neural Dynamic Policies for End-to-End Sensorimotor Learning"
      },
      {
        "paperId": "d415b724fbc35afcc8dd91738123edfa6a5db634",
        "title": "Implementation Matters in Deep Policy Gradients: A Case Study on PPO and TRPO"
      },
      {
        "paperId": "3bc1faf5a59eb124b53888c01f46cc62933e29e3",
        "title": "Learning Via-Point Movement Primitives with Inter- and Extrapolation Capabilities"
      },
      {
        "paperId": "0bc855f84668b35cb65618d996d09f6e434d28c9",
        "title": "Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning"
      },
      {
        "paperId": "e7bb4419a88d15fa8e52c1f4f9cfd65ed58c7379",
        "title": "V-MPO: On-Policy Maximum a Posteriori Policy Optimization for Discrete and Continuous Control"
      },
      {
        "paperId": "2ed89af84aacaadd0d3abed2517a08adf2262793",
        "title": "Projections for Approximate Policy Iteration Algorithms"
      },
      {
        "paperId": "bc0c88b8334ee53978b9986e2396fc1924402364",
        "title": "Contextual Direct Policy Search"
      },
      {
        "paperId": "4debb99c0c63bfaa97dd433bc2828e4dac81c48b",
        "title": "Addressing Function Approximation Error in Actor-Critic Methods"
      },
      {
        "paperId": "19af387c0ee32929b54ff45656129a721a84c192",
        "title": "Back to Basics: Benchmarking Canonical Evolution Strategies for Playing Atari"
      },
      {
        "paperId": "811df72e210e20de99719539505da54762a11c6d",
        "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor"
      },
      {
        "paperId": "561c7fe68668787b084682665a8b36bb0629b613",
        "title": "Contextual Covariance Matrix Adaptation Evolutionary Strategies"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "68061754df2928009fe48983b46eaad4511a4a5c",
        "title": "Model-based contextual policy search for data-efficient generalization of robot skills"
      },
      {
        "paperId": "4ee802a58d32aa049d549d06be440ac947b53987",
        "title": "Evolution Strategies as a Scalable Alternative to Reinforcement Learning"
      },
      {
        "paperId": "9328a4d5ad2d6a7bff5f2d94a8551df318b7f6bb",
        "title": "Policy Search with High-Dimensional Context Variables"
      },
      {
        "paperId": "d20b1b1ebe8f0368f9d7b81cc88ecdd471b01557",
        "title": "Model-Based Relative Entropy Stochastic Search"
      },
      {
        "paperId": "449532187c94af3dd3aa55e16d2c50f7854d2199",
        "title": "Trust Region Policy Optimization"
      },
      {
        "paperId": "8101ec9a994551edfdc7c79ebc89ed939cd07eb3",
        "title": "Hierarchical Relative Entropy Policy Search"
      },
      {
        "paperId": "3a81cfb4a7a880b7cf8979f6067732e961aceb7c",
        "title": "Probabilistic Movement Primitives"
      },
      {
        "paperId": "b6bfae6efa1110a57a4d8362721d152d78aae358",
        "title": "A Survey on Policy Search for Robotics"
      },
      {
        "paperId": "5438f71c01dd713c6f4e05a48c4c8f9c5f82a805",
        "title": "Dynamical Movement Primitives: Learning Attractor Models for Motor Behaviors"
      },
      {
        "paperId": "b7bc7f830f5f936cf9b1dae401dbb96ff968b2f3",
        "title": "Policy Improvement Methods: Between Black-Box Optimization and Episodic Reinforcement Learning"
      },
      {
        "paperId": "8bfda7a7f9c1483e2d51ed13ab9c21dc10392d95",
        "title": "Path Integral Policy Improvement with Covariance Matrix Adaptation"
      },
      {
        "paperId": "2a25f57a5ac627e5f7a2e4502c24bfa51cfdc1cf",
        "title": "2010 Special Issue: Parameter-exploring policy gradients"
      },
      {
        "paperId": "dd5cf95a7af93d2733120d177c593989b19b98fe",
        "title": "Natural Evolution Strategies"
      },
      {
        "paperId": "7ffb463ed53eb7dd53658933d09a3959fe25ca45",
        "title": "The Cross Entropy Method for Fast Policy Search"
      },
      {
        "paperId": "f1bdebedf07fd444628c955568f0d51e1a26835e",
        "title": "Completely Derandomized Self-Adaptation in Evolution Strategies"
      },
      {
        "paperId": "758ba118d7f161fe019d81c4f64a9069c342aa74",
        "title": "Simple random search of static linear policies is competitive for reinforcement learning"
      },
      {
        "paperId": "2065d9eb28be0700a235afb78e4a073845bfb67d",
        "title": "Dynamic Movement Primitives -A Framework for Motor Control in Humans and Humanoid Robotics"
      },
      {
        "paperId": "4c915c1eecb217c123a36dc6d3ce52d12c742614",
        "title": "Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning"
      },
      {
        "paperId": "4d6a87d76ec0c0379f0afbf24f84bba848c6246e",
        "title": "Noname manuscript No. (will be inserted by the editor) Policy Search for Motor Primitives in Robotics"
      }
    ],
    "cited_by": [
      {
        "paperId": "afce1a0a71c37b16b99a73ec3bcd382467543633",
        "title": "MoRe-ERL: Learning Motion Residuals Using Episodic Reinforcement Learning"
      },
      {
        "paperId": "34d8817c85fd7f351e75a6bcbdd851a559877300",
        "title": "Trajectory First: A Curriculum for Discovering Diverse Policies"
      },
      {
        "paperId": "574ec4679c9efdaac629b8fd7f01ee07d78b741d",
        "title": "Chunking the Critic: A Transformer-based Soft Actor-Critic with N-Step Returns"
      },
      {
        "paperId": "27c307a89d3a07563456eba69327371bff2c0c09",
        "title": "Geometry-aware RL for Manipulation of Varying Shapes and Deformable Objects"
      },
      {
        "paperId": "406fb47155714ee76a8750d11f526852c8299043",
        "title": "IRIS: An Immersive Robot Interaction System"
      },
      {
        "paperId": "51bcaf4e9544082893dad1b0ecfa17d7a4632e87",
        "title": "Simulation-Aided Policy Tuning for Black-Box Robot Learning"
      },
      {
        "paperId": "6c282fca36e15f2aeb978c6c6edf54bac43ea5cc",
        "title": "BMP: Bridging the Gap between B-Spline and Movement Primitives"
      },
      {
        "paperId": "a72661ca3c4de880152e69c7e20383645eb20056",
        "title": "TOP-ERL: Transformer-based Off-Policy Episodic Reinforcement Learning"
      },
      {
        "paperId": "d5a2680783cd1bd21557643c026982bbe1f8c38c",
        "title": "Using Goal-Conditioned Reinforcement Learning With Deep Imitation to Control Robot Arm in Flexible Flat Cable Assembly Task"
      },
      {
        "paperId": "89e24f9db3ae7d514d22edc58a0eca3b28b22275",
        "title": "Use the Force, Bot! - Force-Aware ProDMP with Event-Based Replanning"
      },
      {
        "paperId": "2ee493aa7f9939f7d52f7309fd88053b939b87e6",
        "title": "Bridging the gap between Learning-to-plan, Motion Primitives and Safe Reinforcement Learning"
      },
      {
        "paperId": "840061eb2428196f5264896394ee865459d6c9c4",
        "title": "Adaptive collision avoidance decisions in autonomous ship encounter scenarios through rule-guided vision supervised learning"
      },
      {
        "paperId": "59f806ec1f5e25c226088bbf2130f823a9375a6b",
        "title": "Bayesian Optimization for Sample-Efficient Policy Improvement in Robotic Manipulation"
      },
      {
        "paperId": "dd483e4956eafba4bc82af26a29e0075a696f788",
        "title": "Acquiring Diverse Skills using Curriculum Reinforcement Learning with Mixture of Experts"
      },
      {
        "paperId": "ab5b7911932b541da48eff86dc254895b878c60d",
        "title": "Efficient Off-Policy Learning for High-Dimensional Action Spaces"
      },
      {
        "paperId": "134acf45a016fced57cc8f8e171eeb6e0015b0b8",
        "title": "Open the Black Box: Step-based Policy Updates for Temporally-Correlated Episodic Reinforcement Learning"
      },
      {
        "paperId": "b37ea6e8705d01c17eca2c037a7fb2b7b3a221a2",
        "title": "MP3: Movement Primitive-Based (Re-)Planning Policy"
      },
      {
        "paperId": "8d1c713612618317907a3bebb6db768f42aa4f08",
        "title": "ProDMP: A Unified Perspective on Dynamic and Probabilistic Movement Primitives"
      },
      {
        "paperId": "83db0ad783c4dcb73d71b15f02876cc172313fe2",
        "title": "Robust Black-Box Optimization for Stochastic Search and Episodic Reinforcement Learning"
      },
      {
        "paperId": "82de70ec47b89d15f0bcc96e37c4b3c6267ee395",
        "title": "Anticipatory Classifier System With Episode-Based Experience Replay"
      },
      {
        "paperId": "3a3363460f4eab26df397bc25a538b36df38a929",
        "title": "Reinforcement Learning of Diverse Skills using Mixture of Deep Experts"
      }
    ],
    "score": 7.0
  },
  {
    "id": "fbcace16369032bb0292754bd78d03b68b554a95",
    "title": "Stability-Preserving Automatic Tuning of PID Control with Reinforcement Learning",
    "authors": [
      "Ayub I. Lakhani",
      "Myisha A. Chowdhury",
      "Qiugang Lu"
    ],
    "year": 2021,
    "citationCount": 28,
    "abstract": "Proportional-Integral-Derivative (PID) control has been the dominant control strategy in the process industry due to its simplicity in design and effectiveness in controlling a wide range of processes. However, most traditional PID tuning methods rely on trial and error for complex processes where insights about the system are limited and may not yield the optimal PID parameters. To address the issue, this work proposes an automatic PID tuning framework based on reinforcement learning (RL), particularly the deterministic policy gradient (DPG) method. Different from existing studies on using RL for PID tuning, in this work, we explicitly consider the closed-loop stability throughout the RL-based tuning process. In particular, we propose a novel episodic tuning framework that allows for an episodic closed-loop operation under selected PID parameters where the actor and critic networks are updated once at the end of each episode. To ensure the closed-loop stability during the tuning, we initialize the training with a conservative but stable baseline PID controller and the resultant reward is used as a benchmark score. A supervisor mechanism is used to monitor the running reward (e.g., tracking error) at each step in the episode. As soon as the running reward exceeds the benchmark score, the underlying controller is replaced by the baseline controller as an early correction to prevent instability. Moreover, we use layer normalization to standardize the input to each layer in actor and critic networks to overcome the issue of policy saturation at action bounds, to ensure the convergence to the optimum. The developed methods are validated through setpoint tracking experiments on a second-order plus dead-time system. Simulation results show that with our scheme, the closed-loop stability can be maintained throughout RL explorations and the explored PID parameters by the RL agent converge quickly to the optimum. Moreover, through simulation verification, the developed RL-based PID tuning method can adapt the PID parameters to changes in the process model automatically without requiring any knowledge about the underlying operating condition, in contrast to other adaptive methods such as the gain scheduling control.",
    "url": "https://www.semanticscholar.org/paper/fbcace16369032bb0292754bd78d03b68b554a95",
    "pdf_url": "https://arxiv.org/pdf/2112.15187.pdf",
    "venue": "Complex Engineering Systems",
    "publicationDate": "2021-12-30",
    "externalIds": {
      "ArXiv": "2112.15187",
      "DBLP": "journals/corr/abs-2112-15187",
      "DOI": "10.20517/ces.2021.15",
      "CorpusId": 245634826
    },
    "references": [
      {
        "paperId": "74753de00b401df93f6f202a8103d9f376f1157a",
        "title": "A Novel Sample-Efficient Deep Reinforcement Learning with Episodic Policy Transfer for PID-Based Control in Robotic Catheter System"
      },
      {
        "paperId": "e00c995a23c4535edca2bd7b01cf315e6f779d91",
        "title": "Policy Learning with Constraints in Model-free Reinforcement Learning: A Survey"
      },
      {
        "paperId": "ac402f90e2223abae916981a8e27040f3f39a72c",
        "title": "PID Controller Gains Tuning Using Metaheuristic Optimization Methods: A survey"
      },
      {
        "paperId": "74d2757ae2ed66d298b1f86269a939c9c6f03eac",
        "title": "Reinforcement learning-based adaptive PID controller for DPS"
      },
      {
        "paperId": "2e0c640b5cc671b580d5c9f6e9715cf0704fd865",
        "title": "A review of PID control, tuning methods and applications"
      },
      {
        "paperId": "c5ba9196ee30969b16a5f40e4cca89c00165bdd7",
        "title": "Design of a Reinforcement Learning PID controller"
      },
      {
        "paperId": "6592046b956d978db7ace1598c2559f49a43c747",
        "title": "Optimal PID and Antiwindup Control Design as a Reinforcement Learning Problem"
      },
      {
        "paperId": "635680904e5eb1602df20528aad745e4d64b7017",
        "title": "Reinforcement Learning based Design of Linear Fixed Structure Controllers"
      },
      {
        "paperId": "568165abd2ce829cf815910897a250a75b4f8fb9",
        "title": "Multicopter PID Attitude Controller Gain Auto-tuning through Reinforcement Learning Neural Networks"
      },
      {
        "paperId": "a75525a10858bcfa9c67764d9278df9e723ba8e4",
        "title": "Toward Human-in-the-Loop PID Control Based on CACLA Reinforcement Learning"
      },
      {
        "paperId": "917ddb1dcc52a233bebf8af305a13626017a1941",
        "title": "Reinforcement Learning - Overview of recent progress and implications for process control"
      },
      {
        "paperId": "797ddbd5c7fde64c562380c6b59788777a182e32",
        "title": "Deep Reinforcement Learning for Process Control: A Primer for Beginners"
      },
      {
        "paperId": "2c18f0c1f00bf079d536c66df6ab614a0147e5ea",
        "title": "Value constrained model-free continuous control"
      },
      {
        "paperId": "18f447bd8c4f2966207492a885b8ebce87612183",
        "title": "Advanced Methods of PID Controller Tuning for Specified Performance"
      },
      {
        "paperId": "7bd5db9336a5eef98a9c351c2c3539bdd851bfd2",
        "title": "Improve PID controller through reinforcement learning"
      },
      {
        "paperId": "0fe543ca1755ed50e2e110e8827ba68b6f18d68e",
        "title": "An Overview of Dynamic-Linearization-Based Data-Driven Control and Applications"
      },
      {
        "paperId": "97fb4e3d45bb098e27e0071448b6152217bd35a5",
        "title": "Layer Normalization"
      },
      {
        "paperId": "759bbd8dd50cb4790cad7a3bccbdfcbfee5e3e89",
        "title": "Risk-Constrained Reinforcement Learning with Percentile Risk Criteria"
      },
      {
        "paperId": "024006d4c2a89f7acacc6e4438d156525b60a98f",
        "title": "Continuous control with deep reinforcement learning"
      },
      {
        "paperId": "995c5f5e62614fcb4d2796ad2faab969da51713e",
        "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"
      },
      {
        "paperId": "687d0e59d5c35f022ce4638b3e3a6142068efc94",
        "title": "Deterministic Policy Gradient Algorithms"
      },
      {
        "paperId": "24a03f62b87e223a5fbdccdbfbd271609b87965a",
        "title": "Application of reinforcement learning on self-tuning PID controller for soccer robot multi-agent system"
      },
      {
        "paperId": "c3790dc3a8e1d861cc0296110ee22aba7a76b9d1",
        "title": "Tuning fuzzy PD and PI controllers using reinforcement learning."
      },
      {
        "paperId": "03190300bc0f17400e904652726539b5fc8214e0",
        "title": "Adaptive PID Controller based on Reinforcement Learning for Wind Turbine Control"
      },
      {
        "paperId": "adac9aa65c00d582ad62b77fdaebd8f5e8f7060d",
        "title": "A stable self-learning PID control for multivariable time varying systems"
      },
      {
        "paperId": "16df8bf96aeac3aec99ad32ae866ca06e698c929",
        "title": "A Proposal of Adaptive PID Controller Based on Reinforcement Learning"
      },
      {
        "paperId": "da836885804046f4f2e94fba65471f15bf86cafa",
        "title": "Optimization of a Fuzzy PI Controller using Reinforcement Learning"
      },
      {
        "paperId": "ea09a6ebeaa7434054d249cd35bb1614257d6f25",
        "title": "PID tuning rules for SOPDT systems: review and some new results."
      },
      {
        "paperId": "16f6a2b2c24c6fd9278c18553ed6eef90963210b",
        "title": "Applying neural networks to on-line updated PID controllers for nonlinear process control"
      },
      {
        "paperId": "0ec5ebbfa340b9975ac331017526a14d09f83830",
        "title": "A multivariable on-line adaptive PID controller using auto-tuning neurons"
      },
      {
        "paperId": "a20f0ce0616def7cc9a87446c228906cd5da093b",
        "title": "Policy Gradient Methods for Reinforcement Learning with Function Approximation"
      },
      {
        "paperId": "0fe9520bf7141e24027fbbff52e23d74bcd97823",
        "title": "A Stabilizing Switching Scheme for Multi Controller Systems"
      },
      {
        "paperId": "59b50a775542e87f078db35b868ac10ab43d4c75",
        "title": "Learning from delayed rewards"
      },
      {
        "paperId": "03b7e51c52084ac1db5118342a00b5fbcfc587aa",
        "title": "Q-learning"
      },
      {
        "paperId": "3fd2421712871be57cacbfac7fa60560101837bc",
        "title": "Process Dynamics and Control"
      },
      {
        "paperId": "79daa224121f243491a44ce35ce0f6190963e48a",
        "title": "Integrator Windup and How to Avoid It"
      },
      {
        "paperId": "381d192093a3f5d65981125fd6e1a5512b6c5476",
        "title": "Optimum Settings for Automatic Controllers"
      },
      {
        "paperId": "dcd06c9982f3f391d633dd4d0af4a16c66b8f722",
        "title": "Four Types of Controllers"
      },
      {
        "paperId": "8fb6fdfd95b3bd9d08b4f6d121a9477593d7f15d",
        "title": "Control System Design Guide"
      },
      {
        "paperId": "b45dd7bae9ae713c4994b6b78300e12ce177e637",
        "title": "A Pragmatic Approach to Robust Gain Scheduling"
      },
      {
        "paperId": null,
        "title": "O\u21b5-Policy Actor-Critic"
      },
      {
        "paperId": "67fface45f409e5ace486d14212137b02b167b44",
        "title": "Adaptive PID Controller based on Reinforcement Learning for Wind Turbine Control"
      },
      {
        "paperId": "722e0b0a763917b27fc92b4e91c992795aef6908",
        "title": "Nonlinear Systems Third Edition"
      },
      {
        "paperId": null,
        "title": "Optimal design of PID parameters by evolution algorithm"
      },
      {
        "paperId": "3d55f759ca4281d96f906cdbec8123cf50a70004",
        "title": "Off\u2010Policy Actor\u2010Critic\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306b\u3088\u308b\u5f37\u5316\u5b66\u7fd2"
      },
      {
        "paperId": "26470145c04298cf27f912afc4e6ca08515c534a",
        "title": "PID controller tuning for desired closed\u2010loop responses for SI/SO systems"
      },
      {
        "paperId": "97efafdb4a3942ab3efba53ded7413199f79c054",
        "title": "Reinforcement Learning: An Introduction"
      },
      {
        "paperId": "7a09464f26e18a25a948baaa736270bfb84b5e12",
        "title": "On-line Q-learning using connectionist systems"
      },
      {
        "paperId": null,
        "title": "Machine Learning 1992;8:279\u201392"
      },
      {
        "paperId": "937e5c82570d0bb98ab2f3f2d9a7f8c63c780a22",
        "title": "Implementation of Adaptive Controllers Using Digital Signal Processor Chips"
      },
      {
        "paperId": null,
        "title": "King's College, Cambridge United Kingdom\u037e"
      },
      {
        "paperId": "66e97b3e8c46b8b797cc3d85c151c0bd7ea87339",
        "title": "Internal model control: PID controller design"
      }
    ],
    "cited_by": [
      {
        "paperId": "b2196a7448be7fa2246eff5b830de6d634aec54d",
        "title": "Data-driven control law optimization via Kriging surrogate model with adaptive domain reconstruction"
      },
      {
        "paperId": "c46c4f19ce9e0822e2335d635737f08b7e01fb8d",
        "title": "Agentic AI for Real-Time Adaptive PID Control of a Servo Motor"
      },
      {
        "paperId": "1d27917f4006748ae7ffe1eff7c0670ae78e5e09",
        "title": "Design of a PPO-PID Controller Based on Reinforcement Learning"
      },
      {
        "paperId": "650cb0a99dd4c8a76d537111f354078b6b002d1f",
        "title": "Constrained Adaptive Dynamic Programming for Pid Controllers"
      },
      {
        "paperId": "ffe5eeb34e82ddf4e4bf6bbd27c089b8f7bd5944",
        "title": "Reinforcement Learning with Model-Based Static Output Feedback for Vehicle Lateral Control"
      },
      {
        "paperId": "ca1ac71ddb2caed53076acaf745824df8b3d4f75",
        "title": "Real Time Self-Tuning Adaptive Controllers on Temperature Control Loops using Event-based Game Theory"
      },
      {
        "paperId": "63ad899135c27f6d0ffc85c3c206c35ccf88b4c9",
        "title": "Recent Advances in Reinforcement Learning for Chemical Process Control"
      },
      {
        "paperId": "ec08c0527c2aab9953b96221429354282695b44d",
        "title": "Robust Enhanced Auto-Tuning of PID Controllers for Optimal Quality Control of Cement Raw Mix via Neural Networks"
      },
      {
        "paperId": "a1460b18b91ce14b23fb2ae2372aa54c2d853836",
        "title": "Enhancing UAV Altitude Control: PID Tuning with Reinforcement Learning and Genetic Algorithm"
      },
      {
        "paperId": "8b2a6baf552db9a8c56e393fd38cc5ba27f0c13c",
        "title": "Optical stabilization for laser communication satellite systems through proportional-integral-derivative (PID) control and reinforcement learning approach."
      },
      {
        "paperId": "d2cf1465ac2e2174774a633193f68134ef57fb0a",
        "title": "Intelligent control method for automatic voltage regulator: An improved coati optimization algorithm-based strategy"
      },
      {
        "paperId": "d9956ee0a222e9928a97672770af19bb045faf11",
        "title": "PID Control by Safe Reinforcement Learning for Vehicle Lateral Control"
      },
      {
        "paperId": "dc861a4460dc8c3c5dda8aac88509777a05998d2",
        "title": "ADP-based adaptive control of ship course tracking system with prescribed performance"
      },
      {
        "paperId": "390852a7f5af32ff7f8e005873a6ac2e34bbf527",
        "title": "Adaptive Control of Ships\u2019 Oil-Fired Boilers Using Flame Image-Based IMC-PID and Deep Reinforcement Learning"
      },
      {
        "paperId": "d09cb7ca65fb7fb884694a988df3f768749864a6",
        "title": "Control-Informed Reinforcement Learning for Chemical Processes"
      },
      {
        "paperId": "e3934abe919eb156a085c757ea2cef8d1df7a9f6",
        "title": "Development of algorithms for augmenting and replacing conventional process control using reinforcement learning"
      },
      {
        "paperId": "78e49c1ddfcf18b9ed4bfa2b73b967e901109f8e",
        "title": "Machine learning for industrial sensing and control: A survey and practical perspective"
      },
      {
        "paperId": "99d2ad16eb709beec52912a823f79c436338b2f4",
        "title": "Reinforcement Learning Based Autonomous Vehicles Lateral Control"
      },
      {
        "paperId": "1755b7eb7e8d09e7fa359987eaf4a3789ebafd47",
        "title": "Multi-Phase Focused PID Adaptive Tuning with Reinforcement Learning"
      },
      {
        "paperId": "3ec99cf598f3c3e5e8fb9eaab3668c2ea9d5f3ec",
        "title": "MLEAT: An RL-based Approach for Vibration Damping System Control"
      },
      {
        "paperId": "f06065022c01567b447ea2cacfc33e8dd6d459cf",
        "title": "Entropy-maximizing TD3-based reinforcement learning for adaptive PID control of dynamical systems"
      },
      {
        "paperId": "368d6d1a3ef2e6b12afd5bd3065c12ced7890ed1",
        "title": "A Data-Driven Approach for PIDs Tuning in Energy-intensive Industry: Application to Evaporator in Kraft Pulp Mill"
      },
      {
        "paperId": "6740448da1d883e85b09a9deae41e7d07cf38cea",
        "title": "Research and implementation of variable-domain fuzzy PID intelligent control method based on Q-Learning for self-driving in complex scenarios."
      },
      {
        "paperId": "6d6542ce56dd3c4a023f1d458797937e2b551671",
        "title": "A Novel Entropy-Maximizing TD3-based Reinforcement Learning for Automatic PID Tuning"
      },
      {
        "paperId": "e24c2c88979024d45ffe62530c1119efd7692a7c",
        "title": "Meta-reinforcement learning for the tuning of PI controllers: An offline approach"
      },
      {
        "paperId": "6a6f9beb5b7c266e6fd128c3697d2ee3520c42d7",
        "title": "Reinforcement learning based automatic tuning of PID controllers in multivariable grinding mill circuits"
      },
      {
        "paperId": "fc2413e670f81d79cd49601674225d976d89b516",
        "title": "Autonomous PID Tuning: Two-Phase Reinforcement Learning Through Adversarial Imitation Learning Under Imperfect Demonstrations"
      },
      {
        "paperId": "d7b46c4c5611e8579e85eb3f1431a294a3182d34",
        "title": "Assessment of sustainable cement composites with waste material incorporation: Resistance to sulphate attack and property changes"
      }
    ],
    "score": 7.0
  },
  {
    "id": "be33087668f98ac746e72999178d7641d27412f9",
    "title": "A Multi-agent Reinforcement Learning Method for Swarm Robots in Space Collaborative Exploration",
    "authors": [
      "Yixin Huang",
      "Shufan Wu",
      "Z. Mu",
      "Xiangyu Long",
      "Sunhao Chu",
      "G. Zhao"
    ],
    "year": 2020,
    "citationCount": 34,
    "abstract": "Deep-space exploration missions are known as particularly challenging with high risk and cost, as they operate in environments with high uncertainty. The fault of exploration robot can even cause the whole mission to failure. One of the solutions is to use swarm robots to operate missions collaboratively. Compared with a single capable robot, a swarm of less sophisticated robots can cooperate on multiple and complex tasks. Reinforcement learning (RL) has made a variety of progress in multi-agent system autonomous cooperative control domains. In this paper, we construct a collaborative exploration scenario, where a multi-robot system explores an unknown Mars surface. Tasks are assigned to robots by human scientists and each robot takes optimal policies autonomously. The method used to train policies is a multi-agent deep deterministic policy gradient algorithm (MADDPG) and we design an experience sample optimizer to improve this algorithm. The results show that, with the increase of robots and targets number, this method is more efficient than traditional deep RL algorithm in a multi-agent collaborative exploration environment.",
    "url": "https://www.semanticscholar.org/paper/be33087668f98ac746e72999178d7641d27412f9",
    "pdf_url": "https://doi.org/10.1109/ICCAR49639.2020.9107997",
    "venue": "2020 6th International Conference on Control, Automation and Robotics (ICCAR)",
    "publicationDate": "2020-04-01",
    "externalIds": {
      "MAG": "3033770108",
      "DOI": "10.1109/ICCAR49639.2020.9107997",
      "CorpusId": 219547282
    },
    "references": [],
    "cited_by": [
      {
        "paperId": "34d8d0a4e5a556d0d9af518305cdb2ed05dd8206",
        "title": "Optimizing spacecraft collision avoidance maneuvers under risk uncertainty from multiple space debris"
      },
      {
        "paperId": "f2e897e30cdbc798dc359d0794b3c4671bf445d0",
        "title": "CREW-WILDFIRE: Benchmarking Agentic Multi-Agent Collaborations at Scale"
      },
      {
        "paperId": "0baa0132170c89a96d5d5acafc818225fdafdfd4",
        "title": "Autonomous Exploration of Unknown Indoor Environments Using Multi-UAV System"
      },
      {
        "paperId": "efd8746ad7e71cf340e3ade95dfc3766bd6ca1d5",
        "title": "Resilient nonlinear model predictive control for formation-containment of multi-mobile robot systems"
      },
      {
        "paperId": "897a019715b97994cdc97b437551234185a50ba6",
        "title": "Collaborative Swarm Robotics for Sustainable Environment Monitoring and Exploration: Emerging Trends and Research Progress"
      },
      {
        "paperId": "9d1e401831dc5648de567093b161d2d216d7f863",
        "title": "Investigating the Impact of Communication-Induced Action Space on Exploration of Unknown Environments with Decentralized Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "a89714afc20d462663a8559153981436663a0e10",
        "title": "Multi-Task Multi-Agent Reinforcement Learning With Interaction and Task Representations"
      },
      {
        "paperId": "752b65b9450711e9fff73ce2493a19398a565f59",
        "title": "OptiRoute: Operational Routing Algorithm for Swarm Robots"
      },
      {
        "paperId": "e60b0a8840ea195804245d18357d37bc91a69353",
        "title": "A comprehensive survey of space robotic manipulators for on-orbit servicing"
      },
      {
        "paperId": "5f6c4d2c3ea04f400793485660b89a3e30d75a7b",
        "title": "Mitigating Dimensionality in 2D Rectangle Packing Problem under Reinforcement Learning Schema"
      },
      {
        "paperId": "0f7fc61a79fc0a0eb114d9a4d62da2c246826617",
        "title": "STAR: Spatio-Temporal State Compression for Multi-Agent Tasks with Rich Observations"
      },
      {
        "paperId": "4928b41860582d08c906a5a8afd3d06f6f8b47fc",
        "title": "Angle-based Robust Distributed Position Estimation of Multi-agent Systems under Noisy Environment"
      },
      {
        "paperId": "916786f07c64d4b1a87621bfafcc3cd9e6a178f5",
        "title": "Proprioceptive swarms for celestial body exploration"
      },
      {
        "paperId": "29610ea8ed83f61cac02668b69f4a79c338d5496",
        "title": "RLRoverLAB: An Advanced Reinforcement Learning Suite for Planetary Rover Simulation and Training"
      },
      {
        "paperId": "e2a39910649d0ec4bea1fd7693ab1827307e34e7",
        "title": "POWQMIX: Weighted Value Factorization with Potentially Optimal Joint Actions Recognition for Cooperative Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "4b4e20a775e538df321e6449e6b7c40fd913072d",
        "title": "Optimistic Value Instructors for Cooperative Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "41ef64629a91005068b2a6c9039d0132ff0fe809",
        "title": "Cooperative behavior of a heterogeneous robot team for planetary exploration using deep reinforcement learning"
      },
      {
        "paperId": "6a1e717e7dcb626c836c0ec148c80a5023e99de1",
        "title": "Generating collective behavior of a robotic swarm using an attention agent with deep neuroevolution"
      },
      {
        "paperId": "b4b6287080e855b772a3703edcf9fc51a6de8282",
        "title": "Autonomous Swarm Robot Coordination via Mean-Field Control Embedding Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "ba830e846c884e6c6ac4f3ea6c8786920e91ae18",
        "title": "Bio-inspired Decentralized Multi-robot Exploration in Unknown Environments"
      },
      {
        "paperId": "e9f6b92c13a9bf809e9498081ef7aec912ce5a9b",
        "title": "An Expert Data Generation Method for Multi-Agent Cooperative Planning Method"
      },
      {
        "paperId": "13b6c15bdb020d740a2d0c5a0ded8dead6d56123",
        "title": "Reinforcement learning for swarm robotics: An overview of applications, algorithms and simulators"
      },
      {
        "paperId": "166960b521129988f7a86915af01b64432cc545c",
        "title": "An overview of reinforcement learning techniques"
      },
      {
        "paperId": "7aaf8f4a37f4af88e75509f0f01f5aee0ec9d851",
        "title": "Curiosity-driven Exploration in Sparse-reward Multi-agent Reinforcement Learning"
      },
      {
        "paperId": "24e4596103e2468d91402de5bd9dc033238a7a05",
        "title": "Multi-Robot Exploration in Unknown Environments via Multi-Agent Deep Reinforcement Learning"
      },
      {
        "paperId": "aa0f269142df2fa800fbb57a85cb58bffb25ba07",
        "title": "Self-Organizing Internet of Multi-RAT Robotic Things Mesh Network"
      },
      {
        "paperId": "91fbf73cb0cdbd1fb30de97eba778d65a65d37ff",
        "title": "LILAC: Learning a Leader for Cooperative Reinforcement Learning"
      },
      {
        "paperId": "2fc0fcc85753bafa07328127de2002d5e33019e9",
        "title": "Distributed Reinforcement Learning for Robot Teams: a Review"
      },
      {
        "paperId": "b9a8a1af695dc1aea1b3f73fc4539e37b94be79a",
        "title": "A Survey on Automatic Design Methods for Swarm Robotics Systems"
      },
      {
        "paperId": "40e52f5f93721beb53d51da6aada1fb0318594a2",
        "title": "Reward Design for Multi-Agent Reinforcement Learning with a Penalty Based on the Payment Mechanism"
      },
      {
        "paperId": "77907293fceacf26d92ee57129a9f2b9c5e277f9",
        "title": "RL-based Path Planning for Autonomous Aerial Vehicles in Unknown Environments"
      },
      {
        "paperId": "fe39abaab82b612ff041be7e3d84db7645e38c32",
        "title": "Intelligent Multi-Robot System for Collaborative Object Transportation Tasks in Rough Terrains"
      },
      {
        "paperId": "bb87fa619565f539138307619cc3ec1a6acf7573",
        "title": "Using Arti\ufb01cial Intelligence for Space Challenges: A Survey"
      },
      {
        "paperId": "ad8ab14db4206cf7817cfc9a36b270c71fa01a7b",
        "title": "Dynamics of a 9-DOF Heterogeneous Robotic Platform for Spacecraft Motion Emulation"
      }
    ],
    "score": 6.800000000000001
  },
  {
    "id": "cae05340421a18c64ac0897d57bcdcc9a496a3b8",
    "title": "Research on UCAV Maneuvering Decision Method Based on Heuristic Reinforcement Learning",
    "authors": [
      "Wang Yuan",
      "Z. Xiwen",
      "Zhou Rong",
      "Shangqin Tang",
      "Zhou Huan",
      "Dingkai Wei"
    ],
    "year": 2022,
    "citationCount": 20,
    "abstract": "With the rapid development of unmanned combat aerial vehicle (UCAV)-related technologies, UCAVs are playing an increasingly important role in military operations. It has become an inevitable trend in the development of future air combat battlefields that UCAVs complete air combat tasks independently to acquire air superiority. In this paper, the UCAV maneuver decision problem in continuous action space is studied based on the deep reinforcement learning strategy optimization method. The UCAV platform model of continuous action space was established. Focusing on the problem of insufficient exploration ability of Ornstein\u2013Uhlenbeck (OU) exploration strategy in the deep deterministic policy gradient (DDPG) algorithm, a heuristic DDPG algorithm was proposed by introducing heuristic exploration strategy, and then a UCAV air combat maneuver decision method based on a heuristic DDPG algorithm is proposed. The superior performance of the algorithm is verified by comparison with different algorithms in the test environment, and the effectiveness of the decision method is verified by simulation of air combat tasks with different difficulty and attack modes.",
    "url": "https://www.semanticscholar.org/paper/cae05340421a18c64ac0897d57bcdcc9a496a3b8",
    "pdf_url": "https://doi.org/10.1155/2022/1477078",
    "venue": "Computational Intelligence and Neuroscience",
    "publicationDate": "2022-03-03",
    "externalIds": {
      "PubMedCentral": "8913150",
      "DOI": "10.1155/2022/1477078",
      "CorpusId": 247274404,
      "PubMed": "35281202"
    },
    "references": [
      {
        "paperId": "42cbae92b5001a66f84b75ffd9076f785bc99c37",
        "title": "Target tracking strategy using deep deterministic policy gradient"
      },
      {
        "paperId": "3f8390d22b9080b89383516e650e2358b48cee52",
        "title": "Improving Maneuver Strategy in Air Combat by Alternate Freeze Games with a Deep Reinforcement Learning Algorithm"
      },
      {
        "paperId": "aa577f3244ab34670e89f2def9a94a7ef24f6f71",
        "title": "Path Planning for UAV Ground Target Tracking via Deep Reinforcement Learning"
      },
      {
        "paperId": "129983331ca874142a3e8eb2d93d820bdf1f9aca",
        "title": "Deep Reinforcement Learning for Autonomous Driving: A Survey"
      },
      {
        "paperId": "f7fc75a23fb9ea9a464d6b083e5762a3ce0d1f14",
        "title": "Intelligent Decision-Making for 3-Dimensional Dynamic Obstacle Avoidance of UAV Based on Deep Reinforcement Learning"
      },
      {
        "paperId": "91014abf75c4b5a1c886cbd14bd20a1d45933627",
        "title": "UAV Air Combat Autonomous Maneuver Decision Based on DDPG Algorithm"
      },
      {
        "paperId": "91396ce3d4230f95beb4d1850f4d177ef7c5655e",
        "title": "An Application of Continuous Deep Reinforcement Learning Approach to Pursuit-Evasion Differential Game"
      },
      {
        "paperId": "19ddc8219562260ccc517be4c3b92d3648caf9f1",
        "title": "Deep Reinforcement Learning for Autonomous Driving"
      },
      {
        "paperId": "878f135db11defdf3c259ad18276af5e13f5e057",
        "title": "Research on Air Combat Maneuver Decision-Making Method Based on Reinforcement Learning"
      },
      {
        "paperId": "d449080fd546e97f567c7e583a91ed1935c6532d",
        "title": "Autonomous air combat maneuver decision using Bayesian inference and moving horizon optimization"
      },
      {
        "paperId": "c27db32efa8137cbf654902f8f728f338e55cd1c",
        "title": "Mastering the game of Go without human knowledge"
      },
      {
        "paperId": "832110b4a71bbc019f8bf739dc3e99c2c9c4cf3a",
        "title": "A Deep Reinforcement Learning Based Intelligent Decision Method for UCAV Air Combat"
      },
      {
        "paperId": "6127b8dc39497a2388a0fce2512595e0dcb7121b",
        "title": "StarCraft II: A New Challenge for Reinforcement Learning"
      },
      {
        "paperId": "9b8f4b84aa00bce13f7b0657212f43ac622d3818",
        "title": "Robust Adaptive Control for Fractional-order Financial Chaotic Systems with System Uncertainties and External Disturbances"
      },
      {
        "paperId": "8db9df2eadea654f128c1887722c677c708e8a47",
        "title": "Deep Reinforcement Learning framework for Autonomous Driving"
      },
      {
        "paperId": "76dfb1ab698963f3776fe894b3743db4a5419a5f",
        "title": "Multiagent Bidirectionally-Coordinated Nets: Emergence of Human-level Coordination in Learning to Play StarCraft Combat Games"
      },
      {
        "paperId": "755188451c572568e2acbd2ac92a32275ee668d0",
        "title": "Synchronization for fractional-order neural networks with full/under-actuation using fractional-order sliding mode control"
      },
      {
        "paperId": "9f1e9e56d80146766bc2316efbc54d8b770a23df",
        "title": "Deep Reinforcement Learning: An Overview"
      },
      {
        "paperId": "5922beceacad4f234ceee421f17e184e5bfe3007",
        "title": "A precise BP neural network-based online model predictive control strategy for die forging hydraulic press machine"
      },
      {
        "paperId": "024006d4c2a89f7acacc6e4438d156525b60a98f",
        "title": "Continuous control with deep reinforcement learning"
      },
      {
        "paperId": "5a0e8ea43f5004ec716bb328f0b8ff0e6bc57450",
        "title": "Improved shuffled frog leaping algorithm-based BP neural network and its application in bearing early fault diagnosis"
      },
      {
        "paperId": "340f48901f72278f6bf78a04ee5b01df208cc508",
        "title": "Human-level control through deep reinforcement learning"
      },
      {
        "paperId": "df4d4157e866fdee8733f7d97866bcf46d35379e",
        "title": "The overview for UAV Air-Combat Decision method"
      },
      {
        "paperId": "687d0e59d5c35f022ce4638b3e3a6142068efc94",
        "title": "Deterministic Policy Gradient Algorithms"
      },
      {
        "paperId": "2319a491378867c7049b3da055c5df60e1671158",
        "title": "Playing Atari with Deep Reinforcement Learning"
      },
      {
        "paperId": "2b85ce9ea292b96d0b56ef530a5bd2bd3baf783d",
        "title": "Kernel-Based Least Squares Policy Iteration for Reinforcement Learning"
      },
      {
        "paperId": null,
        "title": "OverviewofResearchonModel-freeReinforcementLearning"
      },
      {
        "paperId": "4813c8093c62c559519d20fef2e9cf67ee3d8edf",
        "title": "Influence of unmanned combat aerial vehicle agility on short-range aerial combat effectiveness"
      },
      {
        "paperId": "7d71cedb2a6fbfddc234646b634bdbdd9748e043",
        "title": "Maneuver Decision of UAV in Short-Range Air Combat Based on Deep Reinforcement Learning"
      },
      {
        "paperId": "61fbd028f75909dc6ce0e14038fce8265ae00e8c",
        "title": "Braverman Readings in Machine Learning. Key Ideas from Inception to Current State"
      },
      {
        "paperId": "b5182b8e0e643f0c94fd315a6de4b716f18d61d3",
        "title": "From Reinforcement Learning to Deep Reinforcement Learning: An Overview"
      },
      {
        "paperId": "d6d1a851765dea6d3e5c6b27551a2be55caa8dd7",
        "title": "Tailings saturation line prediction based on genetic algorithm and BP neural network"
      },
      {
        "paperId": "97efafdb4a3942ab3efba53ded7413199f79c054",
        "title": "Reinforcement Learning: An Introduction"
      },
      {
        "paperId": null,
        "title": "Storm Shadow UCAV performance , \u201c Storm Shadow UCAV performance"
      }
    ],
    "cited_by": [
      {
        "paperId": "c7b15e2f508910c5fe52f521373898dcf00a2d31",
        "title": "Intelligent maneuver decision-making for UAVs using the TD3\u2013LSTM reinforcement learning algorithm under uncertain information"
      },
      {
        "paperId": "b0ed25e86ed3aa9a63f2fbc7ebd5c58d109b4cd8",
        "title": "Enhancing Aerial Combat Tactics through Hierarchical Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "a1da5602dff9a7ae9f8ab96d3a8c6b7ef55ef5a6",
        "title": "A Context-Aware Feature Fusion Method for Multi-UAV Cooperative Air Combat"
      },
      {
        "paperId": "368ed4dba00964efd9a8bd055b41c28e434212e6",
        "title": "Decision-making and confrontation in close-range air combat based on reinforcement learning"
      },
      {
        "paperId": "1a9ded8f7d2efa8c61e5ef3fd02318b4624951d0",
        "title": "Multi-Agent Air Combat Decision-making Based on Battlefield Attention Information"
      },
      {
        "paperId": "03d7f32492cd02449f36f7061abd335a6000c9b1",
        "title": "Deep Reinforcement-Learning-Based Air-Combat-Maneuver Generation Framework"
      },
      {
        "paperId": "f5bb099c78c27464a5868b772ef662a26dec06e7",
        "title": "Cross coordination of behavior clone and reinforcement learning for autonomous within-visual-range air combat"
      },
      {
        "paperId": "e73456b3b704fecc8a54d31f373bd5d03d2e9c00",
        "title": "Reinforcement Learning Methods for Fixed-Wing Aircraft Control"
      },
      {
        "paperId": "32d37a3340fd5d97bb85720ad5bdeaaee3fe76da",
        "title": "UCAV autonomous maneuvering decision based on curriculum learning mechanism training"
      },
      {
        "paperId": "45d1178aaf543da56972b38a52b973a67448f252",
        "title": "Memory-Enhanced Twin Delayed Deep Deterministic Policy Gradient (ME-TD3)-Based Unmanned Combat Aerial Vehicle Trajectory Planning for Avoiding Radar Detection Threats in Dynamic and Unknown Environments"
      },
      {
        "paperId": "19f1383e658e7420c01b1d8ffa3b494e511a187c",
        "title": "Multi-intent autonomous decision-making for air combat with deep reinforcement learning"
      },
      {
        "paperId": "97b37027426cc75408f01471680e71ee678139d1",
        "title": "Improved 1vs1 Air Combat Model With Self-Play Soft Actor-Critic and Sparse Rewards"
      },
      {
        "paperId": "f43eb5d7fc76ccb9466348fe1d5ddda7fec90667",
        "title": "Hierarchical Multi-Agent Reinforcement Learning for Air Combat Maneuvering"
      },
      {
        "paperId": "940379fe3cb3830cd0bf32bc19556ef55830c977",
        "title": "Maneuver Decision-Making Through Proximal Policy Optimization And Monte Carlo Tree Search"
      },
      {
        "paperId": "9c49ef84b04053223595012b67e4bc8287642d4a",
        "title": "Short-range air combat maneuver decision of UAV swarm based on multi-agent Transformer introducing virtual objects"
      },
      {
        "paperId": "c13a14ce7312c8f93e904b895fe6247ef3bcb643",
        "title": "Maneuver Decision-Making Through Automatic Curriculum Reinforcement Learning Without Handcrafted Reward functions"
      },
      {
        "paperId": "718b07d9b949b5d2c6a1fa3517f35616657e36df",
        "title": "Maneuver Decision-Making For Autonomous Air Combat Through Curriculum Learning And Reinforcement Learning With Sparse Rewards"
      },
      {
        "paperId": "eea6b8c0d500df2a8fce7f23d5c6988de6b9749d",
        "title": "A Hierarchical Deep Reinforcement Learning Framework for 6-DOF UCAV Air-to-Air Combat"
      },
      {
        "paperId": "147aa8f0b6d64e9a35019075ca07a8ea19a42a0e",
        "title": "Network Architecture for Optimizing Deep Deterministic Policy Gradient Algorithms"
      },
      {
        "paperId": "1f6ecae830f7d55924d7b2bdda1256dbde15876f",
        "title": "Autonomous Air Combat Maneuvering Decision Method of UCAV Based on LSHADE-TSO-MPC under Enemy Trajectory Prediction"
      }
    ],
    "score": 6.666666666666666
  },
  {
    "id": "cac7f83769836707b02adadb0cda8c791ca23c92",
    "title": "Deep Reinforcement Learning-Based Driving Strategy for Avoidance of Chain Collisions and Its Safety Efficiency Analysis in Autonomous Vehicles",
    "authors": [
      "Abu Jafar Md. Muzahid",
      "Syafiq Fauzi Bin Kamarulzaman",
      "Md. Arafatur Rahman",
      "A. Alenezi"
    ],
    "year": 2022,
    "citationCount": 20,
    "abstract": "Vehicle control in autonomous traffic flow is often handled using the best decision-making reinforcement learning methods. However, unexpected critical situations make the collisions more severe and, consequently, the chain collisions. In this work, we first review the leading causes of chain collisions and their subsequent chain events, which might provide an indication of how to prevent and mitigate the crash severity of chain collisions. Then, we consider the problem of chain collision avoidance as a Markov Decision Process problem in order to propose a reinforcement learning-based decision-making strategy and analyse the safety efficiency of existing methods in driving security. To address this, A reward function is being developed to deal with the challenge of multiple vehicle collision avoidance. A perception network structure based on formation and on actor-critic methodologies is employed to enhance the decision-making process. Finally, in the safety efficiency analysis phase, we investigated the safety efficiency performance of the agent vehicle in both single-agent and multi-agent autonomous driving environments. Three state-of-the-art contemporary actor-critic algorithms are used to create an extensive simulation in Unity3D. Moreover, to demonstrate the accuracy of the safety efficiency analysis, multiple training runs of the neural networks in respect of training performance, speed of training, success rate, and stability of rewards with a trade-off between exploitation and exploration during training are presented. Two aspects (single-agent and multi-agent) have assessed the efficiency of algorithms. Every aspect has been analyzed regarding the traffic flows: (1) the controlling efficiency of unexpected traffic situations by the sudden slowdown, (2) abrupt lane change, and (3) smoothly reaching the destination. All the findings of the analysis are intended to shed insight on the benefits of a greater, more reliable autonomous traffic set-up for academics and policymakers, and also to pave the way for the actual carry-out of a driver-less traffic world.",
    "url": "https://www.semanticscholar.org/paper/cac7f83769836707b02adadb0cda8c791ca23c92",
    "pdf_url": "https://doi.org/10.1109/ACCESS.2022.3167812",
    "venue": "IEEE Access",
    "publicationDate": "",
    "externalIds": {
      "DBLP": "journals/access/MuzahidKRA22",
      "DOI": "10.1109/ACCESS.2022.3167812",
      "CorpusId": 248254574
    },
    "references": [
      {
        "paperId": "fb150d1f0c3a6298c87f85748b4e2f9022bee48f",
        "title": "Safe and Efficient Cooperative Platooning"
      },
      {
        "paperId": "f9b1ce056e6d88ec5ab127a3db1f1c811de3dafd",
        "title": "Optimal Safety Planning and Driving Decision-Making for Multiple Autonomous Vehicles: A Learning Based Approach"
      },
      {
        "paperId": "a04bf0654875ca1e3474b22a4ba9f014fd3a6ed5",
        "title": "A Conceptual Anonymity Model to Ensure Privacy for Sensitive Network Data"
      },
      {
        "paperId": "01667264a5f7e9371b98265e397e3edd070315c4",
        "title": "Illumination and Temperature-Aware Multispectral Networks for Edge-Computing-Enabled Pedestrian Detection"
      },
      {
        "paperId": "7cd70877f166948843cd837ad5445edb6934a1c4",
        "title": "Automated Vehicles Sharing the Road: Surveying Detection and Localization of Pedalcyclists"
      },
      {
        "paperId": "679e1797f5e2c2da5ac90aaddbe3d01b8ca50c30",
        "title": "Comparison of PPO and SAC Algorithms Towards Decision Making Strategies for Collision Avoidance Among Multiple Autonomous Vehicles"
      },
      {
        "paperId": "7c0b2e9e99f7bd787f6ddd0e0cb5eb31b81129e2",
        "title": "Vehicle Route Tracking System based on Vehicle Registration Number Recognition using Template Matching Algorithm"
      },
      {
        "paperId": "e34aadd1ad521bed42c842483363a2251e5989a1",
        "title": "A large multi-group decision-making technique for prioritizing the big data-driven circular economy practices in the automobile component manufacturing industry"
      },
      {
        "paperId": "3f1453e6c648f900005fa44266f9da42b8220256",
        "title": "Sensing system of environmental perception technologies for driverless vehicle: A review of state of the art and challenges"
      },
      {
        "paperId": "e17dc858d9ff54c3e8689fb481e2e5795c9d9eda",
        "title": "Public acceptance and perception of autonomous vehicles: a comprehensive review"
      },
      {
        "paperId": "72560e523f6a8ec30c9840284c0fbc592e746f0a",
        "title": "Visual Human\u2013Computer Interactions for Intelligent Vehicles and Intelligent Transportation Systems: The State of the Art and Future Directions"
      },
      {
        "paperId": "073068995d1c73d38a5072aa18a63dfd7fce3fe4",
        "title": "Learning-Based Conceptual framework for Threat Assessment of Multiple Vehicle Collision in Autonomous Driving"
      },
      {
        "paperId": "23c5db7262ba172a2e7c19838dd66d05381238af",
        "title": "Examination of trust and sustainability concerns in autonomous vehicle adoption"
      },
      {
        "paperId": "0fa9d907e2296298a3338311da3ffc62dde710fc",
        "title": "Vision-based robust control framework based on deep reinforcement learning applied to autonomous ground vehicles"
      },
      {
        "paperId": "ec549287e13d82e1985ab436dd9234d54a4b1f4a",
        "title": "A Review of Motion Planning for Highway Autonomous Driving"
      },
      {
        "paperId": "92eaf3a4ab31783eac3c9134452d38e7b55231c4",
        "title": "Comprehensive safety assessment in mixed fleets with connected and automated vehicles: A crash severity and rate evaluation of conventional vehicles."
      },
      {
        "paperId": "5a6a04258020201be056966d110afb77a184f339",
        "title": "Interval Analysis Technique for Versatile and Parallel Multi-Agent Collision Detection and Avoidance"
      },
      {
        "paperId": "5c0db6dad1e6f130445a0b59924c5100ace29275",
        "title": "A new safe lane-change trajectory model and collision avoidance control method for automatic driving vehicles"
      },
      {
        "paperId": "73d109b605ca547d968f097cfef0a1c9d424a781",
        "title": "Controllability Analysis and Optimal Control of Mixed Traffic Flow With Human-Driven and Autonomous Vehicles"
      },
      {
        "paperId": "4883129ec2a158862ea1f8be41b770cffa29bf10",
        "title": "Exploring the who, what, when, where, and why of automated vehicle disengagements."
      },
      {
        "paperId": "3862d7f6cf8086dad1486ca0067f6cd2d91f1eee",
        "title": "Cooperative Multi-Vehicle Behavior Coordination for Autonomous Driving"
      },
      {
        "paperId": "49983cb6cf8af8078503fe4300f8b0ed30015532",
        "title": "Collision Avoidance: A Literature Review on Threat-Assessment Techniques"
      },
      {
        "paperId": "876e2a59ab6b416b47014897048f3675662ed58d",
        "title": "On the potential for one-way electric vehicle car-sharing in future mobility systems"
      },
      {
        "paperId": "bca2130a769a5268b36af446a2d90da4d9f91d96",
        "title": "\u201cWhat Makes a Cooperative Driver?\u201d Identifying parameters of implicit and explicit forms of communication in a lane change scenario"
      },
      {
        "paperId": "a84705ed9a73a0dd0cdd23127d6a2bb18a87d200",
        "title": "Methodological evolution and frontiers of identifying, modeling and preventing secondary crashes on highways."
      },
      {
        "paperId": "2cd88da9f417b3dba740401798ce62ef62c156ad",
        "title": "Piecewise Trajectory Replanner for Highway Collision Avoidance Systems with Safe-Distance Based Threat Assessment Strategy and Nonlinear Model Predictive Control"
      },
      {
        "paperId": "9bf2d6526b480f95fed9b0e0a1125cf07d11e01d",
        "title": "Autonomous agents modelling other agents: A comprehensive survey and open problems"
      },
      {
        "paperId": "76dceabf5f6d3dcedc89746046b42009375978bc",
        "title": "Decentralized non-communicating multiagent collision avoidance with deep reinforcement learning"
      },
      {
        "paperId": "fa01afa262aa382d44156142969ff4e51a7baff3",
        "title": "An Iterative Approach for Collision Free Routing and Scheduling in Multirobot Stations"
      },
      {
        "paperId": "2f5480656fe4a0ea7427913829748dab9e55a84c",
        "title": "Multi-Robot Graph Exploration and Map Building with Collision Avoidance: A Decentralized Approach"
      },
      {
        "paperId": "15091e4116796592e8c6dad0e735e7404afde054",
        "title": "Effects of intelligent control mechanism on multiple-vehicle collision under emergency"
      },
      {
        "paperId": "ce7f08997b8044cb1febb2749f89bef0935e5064",
        "title": "Wireless Vehicular Networks for Car Collision Avoidance"
      },
      {
        "paperId": "7bda1b8e66eaf46a6b1b5de80737adc0544a52d4",
        "title": "Multiple-vehicle collision in traffic flow by a sudden slowdown"
      },
      {
        "paperId": "221c08207ec16116c221894e89e60d358cf3242e",
        "title": "Multiple-vehicle collision induced by a sudden stop in traffic flow"
      },
      {
        "paperId": "90c7d8e6943b2b7134e299ea8e078cf7d0e7d4cd",
        "title": "Deep reinforcement learning based trading agents: Risk curiosity driven learning for financial rules-based policy"
      },
      {
        "paperId": "52add89fc086a9e5315d63a1d9403b23cb7d1a4a",
        "title": "A Framework of IoT-Enabled Vehicular Noise Intensity Monitoring System for Smart City"
      },
      {
        "paperId": "52a2dab40ea1af603c8c885ad4484283a4e85ae1",
        "title": "Optimal Management strategies to solve issues of grid having Electric Vehicles (EV): A review"
      },
      {
        "paperId": "b63869aa27f8c93a0e919ba42af72a8a3d07d261",
        "title": "Adaptive governance of autonomous vehicles: Accelerating the adoption of disruptive technologies in Singapore"
      },
      {
        "paperId": null,
        "title": "Saudi Arabia, the M.S. degree in electrical engineering from the Royal Institute of Technology KTH, Sweden, and the Ph.D. degree in electrical engineering from the New Jersey Institute of Technology"
      },
      {
        "paperId": "cf66e6d557de2e5adaeaf1f01f917f596c98a80a",
        "title": "Effect of velocity-dependent friction on multiple-vehicle collisions in traffic flow"
      },
      {
        "paperId": "940e1b4da165c3200ccc4ace5a44c48e325e03cb",
        "title": "PRACTICAL STRING STABILITY FOR LONGITUDINAL CONTROL OF AUTOMATED VEHICLES"
      },
      {
        "paperId": null,
        "title": "the Ph.D. degree in electronic and telecommunications engineering from the University of Naples Federico II, Naples, Italy"
      }
    ],
    "cited_by": [
      {
        "paperId": "3a984457ffd46fcc9bfa93699be6fc125799aa68",
        "title": "Lane merging in autonomous vehicle urban driving using reinforcement learning models"
      },
      {
        "paperId": "66b81f73518964fb2786606e63585cd1ae32ef6a",
        "title": "A Novel Clustering-Based Anomaly Detection Approach Using a Game Engine for Road Safety"
      },
      {
        "paperId": "a2f8f94cf002887e197101b2374e2a5f6775964b",
        "title": "Hierarchical Deep Reinforcement Learning-Based Path Planning with Underlying High-Order Control Lyapunov Function\u2014Control Barrier Function\u2014Quadratic Programming Collision Avoidance Path Tracking Control of Lane-Changing Maneuvers for Autonomous Vehicles"
      },
      {
        "paperId": "f5f86e1370044d6be5bd076aa7e77865a0a3fbd7",
        "title": "Improving the Robustness of Autonomous Vehicles Through Reinforcement Learning with Abnormally Behaving Traffic Actors"
      },
      {
        "paperId": "88db6d5152dcf12683059060de650f02b46fe040",
        "title": "Impact of Data Balancing and Feature Engineering on Accident Severity Models"
      },
      {
        "paperId": "60790eaf362299a31642de22d6d1cf05c3b3e33b",
        "title": "A Comprehensive Literature Review on Modular Approaches to Autonomous Driving: Deep Learning for Road and Racing Scenarios"
      },
      {
        "paperId": "8240aad7f4995ef0761bd832622722ffd7242694",
        "title": "Deep Reinforcement Learning In Autonomous Cars To Preventing Road Accident"
      },
      {
        "paperId": "263c8a7f561c145a5f3d38354d7ab49d598dcc68",
        "title": "Exploration Techniques in Reinforcement Learning for Autonomous Vehicles"
      },
      {
        "paperId": "86275372d036f87091991622bbf2c532c4cf734f",
        "title": "Data Privacy and Security in Autonomous Connected Vehicles in Smart City Environment"
      },
      {
        "paperId": "c922eede40c9adc97f3de84fff2533c41083f5ee",
        "title": "Comparison between Genetic Algorithms of Proportional\u2013Integral\u2013Derivative and Linear Quadratic Regulator Controllers, and Fuzzy Logic Controllers for Cruise Control System"
      },
      {
        "paperId": "e05fe018a9f5c81aa32ebd4947c0a32a171069f9",
        "title": "Human-Aligned Longitudinal Control for Occluded Pedestrian Crossing With Visual Attention"
      },
      {
        "paperId": "6c67d122e9ed8ecec2f93467796006d8ac735e69",
        "title": "Machine Learning in Metaverse Security: Current Solutions and Future Challenges"
      },
      {
        "paperId": "d94b63552852844bbe304b8c94053c5462745282",
        "title": "Implementing Deep Reinforcement Learning (DRL)-based Driving Styles for Non-Player Vehicles"
      },
      {
        "paperId": "01ab36a5246ff6d67c5aa11f49ef48a7f29dd18e",
        "title": "Chain Collision Avoidance Using Vehicle-to-Everything (V2X) Communication"
      },
      {
        "paperId": "19aedae3685f1d302bebfd12258b17b242b0137b",
        "title": "A Planning Control Strategy Based on Dynamic Safer Buffer to Avoid Traffic Collisions in an Emergency for CAVs at Nonsignalized Intersections"
      },
      {
        "paperId": "948fcc0d6c062ee2f61522231fc986485c629eba",
        "title": "A IoT Edge Advanced VANET Technique for Vehicle Communication and Improve Safety in Hill Station Critical Scenario"
      },
      {
        "paperId": "1c047cfbfe4ff02338569ccff780cb312ae2b4a9",
        "title": "An Improved Adaptive Service Function Chain Mapping Method Based on Deep Reinforcement Learning"
      },
      {
        "paperId": "baf4f1640c867ca453e7026eab714950a469686a",
        "title": "A Review on Cooperative Perception and Control Supported Infrastructure-Vehicle System"
      },
      {
        "paperId": "6cd324c658088c29886fb66848086c644766ba7d",
        "title": "Multi-Agent Collision Avoidance System Based on Centralization and Decentralization Control for UAV Applications"
      },
      {
        "paperId": "6323a16be9ce8f0d62f48fadf529a8434468f153",
        "title": "Improved Collision Risk Assessment for Autonomous Vehicles at on-Ramp Merging Areas"
      }
    ],
    "score": 6.666666666666666
  },
  {
    "id": "f4712cdb025ba4ab879bb47d5cf693a4923f1532",
    "title": "Safe Building HVAC Control via Batch Reinforcement Learning",
    "authors": [
      "Chi Zhang",
      "S. Kuppannagari",
      "V. Prasanna"
    ],
    "year": 2022,
    "citationCount": 20,
    "abstract": "In this paper, we study safe building HVAC control via batch reinforcement learning. Random exploration in building HVAC control is infeasible due to safety considerations. However, diverse states are necessary for RL algorithms to learn useful policies. To enable <italic>safety</italic> during exploration, we propose guided exploration by adding a Gaussian noise to a hand-crafted rule-based controller. Adjusting the variance of the noise provides a tradeoff between the <italic>diversity</italic> of the dataset and the <italic>safety</italic>. We apply Conservative Q Learning (CQL) to learn a policy. CQL ensures that the trained policy stays within the policy distribution used to collect the dataset, thereby guarantees safety at deployment. To select the optimal policy during the offline training, we apply model-based performance evaluation. We use the widely adopted CityLearn testbed to evaluate the performance of our proposed method. Compared with a rule-based controller, our approach obtains <inline-formula><tex-math notation=\"LaTeX\">$12\\%\\sim 35\\%$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>12</mml:mn><mml:mo>%</mml:mo><mml:mo>\u223c</mml:mo><mml:mn>35</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"zhang-ieq1-3164084.gif\"/></alternatives></inline-formula> reduction in ramping, <inline-formula><tex-math notation=\"LaTeX\">$3\\%\\sim 10\\%$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>3</mml:mn><mml:mo>%</mml:mo><mml:mo>\u223c</mml:mo><mml:mn>10</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"zhang-ieq2-3164084.gif\"/></alternatives></inline-formula> reduction in 1-load factor, <inline-formula><tex-math notation=\"LaTeX\">$3\\%\\sim 8\\%$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>3</mml:mn><mml:mo>%</mml:mo><mml:mo>\u223c</mml:mo><mml:mn>8</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"zhang-ieq3-3164084.gif\"/></alternatives></inline-formula> reduction in daily peak at deployment with less than <inline-formula><tex-math notation=\"LaTeX\">$10\\%$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>10</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"zhang-ieq4-3164084.gif\"/></alternatives></inline-formula> performance degradation during the exploration. On the contrary, the performance degradation of the state-of-the-art online reinforcement learning algorithm during exploration is around <inline-formula><tex-math notation=\"LaTeX\">$8\\%\\sim 18\\%$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>8</mml:mn><mml:mo>%</mml:mo><mml:mo>\u223c</mml:mo><mml:mn>18</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"zhang-ieq5-3164084.gif\"/></alternatives></inline-formula>. It also fails to surpass the performance of the rule-based controller at deployment.",
    "url": "https://www.semanticscholar.org/paper/f4712cdb025ba4ab879bb47d5cf693a4923f1532",
    "pdf_url": "https://doi.org/10.1109/TSUSC.2022.3164084",
    "venue": "IEEE Transactions on Sustainable Computing",
    "publicationDate": "2022-10-01",
    "externalIds": {
      "DBLP": "journals/tsusc/ZhangKP22",
      "DOI": "10.1109/TSUSC.2022.3164084",
      "CorpusId": 247917179
    },
    "references": [],
    "cited_by": [
      {
        "paperId": "b11a65367617fcaafd1041c9c66f8ce4b079afeb",
        "title": "Comparative Field Deployment of Reinforcement Learning and Model Predictive Control for Residential HVAC"
      },
      {
        "paperId": "0858a17f0a7b9c465c0b8a827815f69fe269a822",
        "title": "Adaptive Transfer Reinforcement Learning (TRL) for Cooling Water Systems with Uniform Agent Design and Multi-Agent Coordination"
      },
      {
        "paperId": "093ba5ff1bab1a686ceec1d3bef90db64c2d4e66",
        "title": "Online Comfort-Constrained HVAC Control via Feature Transfer"
      },
      {
        "paperId": "d30da52754983f8ef1af18b5f2c0e0381f03b03a",
        "title": "A critical review of safe reinforcement learning strategies in power and energy systems"
      },
      {
        "paperId": "1fd50e27865d4a4e7c187454df9128769a4e65cf",
        "title": "Real-World Implementation of Offline Model-Free Reinforcement Learning for Thermostat Control"
      },
      {
        "paperId": "df6c20aee8d31bbfd024d2051e728990bd73f2c1",
        "title": "Adaptive Policy Regularization for Offline-to-Online Reinforcement Learning in HVAC Control"
      },
      {
        "paperId": "11d78f17ce97ea1b6a9095e534c060799b94ead3",
        "title": "ORCHID: Offline RL for Control of HVAC in Buildings using Historical and Low-Fidelity Simulation Data"
      },
      {
        "paperId": "921d3c2ca15f314d32192034906991997b3a3846",
        "title": "Prospects and Challenges of Reinforcement Learning- Based HVAC Control"
      },
      {
        "paperId": "61bd362e7be9df55c1af3ff58d08b5d80539b780",
        "title": "A Critical Review of Safe Reinforcement Learning Techniques in Smart Grid Applications"
      },
      {
        "paperId": "ef897db392741fc3ae8aae2ce01e820e02713548",
        "title": "Experimental evaluation of offline reinforcement learning for HVAC control in buildings"
      },
      {
        "paperId": "258b9e7e8fb8e68b3d51391620d5c4f56c14d57f",
        "title": "A Safe and Data-Efficient Model-Based Reinforcement Learning System for HVAC Control"
      },
      {
        "paperId": "9a0d705852c8aa0b07277d9b1235d49262081865",
        "title": "A Review of Safe Reinforcement Learning Methods for Modern Power Systems"
      },
      {
        "paperId": "112757427f262ca73bc68753ef4942c3c4d37b77",
        "title": "Energy Allocation for Vehicle-to-Grid Settings: A Low-Cost Proposal Combining DRL and VNE"
      },
      {
        "paperId": "d9b25c8fc0ddd3c0f2f110c1fe4e173d9880a87f",
        "title": "CLUE: Safe Model-Based RL HVAC Control Using Epistemic Uncertainty Estimation"
      },
      {
        "paperId": "ee03bdb42392b61dc2236e02a228332f22b21b03",
        "title": "Fast Human-in-the-Loop Control for HVAC Systems via Meta-Learning and Model-Based Offline Reinforcement Learning"
      },
      {
        "paperId": "d16da2ad581945602b07c004039a1c717d1b1432",
        "title": "Rule-based Policy Regularization for Reinforcement Learning-based Building Control"
      },
      {
        "paperId": "0c3a1cb83c6f9b1075a7fafae14e953164f05fe4",
        "title": "A laboratory test of an Offline-trained Multi-Agent Reinforcement Learning Algorithm for Heating Systems"
      },
      {
        "paperId": "070a6ab8a926f5148f2b41fad79174ce686c8e96",
        "title": "Reinforcement Learning-Based Intelligent Control Strategies for Optimal Power Management in Advanced Power Distribution Systems: A Survey"
      },
      {
        "paperId": "289c7bd0420a88bbff6b99826c2c03f15d9ba750",
        "title": "B2RL: an open-source dataset for building batch reinforcement learning"
      },
      {
        "paperId": "48f05f21ddcf5d9039af909bbab5871f5b35caf6",
        "title": "Diversity for transfer in learning-based control of buildings"
      }
    ],
    "score": 6.666666666666666
  },
  {
    "id": "b1be7ce8c639291adf7663535f0451f9ac03ed55",
    "title": "Exploring Reward Strategies for Wind Turbine Pitch Control by Reinforcement Learning",
    "authors": [
      "J. E. Sierra-Garc\u00eda",
      "Matilde Santos"
    ],
    "year": 2020,
    "citationCount": 33,
    "abstract": "In this work, a pitch controller of a wind turbine (WT) inspired by reinforcement learning (RL) is designed and implemented. The control system consists of a state estimator, a reward strategy, a policy table, and a policy update algorithm. Novel reward strategies related to the energy deviation from the rated power are defined. They are designed to improve the efficiency of the WT. Two new categories of reward strategies are proposed: \u201conly positive\u201d (O-P) and \u201cpositive-negative\u201d (P-N) rewards. The relationship of these categories with the exploration-exploitation dilemma, the use of \u03f5-greedy methods and the learning convergence are also introduced and linked to the WT control problem. In addition, an extensive analysis of the influence of the different rewards in the controller performance and in the learning speed is carried out. The controller is compared with a proportional-integral-derivative (PID) regulator for the same small wind turbine, obtaining better results. The simulations show how the P-N rewards improve the performance of the controller, stabilize the output power around the rated power, and reduce the error over time.",
    "url": "https://www.semanticscholar.org/paper/b1be7ce8c639291adf7663535f0451f9ac03ed55",
    "pdf_url": "https://doi.org/10.3390/app10217462",
    "venue": "Applied Sciences",
    "publicationDate": "2020-10-23",
    "externalIds": {
      "MAG": "3093664514",
      "DOI": "10.3390/app10217462",
      "CorpusId": 226343849
    },
    "references": [
      {
        "paperId": "daf1bbb9904419c0e53bbf4e08cd9bc026aa61dc",
        "title": "Our world in data"
      },
      {
        "paperId": "f7c43a4aea416144f1fac628a198fe738e7e177e",
        "title": "Performance Analysis of a Wind Turbine Pitch Neurocontroller with Unsupervised Learning"
      },
      {
        "paperId": "305ccc669e733381230b930f520fdb53016b0081",
        "title": "Improving response of wind turbines by pitch angle controller based on gain-scheduled recurrent ANFIS type 2 with passive reinforcement learning"
      },
      {
        "paperId": "458c5e3e2421488ff5d0491287a842904a91eb71",
        "title": "Hierarchical Pitch Control for Small Wind Turbines Based on Fuzzy Logic and Anticipated Wind Speed Measurement"
      },
      {
        "paperId": "37593c3803fce4a197ada24180dfe56cd62ea50d",
        "title": "Reinforcement learning for building controls: The opportunities and challenges"
      },
      {
        "paperId": "68bf215912080366ba75b85e08d37e8fe9fb4342",
        "title": "Design of a Robust Adaptive Controller for the Pitch and Torque Control of Wind Turbines"
      },
      {
        "paperId": "3909619aa0e2103818e282d9ebe42985ff5e939b",
        "title": "Performance enhancement of the artificial neural network\u2013based reinforcement learning for wind turbine yaw control"
      },
      {
        "paperId": "0d780e3864fbb2f43bd6203087b9efcea0a23f24",
        "title": "Cooperative Wind Farm Control With Deep Reinforcement Learning and Knowledge-Assisted Learning"
      },
      {
        "paperId": "b72a2132367398132272c8672296727e472f8a6b",
        "title": "Anomaly Detection of Wind Turbines Based on Deep Small-World Neural Network"
      },
      {
        "paperId": "09163da1f795b11edab5ac21ca077cf490c16bc9",
        "title": "Reinforcement-Based Robust Variable Pitch Control of Wind Turbines"
      },
      {
        "paperId": "2cd2d4ab5a08774588235a289ff77370060e6c9b",
        "title": "Adaptive robust integral sliding mode pitch angle control of an electro-hydraulic servo pitch system for wind turbine"
      },
      {
        "paperId": "3b74f42c36ffe2e913fc9f742be4227f508d21fa",
        "title": "Modelado y control de turbinas e\u00f3licas marinas flotantes"
      },
      {
        "paperId": "c178926001f0e9e9c3917430c931353379e86a93",
        "title": "Hierarchical Fault-Tolerant Control using Model Predictive Control for Wind Turbine Pitch Actuator Faults"
      },
      {
        "paperId": "f99b9d835738c6670417f3b73f28814f2d0a4282",
        "title": "A PI-Type Sliding Mode Controller Design for PMSG-Based Wind Turbine"
      },
      {
        "paperId": "8911a7b8acba5ba80ee6463bdf5a264230e61793",
        "title": "Intelligent Control of a Wind Turbine based on Reinforcement Learning"
      },
      {
        "paperId": "dff73de383b4d8fa65c71067af5ffdf06dc89e9c",
        "title": "A Proportional Plus a Hysteretic Term Control Design: A Throttle Experimental Emulation to Wind Turbines Pitch Control"
      },
      {
        "paperId": "756aadfefab64265e74a0a8c9ad7911b3c3b41cb",
        "title": "Super-Twisting Sliding Mode Control for Gearless PMSG-Based Wind Turbine"
      },
      {
        "paperId": "23e2d8579eddc9a3f406367a95bf5a168e4dacf0",
        "title": "A systematic review on deep learning architectures and applications"
      },
      {
        "paperId": "b98738bb048e8e85a7e4efc2497781f94f0bbf51",
        "title": "Multi-Agent Deep Reinforcement Learning for Multi-Object Tracker"
      },
      {
        "paperId": "252769326abd395d682f808a53f13ae593bd6777",
        "title": "Artificial Neural Network Based Reinforcement Learning for Wind Turbine Yaw Control"
      },
      {
        "paperId": "195bee87a29ab99cb97cffb7355c443e148422b8",
        "title": "Numerical and Experimental Methods for the Assessment of Wind Turbine Control Upgrades"
      },
      {
        "paperId": "343ce75308263b9595f3fff05aaffe68ffa16534",
        "title": "A survey of artificial neural network in wind energy systems"
      },
      {
        "paperId": "84b8bf563f2b50d3a743142b47f302c5d80286ad",
        "title": "Model\u2010free adaptive learning control scheme for wind turbines with doubly fed induction generators"
      },
      {
        "paperId": "ccbb00d4e4ccef968adbf3f1d99ca04a32f9ffe9",
        "title": "Simulation of a Fuzzy Control Applied to a Variable Speed Wind System Connected to the Electrical Network"
      },
      {
        "paperId": "4614839914df541aacec6203757bcffbe2ce180c",
        "title": "Hybrid Genetic Algorithm Fuzzy-Based Control Schemes for Small Power System with High-Penetration Wind Farms"
      },
      {
        "paperId": "7a0dee2d54c2c54dd9de7bde0388374d5a8646a8",
        "title": "Adaptive neuro-fuzzy algorithm to estimate effective wind speed and optimal rotor speed for variable-speed wind turbine"
      },
      {
        "paperId": "f7919d4a8b56fa3e7c1388452b896bc8492588a4",
        "title": "Experiments of conditioned reinforcement learning in continuous space control tasks"
      },
      {
        "paperId": "ac952058d2682b4850a83cd53688f81834089dc5",
        "title": "Variable speed wind turbine controller adaptation by reinforcement learning"
      },
      {
        "paperId": "ef48fea818706f109cc7c59ee61551d72f0bf2bc",
        "title": "Reinforcement learning for microgrid energy management"
      },
      {
        "paperId": "8adc016dc2b67784427ff75083150e39b2233675",
        "title": "Electric grid dependence on the configuration of a small-scale wind and solar power hybrid system"
      },
      {
        "paperId": "bd7564efd0bb051e89cab2ddd83f2e072c64943a",
        "title": "Modelado y Simulaci\u00f3n de un Sistema Conjunto de Energ\u00eda Solar y E\u00f3lica para Analizar su Dependencia de la Red El\u00e9ctrica"
      },
      {
        "paperId": "257acab3703552c26aeb42623486b428e480c8ea",
        "title": "Dyna-H: A heuristic planning reinforcement learning algorithm applied to role-playing game strategy decision systems"
      },
      {
        "paperId": "03190300bc0f17400e904652726539b5fc8214e0",
        "title": "Adaptive PID Controller based on Reinforcement Learning for Wind Turbine Control"
      },
      {
        "paperId": "74483a1de3bf10a25eee72cfec95ea0a280f4a70",
        "title": "Wind Turbine Control Systems: Principles, Modelling and Gain Scheduling Design"
      },
      {
        "paperId": "590df854004db8c387530987a56e4e58715434a2",
        "title": "Review of wind turbine control"
      },
      {
        "paperId": "78d038c4c447c25bec600295cdb3fd35a1d13c00",
        "title": "Wind Turbine Pitch Control First Approach Based on Reinforcement Learning"
      },
      {
        "paperId": "86257a79958f2bb351970272b1a1673c55227ed6",
        "title": "Deep Reinforcement Learning for Power System Applications: An Overview"
      },
      {
        "paperId": "448bb14ff5b561c3962082307f91e7dccf523eac",
        "title": "Pitch angle control of a wind turbine operating above the rated wind speed: A sliding mode control approach."
      },
      {
        "paperId": null,
        "title": "Intelligent control for improving the efficiency of a hybrid semi- submersible platform with wind turbine and wave energy converters"
      },
      {
        "paperId": null,
        "title": "Modeling and Simulation of a Hybrid Wind and Solar Power System for the Analysis of Electricity Grid Dependency"
      },
      {
        "paperId": null,
        "title": "Modelling and control of floating offshore wind turbines"
      }
    ],
    "cited_by": [
      {
        "paperId": "095ae94dfa53688b61eb61d321f729d6fc164772",
        "title": "Control neuronal h\u00edbrido ISC-DSC para la m\u00e1xima extracci\u00f3n de energ\u00eda en turbinas e\u00f3licas marinas"
      },
      {
        "paperId": "fd47aefb83145b4a1160294f1bca22c79e3f2a48",
        "title": "Enhancing Energy Generation While Mitigating Noise Emissions in Wind Turbines Through Multi\u2010Objective Optimization: A Deep Reinforcement Learning Approach"
      },
      {
        "paperId": "a23e28f5d3192298a7c631e05583197e57193992",
        "title": "Metaheuristic Optimization of Wind Turbine Airfoils with Maximum-Thickness and Angle-of-Attack Constraints"
      },
      {
        "paperId": "7cb3285c941b8860cbfe24c6acf6ed623592d639",
        "title": "A Review of the Reinforcement Learning for Wind Farm Operation and Control"
      },
      {
        "paperId": "2ae03c6551ba6f6b5e53ff66dd91849ec6a41b74",
        "title": "Review on Predictive Model for Forecast of Renewable Energy Output: a Case Study of National Orthopedic Hospital, Igbobi"
      },
      {
        "paperId": "5f8fc8abc60f19ecfdbef99a8703d67a0fb39df3",
        "title": "A Comprehensive Review of SCADA-Based Wind Turbine Performance and Reliability Modeling with Machine Learning Approaches"
      },
      {
        "paperId": "bc90ae1924fcd024b3587d372e1d423178664ff4",
        "title": "Reinforcement learning to maximize wind turbine energy generation"
      },
      {
        "paperId": "3afb38ebc4dffed2327a57d88ff7e1780ecb8959",
        "title": "Combination of fuzzy control and reinforcement learning for wind turbine pitch control"
      },
      {
        "paperId": "112be728e8221d28cd91cf588e2b30e215f4c922",
        "title": "A review of artificial intelligence applications in wind turbine health monitoring"
      },
      {
        "paperId": "49483d0097745a729029cbdac11a8295e397eeb3",
        "title": "Fuzzy-based collective pitch control for wind turbine via deep reinforcement learning."
      },
      {
        "paperId": "39d51c49f699d7fd358e4268bf3cedafee202a7f",
        "title": "Reinforcement learning to maximise wind turbine energy generation"
      },
      {
        "paperId": "1889c1353aedcec2d051143c34899ca75d2376b5",
        "title": "Reinforcement learning in wind energy - a review"
      },
      {
        "paperId": "c8ef7b0d2416d27740f24f7f101206baf8106949",
        "title": "Federated Discrete Reinforcement Learning for Automatic Guided Vehicle Control"
      },
      {
        "paperId": "757e299668fd5df3b32607f7bd592cd1e5804450",
        "title": "Forecasting Renewable Energy Generation with Machine Learning and Deep Learning: Current Advances and Future Prospects"
      },
      {
        "paperId": "dc902e07eacc294a95fd448fe921723211405908",
        "title": "Mechanical stability analysis of a DFIG floating offshore wind turbine using an oriented-control model"
      },
      {
        "paperId": "926851cb414d736e1288241e350e418a409702ae",
        "title": "Self-adaptive optimized maintenance of offshore wind turbines by intelligent Petri nets"
      },
      {
        "paperId": "2e2552d99b361c791b2f42b1206c0b6e608dcab0",
        "title": "Machine Learning Control for Floating Offshore Wind Turbine Individual Blade Pitch Control"
      },
      {
        "paperId": "aa6192341a3ae7cb1d947f757e6410193d51b62d",
        "title": "Combining reinforcement learning and conventional control to improve automatic guided vehicles tracking of complex trajectories"
      },
      {
        "paperId": "ae95e096d00f0282c6f273ed9e4993c4a1b830bb",
        "title": "Wind turbine pitch reinforcement learning control improved by PID regulator and learning observer"
      },
      {
        "paperId": "a9dfc0453e262bef9fe50297b973e58659818d16",
        "title": "A reinforcement\u2010learning approach for individual pitch control"
      },
      {
        "paperId": "64d57cbcc20544b18ed913541255fe2f6b9b9340",
        "title": "TMD stroke limiting influence on barge-type floating wind turbines"
      },
      {
        "paperId": "ca094385aaefe4ccefefa3a713d0f8d0f4f2e98b",
        "title": "Intelligent control of an UAV with a cable-suspended load using a neural network estimator"
      },
      {
        "paperId": "b5bcede03e9c05e6bde392b9a7d5945b8636dfd4",
        "title": "Redes neuronales y aprendizaje por refuerzo en el control de turbinas e\u00f3licas"
      },
      {
        "paperId": "453dd84201a7deb4caba581e9b359e4ba121b32d",
        "title": "Improving the Performance of Controllers for Wind Turbines on Semi-Submersible Offshore Platforms: Fuzzy Supervisor Control"
      },
      {
        "paperId": "411489c0003f2df12619edbd8ade678f10c66b4c",
        "title": "Modelado y simulaci\u00f3n de un aerogenerador de 5 MW escalado a partir de uno de 7 KW"
      },
      {
        "paperId": "c3982d279fd0ba62d7c4d1b10ae7d99e8d561ac2",
        "title": "Deep learning and fuzzy logic to implement a hybrid wind turbine pitch control"
      },
      {
        "paperId": "d93263687f23e704c800e6748160728b72efaca0",
        "title": "Analysis of the Effects of the Location of Passive Control Devices on the Platform of a Floating Wind Turbine"
      },
      {
        "paperId": "36aeffbc7f793c389816e3836f0c6596c7399092",
        "title": "Lookup Table and Neural Network Hybrid Strategy for Wind Turbine Pitch Control"
      },
      {
        "paperId": "b29be5d319fa8d397edb9a727bdae2daf6080a15",
        "title": "Optimizing Subway Train Operation With Hierarchical Adaptive Control Approach"
      },
      {
        "paperId": "d085ec4da23b61610871adc75f7f3c17cc15efe5",
        "title": "Improving Wind Turbine Pitch Control by Effective Wind Neuro-Estimators"
      },
      {
        "paperId": "721bf80a2660e136b44a6d2b9156604a5488d724",
        "title": "Wind Turbines Control Optimization: A Problem-Driven Proposal to Learn Genetic Algorithms"
      },
      {
        "paperId": "05ab168463bcbaa103ed0b56b3b3986aa249fafb",
        "title": "Iterative Obstacle Avoidance Algorithm for Mobile Robots"
      },
      {
        "paperId": "9bf573c4d8924d74dbae45e602d894a99666ad34",
        "title": "Intelligent Hybrid Controllers for the Blade Angle of Floating Wind Turbines"
      }
    ],
    "score": 6.6000000000000005
  },
  {
    "id": "a064b8183d657178916ae21c43b5099bfef6804d",
    "title": "A Reinforcement Learning Method for a Hybrid Flow-Shop Scheduling Problem",
    "authors": [
      "Wei Han",
      "Fang Guo",
      "Xi-chao Su"
    ],
    "year": 2019,
    "citationCount": 39,
    "abstract": "The scheduling problems in mass production, manufacturing, assembly, synthesis, and transportation, as well as internet services, can partly be attributed to a hybrid flow-shop scheduling problem (HFSP). To solve the problem, a reinforcement learning (RL) method for HFSP is studied for the first time in this paper. HFSP is described and attributed to the Markov Decision Processes (MDP), for which the special states, actions, and reward function are designed. On this basis, the MDP framework is established. The Boltzmann exploration policy is adopted to trade-off the exploration and exploitation during choosing action in RL. Compared with the first-come-first-serve strategy that is frequently adopted when coding in most of the traditional intelligent algorithms, the rule in the RL method is first-come-first-choice, which is more conducive to achieving the global optimal solution. For validation, the RL method is utilized for scheduling in a metal processing workshop of an automobile engine factory. Then, the method is applied to the sortie scheduling of carrier aircraft in continuous dispatch. The results demonstrate that the machining and support scheduling obtained by this RL method are reasonable in result quality, real-time performance and complexity, indicating that this RL method is practical for HFSP.",
    "url": "https://www.semanticscholar.org/paper/a064b8183d657178916ae21c43b5099bfef6804d",
    "pdf_url": "https://doi.org/10.3390/a12110222",
    "venue": "Algorithms",
    "publicationDate": "2019-10-23",
    "externalIds": {
      "DBLP": "journals/algorithms/HanGS19",
      "MAG": "2982233612",
      "DOI": "10.3390/a12110222",
      "CorpusId": 208606940
    },
    "references": [
      {
        "paperId": "dee85f0ed3571d7b591e23000848c584242186ef",
        "title": "Composable Deep Reinforcement Learning for Robotic Manipulation"
      },
      {
        "paperId": "c37e0d93d19efbd8bb50d1a4d92979793f4341d2",
        "title": "Reinforcement Learning from Imperfect Demonstrations"
      },
      {
        "paperId": "811df72e210e20de99719539505da54762a11c6d",
        "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor"
      },
      {
        "paperId": "c40dd8f235aabe6efbb93c59c0536adf491f9ead",
        "title": "PGQ: Combining policy gradient and Q-learning"
      },
      {
        "paperId": "1c9feea757161bf53ee60a454b84a78480aef2f2",
        "title": "Speeding up a Rollout algorithm for complex parallel machine scheduling"
      },
      {
        "paperId": "d358d41c69450b171327ebd99462b6afef687269",
        "title": "Continuous Deep Q-Learning with Model-based Acceleration"
      },
      {
        "paperId": "340f48901f72278f6bf78a04ee5b01df208cc508",
        "title": "Human-level control through deep reinforcement learning"
      },
      {
        "paperId": "8d4d26ed67167ad55fa631260a4bdac7b7414d86",
        "title": "Estimation of distribution algorithm for solving hybrid flow-shop scheduling problem"
      },
      {
        "paperId": "65438e0ba226c1f97bd8a36333ebc3297b1a32fd",
        "title": "Reinforcement learning in robotics: A survey"
      },
      {
        "paperId": "8bce243d3db1d06e51cd9751e079c0012541ce07",
        "title": "An Estimation of Distribution Algorithm for Solving Hybrid Flow-shop Scheduling Problem: An Estimation of Distribution Algorithm for Solving Hybrid Flow-shop Scheduling Problem"
      },
      {
        "paperId": "5f60227f308cd6872b3e6cd69e470579e9ca9e0f",
        "title": "Hybrid Flow Shop Scheduling Problems with Multiprocessor Tasks"
      },
      {
        "paperId": "c946cd683b0e09123f95497ae4b685e4509d6aee",
        "title": "The hybrid flow shop scheduling problem"
      },
      {
        "paperId": "279812f3af30cedbfc7672a4f130518d3af277f2",
        "title": "Immune clonal selection algorithm for hybrid flow-shop scheduling problem"
      },
      {
        "paperId": "63477ba55d84b4cda8d68662f7234c32a0b3f8f1",
        "title": "Scheduling dispensing and counting in secondary pharmaceutical manufacturing"
      },
      {
        "paperId": "92d56e03d740e03b4534268eb6e0acb9acf615b1",
        "title": "A tabu search heuristic for the hybrid flowshop scheduling with finite intermediate buffers"
      },
      {
        "paperId": "08670e03690aa1224928893592b184038c39ace5",
        "title": "A particle swarm optimization algorithm for hybrid flow-shop scheduling with multiprocessor tasks"
      },
      {
        "paperId": "ecf5fd423c117ffb87730d75a473bc05beaae2b8",
        "title": "Self-Optimizing Memory Controllers: A Reinforcement Learning Approach"
      },
      {
        "paperId": "bcc88428dacbd7c55b968529cb6ddec01ceb82fd",
        "title": "A two-stage hybrid flowshop scheduling problem with a function constraint and unrelated alternative machines"
      },
      {
        "paperId": "3af4ba416149873a05643201c8e6fa215b75140f",
        "title": "Using ant colony optimization to solve hybrid flow shop scheduling problems"
      },
      {
        "paperId": "e635d81a617d1239232a9c9a11a196c53dab8240",
        "title": "Bandit Based Monte-Carlo Planning"
      },
      {
        "paperId": "2b6b86f9df90495ea924f48fa30cfb11538a5614",
        "title": "Multiprocessor task scheduling in multistage hybrid flow-shops: an ant colony system approach"
      },
      {
        "paperId": "e32f9aa41fb339cec7535b2ba7bbb190915ba1a7",
        "title": "Optimal Scheduling of a Two-stage Hybrid Flow Shop"
      },
      {
        "paperId": "f84258353751efc1910d9165041c5690abdb6f9e",
        "title": "Simulated annealing heuristic for flow shop scheduling problems with unrelated parallel machines"
      },
      {
        "paperId": "830e9202e6340b0d77104ea12fef13b623d218d5",
        "title": "An exact approach for batch scheduling in flexible flow lines with limited intermediate buffers"
      },
      {
        "paperId": "693d1bbb3b859f5a09ca2e8a838b8a54901f9c47",
        "title": "SCHEDULING OF PRINTED WIRING BOARD ASSEMBLY IN SURFACE MOUNT TECHNOLOGY LINES"
      },
      {
        "paperId": "ae52efaad0888194ad005f74c3f3a509e5eb6911",
        "title": "A palmer-based continuous fuzzy flexible flow-shop scheduling algorithm"
      },
      {
        "paperId": "d17f75e903b68f00502b13a83b81c1d30ef5e6fb",
        "title": "Sequencing Hybrid Two-Stage Flowshops with Dedicated Machines"
      },
      {
        "paperId": "66d6e775379b1c4e37b6f91b35ba4f94b19defa1",
        "title": "Hybrid flow shop scheduling using genetic algorithms"
      },
      {
        "paperId": "0d9cc60d4e4025e96ce13e31ebd4b56d70f7086b",
        "title": "PREEMPTIVE SCHEDULING IN A TWO-STAGE MULTIPROCESSOR FLOW SHOP IS NP-HARD"
      },
      {
        "paperId": "a82db864e472b5aa6313596ef9919f64e3363b1f",
        "title": "Dynamic Programming and Optimal Control, Two Volume Set"
      },
      {
        "paperId": "2b11305f69641ecb8bd4a5e59cfebe41ad9ed989",
        "title": "TD-Gammon, a Self-Teaching Backgammon Program, Achieves Master-Level Play"
      },
      {
        "paperId": "03b7e51c52084ac1db5118342a00b5fbcfc587aa",
        "title": "Q-learning"
      },
      {
        "paperId": "a55724c04cb4b384b05fdad4136ace4a0e6ad576",
        "title": "Hybrid Differential Evolution Algorithm for Sortie Scheduling of Carrier Aircraft"
      },
      {
        "paperId": "f1e633e4b19168513987cb56ac8544744dc86af9",
        "title": "A Hybrid Particle Swarm Optimization Method for Flow Shop Scheduling Problem"
      },
      {
        "paperId": "944e143656aed6cc9941969600e778f926ca8059",
        "title": "Study on an Average Reward Reinforcement Learning Algorithm"
      },
      {
        "paperId": "536cfe86685986e90d26ee8e5bb47d673cce3ff6",
        "title": "Lagrangian Relaxation Algorithm for Real-time Hybrid Flowshop Scheduling with No-wait in Process"
      },
      {
        "paperId": "30786fd837a29415a689ccc681ba4ae3006e1301",
        "title": "A Reinforcement Learning-based Approach to Dynamic Job-shop Scheduling"
      },
      {
        "paperId": "7c347bd3c8af8e6c67cfc7c1fee374220d4b49ff",
        "title": "Hybrid flow-shop scheduling problems with multiprocessor task systems"
      },
      {
        "paperId": "dfccdb109355afa35d3750e031d019d8d93a6f72",
        "title": "Hybrid Flow-shop Scheduling Approach Based on Genetic Algorithm"
      },
      {
        "paperId": null,
        "title": "Study on multi-objective flexible Job-Shop scheduling problem based on hybrid artificial bee colony"
      },
      {
        "paperId": null,
        "title": "Study on multi-objective flexible Job-Shop scheduling problem based on hybrid artificial bee colony algorithm"
      },
      {
        "paperId": null,
        "title": "A Hybrid Particle Swarm Optimization Method for Flow Shop Scheduling Problem. Acta Electron"
      },
      {
        "paperId": null,
        "title": "Search Heuristic for the Hybrid Flow-shop Scheduling with Finite Intermediate Buffers"
      },
      {
        "paperId": null,
        "title": "A novel off policy Q(\u03bb) algorithm based on linear function approximation. Chin"
      },
      {
        "paperId": null,
        "title": "Improved grey wolf optimization algorithm for flexible shop scheduling problem"
      },
      {
        "paperId": null,
        "title": "A novel o ff policy Q ( \u03bb ) algorithm based on linear function approximation"
      },
      {
        "paperId": null,
        "title": "A novel off policy Q(\u03bb) algorithm based on linear function approximation"
      },
      {
        "paperId": null,
        "title": "Research on Agent-based Hybrid Flow Shop Dynamic Scheduling Problem"
      },
      {
        "paperId": null,
        "title": "A Tabu Search Heuristic for the Hybrid Flow - shop Scheduling with Finite Intermediate"
      }
    ],
    "cited_by": [
      {
        "paperId": "6cd8d47f18a212004ab2d94dbf6e00c4c161d7d5",
        "title": "A proximal policy optimization driven hyper-heuristic for workers constrained hybrid flow shop problem"
      },
      {
        "paperId": "47fd23f5c7242d84948571bf8d04196f27b3057f",
        "title": "Algoritmo Gen\u00e9tico para a Otimiza\u00e7\u00e3o do Problema do Sequenciamento de Tarefas com Produ\u00e7\u00e3o Multi-Est\u00e1gio, M\u00e1quinas Paralelas e Tempo de Setup: Estudo de Caso da Log\u00edstica do Beneficiamento de Rochas Ornamentais"
      },
      {
        "paperId": "9aee4f61cca2f0c1a681cd7580610b55ebd3356a",
        "title": "Human Experience-Guided Reinforcement Learning for Carrier-Based Aircraft Support Operation Scheduling"
      },
      {
        "paperId": "b8047c77d2fded74fc0354486f6c1f70a842c282",
        "title": "Carrier-Based Aircraft Operation Scheduling Based on Improved Genetic Algorithm"
      },
      {
        "paperId": "abd20f7accbf75c83f8750652be0e30418c939bc",
        "title": "Multi-DAT: Dynamic Job Task Scheduling Method Based on Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "f84be56c81587a8ae26c19714ff1a7a83abdd569",
        "title": "Dynamic scheduling for flexible job shop under machine breakdown using Improved Double Deep Q-network"
      },
      {
        "paperId": "c4a005fbb1813066b5b69749c23fee83af34d55a",
        "title": "Hybrid Flow Shop Scheduling through Reinforcement Learning: A systematic literature review"
      },
      {
        "paperId": "46e0bcc31be4069876dc2849b21214eba339d5c7",
        "title": "Review on ensemble meta-heuristics and reinforcement learning for manufacturing scheduling problems"
      },
      {
        "paperId": "899621e72eb3de9369ebd97c705ff99c20accb24",
        "title": "Deep Reinforcement Learning-based Multi-Objective Scheduling for Distributed Heterogeneous Hybrid Flow Shops with Blocking Constraints"
      },
      {
        "paperId": "c59a94cc5c1708a199a59325f55d512d7f210f59",
        "title": "Collaborative Scheduling Optimization Method for Multi-Stage Automobile Engine Hybrid Flow Shop"
      },
      {
        "paperId": "c2abbd2b506161d5e4d6abf0ef546415699ecb0f",
        "title": "Dynamic scheduling of hybrid flow shop problem with uncertain process time and flexible maintenance using NeuroEvolution of Augmenting Topologies"
      },
      {
        "paperId": "781cf1c1343dd3935a049cc3cb5d722c881af968",
        "title": "The marriage of operations research and reinforcement learning: Integration of NEH into Q-learning algorithm for the permutation flowshop scheduling problem"
      },
      {
        "paperId": "48b1ab79ff304cb88c1055e35abb63b3d52d1f97",
        "title": "Q-learning based scheduling method for continuous pickling process of titanium strips"
      },
      {
        "paperId": "6dcb5e7f57300173fd1e209b228db5dc3d7bdd24",
        "title": "Research on HFS Scheduling Based on D3QN-RepVGG-CBAM Algorithm"
      },
      {
        "paperId": "4b09e2fd8038d803a6405d276d894c31f851cbfb",
        "title": "Dynamic task allocation of hybrid flow shop for machines in parallel with different speeds based on an MLD prediction model"
      },
      {
        "paperId": "eb206d732c05566cffcbac983c1034025b0a486b",
        "title": "Data-Driven Real-Time Framework for Hybrid Flow Shop Scheduling Based on Dueling Double Deep Q Network"
      },
      {
        "paperId": "71658fed5a7d6f9edd0247db7b4a01061099464a",
        "title": "\u00c7izelgeleme Problemlerinin \u00c7\u00f6z\u00fcm\u00fcnde Peki\u015ftirmeli \u00d6\u011frenme Etkisinin Analizi"
      },
      {
        "paperId": "9e0204394b59e0b2f8887361f1d62264829ac2c3",
        "title": "A discrete group teaching optimization algorithm for solving many-objective sand casting whole process production scheduling problem"
      },
      {
        "paperId": "3190ca11ec29f3510a529a958fa42a82b3fc97a0",
        "title": "Solving flexible job shop scheduling problems via deep reinforcement learning"
      },
      {
        "paperId": "493821fdcdfda663aaa03c5150013688be6712ed",
        "title": "Multi-Objective Flexible Flow Shop Production Scheduling Problem Based on the Double Deep Q-Network Algorithm"
      },
      {
        "paperId": "20ac492a8df352fbcc3fe170c062318854b8209c",
        "title": "Neuro-Evolution of Augmenting Topologies for Dynamic Scheduling of Hybrid Flow Shop Problem"
      },
      {
        "paperId": "f173229ee4bff553c5c67806a6a25048f0da773c",
        "title": "Constraint programming models for the hybrid flow shop scheduling problem and its extensions"
      },
      {
        "paperId": "cd6c134ae477c0b9488a0b0faea91d91c313c75c",
        "title": "A Reinforcing-Learning-Driven Artificial Bee Colony Algorithm for Scheduling Jobs and Flexible Maintenance under Learning and Deteriorating Effects"
      },
      {
        "paperId": "d672478400ee748ac4596ace06e1c666a1e616ea",
        "title": "A Survey of AI-enabled Dynamic Manufacturing Scheduling: From Directed Heuristics to Autonomous Learning"
      },
      {
        "paperId": "f0d8b48343ed0a7b503899a8191bbe501641f5b1",
        "title": "Solving non-permutation flow-shop scheduling problem via a novel deep reinforcement learning approach"
      },
      {
        "paperId": "7cf62e35ead763cb30f1d6be36a9ab35a820c717",
        "title": "Deep Reinforcement Learning Approach for Material Scheduling Considering High-Dimensional Environment of Hybrid Flow-Shop Problem"
      },
      {
        "paperId": "6906e617eadbcb7470616c0d742f327cd291c382",
        "title": "A novel shuffled frog-leaping algorithm with reinforcement learning for distributed assembly hybrid flow shop scheduling"
      },
      {
        "paperId": "5fe27d921648821defad4997c9fb3486ebc6b9eb",
        "title": "A Review of Reinforcement Learning Based Intelligent Optimization for Manufacturing Scheduling"
      },
      {
        "paperId": "5ef5a0ab545aad37552808aefe0d5c58de8e80b8",
        "title": "A bi-population immune algorithm for weapon transportation support scheduling problem with pickup and delivery on aircraft carrier deck"
      },
      {
        "paperId": "2cab6f32833feca09dac004116729675eae3bd7c",
        "title": "Reinforcement learning applications to machine scheduling problems: a comprehensive literature review"
      },
      {
        "paperId": "4df92e25abead9e01f06aa3e1a04dfb387c490da",
        "title": "Solving flow-shop scheduling problem with a reinforcement learning algorithm that generalizes the value function with neural network"
      },
      {
        "paperId": "6d333d83042eddb6fdc2c00858f02a0a9fec83a9",
        "title": "An Agent-Based Approach for Dynamic Scheduling in Hybrid Flow Shops"
      },
      {
        "paperId": "43a7c393313b7b128e26df889fc6bbdf17346dcf",
        "title": "Improved MOMVO algorithm to solve the multi-objective HFSP considering machine shutdown"
      },
      {
        "paperId": "aeed2c15f0dd74feaaf1d4ab2cca916e0dbcc1e1",
        "title": "An energy efficient and resource\u2010constrained scheduling framework for smart city application"
      },
      {
        "paperId": "f8ef89a78f404a71f2176516d1c87cdb53c90cff",
        "title": "Flow Shop Providing Frequency Regulation Service in Electricity Market"
      },
      {
        "paperId": "d8603608b9ceb646f061fb9a80ef9e070e8daa80",
        "title": "Machine learning for online scheduling in manufacturing: A systematic literature review"
      },
      {
        "paperId": "1a22b8ca9c1fc5d1e3e631deb1d0e8af9bf98eec",
        "title": "A Deep Reinforcement Learning Based Scheduling Policy for Reconfigurable Manufacturing Systems"
      },
      {
        "paperId": "4bb47b7f803aa22a94f610ec9fd52fa2cb238107",
        "title": "A Reinforcing-Learning-Driven Arti\ufb01cial Bee Colony Algorithm for Scheduling Jobs and Flexible Maintenance under Learning and Deteriorating Effects"
      },
      {
        "paperId": "5c1465a335859d9dbe7c9d681340e2969ed172e9",
        "title": "Evolution Strategies-Guided Deep Reinforcement Learning for Dynamic Hybrid Flow-Shop Scheduling Problem"
      }
    ],
    "score": 6.5
  },
  {
    "id": "bc98c81467ed3a6b21788f39c20cbe659014e551",
    "title": "Safe exploration of nonlinear dynamical systems: A predictive safety filter for reinforcement learning",
    "authors": [
      "K. P. Wabersich",
      "M. Zeilinger"
    ],
    "year": 2018,
    "citationCount": 44,
    "abstract": "",
    "url": "https://www.semanticscholar.org/paper/bc98c81467ed3a6b21788f39c20cbe659014e551",
    "pdf_url": null,
    "venue": "arXiv.org",
    "publicationDate": "2018-12-13",
    "externalIds": {
      "MAG": "2905111361",
      "DBLP": "journals/corr/abs-1812-05506",
      "CorpusId": 55439609
    },
    "references": [],
    "cited_by": [
      {
        "paperId": "fc22b6797b7c98b6d3a47e884dccca0bd4d170c5",
        "title": "Safely Learning Controlled Stochastic Dynamics"
      },
      {
        "paperId": "0a158e09d9d3a9e706893f69238e104306b891ba",
        "title": "Online End-to-End Learning-Based Predictive Control for Microgrid Energy Management"
      },
      {
        "paperId": "ae62b9134ef8186567097b4e149b82d39605426b",
        "title": "Guaranteed-Safe MPPI Through Composite Control Barrier Functions for Efficient Sampling in Multi-Constrained Robotic Systems"
      },
      {
        "paperId": "86642b3c6af6230614255ce95f33ed7324ec276d",
        "title": "HYPERmotion: Learning Hybrid Behavior Planning for Autonomous Loco-manipulation"
      },
      {
        "paperId": "815efe3d0cbd0ecc88f682f797bedd4d6c7005f5",
        "title": "Probabilistic Motion Planning and Prediction via Partitioned Scenario Replay"
      },
      {
        "paperId": "6d05b085a02f10a65d2e9861aec6bb0f58b457fc",
        "title": "Risk-Aware Reinforcement Learning through Optimal Transport Theory"
      },
      {
        "paperId": "96caddc2cd9f6a71dc07d6353896538442c6d8da",
        "title": "Learning-Based Predictive Control Using a Hybrid Model with Adaptive Domain of Validity*"
      },
      {
        "paperId": "58b33101380226af36d78fc7b8b9ed5ebac7ce66",
        "title": "An Online Learning Method for Microgrid Energy Management Control*"
      },
      {
        "paperId": "80f3eccf437371a74ff0aa04054c48d29f16086e",
        "title": "Towards Safe AI: Sandboxing DNNs-Based Controllers in Stochastic Games"
      },
      {
        "paperId": "fa403c1387bb4f8d70ab375ce37fd1b8e6947580",
        "title": "Soft-Minimum and Soft-Maximum Barrier Functions for Safety with Actuation Constraints"
      },
      {
        "paperId": "1ec2e3c747b4828c538038bcb17c04c6168beec8",
        "title": "Model Predictive Control for Multi-Agent Systems Under Limited Communication and Time-Varying Network Topology"
      },
      {
        "paperId": "928192dd95903b7aed1d490e8d088e95c6e74440",
        "title": "Soft-Minimum Barrier Functions for Safety-Critical Control Subject to Actuation Constraints"
      },
      {
        "paperId": "e3d3bbc9cad738e22780b8465dac405dd6c781e6",
        "title": "Deep Reinforcement Learning With NMPC Assistance Nash Switching for Urban Autonomous Driving"
      },
      {
        "paperId": "3aaa584c0afa5a19bf1205b51508dbc943a1eaf6",
        "title": "Safe deep reinforcement learning in diesel engine emission control"
      },
      {
        "paperId": "b1f5c1f6e54353a3f77c23a01b53d935b8d7abc1",
        "title": "Guaranteed safe control of systems with parametric uncertainties via neural network controllers"
      },
      {
        "paperId": "79f40dde059f244b83aed37f1712ac6f2b87295b",
        "title": "Quad2Plane: An Intermediate Training Procedure for Online Exploration in Aerial Robotics via Receding Horizon Control"
      },
      {
        "paperId": "dd94a7bb7088521d7cae9f522f386cd2118a17b9",
        "title": "Linearization and Identification of Multiple-Attractors Dynamical System through Laplacian Eigenmaps"
      },
      {
        "paperId": "e457b308e2c4909d6fba382c25cedec6b93d1aa3",
        "title": "On reliability of reinforcement learning based production scheduling systems: a comparative survey"
      },
      {
        "paperId": "61bb888b7b60d9758f21ad2d22260bb86b9a29dc",
        "title": "Regret Analysis of Learning-Based MPC With Partially Unknown Cost Function"
      },
      {
        "paperId": "dd525e494de88711dcb56383f1e7984c274983f3",
        "title": "Nonlinear learning\u2010based model predictive control supporting state and input dependent model uncertainty estimates"
      },
      {
        "paperId": "3405963cb5d8b3b0eb2b0b00a538b395a9d6993c",
        "title": "Safe Reinforcement Learning via Statistical Model Predictive Shielding"
      },
      {
        "paperId": "bbbf6228d7471854bb152e4062c7086481ba280b",
        "title": "Safe Reinforcement Learning Using Advantage-Based Intervention"
      },
      {
        "paperId": "50b7de4cb8ca45556a9c87381f71fc6fb49142d2",
        "title": "Reinforced approximate robust nonlinear model predictive control"
      },
      {
        "paperId": "10cb437228252b2fe4698f3b7e468632f6198782",
        "title": "Model-based Reinforcement Learning with Provable Safety Guarantees via Control Barrier Functions"
      },
      {
        "paperId": "380a66599efe271296a166f00a66b743aba68c09",
        "title": "Bias Correction in Deterministic Policy Gradient Using Robust MPC"
      },
      {
        "paperId": "14a0f4a0f3b54b9c2db20df14b78afe1bd86df44",
        "title": "A Predictive Safety Filter for Learning-Based Racing Control"
      },
      {
        "paperId": "2518f2fd52a500e5935c233118b61c0d9ed7f47d",
        "title": "Safe-visor Architecture for Sandboxing (AI-based) Unverified Controllers in Stochastic Cyber-Physical Systems"
      },
      {
        "paperId": "fa882682ab2af52663389d2c2dc567538948f46e",
        "title": "Control Barriers in Bayesian Learning of System Dynamics"
      },
      {
        "paperId": "5ee32f793297c55ee45fd0e3c2fa2b0b3d2ebce6",
        "title": "Constrained Model-Free Reinforcement Learning for Process Optimization"
      },
      {
        "paperId": "3c00b743a024315ea7c78483013e0e6b158fddfc",
        "title": "Distributed Safe Learning using an Invariance-based Safety Framework"
      },
      {
        "paperId": "7908f006d9e5a405519d217b90742925d63ee9dd",
        "title": "Stable Reinforcement Learning with Unbounded State Space"
      },
      {
        "paperId": "932709a41eee54f3eddbd3b3a7baf043c3ed33ec",
        "title": "Learning-Based Model Predictive Control: Toward Safe Learning in Control"
      },
      {
        "paperId": "ae7377049628db95a858fb90727b6aded3422b2a",
        "title": "Safe Multi-Agent Interaction through Robust Control Barrier Functions with Learned Uncertainties"
      },
      {
        "paperId": "41fb0a17011e4dbf4fd06f12f01d098c76c228f5",
        "title": "Augmenting MPC Schemes With Active Learning: Intuitive Tuning and Guaranteed Performance"
      },
      {
        "paperId": "e42b3ead5ff04adfa95c87e0180561f0c3ba4af4",
        "title": "Safe Reinforcement Learning of Control-Affine Systems with Vertex Networks"
      },
      {
        "paperId": "3166e74a4b5f0aaddc1c211b75b8c699b49bcb0b",
        "title": "Routing Using Safe Reinforcement Learning"
      },
      {
        "paperId": "9b7042b83f255b31759b2c9f958956825f4f81fc",
        "title": "Probabilistic Safety Constraints for Learned High Relative Degree System Dynamics"
      },
      {
        "paperId": "534c073e623ae9afb4c8cf7d893c5ffa8dc6d697",
        "title": "A Computationally Efficient Robust Model Predictive Control Framework for Uncertain Nonlinear Systems"
      },
      {
        "paperId": "f587d8c3c77baa28efb07b50b8db0c4637b89ede",
        "title": "Learning-based Model Predictive Control for Safe Exploration and Reinforcement Learning"
      },
      {
        "paperId": "7b8c9353e5f7ea1698d17fb42ff11315c5930f28",
        "title": "Reinforcement Learning for Batch Bioprocess Optimization"
      },
      {
        "paperId": "c15e64a6418344de1eb13cc518805d7782ac5ce2",
        "title": "Multi-step Greedy Reinforcement Learning Based on Model Predictive Control"
      },
      {
        "paperId": "ee2f3948ccc952f7ff057c1f989c1307e7df5d9a",
        "title": "Towards Safe Neural Network Supported Model Predictive Control"
      },
      {
        "paperId": "7288607eafdf8795543442e7fcf01a128e268a40",
        "title": "A Constraint-Tightening Approach to Nonlinear Stochastic Model Predictive Control under General Bounded Disturbances"
      },
      {
        "paperId": "e12680f69f951e2bb70d2cb942d9bdcfb3ec9e5f",
        "title": "Online learning robust MPC: an exploration-exploitation approach"
      }
    ],
    "score": 6.285714285714286
  },
  {
    "id": "621d57c1243f055bc3850c1f3e38f351f53c947f",
    "title": "Tightening Exploration in Upper Confidence Reinforcement Learning",
    "authors": [
      "Hippolyte Bourel",
      "Odalric-Ambrym Maillard",
      "M. S. Talebi"
    ],
    "year": 2020,
    "citationCount": 31,
    "abstract": "The upper confidence reinforcement learning (UCRL2) strategy introduced in (Jaksch et al., 2010) is a popular method to perform regret minimization in unknown discrete Markov Decision Processes under the average-reward criterion. Despite its nice and generic theoretical regret guarantees, this strategy and its variants have remained until now mostly theoretical as numerical experiments on simple environments exhibit long burn-in phases before the learning takes place. Motivated by practical efficiency, we present UCRL3, following the lines of UCRL2, but with two key modifications: First, it uses state-of-the-art time-uniform concentration inequalities, to compute confidence sets on the reward and transition distributions for each state-action pair. To further tighten exploration, we introduce an adaptive computation of the support of each transition distributions. This enables to revisit the extended value iteration procedure to optimize over distributions with reduced support by disregarding low probability transitions, while still ensuring near-optimism. We demonstrate, through numerical experiments on standard environments, that reducing exploration this way yields a substantial numerical improvement compared to UCRL2 and its variants. On the theoretical side, these key modifications enable to derive a regret bound for UCRL3 improving on UCRL2, that for the first time makes appear a notion of local diameter and effective support, thanks to variance-aware concentration bounds.",
    "url": "https://www.semanticscholar.org/paper/621d57c1243f055bc3850c1f3e38f351f53c947f",
    "pdf_url": "https://arxiv.org/pdf/2004.09656.pdf",
    "venue": "International Conference on Machine Learning",
    "publicationDate": "2020-04-20",
    "externalIds": {
      "MAG": "3034482546",
      "DBLP": "journals/corr/abs-2004-09656",
      "ArXiv": "2004.09656",
      "CorpusId": 216035999
    },
    "references": [
      {
        "paperId": "b799c782f168b0a02ebab9e50ff38ded1bc79aee",
        "title": "Optimistic posterior sampling for reinforcement learning: worst-case regret bounds"
      },
      {
        "paperId": "782bf146620366d18efa621a6a2bfcfa75a89b87",
        "title": "Improved Analysis of UCRL2 with Empirical Bernstein Inequality"
      },
      {
        "paperId": "0568839fa045769f28d8fe038a4dd707e6531e99",
        "title": "Regret Minimization for Reinforcement Learning by Evaluating the Optimal Bias Function"
      },
      {
        "paperId": "6246b3d27f6a9f211126cafe39d8c8a7f6ed06f4",
        "title": "Tight Regret Bounds for Model-Based Reinforcement Learning with Greedy Policies"
      },
      {
        "paperId": "ac491b1c26b57e941a7ffcd921e1d1da4b950b8d",
        "title": "Mathematics of Statistiscal Sequential Decision Making"
      },
      {
        "paperId": "9e4b2c69746464cbaa081bb0991f9e0ad47a85cb",
        "title": "Mathematics of Statistical Sequential Decision Making. (Math\u00e9matique de la prise de d\u00e9cision s\u00e9quentielle statistique)"
      },
      {
        "paperId": "f14ea2243fe74cbf1a4ea86cebed1fb9547c3a7c",
        "title": "Tighter Problem-Dependent Regret Bounds in Reinforcement Learning without Domain Knowledge using Value Function Bounds"
      },
      {
        "paperId": "2386895779a1450613e6801ee63ff7f79d66cc6b",
        "title": "Near Optimal Exploration-Exploitation in Non-Communicating Markov Decision Processes"
      },
      {
        "paperId": "4bcfc5a34a22363881b54a906a1dd5040cf0a22e",
        "title": "Variance-Aware Regret Bounds for Undiscounted Reinforcement Learning in MDPs"
      },
      {
        "paperId": "6d993a2111f47b5096fada7bed42cc4a523c294d",
        "title": "Efficient Bias-Span-Constrained Exploration-Exploitation in Reinforcement Learning"
      },
      {
        "paperId": "daea16e16370c9d141ce6bd1b42ee3e5287bf275",
        "title": "Learning Unknown Markov Decision Processes: A Thompson Sampling Approach"
      },
      {
        "paperId": "7586bdad167178c32271000b5914944bae9a6dc0",
        "title": "Posterior sampling for reinforcement learning: worst-case regret bounds"
      },
      {
        "paperId": "67d10cea808937089d6492acff14ad9ef156e3c5",
        "title": "Minimax Regret Bounds for Reinforcement Learning"
      },
      {
        "paperId": "5dcc07acb63cc909c5be701c1c88fef3718ba326",
        "title": "Unifying PAC and Regret: Uniform PAC Bounds for Episodic Reinforcement Learning"
      },
      {
        "paperId": "71bc9493359605f042adf5461b717384c7f8c30e",
        "title": "How hard is my MDP?\" The distribution-norm to the rescue\""
      },
      {
        "paperId": "789783016fb708abbc061790612ebe91273c05d3",
        "title": "(More) Efficient Reinforcement Learning via Posterior Sampling"
      },
      {
        "paperId": "2fb23de9524b13a32d9ed7f2441c46c81558a3c8",
        "title": "Concentration Inequalities: A Nonasymptotic Theory of Independence"
      },
      {
        "paperId": "d0dd716b32c548a63dbd1a119f0f8b92bc19f20e",
        "title": "Concentration of Measure Inequalities in Information Theory, Communications, and Coding"
      },
      {
        "paperId": "aae172daf56b10dc2511a5304155916cafdf1d97",
        "title": "On the concentration of the missing mass"
      },
      {
        "paperId": "9495c28f6faaf8710d16a333113da8776c809920",
        "title": "Optimism in reinforcement learning and Kullback-Leibler divergence"
      },
      {
        "paperId": "ed271815949e2fd283fd62a8e52a5b30cc769594",
        "title": "Natural actor-critic algorithms"
      },
      {
        "paperId": "71886c33a34c6effe14f465ebe2806383e0d76a3",
        "title": "REGAL: A Regularization based Algorithm for Reinforcement Learning in Weakly Communicating MDPs"
      },
      {
        "paperId": "0cafe2903b097fc042782c359cb231ea34ef7ed3",
        "title": "Near-optimal Regret Bounds for Reinforcement Learning"
      },
      {
        "paperId": "1a2a75279ae9daf59772d0cf2cb4b7cd6abcda63",
        "title": "Optimistic Linear Programming gives Logarithmic Regret for Irreducible MDPs"
      },
      {
        "paperId": "94e9d4c63268abdebcff529c452a30b272221b5c",
        "title": "Logarithmic Online Regret Bounds for Undiscounted Reinforcement Learning"
      },
      {
        "paperId": "92e5999c1f733725ce5468e43085dce26d08f8c5",
        "title": "Large Deviation Methods for Approximate Probabilistic Inference"
      },
      {
        "paperId": "59ef34c2ecca183b7d0ff1788b49f2d5ca3e5ab9",
        "title": "Asymptotically Efficient Adaptive Choice of Control Laws inControlled Markov Chains"
      },
      {
        "paperId": "4e3ec923bf91f9cc3d53c135281a49325a5c6649",
        "title": "Optimal Adaptive Policies for Markov Decision Processes"
      },
      {
        "paperId": "a9cd8efe9184dddb1bedbbec3a356c4dfb22fe63",
        "title": "Markov Decision Processes: Discrete Stochastic Dynamic Programming"
      },
      {
        "paperId": "4c9e36a0dbdc3e7b20e38bd288a804c05c77e9fa",
        "title": "Asymptotically efficient adaptive allocation rules"
      },
      {
        "paperId": "ee2cd1d17f833d3c157a1016a778c7c22af555a2",
        "title": "ON THE LIKELIHOOD THAT ONE UNKNOWN PROBABILITY EXCEEDS ANOTHER IN VIEW OF THE EVIDENCE OF TWO SAMPLES"
      },
      {
        "paperId": "6222c8b38521e38ebbe6ea4868d4aaafa619a335",
        "title": "Exploration Bonus for Regret Minimization in Discrete and Continuous Average Reward MDPs"
      },
      {
        "paperId": null,
        "title": "Improved analysis of UCRL2B"
      },
      {
        "paperId": "5cc7c3ebcf5d36fcf826a925333a39caaad3405a",
        "title": "Inequalities for the L1 Deviation of the Empirical Distribution"
      },
      {
        "paperId": "ed76544b6aad2af74c430f857a1171adf7301b2d",
        "title": "Self-Normalized Processes: Limit Theory and Statistical Applications"
      },
      {
        "paperId": "97efafdb4a3942ab3efba53ded7413199f79c054",
        "title": "Reinforcement Learning: An Introduction"
      },
      {
        "paperId": "572379c1d56e7e19422ae38218ee228c61aefb2f",
        "title": "Asymptotically Efficient Adaptive Allocation Rules"
      },
      {
        "paperId": null,
        "title": "Note that the inequality above implies L s,a \u2264 K s,a G s,a . Furthermore, In view of the concavity of z \u2192 x\u2208S z(x)(1 \u2212 z(x)), the maximal value of G s"
      },
      {
        "paperId": null,
        "title": "Therefore, L s,a \u2264 K s,a G s,a \u2264 K s,a \u2212 1"
      }
    ],
    "cited_by": [
      {
        "paperId": "985796f3006ffe41eecaa9096be68ccc2f1096c1",
        "title": "Statistical and Algorithmic Foundations of Reinforcement Learning"
      },
      {
        "paperId": "62f4ea6aa43005429904b8fe1a5076568f9f00fd",
        "title": "Model Selection for Average Reward RL with Application to Utility Maximization in Repeated Games"
      },
      {
        "paperId": "f96ee0e24a1db058f75962653f1fd51ec8ff779c",
        "title": "Learning Infinite-Horizon Average-Reward Linear Mixture MDPs of Bounded Span"
      },
      {
        "paperId": "4cdeb0cc53e9c055ea6d6d11b9f40486a2a58873",
        "title": "How to Shrink Confidence Sets for Many Equivalent Discrete Distributions?"
      },
      {
        "paperId": "13b89c5975c4ab5afd58b67b388d01e11c71c0ca",
        "title": "Reinforcement Learning and Regret Bounds for Admission Control"
      },
      {
        "paperId": "a7b031241bd8d0c3568e703a6e86add7a39f8d1e",
        "title": "Achieving Tractable Minimax Optimal Regret in Average Reward MDPs"
      },
      {
        "paperId": "719ac927846bb6d13766658aaeb8542217689eb2",
        "title": "Safety through Permissibility: Shield Construction for Fast and Safe Reinforcement Learning"
      },
      {
        "paperId": "c7303bed6a0f32ce345a60a8b71a5cb21da12d1f",
        "title": "Finding good policies in average-reward Markov Decision Processes without prior knowledge"
      },
      {
        "paperId": "ffb2f53da10ceef4ab36cec864ec24d653cebfae",
        "title": "Utilizing Maximum Mean Discrepancy Barycenter for Propagating the Uncertainty of Value Functions in Reinforcement Learning"
      },
      {
        "paperId": "049bc549556ddaa4cb156a3f152c4694c9763bbf",
        "title": "CRIMED: Lower and Upper Bounds on Regret for Bandits with Unbounded Stochastic Corruption"
      },
      {
        "paperId": "aeeaf09a81bcf2ee5b8b4c41b97c34e4c20ab008",
        "title": "Scaling Up Q-Learning via Exploiting State\u2013Action Equivalence"
      },
      {
        "paperId": "8747162a771efc52c9a26ec4be5138f0f465bea5",
        "title": "Online Reinforcement Learning in Periodic MDP"
      },
      {
        "paperId": "a25f9bd0613b75d2c3b7e5215d7423b67c5d64de",
        "title": "Reinforcement Learning in a Birth and Death Process: Breaking the Dependence on the State Space"
      },
      {
        "paperId": "c934669e8c0816b4ffbf8fe3cebabb6779e3ec80",
        "title": "Regret Analysis for RL using Renewal Bandit Feedback"
      },
      {
        "paperId": "bfaea8fa42790311f60292fb7714f03a3445fdd3",
        "title": "SIFTER: Space-Efficient Value Iteration for Finite-Horizon MDPs"
      },
      {
        "paperId": "40bda2294008695ffa48b0ef24d5d0952cb83065",
        "title": "An Analysis of Model-Based Reinforcement Learning From Abstracted Observations"
      },
      {
        "paperId": "4c4d1e12d32f09113f9e0459c9de3e49b81cecfd",
        "title": "Online Reinforcement Learning for Periodic MDP"
      },
      {
        "paperId": "d9f0335a399db01bcaa9f55abd71d2b4cec9c3aa",
        "title": "Multiple-Play Stochastic Bandits with Shareable Finite-Capacity Arms"
      },
      {
        "paperId": "9f2e889bf4c099b7b26613320f716c43120fd3f4",
        "title": "Bandits Corrupted by Nature: Lower Bounds on Regret and Robust Optimistic Algorithm"
      },
      {
        "paperId": "902bf980eb2957535c809f999bd42eb7294a42ab",
        "title": "Scaling Power Management in Cloud Data Centers: A Multi-Level Continuous-Time MDP Approach"
      },
      {
        "paperId": "e9770e1fc0f9977a0e890546f4d9062e6a934a28",
        "title": "Learning Algorithms for Markovian Bandits: Is Posterior Sampling more Scalable than Optimism?"
      },
      {
        "paperId": "32b861a5534f4755e8cd92884ac9f6bca7904164",
        "title": "Improved Exploration in Factored Average-Reward MDPs"
      },
      {
        "paperId": "600f187ffb57297362ff82dbd98a1da0501382be",
        "title": "Statistically Robust, Risk-Averse Best Arm Identification in Multi-Armed Bandits"
      },
      {
        "paperId": "4f9a0f2556cdea5f9078973eef387bb806822d74",
        "title": "Reinforcement Learning in Factored MDPs: Oracle-Efficient Algorithms and Tighter Regret Bounds for the Non-Episodic Setting"
      },
      {
        "paperId": "3e24f5f6ee4ae7f4bdc6bfce77848a7b7165c197",
        "title": "Heavy-Tailed Reinforcement Learning With Penalized Robust Estimator"
      },
      {
        "paperId": "14e02786233acca32d923c32aa630b483b6e2efa",
        "title": "Logarithmic regret in communicating MDPs: Leveraging known dynamics with bandits"
      },
      {
        "paperId": "fee5217c933619c7ba418aeab96e653508a52d17",
        "title": "Exploration in Reward Machines with Low Regret"
      },
      {
        "paperId": "1dab0e6e69d61f76c7ffa9d3c371583715de3207",
        "title": "Identification of Blackwell Optimal Policies for Deterministic MDPs"
      },
      {
        "paperId": "fa7307906ac1556e0d0ea1c6b2b303e4e2d0294e",
        "title": "IMED-RL: Regret optimal learning of ergodic Markov decision processes"
      },
      {
        "paperId": "212649be6b096487b63a606c5d2ba1793b64b75c",
        "title": "Bandits with Stochastic Corruption: Lower Bounds on Regret and Robust Optimistic Algorithms"
      },
      {
        "paperId": "b6a527c4960c983a9939c283c7f9ece4513dcec1",
        "title": "Private Online Learning in Adversarial MDPs: Full-Information and Bandit"
      }
    ],
    "score": 6.2
  },
  {
    "id": "9b5aa6a75d8e9f0e68d12e3b331acad6f32667d4",
    "title": "Safe and Efficient Reinforcement Learning using Disturbance-Observer-Based Control Barrier Functions",
    "authors": [
      "Yikun Cheng",
      "Pan Zhao",
      "N. Hovakimyan"
    ],
    "year": 2022,
    "citationCount": 18,
    "abstract": "Safe reinforcement learning (RL) with assured satisfaction of hard state constraints during training has recently received a lot of attention. Safety filters, e.g., based on control barrier functions (CBFs), provide a promising way for safe RL via modifying the unsafe actions of an RL agent on the fly. Existing safety filter-based approaches typically involve learning of uncertain dynamics and quantifying the learned model error, which leads to conservative filters before a large amount of data is collected to learn a good model, thereby preventing efficient exploration. This paper presents a method for safe and efficient RL using disturbance observers (DOBs) and control barrier functions (CBFs). Unlike most existing safe RL methods that deal with hard state constraints, our method does not involve model learning, and leverages DOBs to accurately estimate the pointwise value of the uncertainty, which is then incorporated into a robust CBF condition to generate safe actions. The DOB-based CBF can be used as a safety filter with model-free RL algorithms by minimally modifying the actions of an RL agent whenever necessary to ensure safety throughout the learning process. Simulation results on a unicycle and a 2D quadrotor demonstrate that the proposed method outperforms a state-of-the-art safe RL algorithm using CBFs and Gaussian processes-based model learning, in terms of safety violation rate, and sample and computational efficiency.",
    "url": "https://www.semanticscholar.org/paper/9b5aa6a75d8e9f0e68d12e3b331acad6f32667d4",
    "pdf_url": "https://arxiv.org/pdf/2211.17250.pdf",
    "venue": "Conference on Learning for Dynamics & Control",
    "publicationDate": "2022-11-30",
    "externalIds": {
      "DBLP": "conf/l4dc/ChengZH23",
      "ArXiv": "2211.17250",
      "CorpusId": 254095888
    },
    "references": [
      {
        "paperId": "1f1948540fe68850cb16adebcb1602c606ca17d9",
        "title": "Robust Safe Control Synthesis with Disturbance Observer-Based Control Barrier Functions"
      },
      {
        "paperId": "5cd269d0aa2fad218d39dacdd380ebb9244b88c5",
        "title": "High-Order Control Barrier Functions"
      },
      {
        "paperId": "d6e783bce3b8e3ad082c2757235c34cb86c4e653",
        "title": "Safe Learning in Robotics: From Learning-Based Control to Safe Reinforcement Learning"
      },
      {
        "paperId": "77b258d96cd0bb83967252e366749c4053e2b1f8",
        "title": "Adaptive Robust Quadratic Programs using Control Lyapunov and Barrier Functions"
      },
      {
        "paperId": "536c35a136e327a96be2446292936cd9141b10bb",
        "title": "Probabilistic Model Predictive Safety Certification for Learning-Based Control"
      },
      {
        "paperId": "75f7228bb7a12d16150fd78d84527b3703648829",
        "title": "Uniform Error Bounds for Gaussian Process Regression with Application to Safe Control"
      },
      {
        "paperId": "adcd4bfd88213da0c33d3cb7057411dd15b72c7a",
        "title": "End-to-End Safe Reinforcement Learning through Barrier Functions for Safety-Critical Continuous Control Tasks"
      },
      {
        "paperId": "8e655458d7a9aa2a7a1e67fe5cdf2e22fd65a8df",
        "title": "Barrier-Certified Adaptive Reinforcement Learning With Applications to Brushbot Navigation"
      },
      {
        "paperId": "811df72e210e20de99719539505da54762a11c6d",
        "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor"
      },
      {
        "paperId": "d628275b65c61ec409a5af8d88b5c0260bd9f83d",
        "title": "Adaptive model predictive control of nonlinear systems with state\u2010dependent uncertainties"
      },
      {
        "paperId": "e4b4066e46fd160cb84e246b0895a4cd674d8ce4",
        "title": "Safe Reinforcement Learning via Shielding"
      },
      {
        "paperId": "171bfa2abddd30ad177cd620c86b7f8fa64964d1",
        "title": "A General Safety Framework for Learning-Based Control in Uncertain Robotic Systems"
      },
      {
        "paperId": "907e56f67aedec0cdb2d5fe25f880d0203255d0b",
        "title": "Evaluation of an L1 Adaptive Flight Control Law on Calspan\u2019s Variable-Stability Learjet"
      },
      {
        "paperId": "291a176dc906f31e8faf760911a61f0ca9bde444",
        "title": "Control Barrier Function Based Quadratic Programs for Safety Critical Systems"
      },
      {
        "paperId": "45730dcc4d40c9d04bce45eccee22da826a22080",
        "title": "Optimal robust control for constrained nonlinear hybrid systems with application to bipedal locomotion"
      },
      {
        "paperId": "0ecf1594b8a3fada8bd0bb45c55f2e6c8b300746",
        "title": "L1 Adaptive Control Theory - Guaranteed Robustness with Fast Adaptation"
      },
      {
        "paperId": "8b3fb7ec4d54dd1dd8e775bf98491bb3e1f71eab",
        "title": "Safe Model-Based Reinforcement Learning Using Robust Control Barrier Functions"
      },
      {
        "paperId": null,
        "title": "L 1 - GP : L 1 adaptive control with Bayesian learning"
      },
      {
        "paperId": "63cd9346b2c9c546ea98954bf81dce6f09053f01",
        "title": "Recovery of Desired Flying Characteristics with an L1 Adaptive Control Law: Flight Test Results on Calspan's VSS Learjet"
      },
      {
        "paperId": "ca7c3d7bb51ef3fe42a15eb54e5f0a78875ad071",
        "title": "Disturbance-Observer-Based Control and Related Methods\u2014An Overview"
      },
      {
        "paperId": "c0f2c4104ef6e36bb67022001179887e6600d24d",
        "title": "A comprehensive survey on safe reinforcement learning"
      },
      {
        "paperId": null,
        "title": "Improving the robustness of reinforcement learning policies with L 1 adaptive control"
      }
    ],
    "cited_by": [
      {
        "paperId": "57de18482a43d72eca7132bdda62486ecfc55ed4",
        "title": "Revived transformation-based transfer learning safety control: A geometric interpretation"
      },
      {
        "paperId": "ed504ae1e9fc8767e7c8b25772ebd983c8345dc9",
        "title": "A Review On Safe Reinforcement Learning Using Lyapunov and Barrier Functions"
      },
      {
        "paperId": "01714c4da16ac7f6f3c9f639d593f4e16a732b74",
        "title": "One Subgoal at a Time: Zero-Shot Generalization to Arbitrary Linear Temporal Logic Requirements in Multi-Task Reinforcement Learning"
      },
      {
        "paperId": "b33d8420119c257b446251e7e8ddc47016e1d1fa",
        "title": "Hierarchical Multi-Agent Reinforcement Learning with Control Barrier Functions for Safety-Critical Autonomous Systems"
      },
      {
        "paperId": "5f148c77abebd10d7a74f56088b378af4be6bd08",
        "title": "Safe Reinforcement Learning with Constraints: A Survey"
      },
      {
        "paperId": "3999a531f607113b3e730eee46107db34b2933e5",
        "title": "DBaS-Log-MPPI: Efficient and Safe Trajectory Optimization via Barrier States"
      },
      {
        "paperId": "7c83a2598be6c1ad3e1de2d1d0fdec6985b34744",
        "title": "Back to Base: Towards Hands-Off Learning via Safe Resets with Reach-Avoid Safety Filters"
      },
      {
        "paperId": "7a281653934ed209d4ef91df508cd93b5d3874fb",
        "title": "Safe Control Against Uncertainty: A Comprehensive Review of Control Barrier Function Strategies"
      },
      {
        "paperId": "c75dd8f1fb1fa32fe858b5ee9e34e3b1216d5fed",
        "title": "Disturbance Observer-based Control Barrier Functions with Residual Model Learning for Safe Reinforcement Learning"
      },
      {
        "paperId": "8eafad9ef80ad511401f6360d825d535b09d32a2",
        "title": "Composite\u2010observer\u2010based asynchronous control for hidden Markov nonlinear systems with disturbances"
      },
      {
        "paperId": "aeef7112d8741451051f84161e45b8c11e813f02",
        "title": "Effect of coal moisture content on gas desorption and diffusion: A theoretical model and numerical solution"
      },
      {
        "paperId": "6d78f3342e70065f691e25aa74bd35b206636fa8",
        "title": "Resilient Estimator-based Control Barrier Functions for Dynamical Systems with Disturbances and Noise"
      },
      {
        "paperId": "7b50f70ac5aa391f3c6f2dabd8157eb4b8628332",
        "title": "The New Agronomists: Language Models are Experts in Crop Management"
      },
      {
        "paperId": "ff41838ee635b902e3f8c316b4df19dcec7745d5",
        "title": "Gradient Shaping for Multi-Constraint Safe Reinforcement Learning"
      },
      {
        "paperId": "a6b57110eb045f108f69b328ef9e0c80de1a8f36",
        "title": "RRT Guided Model Predictive Path Integral Method"
      },
      {
        "paperId": "d6a99c13b0cced7e56d7959a1e87b69947c819b2",
        "title": "Safe Control for Nonlinear Systems under Faults and Attacks via Control Barrier Functions"
      },
      {
        "paperId": "7e1aa6105e15438ac095f96dd9ee57f842e6b789",
        "title": "MultiHyRL: Robust Hybrid RL for Obstacle Avoidance against Adversarial Attacks on the Observation Space"
      },
      {
        "paperId": "28a977c4051548cc87f3a283f7fc76c9b56057d6",
        "title": "Convex Synthesis of Control Barrier Functions Under Input Constraints"
      }
    ],
    "score": 6.0
  },
  {
    "id": "6c66fc8000da4d80bb57e60667e35a051016144a",
    "title": "Safe reinforcement learning for multi-energy management systems with known constraint functions",
    "authors": [
      "Glenn Ceusters",
      "L. R. Camargo",
      "R. Franke",
      "Ann Now'e",
      "M. Messagie"
    ],
    "year": 2022,
    "citationCount": 18,
    "abstract": ": Reinforcement learning (RL) is a promising optimal control technique for multi-energy management systems. It does not require a model a priori - reducing the upfront and ongoing project-speci\ufb01c engineering effort and is capable of learning better representations of the underlying system dynamics. However, vanilla RL does not provide constraint satisfaction guarantees - resulting in various unsafe interactions within its safety-critical environment. In this paper, we present two novel safe RL methods, namely SafeFallback and GiveSafe, where the safety constraint formulation is decoupled from the RL formulation and which provides hard-constraint satisfaction guarantees both during training (exploration) and exploitation of the (close-to) optimal policy. In a simulated multi-energy systems case study we have shown that both methods start with a signi\ufb01cantly higher utility (i.e. useful policy) compared to a vanilla RL benchmark (94,6% and 82,8% compared to 35,5%) and that the proposed SafeFallback method even can outperform the vanilla RL benchmark (102,9% to 100%). We conclude that both methods are viably safety constraint handling techniques capable beyond RL, as demonstrated with random agents while still providing hard-constraint guarantees. Finally, we propose fundamental future work to i.a. improve the constraint functions itself as more data becomes available.",
    "url": "https://www.semanticscholar.org/paper/6c66fc8000da4d80bb57e60667e35a051016144a",
    "pdf_url": "https://arxiv.org/pdf/2207.03830.pdf",
    "venue": "Energy and AI",
    "publicationDate": "2022-07-08",
    "externalIds": {
      "ArXiv": "2207.03830",
      "DBLP": "journals/corr/abs-2207-03830",
      "DOI": "10.48550/arXiv.2207.03830",
      "CorpusId": 250408302
    },
    "references": [
      {
        "paperId": "adac9f9eb1809e1982afcd0de7cfdbf61cb78b3a",
        "title": "DIP-QL: A Novel Reinforcement Learning Method for Constrained Industrial Systems"
      },
      {
        "paperId": "7912cb9290d93ced3abe5b88e2f86b649b2ab3d4",
        "title": "Data-driven stochastic energy management of multi energy system using deep reinforcement learning"
      },
      {
        "paperId": "83aa86ae63e610a9b68eba3c802609a3eab4f0c8",
        "title": "Advances in energy flexible buildings\u2012perspectives and challenges"
      },
      {
        "paperId": "c498741cadd66a56e36d9e63c86a5bd629738ac6",
        "title": "A multi-agent deep reinforcement learning approach enabled distributed energy management schedule for the coordinate control of multi-energy hub with gas, electricity, and freshwater"
      },
      {
        "paperId": "1ad8ddbdc5ed602638f893cd157f984a12cc497e",
        "title": "Energy Management Based on Multi-Agent Deep Reinforcement Learning for A Multi-Energy Industrial Park"
      },
      {
        "paperId": "a8f1177a218a8ce8038c45aa757566d91debfeca",
        "title": "Real-world challenges for multi-agent reinforcement learning in grid-interactive buildings"
      },
      {
        "paperId": "92795854967af72f4ba2693477feaf5e0c9ad703",
        "title": "Soft actor-critic \u2013based multi-objective optimized energy conversion and management strategy for integrated energy systems with renewable energy"
      },
      {
        "paperId": "d6e783bce3b8e3ad082c2757235c34cb86c4e653",
        "title": "Safe Learning in Robotics: From Learning-Based Control to Safe Reinforcement Learning"
      },
      {
        "paperId": "5f1adc14a77fb61aa463fac728397bd32e00b617",
        "title": "Challenges of real-world reinforcement learning: definitions, benchmarks and analysis"
      },
      {
        "paperId": "239ec14de8d308892c93c27f0cd32e221e71c64a",
        "title": "Model-predictive control and reinforcement learning in multi-energy system case studies"
      },
      {
        "paperId": "59303b679b5b81f26848931b1ac13f0bddc1a2b1",
        "title": "Decomposed Soft Actor-Critic Method for Cooperative Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "127d6cd7da419cf02a5ae6e84e3035323509a61b",
        "title": "Applications of reinforcement learning in energy systems"
      },
      {
        "paperId": "17ecfb3587e912ae07d339ada016c33556634f04",
        "title": "Multi-Energy Scheduling of an Industrial Integrated Energy System by Reinforcement Learning-Based Differential Evolution"
      },
      {
        "paperId": "d53087ba1bf6496b441dfef3df6589a0cc23b2c7",
        "title": "Multiagent Reinforcement Learning for Energy Management in Residential Buildings"
      },
      {
        "paperId": "9a8b32c211666e4614a1d35e8e6cd7bf6025db89",
        "title": "Reinforcement Learning and Its Applications in Modern Power and Energy Systems: A Review"
      },
      {
        "paperId": "2d635edc8dcc47e69b28bb1ca48c0ef35605d9c4",
        "title": "Dynamic energy conversion and management strategy for an integrated electricity and natural gas system with renewable energy: Deep reinforcement learning approach"
      },
      {
        "paperId": "929b1db3ca8f2b742c3f8c09ae740b7acdf28060",
        "title": "Model-Free Real-Time Autonomous Control for a Residential Multi-Energy System Using Deep Reinforcement Learning"
      },
      {
        "paperId": "0d780e3864fbb2f43bd6203087b9efcea0a23f24",
        "title": "Cooperative Wind Farm Control With Deep Reinforcement Learning and Knowledge-Assisted Learning"
      },
      {
        "paperId": "e0530056f67ffc2659c49e0c0476ac4c88ed50a3",
        "title": "Deep reinforcement learning\u2013based approach for optimizing energy conversion in integrated electrical and heating system with renewable energy"
      },
      {
        "paperId": "8fc5cd104777462cd5c440858a2fa0941fef4325",
        "title": "Bi-level Multi-agents Interactive Decision-making Model in Regional Integrated Energy System"
      },
      {
        "paperId": "7404b0f9067f9fb07625ae3be41ec36a99e55913",
        "title": "ModelicaGym: applying reinforcement learning to Modelica models"
      },
      {
        "paperId": "53f86b5f5f0289faf5ba936c63bf83194c887774",
        "title": "A Learning-Based Power Management Method for Networked Microgrids Under Incomplete Information"
      },
      {
        "paperId": "25dc385ee9dc80e9b492fbb4376a49c55b7573d7",
        "title": "Battery Scheduling in a Residential Multi-Carrier Energy System Using Reinforcement Learning"
      },
      {
        "paperId": "4debb99c0c63bfaa97dd433bc2828e4dac81c48b",
        "title": "Addressing Function Approximation Error in Actor-Critic Methods"
      },
      {
        "paperId": "70760635a1e117c0ae10f879075021b01b456b04",
        "title": "OptLayer - Practical Constrained Optimization for Deep Reinforcement Learning in the Real World"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "f634a4a182a83d15d511c0c45d1c0609e2758dcf",
        "title": "From system model to optimal control - A tool chain for the efficient solution of optimal control problems"
      },
      {
        "paperId": "2b10281297ee001a9f3f4ea1aa9bea6b638c27df",
        "title": "OpenAI Gym"
      },
      {
        "paperId": "a3d2cc0c0405ee0d424dbf7970494a1de2a0c98f",
        "title": "Demand side management for a residential customer in multi-energy systems"
      },
      {
        "paperId": "bffb46bcfbe5d158db371f66beeb265fd1453e0d",
        "title": "Dynamic Energy Management System for a Smart Microgrid"
      },
      {
        "paperId": "024006d4c2a89f7acacc6e4438d156525b60a98f",
        "title": "Continuous control with deep reinforcement learning"
      },
      {
        "paperId": "43d704c589f202869b7801c0e3a9250dcf4095e2",
        "title": "Applying reinforcement learning method to optimize an Energy Hub operation in the smart grid"
      },
      {
        "paperId": "79bfde000254ce19046f338407968ba96c332da8",
        "title": "Trade-off between environmental and economic objectives in the optimization of multi-energy systems"
      },
      {
        "paperId": "28f7b2b57fbb0ee7f6c974b504be1bd9f466bb80",
        "title": "FEEDBACK CONTROL FOR OPTIMAL PROCESS OPERATION"
      },
      {
        "paperId": "948eba8d8807be81520bde63631a02bf332e6ceb",
        "title": "Physical system modeling with Modelica"
      },
      {
        "paperId": "59b50a775542e87f078db35b868ac10ab43d4c75",
        "title": "Learning from delayed rewards"
      },
      {
        "paperId": "e3fc5b5627af62ee6981a02090cf6bae368202d7",
        "title": "Stable-Baselines3: Reliable Reinforcement Learning Implementations"
      },
      {
        "paperId": "343c46c800977f0cee6c4d158ec4ba1a368b3ddb",
        "title": "Reinforcement learning in sustainable energy and electric systems: a survey"
      },
      {
        "paperId": null,
        "title": "Twin Delayed DDPG \u2014 Spinning Up documentation"
      },
      {
        "paperId": "e0a7bd713903f781cdf73f4326e365ae39363244",
        "title": "PyFMI: A Python Package for Simulation of Coupled Dynamic Models with the Functional Mock-up Interface"
      },
      {
        "paperId": "c0f2c4104ef6e36bb67022001179887e6600d24d",
        "title": "A comprehensive survey on safe reinforcement learning"
      }
    ],
    "cited_by": [
      {
        "paperId": "4cc669e620e0ba4efbd2d42c096516d90093ba7d",
        "title": "A systematic review of reinforcement learning in Building-Integrated Photovoltaic (BIPV) optimization"
      },
      {
        "paperId": "44ec4ecf9002e833a757e0141719cfe1c5a4a58d",
        "title": "Sequential Operation of Residential Energy Hubs"
      },
      {
        "paperId": "ef3fdb48e57d675e039eb15bacb07eb170ddbfc2",
        "title": "Constrained Online Decision-Making: A Unified Framework"
      },
      {
        "paperId": "86f5da52817d013adf7a92dfc4a5ef5b9c22e553",
        "title": "Aging-aware Energy Management for Residential Multi-Carrier Energy Systems"
      },
      {
        "paperId": "b3996af797332c895af9df6bc5a6c3153f0c83fd",
        "title": "EnCortex: A General, Extensible and Scalable Framework for Decision Management in New-age Energy Systems"
      },
      {
        "paperId": "d30da52754983f8ef1af18b5f2c0e0381f03b03a",
        "title": "A critical review of safe reinforcement learning strategies in power and energy systems"
      },
      {
        "paperId": "7df0cbce6237985c39c5f2f0f5e51995fc11cb69",
        "title": "Online Systemic Energy Management Strategy of Fuel Cell System With Efficiency Enhancement"
      },
      {
        "paperId": "61bd362e7be9df55c1af3ff58d08b5d80539b780",
        "title": "A Critical Review of Safe Reinforcement Learning Techniques in Smart Grid Applications"
      },
      {
        "paperId": "f530df49a89116dceee845e4a6a5df03b9c60249",
        "title": "Real-world validation of safe reinforcement learning, model predictive control and decision tree-based home energy management systems"
      },
      {
        "paperId": "3675f6d9f1c5684c56598fedf0ef0156d61da9b8",
        "title": "Reinforcement learning for an enhanced energy flexibility controller incorporating predictive safety filter and adaptive policy updates"
      },
      {
        "paperId": "9a0d705852c8aa0b07277d9b1235d49262081865",
        "title": "A Review of Safe Reinforcement Learning Methods for Modern Power Systems"
      },
      {
        "paperId": "8852edda2c7b347e8569c1b1bddf8e7a24eea26b",
        "title": "Learning the Optimal Power Flow: Environment Design Matters"
      },
      {
        "paperId": "5858df4cc2de0b41089a0a0c5f0b390c68e61c69",
        "title": "Critical Review of Optimal Control Methods for Li\u2010Ion Batteries in Electric Vehicles"
      },
      {
        "paperId": "ef702fa9fda182b7eb2c4b720d976f3d43378fa8",
        "title": "Aging-Aware Battery Operation for Multicarrier Energy Systems"
      },
      {
        "paperId": "74eb22eed396aa62b91f4e03b2bf95a1dcdcdfd5",
        "title": "An adaptive safety layer with hard constraints for safe reinforcement learning in multi-energy management systems"
      },
      {
        "paperId": "d525a418f00a06e7c3580da5f1fc5258b4684311",
        "title": "Research and Application of Safe Reinforcement Learning in Power System"
      },
      {
        "paperId": "b1fb5611b7e0e92f1f9d2f269e6e7ce8b4d1cedb",
        "title": "Provably Safe Reinforcement Learning: Conceptual Analysis, Survey, and Benchmarking"
      },
      {
        "paperId": "cca98a9d61880c8873aa8c5775345313ffa6426b",
        "title": "Safe reinforcement learning with self-improving hard constraints for multi-energy management systems"
      }
    ],
    "score": 6.0
  },
  {
    "id": "5fd3ce235f5fcebd3d2807f710b060add527183b",
    "title": "Deep Curiosity Search: Intra-Life Exploration Improves Performance on Challenging Deep Reinforcement Learning Problems",
    "authors": [
      "C. Stanton",
      "J. Clune"
    ],
    "year": 2018,
    "citationCount": 42,
    "abstract": "Traditional exploration methods in RL require agents to perform random actions to find rewards. But these approaches struggle on sparse-reward domains like Montezuma's Revenge where the probability that any random action sequence leads to reward is extremely low. Recent algorithms have performed well on such tasks by encouraging agents to visit new states or perform new actions in relation to all prior training episodes (which we call across-training novelty). But such algorithms do not consider whether an agent exhibits intra-life novelty: doing something new within the current episode, regardless of whether those behaviors have been performed in previous episodes. We hypothesize that across-training novelty might discourage agents from revisiting initially non-rewarding states that could become important stepping stones later in training. We introduce Deep Curiosity Search (DeepCS), which encourages intra-life exploration by rewarding agents for visiting as many different states as possible within each episode, and show that DeepCS matches the performance of current state-of-the-art methods on Montezuma's Revenge. We further show that DeepCS improves exploration on Amidar, Freeway, Gravitar, and Tutankham (many of which are hard exploration games). Surprisingly, DeepCS doubles A2C performance on Seaquest, a game we would not have expected to benefit from intra-life exploration because the arena is small and already easily navigated by naive exploration techniques. In one run, DeepCS achieves a maximum training score of 80,000 points on Seaquest, higher than any methods other than Ape-X. The strong performance of DeepCS on these sparse- and dense-reward tasks suggests that encouraging intra-life novelty is an interesting, new approach for improving performance in Deep RL and motivates further research into hybridizing across-training and intra-life exploration methods.",
    "url": "https://www.semanticscholar.org/paper/5fd3ce235f5fcebd3d2807f710b060add527183b",
    "pdf_url": "https://arxiv.org/pdf/1806.00553.pdf",
    "venue": "arXiv.org",
    "publicationDate": "2018-06-01",
    "externalIds": {
      "MAG": "2807192342",
      "DBLP": "journals/corr/abs-1806-00553",
      "ArXiv": "1806.00553",
      "CorpusId": 46936815
    },
    "references": [
      {
        "paperId": "5cd5f0b313d521cdfb376ce220741eff2b21966e",
        "title": "The Mentality of Apes."
      },
      {
        "paperId": "6398cb8f2af1c988a097ed1e1cefb380195edfb8",
        "title": "(Preprint)"
      },
      {
        "paperId": "601a2d349fc26d7b82f905e924e2f91b0ac4b310",
        "title": "Distributed Prioritized Experience Replay"
      },
      {
        "paperId": "c9660db0c94edfb2b1f3ab4f08eb80acd83a1c07",
        "title": "Evolved Policy Gradients"
      },
      {
        "paperId": "2064020586d5832b55f80a7dffea1fd90a5d94dd",
        "title": "Improving Exploration in Evolution Strategies for Deep Reinforcement Learning via a Population of Novelty-Seeking Agents"
      },
      {
        "paperId": "bd484035307a97c673c2dce9a201eb93c2c21e7d",
        "title": "Learning to Segment Moving Objects"
      },
      {
        "paperId": "0ab3f7ecbdc5a33565a234215604a6ca9d155a33",
        "title": "Rainbow: Combining Improvements in Deep Reinforcement Learning"
      },
      {
        "paperId": "2b6f2b163372e3417b687cc43313f2a630e7bca7",
        "title": "Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation"
      },
      {
        "paperId": "4ee802a58d32aa049d549d06be440ac947b53987",
        "title": "Evolution Strategies as a Scalable Alternative to Reinforcement Learning"
      },
      {
        "paperId": "12f67fb182bc934fc95ce97acff553d83e2ca72e",
        "title": "Count-Based Exploration with Neural Density Models"
      },
      {
        "paperId": "0fcb2034e31a2bc2f12a2b1363d0d77baf445fdf",
        "title": "#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning"
      },
      {
        "paperId": "954b01151ff13aef416d27adc60cd9a076753b1a",
        "title": "RL$^2$: Fast Reinforcement Learning via Slow Reinforcement Learning"
      },
      {
        "paperId": "250b20c201df2dd3fb425a95f9be61ed8b80afda",
        "title": "Curiosity Search: Producing Generalists by Encouraging Individuals to Continually Explore and Acquire Skills throughout Their Lifetime"
      },
      {
        "paperId": "6e90fd78e8a3b98af3954aae5209703aa966603e",
        "title": "Unifying Count-Based Exploration and Intrinsic Motivation"
      },
      {
        "paperId": "d37620e6f8fe678a43e12930743281cd8cca6a66",
        "title": "Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation"
      },
      {
        "paperId": "69e76e16740ed69f4dc55361a3d319ac2f1293dd",
        "title": "Asynchronous Methods for Deep Reinforcement Learning"
      },
      {
        "paperId": "c6170fa90d3b2efede5a2e1660cb23e1c824f2ca",
        "title": "Prioritized Experience Replay"
      },
      {
        "paperId": "3b9732bb07dc99bde5e1f9f75251c6ea5039373e",
        "title": "Deep Reinforcement Learning with Double Q-Learning"
      },
      {
        "paperId": "779bb1441b3f06eab3eb8424336920f6dc10827c",
        "title": "Innovation Engines: Automated Creativity and Improved Stochastic Optimization via Deep Learning"
      },
      {
        "paperId": "340f48901f72278f6bf78a04ee5b01df208cc508",
        "title": "Human-level control through deep reinforcement learning"
      },
      {
        "paperId": "3523710ea363615bb52651f3279bcd350aa65dab",
        "title": "Conflict, arousal, and curiosity"
      },
      {
        "paperId": "99818471743332911db05b1ea9bbab883188589b",
        "title": "Learning to segment moving objects in videos"
      },
      {
        "paperId": "1a632fb89b6b05dc16fbc026d86e390e22ca6ac3",
        "title": "Robots that can adapt like animals"
      },
      {
        "paperId": "f6ca9c148417d4167ba8b72f185a35649dc4b446",
        "title": "Skip Context Tree Switching"
      },
      {
        "paperId": "e85c97028149234fc7d0d3faeb3a7f4a50715b85",
        "title": "Generic behaviour similarity measures for evolutionary swarm robotics"
      },
      {
        "paperId": "1e0c3dd886c830783ade3f4bcb09fde4d7975ce0",
        "title": "Innovative problem solving by wild spotted hyenas"
      },
      {
        "paperId": "9c157d7712596c3ae4ad7bcf2ecc25c424490db0",
        "title": "On the deleterious effects of a priori objectives on evolution and representation"
      },
      {
        "paperId": "0de77eceda6308618132204b28755ac1e63648c5",
        "title": "Abandoning Objectives: Evolution Through the Search for Novelty Alone"
      },
      {
        "paperId": "33224ad0cdf6e2dc4893194dd587309c7887f0ba",
        "title": "Formal Theory of Creativity, Fun, and Intrinsic Motivation (1990\u20132010)"
      },
      {
        "paperId": "4282669947ca340de9f93a9a2a38007fe542195d",
        "title": "Near-Bayesian exploration in polynomial time"
      },
      {
        "paperId": "dc649486b881e672eea6546da48c46e1f98daf32",
        "title": "Near-Optimal Reinforcement Learning in Polynomial Time"
      },
      {
        "paperId": "5b69da724e0e2c0cfec7a81de3c117cf8983fb55",
        "title": "Reward, Motivation, and Reinforcement Learning"
      },
      {
        "paperId": "d3d46afaee8a0f17b405d2f278c14c81802c2be5",
        "title": "Dopamine: generalization and bonuses"
      },
      {
        "paperId": "909a2cb5c6e4450d29e77a50b5c6b883de8e21e4",
        "title": "Mammalian Play: Training for the Unexpected"
      },
      {
        "paperId": "94066dc12fe31e96af7557838159bde598cb4f10",
        "title": "Policy Invariance Under Reward Transformations: Theory and Application to Reward Shaping"
      },
      {
        "paperId": null,
        "title": "Openai baselines."
      },
      {
        "paperId": "c7dbf5ed7e9b63e104adb4e18bfbc98f5d6afdae",
        "title": "R-MAX - A General Polynomial Time Algorithm for Near-Optimal Reinforcement Learning"
      },
      {
        "paperId": "97efafdb4a3942ab3efba53ded7413199f79c054",
        "title": "Reinforcement Learning: An Introduction"
      },
      {
        "paperId": "abafd744e85dbc3122c5463e12f9edcab6770a11",
        "title": "This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination. IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION 1 Intrinsic Motivation Systems for Autonomous Mental Development"
      },
      {
        "paperId": null,
        "title": "Intra - life exploration can improve performance on challenging deep reinforcement learning problems"
      },
      {
        "paperId": null,
        "title": "\u201cMonty python\u2019s ministry of silly walks.\u201d"
      }
    ],
    "cited_by": [
      {
        "paperId": "905cb1672c1d685c171acd8a1ed6db59e671409a",
        "title": "BILE: An Effective Behavior-based Latent Exploration Scheme for Deep Reinforcement Learning"
      },
      {
        "paperId": "55f165e9e729c9291401e2022004fba8f9d4a324",
        "title": "Diverse Prompts: Illuminating the Prompt Space of Large Language Models with MAP-Elites"
      },
      {
        "paperId": "f0b323651a2da34a202249d956ed0fdb23880b07",
        "title": "Can curiosity enhance the quality of on-demand delivery services?"
      },
      {
        "paperId": "098c52a012cfda1b5ed4aef995efbbcf03dce25c",
        "title": "Goal Exploration via Adaptive Skill Distribution for Goal-Conditioned Reinforcement Learning"
      },
      {
        "paperId": "1c592b81a8661f21008b41d8391849ce4c886ccc",
        "title": "Mnemonic Dictionary Learning for Intrinsic Motivation in Reinforcement Learning"
      },
      {
        "paperId": "d16feddfeca2617ca2127b7d134ceb78ce8a8b40",
        "title": "A Study of Global and Episodic Bonuses for Exploration in Contextual MDPs"
      },
      {
        "paperId": "a9ff9a7aaedcca29c93087a24f0b87a6a3a24862",
        "title": "An Overview of Environmental Features that Impact Deep Reinforcement Learning in Sparse-Reward Domains"
      },
      {
        "paperId": "4bb9f478cb464049100abafc77b6491b6e5b731a",
        "title": "Evolutionary Reinforcement Learning: A Survey"
      },
      {
        "paperId": "0351a103915d2f10d1915b4892a988d9b15df406",
        "title": "Exploration via Elliptical Episodic Bonuses"
      },
      {
        "paperId": "c64a99ec780d5e1c6ba242a82e72e96eb0c3ef8a",
        "title": "An Information-Theoretic Perspective on Intrinsic Motivation in Reinforcement Learning: A Survey"
      },
      {
        "paperId": "86d5b93744de12e75ac23276a337627ee7316e63",
        "title": "Improving environmental awareness for autonomous vehicles"
      },
      {
        "paperId": "da1e404ae194dcbcce198f5335f49f090403bbd7",
        "title": "Exploration in Deep Reinforcement Learning: A Survey"
      },
      {
        "paperId": "5a42c0d8f6fa4539493f4bb978d7b22ac9bbb89d",
        "title": "Evolving Neural Networks with Optimal Balance between Information Flow and Connections Cost"
      },
      {
        "paperId": "12075ea34f5fbe32ec5582786761ab34d401209b",
        "title": "Exploration in Deep Reinforcement Learning: From Single-Agent to Multiagent Domain"
      },
      {
        "paperId": "a6dd4e15b4e6b09d0c0584d64db2f335fad033f9",
        "title": "Active Reinforcement Learning over MDPs"
      },
      {
        "paperId": "a734a83f531a416b7b90ca011d9e74c3844792a9",
        "title": "Expressive Cognitive Architecture for a Curious Social Robot"
      },
      {
        "paperId": "59a348666447d32e426ccd0954b689967c6bb538",
        "title": "Learning to Play Hard Exploration Games Using Graph-Guided Self-Navigation"
      },
      {
        "paperId": "9ea792e78a07a30d9b2c80669e074e05dc42b7ee",
        "title": "Multi-task curriculum learning in a complex, visual, hard-exploration domain: Minecraft"
      },
      {
        "paperId": "788744fd40fcef45949a78be7f2925db484295cb",
        "title": "An Autonomous Emotional Virtual Character: An Approach with Deep and Goal-Parameterized Reinforcement Learning"
      },
      {
        "paperId": "a2b00e570440619bbe6a483df6b0da46b1aae665",
        "title": "Fast and slow curiosity for high-level exploration in reinforcement learning"
      },
      {
        "paperId": "654da778204b7f2a33aebbea99d31da7afd6bcc5",
        "title": "An effective maximum entropy exploration approach for deceptive game in reinforcement learning"
      },
      {
        "paperId": "0a38ed285c159e07cbf5edaf4b73ea4e045406d2",
        "title": "ACDER: Augmented Curiosity-Driven Experience Replay"
      },
      {
        "paperId": "616ac6772508e72084360158078058dfa8bda7e7",
        "title": "First return, then explore"
      },
      {
        "paperId": "bebe8ffb0c357ac0c7eea2556f817b03ee22b570",
        "title": "RIDE: Rewarding Impact-Driven Exploration for Procedurally-Generated Environments"
      },
      {
        "paperId": "086159600bede14e00f96043c733d4f3b45855aa",
        "title": "Never Give Up: Learning Directed Exploration Strategies"
      },
      {
        "paperId": "a2e43270a9b1421e452c2975e5163e2a216abeac",
        "title": "A Survey of Deep Reinforcement Learning in Video Games"
      },
      {
        "paperId": "081ef50ed2b05e92bd8d61e753a7af751fcbecff",
        "title": "Skill-based curiosity for intrinsically motivated reinforcement learning"
      },
      {
        "paperId": "6f6eb95b2bee897c3ba90cd6018d8545bed07abe",
        "title": "Self-Imitation Learning via Trajectory-Conditioned Policy for Hard-Exploration Tasks"
      },
      {
        "paperId": "895735cace0de940aa647dbafc046b7f30316fe5",
        "title": "A survey on intrinsic motivation in reinforcement learning"
      },
      {
        "paperId": "b3e20a79d802baee478351d111e22e0e513ba376",
        "title": "Efficient Exploration with Self-Imitation Learning via Trajectory-Conditioned Policy"
      },
      {
        "paperId": "c6011b09783150bcda711c8d3176385875020a60",
        "title": "\u00c9tude de la motivation intrins\u00e8que en apprentissage par renforcement"
      },
      {
        "paperId": "aac19a2af64f914d14cec25989528197639aacaf",
        "title": "Embracing curiosity eliminates the exploration-exploitation dilemma"
      },
      {
        "paperId": "42525a5143c6a87d3ab466684dfa471dc43a5bd0",
        "title": "AI-GAs: AI-generating algorithms, an alternate paradigm for producing general artificial intelligence"
      },
      {
        "paperId": "c520bf47db3360ae3a52219771390a354ed8a91f",
        "title": "Go-Explore: a New Approach for Hard-Exploration Problems"
      },
      {
        "paperId": "36cd89332d305d01605d6d08cd8452c8a752138a",
        "title": "Designing neural networks through neuroevolution"
      },
      {
        "paperId": "4cb3fd057949624aa4f0bbe7a6dcc8777ff04758",
        "title": "Exploration by Random Network Distillation"
      },
      {
        "paperId": "54cb726595cd8c03f59f49c9a97b76b4d3f932f2",
        "title": "Rethinking Exploration in Reinforcement Learning with Effective Metric-Based Exploration Bonus"
      },
      {
        "paperId": "2e5dc4b028e670d1b9ac0b4e15580cdb7197cb68",
        "title": "Exploration in Deep Reinforcement Learning: A Comprehensive Survey"
      },
      {
        "paperId": "6f1c48eaa68322de8500de8f8942ca1926ae433d",
        "title": "Curiosity eliminates the exploration-exploitation dilemma"
      },
      {
        "paperId": "91329f5b8e60ca064b58649ddac3487b7100ed9b",
        "title": "Artificial Neural Networks and Machine Learning \u2013 ICANN 2020: 29th International Conference on Artificial Neural Networks, Bratislava, Slovakia, September 15\u201318, 2020, Proceedings, Part II"
      },
      {
        "paperId": "daf93749171e3575cca5b584a4146b247692c86f",
        "title": "Learning and planning in videogames via task decomposition"
      },
      {
        "paperId": "b5c37ac043c84d1fcf33ece2f0c3db2305a3487f",
        "title": "Deep Curiosity Networks"
      }
    ],
    "score": 6.0
  },
  {
    "id": "a011901fd788fc5ad452dd3d88f9f0970ea547a8",
    "title": "QD-RL: Efficient Mixing of Quality and Diversity in Reinforcement Learning",
    "authors": [
      "Geoffrey Cideron",
      "Thomas Pierrot",
      "Nicolas Perrin",
      "Karim Beguir",
      "Olivier Sigaud"
    ],
    "year": 2020,
    "citationCount": 29,
    "abstract": "",
    "url": "https://www.semanticscholar.org/paper/a011901fd788fc5ad452dd3d88f9f0970ea547a8",
    "pdf_url": null,
    "venue": "arXiv.org",
    "publicationDate": "2020-06-15",
    "externalIds": {
      "MAG": "3035026685",
      "DBLP": "journals/corr/abs-2006-08505",
      "CorpusId": 219687015
    },
    "references": [],
    "cited_by": [
      {
        "paperId": "4a0be5039b2d462fedafec282ac19dce5746dad8",
        "title": "Diversity-Aware Policy Optimization for Large Language Model Reasoning"
      },
      {
        "paperId": "b04162687d7c992b6acdbfe370204a2bc64ac393",
        "title": "Imitation from Diverse Behaviors: Wasserstein Quality Diversity Imitation Learning with Single-Step Archive Exploration"
      },
      {
        "paperId": "9cfdb3cdb63d41de52b68f0ad6219b822a4b37d5",
        "title": "Diversity-Rewarded CFG Distillation"
      },
      {
        "paperId": "4c2bf778ad14aaddbb66a02a2ab2735f7e75f93a",
        "title": "Diversifying Policies With Non-Markov Dispersion to Expand the Solution Space"
      },
      {
        "paperId": "c7ed31463edfb640902e1104a18746be8959a689",
        "title": "Multi-Agent Reinforcement Learning with Asymmetric Representation Assisted by Multi-Objective Evolutionary Algorithms"
      },
      {
        "paperId": "db5d6598901c5c85ada57996c8164f93b8fd8458",
        "title": "Quality with Just Enough Diversity in Evolutionary Policy Search"
      },
      {
        "paperId": "25523c3e70daedcd0f1172844fcbafe30b8933f1",
        "title": "Bridging Evolutionary Algorithms and Reinforcement Learning: A Comprehensive Survey"
      },
      {
        "paperId": "97d63b20825a11679e0fceba3d8f8463d5b2d17b",
        "title": "UOEP: User-Oriented Exploration Policy for Enhancing Long-Term User Experiences in Recommender Systems"
      },
      {
        "paperId": "53f8ce42ae198befe04259c966663705944bc05a",
        "title": "Adaptive Estimation Q-learning with Uncertainty and Familiarity"
      },
      {
        "paperId": "135e90ca09b6d483b86ddf2fb9f901348f0e0d11",
        "title": "Policy Dispersion in Non-Markovian Environment"
      },
      {
        "paperId": "e03d6414dd5a3e7fcac7fe273089ca6e5ad848dd",
        "title": "Efficient Exploration using Model-Based Quality-Diversity with Gradients"
      },
      {
        "paperId": "f515a23a8ad4bd184b681aa2c776f0604dc3b46b",
        "title": "Deep Surrogate Assisted Generation of Environments"
      },
      {
        "paperId": "94747f3dc831ec30ad96da811a2aa6a690facf74",
        "title": "Promoting Quality and Diversity in Population-based Reinforcement Learning via Hierarchical Trajectory Space Exploration"
      },
      {
        "paperId": "89eed2e92270db2789cfb0cf00b387877809ad7a",
        "title": "Continuously Discovering Novel Strategies via Reward-Switching Policy Optimization"
      },
      {
        "paperId": "d0e783eeb54893c7a283ccbd044647f8f390b16e",
        "title": "Combining Evolution and Deep Reinforcement Learning for Policy Search: A Survey"
      },
      {
        "paperId": "a24c3e689c351cd5baf420609b11a7ebc1cbc0a3",
        "title": "Approximating gradients for differentiable quality diversity in reinforcement learning"
      },
      {
        "paperId": "660d660c56e120b4e36229e5531f0b38fbf3e0cd",
        "title": "Evolutionary Action Selection for Gradient-based Policy Learning"
      },
      {
        "paperId": "b2b4d7b5a94479b8d6a2e1f51fcc845392d22ca9",
        "title": "Discovering and Exploiting Sparse Rewards in a Learned Behavior Space"
      },
      {
        "paperId": "1f312f9363eecaae22b65b1d9397cc2737168143",
        "title": "Dynamics-Aware Quality-Diversity for Efficient Learning of Skill Repertoires"
      },
      {
        "paperId": "22d900452b7419ff516a7be5a420046f00406ec4",
        "title": "Few-Shot Quality-Diversity Optimization"
      },
      {
        "paperId": "ec1b017997f405656b2f7850561ef810553d8b99",
        "title": "Experience-Driven PCG via Reinforcement Learning: A Super Mario Bros Study"
      },
      {
        "paperId": "fc83e3bb81ef461f9848fae00885dbea66e76fbc",
        "title": "Policy gradient assisted MAP-Elites"
      },
      {
        "paperId": "a8c695d9755a3b514677fef3aa08c08336237560",
        "title": "Differentiable Quality Diversity"
      },
      {
        "paperId": "d880efab1b8ea66298c353d5c5589dac23b21d30",
        "title": "Selection-Expansion: A Unifying Framework for Motion-Planning and Diversity Search Algorithms"
      },
      {
        "paperId": "da7d5ef65577aafecc8764c4435685bac10e0a46",
        "title": "Sparse reward exploration via novelty search and emitters"
      },
      {
        "paperId": "59c60f49b5e4814d8b6321ab629bd897bde9109c",
        "title": "Offline Reinforcement Learning Hands-On"
      },
      {
        "paperId": "98abf6219d648a18292686670668aef7177dc64a",
        "title": "Quality-Similar Diversity via Population Based Reinforcement Learning"
      },
      {
        "paperId": "fed0701afdfa6896057f7d04bd30ab1328eff110",
        "title": "Evolutionary Diversity Optimization with Clustering-based Selection for Reinforcement Learning"
      },
      {
        "paperId": "80c48bcf55543f11bed49ac6d9804de3bb0e3818",
        "title": "Few-shot Quality-Diversity Optimisation"
      }
    ],
    "score": 5.800000000000001
  },
  {
    "id": "3efc894d0990faeb2f69194195d465ed64694104",
    "title": "Feudal Latent Space Exploration for Coordinated Multi-Agent Reinforcement Learning",
    "authors": [
      "Xiangyu Liu",
      "Ying Tan"
    ],
    "year": 2022,
    "citationCount": 17,
    "abstract": "",
    "url": "https://www.semanticscholar.org/paper/3efc894d0990faeb2f69194195d465ed64694104",
    "pdf_url": "https://doi.org/10.1109/TNNLS.2022.3146201",
    "venue": "IEEE Transactions on Neural Networks and Learning Systems",
    "publicationDate": "2022-02-15",
    "externalIds": {
      "DBLP": "journals/tnn/LiuT23a",
      "DOI": "10.1109/TNNLS.2022.3146201",
      "CorpusId": 246864772,
      "PubMed": "35167482"
    },
    "references": [],
    "cited_by": [
      {
        "paperId": "5c6f86952e2f0ea2e52db682e3563d632096b0af",
        "title": "LLT: Learning Latent Tactics in Uncertainty Cooperative Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "da17a5f0be9537a11c3a4e5cb98ccde4e2f1a495",
        "title": "Hierarchical Causal Discovery From Large-Scale Observed Variables"
      },
      {
        "paperId": "1d2267827822b0da7c14e9058bec49d693123fb8",
        "title": "A Network Connectivity-Aware Reinforcement Learning Method for Task Exploration and Allocation"
      },
      {
        "paperId": "97c1f0b0c664d75fa1401c56c339487ae4424aa0",
        "title": "Learning Multi-Intersection Traffic Signal Control via Coevolutionary Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "17a4e9ee072cf54a56bd68c7201eb29ac4dfc441",
        "title": "Optimization of Urban Target Area Accessibility for Multi-UAV Data Gathering Based on Deep Reinforcement Learning"
      },
      {
        "paperId": "6870bc255b6a03f00dea8ddca6593e5cff0986bb",
        "title": "Boosting Weak-to-Strong Agents in Multiagent Reinforcement Learning via Balanced PPO"
      },
      {
        "paperId": "cdc0a91641fdd24a279ceb5ff3c17b1b68eff9cb",
        "title": "Local connection reinforcement learning method for efficient robotic peg-in-hole assembly"
      },
      {
        "paperId": "e927fda3c132d8eafc486d6604d8995db874c300",
        "title": "Consensus Control for Multi-Agent Systems Based on Distributed Unknown Input Observers"
      },
      {
        "paperId": "18388d8ee4ef6bdf54b0c41ac35d95dc96a5eba0",
        "title": "DSDF: Coordinated look-ahead strategy in multi-agent reinforcement learning with noisy agents"
      },
      {
        "paperId": "b9a7ae902d4c7c08380e2aacebe7e0fd3345feb6",
        "title": "A Contrastive-Enhanced Ensemble Framework for Efficient Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "f69f253b518ec835df744b79724d4550fc2915d2",
        "title": "Credit assignment with predictive contribution measurement in multi-agent reinforcement learning"
      },
      {
        "paperId": "c3a99957a7d7e5d401a2fe1aa78a34699f147e24",
        "title": "Graph Exploration for Effective Multiagent Q-Learning"
      },
      {
        "paperId": "10b81feb827394fd6c906a1b2799c749288bb7c7",
        "title": "SVDE: Scalable Value-Decomposition Exploration for Cooperative Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "24f5384da6cd78cb2f32a2b75d3365e0c0b431d8",
        "title": "Local Connection Reinforcement Learning Method for Efficient Control of Robotic Peg-in-Hole Assembly"
      },
      {
        "paperId": "46e6ab62d7dd90b7c14281e4b5590731c71c6b0a",
        "title": "Collaborative Target Search With a Visual Drone Swarm: An Adaptive Curriculum Embedded Multistage Reinforcement Learning Approach"
      },
      {
        "paperId": "5c56f46425a29d1d8dd4483352bd2dd9205e1624",
        "title": "A Sequential Decision Algorithm of Reinforcement Learning for Composite Action Space"
      },
      {
        "paperId": "23103697b0ef4af8042e8de42f7a0f0f22b9009c",
        "title": "A multi-agent curiosity reward model for task-oriented dialogue systems"
      }
    ],
    "score": 5.666666666666666
  },
  {
    "id": "46bd3c20e6f7a9b6e413ea9f3452965601fd6de9",
    "title": "Unsupervised Reinforcement Learning for Transferable Manipulation Skill Discovery",
    "authors": [
      "Daesol Cho",
      "Jigang Kim",
      "H. J. Kim"
    ],
    "year": 2022,
    "citationCount": 17,
    "abstract": "Current reinforcement learning (RL) in robotics often experiences difficulty in generalizing to new downstream tasks due to the innate task-specific training paradigm. To alleviate it, unsupervised RL, a framework that pre-trains the agent in a task-agnostic manner without access to the task-specific reward, leverages active exploration for distilling diverse experience into essential skills or reusable knowledge. For exploiting such benefits also in robotic manipulation, we propose an unsupervised method for transferable manipulation skill discovery that ties structured exploration toward interacting behavior and transferable skill learning. It not only enables the agent to learn interaction behavior, the key aspect of the robotic manipulation learning, without access to the environment reward, but also to generalize to arbitrary downstream manipulation tasks with the learned task-agnostic skills. Through comparative experiments, we show that our approach achieves the most diverse interacting behavior and significantly improves sample efficiency in downstream tasks including the extension to multi-object, multitask problems.",
    "url": "https://www.semanticscholar.org/paper/46bd3c20e6f7a9b6e413ea9f3452965601fd6de9",
    "pdf_url": "https://arxiv.org/pdf/2204.13906.pdf",
    "venue": "IEEE Robotics and Automation Letters",
    "publicationDate": "2022-04-29",
    "externalIds": {
      "DBLP": "journals/ral/ChoKK22a",
      "ArXiv": "2204.13906",
      "DOI": "10.1109/LRA.2022.3171915",
      "CorpusId": 248475974
    },
    "references": [
      {
        "paperId": "c47a26236cbd5446c4b1897ec5eafa8a9bfbce54",
        "title": "Self-supervised Representation Learning with Relative Predictive Coding"
      },
      {
        "paperId": "64d16d0bef64c9a36ef91877c3687e260430534e",
        "title": "Motion Planner Augmented Reinforcement Learning for Robot Manipulation in Obstructed Environments"
      },
      {
        "paperId": "ec9565cc433a3f8a35dbdb9e8376e92580a40a13",
        "title": "Neural Methods for Point-wise Dependency Estimation"
      },
      {
        "paperId": "84def8c1ae89f1f0fe197eed0c4256fbad2dc02f",
        "title": "Emergent Real-World Robotic Skills via Unsupervised Off-Policy Reinforcement Learning"
      },
      {
        "paperId": "0b9bfbd7a7f4595d99ab7f73862fbcfbac38c4d7",
        "title": "Multi-Task Reinforcement Learning with Soft Modularization"
      },
      {
        "paperId": "48b1646ae5b5d2ec05f881a2a6f51489c5c0a9f4",
        "title": "Mutual Information-based State-Control for Intrinsically Motivated Reinforcement Learning"
      },
      {
        "paperId": "449c5660d637741f7aa7ff42549c32b43c9968bf",
        "title": "Gradient Surgery for Multi-Task Learning"
      },
      {
        "paperId": "0bc855f84668b35cb65618d996d09f6e434d28c9",
        "title": "Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning"
      },
      {
        "paperId": "59a916cdc943f0282908e6f3fa0360f4c5fb78d0",
        "title": "Stabilizing Transformers for Reinforcement Learning"
      },
      {
        "paperId": "cbf595e704fb332bd321d34e7f293fefcd64176e",
        "title": "Regularizing Deep Multi-Task Networks using Orthogonal Gradients"
      },
      {
        "paperId": "ffb3886a253ff927bcc46b78e00409893865a68e",
        "title": "Dynamics-Aware Unsupervised Discovery of Skills"
      },
      {
        "paperId": "84771e205117b8bdcd0982c35b4fcd514d183afd",
        "title": "Composing Task-Agnostic Policies with Deep Reinforcement Learning"
      },
      {
        "paperId": "87a40a2d506fec06d67dc8f14ce2c708a789a2b4",
        "title": "Self-Supervised Exploration via Disagreement"
      },
      {
        "paperId": "7aea82f3b7726b0bd3bb3931dff10c93d1907abf",
        "title": "MCP: Learning Composable Hierarchical Control with Multiplicative Compositional Policies"
      },
      {
        "paperId": "4cb3fd057949624aa4f0bbe7a6dcc8777ff04758",
        "title": "Exploration by Random Network Distillation"
      },
      {
        "paperId": "b65a6be07ce9c86797e6917258cf5ba45273ee73",
        "title": "Unsupervised Control Through Non-Parametric Discriminative Rewards"
      },
      {
        "paperId": "5b01eaef54a653ba03ddd5a978690380fbc19bfc",
        "title": "Diversity is All You Need: Learning Skills without a Reward Function"
      },
      {
        "paperId": "811df72e210e20de99719539505da54762a11c6d",
        "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor"
      },
      {
        "paperId": "429ed4c9845d0abd1f8204e1d7705919559bc2a2",
        "title": "Hindsight Experience Replay"
      },
      {
        "paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
        "title": "Attention is All you Need"
      },
      {
        "paperId": "225ab689f41cef1dc18237ef5dab059a49950abf",
        "title": "Curiosity-Driven Exploration by Self-Supervised Prediction"
      },
      {
        "paperId": "12f67fb182bc934fc95ce97acff553d83e2ca72e",
        "title": "Count-Based Exploration with Neural Density Models"
      },
      {
        "paperId": "8423cc50c18d68f797adaa4f571f5e4efbe325a5",
        "title": "A Laplacian Framework for Option Discovery in Reinforcement Learning"
      },
      {
        "paperId": "e37b999f0c96d7136db07b0185b837d5decd599a",
        "title": "Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates"
      },
      {
        "paperId": "8fab7d7dfd233fd5d19bc2641b4c1ca74fc7bc6a",
        "title": "Learning modular neural network policies for multi-task and multi-robot transfer"
      },
      {
        "paperId": "53c9443e4e667170acc60ca1b31a0ec7151fe753",
        "title": "Progressive Neural Networks"
      },
      {
        "paperId": "6e90fd78e8a3b98af3954aae5209703aa966603e",
        "title": "Unifying Count-Based Exploration and Intrinsic Motivation"
      },
      {
        "paperId": "ffdcad14d2f6a12f607b59f88da4a939f4821691",
        "title": "f-GAN: Training Generative Neural Samplers using Variational Divergence Minimization"
      }
    ],
    "cited_by": [
      {
        "paperId": "9bd7088ea70df34f5993c5d797efc4dc15fd2660",
        "title": "PDRL: Towards Deeper States and Further Behaviors in Unsupervised Skill Discovery by Progressive Diversity"
      },
      {
        "paperId": "6ab7a68a824d9b1ef92d7f9ffe766902d50911ad",
        "title": "Goal-Conditioned Reinforcement Learning With Adaptive Intrinsic Curiosity and Universal Value Network Fitting for Robotic Manipulation"
      },
      {
        "paperId": "a69342fae99b2ba2149f768704caec090223e7d7",
        "title": "CoSD: Balancing behavioral consistency and diversity in unsupervised skill discovery"
      },
      {
        "paperId": "a7a335450bb864f66bc5263bd79afbc435d12bdf",
        "title": "Unsupervised Skill Discovery for Robotic Manipulation through Automatic Task Generation"
      },
      {
        "paperId": "4e55bd70a0c9c845cb8f79de10551ff7a751f57c",
        "title": "Unsupervised Reinforcement Learning for Multi-Task Autonomous Driving: Expanding Skills and Cultivating Curiosity"
      },
      {
        "paperId": "d33a2c50c4a8c12f2b78b905d9a5cca10920338b",
        "title": "Refine to the essence: Less-redundant skill learning via diversity clustering"
      },
      {
        "paperId": "327d149454994765d689d7c656c73c5a14549086",
        "title": "A Pseudo-Hierarchical Planning Framework with Dynamic-Aware Reinforcement Learning for Autonomous Driving"
      },
      {
        "paperId": "b4c1587e4e43b77991211cddf214eb7232e792d0",
        "title": "Unsupervised Object Interaction Learning with Counterfactual Dynamics Models"
      },
      {
        "paperId": "e0817781261f075fca19aa5eac370b689eb90c43",
        "title": "SLIM: Skill Learning with Multiple Critics"
      },
      {
        "paperId": "09b2fd5423eeb5f08cc23d7c928a59e93ee76d60",
        "title": "Spatial Transform Soft Actor-Critic for Robot Grasping Skill Learning"
      },
      {
        "paperId": "7d82f1021c3f385ec2fdd7b1a06ea71430e650e8",
        "title": "Hierarchical reinforcement learning with adaptive scheduling for robot control"
      },
      {
        "paperId": "92551248ee94ae1fabbb032f47efe865e27aa27f",
        "title": "Hierarchical reinforcement learning with unlimited option scheduling for sparse rewards in continuous spaces"
      },
      {
        "paperId": "84c50dfd732b6f4ff5d79d94bb4a905b007ba7c1",
        "title": "Distributional and hierarchical reinforcement learning for physical systems with noisy state observations and exogenous perturbations"
      },
      {
        "paperId": "059fb62256b46a6020db103212124025a49804a6",
        "title": "Safety-Aware Unsupervised Skill Discovery"
      },
      {
        "paperId": "c18bed90f9781e0750ffc634143015919886725b",
        "title": "POLTER: Policy Trajectory Ensemble Regularization for Unsupervised Reinforcement Learning"
      },
      {
        "paperId": "a2d5d9d092d1a149193d35b50f0ceb95c76d18ad",
        "title": "Discovering and Exploiting Skills in Hierarchical Reinforcement Learning"
      },
      {
        "paperId": "d239bb8e26e388f9053f51800239db1360ba0bf5",
        "title": "A Toolkit for Encouraging Safe Diversity in Skill Discovery"
      }
    ],
    "score": 5.666666666666666
  },
  {
    "id": "1d4288c47a7802575d2a0d231d1283a4f225a85b",
    "title": "MetaCURE: Meta Reinforcement Learning with Empowerment-Driven Exploration",
    "authors": [
      "Jin Zhang",
      "Jianhao Wang",
      "Hao Hu",
      "Tong Chen",
      "Yingfeng Chen",
      "Changjie Fan",
      "Chongjie Zhang"
    ],
    "year": 2020,
    "citationCount": 28,
    "abstract": "Meta reinforcement learning (meta-RL) extracts knowledge from previous tasks and achieves fast adaptation to new tasks. Despite recent progress, efficient exploration in meta-RL remains a key challenge in sparse-reward tasks, as it requires quickly finding informative task-relevant experiences in both meta-training and adaptation. To address this challenge, we explicitly model an exploration policy learning problem for meta-RL, which is separated from exploitation policy learning, and introduce a novel empowerment-driven exploration objective, which aims to maximize information gain for task identification. We derive a corresponding intrinsic reward and develop a new off-policy meta-RL framework, which efficiently learns separate context-aware exploration and exploitation policies by sharing the knowledge of task inference. Experimental evaluation shows that our meta-RL method significantly outperforms state-of-the-art baselines on various sparse-reward MuJoCo locomotion tasks and more complex sparse-reward Meta-World tasks.",
    "url": "https://www.semanticscholar.org/paper/1d4288c47a7802575d2a0d231d1283a4f225a85b",
    "pdf_url": "https://arxiv.org/pdf/2006.08170.pdf",
    "venue": "International Conference on Machine Learning",
    "publicationDate": "2020-06-15",
    "externalIds": {
      "DBLP": "conf/icml/ZhangWHCCFZ21",
      "ArXiv": "2006.08170",
      "CorpusId": 235826403
    },
    "references": [
      {
        "paperId": "16ce156a802e43d34929f0b8d32b93db7e852690",
        "title": "Context-aware Dynamics Model for Generalization in Model-Based Reinforcement Learning"
      },
      {
        "paperId": "0cc956565c7d249d4197eeb1dbab6523c648b2c9",
        "title": "Dream to Control: Learning Behaviors by Latent Imagination"
      },
      {
        "paperId": "3c8a456509e6c0805354bd40a35e3f2dbf8069b1",
        "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library"
      },
      {
        "paperId": "34075ea242d6d76e360fb9fe7998ba859151dbca",
        "title": "MAME : Model-Agnostic Meta-Exploration"
      },
      {
        "paperId": "361c00b22e29d0816ca896513d2c165e26399821",
        "title": "Grandmaster level in StarCraft II using multi-agent reinforcement learning"
      },
      {
        "paperId": "0bc855f84668b35cb65618d996d09f6e434d28c9",
        "title": "Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning"
      },
      {
        "paperId": "361e953f792a585496834ee14216b94d0ce9ae74",
        "title": "VariBAD: A Very Good Method for Bayes-Adaptive Deep RL via Meta-Learning"
      },
      {
        "paperId": "633266814150ab66f0474d7b9a6807b729c7e0af",
        "title": "Environment Probing Interaction Policies"
      },
      {
        "paperId": "9a3c9a0ac460c7891d03d56146f2d566c7e0fb08",
        "title": "Meta reinforcement learning as task inference"
      },
      {
        "paperId": "4625628163a2ee0e6cd320cd7a14b4ccded2a631",
        "title": "Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables"
      },
      {
        "paperId": "c456941a270cb2040eed4abfb39150508caf920c",
        "title": "ProMP: Proximal Meta-Policy Search"
      },
      {
        "paperId": "4cb3fd057949624aa4f0bbe7a6dcc8777ff04758",
        "title": "Exploration by Random Network Distillation"
      },
      {
        "paperId": "ca14dce53be20d3d23d4f0db844a8389ab619db3",
        "title": "Large-Scale Study of Curiosity-Driven Learning"
      },
      {
        "paperId": "68c108795deef06fa929d1f6e96b75dbf7ce8531",
        "title": "Meta-Reinforcement Learning of Structured Exploration Strategies"
      },
      {
        "paperId": "b80991d12b41a5a68dc14dd87b692c0f903ceb9c",
        "title": "Some Considerations on Learning to Explore via Meta-Reinforcement Learning"
      },
      {
        "paperId": "811df72e210e20de99719539505da54762a11c6d",
        "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor"
      },
      {
        "paperId": "59d86da5c5936e7a236678bf5eaaa7753c226fb1",
        "title": "Stochastic Variational Video Prediction"
      },
      {
        "paperId": "0ab3f7ecbdc5a33565a234215604a6ca9d155a33",
        "title": "Rainbow: Combining Improvements in Deep Reinforcement Learning"
      },
      {
        "paperId": "7e9c1e0d247b20a0683f4797d9ea248c3b53d424",
        "title": "A Simple Neural Attentive Meta-Learner"
      },
      {
        "paperId": "225ab689f41cef1dc18237ef5dab059a49950abf",
        "title": "Curiosity-Driven Exploration by Self-Supervised Prediction"
      },
      {
        "paperId": "c889d6f98e6d79b89c3a6adf8a921f88fa6ba518",
        "title": "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks"
      },
      {
        "paperId": "954b01151ff13aef416d27adc60cd9a076753b1a",
        "title": "RL$^2$: Fast Reinforcement Learning via Slow Reinforcement Learning"
      },
      {
        "paperId": "5654ba5acbe36701dbb369206eb8a971106afcc1",
        "title": "A Translation : MURRAY S. DAVIS, That's Interesting! : Towards a Phenomenology of Sociology and a Sociology of Phenomenology"
      },
      {
        "paperId": "317cd4522b1f4a6f889743578143bb8823623f8b",
        "title": "VIME: Variational Information Maximizing Exploration"
      },
      {
        "paperId": "1a81f1155f847b268ae4bed160540c2ee3acb5a9",
        "title": "Thompson Sampling is Asymptotically Optimal in General Environments"
      },
      {
        "paperId": "e4257bc131c36504a04382290cbc27ca8bb27813",
        "title": "Action-Conditional Video Prediction using Deep Networks in Atari Games"
      },
      {
        "paperId": "2470fcf0f89082de874ac9133ccb3a8667dd89a8",
        "title": "Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models"
      },
      {
        "paperId": "0c3b69b5247ef18fd5bab1109d87a04184ea8f4b",
        "title": "A Recurrent Latent Variable Model for Sequential Data"
      },
      {
        "paperId": "5f5dc5b9a2ba710937e2c413b37b053cd673df02",
        "title": "Auto-Encoding Variational Bayes"
      },
      {
        "paperId": "789783016fb708abbc061790612ebe91273c05d3",
        "title": "(More) Efficient Reinforcement Learning via Posterior Sampling"
      },
      {
        "paperId": "b354ee518bfc1ac0d8ac447eece9edb69e92eae1",
        "title": "MuJoCo: A physics engine for model-based control"
      },
      {
        "paperId": "5df39cc393907ea78fddf461b494b4c5b1b5a2e4",
        "title": "Planning to Be Surprised: Optimal Bayesian Exploration in Dynamic Environments"
      },
      {
        "paperId": "2980dfe5c99658dc3e508d9d6e1d7f26e6fc8934",
        "title": "A possibility for implementing curiosity and boredom in model-building neural controllers"
      },
      {
        "paperId": "ee2cd1d17f833d3c157a1016a778c7c22af555a2",
        "title": "ON THE LIKELIHOOD THAT ONE UNKNOWN PROBABILITY EXCEEDS ANOTHER IN VIEW OF THE EVIDENCE OF TWO SAMPLES"
      },
      {
        "paperId": null,
        "title": "Meta-learning: from few-shot learning to rapid reinforcement learning. https://sites"
      },
      {
        "paperId": "a181fb5a42ad8fe2cc27b5542fa40384e9a8d72c",
        "title": "Deep Variational Information Bottleneck"
      },
      {
        "paperId": "2547be25e1e07728aa0966a0354e90664816d15e",
        "title": "REINFORCEMENT DRIVEN INFORMATION ACQUISITION IN NON-DETERMINISTIC ENVIRONMENTS"
      },
      {
        "paperId": "9136509aa692002a573fa18e3845568bd652e5b3",
        "title": "On learning how to learn learning strategies"
      }
    ],
    "cited_by": [
      {
        "paperId": "846f52edcdee8534b9bd2a97e13357526f439987",
        "title": "Learning Task Belief Similarity with Latent Dynamics for Meta-Reinforcement Learning"
      },
      {
        "paperId": "bddc210ec0ecc42b617fb84bd5fe331f5d294374",
        "title": "Learning to Plan Before Answering: Self-Teaching LLMs to Learn Abstract Plans for Problem Solving"
      },
      {
        "paperId": "981063181df8255e41f153907458fa657e6ba647",
        "title": "Global\u2013Local Decomposition of Contextual Representations in Meta-Reinforcement Learning"
      },
      {
        "paperId": "a2f5879e01f52c3cb922481feec40d858c400be6",
        "title": "BAMDP Shaping: a Unified Framework for Intrinsic Motivation and Reward Shaping"
      },
      {
        "paperId": "63eb1afe96024c84bd4ba178842e615dae872475",
        "title": "MESA: Cooperative Meta-Exploration in Multi-Agent Learning through Exploiting State-Action Space Structure"
      },
      {
        "paperId": "95ecd65828a96c74ea9c5f19e7494f3ba7d4a8a9",
        "title": "Context-Based Meta-Reinforcement Learning With Bayesian Nonparametric Models"
      },
      {
        "paperId": "3410b08ec0d23e89390669bae23471555846660a",
        "title": "MetaCARD: Meta-Reinforcement Learning with Task Uncertainty Feedback via Decoupled Context-Aware Reward and Dynamics Components"
      },
      {
        "paperId": "8d6d320dbe9bdd6a1ea844faaf3dc0f5fff2543b",
        "title": "Contrastive Learning-Based Bayes-Adaptive Meta-Reinforcement Learning for Active Pantograph Control in High-Speed Railways"
      },
      {
        "paperId": "c3eed6435ae609ee800c6e9daee038535af2ef22",
        "title": "Bridging State and History Representations: Understanding Self-Predictive RL"
      },
      {
        "paperId": "feb2255dbad1c829fff48922fbf0169d1b3ee4df",
        "title": "Generalizable Task Representation Learning for Offline Meta-Reinforcement Learning with Data Limitations"
      },
      {
        "paperId": "8f9c2761662a3fab59fed11f22d3b2cfe5cd7f8e",
        "title": "Context Shift Reduction for Offline Meta-Reinforcement Learning"
      },
      {
        "paperId": "b02c1be8e61f3dea42130ce21ea9ee504606a9b1",
        "title": "Efficient Symbolic Policy Learning with Differentiable Symbolic Expression"
      },
      {
        "paperId": "bdce611267d2349a176a2f77ae0c22653c542c0d",
        "title": "Continual Robot Learning Using Self-Supervised Task Inference"
      },
      {
        "paperId": "f867e0e9b02033b726c1c676d9353e05774a6562",
        "title": "First-Explore, then Exploit: Meta-Learning to Solve Hard Exploration-Exploitation Trade-Offs"
      },
      {
        "paperId": "7553c10f5b6dec33e0562430659adfabf9dff536",
        "title": "Offline Meta Reinforcement Learning with In-Distribution Online Adaptation"
      },
      {
        "paperId": "3eb75f5f35a7963b46ad4c43c68082283c9c0489",
        "title": "Accelerating exploration and representation learning with offline pre-training"
      },
      {
        "paperId": "06ad7a1b42b4a6a395ac01091e5cafeea23918ab",
        "title": "Meta-Reinforcement Learning via Exploratory Task Clustering"
      },
      {
        "paperId": "0fd6c747b48526ba4abc05b4ae9260f93718ce8f",
        "title": "Efficient Meta Reinforcement Learning for Preference-based Fast Adaptation"
      },
      {
        "paperId": "70e1d6b227fdd605fe61239a953e803df97e521d",
        "title": "Model-based Lifelong Reinforcement Learning with Bayesian Exploration"
      },
      {
        "paperId": "60b88aab5b8c712dedeff7b5283004a09e01ccc6",
        "title": "Distributionally Adaptive Meta Reinforcement Learning"
      },
      {
        "paperId": "41a376676f9166a4b5ae2d8c222129f4d00c24c8",
        "title": "On the Convergence Theory of Meta Reinforcement Learning with Personalized Policies"
      },
      {
        "paperId": "5a407a8312ce6237d30a4c9d3a28db9ab3f7709f",
        "title": "On the Effectiveness of Fine-tuning Versus Meta-reinforcement Learning"
      },
      {
        "paperId": "5903fa23a67f15d5e56c5939b83e256ad9469411",
        "title": "Robust Policy Learning over Multiple Uncertainty Sets"
      },
      {
        "paperId": "44164c068499fbe387a1765104d69a8cbc5f0327",
        "title": "Procedural Generalization by Planning with Self-Supervised World Models"
      },
      {
        "paperId": "92edf4d699a139b757e6cfb5407872af5472a758",
        "title": "Explore and Control with Adversarial Surprise"
      },
      {
        "paperId": "ee3f989a4c39836e92e801fa71266dc258049e00",
        "title": "MetaVIM: Meta Variationally Intrinsic Motivated Reinforcement Learning for Decentralized Traffic Signal Control"
      },
      {
        "paperId": "0dcdfa0f6c6d121124cfe2431c8cb6fe93bc7cff",
        "title": "On the Eectiveness of Fine Tuning versus Meta-reinforcement Learning"
      },
      {
        "paperId": "a2adaeb3205e05f99f5858f3ad082ade4e1a77fd",
        "title": "On the Effectiveness of Fine-tuning Versus Meta-RL for Robot Manipulation"
      }
    ],
    "score": 5.6000000000000005
  },
  {
    "id": "1a39ad2d1553d07084b5e44a28878c8bb5018cef",
    "title": "PC-MLP: Model-based Reinforcement Learning with Policy Cover Guided Exploration",
    "authors": [
      "Yuda Song",
      "Wen Sun"
    ],
    "year": 2021,
    "citationCount": 22,
    "abstract": "Model-based Reinforcement Learning (RL) is a popular learning paradigm due to its potential sample efficiency compared to model-free RL. However, existing empirical model-based RL approaches lack the ability to explore. This work studies a computationally and statistically efficient model-based algorithm for both Kernelized Nonlinear Regulators (KNR) and linear Markov Decision Processes (MDPs). For both models, our algorithm guarantees polynomial sample complexity and only uses access to a planning oracle. Experimentally, we first demonstrate the flexibility and efficacy of our algorithm on a set of exploration challenging control tasks where existing empirical model-based RL approaches completely fail. We then show that our approach retains excellent performance even in common dense reward control benchmarks that do not require heavy exploration. Finally, we demonstrate that our method can also perform reward-free exploration efficiently. Our code can be found at https://github.com/yudasong/PCMLP.",
    "url": "https://www.semanticscholar.org/paper/1a39ad2d1553d07084b5e44a28878c8bb5018cef",
    "pdf_url": "https://arxiv.org/pdf/2107.07410.pdf",
    "venue": "International Conference on Machine Learning",
    "publicationDate": "2021-07-15",
    "externalIds": {
      "DBLP": "conf/icml/SongS21",
      "ArXiv": "2107.07410",
      "CorpusId": 235826015
    },
    "references": [
      {
        "paperId": "157c830c85fc7ee4aa360c72fa7bb9426de5f5b2",
        "title": "Provably Efficient Reinforcement Learning with Linear Function Approximation Under Adaptivity Constraints"
      },
      {
        "paperId": "5da1b4e1ddc612970530e5bb29470fe43bfcf2d6",
        "title": "PC-PG: Policy Cover Directed Exploration for Provable Policy Gradient Learning"
      },
      {
        "paperId": "666cd92d9e96ae68b36995729ce777c9151dd7a5",
        "title": "Provably Efficient Reinforcement Learning for Discounted MDPs with Feature Mapping"
      },
      {
        "paperId": "458d4f3d398da068493c63687e285b691514dff5",
        "title": "Information Theoretic Regret Bounds for Online Nonlinear Control"
      },
      {
        "paperId": "712e3a8b0291413ee44f27058853cfd1e5dad7b6",
        "title": "Active Learning for Nonlinear System Identification with Guarantees"
      },
      {
        "paperId": "034b2e3d957df5405783f2c5695a8c2bd87d6334",
        "title": "FLAMBE: Structural Complexity and Representation Learning of Low Rank MDPs"
      },
      {
        "paperId": "175a9a3f0bb4f31fa235386aff52ad18c67275d3",
        "title": "Model-Based Reinforcement Learning with Value-Targeted Regression"
      },
      {
        "paperId": "325488b29a33ba8e3f061dda9aed7b2df52f0447",
        "title": "A Game Theoretic Framework for Model Based Reinforcement Learning"
      },
      {
        "paperId": "c93632b6b0ad4742b3ad838732e2ce2451588a3e",
        "title": "Naive Exploration is Optimal for Online LQR"
      },
      {
        "paperId": "3231ac937b2620cd3ea7c39fdacaf416a558d31c",
        "title": "Information-Theoretic Confidence Bounds for Reinforcement Learning"
      },
      {
        "paperId": "c39fb7a46335c23f7529dd6f9f980462fd38653a",
        "title": "Mastering Atari, Go, chess and shogi by planning with a learned model"
      },
      {
        "paperId": "e30fa08b1ef8f3e7a2394a4467dc0eddcff04681",
        "title": "Keeping Your Distance: Solving Sparse Reward Tasks Using Self-Balancing Shaped Rewards"
      },
      {
        "paperId": "525a8966c0399ec606c345213c6e2111767e67a6",
        "title": "Sample Complexity of Reinforcement Learning using Linearly Combined Model Ensembles"
      },
      {
        "paperId": "4ee70fb32981f84f9dddc57bd59a69e677c91759",
        "title": "Benchmarking Model-Based Reinforcement Learning"
      },
      {
        "paperId": "9001698e033524864d4d45f051a5ba362d4afd9e",
        "title": "When to Trust Your Model: Model-Based Policy Optimization"
      },
      {
        "paperId": "a4af0eb16016c5e1f36bafb7291a8a4932c0f17c",
        "title": "Reinforcement Learning in Feature Space: Matrix Bandit, Kernels, and Regret Bound"
      },
      {
        "paperId": "1fd4694e7c2d9c872a427d50e81b5475056de6bc",
        "title": "Model-Based Reinforcement Learning for Atari"
      },
      {
        "paperId": "2c6f55132fe4b6dea414a300cc599c8d79c74a97",
        "title": "Certainty Equivalent Control of LQR is Efficient"
      },
      {
        "paperId": "a0dfed159ea90e7e4659d67faabcb26678d3bd79",
        "title": "Learning Linear-Quadratic Regulators Efficiently with only $\\sqrt{T}$ Regret"
      },
      {
        "paperId": "b19feda0f1a15e22a0a37c8de3aaeee06d841bb6",
        "title": "Model-based RL in Contextual Decision Processes: PAC bounds and Exponential Improvements over Model-free Approaches"
      },
      {
        "paperId": "b1048213518de9b7dba8225cfc5fe350c1557759",
        "title": "The total variation distance between high-dimensional Gaussians"
      },
      {
        "paperId": "4cb3fd057949624aa4f0bbe7a6dcc8777ff04758",
        "title": "Exploration by Random Network Distillation"
      },
      {
        "paperId": "d333f99881b09426283a9c7a1d25f7ac30d63062",
        "title": "Algorithmic Framework for Model-based Deep Reinforcement Learning with Theoretical Guarantees"
      },
      {
        "paperId": "8dd3f558c92255b020d56c662beb959fb2949fe0",
        "title": "An Uncertainty-Based Control Lyapunov Approach for Control-Affine Systems Modeled by Gaussian Process"
      },
      {
        "paperId": "56136aa0b2c347cbcf3d50821f310c4253155026",
        "title": "Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models"
      },
      {
        "paperId": "3a523641cab99381db21ff30df4050f7e2f546b3",
        "title": "Dual Policy Iteration"
      },
      {
        "paperId": "a87518fff4109d857ba624fb95296fe3b86bbf77",
        "title": "Regret Bounds for Robust Adaptive Control of the Linear Quadratic Regulator"
      },
      {
        "paperId": "ce1c28ca2f52a42c6e60d792cd71ba894abc47d5",
        "title": "Multi-Goal Reinforcement Learning: Challenging Robotics Environments and Request for Research"
      },
      {
        "paperId": "27dfecb6bb0308c7484e13dcaefd5eeebba677d3",
        "title": "Model-Ensemble Trust-Region Policy Optimization"
      },
      {
        "paperId": "811df72e210e20de99719539505da54762a11c6d",
        "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor"
      },
      {
        "paperId": "cce22bf6405042a965a86557684c46a441f2a736",
        "title": "Neural Network Dynamics for Model-Based Deep Reinforcement Learning with Model-Free Fine-Tuning"
      },
      {
        "paperId": "171bfa2abddd30ad177cd620c86b7f8fa64964d1",
        "title": "A General Safety Framework for Learning-Based Control in Uncertain Robotic Systems"
      },
      {
        "paperId": "869f4fc59d74a96098ed46935cb6fd1a537c38ce",
        "title": "Information theoretic MPC for model-based reinforcement learning"
      },
      {
        "paperId": "2ad1071c62b33e0d9daac1a27e819bb2b151654f",
        "title": "Goal-driven dynamics learning via Bayesian optimization"
      },
      {
        "paperId": "ddc1d2830c5979bb07ca631fa711e1e384d5ddfa",
        "title": "Model Predictive Path Integral Control: From Theory to Parallel Computation"
      },
      {
        "paperId": "2b10281297ee001a9f3f4ea1aa9bea6b638c27df",
        "title": "OpenAI Gym"
      },
      {
        "paperId": "649ed67d536a8efc631e1cc8b5c4167db5401498",
        "title": "Stochastic Extended LQR for Optimization-Based Motion Planning Under Uncertainty"
      },
      {
        "paperId": "f8f03a2b287aaa712b8dbd14024b452da6b04956",
        "title": "RLPy: a value-function-based reinforcement learning framework for education and research"
      },
      {
        "paperId": "449532187c94af3dd3aa55e16d2c50f7854d2199",
        "title": "Trust Region Policy Optimization"
      },
      {
        "paperId": "d0c61536927c2f5dc2ddb74664268a3623580b9c",
        "title": "Learning Neural Network Policies with Guided Policy Search under Unknown Dynamics"
      },
      {
        "paperId": "8783688bfe249bd1cab13146a76ba50fe88128c7",
        "title": "Model-based Reinforcement Learning and the Eluder Dimension"
      },
      {
        "paperId": "541571ba3ca170e5429454d9ad190a867263effd",
        "title": "From Bandits to Monte-Carlo Tree Search: The Optimistic Principle Applied to Optimization and Planning"
      },
      {
        "paperId": "6b6b078ee9aabf3c17220a2da1e3f0dd822956b7",
        "title": "Chapter 3 \u2013 The Cross-Entropy Method for Optimization"
      },
      {
        "paperId": "b354ee518bfc1ac0d8ac447eece9edb69e92eae1",
        "title": "MuJoCo: A physics engine for model-based control"
      },
      {
        "paperId": "43be33ef48e66d1f293c73af73f2f6753c6c392c",
        "title": "Agnostic System Identification for Model-Based Reinforcement Learning"
      },
      {
        "paperId": "60b7d47758a71978e74edff6dd8dea4d9c791d7a",
        "title": "PILCO: A Model-Based and Data-Efficient Approach to Policy Search"
      },
      {
        "paperId": "4326d7e9933c77ff9dc53056c62ef6712d90c633",
        "title": "Sampling-based algorithms for optimal motion planning"
      },
      {
        "paperId": "2d253c738ea7a1e30b748634cfaaad24cfd90af9",
        "title": "CHOMP: Gradient optimization techniques for efficient motion planning"
      },
      {
        "paperId": "7a59fde27461a3ef4a21a249cc403d0d96e4a0d7",
        "title": "Random Features for Large-Scale Kernel Machines"
      },
      {
        "paperId": "c4673454332a692b259840a3ef80fc51557ac85b",
        "title": "Gaussian Processes and Reinforcement Learning for Identification and Control of an Autonomous Blimp"
      },
      {
        "paperId": "3f99769ca3f0e3f41a239a1b58adf26bb930b9b2",
        "title": "Off-policy Learning with Options and Recognizers"
      },
      {
        "paperId": "cc45fa649a3153a61182222f496eb38554caf2bc",
        "title": "A generalized iterative LQG method for locally-optimal feedback control of constrained nonlinear stochastic systems"
      },
      {
        "paperId": "7ca8ac34767d6e6cb389eeebcdabc4225b39edfe",
        "title": "Generalization in Reinforcement Learning: Successful Examples Using Sparse Coarse Coding"
      },
      {
        "paperId": "551e19e5113cdff60a3c545d684fc4b9eb9a7306",
        "title": "Stochastic Linear Optimization under Bandit Feedback"
      },
      {
        "paperId": "874b3a63422eeaf24c14435ee6091ed48247bff3",
        "title": "Efficient memory-based learning for robot control"
      }
    ],
    "cited_by": [
      {
        "paperId": "0fe8f2f55046ad0b8d6337f57a78466790923264",
        "title": "Outcome-based Exploration for LLM Reasoning"
      },
      {
        "paperId": "da6f0ad7650d062202930d6e6e81641e491f6799",
        "title": "Non-linear, Reinforcement Learning-based Algorithm for Real-Time MANET Configuration and Threat Prediction"
      },
      {
        "paperId": "6f3aa5c01e230e4cd9ee0ea870945423bccf50c9",
        "title": "Digital Twin Calibration with Model-Based Reinforcement Learning"
      },
      {
        "paperId": "f959627bf4f4e0eadac42dafd2a22b76e3ae6dde",
        "title": "Model-based RL as a Minimalist Approach to Horizon-Free and Second-Order Bounds"
      },
      {
        "paperId": "7cf97c150f8a0cc3bcad648019e5d858ef78b7ef",
        "title": "Active Learning for Control-Oriented Identification of Nonlinear Systems"
      },
      {
        "paperId": "38044180586c1fcee63bf42c22f0ec5e418bc3bf",
        "title": "Fully-fused Multi-Layer Perceptrons on Intel Data Center GPUs"
      },
      {
        "paperId": "49adbaa388de5fb679929ff419280ea474035c59",
        "title": "Double Duality: Variational Primal-Dual Policy Optimization for Constrained Reinforcement Learning"
      },
      {
        "paperId": "9b769a231093e95a6d0969b4aed0b15cf8cb05c3",
        "title": "Leveraging Approximate Model-based Shielding for Probabilistic Safety Guarantees in Continuous Environments"
      },
      {
        "paperId": "8037c0795aa73f7a5f4be1d452cccbbad70254a2",
        "title": "Optimal Exploration for Model-Based RL in Nonlinear Systems"
      },
      {
        "paperId": "25343c4ea34d33ecd772be511d73d405a5694f96",
        "title": "Drug discovery through Covid-19 genome sequencing with siamese graph convolutional neural network"
      },
      {
        "paperId": "a8e839087b28dc469a47a184cbd197b21f32d9e6",
        "title": "Models as Agents: Optimizing Multi-Step Predictions of Interactive Local Models in Model-Based Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "dc744d27a13da837f332b542434a4bc877b0fd17",
        "title": "The Virtues of Laziness in Model-based RL: A Unified Objective and Algorithms"
      },
      {
        "paperId": "218cf1ccda75676cfdcb5e7008b8d0ea8efee57c",
        "title": "Neural Optimal Control using Learned System Dynamics"
      },
      {
        "paperId": "a8be9d9f01020d64cedbe70f1aaf79565c829a57",
        "title": "Exploration in Model-based Reinforcement Learning with Randomized Reward"
      },
      {
        "paperId": "9db58b5b8846d0da26a676522e2d1ce99b00e001",
        "title": "A Rumor Detection Method Based on Multimodal Feature Fusion by a Joining Aggregation Structure"
      },
      {
        "paperId": "81c1916def3a2a93e955d917eea8229a6e0e3fc4",
        "title": "A Rumor Detection Method Based on Multimodal Information Fusion"
      },
      {
        "paperId": "91cf44452e80afe1b4f0c2c766f6b12ea388e1b0",
        "title": "Online No-regret Model-Based Meta RL for Personalized Navigation"
      },
      {
        "paperId": "0ff980dad54588198d66b1181467ef30b0a5dfb9",
        "title": "Proximal policy optimization with model-based methods"
      },
      {
        "paperId": "8d9f23092fe61b0cdcc1251d29d01fa4bea364b2",
        "title": "Multi-Agent Reinforcement Learning via Adaptive Kalman Temporal Difference and Successor Representation"
      },
      {
        "paperId": "8d16ffb11c7b62181146db43296852424426a3cd",
        "title": "Representation Learning for Online and Offline RL in Low-rank MDPs"
      },
      {
        "paperId": "1930877e3bd43a684154f28fa4a226d030e1624b",
        "title": "Gradient Information Matters in Policy Optimization by Back-propagating through Model"
      },
      {
        "paperId": "abb81a492f3f7e1743f83dd2f0f58e73991eb3a0",
        "title": "Solving the Rubik\u2019s Cube in a Human-like Manner with Assisted Reinforcement Learning (ARL)"
      }
    ],
    "score": 5.5
  },
  {
    "id": "ecf5dc817fd6326e943b759c889d1285e673b24a",
    "title": "Digital Twin-Assisted Efficient Reinforcement Learning for Edge Task Scheduling",
    "authors": [
      "Xiucheng Wang",
      "Longfei Ma",
      "Hao Li",
      "Zhisheng Yin",
      "T. Luan",
      "Nan Cheng"
    ],
    "year": 2022,
    "citationCount": 16,
    "abstract": "Task scheduling is a critical problem when one user offloads multiple different tasks to the edge server. When a user has multiple tasks to offload and only one task can be transmitted to server at a time, while server processes tasks according to the transmission order, the problem is NP-hard. However, it is difficult for traditional optimization methods to quickly obtain the optimal solution, while approaches based on reinforcement learning face with the challenge of excessively large action space and slow convergence. In this paper, we propose a Digital Twin (DT)-assisted RL-based task scheduling method in order to improve the performance and convergence of the RL. We use DT to simulate the results of different decisions made by the agent, so that one agent can try multiple actions at a time, or, similarly, multiple agents can interact with environment in parallel in DT. In this way, the exploration efficiency of RL can be significantly improved via DT, and thus RL can converges faster and local optimality is less likely to happen. Particularly, two algorithms are designed to made task scheduling decisions, i.e., DT-assisted asynchronous Q-learning (DTAQL) and DT-assisted exploring Q-learning (DTEQL). Simulation results show that both algorithms significantly improve the convergence speed of Q-learning by increasing the exploration efficiency.",
    "url": "https://www.semanticscholar.org/paper/ecf5dc817fd6326e943b759c889d1285e673b24a",
    "pdf_url": "https://arxiv.org/pdf/2208.01781.pdf",
    "venue": "IEEE Vehicular Technology Conference",
    "publicationDate": "2022-06-01",
    "externalIds": {
      "ArXiv": "2208.01781",
      "DBLP": "journals/corr/abs-2208-01781",
      "DOI": "10.1109/VTC2022-Spring54318.2022.9860495",
      "CorpusId": 251280290
    },
    "references": [
      {
        "paperId": "922e7192cb965db83ef9a2d762812f64d5175b17",
        "title": "Deadline-Aware Task Offloading With Partially-Observable Deep Reinforcement Learning for Multi-Access Edge Computing"
      },
      {
        "paperId": "afa2ccab4676037c15c415b0117e42226fd81353",
        "title": "Deep Reinforcement Learning for Delay-Oriented IoT Task Scheduling in Space-Air-Ground Integrated Network"
      },
      {
        "paperId": "12b454e72e35a9e558be666d055e0d02bb8a47a8",
        "title": "Reducing Offloading Latency for Digital Twin Edge Networks in 6G"
      },
      {
        "paperId": "3fe4ad2aa99618120fd6af25421671bb00780efc",
        "title": "Review of digital twin about concepts, technologies, and industrial applications"
      },
      {
        "paperId": "f70f3c48ea8c8c99d64db1daac4ce111d09205cd",
        "title": "Q-Learning Algorithms: A Comprehensive Classification and Applications"
      },
      {
        "paperId": "c96ce39da23ac459fa21220404d739dceb3558b4",
        "title": "Spear: Optimized Dependency-Aware Task Scheduling with Deep Reinforcement Learning"
      },
      {
        "paperId": "d2d00d0a9c4f06f8a395a9775ee555e2e8693c22",
        "title": "Digital Twin in Industry: State-of-the-Art"
      },
      {
        "paperId": "7de203968af7e2e6d0552a389b2b1390745709ce",
        "title": "Space/Aerial-Assisted Computing Offloading for IoT Applications: A Learning-Based Approach"
      },
      {
        "paperId": "b5c1351c5aa35b11f52b8e69a2119108afa52d65",
        "title": "Joint Task Offloading Scheduling and Transmit Power Allocation for Mobile-Edge Computing Systems"
      },
      {
        "paperId": "69e76e16740ed69f4dc55361a3d319ac2f1293dd",
        "title": "Asynchronous Methods for Deep Reinforcement Learning"
      },
      {
        "paperId": "566f45f11f2faf6e859d97ca6f55b3c07f2727a8",
        "title": "Preemptive Scheduling of Equal Length Jobs on Two Machines to Minimize Mean Flow Time"
      }
    ],
    "cited_by": [
      {
        "paperId": "495c434a736ac41ace261a99dadfc830ae598370",
        "title": "Digital twin-driven reinforcement learning-based operational management for customized manufacturing"
      },
      {
        "paperId": "d26e0eaf553ea494e2ce4787de6e04e857644b96",
        "title": "Autonomous Link Control in Digital-Twin-Aided Mobile Network: From Virtual Channel Generation to Intelligent Power Allocation"
      },
      {
        "paperId": "5457ec19d013f59a4fbbc24d655239877fb1ab8e",
        "title": "Trapezoidal Gradient Descent for Effective Reinforcement Learning in Spiking Networks"
      },
      {
        "paperId": "3ebfff2aa6a33481ac1a7949fd31c78942f5321d",
        "title": "Constructing and Evaluating Digital Twins: An Intelligent Framework for DT Development"
      },
      {
        "paperId": "a694a3fb49f3c6e63e2fa5ddb72fe3aa490592f1",
        "title": "Toward Enhanced Reinforcement Learning-Based Resource Management via Digital Twin: Opportunities, Applications, and Challenges"
      },
      {
        "paperId": "c127126c51f8e2b79eb63fbce6c9f4a14a9f5039",
        "title": "Integration of Decentralized Graph-Based Multi-Agent Reinforcement Learning with Digital Twin for Traffic Signal Optimization"
      },
      {
        "paperId": "57874ba46b9e9a8642d263952e969fa845a2d52c",
        "title": "Decision support for personalized therapy in implantable medical devices: A digital twin approach"
      },
      {
        "paperId": "6dd0d9aa6dab517a0fca1a7303f7324e6ba749dd",
        "title": "Digital Twin Driven Service Self-Healing With Graph Neural Networks in 6G Edge Networks"
      },
      {
        "paperId": "d9596b11ab7ca8587e99822f3be6b2f225594569",
        "title": "Effectively Heterogeneous Federated Learning: A Pairing and Split Learning Based Approach"
      },
      {
        "paperId": "f4c445251491569f8838bb571733ce514ac0b9b6",
        "title": "Knowledge-Driven Multi-Agent Reinforcement Learning for Computation Offloading in Cybertwin-Enabled Internet of Vehicles"
      },
      {
        "paperId": "d5989b8c6da1224b7c801e1db961de80baf8e441",
        "title": "Scalable Resource Management for Dynamic MEC: An Unsupervised Link-Output Graph Neural Network Approach"
      },
      {
        "paperId": "d9b6f33701ccc0cec789b28835faa5284a4adcae",
        "title": "Imperfect Digital Twin Assisted Low Cost Reinforcement Training for Multi-UAV Networks"
      },
      {
        "paperId": "669155ba3469eb81f15d01a325dec9a0f5cefafc",
        "title": "Digital twin-assisted knowledge distillation framework for heterogeneous federated learning"
      },
      {
        "paperId": "7a4c9339ad1720e94c43440edd138c7c6ffd85a1",
        "title": "A Bayesian Framework for Digital Twin-Based Control, Monitoring, and Data Collection in Wireless Systems"
      },
      {
        "paperId": "672ba5d92a0f33d414c01626f9a3bca73161bafd",
        "title": "Digital Twin-Based Multiple Access Optimization and Monitoring via Model-Driven Bayesian Learning"
      },
      {
        "paperId": "70a923b41ca8c8f06148ac7fb5f629f41a0d18eb",
        "title": "Task Offloading in Edge-Cloud Computing Using a Q-Learning Algorithm"
      }
    ],
    "score": 5.333333333333333
  },
  {
    "id": "02ad21eea9ec32783ba529487e74a76e85499a53",
    "title": "Model-Based Offline Meta-Reinforcement Learning with Regularization",
    "authors": [
      "Sen Lin",
      "Jialin Wan",
      "Tengyu Xu",
      "Yingbin Liang",
      "Junshan Zhang"
    ],
    "year": 2022,
    "citationCount": 16,
    "abstract": "Existing offline reinforcement learning (RL) methods face a few major challenges, particularly the distributional shift between the learned policy and the behavior policy. Offline Meta-RL is emerging as a promising approach to address these challenges, aiming to learn an informative meta-policy from a collection of tasks. Nevertheless, as shown in our empirical studies, offline Meta-RL could be outperformed by offline single-task RL methods on tasks with good quality of datasets, indicating that a right balance has to be delicately calibrated between\"exploring\"the out-of-distribution state-actions by following the meta-policy and\"exploiting\"the offline dataset by staying close to the behavior policy. Motivated by such empirical analysis, we explore model-based offline Meta-RL with regularized Policy Optimization (MerPO), which learns a meta-model for efficient task structure inference and an informative meta-policy for safe exploration of out-of-distribution state-actions. In particular, we devise a new meta-Regularized model-based Actor-Critic (RAC) method for within-task policy optimization, as a key building block of MerPO, using conservative policy evaluation and regularized policy improvement; and the intrinsic tradeoff therein is achieved via striking the right balance between two regularizers, one based on the behavior policy and the other on the meta-policy. We theoretically show that the learnt policy offers guaranteed improvement over both the behavior policy and the meta-policy, thus ensuring the performance improvement on new tasks via offline Meta-RL. Experiments corroborate the superior performance of MerPO over existing offline Meta-RL methods.",
    "url": "https://www.semanticscholar.org/paper/02ad21eea9ec32783ba529487e74a76e85499a53",
    "pdf_url": "https://arxiv.org/pdf/2202.02929.pdf",
    "venue": "International Conference on Learning Representations",
    "publicationDate": "2022-02-07",
    "externalIds": {
      "DBLP": "conf/iclr/LinWXLZ22",
      "ArXiv": "2202.02929",
      "CorpusId": 246633939
    },
    "references": [
      {
        "paperId": "6c1a13f1479227f921a20e92671407b195159dee",
        "title": "Conservative Data Sharing for Multi-Task Offline Reinforcement Learning"
      },
      {
        "paperId": "6578b23ffa1be9fda80c35bc10fc83b193ff725a",
        "title": "Continuous Doubly Constrained Batch Reinforcement Learning"
      },
      {
        "paperId": "245682e8b3fa76f4a3e2991b5497577af95cbb3f",
        "title": "COMBO: Conservative Offline Model-Based Policy Optimization"
      },
      {
        "paperId": "e78381bdcc5b1d32922449369e67905684d8fb55",
        "title": "Efficient Fully-Offline Meta-Reinforcement Learning via Distance Metric Learning and Behavior Regularization"
      },
      {
        "paperId": "17908c7db26985704c00dd4521932f25c43dbe17",
        "title": "Offline Meta-Reinforcement Learning with Advantage Weighting"
      },
      {
        "paperId": "9342fce9c5a69f545a778ca7e885ba9d63af928f",
        "title": "Offline Meta Reinforcement Learning"
      },
      {
        "paperId": "fc65f4c043e16834c95cbb55a51531a11b605ed7",
        "title": "Improving Generalization in Meta-learning via Task Augmentation"
      },
      {
        "paperId": "759af360f450ef76d903a907e8a39b10845cdeaf",
        "title": "Meta-Learning Requires Meta-Augmentation"
      },
      {
        "paperId": "f1b5f17f836bf96d144dc2dd9b669440159abc84",
        "title": "On the Global Optimality of Model-Agnostic Meta-Learning"
      },
      {
        "paperId": "28db20a81eec74a50204686c3cf796c42a020d2e",
        "title": "Conservative Q-Learning for Offline Reinforcement Learning"
      },
      {
        "paperId": "79ebde314ab90d066cee3b82193ef05666323394",
        "title": "Deployment-Efficient Reinforcement Learning via Model-Based Offline Optimization"
      },
      {
        "paperId": "dea0f1c5949f8d898b9b6ff68226a781558e413c",
        "title": "MOPO: Model-based Offline Policy Optimization"
      },
      {
        "paperId": "309c2c5ee60e725244da09180f913cd8d4b8d4e9",
        "title": "MOReL : Model-Based Offline Reinforcement Learning"
      },
      {
        "paperId": "5e7bc93622416f14e6948a500278bfbe58cd3890",
        "title": "Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems"
      },
      {
        "paperId": "a326d9f2d2d351001fece788165dbcbb524da2e4",
        "title": "D4RL: Datasets for Deep Data-Driven Reinforcement Learning"
      },
      {
        "paperId": "27d78a26ddb9698b9cefcf6cdeafa4f834466103",
        "title": "AlgaeDICE: Policy Gradient from Arbitrary Experience"
      },
      {
        "paperId": "361e953f792a585496834ee14216b94d0ce9ae74",
        "title": "VariBAD: A Very Good Method for Bayes-Adaptive Deep RL via Meta-Learning"
      },
      {
        "paperId": "516643d89835d9ac40bee7a88ec9516f399c3b9a",
        "title": "Multi-task Batch Reinforcement Learning with Metric Learning"
      },
      {
        "paperId": "9001698e033524864d4d45f051a5ba362d4afd9e",
        "title": "When to Trust Your Model: Model-Based Policy Optimization"
      },
      {
        "paperId": "82b4b03a4659d6e04bd7cbf51d6e08fde1348dbd",
        "title": "Stabilizing Off-Policy Q-Learning via Bootstrapping Error Reduction"
      },
      {
        "paperId": "922c32ccc83b9054df2bbef7e81e20e8edca1605",
        "title": "Off-Policy Policy Gradient with Stationary Distribution Correction"
      },
      {
        "paperId": "4625628163a2ee0e6cd320cd7a14b4ccded2a631",
        "title": "Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables"
      },
      {
        "paperId": "5285cb8faada5de8a92a47622950f6cfd476ac1d",
        "title": "Off-Policy Deep Reinforcement Learning without Exploration"
      },
      {
        "paperId": "90dc22818bd2d97d8deaff168b0137b75a962767",
        "title": "On First-Order Meta-Learning Algorithms"
      },
      {
        "paperId": "4debb99c0c63bfaa97dd433bc2828e4dac81c48b",
        "title": "Addressing Function Approximation Error in Actor-Critic Methods"
      },
      {
        "paperId": "811df72e210e20de99719539505da54762a11c6d",
        "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor"
      },
      {
        "paperId": "c889d6f98e6d79b89c3a6adf8a921f88fa6ba518",
        "title": "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks"
      },
      {
        "paperId": "41c09567e5b3c86cb52bdc5471a306b71730ef49",
        "title": "Efficient Meta Learning via Minibatch Proximal Update"
      }
    ],
    "cited_by": [
      {
        "paperId": "2b0d7cf925cb749df5f6078d57b786f82b8994d9",
        "title": "Uncertainty-based Offline Variational Bayesian Reinforcement Learning for Robustness under Diverse Data Corruptions"
      },
      {
        "paperId": "6ec0c8668d0cd595fcc2c6a780b312c815c223a4",
        "title": "Diversification of Adaptive Policy for Effective Offline Reinforcement Learning"
      },
      {
        "paperId": "8e21f9062ba60b4d6aa627a9234757f038b14311",
        "title": "Federated Offline Policy Optimization with Dual Regularization"
      },
      {
        "paperId": "a29649e3a79ea1d350103c3a55e4067c4c9a4f88",
        "title": "L-MBOP-E: Latent-Model Based Offline Planning with Extrinsic Policy Guided Exploration"
      },
      {
        "paperId": "b01f4c12eec2d23e6b3ec67851634a24369bb3d8",
        "title": "Multi-task convex combination interpolation for meta-learning with fewer tasks"
      },
      {
        "paperId": "6ed0290f0497b4320e9bec15c4e14c0ba44d9a3c",
        "title": "Debiased Offline Representation Learning for Fast Online Adaptation in Non-stationary Dynamics"
      },
      {
        "paperId": "feb2255dbad1c829fff48922fbf0169d1b3ee4df",
        "title": "Generalizable Task Representation Learning for Offline Meta-Reinforcement Learning with Data Limitations"
      },
      {
        "paperId": "1028dc2a5d049b8568c5ebffc16d0b97554f5f57",
        "title": "Learning Top-k Subtask Planning Tree based on Discriminative Representation Pre-training for Decision Making"
      },
      {
        "paperId": "8f9c2761662a3fab59fed11f22d3b2cfe5cd7f8e",
        "title": "Context Shift Reduction for Offline Meta-Reinforcement Learning"
      },
      {
        "paperId": "70c6a7f772dc606e0f81ecd5cd3cebd3b4406270",
        "title": "Provably Efficient Offline Reinforcement Learning with Perturbed Data Sources"
      },
      {
        "paperId": "47ce36618ea2426b076e62fe4d5bedf86de708c1",
        "title": "CLARE: Conservative Model-Based Reward Learning for Offline Inverse Reinforcement Learning"
      },
      {
        "paperId": "0fd1a48877f62d8fa5e4c9adc8189e84b507a349",
        "title": "Contextual Transformer for Offline Meta Reinforcement Learning"
      },
      {
        "paperId": "05c4bfd0b16fd0ab0e72e0866d0a5bfec5ad7ded",
        "title": "Pre-Training for Robots: Offline RL Enables Learning New Tasks in a Handful of Trials"
      },
      {
        "paperId": "a5f22a3377830e8040287726b0bf5656d26efdb3",
        "title": "Robust Task Representations for Offline Meta-Reinforcement Learning via Contrastive Learning"
      },
      {
        "paperId": "2d39ce0421b85f0b5525ac0f282ea2a6599553f3",
        "title": "P RE -T RAINING FOR R OBOTS : L EVERAGING D IVERSE M ULTITASK D ATA VIA O FFLINE RL"
      },
      {
        "paperId": "32b0dbf9e13fe75b181be910c5e31b47941327ae",
        "title": "Pre-Training for Robots: Offline RL Enables Learning New Tasks from a Handful of Trials"
      }
    ],
    "score": 5.333333333333333
  },
  {
    "id": "a4a509d9019deac486087a0b10158ac115274de6",
    "title": "UniRLTest: universal platform-independent testing with reinforcement learning via image understanding",
    "authors": [
      "Ziqian Zhang",
      "Yulei Liu",
      "Shengcheng Yu",
      "Xin Li",
      "Yexiao Yun",
      "Chunrong Fang",
      "Zhenyu Chen"
    ],
    "year": 2022,
    "citationCount": 16,
    "abstract": "GUI testing has been prevailing in software testing. However, existing automated GUI testing tools mostly rely on frameworks of a specific platform. Testers have to fully understand platform features before developing platform-dependent GUI testing tools. Starting from the perspective of tester\u2019s vision, we observe that GUIs on different platforms share commonalities of widget images and layout designs, which can be leveraged to achieve platform-independent testing. We propose UniRLTest, an automated software testing framework, to achieve platform independence testing. UniRLTest utilizes computer vision techniques to capture all the widgets in the screenshot and constructs a widget tree for each page. A set of all the executable actions in each tree will be generated accordingly. UniRLTest adopts a Deep Q-Network, a reinforcement learning (RL) method, to the exploration process and formalize the Android GUI testing problem to a Marcov Decision Process (MDP), where RL could work. We have conducted evaluation experiments on 25 applications from different platforms. The result shows that UniRLTest outperforms baselines in terms of efficiency and effectiveness.",
    "url": "https://www.semanticscholar.org/paper/a4a509d9019deac486087a0b10158ac115274de6",
    "pdf_url": "https://doi.org/10.1145/3533767.3543292",
    "venue": "International Symposium on Software Testing and Analysis",
    "publicationDate": "2022-07-18",
    "externalIds": {
      "DBLP": "conf/issta/ZhangLYLYFC22",
      "DOI": "10.1145/3533767.3543292",
      "CorpusId": 250562422
    },
    "references": [],
    "cited_by": [
      {
        "paperId": "a2c778c2aef1f066b3369b0662f382bf73937c4b",
        "title": "Intent Based E2E Automated Test Case Generation for Web Applications Using LLM"
      },
      {
        "paperId": "2dc23dc62806e248c05f1e01bc0772835ec041f8",
        "title": "Temac: Multi-Agent Collaboration for Automated Web GUI Testing"
      },
      {
        "paperId": "d56ae2e23255068b340246e2790f1d0b9b25dfdc",
        "title": "Judge: Effective State Abstraction for Guiding Automated Web GUI Testing"
      },
      {
        "paperId": "ffb4673f15d81305d024632d2dc58be87dc4b5f5",
        "title": "Unlocking User-oriented Pages: Intention-driven Black-box Scanner for Real-world Web Applications"
      },
      {
        "paperId": "3335117209ca32a48a71bdda37a427bfc142bb0f",
        "title": "Enhanced Crowdsourced Test Report Prioritization via Image-and-Text Semantic Understanding and Feature Integration"
      },
      {
        "paperId": "af1e16dfd4f7967e66c754ccb0e5257aef26e4a5",
        "title": "Testing and Reinforcement Learning - A Structured Literature Review"
      },
      {
        "paperId": "0f78bbe1fbc85434a1a794e391bc7bf5b3898d22",
        "title": "Practical, Automated Scenario-Based Mobile App Testing"
      },
      {
        "paperId": "42eab96ea9ef8c144cdab47d48943b59bdd25bb4",
        "title": "Are your apps accessible? A GCN-based accessibility checker for low vision users"
      },
      {
        "paperId": "4a486ea4ab784e65095846dedcd9ffc4c592b3c0",
        "title": "When the dragons defeat the knight: Basilisk an architectural pattern for platform and language independent development"
      },
      {
        "paperId": "964458fbb31e1ef19fa8c1d77722da14ede6f82e",
        "title": "Practical Non-Intrusive GUI Exploration Testing with Visual-based Robotic Arms"
      },
      {
        "paperId": "4242ceee985e3a0a56dbbfaf00817257f1652cae",
        "title": "Vision-Based Mobile App GUI Testing: A Survey"
      },
      {
        "paperId": "cdebe4cfbe298daf9288a606d7ecd0dcf47918a0",
        "title": "APIMind: API-driven Assessment of Runtime Description-to-permission Fidelity in Android Apps"
      },
      {
        "paperId": "37e17a0b8f5cfe8de4c0f6e405f2f3d522abcd2d",
        "title": "How Android Apps Break the Data Minimization Principle: An Empirical Study"
      },
      {
        "paperId": "2d5e2fde41e26ef73f061ac97f82df29cb0dc31a",
        "title": "A Reinforcement Learning Approach to Generating Test Cases for Web Applications"
      },
      {
        "paperId": "f2eec11d4a6fde40ce2676e5111108943566fa1c",
        "title": "DinoDroid: Testing Android Apps Using Deep Q-Networks"
      },
      {
        "paperId": "121b4ed405d35a8ad1c5ced223de8f3021d429c3",
        "title": "A Comprehensive Study of Open-Source Printed Circuit Board (PCB) Design Software Bugs"
      }
    ],
    "score": 5.333333333333333
  },
  {
    "id": "a17a7256c04afee68f9aa0b7bfdc67fbca998b9c",
    "title": "Accelerating Reinforcement Learning for Autonomous Driving Using Task-Agnostic and Ego-Centric Motion Skills",
    "authors": [
      "Tong Zhou",
      "Letian Wang",
      "Ruobing Chen",
      "Wenshuo Wang",
      "Y. Liu"
    ],
    "year": 2022,
    "citationCount": 16,
    "abstract": "Efficient and effective exploration in continuous space is a central problem in applying reinforcement learning (RL) to autonomous driving. Skills learned from expert demonstrations or designed for specific tasks can benefit the exploration, but they are usually costly-collected, unbalanced/suboptimal, or failing to transfer to diverse tasks. However, human drivers can adapt to varied driving tasks without demonstrations by taking efficient and structural explorations in the entire skill space rather than a limited space with task-specific skills. Inspired by the above fact, we propose an RL algorithm exploring all feasible motion skills instead of a limited set of task-specific and object-centric skills. Without demonstrations, our method can still perform well in diverse tasks. First, we build a task-agnostic and ego-centric (TaEc) motion skill library in a pure motion perspective, which is diverse enough to be reusable in different complex tasks. The motion skills are then encoded into a low-dimension latent skill space, in which RL can do exploration efficiently. Validations in various challenging driving scenarios demonstrate that our proposed method, TaEc-RL, outperforms its counterparts significantly in learning efficiency and task performance.",
    "url": "https://www.semanticscholar.org/paper/a17a7256c04afee68f9aa0b7bfdc67fbca998b9c",
    "pdf_url": "https://arxiv.org/pdf/2209.12072.pdf",
    "venue": "IEEE/RJS International Conference on Intelligent RObots and Systems",
    "publicationDate": "2022-09-24",
    "externalIds": {
      "DBLP": "journals/corr/abs-2209-12072",
      "ArXiv": "2209.12072",
      "DOI": "10.1109/IROS55552.2023.10341449",
      "CorpusId": 252531963
    },
    "references": [
      {
        "paperId": "2ce42614bbffe69dfbcda4d4b13ffaac87213412",
        "title": "Transferable and Adaptable Driving Behavior Prediction"
      },
      {
        "paperId": "b69a7afd74c6a70fa3a1bf2934143afcd1f54dc3",
        "title": "Hierarchical Adaptable and Transferable Networks (HATN) for Driving Behavior Prediction"
      },
      {
        "paperId": "4a8b0e3b9e93c52670062b15cb2a8eae25b035a6",
        "title": "Accelerating Robotic Reinforcement Learning via Parameterized Action Primitives"
      },
      {
        "paperId": "9dae8d598f9dd9d8c96f771699f5e37a27e71df3",
        "title": "MetaDrive: Composing Diverse Driving Scenarios for Generalizable Reinforcement Learning"
      },
      {
        "paperId": "c3bba4ae7442747a980fdd370b1d31bf4da5600d",
        "title": "Learning Interaction-aware Guidance Policies for Motion Planning in Dense Traffic Scenarios"
      },
      {
        "paperId": "aecd3d775fc31c86bf145b0216b73fd889e2c85e",
        "title": "Reinforcement Learning based Negotiation-aware Motion Planning of Autonomous Vehicles"
      },
      {
        "paperId": "41e43d9c766128cdd715c64fbd30e0c9fdf14652",
        "title": "From Motor Control to Team Play in Simulated Humanoid Football"
      },
      {
        "paperId": "a66df6a55c25e8f131502c922000b9197debe14a",
        "title": "Shaping Rewards for Reinforcement Learning with Imperfect Demonstrations using Generative Models"
      },
      {
        "paperId": "538f97d36125b704f91894839107c8f29c129a9d",
        "title": "Socially-Compatible Behavior Design of Autonomous Vehicles With Verification on Real Human Data"
      },
      {
        "paperId": "b68b8b980db62308864b2a7d33718182c5f8335b",
        "title": "Accelerating Reinforcement Learning with Learned Skill Priors"
      },
      {
        "paperId": "57a121e51b4cd1cd2a81506ce32196217b439a46",
        "title": "Reinforcement Learning based Control of Imitative Policies for Near-Accident Driving"
      },
      {
        "paperId": "076d1585bc4603c47a0735b599164604ef1b0199",
        "title": "Learning compositional models of robot skills for task and motion planning"
      },
      {
        "paperId": "7583ce60cc38983e3f568591d7590ecf34f82f84",
        "title": "Navigation Command Matching for Vision-based Autonomous Driving"
      },
      {
        "paperId": "e90323d515a024be8a6d0465dd90eefd681f9245",
        "title": "Discovering Motor Programs by Recomposing Demonstrations"
      },
      {
        "paperId": "1d6d157f4586ee5fffa172b7198ecb8f7101f921",
        "title": "Catch & Carry"
      },
      {
        "paperId": "361c00b22e29d0816ca896513d2c165e26399821",
        "title": "Grandmaster level in StarCraft II using multi-agent reinforcement learning"
      },
      {
        "paperId": "be97603d18c9ea29a42877de05465caedf7e3e49",
        "title": "Driving in Dense Traffic with Model-Free Reinforcement Learning"
      },
      {
        "paperId": "4f0c4189d9a82f94dcd84fe879cfbe124aaf270b",
        "title": "Keyframing the Future: Keyframe Discovery for Visual Prediction and Planning"
      },
      {
        "paperId": "99a7df93a2e16bd7ac3349d52cc34417cda7909d",
        "title": "Learning Latent Plans from Play"
      },
      {
        "paperId": "12c0751b4f51ed833172a713b7e32390032ead93",
        "title": "Soft Actor-Critic Algorithms and Applications"
      },
      {
        "paperId": "fbf03bf621ffee283911e765d525a75fc0d11bae",
        "title": "CompILE: Compositional Imitation Learning and Execution"
      },
      {
        "paperId": "683599f260a877fef5e97a643852b854ae3db9a1",
        "title": "Compositional Imitation Learning: Explaining and executing one task at a time"
      },
      {
        "paperId": "96ae5d3ac1a1dcc365684bc92fcfa4d40d802bca",
        "title": "Neural probabilistic motor primitives for humanoid control"
      },
      {
        "paperId": "74e12851de2d542aa2aef7b8a39ef021a5802689",
        "title": "Composing Complex Skills by Learning Transition Policies"
      },
      {
        "paperId": "6ecc4b1ab05f3ec12484a0ea36abfd6271c5c5ba",
        "title": "Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review"
      },
      {
        "paperId": "d356a5603f14c7a6873272774782d7812871f952",
        "title": "Reinforcement and Imitation Learning for Diverse Visuomotor Skills"
      },
      {
        "paperId": "5b01eaef54a653ba03ddd5a978690380fbc19bfc",
        "title": "Diversity is All You Need: Learning Skills without a Reward Function"
      },
      {
        "paperId": "79e8af07d7be3e41ebdc2d1a219d5c867693bf4c",
        "title": "How Would Surround Vehicles Move? A Unified Framework for Maneuver Classification and Motion Prediction"
      },
      {
        "paperId": "811df72e210e20de99719539505da54762a11c6d",
        "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor"
      },
      {
        "paperId": "c28ec2a40a2c77e20d64cf1c85dc931106df8e83",
        "title": "Overcoming Exploration in Reinforcement Learning with Demonstrations"
      },
      {
        "paperId": "1bead9000a719cb258bac7320228055aee650d2c",
        "title": "Leveraging Demonstrations for Deep Reinforcement Learning on Robotics Problems with Sparse Rewards"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "e6e01f580c973d91f6445d839389f9f2d5efc78e",
        "title": "Learning human behaviors from motion capture by adversarial imitation"
      },
      {
        "paperId": "e3b0ea7209731c47b582215c6c67f9c691ad9863",
        "title": "Deep Q-learning From Demonstrations"
      },
      {
        "paperId": "15b26d8cb35d7e795c8832fe08794224ee1e9f84",
        "title": "The Option-Critic Architecture"
      },
      {
        "paperId": "846aedd869a00c09b40f1f1f35673cb22bc87490",
        "title": "Mastering the game of Go with deep neural networks and tree search"
      },
      {
        "paperId": "30b0baee14e9b0e9f9e6afa972912eb4a3e593ba",
        "title": "A Computationally Efficient Motion Primitive for Quadrocopter Trajectory Generation"
      },
      {
        "paperId": "ffced5b53ad956474a12d73b5cbfd38355dfb70a",
        "title": "Reinforcement learning of motor skills with policy gradients"
      },
      {
        "paperId": "0515b1803656762dcad0ad66097578e810d7d5dd",
        "title": "Optimal Rough Terrain Trajectory Generation for Wheeled Mobile Robots"
      },
      {
        "paperId": "a364338bf39c405f78035ebef19a919442fd9c57",
        "title": "The Construction of Movement with Behavior-Specific and Behavior-Independent Modules"
      },
      {
        "paperId": "0e7638dc16a5e5e9e46c91272bfb9c3dd242ef6d",
        "title": "Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning"
      },
      {
        "paperId": "167095a9d674eba6e059f609384043d232c3a2b1",
        "title": "Consistency Regularization for Ensemble Model Based Reinforcement Learning"
      },
      {
        "paperId": null,
        "title": "Latent Skill Dimension 2"
      },
      {
        "paperId": "14e956857571057afa71d1e86720877060af7aae",
        "title": "Adaptive sampling-based motion planning with a non-conservatively defensive strategy for autonomous driving"
      },
      {
        "paperId": "7cd316505f52aa337ef8a2aff10bc6bf1df561d0",
        "title": "and s"
      },
      {
        "paperId": "ead1abc362819539d454f2d85216e58d540f5124",
        "title": "Improved Trajectory Planning for On-Road Self-Driving Vehicles Via Combined Graph Search, Optimization & Topology Analysis"
      },
      {
        "paperId": "3d2218b17e7898a222e5fc2079a3f1531990708f",
        "title": "I and J"
      },
      {
        "paperId": "2a65434d43ffa6554eaf14b728780919ad4f33eb",
        "title": "Modeling Purposeful Adaptive Behavior with the Principle of Maximum Causal Entropy"
      },
      {
        "paperId": "ca5cb4e0826b424adb81cbb4f2e3c88c391a4075",
        "title": "Influence of cultivation temperature on the ligninolytic activity of selected fungal strains"
      },
      {
        "paperId": "8065f1bde165594e2d7bcaaf37c41bcf08a505f4",
        "title": "\u0393 and B"
      },
      {
        "paperId": "12b03af504d0960334c77567dab38791bf0f739a",
        "title": "AND T"
      },
      {
        "paperId": "73f36ff3a6d340606e09d2d0091da27a13af7a6f",
        "title": "and as an in"
      },
      {
        "paperId": null,
        "title": "ON, CA (lt.wang@mail.utoronto.ca). 4 California PATH"
      },
      {
        "paperId": null,
        "title": "(b) Ablation Study on Latent Skill Dimension"
      }
    ],
    "cited_by": [
      {
        "paperId": "9dc11aa0538a646aa2da254d53eb1617284ec452",
        "title": "PE-RLHF: Reinforcement Learning with Human Feedback and physics knowledge for safe and trustworthy autonomous driving"
      },
      {
        "paperId": "d2e28c42829e199261c05e615c0dba33fbc121b3",
        "title": "IRL-VLA: Training an Vision-Language-Action Policy via Reward World Model"
      },
      {
        "paperId": "9732fe79ce9f67acd53a19a444537ee6a7f3bd3a",
        "title": "Multi-Timescale Hierarchical Reinforcement Learning for Unified Behavior and Control of Autonomous Driving"
      },
      {
        "paperId": "31e397e9dc4255b2e4585e65f378b3b8ece90479",
        "title": "VSG: Rapid Adaptation in Autonomous Driving via Vehicle Skill Graph"
      },
      {
        "paperId": "61fe493bf3a4a87531f95d9e02f6676fbd2521a6",
        "title": "DriveMind: A Dual-VLM based Reinforcement Learning Framework for Autonomous Driving"
      },
      {
        "paperId": "10a5ecd2838e66b3a1f565bcf20a5e8033ef6e14",
        "title": "CarPlanner: Consistent Auto-regressive Trajectory Planning for Large-scale Reinforcement Learning in Autonomous Driving"
      },
      {
        "paperId": "cf2cad78a014538ab7f5003f60ca7d1025f282a1",
        "title": "Heuristic-Based Multi-Agent Deep Reinforcement Learning Approach for Coordinating Connected and Automated Vehicles at Non-Signalized Intersection"
      },
      {
        "paperId": "7b04273b981154441b527b93107303463f2ae073",
        "title": "Solving Multi-Goal Robotic Tasks with Decision Transformer"
      },
      {
        "paperId": "b541098888d936ca86244b561da7fd6e8de85df8",
        "title": "Knowledge Distillation-Based Edge-Decision Hierarchies for Interactive Behavior-Aware Planning in Autonomous Driving System"
      },
      {
        "paperId": "607fbf81621cc806b03b46f20417a333e97e0912",
        "title": "Trustworthy Human-AI Collaboration: Reinforcement Learning with Human Feedback and Physics Knowledge for Safe Autonomous Driving"
      },
      {
        "paperId": "a8beed76f1e0e1e0f697e1063c8a7d2906ad5654",
        "title": "Act Better by Timing: A timing-Aware Reinforcement Learning for Autonomous Driving"
      },
      {
        "paperId": "327d149454994765d689d7c656c73c5a14549086",
        "title": "A Pseudo-Hierarchical Planning Framework with Dynamic-Aware Reinforcement Learning for Autonomous Driving"
      },
      {
        "paperId": "e0b05e314372ed580d9612ef5f0ee672b17ad2e4",
        "title": "LMDrive: Closed-Loop End-to-End Driving with Large Language Models"
      },
      {
        "paperId": "3915dd919e6b5fe65479060ead7ec94403f0fab6",
        "title": "Towards High Efficient Long-Horizon Planning With Expert-Guided Motion-Encoding Tree Search"
      },
      {
        "paperId": "341975e953eae14932b708d7f80e378d11013a57",
        "title": "Boosting Offline Reinforcement Learning for Autonomous Driving with Hierarchical Latent Skills"
      },
      {
        "paperId": "21f76e304b3fb958e6dd2a445fc750cc6e4f8f69",
        "title": "Mastering Cooperative Driving Strategy in Complex Scenarios using Multi-Agent Reinforcement Learning"
      }
    ],
    "score": 5.333333333333333
  },
  {
    "id": "69bdc99655204190697067c3da5296e544e6865d",
    "title": "Safe Model-Based Reinforcement Learning With an Uncertainty-Aware Reachability Certificate",
    "authors": [
      "Dongjie Yu",
      "Wenjun Zou",
      "Yujie Yang",
      "Haitong Ma",
      "Sheng Li",
      "Yuming Yin",
      "Jianyu Chen",
      "Jingliang Duan"
    ],
    "year": 2022,
    "citationCount": 16,
    "abstract": "Safe reinforcement learning (RL) that solves constraint-satisfactory policies provides a promising way to the broader safety-critical applications of RL in real-world problems such as robotics. Among all safe RL approaches, model-based methods reduce training time violations further due to their high sample efficiency. However, lacking safety robustness against the model uncertainties remains an issue in safe model-based RL, especially in training time safety. In this paper, we propose a distributional reachability certificate (DRC) and its Bellman equation to address model uncertainties and characterize robust persistently safe states. Furthermore, we build a safe RL framework to resolve constraints required by the DRC and its corresponding shield policy. We also devise a line search method to maintain safety and reach higher returns simultaneously while leveraging the shield policy. Comprehensive experiments on classical benchmarks such as constrained tracking and navigation indicate that the proposed algorithm achieves comparable returns with much fewer constraint violations during training. Our code is available at https://github.com/ManUtdMoon/Distributional-Reachability-Policy-Optimization. Note to Practitioners\u2014Although it has been proven that RL can be applied in complex robotics control tasks, the training process of an RL control policy induces frequent failures because the agent needs to learn safety through constraint violations. This issue hinders the promotion of RL because a large amount of failure of robots is too expensive to afford. This paper aims to reduce the training-time violations of RL-based control methods, enabling RL to be leveraged in a broader application area. To achieve the goal, we first introduce a safety quantity describing the distribution of potential constraint violations in the long term. By imposing constraints on the quantile of the safety distribution, we can realize safety robust to the model uncertainty, which is necessary for real-world robot learning with environment uncertainty. Second, we further devise a shield policy aiming to minimize the constraint violation. The policy will intervene when the agent is about to violate state constraints, further enhancing exploration safety. Third, we implement a line search method to find an action pursuing near-optimal performance when fulfilling safety requirements strictly. Our experimental results indicate that the proposed algorithm reduces training-time violations significantly while maintaining competitive task performance. We make a step towards applying RL safely in real-world tasks. Our future work includes conducting physical verification on real robots to evaluate the algorithm and improving safety further by starting from an initially safe control policy that comes from domain knowledge.",
    "url": "https://www.semanticscholar.org/paper/69bdc99655204190697067c3da5296e544e6865d",
    "pdf_url": "https://arxiv.org/pdf/2210.07553.pdf",
    "venue": "IEEE Transactions on Automation Science and Engineering",
    "publicationDate": "2022-10-14",
    "externalIds": {
      "ArXiv": "2210.07553",
      "DBLP": "journals/corr/abs-2210-07553",
      "DOI": "10.1109/TASE.2023.3292388",
      "CorpusId": 252907858
    },
    "references": [
      {
        "paperId": "a9eebb54c1d55b45d52169121c1e48c55014931a",
        "title": "Scalable Autonomous Separation Assurance With Heterogeneous Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "c12dff5ce0c189dab6307781ac2133625aff85d6",
        "title": "Visuomotor Reinforcement Learning for Multirobot Cooperative Navigation"
      },
      {
        "paperId": "af4aff4b7c9466d19fdbc8aacdcfacab3b7f10cf",
        "title": "Unified Automatic Control of Vehicular Systems With Reinforcement Learning"
      },
      {
        "paperId": "76c5d61c82f9654b95135a58bf760593ae95d411",
        "title": "Safety-constrained reinforcement learning with a distributional safety critic"
      },
      {
        "paperId": "8cf45c7e1d9d02ccf2a764fdedacc63906af546a",
        "title": "Lyapunov Density Models: Constraining Distribution Shift in Learning-Based Control"
      },
      {
        "paperId": "7310a959dae090f520a63982c933f348124f1dcd",
        "title": "Reachability Constrained Reinforcement Learning"
      },
      {
        "paperId": "7b4d4e974ff96e04823bc96ed13a4f679fcb03fe",
        "title": "TRC: Trust Region Conditional Value at Risk for Safe Reinforcement Learning"
      },
      {
        "paperId": "d8da5d28807fe5f8af2e6942c67738bd9f12a1ec",
        "title": "Safe Reinforcement Learning by Imagining the Near Future"
      },
      {
        "paperId": "f1e48bfb4464fedb94ced2d85b74991efcfe2856",
        "title": "Constrained Policy Optimization via Bayesian World Models"
      },
      {
        "paperId": "388f63776d458c1d818df91abe5ee75bffa40d73",
        "title": "Joint Synthesis of Safety Certificate and Safe Control Policy using Constrained Reinforcement Learning"
      },
      {
        "paperId": "184b8554d6efab36cfa3bc9c4153cdbdc399acae",
        "title": "Safe Nonlinear Control Using Robust Neural Lyapunov-Barrier Functions"
      },
      {
        "paperId": "3d8010c65bcdac6bcde0b92967a6c7b5dd5cad85",
        "title": "Safe-Control-Gym: A Unified Benchmark Suite for Safe Learning-Based Control and Reinforcement Learning in Robotics"
      },
      {
        "paperId": "36d55c909e8c1be84f3a4f2631e3303ef5392fb0",
        "title": "Learning Barrier Certificates: Towards Safe Reinforcement Learning with Zero Training-time Violations"
      },
      {
        "paperId": "b967f46d1189c4e91916a51f7e13c4f447c33e64",
        "title": "Safety and Liveness Guarantees through Reach-Avoid Reinforcement Learning"
      },
      {
        "paperId": "9333138ad1d0ef3778bae9ae517aaf396de1b287",
        "title": "Density Constrained Reinforcement Learning"
      },
      {
        "paperId": "5d41b075266a13b57f55144e438b68f9de1097e6",
        "title": "Feasible Actor-Critic: Constrained Reinforcement Learning for Ensuring Statewise Safety"
      },
      {
        "paperId": "5b2370ebd3439ff60ea64a0c8db88fea2dd86a9c",
        "title": "WCSAC: Worst-Case Soft Actor Critic for Safety-Constrained Reinforcement Learning"
      },
      {
        "paperId": "068660cdd795e20177a9ea7314e103e740f85e5c",
        "title": "Safe Continuous Control with Constrained Model-Based Policy Optimization"
      },
      {
        "paperId": "558e38786c18df7a217d719e08310e144a64dadb",
        "title": "Robust Control Barrier\u2013Value Functions for Safety-Critical Control"
      },
      {
        "paperId": "eaa4c1302e40cb10f37ba98209047965a17cac19",
        "title": "Integrated Decision and Control: Toward Interpretable and Computationally Efficient Driving Intelligence"
      },
      {
        "paperId": "431dc05ac25510de6264084434254cca877f9ab3",
        "title": "Recovery RL: Safe Reinforcement Learning With Learned Recovery Zones"
      },
      {
        "paperId": "a334f9897a330abddf99cfec0b5a70f751e9497b",
        "title": "Conservative Safety Critics for Exploration"
      },
      {
        "paperId": "9ec0bd60df62b997bb371e65d254434f8851ea9b",
        "title": "Projection-Based Constrained Policy Optimization"
      },
      {
        "paperId": "ce54f25907510d52c659e017ef65c7b8d63f2296",
        "title": "DSAC: Distributional Soft Actor Critic for Risk-Sensitive Reinforcement Learning"
      },
      {
        "paperId": "23a061f719c6c6f1f8c0b5157b5c5145ca2ef8b7",
        "title": "Distributional Soft Actor-Critic: Off-Policy Reinforcement Learning for Addressing Value Estimation Errors"
      },
      {
        "paperId": "0cc956565c7d249d4197eeb1dbab6523c648b2c9",
        "title": "Dream to Control: Learning Behaviors by Latent Imagination"
      },
      {
        "paperId": "a297b30c04055222ab60c610d41044e189df1c5f",
        "title": "Adaptive dynamic programming for nonaffine nonlinear optimal control problem with state constraints"
      },
      {
        "paperId": "c919ae4366f5cc4901b854cc259101ccc13e6f3f",
        "title": "Constrained Reinforcement Learning Has Zero Duality Gap"
      },
      {
        "paperId": "9001698e033524864d4d45f051a5ba362d4afd9e",
        "title": "When to Trust Your Model: Model-Based Policy Optimization"
      },
      {
        "paperId": "331638e2105fd2ed70ec9e700255af1bc962f1be",
        "title": "Bridging Hamilton-Jacobi Safety Analysis and Reinforcement Learning"
      },
      {
        "paperId": "fea3e63c97c7292dc6fbcb3ffe7131eb54053986",
        "title": "Learning Latent Dynamics for Planning from Pixels"
      },
      {
        "paperId": "6398cb8f2af1c988a097ed1e1cefb380195edfb8",
        "title": "(Preprint)"
      },
      {
        "paperId": "d333f99881b09426283a9c7a1d25f7ac30d63062",
        "title": "Algorithmic Framework for Model-based Deep Reinforcement Learning with Theoretical Guarantees"
      },
      {
        "paperId": "56136aa0b2c347cbcf3d50821f310c4253155026",
        "title": "Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models"
      },
      {
        "paperId": "cb7c479a36520da1caeeec67db10772351a390c6",
        "title": "Reward Constrained Policy Optimization"
      },
      {
        "paperId": "811df72e210e20de99719539505da54762a11c6d",
        "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor"
      },
      {
        "paperId": "c1f4ef741242d629d1f56e442a09a7ba29595a0e",
        "title": "A Distributional Perspective on Reinforcement Learning"
      },
      {
        "paperId": "7a4193d0b042643a8bb9ec262ed7f9d509bdb12e",
        "title": "Constrained Policy Optimization"
      },
      {
        "paperId": "2b10281297ee001a9f3f4ea1aa9bea6b638c27df",
        "title": "OpenAI Gym"
      },
      {
        "paperId": "78aae90aa8c7a5496b5003bcee6ef4b6f1eebcef",
        "title": "Safe learning of regions of attraction for uncertain, nonlinear systems with Gaussian processes"
      },
      {
        "paperId": "759bbd8dd50cb4790cad7a3bccbdfcbfee5e3e89",
        "title": "Risk-Constrained Reinforcement Learning with Percentile Risk Criteria"
      },
      {
        "paperId": "286e3bc65c2bd4a59ec6b93d9bf42778fa1e87dc",
        "title": "Reach-avoid problems with time-varying dynamics, targets and constraints"
      },
      {
        "paperId": "b354ee518bfc1ac0d8ac447eece9edb69e92eae1",
        "title": "MuJoCo: A physics engine for model-based control"
      },
      {
        "paperId": "75c827ed14c8794b6babf05b944751877e7d2b77",
        "title": "Model-based reinforcement learning with nearly tight exploration complexity bounds"
      },
      {
        "paperId": "5083e43969a00b9509f263f4cb77e016e3c44d93",
        "title": "GOPS: A general optimal control problem solver for autonomous driving and industrial control applications"
      },
      {
        "paperId": "6830b6bf4af0882a734761c4f0692d7892d3bd11",
        "title": "Reinforcement Learning for Sequential Decision and Optimal Control"
      },
      {
        "paperId": null,
        "title": "Constrained Markov Decision Processes: Stochastic Modeling, 1st ed"
      },
      {
        "paperId": "3be5199010012448af287767297d81aa4a87567c",
        "title": "Model-free Safe Control for Zero-Violation Reinforcement Learning"
      },
      {
        "paperId": "4d0f6a6ffcd6ab04732ff76420fd9f8a7bb649c3",
        "title": "Benchmarking Safe Exploration in Deep Reinforcement Learning"
      },
      {
        "paperId": "37cf85a06f4d2f48498fa390e4e85f71eb38f54b",
        "title": "Safe Exploration in Reinforcement Learning: Theory and Applications in Robotics"
      },
      {
        "paperId": null,
        "title": "Reinforcement Learning: An Introduc-tion (Adaptive Computation and Machine Learning Series), 2nd ed. Cambridge, MA, USA"
      },
      {
        "paperId": null,
        "title": "SAFE MODEL-BASED RL WITH AN UNCERTAINTY-AWARE REACHABILITY CERTIFICATE"
      },
      {
        "paperId": null,
        "title": "[ journals/conference papers, and the co-inventor of over 20 Chinese patents"
      },
      {
        "paperId": null,
        "title": "Hisresearch interests include decision and control of autonomous vehicles, reinforcement learning, and optimal control"
      },
      {
        "paperId": null,
        "title": "automotive engineering from Tsinghua University, Beijing, China"
      }
    ],
    "cited_by": [
      {
        "paperId": "50f0c65126cfc65b80bb04575ac64df54bbcafdb",
        "title": "Group Confident Policy Optimization"
      },
      {
        "paperId": "f12062e70b77bf64cd053df3bc969f5de0bd0a03",
        "title": "Dual-Objective Reinforcement Learning with Novel Hamilton-Jacobi-Bellman Formulations"
      },
      {
        "paperId": "459c0fa2605be2c3e4febc7f95b6178d9760809a",
        "title": "Uncertainty-aware Latent Safety Filters for Avoiding Out-of-Distribution Failures"
      },
      {
        "paperId": "2dd988da09f644bac28e4e6fe1a9108b9f030c96",
        "title": "LaMOuR: Leveraging Language Models for Out-of-Distribution Recovery in Reinforcement Learning"
      },
      {
        "paperId": "0c3ac77f45408d3d41018783ddedba6b87b3833b",
        "title": "NLBAC: A neural ODE-based algorithm for state-wise stable and safe reinforcement learning"
      },
      {
        "paperId": "f6028a3e815e5eb403b12cb3b589f550b5532561",
        "title": "Scalable Synthesis of Formally Verified Neural Value Function for Hamilton-Jacobi Reachability Analysis"
      },
      {
        "paperId": "63634b38fd4e4718eb2247dca3ca296339c3ba52",
        "title": "Hamilton-Jacobi Reachability in Reinforcement Learning: A Survey"
      },
      {
        "paperId": "e1f2d6b7b1c3e69e3781dcd5693548d2e69fca55",
        "title": "FOSP: Fine-tuning Offline Safe Policy through World Models"
      },
      {
        "paperId": "ed77da2cd99b424dd644da6026c415ed11cd1968",
        "title": "Optimal Gait Control for a Tendon-driven Soft Quadruped Robot by Model-based Reinforcement Learning"
      },
      {
        "paperId": "62d7edfb407984d96c749723927a1c4f3ae40113",
        "title": "FREA: Feasibility-Guided Generation of Safety-Critical Scenarios with Reasonable Adversariality"
      },
      {
        "paperId": "399378f4e3994a4c241d2ce5dcf509ddc4768408",
        "title": "Policy Bifurcation in Safe Reinforcement Learning"
      },
      {
        "paperId": "a2246e09ab0b56ba91322747addad030eae56c50",
        "title": "Safe Offline Reinforcement Learning with Feasibility-Guided Diffusion Model"
      },
      {
        "paperId": "8da73e2e0d6ff6d7ebc912605d50f54eb6d3736a",
        "title": "Iterative Reachability Estimation for Safe Reinforcement Learning"
      },
      {
        "paperId": "9f713b6194ef6a6001c99450a151431898b07867",
        "title": "Feasible Policy Iteration for Safe Reinforcement Learning"
      },
      {
        "paperId": "f173a8652eeda3fa3ede5cdf5a982ae9c9eff0b7",
        "title": "A human-centered safe robot reinforcement learning framework with interactive behaviors"
      },
      {
        "paperId": "fd87b991efc47aefae4a0ba0ec958ae00146a68d",
        "title": "Practical Reinforcement Learning Using Time-Efficient Model-Based Policy Optimization"
      }
    ],
    "score": 5.333333333333333
  },
  {
    "id": "abaa1a3a8468473fd2827e49623eabc36ffaf8fe",
    "title": "Model-based reinforcement learning with parametrized physical models and optimism-driven exploration",
    "authors": [
      "Christopher Xie",
      "S. Patil",
      "T. Moldovan",
      "S. Levine",
      "P. Abbeel"
    ],
    "year": 2015,
    "citationCount": 50,
    "abstract": "In this paper, we present a robotic model-based reinforcement learning method that combines ideas from model identification and model predictive control. We use a feature-based representation of the dynamics that allows the dynamics model to be fitted with a simple least squares procedure, and the features are identified from a high-level specification of the robot's morphology, consisting of the number and connectivity structure of its links. Model predictive control is then used to choose the actions under an optimistic model of the dynamics, which produces an efficient and goal-directed exploration strategy. We present real time experimental results on standard benchmark problems involving the pendulum, cartpole, and double pendulum systems. Experiments indicate that our method is able to learn a range of benchmark tasks substantially faster than the previous best methods. To evaluate our approach on a realistic robotic control task, we also demonstrate real time control of a simulated 7 degree of freedom arm.",
    "url": "https://www.semanticscholar.org/paper/abaa1a3a8468473fd2827e49623eabc36ffaf8fe",
    "pdf_url": "https://arxiv.org/pdf/1509.06824.pdf",
    "venue": "IEEE International Conference on Robotics and Automation",
    "publicationDate": "2015-09-23",
    "externalIds": {
      "DBLP": "conf/icra/XiePMLA16",
      "ArXiv": "1509.06824",
      "MAG": "2963781688",
      "DOI": "10.1109/ICRA.2016.7487172",
      "CorpusId": 10257651
    },
    "references": [
      {
        "paperId": "6a40dd75dc272e8d4a253aad548cc22507fd93ef",
        "title": "Model Identification"
      },
      {
        "paperId": "30d6c606d741020c1afd82b29e4a0318e789cddc",
        "title": "System Identification"
      },
      {
        "paperId": "936be530d31fa7ee03de9bd1551cc149408d0432",
        "title": "Efficient reinforcement learning for robots using informative simulated priors"
      },
      {
        "paperId": "7deb1a775a023aaf0351b17ab07e159307378c54",
        "title": "Optimism-driven exploration for nonlinear systems"
      },
      {
        "paperId": "e517fbe9ab74d3104946a5bb1634759937327608",
        "title": "Approximate real-time optimal control based on sparse Gaussian process models"
      },
      {
        "paperId": "30eecac2b259bbeaeb7682d90688c623cfa3ceda",
        "title": "Fast planning of well conditioned trajectories for model learning"
      },
      {
        "paperId": "aa2787a4dd6fdc14f7ca36d80ec469cb2435c44a",
        "title": "Reinforcement learning with multi-fidelity simulators"
      },
      {
        "paperId": "6a28056e9670c4d7681045777e9e02c76f318c54",
        "title": "Control-limited differential dynamic programming"
      },
      {
        "paperId": "98a5e7b37a36d799e988e8801323feaa441b86d2",
        "title": "Learning-based nonlinear model predictive control to improve vision-based mobile robot path-tracking in challenging outdoor environments"
      },
      {
        "paperId": "6dbe03c07d6abaf75a73e5b62ebf4b20f776a630",
        "title": "Physical feasibility of robot base inertial parameter identification: A linear matrix inequality approach"
      },
      {
        "paperId": "65438e0ba226c1f97bd8a36333ebc3297b1a32fd",
        "title": "Reinforcement learning in robotics: A survey"
      },
      {
        "paperId": "62af8a0336cabfb0c0bd8444f2a38dff9639cca3",
        "title": "Identification of consistent standard dynamic parameters of industrial robots"
      },
      {
        "paperId": "71b552b2e058d5a6a760ba203f10f13be759edd3",
        "title": "Synthesis and stabilization of complex behaviors through online trajectory optimization"
      },
      {
        "paperId": "fed9f71f206354853145b6bab56c105437ade651",
        "title": "Variational Bayesian Optimization for Runtime Risk-Sensitive Control"
      },
      {
        "paperId": "eec4783610ca8405fa2ff134aeaa410a4a3f14fd",
        "title": "Near-Optimal BRL using Optimistic Local Transitions"
      },
      {
        "paperId": "f01a87f319539657cc64b24f1aeb262e3b365327",
        "title": "Robot excitation trajectories for dynamic parameter estimation using optimized B-splines"
      },
      {
        "paperId": "60b7d47758a71978e74edff6dd8dea4d9c791d7a",
        "title": "PILCO: A Model-Based and Data-Efficient Approach to Policy Search"
      },
      {
        "paperId": "fc1238a45dbbfecbdd71c10fa3295462969b7a49",
        "title": "Review: An overview of dynamic parameter identification of robots"
      },
      {
        "paperId": "ea639900bd73c08091a799b2d52e3496a03543de",
        "title": "Using model knowledge for learning inverse dynamics"
      },
      {
        "paperId": "8d2820ac17ff3cedf59f173b16b98872848bf3ad",
        "title": "Exploration-exploitation tradeoff using variance estimates in multi-armed bandits"
      },
      {
        "paperId": "1a2a75279ae9daf59772d0cf2cb4b7cd6abcda63",
        "title": "Optimistic Linear Programming gives Logarithmic Regret for Irreducible MDPs"
      },
      {
        "paperId": "0bfbdafdfbcc268860fe54ae4d8f08d487bcc762",
        "title": "An Application of Reinforcement Learning to Aerobatic Helicopter Flight"
      },
      {
        "paperId": "3f99769ca3f0e3f41a239a1b58adf26bb930b9b2",
        "title": "Off-policy Learning with Options and Recognizers"
      },
      {
        "paperId": "9d9df2110ce50e484ce1574d5048890b05216e2a",
        "title": "Optimal robot excitation and identification"
      },
      {
        "paperId": "e0a8f3e540d9c787b6d1c70bd65f994926f0ec54",
        "title": "Exciting Trajectories for the Identification of Base Inertial Parameters of Robots"
      },
      {
        "paperId": "0a29eb3e524809f869f99481549ebbb25f487494",
        "title": "Reinforcement learning is direct adaptive optimal control"
      },
      {
        "paperId": "696724e94ec39aff99074bf3e99ed01caff4ed93",
        "title": "On Finding Exciting Trajectories for Identification Experiments Involving Systems with Nonlinear Dynamics"
      },
      {
        "paperId": "c03d1fca5457910ab9ccecd5eb9f575b12571ff4",
        "title": "System Identi cation"
      },
      {
        "paperId": null,
        "title": "Conjugate bayesian analysis of the gaussian distribution"
      },
      {
        "paperId": "f8cf46a1fecbbbd14ae5a8585e5239cdc5ed3979",
        "title": "The joint."
      },
      {
        "paperId": null,
        "title": "Adaptive Control, 2nd ed"
      },
      {
        "paperId": null,
        "title": "Differential dynamic programming, ser. Modern analytic and computational methods in science and mathematics"
      },
      {
        "paperId": null,
        "title": "Pendulum: The pendulum system is a simple single link system with these nonlinear dynamics"
      }
    ],
    "cited_by": [
      {
        "paperId": "d73e0b74deac282e84f52a1c309f1a5caa8f6fb6",
        "title": "Parameter-Regression MPC (PaReMPC): Experiments and Extensions of a Broadly Adaptive Approach for SISO Robotic Applications"
      },
      {
        "paperId": "9c9eb42629b24feb04fdf778584af0094603f815",
        "title": "Boosting Exploration in Reinforcement Learning for Sparse Reward Tasks"
      },
      {
        "paperId": "c51691c9bdc3a54a80d5984d8fc2810b9ce1b80b",
        "title": "Physics-Informed Model and Hybrid Planning for Efficient Dyna-Style Reinforcement Learning"
      },
      {
        "paperId": "cb04d4f99af770921c326486a911755ea8f182f6",
        "title": "Guided Cooperation in Hierarchical Reinforcement Learning via Model-Based Rollout"
      },
      {
        "paperId": "2aa8e47f9b5db501c6b0b65f75388aae57606119",
        "title": "A Survey on Physics Informed Reinforcement Learning: Review and Open Problems"
      },
      {
        "paperId": "eb1e6c3b3704859d40df0e98fc456faf054a7ecd",
        "title": "Learning-Based Near-Optimal Motion Planning for Intelligent Vehicles With Uncertain Dynamics"
      },
      {
        "paperId": "d6234d686b22db52ebb6c41f74706bb99df0faaf",
        "title": "On Physical Origins of Learning"
      },
      {
        "paperId": "0c5c5f100dec9c758abf4dcc526a6883671fd3bd",
        "title": "Physics-Informed Machine Learning: A Survey on Problems, Methods and Applications"
      },
      {
        "paperId": "9c02d2c91027c7a774fc0181f1e5d858ead1c7e4",
        "title": "Model Identification and Control of a Low-cost Mobile Robot with Omnidirectional Wheels using Differentiable Physics"
      },
      {
        "paperId": "da1e404ae194dcbcce198f5335f49f090403bbd7",
        "title": "Exploration in Deep Reinforcement Learning: A Survey"
      },
      {
        "paperId": "cd14bb267b2badb0d4407b3327d162c2b4346b35",
        "title": "Deep Learning for Embodied Vision Navigation: A Survey"
      },
      {
        "paperId": "3803ea42e1fc773db3b1d0fa05f41b5ebf0a61d1",
        "title": "Toward Causal Representation Learning"
      },
      {
        "paperId": "9a7402094f08cf99a1ec515509160a194efdf887",
        "title": "Meta Learning With Paired Forward and Inverse Models for Efficient Receding Horizon Control"
      },
      {
        "paperId": "8f566001453bc6be0a935bf69ffd90d9db3af32b",
        "title": "Towards Causal Representation Learning"
      },
      {
        "paperId": "1c70580ee9d59df84b5e19a8ac837b62231c3808",
        "title": "Learning to control in power systems: Design and analysis guidelines for concrete safety problems"
      },
      {
        "paperId": "efce12ecc1aaf9973f0a3e1bc1ec6834ec198067",
        "title": "Model Identification and Control of a Low-Cost Wheeled Mobile Robot Using Differentiable Physics"
      },
      {
        "paperId": "29be0eddfff83aab67a2edab40e04d97a226c0c5",
        "title": "Learning Task-Agnostic Action Spaces for Movement Optimization"
      },
      {
        "paperId": "175a9a3f0bb4f31fa235386aff52ad18c67275d3",
        "title": "Model-Based Reinforcement Learning with Value-Targeted Regression"
      },
      {
        "paperId": "392fc768f3f054be49767a8948de7fa0d9d09edf",
        "title": "Reinforcement Learning from Simulated Environments: An Encoder Decoder Framework"
      },
      {
        "paperId": "9d55314573ec254569df35ecc4cc8d464431f2cc",
        "title": "Causally Correct Partial Models for Reinforcement Learning"
      },
      {
        "paperId": "2066b64c446968143e59f8e6d1637a80acfd4739",
        "title": "Scoring-Aggregating-Planning: Learning task-agnostic priors from interactions and sparse rewards for zero-shot generalization"
      },
      {
        "paperId": "836bfba6faf0d72f5202e002ef01c7b50c718fde",
        "title": "Rapid trial-and-error learning with simulation supports flexible tool use and physical reasoning"
      },
      {
        "paperId": "d8e996fa4be2a967d0c7ca323019321bdb3c7424",
        "title": "The Tools Challenge: Rapid Trial-and-Error Learning in Physical Problem Solving"
      },
      {
        "paperId": "f587d8c3c77baa28efb07b50b8db0c4637b89ede",
        "title": "Learning-based Model Predictive Control for Safe Exploration and Reinforcement Learning"
      },
      {
        "paperId": "401dfc5efbd182eaef6945249aea8a0a1e3cced5",
        "title": "Shaping Belief States with Generative Environment Models for RL"
      },
      {
        "paperId": "8fd6fd872ea64347cda7d87991baf2f9ba84913c",
        "title": "Learning Probabilistic Models for Safe Predictive Control in Unknown Environments"
      },
      {
        "paperId": "c31f9302a48b8c40fe89650a154ad58e5103894b",
        "title": "Task-Agnostic Dynamics Priors for Deep Reinforcement Learning"
      },
      {
        "paperId": "20f5f46d7ef755c1a7551bcab1863b863eb5778a",
        "title": "Optimistic Sampling Strategy for Data-Efficient Reinforcement Learning"
      },
      {
        "paperId": "5898dd9ecc9b51c76bc416ce979a8d9012226335",
        "title": "Online Gaussian Process State-space Model: Learning and Planning for Partially Observable Dynamical Systems"
      },
      {
        "paperId": "7b4dd3364a74ee6490996013ea2a2d69d3828e7d",
        "title": "Mapless Motion Planning System for an Autonomous Underwater Vehicle Using Policy Gradient-based Deep Reinforcement Learning"
      },
      {
        "paperId": "d526df319bf8697a6b88008951324efe425bad5b",
        "title": "Learn Fast, Forget Slow: Safe Predictive Learning Control for Systems With Unknown and Changing Dynamics Performing Repetitive Tasks"
      },
      {
        "paperId": "5425d7bd33ed5643a4c24c41245829ff1cb963d7",
        "title": "Experience-Based Model Selection to Enable Long-Term, Safe Control for Repetitive Tasks Under Changing Conditions"
      },
      {
        "paperId": "d333f99881b09426283a9c7a1d25f7ac30d63062",
        "title": "Algorithmic Framework for Model-based Deep Reinforcement Learning with Theoretical Guarantees"
      },
      {
        "paperId": "dc37b9513158df3374aaba91e2c7e3227ab1891c",
        "title": "Multi-objective Model-based Policy Search for Data-efficient Learning with Sparse Rewards"
      },
      {
        "paperId": "fd2f725d34898bdc363e3445729e852dbec16fb4",
        "title": "Socially Aware Robot Navigation Using Deep Reinforcement Learning"
      },
      {
        "paperId": "ede9f86e18d39993589963205d56de7a926c7196",
        "title": "Learning-Based Model Predictive Control for Safe Exploration"
      },
      {
        "paperId": "ae158d50c10f8f2c4d1912382167e0b892bb2b5d",
        "title": "Experience Recommendation for Long Term Safe Learning-based Model Predictive Control in Changing Operating Conditions"
      },
      {
        "paperId": "f7909cc8cab188c1d3c54497582c89a7aefc39c7",
        "title": "Decoupling Dynamics and Reward for Transfer Learning"
      },
      {
        "paperId": "ba82a4b693d11b5c83a527a81a11e75980c914d6",
        "title": "An Efficient Navigation Framework for Autonomous Mobile Robots in Dynamic Environments using Learning Algorithms"
      },
      {
        "paperId": "0e238f08226becb569e2274875ce4d62796a5186",
        "title": "Using Parameterized Black-Box Priors to Scale Up Model-Based Policy Search for Robotics"
      },
      {
        "paperId": "7137b527fe00424e8e20f6c5dc949adfadcbc6d1",
        "title": "Learning multimodal models for robot dynamics online with a mixture of Gaussian process experts"
      },
      {
        "paperId": "0fa067ace34a04c9dc08b4abc1fbb4b0941f4864",
        "title": "Robust and explorative behavior in model-based Bayesian reinforcement learning"
      },
      {
        "paperId": "d7bd6e3addd8bc8e2e154048300eea15f030ed33",
        "title": "Reinforcement Learning with Unsupervised Auxiliary Tasks"
      },
      {
        "paperId": "01dfe1868e8abc090b1485482929f65743e23743",
        "title": "Towards Cognitive Exploration through Deep Reinforcement Learning for Mobile Robots"
      },
      {
        "paperId": "a1eb6ebfcf9e081f77776883ece0fce0ebf90cd7",
        "title": "Path tracking control and identification of tire parameters using on-line model-based reinforcement learning"
      },
      {
        "paperId": "2b3ec72786da1853fc379e9494a73bcce88f47b8",
        "title": "Exploratory Gradient Boosting for Reinforcement Learning in Complex Domains"
      },
      {
        "paperId": "0fd446ac56d4b84b0b7b4b84b50ba8e3d3697b1a",
        "title": "Learning Policies for Automated Racing Using Vehicle Model Gradients"
      },
      {
        "paperId": "00949371ea4bb20bff539943c9e4b84cbc8f6418",
        "title": "Residual Model-Based Reinforcement Learning for Physical Dynamics"
      },
      {
        "paperId": "3ed715e6d0cda5897b71d03691cfeae2201bccc5",
        "title": "Deep Learning for Embodied Vision Navigation: A Survey"
      },
      {
        "paperId": "48b82af53db2dc989331ec1d48d6717cb75fa9a4",
        "title": "Socially Aware Robot Navigation Using Deep Reinforcement Learning"
      }
    ],
    "score": 5.0
  },
  {
    "id": "2029ebd195491dd845e14866045225b238f6c392",
    "title": "Multi-Agent Deep Reinforcement Learning-Based Cooperative Spectrum Sensing With Upper Confidence Bound Exploration",
    "authors": [
      "Yu Zhang",
      "Peixiang Cai",
      "Changyong Pan",
      "Subing Zhang"
    ],
    "year": 2019,
    "citationCount": 29,
    "abstract": "In this paper, a multi-agent deep reinforcement learning method was adopted to realize cooperative spectrum sensing in cognitive radio networks. Each secondary user learns an efficient sensing strategy from the sensing results of some of the selected spectra to avoid interference to the primary users and to coordinate with other secondary users. It is necessary to balance exploration and exploitation in the learning process when using deep reinforcement learning methods, helping explain that upper confidence bound with Hoeffding-style bonus has been adopted in this paper to improve the efficiency of exploration. The simulation results verify that the proposed algorithm, when compared with the conventional reinforcement learning methods with $\\varepsilon $ -greedy exploration, is much easier to achieve faster convergence speed and better reward performance.",
    "url": "https://www.semanticscholar.org/paper/2029ebd195491dd845e14866045225b238f6c392",
    "pdf_url": "https://doi.org/10.1109/ACCESS.2019.2937108",
    "venue": "IEEE Access",
    "publicationDate": "2019-08-23",
    "externalIds": {
      "DBLP": "journals/access/ZhangCPZ19",
      "MAG": "2969473779",
      "DOI": "10.1109/ACCESS.2019.2937108",
      "CorpusId": 201847256
    },
    "references": [
      {
        "paperId": "03cc81e98942bdafd994af7a1d1e62a68ff8b682",
        "title": "Is Q-learning Provably Efficient?"
      },
      {
        "paperId": "3b4c398f234e946d599b492d8a4bba4eb8376122",
        "title": "Deep Reinforcement Learning for Dynamic Multichannel Access in Wireless Networks"
      },
      {
        "paperId": "c9ca2122e7e8c8a94a933590851f8ba18d2475a0",
        "title": "Cooperative Spectrum Sensing: A Blind and Soft Fusion Detector"
      },
      {
        "paperId": "b3e77e9fcd4469458d50e6aad54163c43fb6a411",
        "title": "Intelligent Power Control for Spectrum Sharing in Cognitive Radios: A Deep Reinforcement Learning Approach"
      },
      {
        "paperId": "da39697c4e052461c3dd4abf1360d8a40ae1c022",
        "title": "Multi-Agent Reinforcement Learning Based Cognitive Anti-Jamming"
      },
      {
        "paperId": "0b8be881b5c4611087713e7b5d823119742bce26",
        "title": "Two-dimensional anti-jamming communication based on deep reinforcement learning"
      },
      {
        "paperId": "dc22b175d3a1c5b1a7f4ec4ff051e8b9f6186f0e",
        "title": "Optimal sensing scheduling for green Cognitive Radio"
      },
      {
        "paperId": "3daceb079b10c6d4dc48163be6148281bfe9da3b",
        "title": "Energy and throughput efficient cooperative spectrum sensing in cognitive radio sensor networks"
      },
      {
        "paperId": "340f48901f72278f6bf78a04ee5b01df208cc508",
        "title": "Human-level control through deep reinforcement learning"
      },
      {
        "paperId": "0e0d0bb43edc3946e922d1ed299dde10d12440d3",
        "title": "Reinforcement learning for cooperative sensing gain in cognitive radio ad hoc networks"
      },
      {
        "paperId": "cff8f93f35e76268db8bc072804cf6775371b5a5",
        "title": "Reinforcement learning based distributed multiagent sensing policy for cognitive radio networks"
      },
      {
        "paperId": "9b11dc9d9ac2775f0f95e479d663e4b41b16f086",
        "title": "Reinforcement Learning Based Auction Algorithm for Dynamic Spectrum Access in Cognitive Radio Networks"
      },
      {
        "paperId": "3cc2b3a507fb08e4ff89affd64213e060500558b",
        "title": "Grouping technique for cooperative spectrum sensing in cognitive radios"
      },
      {
        "paperId": "5e9d92c0385a40368c7e444960200eae62635a72",
        "title": "Double Threshold Energy Detection of Cooperative Spectrum Sensing in Cognitive Radio"
      },
      {
        "paperId": "f4af916e112f5c02857935ac58518cf2e02b37d1",
        "title": "Occupation Measurements Supporting Dynamic Spectrum Allocation for Cognitive Radio Design"
      },
      {
        "paperId": "f5f2336371087d232528af74eeff83054e5abd6e",
        "title": "Cooperative Sensing among Cognitive Radios"
      },
      {
        "paperId": "fb5d1bb23724d9a5a5eae036a2e3cf291cac2c1b",
        "title": "Cognitive radio: making software radios more personal"
      },
      {
        "paperId": "7b265d9fa9d4beeb699f592efd73d06694c72df7",
        "title": "Multiagent Reinforcement Learning: Theoretical Framework and an Algorithm"
      },
      {
        "paperId": "f64d76cc8f6919e8555932e31853ca221d8bf6d7",
        "title": "A Contention-Free Reporting Scheme Based MAC Protocol for Cooperative Spectrum Sensing in Cognitive Radio Networks"
      },
      {
        "paperId": "5235ef1c075a43e185aa3152f70c3e299f31bf4a",
        "title": "Multi-Modal Cooperative Spectrum Sensing Based on Dempster-Shafer Fusion in 5G-Based Cognitive Radio"
      },
      {
        "paperId": "ef7990390a482c5852094ceb1c1cf819c7e6d271",
        "title": "Cooperative Spectrum Sensing for Cognitive Radio Networks Application: Performance Analysis for Realistic Channel Conditions"
      },
      {
        "paperId": "cb7f1a31c8c6bf2640c43aec328e306b0bb5b2fb",
        "title": "A survey of spectrum sensing algorithms for cognitive radio applications"
      },
      {
        "paperId": "f5d0c5e631385bcb04ceda3280fe308435773c86",
        "title": "Cooperative Spectrum Sensing in Cognitive Radio Networks"
      },
      {
        "paperId": null,
        "title": "Spectrum policy task force"
      }
    ],
    "cited_by": [
      {
        "paperId": "0ab05515b3bc39aedc430680c534413c57373057",
        "title": "An Application of Explainable Multi-Agent Reinforcement Learning for Spectrum Situational Awareness"
      },
      {
        "paperId": "ba753d97dc5b2614686fe5c57be297b539aa6334",
        "title": "Adaptive network approach to exploration-exploitation trade-off in reinforcement learning."
      },
      {
        "paperId": "38701c4d0ae47fe93a39ceae981effb09971856b",
        "title": "A Review of Research on Spectrum Sensing Based on Deep Learning"
      },
      {
        "paperId": "09120320c69184d7e01e0d35578a60824ea82ae6",
        "title": "A Review on Reinforcement Learning enabled Cooperative Spectrum Sensing"
      },
      {
        "paperId": "74c3de0af0ee977a2ca88f4fcf28c9243cb8ac1d",
        "title": "SplitPlace: AI Augmented Splitting and Placement of Large-Scale Neural Networks in Mobile Edge Environments"
      },
      {
        "paperId": "09ca1232276ea39002d99d52d8092bf463509249",
        "title": "Least Mean Square Adaptive Filter Detection Based on Wavelet Transform in Spectrum Monitoring"
      },
      {
        "paperId": "0dccdb8bd55fbaa3fe49d64b8ee7140a9871679f",
        "title": "Spectrum sensing in cognitive radio networks and metacognition for dynamic spectrum sharing between radar and communication system: A review"
      },
      {
        "paperId": "8a51ca286c675569500d84e8f8ae7d6276af3e31",
        "title": "A Survey on Machine Learning Algorithms for Applications in Cognitive Radio Networks"
      },
      {
        "paperId": "b8c754db0a3190e0824d399ee1ef36002c1952c8",
        "title": "Deep learning application for sensing available spectrum for cognitive radio: An ECRNN approach"
      },
      {
        "paperId": "5b558086bed768a3486ae8936e089b6afd02f2d7",
        "title": "Improved Q-Learning Algorithm Based on Approximate State Matching in Agricultural Plant Protection Environment"
      },
      {
        "paperId": "c2de849a00eb905fe347ec9ff4d08cd09a6be4e5",
        "title": "A Modified Quad Q Network Algorithm for Predicting Resource Management"
      },
      {
        "paperId": "10fe90fd95b29b8b426a057eccb97b956f4ed0c1",
        "title": "The Frontiers of Deep Reinforcement Learning for Resource Management in Future Wireless HetNets: Techniques, Challenges, and Research Directions"
      },
      {
        "paperId": "c847c2079035fc1390407a1e798574a0eb09ff64",
        "title": "Multi-Agent Reinforcement Learning for Joint Cooperative Spectrum Sensing and Channel Access in Cognitive UAV Networks"
      },
      {
        "paperId": "bf8a2e4cfa0f27c4c4e3a7a52d78cc6a2dc62c28",
        "title": "Optimization Method of Power Equipment Maintenance Plan Decision-Making Based on Deep Reinforcement Learning"
      },
      {
        "paperId": "bc68d947961f1d4c2df3288a9ee290052cedd27f",
        "title": "Q-Learning-Based Spectrum Access for Multimedia Transmission Over Cognitive Radio Networks"
      },
      {
        "paperId": "ada574d696744ad5789f61f3b42d759cae63c459",
        "title": "Dynamic Cooperative Spectrum Sensing Based on Deep Multi-User Reinforcement Learning"
      },
      {
        "paperId": "fe82f65bca456d871620e01e8e53ef36c469c350",
        "title": "Deep Reinforcement Learning Based Energy-efficient Task Offloading for Secondary Mobile Edge Systems"
      },
      {
        "paperId": "d7198d2808d62bd9e3f91c4274eb409d55e1c512",
        "title": "Cognitive Method Discussion on Communication Jamming Intelligent Decision"
      },
      {
        "paperId": "3ef1c9df772c809ed5dda8e2505351a49c5a5110",
        "title": "Coordination Graph-Based Deep Reinforcement Learning for Cooperative Spectrum Sensing Under Correlated Fading"
      },
      {
        "paperId": "1734a848d70bdc20938d9ee96d8de8f08d366597",
        "title": "Towards a Learning-Based Framework for Self-Driving Design of Networking Protocols"
      },
      {
        "paperId": "83d81b5740466cdf30e020fda51c9fbeccf81cc8",
        "title": "Intelligent cognitive spectrum collaboration: Convergence of spectrum sensing, spectrum access, and coding technology"
      },
      {
        "paperId": "12c13f2ade907fa76df8c898ed8f7af609b9c757",
        "title": "Intelligent Resource Allocation for Train-to-Train Communication: A Multi-Agent Deep Reinforcement Learning Approach"
      },
      {
        "paperId": "ad628c1b5256218048622539fbe123442b90469a",
        "title": "Toward Holistic Integration of Computing and Wireless Networking"
      },
      {
        "paperId": "8cd8578c557982c27143600cbb86e59ba6a06a79",
        "title": "Federated Reinforcement Learning for Wireless Networks: Fundamentals, Challenges and Future Research Trends"
      },
      {
        "paperId": "875bb853ee46cf8af7da409c7fef8a2dcd035dd1",
        "title": "SIGMETRICS: G: SplitPlace: Efficient Splitting and Placement of Deep Neural Networks on Mobile Edge Environments"
      },
      {
        "paperId": "330463cc0359d9431d8e83dc08eaaf8ad9b79173",
        "title": "Dynamic Multichannel Sensing in Cognitive Radio: Hierarchical Reinforcement Learning"
      },
      {
        "paperId": "b2cc051a4184618103a16f21eff54c3b99034388",
        "title": "Multi-Agent Reinforcement Learning based Joint Cooperative Spectrum Sensing and Channel Access for Cognitive UAV Networks"
      },
      {
        "paperId": "d848b2dccf4e423ff6a7b187d0b5b4aa4bfe7be1",
        "title": "Internet of Things. A Confluence of Many Disciplines: Second IFIP International Cross-Domain Conference, IFIPIoT 2019, Tampa, FL, USA, October 31 \u2013 November 1, 2019, Revised Selected Papers"
      },
      {
        "paperId": "800079af4a35fd59f07515234a85bf8d1f31a88a",
        "title": "Improving Medium Access Efficiency With Intelligent Spectrum Learning"
      }
    ],
    "score": 4.833333333333333
  },
  {
    "id": "c447bc7891c2a3a775af4f8fb94e34e7968c1cb7",
    "title": "A Reinforcement Learning-Based Reconstruction Method for Complex Defect Profiles in MFL Inspection",
    "authors": [
      "Zhenning Wu",
      "Yiming Deng",
      "Jinhai Liu",
      "Lixing Wang"
    ],
    "year": 2021,
    "citationCount": 19,
    "abstract": "Magnetic flux leakage (MFL) inspection is one of the most commonly used nondestructive evaluation (NDT) methods for detecting anomalies of ferromagnetic materials. Sizing of the defect with MFL signal is the key problem of inspection, which is important for evaluating the health condition of the material. However, it is an ill-posed inverse problem that is hard to solve and harder to be accurate. Forward models that give a highly accurate simulated signal with a corresponding depth profile are widely used in solving the inverse problem iteratively. Unfortunately, the policy which determines the iteration process is hard to design. In this article, a reinforcement learning (RL)-based algorithm is proposed to reconstruct the depth of defects with a complex depth profile. Instead of designing the policy, the iterative process of the classic iteration-based method is embedded into the learning process of the RL-based algorithm proposed in this article. The policy is learned from the data generated during the reconstructing iteration. By designing the states, actions, and rewards in the RL structure, the most computationally costly part of calling the forward model during iteration is avoided. An adaptive limited exploration process is given to balance the exploration and exploitation in the inverse problem of MFL inspection in this article. The effectiveness of the proposed algorithm is demonstrated with simulation results under different noise levels. The results demonstrate that the proposed algorithm is robust with good reconstruction accuracy.",
    "url": "https://www.semanticscholar.org/paper/c447bc7891c2a3a775af4f8fb94e34e7968c1cb7",
    "pdf_url": "https://doi.org/10.1109/TIM.2021.3052000",
    "venue": "IEEE Transactions on Instrumentation and Measurement",
    "publicationDate": "",
    "externalIds": {
      "DBLP": "journals/tim/WuDLW21",
      "DOI": "10.1109/TIM.2021.3052000",
      "CorpusId": 232043115
    },
    "references": [],
    "cited_by": [
      {
        "paperId": "4f8e0fab7cdc8595c7622fe1f824e79b4aa29cdd",
        "title": "Nondestructive Inspection of Steel Cables Based on YOLOv9 with Magnetic Flux Leakage Images"
      },
      {
        "paperId": "93cf75b14dd3bf0446586bdc3ca0fb998030f7b6",
        "title": "A 3-D Defect Profile Inversion Method Based on the Continuity Correction Strategy"
      },
      {
        "paperId": "bd700b62965ad7218ebf5af64700238d2132be8f",
        "title": "Application of machine learning to leakage detection of fluid pipelines in recent years: A review and prospect"
      },
      {
        "paperId": "153d42ad907e467c7a0179966dd2b77949b480be",
        "title": "Defect classification and quantification method based on AC magnetic flux leakage time domain signal characteristics"
      },
      {
        "paperId": "da6902e0ddcfea23233d9f0a5d4a39c5574e1243",
        "title": "Multi-modality hierarchical attention networks for defect identification in pipeline MFL detection"
      },
      {
        "paperId": "ca27c8ab41b07e1937d24148182223f1bf05d4b9",
        "title": "Reconstruction of 3-D pipeline corrosion defect profile from MFL signals with deep learning approach"
      },
      {
        "paperId": "63001c2b2e03aed54069f9eee2236e1c1517ce90",
        "title": "Defect-depth-field algorithm for simulating magnetic flux leakage signals based on discrete magnetic dipole model"
      },
      {
        "paperId": "e078ee00ad8e83a455e4b3f392179ea9b545099e",
        "title": "Deep Learning for Magnetic Flux Leakage Detection and Evaluation of Oil & Gas Pipelines: A Review"
      },
      {
        "paperId": "f25a38d618da8ff057ead29ffc9fe8639540a31f",
        "title": "Image Registration for Visualizing Magnetic Flux Leakage Testing under Different Orientations of Magnetization"
      },
      {
        "paperId": "85cb9ab45b7b55192bf43801ebcbedf2ad799b25",
        "title": "Crack opening shape reconstruction method in magnetic flux leakage imaging"
      },
      {
        "paperId": "897e1872e6d0b49374edf3629c8a68981fe26214",
        "title": "A Review of Magnetic Flux Leakage Nondestructive Testing"
      },
      {
        "paperId": "c97b71e4fbfe8287d5bc2c12be537320098e4131",
        "title": "MAR and UAR: Solutions for Depth Profile Reconstruction of MFL Inspection With High Degree of Freedoms"
      },
      {
        "paperId": "93dfa0d79f433056669a296b9424cd36b4d9d41c",
        "title": "Enhanced MFL Method Based on Magneto-Mechanical-Electrical Coupling Effect of Metglas/PZT Composite With High Permeability"
      },
      {
        "paperId": "bd0db08f1d46bc5d0004fb78a6e39353b2891349",
        "title": "A MFL Mechanism-Based Self-Supervised Method for Defect Detection With Limited Labeled Samples"
      },
      {
        "paperId": "fa40994fa77be9b911d8e950ba7a39f2007a37c8",
        "title": "A Crack Detection Method for Pipelines Using Wavelet-Based Decision-Level Data Fusion"
      },
      {
        "paperId": "3ebe1232e7aabe7ecc9f84163a7115b6c209cb40",
        "title": "FMD-Framework: A Size Estimation Method for Pipeline Defects in Weld-Affected Zones"
      },
      {
        "paperId": "9d2a558dc2e47e80c50e3edab8e116086e8b052a",
        "title": "A Physiological Control System for Pulsatile Ventricular Assist Device Using an Energy-Efficient Deep Reinforcement Learning Method"
      },
      {
        "paperId": "5efa5d46fd489d9956008481ff34083575753959",
        "title": "A Target-Focusing Optimization Method for 3-D Profile Reconstruction of Defects Using MFL Measurements"
      },
      {
        "paperId": "194051b84af9e5d650557d2401fdbbd9ff740827",
        "title": "MFL Image Recognition Method of Pipeline Corrosion Defects Based on Multilayer Feature Fusion Multiscale GhostNet"
      }
    ],
    "score": 4.75
  },
  {
    "id": "70e1d6b227fdd605fe61239a953e803df97e521d",
    "title": "Model-based Lifelong Reinforcement Learning with Bayesian Exploration",
    "authors": [
      "Haotian Fu",
      "Shangqun Yu",
      "Michael S. Littman",
      "G. Konidaris"
    ],
    "year": 2022,
    "citationCount": 14,
    "abstract": "We propose a model-based lifelong reinforcement-learning approach that estimates a hierarchical Bayesian posterior distilling the common structure shared across different tasks. The learned posterior combined with a sample-based Bayesian exploration procedure increases the sample efficiency of learning across a family of related tasks. We first derive an analysis of the relationship between the sample complexity and the initialization quality of the posterior in the finite MDP setting. We next scale the approach to continuous-state domains by introducing a Variational Bayesian Lifelong Reinforcement Learning algorithm that can be combined with recent model-based deep RL methods, and that exhibits backward transfer. Experimental results on several challenging domains show that our algorithms achieve both better forward and backward transfer performance than state-of-the-art lifelong RL methods.",
    "url": "https://www.semanticscholar.org/paper/70e1d6b227fdd605fe61239a953e803df97e521d",
    "pdf_url": "https://arxiv.org/pdf/2210.11579.pdf",
    "venue": "Neural Information Processing Systems",
    "publicationDate": "2022-10-20",
    "externalIds": {
      "DBLP": "conf/nips/FuYL022",
      "ArXiv": "2210.11579",
      "DOI": "10.48550/arXiv.2210.11579",
      "CorpusId": 253080700
    },
    "references": [
      {
        "paperId": "77be65bb396cb6309b6d03023c5f71203b3a39ea",
        "title": "Towards Effective Context for Meta-Reinforcement Learning: an Approach based on Contrastive Learning"
      },
      {
        "paperId": "4650f9265eef5e5fd05835860c06b814d0f6db34",
        "title": "Lifelong Policy Gradient Learning of Factored Policies for Faster Training Without Forgetting"
      },
      {
        "paperId": "1d4288c47a7802575d2a0d231d1283a4f225a85b",
        "title": "MetaCURE: Meta Reinforcement Learning with Empowerment-Driven Exploration"
      },
      {
        "paperId": "0cc956565c7d249d4197eeb1dbab6523c648b2c9",
        "title": "Dream to Control: Learning Behaviors by Latent Imagination"
      },
      {
        "paperId": "0bc855f84668b35cb65618d996d09f6e434d28c9",
        "title": "Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning"
      },
      {
        "paperId": "361e953f792a585496834ee14216b94d0ce9ae74",
        "title": "VariBAD: A Very Good Method for Bayes-Adaptive Deep RL via Meta-Learning"
      },
      {
        "paperId": "4ee70fb32981f84f9dddc57bd59a69e677c91759",
        "title": "Benchmarking Model-Based Reinforcement Learning"
      },
      {
        "paperId": "4625628163a2ee0e6cd320cd7a14b4ccded2a631",
        "title": "Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables"
      },
      {
        "paperId": "58bd352c179e38d11b37a087764db64ad78f6030",
        "title": "\u5f37\u5316\u5b66\u7fd2\u306b\u304a\u3051\u308bVariational Information Maximizing Exploration\u306b\u57fa\u3065\u304f\u72b6\u614b\u63a2\u7d22\u306e\u52b9\u7387\u5316"
      },
      {
        "paperId": "42de54e614110c0c0a0bbbfee045e11e53eb4a7d",
        "title": "Deep Online Learning via Meta-Learning: Continual Adaptation for Model-Based RL"
      },
      {
        "paperId": "849c91ff8bb3ff67d278adb5295fee78049c6288",
        "title": "Bayesian Model-Agnostic Meta-Learning"
      },
      {
        "paperId": "38e2f851b705faa0d0a698ed9885bd6834440073",
        "title": "Probabilistic Model-Agnostic Meta-Learning"
      },
      {
        "paperId": "56136aa0b2c347cbcf3d50821f310c4253155026",
        "title": "Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models"
      },
      {
        "paperId": "dcf90546433cc5c00f97cb3e2947bc4286e70cec",
        "title": "Learning Invariances for Policy Generalization"
      },
      {
        "paperId": "46b072cf918ec6f50403568a73d4347ea86b7e66",
        "title": "Recasting Gradient-Based Meta-Learning as Hierarchical Bayes"
      },
      {
        "paperId": "811df72e210e20de99719539505da54762a11c6d",
        "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor"
      },
      {
        "paperId": "c27db32efa8137cbf654902f8f728f338e55cd1c",
        "title": "Mastering the game of Go without human knowledge"
      },
      {
        "paperId": "cf90552b5d2e992e93ab838fd615e1c36618e31c",
        "title": "Distral: Robust multitask reinforcement learning"
      },
      {
        "paperId": "118fae4b4d07453561f1eded88654f812c7c61ec",
        "title": "Gradient Episodic Memory for Continual Learning"
      },
      {
        "paperId": "a99d857ecc78316a0d9a774972b775058d5644ca",
        "title": "Continual Learning Through Synaptic Intelligence"
      },
      {
        "paperId": "c889d6f98e6d79b89c3a6adf8a921f88fa6ba518",
        "title": "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks"
      },
      {
        "paperId": "7270bee98bf9ed9f02037c5149e35d3871ba4c55",
        "title": "Robust and Efficient Transfer Learning with Hidden Parameter Markov Decision Processes"
      },
      {
        "paperId": "2e55ba6c97ce5eb55abd959909403fe8da7e9fe9",
        "title": "Overcoming catastrophic forgetting in neural networks"
      },
      {
        "paperId": "282a380fb5ac26d99667224cef8c630f6882704f",
        "title": "Learning to reinforcement learn"
      },
      {
        "paperId": "d7a0faf933616b078981bac1f1f7bc45beff4f8e",
        "title": "Using Task Features for Zero-Shot Knowledge Transfer in Lifelong Learning"
      },
      {
        "paperId": "8f3b80ddc0dd62e6c3369fabb1715990c29e9b9a",
        "title": "Learning without Forgetting"
      },
      {
        "paperId": "317cd4522b1f4a6f889743578143bb8823623f8b",
        "title": "VIME: Variational Information Maximizing Exploration"
      },
      {
        "paperId": "1464776f20e2bccb6182f183b5ff2e15b0ae5e56",
        "title": "Benchmarking Deep Reinforcement Learning for Continuous Control"
      },
      {
        "paperId": "4b63e34276aa98d5345efa7fe09bb06d8a9d8f52",
        "title": "Deep Exploration via Bootstrapped DQN"
      },
      {
        "paperId": "1def5d3711ebd1d86787b1ed57c91832c5ddc90b",
        "title": "Actor-Mimic: Deep Multitask and Transfer Reinforcement Learning"
      },
      {
        "paperId": "3b9732bb07dc99bde5e1f9f75251c6ea5039373e",
        "title": "Deep Reinforcement Learning with Double Q-Learning"
      },
      {
        "paperId": "da6057368920585bcf2443295b98418840f1fc80",
        "title": "Weight Uncertainty in Neural Networks"
      },
      {
        "paperId": "3cdcfe28827bf5bd6f0717ba24af991746e6050e",
        "title": "Online Multi-Task Learning for Policy Gradient Methods"
      },
      {
        "paperId": "d163ae2ae7ee2b7991ab017113f13f54fc5c8c5f",
        "title": "PAC-inspired Option Discovery in Lifelong Reinforcement Learning"
      },
      {
        "paperId": "075f328ef87a076151feb4d5b1f97b66aa597a90",
        "title": "Convex Optimization: Algorithms and Complexity"
      },
      {
        "paperId": "6b6b078ee9aabf3c17220a2da1e3f0dd822956b7",
        "title": "Chapter 3 \u2013 The Cross-Entropy Method for Optimization"
      },
      {
        "paperId": "9834f41482ca5b87931d41af3923d3c13fa8e298",
        "title": "Hidden Parameter Markov Decision Processes: A Semiparametric Regression Approach for Discovering Latent Task Parametrizations"
      },
      {
        "paperId": "b354ee518bfc1ac0d8ac447eece9edb69e92eae1",
        "title": "MuJoCo: A physics engine for model-based control"
      },
      {
        "paperId": "918e8781344e7eee60a4f6bf883c858205a917d0",
        "title": "Efficient Bayes-Adaptive Reinforcement Learning using Sample-Based Search"
      },
      {
        "paperId": "5a9ef216bf11f222438fff130c778267d39a9564",
        "title": "Practical Variational Inference for Neural Networks"
      },
      {
        "paperId": "53035588fefbbe9ebd1ff19ad697eee2e5f46df7",
        "title": "Learning is planning: near Bayes-optimal reinforcement learning via Monte-Carlo tree search"
      },
      {
        "paperId": "5d8e1eeeb0e4b0e0846a355532d0f9452249e68a",
        "title": "Reinforcement Learning in Finite MDPs: PAC Analysis"
      },
      {
        "paperId": "606c3108fe948d9a8a0da8759f88de4df53b5d94",
        "title": "A Bayesian Sampling Approach to Exploration in Reinforcement Learning"
      },
      {
        "paperId": "638ab3eb4723add266e484f8f792cd6e14ce5e3e",
        "title": "Multi-task reinforcement learning: a hierarchical Bayesian approach"
      },
      {
        "paperId": "ac4b7a6dad66a9a09e69019936d1c182f7f2a6e0",
        "title": "From \u025b-entropy to KL-entropy: Analysis of minimum information complexity density estimation"
      },
      {
        "paperId": "c962e92295affd68556bdcda5bf1882cb73bd35d",
        "title": "Incremental Model-based Learners With Formal Learning-Time Guarantees"
      },
      {
        "paperId": "48cce5ee49facf75eeb12832c387452424b645dd",
        "title": "A Bayesian Framework for Reinforcement Learning"
      },
      {
        "paperId": "12d1d070a53d4084d88a77b8b143bad51c40c38f",
        "title": "Reinforcement Learning: A Survey"
      },
      {
        "paperId": "a9cd8efe9184dddb1bedbbec3a356c4dfb22fe63",
        "title": "Markov Decision Processes: Discrete Stochastic Dynamic Programming"
      },
      {
        "paperId": "25c9f33aceac6dcff357727cbe2faf145b01d13c",
        "title": "Keeping the neural networks simple by minimizing the description length of the weights"
      },
      {
        "paperId": "ee2cd1d17f833d3c157a1016a778c7c22af555a2",
        "title": "ON THE LIKELIHOOD THAT ONE UNKNOWN PROBABILITY EXCEEDS ANOTHER IN VIEW OF THE EVIDENCE OF TWO SAMPLES"
      },
      {
        "paperId": "c7dbf5ed7e9b63e104adb4e18bfbc98f5d6afdae",
        "title": "R-MAX - A General Polynomial Time Algorithm for Near-Optimal Reinforcement Learning"
      },
      {
        "paperId": "97efafdb4a3942ab3efba53ded7413199f79c054",
        "title": "Reinforcement Learning: An Introduction"
      },
      {
        "paperId": null,
        "title": "Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)?"
      },
      {
        "paperId": null,
        "title": "c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)?"
      },
      {
        "paperId": null,
        "title": "c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation?"
      },
      {
        "paperId": null,
        "title": "For each task,"
      }
    ],
    "cited_by": [
      {
        "paperId": "da5e3e299854419ea47e19a2b82800ee71e927bd",
        "title": "A Survey of Continual Reinforcement Learning"
      },
      {
        "paperId": "1dd358d3bc8bbd4c3d4d04a4b0732e5a66bc9874",
        "title": "Entropy-regularized Gradient Estimators for Approximate Bayesian Inference"
      },
      {
        "paperId": "de8856e1755fdbbb9ba94e3a4d56628a0526ad8f",
        "title": "Knowledge Retention for Continual Model-Based Reinforcement Learning"
      },
      {
        "paperId": "89b7d22582b4ce7f4bcd70fda6caaf38dc283ff5",
        "title": "Statistical Guarantees for Lifelong Reinforcement Learning using PAC-Bayesian Theory"
      },
      {
        "paperId": "8c2a8b876d7d2ed3df7b520d4310287303fafc1e",
        "title": "Continual Offline Reinforcement Learning via Diffusion-based Dual Generative Replay"
      },
      {
        "paperId": "3d7e5485fae2965ddf081dc64be6ab52f5834cf8",
        "title": "Continual Learning: Applications and the Road Forward"
      },
      {
        "paperId": "04fb4b1d88f485cacba6e90c1137448f0d770498",
        "title": "TAIL: Task-specific Adapters for Imitation Learning with Large Pretrained Models"
      },
      {
        "paperId": "da1766e02346e7eb238249d06643459450ffdde6",
        "title": "A Definition of Continual Reinforcement Learning"
      },
      {
        "paperId": "ec446a32701d9e98d1f5c68ea57664192b75784e",
        "title": "Ensemble Value Functions for Efficient Exploration in Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "6895c7e5143d64d23cd1a1bd721edc07be8b6f02",
        "title": "The Effectiveness of World Models for Continual Reinforcement Learning"
      },
      {
        "paperId": "42306a959a67d237e90d96b4823a98642978ea79",
        "title": "Meta-learning Parameterized Skills"
      },
      {
        "paperId": "003b229489d88890674f0c6bffa41fed8df93cec",
        "title": "Performance Bounds for Model and Policy Transfer in Hidden-parameter MDPs"
      },
      {
        "paperId": "1532ce2de5f8fa722d119dd1d276d7c6ef65f1db",
        "title": "Recurrent Policies Are Not Enough for Continual Reinforcement Learning"
      },
      {
        "paperId": "0410942deba14df1cf37f3045ea6ddb788a674f7",
        "title": "Knowledge Retention in Continual Model-Based Reinforcement Learning"
      }
    ],
    "score": 4.666666666666666
  },
  {
    "id": "a45df3efbd472d4d43ddc4072c61f7f674981ac6",
    "title": "Causal Discovery and Reinforcement Learning: A Synergistic Integration",
    "authors": [
      "Arqu\u00edmides M\u00e9ndez-Molina",
      "E. Morales",
      "L. Sucar"
    ],
    "year": 2022,
    "citationCount": 14,
    "abstract": "",
    "url": "https://www.semanticscholar.org/paper/a45df3efbd472d4d43ddc4072c61f7f674981ac6",
    "pdf_url": null,
    "venue": "European Workshop on Probabilistic Graphical Models",
    "publicationDate": "",
    "externalIds": {
      "DBLP": "conf/pgm/Mendez-Molina0S22",
      "CorpusId": 252898912
    },
    "references": [],
    "cited_by": [
      {
        "paperId": "c84f2784208cf671c8e9ccb939f73a324efa1c84",
        "title": "Causal Reflection with Language Models"
      },
      {
        "paperId": "aad755573f2854827834120c6f32fabe4f8ff612",
        "title": "Towards Causal Model-Based Policy Optimization"
      },
      {
        "paperId": "803b21f5c183b8406a6da5b9e111501a0156e50c",
        "title": "Discovering emergent connections in quantum physics research via dynamic word embeddings"
      },
      {
        "paperId": "d8bdfe5a328f806864840ebca91805bb940ebe4d",
        "title": "Adaptive scheduling for Internet of Vehicles using deconfounded graph transfer learning"
      },
      {
        "paperId": "ddb44c4a2aa0db9a6c2376925533bf4547b81824",
        "title": "CORE: Towards Scalable and Efficient Causal Discovery with Reinforcement Learning"
      },
      {
        "paperId": "b5c7eca234413753f4fce66cb23cab3527c8c8f3",
        "title": "Causal reinforcement learning based on Bayesian networks applied to industrial settings"
      },
      {
        "paperId": "110e47eb13e0e1aed94608a213e7baa709e0bc38",
        "title": "Causal Deep Learning"
      },
      {
        "paperId": "cca6862e4661534e2bbbfc60c36564df1c484c36",
        "title": "Short: Causal structural learning of conversational engagement for socially isolated older adults."
      },
      {
        "paperId": "2174456f0da8d7c4464bd338b374e3244956f06e",
        "title": "Q-Cogni: An Integrated Causal Reinforcement Learning Framework"
      },
      {
        "paperId": "f95969cdf3c9861cd39559940e023a09297f3faa",
        "title": "A Meta-Reinforcement Learning Algorithm for Causal Discovery"
      },
      {
        "paperId": "b2aa989af58ebad330340265acb26d39e71676a0",
        "title": "CARL: A Synergistic Framework for Causal Reinforcement Learning"
      },
      {
        "paperId": "b1eff913e58937d65d3638b0d5a6892166015883",
        "title": "UvA-DARE (Digital Academic Repository) A Meta-Reinforcement Learning Algorithm for Causal Discovery"
      },
      {
        "paperId": "a312ab4f7dd50641471f926daca5e1f32041b53b",
        "title": "CORE: Towards Scalable and E(cid:30)icient Causal Discovery with Reinforcement Learning"
      },
      {
        "paperId": "549bb8f2842655ea7c4c2d2a25f7df53d4f02e2d",
        "title": "Engineering Applications of Artificial Intelligence"
      }
    ],
    "score": 4.666666666666666
  },
  {
    "id": "5c0b56d9440ea70c79d1d0032d46c14e06f541f5",
    "title": "Reactive Exploration to Cope with Non-Stationarity in Lifelong Reinforcement Learning",
    "authors": [
      "C. Steinparz",
      "Thomas Schmied",
      "Fabian Paischer",
      "Marius-Constantin Dinu",
      "Vihang Patil",
      "Angela Bitto-Nemling",
      "Hamid Eghbalzadeh",
      "Sepp Hochreiter"
    ],
    "year": 2022,
    "citationCount": 14,
    "abstract": "In lifelong learning, an agent learns throughout its entire life without resets, in a constantly changing environment, as we humans do. Consequently, lifelong learning comes with a plethora of research problems such as continual domain shifts, which result in non-stationary rewards and environment dynamics. These non-stationarities are difficult to detect and cope with due to their continuous nature. Therefore, exploration strategies and learning methods are required that are capable of tracking the steady domain shifts, and adapting to them. We propose Reactive Exploration to track and react to continual domain shifts in lifelong reinforcement learning, and to update the policy correspondingly. To this end, we conduct experiments in order to investigate different exploration strategies. We empirically show that representatives of the policy-gradient family are better suited for lifelong learning, as they adapt more quickly to distribution shifts than Q-learning. Thereby, policy-gradient methods profit the most from Reactive Exploration and show good results in lifelong learning with continual domain shifts. Our code is available at: https://github.com/ml-jku/reactive-exploration.",
    "url": "https://www.semanticscholar.org/paper/5c0b56d9440ea70c79d1d0032d46c14e06f541f5",
    "pdf_url": "https://arxiv.org/pdf/2207.05742.pdf",
    "venue": "CoLLAs",
    "publicationDate": "2022-07-12",
    "externalIds": {
      "DBLP": "journals/corr/abs-2207-05742",
      "ArXiv": "2207.05742",
      "DOI": "10.48550/arXiv.2207.05742",
      "CorpusId": 250451599
    },
    "references": [
      {
        "paperId": "3f3c01adbdd433d515c19ac8cf6c61c905f0061a",
        "title": "History Compression via Language Models in Reinforcement Learning"
      },
      {
        "paperId": "07847dcd97b1cdda19c5b8223fbbff28c8a616da",
        "title": "Abstraction for Deep Reinforcement Learning"
      },
      {
        "paperId": "26bd86d04fc5d741d36868ff4aceeb65db1be441",
        "title": "Lifelong Hyper-Policy Optimization with Multiple Importance Sampling Regularization"
      },
      {
        "paperId": "f8befa0bc3442979ff19e070f7c6b16d66a776c5",
        "title": "Bootstrapped Meta-Learning"
      },
      {
        "paperId": "94cefa04e0f834272d85cc425e0adfb27fd17e08",
        "title": "Same State, Different Task: Continual Reinforcement Learning without Interference"
      },
      {
        "paperId": "090273ad6b3720027e34f9183576dd2812bb4454",
        "title": "Continual World: A Robotic Benchmark For Continual Reinforcement Learning"
      },
      {
        "paperId": "e30bdcfc8cc2769ffd6d39c5d60651787a8d9ab3",
        "title": "Minimum-Delay Adaptation in Non-Stationary Reinforcement Learning via Online High-Confidence Change-Point Detection"
      },
      {
        "paperId": "9faecf3e18a833f2d49b030d591cc2ded0b54336",
        "title": "Towards Continual Reinforcement Learning: A Review and Perspectives"
      },
      {
        "paperId": "43ec60bff07bd4508e2014428a3531581d90adac",
        "title": "Convergence Proof for Actor-Critic Methods Applied to PPO and RUDDER"
      },
      {
        "paperId": "2999921763d317c253332289500a2e24c44f28e1",
        "title": "Cross-Domain Few-Shot Learning by Representation Fusion"
      },
      {
        "paperId": "3dac7e72c1b691bbc8ccae98883b8dd4b18009cc",
        "title": "Task-Agnostic Continual Learning Using Online Variational Bayes With Fixed-Point Updates"
      },
      {
        "paperId": "580d85d3ce5475a2931b2147648b1fb79ef03358",
        "title": "Align-RUDDER: Learning From Few Demonstrations by Reward Redistribution"
      },
      {
        "paperId": "d531d2eda61a328c04c4fd36b14770c0195108b0",
        "title": "La-MAML: Look-ahead Meta Learning for Continual Learning"
      },
      {
        "paperId": "57ffba5ea5e602b5365752e19a6f85a716db1d8b",
        "title": "Bandit Algorithms"
      },
      {
        "paperId": "3ecfe6f3daafc1b843879ba89e4a06260b120312",
        "title": "Supermasks in Superposition"
      },
      {
        "paperId": "5b3e68658c99ed9c461a909b16b862221946d6ad",
        "title": "Reinforcement Learning for Non-Stationary Markov Decision Processes: The Blessing of (More) Optimism"
      },
      {
        "paperId": "b8ae665b04dbdc67d6126bff9c7ea12fd3f2b5d2",
        "title": "Continual Learning in Recurrent Neural Networks with Hypernetworks"
      },
      {
        "paperId": "43cfafb95bbfc2974437c517a6d63107ecfca12d",
        "title": "The Impact of Non-stationarity on Generalisation in Deep Reinforcement Learning"
      },
      {
        "paperId": "38dc7424bf7de0da9934283ee26dc945b17f276b",
        "title": "A Simple Approach for Non-stationary Linear Bandits"
      },
      {
        "paperId": "48487c1d8e922ab4ee40f7544d4866cdc67ba79e",
        "title": "Optimizing for the Future in Non-Stationary MDPs"
      },
      {
        "paperId": "bebe8ffb0c357ac0c7eea2556f817b03ee22b570",
        "title": "RIDE: Rewarding Impact-Driven Exploration for Procedurally-Generated Environments"
      },
      {
        "paperId": "665fbb2645d1213e7eb95d870acd2ed75c74d1a5",
        "title": "Jelly Bean World: A Testbed for Never-Ending Learning"
      },
      {
        "paperId": "086159600bede14e00f96043c733d4f3b45855aa",
        "title": "Never Give Up: Learning Directed Exploration Strategies"
      },
      {
        "paperId": "821cb38c5f408e681840b3237093ad0cd33b6aa7",
        "title": "Weighted Linear Bandits for Non-Stationary Environments"
      },
      {
        "paperId": "2aeb13802038f0b37ad39400e41d2d81ef1597a8",
        "title": "Forward and Backward Knowledge Transfer for Sentiment Classification"
      },
      {
        "paperId": "4f7cfa0add7b20e2043373dc17604cc9cd17ff23",
        "title": "Neural Replicator Dynamics."
      },
      {
        "paperId": "475d35a691580d0082ecae212a81d9a4b6be787d",
        "title": "Meta-Learning Representations for Continual Learning"
      },
      {
        "paperId": "a27ae33793a51aadf51b1c1133c74d9c63cbdf74",
        "title": "Reinforcement Learning in Non-Stationary Environments"
      },
      {
        "paperId": "64f3ba0e812dd88c2b8288351a8e01e01dbc4e86",
        "title": "Non-Stationary Markov Decision Processes a Worst-Case Approach using Model-Based Reinforcement Learning"
      },
      {
        "paperId": "b8c0071c74e04ea1598ed2a208cbc255656f50b0",
        "title": "A Meta-MDP Approach to Exploration for Lifelong Reinforcement Learning"
      },
      {
        "paperId": "c520bf47db3360ae3a52219771390a354ed8a91f",
        "title": "Go-Explore: a New Approach for Hard-Exploration Problems"
      },
      {
        "paperId": "d3624ffcd77acf8319d9c517cb3e7e91d756bc18",
        "title": "Change point detection for compositional multivariate data"
      },
      {
        "paperId": "4cb3fd057949624aa4f0bbe7a6dcc8777ff04758",
        "title": "Exploration by Random Network Distillation"
      },
      {
        "paperId": "4cb294eb1935937bdc1b311f6dccbcd41d89a43a",
        "title": "Memory Efficient Experience Replay for Streaming Learning"
      },
      {
        "paperId": "9f67b3edc67a35c884bd532a5e73fa3a7f3660d8",
        "title": "Count-Based Exploration with the Successor Representation"
      },
      {
        "paperId": "386cba43ddb84d806e4447d80cc8432b019d055e",
        "title": "Continuous Learning in Single-Incremental-Task Scenarios"
      },
      {
        "paperId": "bad355642cd299caca2328dae02563278ea74e8c",
        "title": "RUDDER: Return Decomposition for Delayed Rewards"
      },
      {
        "paperId": "a4b158ec2c64731c1fc6a293476f611a15fa32eb",
        "title": "Lifelong Learning of Spatiotemporal Representations With Dual-Memory Recurrent Self-Organization"
      },
      {
        "paperId": "2ac1373744bec77a2e2a6f98753ff94be1cf9dd6",
        "title": "Learning Contextual Bandits in a Non-stationary Environment"
      },
      {
        "paperId": "73d5c3cff5a777169dc62d0b351ddcfda503cd33",
        "title": "Reinforced Continual Learning"
      },
      {
        "paperId": "3a1dc502dfda4bde3de3afa4063aac801705b57f",
        "title": "Nearly Optimal Adaptive Procedure with Change Detection for Piecewise-Stationary Bandit"
      },
      {
        "paperId": "3aa673abd49f0837ed2fbf5cffcada9ba6f48693",
        "title": "Overcoming catastrophic forgetting with hard attention to the task"
      },
      {
        "paperId": "47bc048efb90e7b8bae5c1fcc979a78b65763fe9",
        "title": "PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning"
      },
      {
        "paperId": "49f03a1944f32d22453f87ba33965ce84776a888",
        "title": "Taming Non-stationary Bandits: A Bayesian Approach"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "0f810eb4777fd05317951ebaa7a3f5835ee84cf4",
        "title": "Count-Based Exploration in Feature Space for Reinforcement Learning"
      },
      {
        "paperId": "225ab689f41cef1dc18237ef5dab059a49950abf",
        "title": "Curiosity-Driven Exploration by Self-Supervised Prediction"
      },
      {
        "paperId": "0a9a3380090d994e867026388f64a85481cdb700",
        "title": "Overcoming Catastrophic Forgetting by Incremental Moment Matching"
      },
      {
        "paperId": "a99d857ecc78316a0d9a774972b775058d5644ca",
        "title": "Continual Learning Through Synaptic Intelligence"
      },
      {
        "paperId": "d6757aedcb53142bc439ec64bfd0b056d99b1881",
        "title": "Surprise-Based Intrinsic Motivation for Deep Reinforcement Learning"
      },
      {
        "paperId": "12f67fb182bc934fc95ce97acff553d83e2ca72e",
        "title": "Count-Based Exploration with Neural Density Models"
      },
      {
        "paperId": "9172cd6c253edf7c3a1568e03577db20648ad0c4",
        "title": "Reinforcement Learning with Deep Energy-Based Policies"
      },
      {
        "paperId": "321f1877bc570ff9b318e909cefb7c27138458df",
        "title": "PathNet: Evolution Channels Gradient Descent in Super Neural Networks"
      },
      {
        "paperId": "2e55ba6c97ce5eb55abd959909403fe8da7e9fe9",
        "title": "Overcoming catastrophic forgetting in neural networks"
      },
      {
        "paperId": "0fcb2034e31a2bc2f12a2b1363d0d77baf445fdf",
        "title": "#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning"
      },
      {
        "paperId": "8f3b80ddc0dd62e6c3369fabb1715990c29e9b9a",
        "title": "Learning without Forgetting"
      },
      {
        "paperId": "53c9443e4e667170acc60ca1b31a0ec7151fe753",
        "title": "Progressive Neural Networks"
      },
      {
        "paperId": "6e90fd78e8a3b98af3954aae5209703aa966603e",
        "title": "Unifying Count-Based Exploration and Intrinsic Motivation"
      },
      {
        "paperId": "2b10281297ee001a9f3f4ea1aa9bea6b638c27df",
        "title": "OpenAI Gym"
      },
      {
        "paperId": "c6170fa90d3b2efede5a2e1660cb23e1c824f2ca",
        "title": "Prioritized Experience Replay"
      },
      {
        "paperId": "2470fcf0f89082de874ac9133ccb3a8667dd89a8",
        "title": "Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models"
      },
      {
        "paperId": "340f48901f72278f6bf78a04ee5b01df208cc508",
        "title": "Human-level control through deep reinforcement learning"
      },
      {
        "paperId": "0ba86604228b555475496e200f31878df3aabd6e",
        "title": "Never-Ending Learning"
      },
      {
        "paperId": "2295f2034c2a45a39fd1a08605d2a8e0588e7e4d",
        "title": "Stochastic Multi-Armed-Bandit Problem with Non-stationary Rewards"
      },
      {
        "paperId": "fbd99f4c29d6e5859e502510f92f65b7a08c4759",
        "title": "Optimal Exploration-Exploitation in a Multi-Armed-Bandit Problem with Non-Stationary Rewards"
      },
      {
        "paperId": "2319a491378867c7049b3da055c5df60e1671158",
        "title": "Playing Atari with Deep Reinforcement Learning"
      },
      {
        "paperId": "3039ceebdf1b2f4831482a64ee1646b1414e790b",
        "title": "The stability-plasticity dilemma: investigating the continuum from catastrophic forgetting to age-limited learning effects"
      },
      {
        "paperId": "e1f2997d3051dc89c5e8bb22f18b4737adae6629",
        "title": "Motion-Dependent Representation of Space in Area MT+"
      },
      {
        "paperId": "a78273144520d57e150744cf75206e881e11cc5b",
        "title": "Multimodal Deep Learning"
      },
      {
        "paperId": "66d398aeaeb7ec24ededb1adaa4b4f09a6c1bcde",
        "title": "A theory of learning from different domains"
      },
      {
        "paperId": "237a1cf18ed83bb3ad852b34f443c6c1ff3336c1",
        "title": "An analysis of model-based Interval Estimation for Markov Decision Processes"
      },
      {
        "paperId": "6eb7f22b9329ff77d0bdb6d86f35a7b6e62be1e3",
        "title": "On Upper-Confidence Bound Policies for Non-Stationary Bandit Problems"
      },
      {
        "paperId": "9cacd41e4fbe518436877a6f1b24982099216e46",
        "title": "Bayesian Online Changepoint Detection"
      },
      {
        "paperId": "e8077729ef723d71a0b7492f2f271ae413b5a4d5",
        "title": "What is Intrinsic Motivation? A Typology of Computational Approaches"
      },
      {
        "paperId": "ec6c97715c02d6f27cb5d861f424f2a54c5e23bb",
        "title": "Detecting Change in Data Streams"
      },
      {
        "paperId": "03dbb37d2373500115735ed69871e94c7f6b2c5e",
        "title": "Automatic Discovery of Subgoals in Reinforcement Learning using Diverse Density"
      },
      {
        "paperId": "2722b9e5ab8da95f03e578bb65879c452c105385",
        "title": "Catastrophic forgetting in connectionist networks"
      },
      {
        "paperId": "116d7798c1123cf7fad4176e98f58fd49de4f8f1",
        "title": "Planning and Acting in Partially Observable Stochastic Domains"
      },
      {
        "paperId": "dde691805cfa7d6f1bb88c7411c1c3377b6cdc67",
        "title": "Lifelong Learning Algorithms"
      },
      {
        "paperId": "2e9d221c206e9503ceb452302d68d10e293f2a10",
        "title": "Long Short-Term Memory"
      },
      {
        "paperId": "161ffb54a3fdf0715b198bb57bd22f910242eb49",
        "title": "Multitask Learning"
      },
      {
        "paperId": "83cdbbfad2d4bb4c348e09cf3db779f020809b05",
        "title": "Learning in the Presence of Concept Drift and Hidden Contexts"
      },
      {
        "paperId": "08627b368e6b7d142cfcece74d52d21d48cc64a6",
        "title": "Neural Network Exploration Using Optimal Experiment Design"
      },
      {
        "paperId": "6c8f0f28bcbc358726035cfd243239fd32eb8cba",
        "title": "Lifelong robot learning"
      },
      {
        "paperId": "fb61c5320b4f61b6629e0c6fe6fe1b005220ca54",
        "title": "Detection of abrupt changes: theory and application"
      },
      {
        "paperId": "03b7e51c52084ac1db5118342a00b5fbcfc587aa",
        "title": "Q-learning"
      },
      {
        "paperId": "9cd8193a66cf53143cbba6ccb0c7b9c2ebf2452b",
        "title": "Self-improving reactive agents based on reinforcement learning, planning and teaching"
      },
      {
        "paperId": "2980dfe5c99658dc3e508d9d6e1d7f26e6fc8934",
        "title": "A possibility for implementing curiosity and boredom in model-building neural controllers"
      },
      {
        "paperId": "65f9020e5e3fa83c3847cee33045af52c4a24faa",
        "title": "ART 2: self-organization of stable category recognition codes for analog input patterns."
      },
      {
        "paperId": "5aa24533e5ad3ef48a04b90f7484791b94e6870f",
        "title": "Counterintuitive behavior of social systems"
      },
      {
        "paperId": "b9164335be5808ddd59786869a9f992331af5218",
        "title": "The organization of behavior: A neuropsychological theory"
      },
      {
        "paperId": "59d823fc9877f8d448ac4c2d90e38051026e3201",
        "title": "Individual Comparisons by Ranking Methods"
      },
      {
        "paperId": "8d15f17ea8f807efe8801d236b7218b6659ac1d9",
        "title": "NovelD: A Simple yet Effective Exploration Criterion"
      },
      {
        "paperId": "1948a3ea0a68d0c7b70178c3d2c408e2920942ac",
        "title": "Non-Stationary Off-Policy Optimization"
      },
      {
        "paperId": "9f02657034d5cef9279083bb66706ee3ce84f46e",
        "title": "Deep Reinforcement Learning amidst Continual Structured Non-Stationarity"
      },
      {
        "paperId": null,
        "title": "Vihang Prakash Patil, Angela Bitto-Nemling, and Sepp Hochreiter. Modern Hopfield Networks for Return Decomposition for Delayed Rewards"
      },
      {
        "paperId": null,
        "title": "Infinite steps cartpole problem with variable reward"
      },
      {
        "paperId": "97c76bf86b740e6a47a9cc992279799d0c93b367",
        "title": "Model-Free Non-Stationarity Detection and Adaptation in Reinforcement Learning"
      },
      {
        "paperId": "95cfa4a07e1f4958bd7ae27d033686f873af9565",
        "title": "XAI and Strategy Extraction via Reward Redistribution"
      },
      {
        "paperId": null,
        "title": "Minimalistic gridworld environment for openai gym, 2018"
      },
      {
        "paperId": "8784f905f4f9fb6fa4a3cc9b0faa5b5479c687ec",
        "title": "On the Optimization of a Synaptic Learning Rule"
      },
      {
        "paperId": "4dee4417277b80101d0a8664ffec60091a5e9abe",
        "title": "Change Point Detection and Meta-Bandits for Online Learning in Dynamic Environments"
      },
      {
        "paperId": "4777cec051971edb5281172189a34f0325466bec",
        "title": "Subgoal Discovery for Hierarchical Reinforcement Learning Using Learned Policies"
      },
      {
        "paperId": "97efafdb4a3942ab3efba53ded7413199f79c054",
        "title": "Reinforcement Learning: An Introduction"
      },
      {
        "paperId": "2547be25e1e07728aa0966a0354e90664816d15e",
        "title": "REINFORCEMENT DRIVEN INFORMATION ACQUISITION IN NON-DETERMINISTIC ENVIRONMENTS"
      },
      {
        "paperId": "082b1f5c791cadef18c4920ecc1396615a3fe7cb",
        "title": "Continual learning in reinforcement environments"
      },
      {
        "paperId": "f4c517d1e5f5dfd09f78e3976fc99c36e98aa36a",
        "title": "Detection of abrupt changes"
      },
      {
        "paperId": null,
        "title": "Making the world differentiable: On using fully recurrent self-supervised neural networks for dynamic reinforcement learning and planning in non-stationary environments"
      },
      {
        "paperId": "c213af6582c0d518a6e8e14217611c733eeb1ef1",
        "title": "Catastrophic Interference in Connectionist Networks: The Sequential Learning Problem"
      },
      {
        "paperId": "bdaec1b3eb9a8f7a2b296be009a148c35236f3ce",
        "title": "Evolutionary principles in self-referential learning, or on learning how to learn: The meta-meta-. hook"
      },
      {
        "paperId": "22069cd4504656d3bb85748a4d43be7a4d7d5545",
        "title": "Temporal credit assignment in reinforcement learning"
      }
    ],
    "cited_by": [
      {
        "paperId": "da5e3e299854419ea47e19a2b82800ee71e927bd",
        "title": "A Survey of Continual Reinforcement Learning"
      },
      {
        "paperId": "87412ed8f790d73e8daa0ed01f61ba2126a2f991",
        "title": "Overcoming Non-stationary Dynamics with Evidential Proximal Policy Optimization"
      },
      {
        "paperId": "1e3f7f3c95797ffe8b60e13e556818f402e39374",
        "title": "Investigation of Lifelong Learning Research Hotspots and Trends: A Bibliometric Analysis Based on CNKI Core Journals"
      },
      {
        "paperId": "11d89b4578abfb7b34e7378db61412040b28ecb1",
        "title": "A Survey on Recent Advancements in Autonomous Driving Using Deep Reinforcement Learning: Applications, Challenges, and Solutions"
      },
      {
        "paperId": "89b7d22582b4ce7f4bcd70fda6caaf38dc283ff5",
        "title": "Statistical Guarantees for Lifelong Reinforcement Learning using PAC-Bayesian Theory"
      },
      {
        "paperId": "aa457f74c5e56fec5a9c4eb7391d3b9f7073ef11",
        "title": "Contrastive Abstraction for Reinforcement Learning"
      },
      {
        "paperId": "137f90612e92ba07c915deea0f9241cab7ea9042",
        "title": "Self-Evolution Policy Learning: Leveraging Basic Tasks to Complex Ones"
      },
      {
        "paperId": "d1f7a3e0d40986aafb6c4ca7e3881813f9cc2cb1",
        "title": "Using Weighted Mixture Policy Prior Achieving Individual Optimal Policies in Nonstationary Environments"
      },
      {
        "paperId": "f7f00090e3660c0cb061324522c73f16eb52008b",
        "title": "Predictive reinforcement learning in non-stationary environments using weighted mixture policy"
      },
      {
        "paperId": "80543d5e827605d5d42f0af7ea42697e2dd7f62a",
        "title": "Learning to Modulate pre-trained Models in RL"
      },
      {
        "paperId": "6895c7e5143d64d23cd1a1bd721edc07be8b6f02",
        "title": "The Effectiveness of World Models for Continual Reinforcement Learning"
      },
      {
        "paperId": "9faecf3e18a833f2d49b030d591cc2ded0b54336",
        "title": "Towards Continual Reinforcement Learning: A Review and Perspectives"
      },
      {
        "paperId": "2e1ca97d8f7604e3b334ff4903bfa67267379317",
        "title": "The Surprising Effectiveness of Latent World Models for Continual Reinforcement Learning"
      },
      {
        "paperId": "452452f203866be2ede5416fb598618a6419d4b9",
        "title": "InfODist: Online distillation with Informative rewards improves generalization in Curriculum Learning"
      }
    ],
    "score": 4.666666666666666
  },
  {
    "id": "0cdbd90989e5f60b6a42dae76b23bd489fcf65cc",
    "title": "Robust Policy Optimization in Deep Reinforcement Learning",
    "authors": [
      "Md Masudur Rahman",
      "Yexiang Xue"
    ],
    "year": 2022,
    "citationCount": 14,
    "abstract": "The policy gradient method enjoys the simplicity of the objective where the agent optimizes the cumulative reward directly. Moreover, in the continuous action domain, parameterized distribution of action distribution allows easy control of exploration, resulting from the variance of the representing distribution. Entropy can play an essential role in policy optimization by selecting the stochastic policy, which eventually helps better explore the environment in reinforcement learning (RL). However, the stochasticity often reduces as the training progresses; thus, the policy becomes less exploratory. Additionally, certain parametric distributions might only work for some environments and require extensive hyperparameter tuning. This paper aims to mitigate these issues. In particular, we propose an algorithm called Robust Policy Optimization (RPO), which leverages a perturbed distribution. We hypothesize that our method encourages high-entropy actions and provides a way to represent the action space better. We further provide empirical evidence to verify our hypothesis. We evaluated our methods on various continuous control tasks from DeepMind Control, OpenAI Gym, Pybullet, and IsaacGym. We observed that in many settings, RPO increases the policy entropy early in training and then maintains a certain level of entropy throughout the training period. Eventually, our agent RPO shows consistently improved performance compared to PPO and other techniques: entropy regularization, different distributions, and data augmentation. Furthermore, in several settings, our method stays robust in performance, while other baseline mechanisms fail to improve and even worsen the performance.",
    "url": "https://www.semanticscholar.org/paper/0cdbd90989e5f60b6a42dae76b23bd489fcf65cc",
    "pdf_url": "https://arxiv.org/pdf/2212.07536.pdf",
    "venue": "arXiv.org",
    "publicationDate": "2022-12-14",
    "externalIds": {
      "DBLP": "journals/corr/abs-2212-07536",
      "ArXiv": "2212.07536",
      "DOI": "10.48550/arXiv.2212.07536",
      "CorpusId": 254685808
    },
    "references": [
      {
        "paperId": "196d0e7f1ce738a4ec5b65120713f02f6f7204ac",
        "title": "Bootstrap State Representation using Style Transfer for Better Generalization in Deep Reinforcement Learning"
      },
      {
        "paperId": "69b80ce5ab6c2263b145b1eaa23664145938f351",
        "title": "The Primacy Bias in Deep Reinforcement Learning"
      },
      {
        "paperId": "49142e3e381c0dc7fee0049ea41d2ef02c0340d7",
        "title": "Isaac Gym: High Performance GPU-Based Physics Simulation For Robot Learning"
      },
      {
        "paperId": "05c82617cdaa16c9dc17c32f3cb5ed4a7182b13e",
        "title": "Automatic Data Augmentation for Generalization in Deep Reinforcement Learning"
      },
      {
        "paperId": "8ba600c169f0d2422625822223976bce562eabe1",
        "title": "dm_control: Software and Tasks for Continuous Control"
      },
      {
        "paperId": "b76ee983dd28a388ee95a2ab4cf70e7b6f169e12",
        "title": "On the Global Convergence Rates of Softmax Policy Gradient Methods"
      },
      {
        "paperId": "744139d65c3bf6da6a6acd384a32d94a06f44f62",
        "title": "Reinforcement Learning with Augmented Data"
      },
      {
        "paperId": "f1697ce4dddb58533d7d3f937fed74807d46edb8",
        "title": "Implementation Matters in Deep RL: A Case Study on PPO and TRPO"
      },
      {
        "paperId": "c4955faa27e082a80504285c28324c58eb52250c",
        "title": "Understanding the impact of entropy on policy optimization"
      },
      {
        "paperId": "ffb90ac625db6c398d019c1d7e5a180581114517",
        "title": "A Closer Look at Deep Policy Gradients"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "2b10281297ee001a9f3f4ea1aa9bea6b638c27df",
        "title": "OpenAI Gym"
      },
      {
        "paperId": "69e76e16740ed69f4dc55361a3d319ac2f1293dd",
        "title": "Asynchronous Methods for Deep Reinforcement Learning"
      },
      {
        "paperId": "d316c82c12cf4c45f9e85211ef3d1fa62497bff8",
        "title": "High-Dimensional Continuous Control Using Generalized Advantage Estimation"
      },
      {
        "paperId": "449532187c94af3dd3aa55e16d2c50f7854d2199",
        "title": "Trust Region Policy Optimization"
      },
      {
        "paperId": "b18833db0de9393d614d511e60821a1504fc6cd1",
        "title": "A Natural Policy Gradient"
      },
      {
        "paperId": "c375c33093d2be25bafd894866ad959fab910c00",
        "title": "CleanRL: High-quality Single-file Implementations of Deep Reinforcement Learning Algorithms"
      },
      {
        "paperId": null,
        "title": "Principles and Practice of Knowledge Discovery in Databases (ECML-PKDD"
      },
      {
        "paperId": "f6264b11ec0dd9133f1d88a5288a3267a93182f8",
        "title": "What Matters for On-Policy Deep Actor-Critic Methods? A Large-Scale Study"
      },
      {
        "paperId": "0e658618c9dad4d70dd7dcd5c519185ec4f845f5",
        "title": "In Advances in Neural Information Processing Systems"
      },
      {
        "paperId": "6bc8db0c7444d9c07aad440393b2fd300fb3595c",
        "title": "Function Optimization using Connectionist Reinforcement Learning Algorithms"
      },
      {
        "paperId": null,
        "title": "Data augmentation return curve and entropy: Figure 12 shows data augmentation return curve and entropy comparison with RPO"
      },
      {
        "paperId": null,
        "title": "Entropy Plot of Gym and Pybullet Environments for different action distributions"
      },
      {
        "paperId": null,
        "title": "Pybullet, a python module for physics simulation for games, robotics and machine learning"
      },
      {
        "paperId": null,
        "title": "Entropy Regularization The results comparison of RPO with entropy coefficient are in Figure 10"
      },
      {
        "paperId": null,
        "title": "The 37 implementation details of proximal policy optimization"
      }
    ],
    "cited_by": [
      {
        "paperId": "8128dbed4f3dd6190352ac582b2daf4074f87b86",
        "title": "Rich State Observations Empower Reinforcement Learning to Surpass PID: A Drone Ball Balancing Study"
      },
      {
        "paperId": "92727378cc6bd03873ceaa519bed75ff9a26ba18",
        "title": "An effective control of large systems of active particles: An application to evacuation problem"
      },
      {
        "paperId": "d0f525dba7d3425e36316127424e67fe2c2fdb0d",
        "title": "Real-Time Execution of Action Chunking Flow Policies"
      },
      {
        "paperId": "b2886354eead522505af60d6881ad3e9b8e36b76",
        "title": "Causal Policy Learning in Reinforcement Learning: Backdoor-Adjusted Soft Actor-Critic"
      },
      {
        "paperId": "fcb53418dd03d4f6306d3237f04412adb769a415",
        "title": "KIPPO: Koopman-Inspired Proximal Policy Optimization"
      },
      {
        "paperId": "fd68eda1d22bcbd6813c59355d0196a3989f03b5",
        "title": "Reinforcement learning for healthcare operations management: methodological framework, recent developments, and future research directions"
      },
      {
        "paperId": "6242303bea1cc86ced153ee9274d491316725495",
        "title": "Learning with Expert Abstractions for Efficient Multi-Task Continuous Control"
      },
      {
        "paperId": "cd2258dca551bbf1fadfe42f128cddd88b500ee7",
        "title": "Parseval Regularization for Continual Reinforcement Learning"
      },
      {
        "paperId": "a685d5390d783cf8f9d1370feca0e7796ee40913",
        "title": "RaceMOP: Mapless Online Path Planning for Multi-Agent Autonomous Racing using Residual Policy Learning"
      },
      {
        "paperId": "5ed9afda50ad0f9569dd5013ea8b9203038f03db",
        "title": "The Definitive Guide to Policy Gradients in Deep Reinforcement Learning: Theory, Algorithms and Implementations"
      },
      {
        "paperId": "a1ddbc7a0f33875afea2c0087247e42826b99085",
        "title": "Learning Adaptive Safety for Multi-Agent Systems"
      },
      {
        "paperId": "84efea29ff6ab23aaa1385c74f5f63dbfb7602f5",
        "title": "Navigation of micro-robot swarms for targeted delivery using reinforcement learning"
      },
      {
        "paperId": "53cf80e5eadc8e4b3df358ce856cf14cb71efc18",
        "title": "Accelerating Policy Gradient by Estimating Value Function from Prior Computation in Deep Reinforcement Learning"
      },
      {
        "paperId": "e22432c9be164104187667aedce1d67eab7d8f07",
        "title": "skrl: Modular and Flexible Library for Reinforcement Learning"
      }
    ],
    "score": 4.666666666666666
  },
  {
    "id": "5f3b337e74618a2364778222162b13bd55a15e27",
    "title": "A Comparative Study of Deep Reinforcement Learning-based Transferable Energy Management Strategies for Hybrid Electric Vehicles",
    "authors": [
      "Jingyi Xu",
      "Zirui Li",
      "Li Gao",
      "Junyi Ma",
      "Qi Liu",
      "Yanan Zhao"
    ],
    "year": 2022,
    "citationCount": 14,
    "abstract": "The deep reinforcement learning-based energy management strategies (EMS) have become a promising solution for hybrid electric vehicles (HEVs). When driving cycles are changed, the neural network will be retrained, which is a time-consuming and laborious task. A more efficient way of choosing EMS is to combine deep reinforcement learning (DRL) with transfer learning, which can transfer knowledge of one domain to the other new domain, making the network of the new domain reach convergence values quickly. Different exploration methods of DRL, including adding action space noise and parameter space noise, are compared against each other in the transfer learning process in this work. Results indicate that the network added parameter space noise is more stable and faster convergent than the others. In conclusion, the best exploration method for transferable EMS is to add noise in the parameter space, while the combination of action space noise and parameter space noise generally performs poorly. Our code is available at https://github.com/BIT-XJY/RL-based-Transferable-EMS.git.",
    "url": "https://www.semanticscholar.org/paper/5f3b337e74618a2364778222162b13bd55a15e27",
    "pdf_url": "https://arxiv.org/pdf/2202.11514.pdf",
    "venue": "2022 IEEE Intelligent Vehicles Symposium (IV)",
    "publicationDate": "2022-02-22",
    "externalIds": {
      "DBLP": "conf/ivs/XuLGMLZ22",
      "ArXiv": "2202.11514",
      "DOI": "10.1109/iv51971.2022.9827042",
      "CorpusId": 247058862
    },
    "references": [
      {
        "paperId": "7229de625b96f644645ad1128075c7c9e53facd1",
        "title": "Driving conditions-driven energy management strategies for hybrid electric vehicles: A review"
      },
      {
        "paperId": "843891a4ea77e5010649f8cf582620d67d6f0d2e",
        "title": "Distributed Deep Reinforcement Learning-Based Energy and Emission Management Strategy for Hybrid Electric Vehicles"
      },
      {
        "paperId": "2820cdc13b2ea346abcc4067298a19792dbeb661",
        "title": "Energy Management for a Hybrid Electric Vehicle Based on Blended Reinforcement Learning With Backward Focusing and Prioritized Sweeping"
      },
      {
        "paperId": "f8492a321d66c381637b693a24af994af41b3cdf",
        "title": "Transfer Learning in Deep Reinforcement Learning: A Survey"
      },
      {
        "paperId": "3f88f823d7f77729e839d32f05b15a9200e12fc4",
        "title": "Data-Driven Transferred Energy Management Strategy for Hybrid Electric Vehicles via Deep Reinforcement Learning"
      },
      {
        "paperId": "f402552dc9a72e6e5d1c85560a9f187f1f8ac668",
        "title": "Importance Weighted Gaussian Process Regression for Transferable Driver Behaviour Learning in the Lane Change Scenario"
      },
      {
        "paperId": "6e68d2db07dcc3d87ba3115d9a9a8b1545d357b5",
        "title": "Transfer Learning for Driver Model Adaptation in Lane-Changing Scenarios Using Manifold Alignment"
      },
      {
        "paperId": "8d63e3da2f8fdfa3cfaa7fc923cf4217eccabae6",
        "title": "Transfer Deep Reinforcement Learning-Enabled Energy Management Strategy for Hybrid Tracked Vehicle"
      },
      {
        "paperId": "1373206f36def7313836d88cac1544c62f99f514",
        "title": "Cross-Type Transfer for Deep Reinforcement Learning Based Hybrid Electric Vehicle Energy Management"
      },
      {
        "paperId": "ce02fa51389cceef95ace87bd04d93ef162abce6",
        "title": "Rule-interposing deep reinforcement learning based energy management strategy for power-split hybrid electric vehicle"
      },
      {
        "paperId": "72127e3b5f5008b8dc302821e0100f9ece584a7f",
        "title": "State-of-charge-constraint-based energy management strategy of plug-in hybrid electric vehicle with bus route"
      },
      {
        "paperId": "c3b08304850f363ca96ba34d445cf37309657175",
        "title": "A Comparative Study on Transferable Driver Behavior Learning Methods in the Lane-Changing Scenario"
      },
      {
        "paperId": "2b6d412484d1ad4f4f0c55c9564ca1c862fbbf56",
        "title": "Reinforcement Learning for Hybrid and Plug-In Hybrid Electric Vehicle Energy Management: Recent Advances and Prospects"
      },
      {
        "paperId": "13b871c2a5fc2748bd689bfd7f465fa8915f5c9f",
        "title": "Deep reinforcement learning of energy management with continuous control strategy and traffic information for a series-parallel plug-in hybrid electric bus"
      },
      {
        "paperId": "ac2d3f0889313804ec8784b265103fed460636f8",
        "title": "Transferable Driver Behavior Learning via Distribution Adaption in the Lane Change Scenario*"
      },
      {
        "paperId": "d1ec0df7af8886100cf8fc86d559e1dfd419475d",
        "title": "Virtual-to-Real Knowledge Transfer for Driving Behavior Recognition: Framework and a Case Study"
      },
      {
        "paperId": "f4329d8d92c65ba2925cd58271deda13d394f3ff",
        "title": "A Heuristic Planning Reinforcement Learning-Based Energy Management for Power-Split Plug-in Hybrid Electric Vehicles"
      },
      {
        "paperId": "62a228c487fda72c403a933e615b38d2fafb1d59",
        "title": "Deep reinforcement learning enabled self-learning control for energy efficient driving"
      },
      {
        "paperId": "f2c868cf725b369bb92e4d210bce81a67c415302",
        "title": "Estimation of the ECMS Equivalent Factor Bounds for Hybrid Electric Vehicles"
      },
      {
        "paperId": "73bf64949db25fff8c2d438817e5e1543f954cbb",
        "title": "Continuous reinforcement learning of energy management with deep Q network for a power split hybrid electric bus"
      },
      {
        "paperId": "f9192766714c7bbfb49db795f9a8376e44961066",
        "title": "A Bi-Level Control for Energy Efficiency Improvement of a Hybrid Tracked Vehicle"
      },
      {
        "paperId": "498238a3bd5fd322fc3ce1572e33bbe3853a356f",
        "title": "Deep Reinforcement Learning: A Brief Survey"
      },
      {
        "paperId": "f8f18db3b8762135c4170f19730c39435c48ac97",
        "title": "Comparative Study of Real-Time HEV Energy Management Strategies"
      },
      {
        "paperId": "1fdf0319b4a1db62c611980351e5f4c2f08958cd",
        "title": "GEP-PG: Decoupling Exploration and Exploitation in Deep Reinforcement Learning Algorithms"
      },
      {
        "paperId": "142497432fe179ddb6ffe600c64a837ec6179550",
        "title": "Parameter Space Noise for Exploration"
      },
      {
        "paperId": "c68727c9d87868669ceafd5198550e610b8ee992",
        "title": "Correctional DP-Based Energy Management Strategy of Plug-In Hybrid Electric Bus for City-Bus Route"
      },
      {
        "paperId": "9e4d29c433ef1e2ece92feab71ad1453382d35f5",
        "title": "Application Study on the Dynamic Programming Algorithm for Energy Management of Plug-in Hybrid Electric Vehicles"
      },
      {
        "paperId": "c3cebcc43263fb9a78a7a5d04dc432ccd100a414",
        "title": "Optimal energy management of a series hybrid vehicle with combined fuel economy and low-emission objectives"
      },
      {
        "paperId": "b723d657397083d968973c255b33379a33660bf8",
        "title": "Optimal energy management of an autonomous hybrid system by using the linear programming method"
      },
      {
        "paperId": "043a740b3cdf42f6a014a1678d0669765bd8f21a",
        "title": "MPC-Based Energy Management of a Power-Split Hybrid Electric Vehicle"
      },
      {
        "paperId": "fe8052b9caf878a34f5897ed686c426112b73504",
        "title": "Toyota Prius HEV neurocontrol and diagnostics"
      },
      {
        "paperId": "0f8e879754884cf17ee50e3f275cbd77d0115a77",
        "title": "Rule-based energy management strategies for hybrid vehicles"
      },
      {
        "paperId": "2084164c109b4acda7eceb896a163efd27879cfd",
        "title": "Scenario!"
      },
      {
        "paperId": "44ada8a37d4eba70e80e0676f9ad4bacc33cd9b3",
        "title": "Power management strategy for a parallel hybrid electric truck"
      },
      {
        "paperId": "3486c898002009398372e5dc82fbd7f3fdcfe15e",
        "title": "Personalized Driver Braking Behavior Modelling in the Car-following Scenario: An Importance Weight-based Transfer Learning Approach"
      },
      {
        "paperId": "9bd1cccc8e3540a6cab4d6db738677ffa6524169",
        "title": "Rule based energy management strategy for a series\u2013parallel plug-in hybrid electric bus optimized by dynamic programming"
      },
      {
        "paperId": "6aeb22e31b1d808754bfca8ba2bf597d92972d06",
        "title": "On the theory of brownian motion"
      }
    ],
    "cited_by": [
      {
        "paperId": "771e8eafc13b44913851f6c4ceaa15f6d2c83f55",
        "title": "AI-based energy management strategies for electric vehicles: Challenges and future directions"
      },
      {
        "paperId": "be8bb38a3abf8d971a7d0c56b0df4b9b47ad5a1e",
        "title": "Experimental and simulation insights into series-to-parallel transition: Exploring critical speed for optimizing energy utilization efficiency in PHEV"
      },
      {
        "paperId": "60ac1bfaf8f086ee50fbc3a6ebb8035f38ce6f93",
        "title": "RL-ADN: A High-Performance Deep Reinforcement Learning Environment for Optimal Energy Storage Systems Dispatch in Active Distribution Networks"
      },
      {
        "paperId": "c847fb65de498c216095e46ce9b6f77ec1f573bd",
        "title": "Reinforcement Learning-Based Energy Management for Hybrid Power Systems: State-of-the-Art Survey, Review, and Perspectives"
      },
      {
        "paperId": "c25a09b5e4b0c399b56c17aedbbe57cf57ac72cf",
        "title": "A transfer-based reinforcement learning collaborative energy management strategy for extended-range electric buses with cabin temperature comfort consideration"
      },
      {
        "paperId": "8eece0c51ff25a6a7ebdd3c7a67069e5250fa5e9",
        "title": "Intelligent Learning Algorithm and Intelligent Transportation-Based Energy Management Strategies for Hybrid Electric Vehicles: A Review"
      },
      {
        "paperId": "29719c920e3783a2e243f77d519aa7abc2d2d8e4",
        "title": "Intelligent Energy Management Strategies for Hybrid Electric Transportation"
      },
      {
        "paperId": "e4aff9551de6f0ff42b495a17c312e267f9175fe",
        "title": "Recent Progress in Learning Algorithms Applied in Energy Management of Hybrid Vehicles: A Comprehensive Review"
      },
      {
        "paperId": "2a7044eb3c89b9642508a4802677becd9bcd5bdb",
        "title": "Heterogeneous Multi-Source Deep Adaptive Knowledge-Aware Learning for E-Mobility"
      },
      {
        "paperId": "4db3f1b24c6b1b7e60612caad76da5b7e9ddad09",
        "title": "A novel cloud-edge collaboration based short-term load forecasting method for smart grid"
      },
      {
        "paperId": "83652d150b79837c9a9814cf8fb0a70a1506b2eb",
        "title": "Evolving Electric Mobility Energy Efficiency: In-Depth Analysis of Integrated Electronic Control Unit Development in Electric Vehicles"
      },
      {
        "paperId": "7270e3889d24b27ad174314037d87856e398f488",
        "title": "Multi-Objective Optimization Of Gear Ratios In Two-Speed Dual Clutch Transmissions For Electric Vehicles"
      },
      {
        "paperId": "1709bd698b6bc8002e2533faea31795dea84f46b",
        "title": "Uncertainty-Aware Energy Management Strategy for Hybrid Electric Vehicle Using Hybrid Deep Learning Method"
      },
      {
        "paperId": "b8e7ca5ef0cec3b2953cc309087f0763c232a4b0",
        "title": "Review of Arti\ufb01cial Intelligent Algorithms for Engine Performance, Control, and Diagnosis"
      }
    ],
    "score": 4.666666666666666
  },
  {
    "id": "b0c40766974df3eae8ff500379e66e5566cd16c9",
    "title": "An Efficient Asynchronous Method for Integrating Evolutionary and Gradient-based Policy Search",
    "authors": [
      "Kyunghyun Lee",
      "Byeong-uk Lee",
      "Ukcheol Shin",
      "In-So Kweon"
    ],
    "year": 2020,
    "citationCount": 23,
    "abstract": "Deep reinforcement learning (DRL) algorithms and evolution strategies (ES) have been applied to various tasks, showing excellent performances. These have the opposite properties, with DRL having good sample efficiency and poor stability, while ES being vice versa. Recently, there have been attempts to combine these algorithms, but these methods fully rely on synchronous update scheme, making it not ideal to maximize the benefits of the parallelism in ES. To solve this challenge, asynchronous update scheme was introduced, which is capable of good time-efficiency and diverse policy exploration. In this paper, we introduce an Asynchronous Evolution Strategy-Reinforcement Learning (AES-RL) that maximizes the parallel efficiency of ES and integrates it with policy gradient methods. Specifically, we propose 1) a novel framework to merge ES and DRL asynchronously and 2) various asynchronous update methods that can take all advantages of asynchronism, ES, and DRL, which are exploration and time efficiency, stability, and sample efficiency, respectively. The proposed framework and update methods are evaluated in continuous control benchmark work, showing superior performance as well as time efficiency compared to the previous methods.",
    "url": "https://www.semanticscholar.org/paper/b0c40766974df3eae8ff500379e66e5566cd16c9",
    "pdf_url": "https://arxiv.org/pdf/2012.05417.pdf",
    "venue": "Neural Information Processing Systems",
    "publicationDate": "2020-12-10",
    "externalIds": {
      "ArXiv": "2012.05417",
      "MAG": "3111468295",
      "DBLP": "journals/corr/abs-2012-05417",
      "CorpusId": 227275545
    },
    "references": [
      {
        "paperId": "5642e4d59b22ffc8d74eb585f4b52a76fbff1db7",
        "title": "Stabilizing Deep Reinforcement Learning with Conservative Updates"
      },
      {
        "paperId": "159ac7c47b99bf9d6ff95fe4051ce41a1966dfa4",
        "title": "Stabilizing Off-Policy Reinforcement Learning with Conservative Policy Gradients"
      },
      {
        "paperId": "50ba129bb69e4560d57c412e85d14bb43555abf4",
        "title": "Proximal Distilled Evolutionary Reinforcement Learning"
      },
      {
        "paperId": "98c08d28a2319ce79b62b108dfed3acc0dd80983",
        "title": "Collaborative Evolutionary Reinforcement Learning"
      },
      {
        "paperId": "40f54e88a391214afc7a939b7b40e2375a64b5d6",
        "title": "Reinforcement learning versus evolutionary computation: A survey on hybrid algorithms"
      },
      {
        "paperId": "d9e08e872c69e5acfed2a93ec6f6d623cfd0c680",
        "title": "CEM-RL: Combining evolutionary and gradient-based methods for policy search"
      },
      {
        "paperId": "d5805a80b63ed0a605e5469e321a7e3c42eaf324",
        "title": "Evolution-Guided Policy Gradient in Reinforcement Learning"
      },
      {
        "paperId": "dfb2b26f15466bf3ec34fbd72a22bb9d6ecd42f4",
        "title": "Policy Search in Continuous Action Domains: an Overview"
      },
      {
        "paperId": "4debb99c0c63bfaa97dd433bc2828e4dac81c48b",
        "title": "Addressing Function Approximation Error in Actor-Critic Methods"
      },
      {
        "paperId": "601a2d349fc26d7b82f905e924e2f91b0ac4b310",
        "title": "Distributed Prioritized Experience Replay"
      },
      {
        "paperId": "c9660db0c94edfb2b1f3ab4f08eb80acd83a1c07",
        "title": "Evolved Policy Gradients"
      },
      {
        "paperId": "80196cdfcd0c6ce2953bf65a7f019971e2026386",
        "title": "IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures"
      },
      {
        "paperId": "811df72e210e20de99719539505da54762a11c6d",
        "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor"
      },
      {
        "paperId": "819bcae49054e00cef3c0972d48b4e40a525f4d9",
        "title": "Deep Neuroevolution: Genetic Algorithms Are a Competitive Alternative for Training Deep Neural Networks for Reinforcement Learning"
      },
      {
        "paperId": "38fb1902c6a2ab4f767d4532b28a92473ea737aa",
        "title": "Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm"
      },
      {
        "paperId": "33690ff21ef1efb576410e656f2e60c89d0307d6",
        "title": "Deep Reinforcement Learning that Matters"
      },
      {
        "paperId": "4ee802a58d32aa049d549d06be440ac947b53987",
        "title": "Evolution Strategies as a Scalable Alternative to Reinforcement Learning"
      },
      {
        "paperId": "1464776f20e2bccb6182f183b5ff2e15b0ae5e56",
        "title": "Benchmarking Deep Reinforcement Learning for Continuous Control"
      },
      {
        "paperId": "7c6409ec154ba64f5eb63d8c6e9f419ce1472289",
        "title": "The CMA Evolution Strategy: A Tutorial"
      },
      {
        "paperId": "69e76e16740ed69f4dc55361a3d319ac2f1293dd",
        "title": "Asynchronous Methods for Deep Reinforcement Learning"
      },
      {
        "paperId": "024006d4c2a89f7acacc6e4438d156525b60a98f",
        "title": "Continuous control with deep reinforcement learning"
      },
      {
        "paperId": "780ed223f495ea517e77a17c233e54e5d2aee2f2",
        "title": "Asynchronous Parallel Evolutionary Algorithms: Leveraging Heterogeneous Fitness Evaluation Times for Scalability and Elitist Parsimony Pressure"
      },
      {
        "paperId": "6ddb2726b1bc6875cac0e0ea77272c95fda68fa6",
        "title": "Understanding Simple Asynchronous Evolutionary Algorithms"
      },
      {
        "paperId": "2319a491378867c7049b3da055c5df60e1671158",
        "title": "Playing Atari with Deep Reinforcement Learning"
      },
      {
        "paperId": "a446e09ccc0f05b4cf194eeca73d3737747bb889",
        "title": "A natural evolution strategy with asynchronous strategy updates"
      },
      {
        "paperId": "b354ee518bfc1ac0d8ac447eece9edb69e92eae1",
        "title": "MuJoCo: A physics engine for model-based control"
      },
      {
        "paperId": "b70acedbe995e9fc56886e18e77057dea05fa386",
        "title": "Parallel Genetic Algorithms"
      },
      {
        "paperId": "e1262319851d78ffa40c89d98d8afba10d0aec2d",
        "title": "Convergent Temporal-Difference Learning with Arbitrary Smooth Function Approximation"
      },
      {
        "paperId": "7546edc6039a21c3929516c9352902a269c6d408",
        "title": "A study of master-slave approaches to parallelize NSGA-II"
      },
      {
        "paperId": "5c5e69387020d7ca7d49487ca841958dc5e08ce6",
        "title": "The Cross-Entropy Method: A Unified Approach to Combinatorial Optimization, Monte-Carlo Simulation, and Machine Learning"
      },
      {
        "paperId": "28271f5e25cabca2905d4c9113f0fef3f16751c8",
        "title": "The Cross-Entropy Method"
      },
      {
        "paperId": "c57f21ff74ba8d225c24cf0dabbe31fcbbdcb478",
        "title": "A survey of parallel distributed genetic algorithms"
      },
      {
        "paperId": "e1991d18bd45442f39d5967970dc16a6742e5e00",
        "title": "Practical Handbook of Genetic Algorithms"
      },
      {
        "paperId": "bfe19cd6eb938388c5783713517c525210816815",
        "title": "Practical Handbook of Genetic Algorithms : Complex Coding Systems, Volume III"
      },
      {
        "paperId": "bba1f0b7265ce66f5c3a0197ce8df5eb3f69df85",
        "title": "Asynchronous Genetic Algorithms on Parallel Computers"
      },
      {
        "paperId": "946c0b839e5dcca3fcf5a39d07d9f6f7d67887e3",
        "title": "Note on a Method for Calculating Corrected Sums of Squares and Products"
      },
      {
        "paperId": "911f5ed9ed7a3a6c71b3e091f4e172120916cc08",
        "title": "Practical Handbook Of Genetic Algorithms Complex Coding Systems"
      },
      {
        "paperId": "a7c34eee3923a88f6805aac03c504968ec845ee5",
        "title": "Interactions between Learning and Evolution : The Outstanding Strategy Generated by the Baldwin E ect"
      },
      {
        "paperId": "a13fcba4fab4713d5acb2d970eddc6b148d2596d",
        "title": "A Survey of Parallel Genetic Algorithms"
      },
      {
        "paperId": "d04942a086f9cafbb1c6453b64ba188beeb03823",
        "title": "Evolutionsstrategie : Optimierung technischer Systeme nach Prinzipien der biologischen Evolution"
      },
      {
        "paperId": null,
        "title": "AES-RL algorithm mainly consists of three novel methods; an asynchronism, the mean update rule, and the variance update rule. In this section, we evaluate the effectiveness of each methods"
      }
    ],
    "cited_by": [
      {
        "paperId": "4390c7e5d0e546a1c3b2e790d216ac72794fc8e7",
        "title": "Stability of networked evolutionary games with asynchronous strategy updating rules and different memory lengths"
      },
      {
        "paperId": "1916afdced50f619ddec968e289ad57d3e068a09",
        "title": "LASCA: A Large-Scale Stable Customer Segmentation Approach to Credit Risk Assessment"
      },
      {
        "paperId": "6ffda3e657ed99c5d28039316f7fc8c48f91320c",
        "title": "Deep Reinforcement Learning for Swarm Navigation Using Different Evolution Strategies"
      },
      {
        "paperId": "1d430a53fc68b68043d50d746e3097903f04dc5a",
        "title": "Evolution Guided Generative Flow Networks"
      },
      {
        "paperId": "357c871f33b5fe11be8f4efc2008eb6c9207e6bb",
        "title": "A survey on Evolutionary Reinforcement Learning algorithms"
      },
      {
        "paperId": "b92f06e2ff7175910ee7c559135449d4c2dac617",
        "title": "Initialization Matters for Asynchronous Steady-State Evolutionary Algorithms"
      },
      {
        "paperId": "b9e883e168d5b25ee23fc361e3a5c51401c76123",
        "title": "Evolutionary Strategy Guided Reinforcement Learning via MultiBuffer Communication"
      },
      {
        "paperId": "47fb477dc3ba13bde1052cf8b8d1adbfbd43dc20",
        "title": "Supplementing Gradient-Based Reinforcement Learning with Simple Evolutionary Ideas"
      },
      {
        "paperId": "200726cba07dec06a56ff46aa38836e9730a23a2",
        "title": "Rethinking Population-assisted Off-policy Reinforcement Learning"
      },
      {
        "paperId": "4bb9f478cb464049100abafc77b6491b6e5b731a",
        "title": "Evolutionary Reinforcement Learning: A Survey"
      },
      {
        "paperId": "b6af7dcb27ebe6b7b5e755025cbdc0ab802f5628",
        "title": "Adversarial agent-learning for cybersecurity: a comparison of algorithms"
      },
      {
        "paperId": "a35e5c1c683ca246349f0bc804fdaf169ed752b2",
        "title": "A Simple Decentralized Cross-Entropy Method"
      },
      {
        "paperId": "9eca0adbd644755d07ee1f503f7324c29d0ec907",
        "title": "PowRL: A Reinforcement Learning Framework for Robust Management of Power Networks"
      },
      {
        "paperId": "a38519068e8da9ebd1347af8e6831b32741c13ae",
        "title": "Avoiding excess computation in asynchronous evolutionary algorithms"
      },
      {
        "paperId": "b0cf93751bcd080bc5ea8a7871d6d1e7ad58508d",
        "title": "Reinforcement learning for Energies of the future and carbon neutrality: a Challenge Design"
      },
      {
        "paperId": "3d070f1f9ac49cff26ac9f4a85e458aa518e8d34",
        "title": "DRL-ISP: Multi-Objective Camera ISP with Deep Reinforcement Learning"
      },
      {
        "paperId": "d0e783eeb54893c7a283ccbd044647f8f390b16e",
        "title": "Combining Evolution and Deep Reinforcement Learning for Policy Search: A Survey"
      },
      {
        "paperId": "660d660c56e120b4e36229e5531f0b38fbf3e0cd",
        "title": "Evolutionary Action Selection for Gradient-based Policy Learning"
      },
      {
        "paperId": "d252d55162949bd3c8fc1601e9b9cfcaf6ba6b53",
        "title": "Optimal Model for Fewer-Qubit CNOT Gates With Rydberg Atoms"
      },
      {
        "paperId": "f454afc371781946b9a5c43e08c82e17f2d6d83a",
        "title": "Avoiding Excess Computation in Asynchronous Evolutionary Algorithms"
      },
      {
        "paperId": "b82b38928225f8079191d62d1e77c8c07d0ab135",
        "title": "Learning to run a Power Network Challenge: a Retrospective Analysis"
      },
      {
        "paperId": "b390cd05c301b4d887c56b73e4960feaa70187cf",
        "title": "A-TD3: An Adaptive Asynchronous Twin Delayed Deep Deterministic for Continuous Action Spaces"
      },
      {
        "paperId": "31c7ad46bf12a9cb81ad4af463de786356020f32",
        "title": "Efficient and Stable Off-policy Training via Behavior-aware Evolutionary Learning"
      }
    ],
    "score": 4.6000000000000005
  },
  {
    "id": "4c4d41a977e6d8d82b6cc19ac28e50de3c814fd8",
    "title": "HyperDQN: A Randomized Exploration Method for Deep Reinforcement Learning",
    "authors": [
      "Ziniu Li",
      "Yingru Li",
      "Yushun Zhang",
      "Tong Zhang",
      "Zhimin Luo"
    ],
    "year": 2022,
    "citationCount": 13,
    "abstract": "",
    "url": "https://www.semanticscholar.org/paper/4c4d41a977e6d8d82b6cc19ac28e50de3c814fd8",
    "pdf_url": null,
    "venue": "International Conference on Learning Representations",
    "publicationDate": "",
    "externalIds": {
      "DBLP": "conf/iclr/LiLZZL22",
      "CorpusId": 251647755
    },
    "references": [],
    "cited_by": [
      {
        "paperId": "475cb759ee7f3fa56c0cdfeac035907da6b8a115",
        "title": "Q-learning with Posterior Sampling"
      },
      {
        "paperId": "e861a51688b6fc086854161c50eef9b04cf11e02",
        "title": "Langevin Soft Actor-Critic: Efficient Exploration through Uncertainty-Driven Critic Learning"
      },
      {
        "paperId": "08dfb362e96cdd99a3b4b3919fad30b4752a4a00",
        "title": "Diverse Randomized Value Functions: A Provably Pessimistic Approach for Offline Reinforcement Learning"
      },
      {
        "paperId": "dd089f7e75f8e7fc12b0e8051b328fa6779d9632",
        "title": "Prior-dependent analysis of posterior sampling reinforcement learning with function approximation"
      },
      {
        "paperId": "6a9676374bf545b0924bf256defcb1950721c648",
        "title": "Probability Tools for Sequential Random Projection"
      },
      {
        "paperId": "b207c4faa3b69a89e78027deb4947e7653694653",
        "title": "Simple, unified analysis of Johnson-Lindenstrauss with applications"
      },
      {
        "paperId": "dd005072d07d56ac83b198fc4266d39a84b7adb5",
        "title": "Optimistic Thompson Sampling for No-Regret Learning in Unknown Games"
      },
      {
        "paperId": "ec2aecde467988f67b6d190479cf4b7ae7806b8c",
        "title": "Posterior Sampling with Delayed Feedback for Reinforcement Learning with Linear Function Approximation"
      },
      {
        "paperId": "c90aa0f206c6fd41c490c142f63f7ba046cae6b7",
        "title": "Provable and Practical: Efficient Exploration in Reinforcement Learning via Langevin Monte Carlo"
      },
      {
        "paperId": "7f437f4af59ff994d97482ee1c12aaeb4b310e85",
        "title": "Implicit Posteriori Parameter Distribution Optimization in Reinforcement Learning"
      },
      {
        "paperId": "2f88f3141d959e62150aa3f3e63bf402831d3ce5",
        "title": "Deploying Offline Reinforcement Learning with Human Feedback"
      },
      {
        "paperId": "47049bc0c666acddba1bb1fa09e3e4b6a4cae7c7",
        "title": "HyperAgent: A Simple, Scalable, Efficient and Provable Reinforcement Learning Framework for Complex Environments"
      },
      {
        "paperId": "db59b22ae9556458d98107297736f640997c31be",
        "title": "Robust exploration with adversary via Langevin Monte Carlo"
      }
    ],
    "score": 4.333333333333333
  },
  {
    "id": "3d3b0704c61d47c7bafb70ae2670b2786b8e4d81",
    "title": "Efficient Exploration in Resource-Restricted Reinforcement Learning",
    "authors": [
      "Zhihai Wang",
      "Taoxing Pan",
      "Qi Zhou",
      "Jie Wang"
    ],
    "year": 2022,
    "citationCount": 13,
    "abstract": "In many real-world applications of reinforcement learning (RL), performing actions requires consuming certain types of resources that are non-replenishable in each episode. Typical applications include robotic control with limited energy and video games with consumable items. In tasks with non-replenishable resources, we observe that popular RL methods such as soft actor critic suffer from poor sample efficiency. The major reason is that, they tend to exhaust resources fast and thus the subsequent exploration is severely restricted due to the absence of resources. To address this challenge, we first formalize the aforementioned problem as a resource-restricted reinforcement learning, and then propose a novel resource-aware exploration bonus (RAEB) to make reasonable usage of resources. An appealing feature of RAEB is that, it can significantly reduce unnecessary resource-consuming trials while effectively encouraging the agent to explore unvisited states. Experiments demonstrate that the proposed RAEB significantly outperforms state-of-the-art exploration strategies in resource-restricted reinforcement learning environments, improving the sample efficiency by up to an order of magnitude.",
    "url": "https://www.semanticscholar.org/paper/3d3b0704c61d47c7bafb70ae2670b2786b8e4d81",
    "pdf_url": "https://arxiv.org/pdf/2212.06988.pdf",
    "venue": "AAAI Conference on Artificial Intelligence",
    "publicationDate": "2022-12-14",
    "externalIds": {
      "ArXiv": "2212.06988",
      "DBLP": "journals/corr/abs-2212-06988",
      "DOI": "10.48550/arXiv.2212.06988",
      "CorpusId": 254636119
    },
    "references": [
      {
        "paperId": "55fba8649cfd19f19b5b3d1267187c71bf6c86f7",
        "title": "A Reinforcement Learning-Based Mixed Job Scheduler Scheme for Grid or IaaS Cloud"
      },
      {
        "paperId": "361c00b22e29d0816ca896513d2c165e26399821",
        "title": "Grandmaster level in StarCraft II using multi-agent reinforcement learning"
      },
      {
        "paperId": "46f92838e7cf80cdd165cea939b44014ca2fb0ee",
        "title": "Resource Constrained Deep Reinforcement Learning"
      },
      {
        "paperId": "6ab8aca1f727e379632292e1ec4a24ea2739cf89",
        "title": "Model-Based Active Exploration"
      },
      {
        "paperId": "3feb79e8be23f0a9ef2beee964bfe4c36907dfc8",
        "title": "Pommerman: A Multi-Agent Playground"
      },
      {
        "paperId": "ca14dce53be20d3d23d4f0db844a8389ab619db3",
        "title": "Large-Scale Study of Curiosity-Driven Learning"
      },
      {
        "paperId": "03cc81e98942bdafd994af7a1d1e62a68ff8b682",
        "title": "Is Q-learning Provably Efficient?"
      },
      {
        "paperId": "811df72e210e20de99719539505da54762a11c6d",
        "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor"
      },
      {
        "paperId": "c27db32efa8137cbf654902f8f728f338e55cd1c",
        "title": "Mastering the game of Go without human knowledge"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "7a4193d0b042643a8bb9ec262ed7f9d509bdb12e",
        "title": "Constrained Policy Optimization"
      },
      {
        "paperId": "225ab689f41cef1dc18237ef5dab059a49950abf",
        "title": "Curiosity-Driven Exploration by Self-Supervised Prediction"
      },
      {
        "paperId": "d6757aedcb53142bc439ec64bfd0b056d99b1881",
        "title": "Surprise-Based Intrinsic Motivation for Deep Reinforcement Learning"
      },
      {
        "paperId": "0fcb2034e31a2bc2f12a2b1363d0d77baf445fdf",
        "title": "#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning"
      },
      {
        "paperId": "6e90fd78e8a3b98af3954aae5209703aa966603e",
        "title": "Unifying Count-Based Exploration and Intrinsic Motivation"
      },
      {
        "paperId": "2b10281297ee001a9f3f4ea1aa9bea6b638c27df",
        "title": "OpenAI Gym"
      },
      {
        "paperId": "a473f545318325ba23b7a6b477485d29777ba873",
        "title": "ViZDoom: A Doom-based AI research platform for visual reinforcement learning"
      },
      {
        "paperId": "317cd4522b1f4a6f889743578143bb8823623f8b",
        "title": "VIME: Variational Information Maximizing Exploration"
      },
      {
        "paperId": "ab68ddbdd8d0b61d9f9c8fa500a4c13d06158060",
        "title": "Variational Information Maximisation for Intrinsically Motivated Reinforcement Learning"
      },
      {
        "paperId": "449532187c94af3dd3aa55e16d2c50f7854d2199",
        "title": "Trust Region Policy Optimization"
      },
      {
        "paperId": "a636e1fb33c0cd2b8c590fc64cd6e269879f5bcc",
        "title": "Learning to soar: Resource-constrained exploration in reinforcement learning"
      },
      {
        "paperId": "fdf409152397b4914e285f97ca8a262603fdac3b",
        "title": "Models, feedback control, and open problems of 3D bipedal robotic walking"
      },
      {
        "paperId": "1f7f6b462ac16543e32934336b93732bc2588188",
        "title": "Reinforcement Learning in Robotics: Applications and Real-World Challenges"
      },
      {
        "paperId": "b354ee518bfc1ac0d8ac447eece9edb69e92eae1",
        "title": "MuJoCo: A physics engine for model-based control"
      },
      {
        "paperId": "54813d87942cbf00ecc6a8b0a592a9d78106eb64",
        "title": "Bipedal walking energy minimization by reinforcement learning with evolving policy parameterization"
      },
      {
        "paperId": "536566cd7fab16eb3dc1e3f3cc2c681293f53b6e",
        "title": "Online Resource Allocation Using Decompositional Reinforcement Learning"
      },
      {
        "paperId": "849aaec99f3684505876a6d0f3e9a73621a8daff",
        "title": "An empirical evaluation of interval estimation for Markov decision processes"
      },
      {
        "paperId": "b550e3e05701cbf6c76a8c71e91beb95f950b080",
        "title": "A Reinforcement Learning Approach to job-shop Scheduling"
      },
      {
        "paperId": "b9124861e4e874bbc477b4b726adf94f7d2ecdc4",
        "title": "Learning"
      },
      {
        "paperId": "8d15f17ea8f807efe8801d236b7218b6659ac1d9",
        "title": "NovelD: A Simple yet Effective Exploration Criterion"
      },
      {
        "paperId": "adabcfc03fa110d40d6ab2a613a36dda9c12dfff",
        "title": "Exploration via Empowerment Gain: Combining Novelty, Surprise and Learning Progress"
      },
      {
        "paperId": "fe4f9d8e18c4e70d3f75fc0d1d343815be6a5245",
        "title": "Natural Policy Gradient Primal-Dual Method for Constrained Markov Decision Processes"
      },
      {
        "paperId": "191ff26959d090d28e616625156c977e267a18bc",
        "title": "Constrained Reinforcement Learning via Policy Splitting"
      },
      {
        "paperId": "4d0f6a6ffcd6ab04732ff76420fd9f8a7bb649c3",
        "title": "Benchmarking Safe Exploration in Deep Reinforcement Learning"
      },
      {
        "paperId": null,
        "title": "2016) and Mujoco (Todorov, Erez"
      },
      {
        "paperId": "97efafdb4a3942ab3efba53ded7413199f79c054",
        "title": "Reinforcement Learning: An Introduction"
      }
    ],
    "cited_by": [
      {
        "paperId": "e92dccc8157803c69643839f90c113b40ac9449e",
        "title": "Rectified Robust Policy Optimization for Model-Uncertain Constrained Reinforcement Learning without Strong Duality"
      },
      {
        "paperId": "65f95f74b537053fd499b3c95385f4b8943b1c15",
        "title": "HyperTree Planning: Enhancing LLM Reasoning via Hierarchical Thinking"
      },
      {
        "paperId": "56dae4c0267c6e65225e2d14973ad38edd1cbba7",
        "title": "Apollo-MILP: An Alternating Prediction-Correction Neural Solving Framework for Mixed-Integer Linear Programming"
      },
      {
        "paperId": "9d3c88f03bcbf0d4e13495f01824d00df6ad86e0",
        "title": "Long-Term Feature Extraction via Frequency Prediction for Efficient Reinforcement Learning"
      },
      {
        "paperId": "2b0d7cf925cb749df5f6078d57b786f82b8994d9",
        "title": "Uncertainty-based Offline Variational Bayesian Reinforcement Learning for Robustness under Diverse Data Corruptions"
      },
      {
        "paperId": "fb350778b51672b1ac9f36a40c9efbdf746e35d5",
        "title": "Scalable and Effective Arithmetic Tree Generation for Adder and Multiplier Designs"
      },
      {
        "paperId": "1b5f065d2594d5b13d48a2afb3b97fff028c1fcb",
        "title": "Learning to Cut via Hierarchical Sequence/Set Model for Efficient Mixed-Integer Programming"
      },
      {
        "paperId": "e21d55d7ce0a3f7423c13e92e9cb71e6194c49df",
        "title": "Learning to Stop Cut Generation for Efficient Mixed-Integer Linear Programming"
      },
      {
        "paperId": "4839c9dfb29076ec78407fc8281e41cd3583a18b",
        "title": "State Sequences Prediction via Fourier Transform for Representation Learning"
      },
      {
        "paperId": "4a810f17843145195d73d5c63dc0f1477ba9cafc",
        "title": "ChiPFormer: Transferable Chip Placement via Offline Decision Transformer"
      },
      {
        "paperId": "506a7a38ec74180f4b0940853af3693e59fd8792",
        "title": "A Review for Deep Reinforcement Learning in Atari: Benchmarks, Challenges, and Solutions"
      },
      {
        "paperId": "ad272e77928dfce82ffb8739ee03afd9e0355668",
        "title": "A Hierarchical Adaptive Multi-Task Reinforcement Learning Framework for Multiplier Circuit Design"
      },
      {
        "paperId": "54f78ec43ae05b3d40001736bbf3b00fb3ce4478",
        "title": "Towards Next-Generation Logic Synthesis: A Scalable Neural Circuit Generation Framework"
      }
    ],
    "score": 4.333333333333333
  },
  {
    "id": "9e5fe2ba652774ba3b1127f626c192668a907132",
    "title": "Decoupled Exploration and Exploitation Policies for Sample-Efficient Reinforcement Learning",
    "authors": [
      "William F. Whitney",
      "Michael Bloesch",
      "Jost Tobias Springenberg",
      "A. Abdolmaleki",
      "Kyunghyun Cho",
      "Martin A. Riedmiller"
    ],
    "year": 2021,
    "citationCount": 17,
    "abstract": "Despite the close connection between exploration and sample efficiency, most state of the art reinforcement learning algorithms include no considerations for exploration beyond maximizing the entropy of the policy. In this work we address this seeming missed opportunity. We observe that the most common formulation of directed exploration in deep RL, known as bonus-based exploration (BBE), suffers from bias and slow coverage in the few-sample regime. This causes BBE to be actively detrimental to policy learning in many control tasks. We show that by decoupling the task policy from the exploration policy, directed exploration can be highly effective for sample-efficient continuous control. Our method, Decoupled Exploration and Exploitation Policies (DEEP), can be combined with any off-policy RL algorithm without modification. When used in conjunction with soft actor-critic, DEEP incurs no performance penalty in densely-rewarding environments. On sparse environments, DEEP gives a several-fold improvement in data efficiency due to better exploration.",
    "url": "https://www.semanticscholar.org/paper/9e5fe2ba652774ba3b1127f626c192668a907132",
    "pdf_url": "https://arxiv.org/pdf/2101.09458.pdf",
    "venue": "",
    "publicationDate": "2021-01-23",
    "externalIds": {
      "ArXiv": "2101.09458",
      "CorpusId": 235699591
    },
    "references": [
      {
        "paperId": "5de6d8b4436f6598f5bcba00bc07d864e962f1fb",
        "title": "Temporally-Extended {\\epsilon}-Greedy Exploration"
      },
      {
        "paperId": "2b2735cffb0d2321a456363880ff5671e80df4cb",
        "title": "On Bonus Based Exploration Methods In The Arcade Learning Environment"
      },
      {
        "paperId": "3f172dffae897113062fc0198f6b05c0195bf5fc",
        "title": "Kernel Operations on the GPU, with Autodiff, without Memory Overflows"
      },
      {
        "paperId": "62c1bc6a8bffe09a1e1138ffd37a64988dc27d48",
        "title": "Optimistic Exploration even with a Pessimistic Initialisation"
      },
      {
        "paperId": "086159600bede14e00f96043c733d4f3b45855aa",
        "title": "Never Give Up: Learning Directed Exploration Strategies"
      },
      {
        "paperId": "c6c5155c38d4e4ed3db17ca3ab9bcec3a2e7be4d",
        "title": "Continuous-Discrete Reinforcement Learning for Hybrid Control in Robotics"
      },
      {
        "paperId": "3c8a456509e6c0805354bd40a35e3f2dbf8069b1",
        "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library"
      },
      {
        "paperId": "5ab999687734ddf8c480315bde537e76ac358a80",
        "title": "Dynamics-aware Embeddings"
      },
      {
        "paperId": "12c0751b4f51ed833172a713b7e32390032ead93",
        "title": "Soft Actor-Critic Algorithms and Applications"
      },
      {
        "paperId": "4cb3fd057949624aa4f0bbe7a6dcc8777ff04758",
        "title": "Exploration by Random Network Distillation"
      },
      {
        "paperId": "9f67b3edc67a35c884bd532a5e73fa3a7f3660d8",
        "title": "Count-Based Exploration with the Successor Representation"
      },
      {
        "paperId": "03cc81e98942bdafd994af7a1d1e62a68ff8b682",
        "title": "Is Q-learning Provably Efficient?"
      },
      {
        "paperId": "eb37e7b76d26b75463df22b2a3aa32b6a765c672",
        "title": "QT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation"
      },
      {
        "paperId": "cab81775baae7ba2d056ebbc60437f2e03358ca3",
        "title": "Learning by Playing - Solving Sparse Reward Tasks from Scratch"
      },
      {
        "paperId": "4debb99c0c63bfaa97dd433bc2828e4dac81c48b",
        "title": "Addressing Function Approximation Error in Actor-Critic Methods"
      },
      {
        "paperId": "a8ef08940341381390d9a5672546354d0ce51328",
        "title": "Maximum a Posteriori Policy Optimisation"
      },
      {
        "paperId": "811df72e210e20de99719539505da54762a11c6d",
        "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor"
      },
      {
        "paperId": "a9a3ed69c94a3e1c08ef1f833d9199f57736238b",
        "title": "DeepMind Control Suite"
      },
      {
        "paperId": "3fab4d28be5ff73f42c8b307117aa8d680345ad9",
        "title": "Noisy Networks for Exploration"
      },
      {
        "paperId": "142497432fe179ddb6ffe600c64a837ec6179550",
        "title": "Parameter Space Noise for Exploration"
      },
      {
        "paperId": "225ab689f41cef1dc18237ef5dab059a49950abf",
        "title": "Curiosity-Driven Exploration by Self-Supervised Prediction"
      },
      {
        "paperId": "017b07fe36e8d43965b2125f6170a97c9d747fca",
        "title": "Data-efficient Deep Reinforcement Learning for Dexterous Manipulation"
      },
      {
        "paperId": "a441728f9fd6af1946368240162a72c2028c8cb1",
        "title": "Deep Exploration via Randomized Value Functions"
      },
      {
        "paperId": "12f67fb182bc934fc95ce97acff553d83e2ca72e",
        "title": "Count-Based Exploration with Neural Density Models"
      },
      {
        "paperId": "0fcb2034e31a2bc2f12a2b1363d0d77baf445fdf",
        "title": "#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning"
      },
      {
        "paperId": "6e90fd78e8a3b98af3954aae5209703aa966603e",
        "title": "Unifying Count-Based Exploration and Intrinsic Motivation"
      },
      {
        "paperId": "317cd4522b1f4a6f889743578143bb8823623f8b",
        "title": "VIME: Variational Information Maximizing Exploration"
      },
      {
        "paperId": "4b63e34276aa98d5345efa7fe09bb06d8a9d8f52",
        "title": "Deep Exploration via Bootstrapped DQN"
      },
      {
        "paperId": "c6170fa90d3b2efede5a2e1660cb23e1c824f2ca",
        "title": "Prioritized Experience Replay"
      },
      {
        "paperId": "3b9732bb07dc99bde5e1f9f75251c6ea5039373e",
        "title": "Deep Reinforcement Learning with Double Q-Learning"
      },
      {
        "paperId": "ae897d94109b00e0c143076f60d0f9013ccf5886",
        "title": "Pareto Smoothed Importance Sampling"
      },
      {
        "paperId": "2470fcf0f89082de874ac9133ccb3a8667dd89a8",
        "title": "Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models"
      },
      {
        "paperId": "340f48901f72278f6bf78a04ee5b01df208cc508",
        "title": "Human-level control through deep reinforcement learning"
      },
      {
        "paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8",
        "title": "Adam: A Method for Stochastic Optimization"
      },
      {
        "paperId": "b354ee518bfc1ac0d8ac447eece9edb69e92eae1",
        "title": "MuJoCo: A physics engine for model-based control"
      },
      {
        "paperId": "7ff621f1213eb12021b1eff0a44a78ff9c528ada",
        "title": "Normal reference bandwidths for the general order, multivariate kernel density derivative estimator"
      },
      {
        "paperId": "f82e4ff4f003581330338aaae71f60316e58dd26",
        "title": "The Arcade Learning Environment: An Evaluation Platform for General Agents"
      },
      {
        "paperId": "4282669947ca340de9f93a9a2a38007fe542195d",
        "title": "Near-Bayesian exploration in polynomial time"
      },
      {
        "paperId": "0cafe2903b097fc042782c359cb231ea34ef7ed3",
        "title": "Near-optimal Regret Bounds for Reinforcement Learning"
      },
      {
        "paperId": "237a1cf18ed83bb3ad852b34f443c6c1ff3336c1",
        "title": "An analysis of model-based Interval Estimation for Markov Decision Processes"
      },
      {
        "paperId": "187f3f984e6f375178f41827ab90c4e748773fa7",
        "title": "PAC model-free reinforcement learning"
      },
      {
        "paperId": "682fb61d887afd9c1194cb21656a5d73987b622b",
        "title": "Reinforcement learning on explicitly specified time scales"
      },
      {
        "paperId": "dc649486b881e672eea6546da48c46e1f98daf32",
        "title": "Near-Optimal Reinforcement Learning in Polynomial Time"
      },
      {
        "paperId": "ee2cd1d17f833d3c157a1016a778c7c22af555a2",
        "title": "ON THE LIKELIHOOD THAT ONE UNKNOWN PROBABILITY EXCEEDS ANOTHER IN VIEW OF THE EVIDENCE OF TWO SAMPLES"
      },
      {
        "paperId": null,
        "title": "Soft actor-critic (sac) implementation in pytorch"
      },
      {
        "paperId": null,
        "title": "Flax: A neural network library and ecosystem for JAX"
      },
      {
        "paperId": null,
        "title": "2018), an Apache-licensed standard set of benchmarks implemented using the commer"
      },
      {
        "paperId": null,
        "title": "JAX: composable transformations of Python+NumPy programs"
      },
      {
        "paperId": "74921bc8762812345b7010746faa22988b85252e",
        "title": "Prioritized sweeping: Reinforcement learning with less data and less time"
      },
      {
        "paperId": "c7dbf5ed7e9b63e104adb4e18bfbc98f5d6afdae",
        "title": "R-MAX - A General Polynomial Time Algorithm for Near-Optimal Reinforcement Learning"
      },
      {
        "paperId": null,
        "title": "For the grid-world environment used in Figures 2 and 3, we use a tabular visit count rather than pseudo-counts"
      }
    ],
    "cited_by": [
      {
        "paperId": "37c4bea3d5af535566b42d50a1c6153f10b478ab",
        "title": "Learning to Explore in Diverse Reward Settings via Temporal-Difference-Error Maximization"
      },
      {
        "paperId": "6b3f8a2c83bf21f7aada810fc0e40fff3bc95e95",
        "title": "Wasserstein Barycenter Soft Actor-Critic"
      },
      {
        "paperId": "4c976faf3b7ed140678a94f4aba56419f54e2e57",
        "title": "Disentangling Exploration of Large Language Models by Optimal Exploitation"
      },
      {
        "paperId": "1ca137888bba74eaba2f99a89405e6e9bce5229d",
        "title": "The Exploration-Exploitation Dilemma Revisited: An Entropy Perspective"
      },
      {
        "paperId": "99f9198f9d7ae72d09c9ddc7a8451f119dfeab84",
        "title": "Improving Intrinsic Exploration by Creating Stationary Objectives"
      },
      {
        "paperId": "64170ce0f75998a1175a2f081238c80539391d29",
        "title": "Improving Offline-to-Online Reinforcement Learning with Q Conditioned State Entropy Exploration"
      },
      {
        "paperId": "9eed36fb9b9bbb97579953d5e303dc0cf6c70a58",
        "title": "Safe Reinforcement Learning With Dead-Ends Avoidance and Recovery"
      },
      {
        "paperId": "b9e4ea7ff34a304cc0e7bfaa638eaa850391b676",
        "title": "Accelerating Reinforcement Learning with Value-Conditional State Entropy Exploration"
      },
      {
        "paperId": "027a402f21a6c76a82f315f41a4e0aba2b1d7a7a",
        "title": "Failure-aware Policy Learning for Self-assessable Robotics Tasks"
      },
      {
        "paperId": "ea14465601f45bf50a148563c73c4d6e1971dcb1",
        "title": "Redeeming Intrinsic Rewards via Constrained Optimization"
      },
      {
        "paperId": "ac3fd058adc8109bcb2666caa6fff8eba425e5f6",
        "title": "Safety Correction from Baseline: Towards the Risk-aware Policy in Robotics via Dual-agent Reinforcement Learning"
      },
      {
        "paperId": "57d9bf2fd87de57996829129697aa8e3b5abec67",
        "title": "Unentangled quantum reinforcement learning agents in the OpenAI Gym"
      },
      {
        "paperId": "09da56cd3bf72b632c43969be97874fa14a3765c",
        "title": "The Challenges of Exploration for Offline Reinforcement Learning"
      },
      {
        "paperId": "3b61bc41dff751edbead03aab5e4a1da1aafcc06",
        "title": "Multi-Stage Episodic Control for Strategic Exploration in Text Games"
      },
      {
        "paperId": "1f601a78724e9e6c26e0a46333b88b3aae134410",
        "title": "Decoupled Reinforcement Learning to Stabilise Intrinsically-Motivated Exploration"
      },
      {
        "paperId": "9bd59bab0eea6c56de7069da9fc4679a86671017",
        "title": "Trust Region Optimization of Optimistic Actor Critic"
      },
      {
        "paperId": "04c4f2d76a7f3c3445c52f485c335730ddc73570",
        "title": "Does Novelty-Based Exploration Maximize Novelty?"
      }
    ],
    "score": 4.25
  },
  {
    "id": "678cd30fac3fcf215d0e714936f6907ecc6d4361",
    "title": "Off-Policy Evolutionary Reinforcement Learning with Maximum Mutations",
    "authors": [
      "Karush Suri"
    ],
    "year": 2022,
    "citationCount": 12,
    "abstract": "",
    "url": "https://www.semanticscholar.org/paper/678cd30fac3fcf215d0e714936f6907ecc6d4361",
    "pdf_url": "https://doi.org/10.5555/3535850.3535988",
    "venue": "Adaptive Agents and Multi-Agent Systems",
    "publicationDate": "",
    "externalIds": {
      "DBLP": "conf/atal/Suri22",
      "DOI": "10.5555/3535850.3535988",
      "CorpusId": 246072576
    },
    "references": [],
    "cited_by": [
      {
        "paperId": "f2b1ef7f01c2f626429d90847589fd48d405c687",
        "title": "Evolutionary integrated thermal and energy management strategy based on reinforcement learning for distributed four-wheel drive electric vehicles"
      },
      {
        "paperId": "5fb99fd13a583f56ea37d283845fb49573bbb4d3",
        "title": "Scaling DRL for Decision Making: A Survey on Data, Network, and Training Budget Strategies"
      },
      {
        "paperId": "7f83c8a644f873b99bb925643019846455036fc6",
        "title": "An Efficient Task-Oriented Dialogue Policy: Evolutionary Reinforcement Learning Injected by Elite Individuals"
      },
      {
        "paperId": "8357670aac3c98a71b454ab5bca89558f265369d",
        "title": "Two-Stage Evolutionary Reinforcement Learning for Enhancing Exploration and Exploitation"
      },
      {
        "paperId": "25523c3e70daedcd0f1172844fcbafe30b8933f1",
        "title": "Bridging Evolutionary Algorithms and Reinforcement Learning: A Comprehensive Survey"
      },
      {
        "paperId": "9bd9db1af9c004c3b13b9e185b22791f21f3bb78",
        "title": "BiERL: A Meta Evolutionary Reinforcement Learning Framework via Bilevel Optimization"
      },
      {
        "paperId": "0b12e5e44ede3ccaf97ae02e37826279407fe12c",
        "title": "Evolution Strategies Enhanced Complex Multiagent Coordination"
      },
      {
        "paperId": "4bb9f478cb464049100abafc77b6491b6e5b731a",
        "title": "Evolutionary Reinforcement Learning: A Survey"
      },
      {
        "paperId": "5a983ff97dda83f5a5b5a3fede735073f27961c1",
        "title": "Actor-Director-Critic: A Novel Deep Reinforcement Learning Framework"
      },
      {
        "paperId": "660d660c56e120b4e36229e5531f0b38fbf3e0cd",
        "title": "Evolutionary Action Selection for Gradient-based Policy Learning"
      },
      {
        "paperId": "8fdf267a5f959c42ff81cc29784c9fadcd552c2c",
        "title": "Adaptive Evolutionary Reinforcement Learning Algorithm with Early Termination Strategy"
      },
      {
        "paperId": "a6d2a25db7753aea4a143576c418c27a8d61e68a",
        "title": "Adaptive Optimization in Evolutionary Reinforcement Learning Using Evolutionary Mutation Rates"
      }
    ],
    "score": 4.0
  },
  {
    "id": "103f1674121780097f896ffe525bab2c6ae0bcdc",
    "title": "Exploration Entropy for Reinforcement Learning",
    "authors": [
      "Bo Xin",
      "Haixu Yu",
      "You Qin",
      "Qing Tang",
      "Zhangqing Zhu"
    ],
    "year": 2020,
    "citationCount": 19,
    "abstract": "The training process analysis and termination condition of the training process of a Reinforcement Learning (RL) system have always been the key issues to train an RL agent. In this paper, a new approach based on State Entropy and Exploration Entropy is proposed to analyse the training process. The concept of State Entropy is used to denote the uncertainty for an RL agent to select the action at every state that the agent will traverse, while the Exploration Entropy denotes the action selection uncertainty of the whole system. Actually, the action selection uncertainty of a certain state or the whole system reflects the degree of exploration and the stage of the learning process for an agent. The Exploration Entropy is a new criterion to analyse and manage the training process of RL. The theoretical analysis and experiment results illustrate that the curve of Exploration Entropy contains more information than the existing analytical methods.",
    "url": "https://www.semanticscholar.org/paper/103f1674121780097f896ffe525bab2c6ae0bcdc",
    "pdf_url": "https://doi.org/10.1155/2020/2672537",
    "venue": "",
    "publicationDate": "2020-01-09",
    "externalIds": {
      "MAG": "3000134327",
      "DOI": "10.1155/2020/2672537",
      "CorpusId": 210721114
    },
    "references": [
      {
        "paperId": "6a36441ea5ce1e00855d06fa637cf8cb72c91ecd",
        "title": "Incremental Reinforcement Learning With Prioritized Sweeping for Dynamic Environments"
      },
      {
        "paperId": "3025be1b45ff8953b803e0ed84595f8708216a1d",
        "title": "Knowledge Transfer between Multi-granularity Models for Reinforcement Learning"
      },
      {
        "paperId": "675b2576eda5ec6d805e57066f51b72d36a60533",
        "title": "Entropy-based variational Bayes learning framework for data clustering"
      },
      {
        "paperId": "8e100bf6f403d129bb1ab98012da352ba546f3e4",
        "title": "Ensemble Network Architecture for Deep Reinforcement Learning"
      },
      {
        "paperId": "43c0f5ebd97225369e88418b4e957c3d592c0389",
        "title": "Self-Paced Prioritized Curriculum Learning With Coverage Penalty in Deep Reinforcement Learning"
      },
      {
        "paperId": "e910bb7339c2b993db18d24f81b3e21b2e8971cd",
        "title": "Sparse Markov Decision Processes With Causal Sparse Tsallis Entropy Regularization for Reinforcement Learning"
      },
      {
        "paperId": "15bc7b5fcf8c9d0bae51b8bbfa13438061b0e0e5",
        "title": "Complexity analysis of reinforcement learning and its application to robotics"
      },
      {
        "paperId": "23d3b9c47088c620ad1fe3f370c803c75d68dbb6",
        "title": "Entropy-based prioritized sampling in Deep Q-learning"
      },
      {
        "paperId": "1d7300adcb5335194bf555dbc91709d856560c61",
        "title": "Training an Actor-Critic Reinforcement Learning Controller for Arm Movement Using Human-Generated Rewards"
      },
      {
        "paperId": "4201ccebe828be1d40fd8cf6c236ba50fbdb4838",
        "title": "Reinforcement Learning Based Novel Adaptive Learning Framework for Smart Grid Prediction"
      },
      {
        "paperId": "3b9732bb07dc99bde5e1f9f75251c6ea5039373e",
        "title": "Deep Reinforcement Learning with Double Q-Learning"
      },
      {
        "paperId": "cda71f28da8cc23ad8770f2172a12c24e3cd4469",
        "title": "Multiagent Reinforcement Learning With Sparse Interactions by Negotiation and Knowledge Transfer"
      },
      {
        "paperId": "aab4453881ecf45c6377e3b7ef6134dd38bb19ab",
        "title": "An event-based probabilistic Q-learning method for navigation control of mobile robots"
      },
      {
        "paperId": "ec4de9daa9c00da1a0b156b373a760ef2b4fe639",
        "title": "Fidelity-Based Probabilistic Q-Learning for Control of Quantum Systems"
      },
      {
        "paperId": "c5791667dbcb51e1ee822df345810692abcc7f3b",
        "title": "Multiagent-Based Simulation of Temporal-Spatial Characteristics of Activity-Travel Patterns Using Interactive Reinforcement Learning"
      },
      {
        "paperId": "55ee0fed085770cabb1f1e2a2014c999e7b19a31",
        "title": "The Study of Reinforcement Learning for Traffic Self-Adaptive Control under Multiagent Markov Game Environment"
      },
      {
        "paperId": "402320d87aee52506f1eb00e784f04fc9b53b8d1",
        "title": "Control Design of Uncertain Quantum Systems With Fuzzy Estimators"
      },
      {
        "paperId": "ddbf9bc7a13e503f9afcaa4aea1a6495afb41dc8",
        "title": "Quantum Computation and Quantum Information"
      },
      {
        "paperId": "b133dc46c7536b66bc03b1112039c0a5889a050d",
        "title": "Computation and Estimation of Generalized Entropy Rates for Denumerable Markov Chains"
      },
      {
        "paperId": "5a444d2a04d46c9e78a85f8a64cdae233e0f5f0f",
        "title": "Utilizing Learning Automata and Entropy to Improve the Exploration Power of Rescue Agents"
      },
      {
        "paperId": "7064485842d3ba9d40fa924650c3aa982918e4b2",
        "title": "Complexity analysis of Quantum reinforcement learning"
      },
      {
        "paperId": "6a8484e5bb5ef40323e6b2cff94a3d6ee3034a61",
        "title": "Maximum Entropy-Based Reinforcement Learning Using a Confidence Measure in Speech Recognition for Telephone Speech"
      },
      {
        "paperId": "ee20133530650b29d57d7de4d918b1d82a5a0987",
        "title": "Quantum control theory and applications: A survey"
      },
      {
        "paperId": "a72d9bc5003aa7abb5571524426103a2b5594328",
        "title": "Strategy Entropy as a Measure of Strategy Convergence in Reinforcement Learning"
      },
      {
        "paperId": "86b2a1b37ae0d4591dc9902138e9db8ea303fabf",
        "title": "Quantum Reinforcement Learning"
      },
      {
        "paperId": "bf0d02ced19717e05dafeda5399629bb9c74a614",
        "title": "Estimation of the Entropy Rate of a Countable Markov Chain"
      },
      {
        "paperId": "c522469fcd31154f784217081222b038cb6c684c",
        "title": "Quantum control landscapes"
      },
      {
        "paperId": "0f7112bd7dc7c3e094e1cff641336a687fa143f4",
        "title": "Quantum Control of a Single Qubit"
      },
      {
        "paperId": "2e06c85f088b1221cc8bca4392e9b0505ed9a1fd",
        "title": "Reinforcement Strategy Using Quantum Amplitude Amplification for Robot Learning"
      },
      {
        "paperId": "da50285ba941c286982908a5c4cbbb7564a2816d",
        "title": "The Strategy Entropy of Reinforcement Learning for Mobile Robot Navigation in Complex Environments"
      },
      {
        "paperId": "d0c52318c8c407857d02d3155234b969ac787bb0",
        "title": "Entropy Rate and Maximum Entropy Methods for Countable Semi-Markov Chains"
      },
      {
        "paperId": "9bcc467b8b6380f57008d7090220147fc840984a",
        "title": "Optimal control of two-level quantum systems"
      },
      {
        "paperId": "03b7e51c52084ac1db5118342a00b5fbcfc587aa",
        "title": "Q-learning"
      },
      {
        "paperId": "a54194422c56399b2923b2ad706b8175c8c48258",
        "title": "A mathematical theory of communication"
      },
      {
        "paperId": null,
        "title": "Intermediate QuantumMechanics, CRC Press"
      },
      {
        "paperId": "3c81be86261a0e4be2c500dee8641e7fb0499dc6",
        "title": "Infinite time horizon maximum causal entropy inverse reinforcement learning"
      },
      {
        "paperId": null,
        "title": "e apprentice modeling through reinforcement with a temporal analysis using the q-learning algorithm"
      },
      {
        "paperId": null,
        "title": "Entropy\u2014a measure of uncertainty of random variable"
      }
    ],
    "cited_by": [
      {
        "paperId": "d4cc1918e071be4148579a4984ee2a5ea682e31e",
        "title": "Learn the Ropes, Then Trust the Wins: Self-imitation with Progressive Exploration for Agentic Reinforcement Learning"
      },
      {
        "paperId": "bf765ba61bab9bc84ce9a1ab17ed1e3f099c7eca",
        "title": "Mechanical Characterization of 3D Printed Fiber-Reinforced Composite Structures for Reinforcement Learning-Aided Design"
      },
      {
        "paperId": "b5f04855594a6777dac7cfcc7edb47b683118b95",
        "title": "An enhanced deep reinforcement learning approach for efficient, effective, and equitable disaster relief distribution"
      },
      {
        "paperId": "13e9950341a26816c01bc0c6a051d698641c7c82",
        "title": "Quantum-Inspired Reinforcement Learning for Quantum Control"
      },
      {
        "paperId": "a1daa87f8b4233876852c2343b3be6c2bca04dfe",
        "title": "Improved exploration-exploitation trade-off through adaptive prioritized experience replay"
      },
      {
        "paperId": "19168378df48c3a39200dbfb6ccbde878138852c",
        "title": "Traffic navigation via reinforcement learning with episodic-guided prioritized experience replay"
      },
      {
        "paperId": "6ba6da5c9c2929f0912728b27dea0853f0c402ce",
        "title": "Toward Debugging Deep Reinforcement Learning Programs with RLExplorer"
      },
      {
        "paperId": "6677a31e77b3eaa74b0b9a51ec970d2266c53dfe",
        "title": "Runtime Verification of Learning Properties for Reinforcement Learning Algorithms"
      },
      {
        "paperId": "41f9010f2533450c5a08c6fdaef3a8f550247168",
        "title": "An Empirical Study of On-Policy and Off-Policy Actor-Critic Algorithms in the Context of Exploration-Exploitation Dilemma"
      },
      {
        "paperId": "b67243cb71a0fe0356ee3326232c63ed99c41472",
        "title": "Machine Learning-Based Multi-Domain Actuation Orchestration in Support of End-to-End Service Quality-Assurance"
      },
      {
        "paperId": "90491099254e942297ff73f2cb2bfaef54424c8d",
        "title": "Censored Deep Reinforcement Patrolling with Information Criterion for Monitoring Large Water Resources using Autonomous Surface Vehicles"
      },
      {
        "paperId": "c99045c5cde87d6e1a02231627c346a242602a15",
        "title": "Research on Sports Dance Video Recommendation Method Based on Style"
      },
      {
        "paperId": "d4ff1af41e96578e32432bdbbf5fa01ea6a8fd4a",
        "title": "Prioritized Experience Replay based on Multi-armed Bandit"
      },
      {
        "paperId": "29b144d2267fd8ea7b144ee0a9c74dbf87a3cd05",
        "title": "Joint Perception and Control as Inference with an Object-based Implementation."
      },
      {
        "paperId": "6be61525ee8b21c3bef6564df17b435fc4f84282",
        "title": "Action and Perception as Divergence Minimization"
      },
      {
        "paperId": "5fcb853ed40bd53b322aa48a3a59b2e38c3c4e9d",
        "title": "A Hierarchical Framework for Multi-Lane Autonomous Driving Based on Reinforcement Learning"
      },
      {
        "paperId": "57e9f7d2d8ab87bf18cd200bddd8c4dc2591cff7",
        "title": "Proactive Edge Caching in Vehicular Networks: An Online Bandit Learning Approach"
      },
      {
        "paperId": "d4e6b19cbec7f6a9ec60ca22295ca33426a248e1",
        "title": "Using time-correlated noise to encourage exploration and improve autonomous agents performance in Reinforcement Learning"
      },
      {
        "paperId": "ec684b9cf2433680f6bd70779186f34bcd5b4f06",
        "title": "A CTION AND P ERCEPTION AS D IVERGENCE M INIMIZATION"
      }
    ],
    "score": 3.8000000000000003
  },
  {
    "id": "de93c8aed64229571b03e40b36499d4f07ce875d",
    "title": "PBCS : Efficient Exploration and Exploitation Using a Synergy between Reinforcement Learning and Motion Planning",
    "authors": [
      "Guillaume Matheron",
      "Nicolas Perrin",
      "Olivier Sigaud"
    ],
    "year": 2020,
    "citationCount": 19,
    "abstract": "The exploration-exploitation trade-off is at the heart of reinforcement learning (RL). However, most continuous control benchmarks used in recent RL research only require local exploration. This led to the development of algorithms that have basic exploration capabilities, and behave poorly in benchmarks that require more versatile exploration. For instance, as demonstrated in our empirical study, state-of-the-art RL algorithms such as DDPG and TD3 are unable to steer a point mass in even small 2D mazes. In this paper, we propose a new algorithm called \"Plan, Backplay, Chain Skills\" (PBCS) that combines motion planning and reinforcement learning to solve hard exploration environments. In a first phase, a motion planning algorithm is used to find a single good trajectory, then an RL algorithm is trained using a curriculum derived from the trajectory, by combining a variant of the Backplay algorithm and skill chaining. We show that this method outperforms state-of-the-art RL algorithms in 2D maze environments of various sizes, and is able to improve on the trajectory obtained by the motion planning phase.",
    "url": "https://www.semanticscholar.org/paper/de93c8aed64229571b03e40b36499d4f07ce875d",
    "pdf_url": "https://arxiv.org/pdf/2004.11667.pdf",
    "venue": "International Conference on Artificial Neural Networks",
    "publicationDate": "2020-04-24",
    "externalIds": {
      "MAG": "3020367954",
      "ArXiv": "2004.11667",
      "DBLP": "conf/icann/Matheron0S20",
      "DOI": "10.1007/978-3-030-61616-8_24",
      "CorpusId": 216144401
    },
    "references": [
      {
        "paperId": "007fd689a806de99f053a0f5ef90aedb44f9ec7e",
        "title": "Better Exploration with Optimistic Actor-Critic"
      },
      {
        "paperId": "84eb3a26622d359650bfd467055482f3a953db33",
        "title": "The problem with DDPG: understanding failures in deterministic environments with sparse rewards"
      },
      {
        "paperId": "2f9d13cbb9bc01b3261f8ff6afbace923eefa341",
        "title": "Reinforcement Learning with Probabilistically Complete Exploration"
      },
      {
        "paperId": "428e2bf31b1e6ef6b5f2e982d39575158bded349",
        "title": "Making Efficient Use of Demonstrations to Solve Hard Exploration Problems"
      },
      {
        "paperId": "b84e42ce68f2efa22ad96e1845a17e79ef2a1fe1",
        "title": "RL-RRT: Kinodynamic Motion Planning via Learning Reachability Estimators From RL Policies"
      },
      {
        "paperId": "45602b4aa5d3d67dac367c5a73f857ea7ab787bc",
        "title": "Towards Characterizing Divergence in Deep Q-Learning"
      },
      {
        "paperId": "c520bf47db3360ae3a52219771390a354ed8a91f",
        "title": "Go-Explore: a New Approach for Hard-Exploration Problems"
      },
      {
        "paperId": "e2c99bd61188ee58bd76126678cb4f240a9ee1bb",
        "title": "CLIC: Curriculum Learning and Imitation for Object Control in Nonrewarding Environments"
      },
      {
        "paperId": "5d4ebbaa1ef8feeac885e2869f45c0276c18834f",
        "title": "Learning Montezuma's Revenge from a Single Demonstration"
      },
      {
        "paperId": "5285cb8faada5de8a92a47622950f6cfd476ac1d",
        "title": "Off-Policy Deep Reinforcement Learning without Exploration"
      },
      {
        "paperId": "6bc692616db7b1a7ef2ea7c270c893adfb57ed0e",
        "title": "Deep Reinforcement Learning and the Deadly Triad"
      },
      {
        "paperId": "4cb3fd057949624aa4f0bbe7a6dcc8777ff04758",
        "title": "Exploration by Random Network Distillation"
      },
      {
        "paperId": "9b4d48eca82e8a3907b0e941a7ebd6976c95d6ae",
        "title": "Backplay: \"Man muss immer umkehren\""
      },
      {
        "paperId": "02b0669f522440e25cdde17663e9fcd1d883c763",
        "title": "Temporal Difference Learning with Neural Networks - Study of the Leakage Propagation Problem"
      },
      {
        "paperId": "29aab768e642588352134a03c0368e1bdc1f1e8d",
        "title": "Recall Traces: Backtracking Models for Efficient Reinforcement Learning"
      },
      {
        "paperId": "cab81775baae7ba2d056ebbc60437f2e03358ca3",
        "title": "Learning by Playing - Solving Sparse Reward Tasks from Scratch"
      },
      {
        "paperId": "4debb99c0c63bfaa97dd433bc2828e4dac81c48b",
        "title": "Addressing Function Approximation Error in Actor-Critic Methods"
      },
      {
        "paperId": "5b01eaef54a653ba03ddd5a978690380fbc19bfc",
        "title": "Diversity is All You Need: Learning Skills without a Reward Function"
      },
      {
        "paperId": "a9a3ed69c94a3e1c08ef1f833d9199f57736238b",
        "title": "DeepMind Control Suite"
      },
      {
        "paperId": "551c60bd9178a199c20723122cd26ddd9c0c93b6",
        "title": "PRM-RL: Long-range Robotic Navigation Tasks by Combining Reinforcement Learning and Sampling-Based Planning"
      },
      {
        "paperId": "c28ec2a40a2c77e20d64cf1c85dc931106df8e83",
        "title": "Overcoming Exploration in Reinforcement Learning with Demonstrations"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "9862caed8ee93321c78b0196e0b7eef516b545ba",
        "title": "Reverse Curriculum Generation for Reinforcement Learning"
      },
      {
        "paperId": "1fdf0319b4a1db62c611980351e5f4c2f08958cd",
        "title": "GEP-PG: Decoupling Exploration and Exploitation in Deep Reinforcement Learning Algorithms"
      },
      {
        "paperId": "225ab689f41cef1dc18237ef5dab059a49950abf",
        "title": "Curiosity-Driven Exploration by Self-Supervised Prediction"
      },
      {
        "paperId": "1252ce88d42db2810de848e10f0a2d85f9bfdf7b",
        "title": "Quality and Diversity Optimization: A Unifying Modular Framework"
      },
      {
        "paperId": "0fcb2034e31a2bc2f12a2b1363d0d77baf445fdf",
        "title": "#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning"
      },
      {
        "paperId": "6fab0b3b321988cedd0a017c1dad997f6d4da930",
        "title": "Playing Atari Games with Deep Reinforcement Learning and Human Checkpoint Replay"
      },
      {
        "paperId": "eaa518ad767dae7875f7f8fbc9eca6e05f9d11d0",
        "title": "Quality Diversity: A New Frontier for Evolutionary Computation"
      },
      {
        "paperId": "3ebc15e4e43dafea680fce41d5220a485da357c0",
        "title": "Behavioral Diversity Generation in Autonomous Exploration through Reuse of Past Experience"
      },
      {
        "paperId": "4b63e34276aa98d5345efa7fe09bb06d8a9d8f52",
        "title": "Deep Exploration via Bootstrapped DQN"
      },
      {
        "paperId": "c6170fa90d3b2efede5a2e1660cb23e1c824f2ca",
        "title": "Prioritized Experience Replay"
      },
      {
        "paperId": "024006d4c2a89f7acacc6e4438d156525b60a98f",
        "title": "Continuous control with deep reinforcement learning"
      },
      {
        "paperId": "6e487937856c3e70d4c5062548ce4533754a27c6",
        "title": "Confronting the Challenge of Quality Diversity"
      },
      {
        "paperId": "2470fcf0f89082de874ac9133ccb3a8667dd89a8",
        "title": "Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models"
      },
      {
        "paperId": "449532187c94af3dd3aa55e16d2c50f7854d2199",
        "title": "Trust Region Policy Optimization"
      },
      {
        "paperId": "ea8a816f360e17048f00db780d63b9095b01f660",
        "title": "Incentivizing exploration"
      },
      {
        "paperId": "2319a491378867c7049b3da055c5df60e1671158",
        "title": "Playing Atari with Deep Reinforcement Learning"
      },
      {
        "paperId": "2fc26ef1f69677bad6c64f8bb5587f18a1c2e374",
        "title": "Constructing Skill Trees for Reinforcement Learning Agents from Demonstration Trajectories"
      },
      {
        "paperId": "bdc5a10aa5805808cfca58ac527ddc23e737bee8",
        "title": "Skill Discovery in Continuous Reinforcement Learning Domains using Skill Chaining"
      },
      {
        "paperId": "bff60ab1adbb76de6f9d75be2e6d07777eb5cdb8",
        "title": "Survivability: Measuring and ensuring path diversity"
      },
      {
        "paperId": "af230c30b1d0f234a044eab5dda7a3f70209e958",
        "title": "Path diversity is only part of the problem"
      },
      {
        "paperId": "94066dc12fe31e96af7557838159bde598cb4f10",
        "title": "Policy Invariance Under Reward Transformations: Theory and Application to Reward Shaping"
      },
      {
        "paperId": "b14f4269f09ef77ad310581ea82d502402d90fb9",
        "title": "Probable Inference, the Law of Succession, and Statistical Inference"
      },
      {
        "paperId": "d967d9550f831a8b3f5cb00f8835a4c866da60ad",
        "title": "Rapidly-exploring random trees : a new tool for path planning"
      },
      {
        "paperId": "97efafdb4a3942ab3efba53ded7413199f79c054",
        "title": "Reinforcement Learning: An Introduction"
      }
    ],
    "cited_by": [
      {
        "paperId": "df26c47a1e27848300b1b702fde53957cca63e3d",
        "title": "Skill Learning Using a Single Demonstration for Hierarchical Robotic Navigation"
      },
      {
        "paperId": "44aa650dc87e2a84275a368d458f46c433d451bd",
        "title": "Curriculum-guided skill learning for long-horizon robot manipulation tasks"
      },
      {
        "paperId": "11d2f54ed92da028b5ba00c0ab213a4c852b6853",
        "title": "The Trends of Potential User Research from 2014-2023 Based on Bibliometric and Bertopic"
      },
      {
        "paperId": "9fdbf9ec23ab241205122a7a5973c735e7a1ad1a",
        "title": "CalliRewrite: Recovering Handwriting Behaviors from Calligraphy Images without Supervision"
      },
      {
        "paperId": "5dce3f2cfe018ada4bebeaa15aac6fe1d01b18ba",
        "title": "Imagine, Initialize, and Explore: An Effective Exploration Method in Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "234e31a541d5d4d1f64497cdd98dfde5131a342a",
        "title": "BERTopic Modeling of Natural Language Processing Abstracts: Thematic Structure and Trajectory"
      },
      {
        "paperId": "6ea2aa7eb2e11b7c4512d8029d071d2e2a99bb80",
        "title": "Leveraging Sequentiality in Reinforcement Learning from a Single Demonstration"
      },
      {
        "paperId": "0ffb27014230383f576ffa6b9c6c3a3f30b5fc8a",
        "title": "Cell-Free Latent Go-Explore"
      },
      {
        "paperId": "614db29a7a63f09d96b1bfddff265196c8fa3030",
        "title": "Play with Emotion: Affect-Driven Reinforcement Learning"
      },
      {
        "paperId": "764c536412f4de1efbed1302b6a4ea881366fdea",
        "title": "Generative Personas That Behave and Experience Like Humans"
      },
      {
        "paperId": "cad11d1d2d57c2ac4c4b79de43c99d34166e0471",
        "title": "Divide & Conquer Imitation Learning"
      },
      {
        "paperId": "da1e404ae194dcbcce198f5335f49f090403bbd7",
        "title": "Exploration in Deep Reinforcement Learning: A Survey"
      },
      {
        "paperId": "1dcb8d9b305a75e5c5ae7e7d22a5e6a28db1faf4",
        "title": "Distilling Motion Planner Augmented Policies into Visual Control Policies for Robot Manipulation"
      },
      {
        "paperId": "5f5c02ccf0bb417e7b9b0a9850e30655d699e775",
        "title": "Go-Blend Behavior and Affect"
      },
      {
        "paperId": "d880efab1b8ea66298c353d5c5589dac23b21d30",
        "title": "Selection-Expansion: A Unifying Framework for Motion-Planning and Diversity Search Algorithms"
      },
      {
        "paperId": "a011901fd788fc5ad452dd3d88f9f0970ea547a8",
        "title": "QD-RL: Efficient Mixing of Quality and Diversity in Reinforcement Learning"
      },
      {
        "paperId": "ae6d32cb94421bad18b095949cad4ae84a126af7",
        "title": "Multiscale Computation and Dynamic Attention in Biological and Artificial Intelligence"
      },
      {
        "paperId": "ac4d12519fc5a7e267967b48ba237aa36cca2164",
        "title": "Energy-Based Exploration for Reinforcement Learning of Underactuated Mechanical Systems"
      },
      {
        "paperId": "4db734163d6cbdbd75bd40a40b73cb7cd2946e3e",
        "title": "S AMPLE EFFICIENT Q UALITY D IVERSITY FOR NEURAL CONTINUOUS CONTROL"
      }
    ],
    "score": 3.8000000000000003
  },
  {
    "id": "9d1445f1845a2880ff9c752845660e9c294aa7b5",
    "title": "Flow to Control: Offline Reinforcement Learning with Lossless Primitive Discovery",
    "authors": [
      "Yiqin Yang",
      "Haotian Hu",
      "Wenzhe Li",
      "Siyuan Li",
      "Jun Yang",
      "Qianchuan Zhao",
      "Chongjie Zhang"
    ],
    "year": 2022,
    "citationCount": 11,
    "abstract": "Offline reinforcement learning (RL) enables the agent to effectively learn from logged data, which significantly extends the applicability of RL algorithms in real-world scenarios where exploration can be expensive or unsafe. Previous works have shown that extracting primitive skills from the recurring and temporally extended structures in the logged data yields better learning. However, these methods suffer greatly when the primitives have limited representation ability to recover the original policy space, especially in offline settings. In this paper, we give a quantitative characterization of the performance of offline hierarchical learning and highlight the importance of learning lossless primitives. To this end, we propose to use a flow-based structure as the representation for low-level policies. This allows us to represent the behaviors in the dataset faithfully while keeping the expression ability to recover the whole policy space. We show that such lossless primitives can drastically improve the performance of hierarchical policies. The experimental results and extensive ablation studies on the standard D4RL benchmark show that our method has a good representation ability for policies and achieves superior performance in most tasks.",
    "url": "https://www.semanticscholar.org/paper/9d1445f1845a2880ff9c752845660e9c294aa7b5",
    "pdf_url": "https://arxiv.org/pdf/2212.01105.pdf",
    "venue": "AAAI Conference on Artificial Intelligence",
    "publicationDate": "2022-12-02",
    "externalIds": {
      "DBLP": "conf/aaai/YangHLL0ZZ23",
      "ArXiv": "2212.01105",
      "DOI": "10.48550/arXiv.2212.01105",
      "CorpusId": 254221024
    },
    "references": [
      {
        "paperId": "514a1e7af5884a8e1c6dae488fa618787b7a69f7",
        "title": "On the Role of Discount Factor in Offline Reinforcement Learning"
      },
      {
        "paperId": "23bac2542b145bf2fcd17d7fa0a02ae03d0a45f7",
        "title": "Skill-based Meta-Reinforcement Learning"
      },
      {
        "paperId": "b7ba291af983b5e31f3adfa9a0ceb7a7b3114c7f",
        "title": "Offline Reinforcement Learning with Value-based Episodic Memory"
      },
      {
        "paperId": "348a855fe01f3f4273bf0ecf851ca688686dbfcc",
        "title": "Offline Reinforcement Learning with Implicit Q-Learning"
      },
      {
        "paperId": "4ff8454c524163bbc5d25f6c8984b1c31ad057e4",
        "title": "Pessimistic Model-based Offline Reinforcement Learning under Partial Coverage"
      },
      {
        "paperId": "19c647c8a315613f4573dc1ab4c29c91cf3337cf",
        "title": "Conservative Offline Distributional Reinforcement Learning"
      },
      {
        "paperId": "a3b82f4fc10caf6243afbd77c9c990ce03ae36d1",
        "title": "Offline RL Without Off-Policy Evaluation"
      },
      {
        "paperId": "c879b25308026d6538e52b27bcf4fd3cb60855f3",
        "title": "A Minimalist Approach to Offline Reinforcement Learning"
      },
      {
        "paperId": "9b4cd7b9865a34c8dffb8bfb516a8a66717d0c58",
        "title": "Believe What You See: Implicit Constraint Approach for Offline Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "245682e8b3fa76f4a3e2991b5497577af95cbb3f",
        "title": "COMBO: Conservative Offline Model-Based Policy Optimization"
      },
      {
        "paperId": "9f8a70e9188a913317e97ba1874c8f763fd13c04",
        "title": "Is Pessimism Provably Efficient for Offline RL?"
      },
      {
        "paperId": "f5275f5eb6569ddb5ba9a959ede09875d56e3bac",
        "title": "Parrot: Data-Driven Behavioral Priors for Reinforcement Learning"
      },
      {
        "paperId": "f204041dd567025217adc8070ca292e89cc80488",
        "title": "COG: Connecting New Skills to Past Experience with Offline Reinforcement Learning"
      },
      {
        "paperId": "0a321a38ba98499f17a2423f84972de29a5b2e7f",
        "title": "OPAL: Offline Primitive Discovery for Accelerating Offline Reinforcement Learning"
      },
      {
        "paperId": "b68b8b980db62308864b2a7d33718182c5f8335b",
        "title": "Accelerating Reinforcement Learning with Learned Skill Priors"
      },
      {
        "paperId": "3cb8e96faba73efa027fa858e2a78cd1fc3c6e4d",
        "title": "Model-Based Offline Planning"
      },
      {
        "paperId": "fa7f88f77de02ae9389e514a1cd13083a624ec78",
        "title": "EMaQ: Expected-Max Q-Learning Operator for Simple Yet Effective Offline and Online RL"
      },
      {
        "paperId": "d242950c9d4903d078055b3f5bbbad1b5e626e74",
        "title": "Learning Robot Skills with Temporal Variational Inference"
      },
      {
        "paperId": "0272b14dd471fe7b81df703af1b71d7600b77215",
        "title": "Accelerating Online Reinforcement Learning with Offline Datasets"
      },
      {
        "paperId": "28db20a81eec74a50204686c3cf796c42a020d2e",
        "title": "Conservative Q-Learning for Offline Reinforcement Learning"
      },
      {
        "paperId": "309c2c5ee60e725244da09180f913cd8d4b8d4e9",
        "title": "MOReL : Model-Based Offline Reinforcement Learning"
      },
      {
        "paperId": "5e7bc93622416f14e6948a500278bfbe58cd3890",
        "title": "Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems"
      },
      {
        "paperId": "a326d9f2d2d351001fece788165dbcbb524da2e4",
        "title": "D4RL: Datasets for Deep Data-Driven Reinforcement Learning"
      },
      {
        "paperId": "292bbd1287a0674cd9e3e79224e768ca557dcf81",
        "title": "Mastering Complex Control in MOBA Games with Deep Reinforcement Learning"
      },
      {
        "paperId": "8c54e8575e7c17a4097838305915e6e7b00fd4af",
        "title": "Relay Policy Learning: Solving Long-Horizon Tasks via Imitation and Reinforcement Learning"
      },
      {
        "paperId": "ad14227e4f51276892ffc37aa43fd8750bb5eba8",
        "title": "Advantage-Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning"
      },
      {
        "paperId": "d423fa6cd0f4d088941d9fe4bebd834d0137c9b9",
        "title": "Provably Efficient Reinforcement Learning with Linear Function Approximation"
      },
      {
        "paperId": "ffb3886a253ff927bcc46b78e00409893865a68e",
        "title": "Dynamics-Aware Unsupervised Discovery of Skills"
      },
      {
        "paperId": "12cd30cab8281567f5a1e8e3145641336fbb819c",
        "title": "Sample-Optimal Parametric Q-Learning Using Linearly Additive Features"
      },
      {
        "paperId": "e4a89a978f747d0b548f5887b2380c5f618061f0",
        "title": "Near-Optimal Representation Learning for Hierarchical Reinforcement Learning"
      },
      {
        "paperId": "5b01eaef54a653ba03ddd5a978690380fbc19bfc",
        "title": "Diversity is All You Need: Learning Skills without a Reward Function"
      },
      {
        "paperId": "fe3e91e40a950c6b6601b8f0a641884774d949ae",
        "title": "Distributional Reinforcement Learning with Quantile Regression"
      },
      {
        "paperId": "75a760c6bd5ae15e0fc489a074bc42bc1fc4e697",
        "title": "Safe, Multi-Agent, Reinforcement Learning for Autonomous Driving"
      },
      {
        "paperId": "09879f7956dddc2a9328f5c1472feeb8402bcbcf",
        "title": "Density estimation using Real NVP"
      },
      {
        "paperId": "d37620e6f8fe678a43e12930743281cd8cca6a66",
        "title": "Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation"
      },
      {
        "paperId": "024006d4c2a89f7acacc6e4438d156525b60a98f",
        "title": "Continuous control with deep reinforcement learning"
      },
      {
        "paperId": "dc8301b67f98accbb331190dd7bd987952a692af",
        "title": "NICE: Non-linear Independent Components Estimation"
      },
      {
        "paperId": "0e7638dc16a5e5e9e46c91272bfb9c3dd242ef6d",
        "title": "Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning"
      },
      {
        "paperId": "9aa1d909544fd9ffe061b84a90eb344ac303e6d9",
        "title": "The MAXQ Method for Hierarchical Reinforcement Learning"
      },
      {
        "paperId": "861e093c9c6da05c474658f65852781e388d1493",
        "title": "Batch learning from logged bandit feedback through counterfactual risk minimization"
      }
    ],
    "cited_by": [
      {
        "paperId": "fb3b84e597ee90d3779737e5f74f579dc1185f7a",
        "title": "State slow feature softmax Q-value regularization for offline reinforcement learning"
      },
      {
        "paperId": "e7f8edf4f95c9c1edc1ed1c577603914390580ac",
        "title": "Decision Flow Policy Optimization"
      },
      {
        "paperId": "d10434f17006c19ee7727605ec0c0433d725f562",
        "title": "Fewer May Be Better: Enhancing Offline Reinforcement Learning with Reduced Dataset"
      },
      {
        "paperId": "4b656d0e4bb612a53cf704d830bff6292d8958bb",
        "title": "DIAR: Diffusion-model-guided Implicit Q-learning with Adaptive Revaluation"
      },
      {
        "paperId": "e825e6325dfb5b93066817e7cc6b226ba7d3b799",
        "title": "Bayesian Design Principles for Offline-to-Online Reinforcement Learning"
      },
      {
        "paperId": "c4972899ee166d076ab5ed8107bd1ea12de0ab9a",
        "title": "Disentangling Policy from Offline Task Representation Learning via Adversarial Data Augmentation"
      },
      {
        "paperId": "e3b365cc2ea3a6c82eaf9428a5e7a716aa54da72",
        "title": "Unsupervised Behavior Extraction via Random Intent Priors"
      },
      {
        "paperId": "02aaccf045178a0ad70906941f21b1eabde34227",
        "title": "Efficient Planning with Latent Diffusion"
      },
      {
        "paperId": "bcc5820c7a84f84347bbf1062dcf7330fe2b0870",
        "title": "Reasoning with Latent Diffusion in Offline Reinforcement Learning"
      },
      {
        "paperId": "c293b63151e9e02f7694938cabb7bbae442ae4d2",
        "title": "The Provable Benefits of Unsupervised Data Sharing for Offline Reinforcement Learning"
      },
      {
        "paperId": "bd2755fd96e20183f642628f818d7d161d3d0f0b",
        "title": "Hierarchical Diffusion for Offline Decision Making"
      }
    ],
    "score": 3.6666666666666665
  },
  {
    "id": "83f1343500c9f0df62da0d61736738d8d7a9bba0",
    "title": "Zero-Shot Policy Transfer with Disentangled Task Representation of Meta-Reinforcement Learning",
    "authors": [
      "Zheng Wu",
      "Yichen Xie",
      "Wenzhao Lian",
      "Changhao Wang",
      "Yanjiang Guo",
      "Jianyu Chen",
      "S. Schaal",
      "M. Tomizuka"
    ],
    "year": 2022,
    "citationCount": 11,
    "abstract": "Humans are capable of abstracting various tasks as different combinations of multiple attributes. This perspective of compositionality is vital for human rapid learning and adaption since previous experiences from related tasks can be combined to generalize across novel compositional settings. In this work, we aim to achieve zero-shot policy generalization of Reinforcement Learning (RL) agents by leveraging the task compositionality. Our proposed method is a meta-RL algorithm with disentangled task representation, explicitly encoding different aspects of the tasks. Policy generalization is then performed by inferring unseen compositional task representations via the obtained disentanglement without extra exploration. The evaluation is conducted on three simulated tasks and a challenging real-world robotic insertion task. Experimental results demonstrate that our proposed method achieves policy generalization to unseen compositional tasks in a zero-shot manner.",
    "url": "https://www.semanticscholar.org/paper/83f1343500c9f0df62da0d61736738d8d7a9bba0",
    "pdf_url": "https://arxiv.org/pdf/2210.00350.pdf",
    "venue": "IEEE International Conference on Robotics and Automation",
    "publicationDate": "2022-10-01",
    "externalIds": {
      "ArXiv": "2210.00350",
      "DBLP": "journals/corr/abs-2210-00350",
      "DOI": "10.1109/ICRA48891.2023.10160764",
      "CorpusId": 252683807
    },
    "references": [
      {
        "paperId": "ca47b92d53f4554495e4452c1057e2a6674c3864",
        "title": "Policy Architectures for Compositional Generalization in Control"
      },
      {
        "paperId": "acf24ff124d9359d0404ed77967d292fc2e0a342",
        "title": "MELD: Meta-Reinforcement Learning from Images via Latent State Models"
      },
      {
        "paperId": "cf34efc663284da131e747407ac3f389f898e471",
        "title": "robosuite: A Modular Simulation Framework and Benchmark for Robot Learning"
      },
      {
        "paperId": "5a1b92aa50797a7c1e99b8840ff01aad66038596",
        "title": "Sim-to-Real Transfer in Deep Reinforcement Learning for Robotics: a Survey"
      },
      {
        "paperId": "38f93092ece8eee9771e61c1edaf11b1293cae1b",
        "title": "Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning"
      },
      {
        "paperId": "5f5f3c5a395f66691bc47e7877e83168cf0454bf",
        "title": "Meta-Reinforcement Learning for Robotic Industrial Insertion Tasks"
      },
      {
        "paperId": "361e953f792a585496834ee14216b94d0ce9ae74",
        "title": "VariBAD: A Very Good Method for Bayes-Adaptive Deep RL via Meta-Learning"
      },
      {
        "paperId": "c2c8482c713b94073f3d59895b373db4398ddfbb",
        "title": "Language as an Abstraction for Hierarchical Deep Reinforcement Learning"
      },
      {
        "paperId": "9a3c9a0ac460c7891d03d56146f2d566c7e0fb08",
        "title": "Meta reinforcement learning as task inference"
      },
      {
        "paperId": "4625628163a2ee0e6cd320cd7a14b4ccded2a631",
        "title": "Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables"
      },
      {
        "paperId": "fbf03bf621ffee283911e765d525a75fc0d11bae",
        "title": "CompILE: Compositional Imitation Learning and Execution"
      },
      {
        "paperId": "e8e7b0b23e70624fbc3bc5ab74ae01e39c6750a5",
        "title": "Making Sense of Vision and Touch: Self-Supervised Learning of Multimodal Representations for Contact-Rich Tasks"
      },
      {
        "paperId": "2f240424ca0761d0549252dacfbbeece14bb3cb6",
        "title": "Fast Context Adaptation via Meta-Learning"
      },
      {
        "paperId": "5df5561b55ab872f2b3df559ddd475299f660b42",
        "title": "Neural Task Graphs: Generalizing to Unseen Tasks From a Single Video Demonstration"
      },
      {
        "paperId": "eb37e7b76d26b75463df22b2a3aa32b6a765c672",
        "title": "QT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation"
      },
      {
        "paperId": "38e2f851b705faa0d0a698ed9885bd6834440073",
        "title": "Probabilistic Model-Agnostic Meta-Learning"
      },
      {
        "paperId": "dee85f0ed3571d7b591e23000848c584242186ef",
        "title": "Composable Deep Reinforcement Learning for Robotic Manipulation"
      },
      {
        "paperId": "68c108795deef06fa929d1f6e96b75dbf7ce8531",
        "title": "Meta-Reinforcement Learning of Structured Exploration Strategies"
      },
      {
        "paperId": "811df72e210e20de99719539505da54762a11c6d",
        "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor"
      },
      {
        "paperId": "0af8cdb71ce9e5bf37ad2a11f05af293cfe62172",
        "title": "Sim-to-Real Transfer of Robotic Control with Dynamics Randomization"
      },
      {
        "paperId": "ae096b868323f74a68414ae8855e20669540e2ba",
        "title": "Asymmetric Actor Critic for Image-Based Robot Learning"
      },
      {
        "paperId": "30834ae1497c35d362eea14857d93c28d2d12b57",
        "title": "Zero-Shot Task Generalization with Multi-Task Deep Reinforcement Learning"
      },
      {
        "paperId": "32ceb28e45a445df4d89df281bb0e3ab5aab1a2a",
        "title": "Domain randomization for transferring deep neural networks from simulation to the real world"
      },
      {
        "paperId": "c889d6f98e6d79b89c3a6adf8a921f88fa6ba518",
        "title": "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks"
      },
      {
        "paperId": "7493389667058116dbc7e808987f129325ee60d7",
        "title": "Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results"
      },
      {
        "paperId": "3a13f7c43b767b1fb72ef107ef62a4ddd48dd2a7",
        "title": "Modular Multitask Reinforcement Learning with Policy Sketches"
      },
      {
        "paperId": "954b01151ff13aef416d27adc60cd9a076753b1a",
        "title": "RL$^2$: Fast Reinforcement Learning via Slow Reinforcement Learning"
      },
      {
        "paperId": "8fab7d7dfd233fd5d19bc2641b4c1ca74fc7bc6a",
        "title": "Learning modular neural network policies for multi-task and multi-robot transfer"
      },
      {
        "paperId": "970588dae9fd209ece20907e78a36cbc1efd41ae",
        "title": "Feature Space Decomposition for effective robot adaptation"
      },
      {
        "paperId": "f762cc39a824de1360e8223222739aaa4cd4168c",
        "title": "Full regularization path for sparse principal component analysis"
      },
      {
        "paperId": "33576c0fc316c45c3672523114b20a5bb996e1f4",
        "title": "A unified approach for motion and force control of robot manipulators: The operational space formulation"
      },
      {
        "paperId": null,
        "title": "\u201cEf\ufb01cient off-policy meta-reinforcement"
      },
      {
        "paperId": "0b5479b77b56265cc77ebe65cab7b68a966b3580",
        "title": "Compositional Plan Vectors"
      },
      {
        "paperId": "97efafdb4a3942ab3efba53ded7413199f79c054",
        "title": "Reinforcement Learning: An Introduction"
      }
    ],
    "cited_by": [
      {
        "paperId": "7c67d707d9d0022a02ee1aeebfaaff10537c9eb4",
        "title": "DmC: Nearest Neighbor Guidance Diffusion Model for Offline Cross-domain Reinforcement Learning"
      },
      {
        "paperId": "a552bef347f7992870a96682ba7534a60468c018",
        "title": "Hybrid Cross-domain Robust Reinforcement Learning"
      },
      {
        "paperId": "814ca36dae45689f9959524df0cc8ed069bb5dda",
        "title": "A Survey on Transfer Reinforcement Learning"
      },
      {
        "paperId": "515111176d6fbb5e7c5e718364b32a8de2724c0d",
        "title": "Reducing Sim-to-Real Domain Gaps of Visual Sensors for Learning Environment-Constrained Visuomotor Policy"
      },
      {
        "paperId": "511c34b0bfa96301e0695e332d36d29af9f68b2d",
        "title": "Adaptive Meta Policy Learning With Virtual Model for Multi-Category Peg-in-Hole Assembly Skills"
      },
      {
        "paperId": "cf73b9b5d16f2b90e2805acce73c38e76011e0d0",
        "title": "TIMRL: A Novel Meta-Reinforcement Learning Framework for Non-Stationary and Multi-Task Environments"
      },
      {
        "paperId": "d1d46b3486818de77deef856ae0b6b3bed355998",
        "title": "Multimodal Variational DeepMDP: An Efficient Approach for Industrial Assembly in High-Mix, Low-Volume Production"
      },
      {
        "paperId": "eb5953af8aa1143994951674c8bf8c852e8b399d",
        "title": "Cross-Domain Policy Adaptation by Capturing Representation Mismatch"
      },
      {
        "paperId": "4ad32ae678576727fb819fab6b5d0b2181e7594f",
        "title": "Bridging the Sim-to-Real Gap with Dynamic Compliance Tuning for Industrial Insertion"
      },
      {
        "paperId": "4c20b3151f57e339765bad5982f819286e045ddc",
        "title": "Efficient Sim-to-real Transfer of Contact-Rich Manipulation Skills with Online Admittance Residual Learning"
      },
      {
        "paperId": "1433c709492dc79b0c58a5090f543d374990a000",
        "title": "Cross-Domain Offline Policy Adaptation with Optimal Transport and Dataset Constraint"
      }
    ],
    "score": 3.6666666666666665
  },
  {
    "id": "2e1ca97d8f7604e3b334ff4903bfa67267379317",
    "title": "The Surprising Effectiveness of Latent World Models for Continual Reinforcement Learning",
    "authors": [
      "Samuel Kessler",
      "Piotr Milo's",
      "Jack Parker-Holder",
      "Stephen J. Roberts"
    ],
    "year": 2022,
    "citationCount": 11,
    "abstract": "",
    "url": "https://www.semanticscholar.org/paper/2e1ca97d8f7604e3b334ff4903bfa67267379317",
    "pdf_url": "https://doi.org/10.48550/arXiv.2211.15944",
    "venue": "arXiv.org",
    "publicationDate": "",
    "externalIds": {
      "DBLP": "journals/corr/abs-2211-15944",
      "DOI": "10.48550/arXiv.2211.15944",
      "CorpusId": 254069878
    },
    "references": [],
    "cited_by": [
      {
        "paperId": "1c3b90db450290040aa2d22e1e1442ea9d18553f",
        "title": "A Survey on Autonomy-Induced Security Risks in Large Model-Based Agents"
      },
      {
        "paperId": "33bfced3ccc883ecac60da7888028a9a3c4d7f43",
        "title": "Policy Search, Retrieval, and Composition via Task Similarity in Collaborative Agentic Systems"
      },
      {
        "paperId": "77a58e445e34f1a6655b83233b444037bfc9e30a",
        "title": "Multi-world Model in Continual Reinforcement Learning"
      },
      {
        "paperId": "40c52f7a1df09143f7cc4d686e3d78e9e0f77f21",
        "title": "Fine-tuning Reinforcement Learning Models is Secretly a Forgetting Mitigation Problem"
      },
      {
        "paperId": "6dfd0612b20a5a8d0039d28b2c8988d71b011836",
        "title": "Augmenting Replay in World Models for Continual Reinforcement Learning"
      },
      {
        "paperId": "9b4528faff2a05bbe31069cdfd03d3679d939cf9",
        "title": "Learning to Coordinate with Anyone"
      },
      {
        "paperId": "c036ffd570fc96d9d6b70c7812a9edd9dd91a779",
        "title": "LuckyMera: a modular AI framework for building hybrid NetHack agents"
      },
      {
        "paperId": "00e730f1a001c200cd93883e1cbeb0337c11faee",
        "title": "Multiagent Continual Coordination via Progressive Task Contextualization"
      },
      {
        "paperId": "5802f94ba38dd06f2893fa027d5f18469a425a8c",
        "title": "Approximate Shielding of Atari Agents for Safe Exploration"
      },
      {
        "paperId": "d2553058dcdcc8fffe1d77922cc41e6d06c5fa64",
        "title": "One by One, Continual Coordinating with Humans via Hyper-Teammate Identification"
      },
      {
        "paperId": "04f1feccbea7d93037bd5e507d3f8e199b9acb09",
        "title": "O N T HE R OLE OF F ORGETTING IN F INE -T UNING R E - INFORCEMENT L EARNING M ODELS"
      }
    ],
    "score": 3.6666666666666665
  },
  {
    "id": "7551b7f7420087de59ad36e445cb3bdef9c382ee",
    "title": "Generalized State-Dependent Exploration for Deep Reinforcement Learning in Robotics",
    "authors": [
      "A. Raffin",
      "F. Stulp"
    ],
    "year": 2020,
    "citationCount": 18,
    "abstract": "",
    "url": "https://www.semanticscholar.org/paper/7551b7f7420087de59ad36e445cb3bdef9c382ee",
    "pdf_url": null,
    "venue": "arXiv.org",
    "publicationDate": "2020-05-12",
    "externalIds": {
      "DBLP": "journals/corr/abs-2005-05719",
      "MAG": "3024597329",
      "CorpusId": 218596121
    },
    "references": [],
    "cited_by": [
      {
        "paperId": "574ec4679c9efdaac629b8fd7f01ee07d78b741d",
        "title": "Chunking the Critic: A Transformer-based Soft Actor-Critic with N-Step Returns"
      },
      {
        "paperId": "6a4fb00554655b7400697f55ea727aeeadf300f2",
        "title": "End-to-End Reinforcement Learning for Torque Based Variable Height Hopping"
      },
      {
        "paperId": "4b6a968b3044b7d910dfaf979c12343068891a35",
        "title": "Reducing the Cost of Cycle-Time Tuning for Real-World Policy Optimization"
      },
      {
        "paperId": "8d03f604071ed94f3165380c745c2fae5c613d87",
        "title": "Reinforcement learning with experience replay and adaptation of action dispersion"
      },
      {
        "paperId": "068fe983f0e2c8f0430d05c6130ea270336a8ad4",
        "title": "Qualitative Differences Between Evolutionary Strategies and Reinforcement Learning Methods for Control of Autonomous Agents"
      },
      {
        "paperId": "7974c3d631ab7e2d8e05ce6ad69cadf06667f15c",
        "title": "MEPG: A Minimalist Ensemble Policy Gradient Framework for Deep Reinforcement Learning"
      },
      {
        "paperId": "30ee3f779c12ff1b0398994ed46882dc0ddcc24d",
        "title": "Fractional Transfer Learning for Deep Model-Based Reinforcement Learning"
      },
      {
        "paperId": "4a8bb056babc4193217e3531aa0e29aed2157b47",
        "title": "Online velocity fluctuation of off-road wheeled mobile robots: A reinforcement learning approach"
      },
      {
        "paperId": "a7c846f2f1cbe63e0c675671aa08cdf9c20ea856",
        "title": "Fault-Tolerant Six-DoF Pose Estimation for Tendon-Driven Continuum Mechanisms"
      },
      {
        "paperId": "027c32efb3a3c14cbc5f6c9b94cf1d8b05dcd7b5",
        "title": "A Novel Approach to Curiosity and Explainable Reinforcement Learning via Interpretable Sub-Goals"
      },
      {
        "paperId": "aa46c9045e74f550147a44781ef2992cab8a6de2",
        "title": "A fast learning approach for autonomous navigation using a deep reinforcement learning method"
      },
      {
        "paperId": "4a06ec48e4b413ab2563273981018df82faac32e",
        "title": "Pink Noise Is All You Need: Colored Noise Exploration in Deep Reinforcement Learning"
      },
      {
        "paperId": "9acf5f6f3ad8ba6de8c324f1d76e6bd0894fb2d7",
        "title": "Representation Gap in Deep Reinforcement Learning"
      },
      {
        "paperId": "97e5f79f2833165c8fd382228912d2cd58cb897f",
        "title": "Colored Noise Exploration in Reinforcement Learning"
      },
      {
        "paperId": "fa7b186382cfc93454830b5d947a4ac1a9453cff",
        "title": "Model-augmented Prioritized Experience Replay"
      },
      {
        "paperId": "e41edc5d14476ba0f57b875120bac94462ea3113",
        "title": "O UT - OF - DISTRIBUTION GENERALIZATION OF INTERNAL MODELS IS CORRELATED WITH REWARD"
      },
      {
        "paperId": "e3fc5b5627af62ee6981a02090cf6bae368202d7",
        "title": "Stable-Baselines3: Reliable Reinforcement Learning Implementations"
      },
      {
        "paperId": "83cb4a120a5a425942d914e5bd1a73ca4ff6c00d",
        "title": "Experience Replay Methods in Soft Actor-Critic"
      }
    ],
    "score": 3.6
  },
  {
    "id": "b6ffc15ab55281dcf08b6a19d6f8bfcb190765b8",
    "title": "Trust-Region Method with Deep Reinforcement Learning in Analog Design Space Exploration",
    "authors": [
      "Kai-En Yang",
      "Chia-Yu Tsai",
      "Hung-Hao Shen",
      "Chen-Feng Chiang",
      "Feng-Ming Tsai",
      "Chunguang Wang",
      "Yiju Ting",
      "Chia-Shun Yeh",
      "C. Lai"
    ],
    "year": 2020,
    "citationCount": 17,
    "abstract": "This paper introduces new perspectives on analog design space search. To minimize the time-to-market, this endeavor better cast as constraint satisfaction problem than global optimization defined in prior arts. We incorporate model based agents, contrasted with model-free learning, to implement a trust-region strategy. As such, simple feed-forward networks can be trained with supervised learning, where the convergence is relatively trivial. Experiment results demonstrate orders of magnitude improvement on search iterations. Additionally, the unprecedented consideration of PVT conditions are accommodated. On circuits with TSMC 5/6nm process, our method achieve performance surpassing human designers. Furthermore, this framework is in production in industrial settings.",
    "url": "https://www.semanticscholar.org/paper/b6ffc15ab55281dcf08b6a19d6f8bfcb190765b8",
    "pdf_url": "https://arxiv.org/pdf/2009.13772.pdf",
    "venue": "Design Automation Conference",
    "publicationDate": "2020-09-29",
    "externalIds": {
      "DBLP": "conf/dac/YangTSCTWTYL21",
      "ArXiv": "2009.13772",
      "DOI": "10.1109/DAC18074.2021.9586087",
      "CorpusId": 243897249
    },
    "references": [
      {
        "paperId": "027d05ab0c9e3ad91103eec2fe6ee2d2951a1afa",
        "title": "Towards Fast Adaptation of Neural Architectures with Meta Learning"
      },
      {
        "paperId": "ef6bee764ce1fed9f6c01076374135af01e26385",
        "title": "AutoCkt: Deep Reinforcement Learning of Analog Circuit Designs"
      },
      {
        "paperId": "1d5507b964a1026b7237d6870a9bab7078969dfb",
        "title": "Using Artificial Neural Networks for Analog Integrated Circuit Design Automation"
      },
      {
        "paperId": "553e43cf68198de2a49e1204488962bb712e9ed6",
        "title": "An Energy-Efficient Network-on-Chip Design using Reinforcement Learning"
      },
      {
        "paperId": "96d9c050239a71e897ad4e3ecc934e54bfd9e8d6",
        "title": "An Efficient Multi-fidelity Bayesian Optimization Approach for Analog Circuit Synthesis"
      },
      {
        "paperId": "bc6c40b21aca842cc5ce62cfa55106779ecf59d9",
        "title": "Late Breaking Results: Analog Circuit Generator based on Deep Neural Network enhanced Combinatorial optimization"
      },
      {
        "paperId": "1fd4694e7c2d9c872a427d50e81b5475056de6bc",
        "title": "Model-Based Reinforcement Learning for Atari"
      },
      {
        "paperId": "8bd5c9ae80ba48483acf5fb04af4badfc4dc81fa",
        "title": "Learning to Design Circuits"
      },
      {
        "paperId": "9e3f4807fda1d876836e2fa3fd49d460fbf5711a",
        "title": "Batch Bayesian Optimization via Multi-objective Acquisition Ensemble for Automated Analog Circuit Design"
      },
      {
        "paperId": "005865ef16e944a7d5a6d6b16093b54bbe0245d0",
        "title": "Parametric Circuit Optimization with Reinforcement Learning"
      },
      {
        "paperId": "5734740064e36db535dfd23b7263abcf4172d03b",
        "title": "Invited: Efficient Reinforcement Learning for Automating Human Decision-Making in SoC Design"
      },
      {
        "paperId": "d09bec5af4eef5038e48b26b6c14098f95997114",
        "title": "AI Safety Gridworlds"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "6cd5dfccd9f52538b19a415e00031d0ee4e5b181",
        "title": "Designing Neural Network Architectures using Reinforcement Learning"
      },
      {
        "paperId": "67d968c7450878190e45ac7886746de867bf673d",
        "title": "Neural Architecture Search with Reinforcement Learning"
      },
      {
        "paperId": "69e76e16740ed69f4dc55361a3d319ac2f1293dd",
        "title": "Asynchronous Methods for Deep Reinforcement Learning"
      },
      {
        "paperId": "846aedd869a00c09b40f1f1f35673cb22bc87490",
        "title": "Mastering the game of Go with deep neural networks and tree search"
      },
      {
        "paperId": "024006d4c2a89f7acacc6e4438d156525b60a98f",
        "title": "Continuous control with deep reinforcement learning"
      },
      {
        "paperId": "b6b8a1b80891c96c28cc6340267b58186157e536",
        "title": "End-to-End Training of Deep Visuomotor Policies"
      },
      {
        "paperId": "340f48901f72278f6bf78a04ee5b01df208cc508",
        "title": "Human-level control through deep reinforcement learning"
      },
      {
        "paperId": "449532187c94af3dd3aa55e16d2c50f7854d2199",
        "title": "Trust Region Policy Optimization"
      },
      {
        "paperId": null,
        "title": "Tts: Transferable transistor sizing with graph neural networks and reinforcement learning"
      },
      {
        "paperId": null,
        "title": "Stable baselines"
      },
      {
        "paperId": null,
        "title": "\u201cArti\ufb01cial intelligence: a modern approach,\u201d"
      }
    ],
    "cited_by": [
      {
        "paperId": "f2e303cd4adca4732d478630f92b205ff9d54932",
        "title": "TopoSizing: An LLM-aided Framework of Topology-based Understanding and Sizing for AMS Circuits"
      },
      {
        "paperId": "253edd4ec59bf6fc1babc63a49d8a6e6cc59b371",
        "title": "PPAAS: PVT and Pareto Aware Analog Sizing via Goal-conditioned Reinforcement Learning"
      },
      {
        "paperId": "98e380f7166e7990cb86ed81839b1446bc3df5c7",
        "title": "Combining Machine Learning and Optimization Techniques for the High-Level Design of \u03a3\u2206Ms"
      },
      {
        "paperId": "f8d7e645c69a6c19daceaf56a365e40ef07485d1",
        "title": "Accelerating Comprehensive Specification Optimization of Analog Circuits Using Transient Assertions and Graph Neural Networks"
      },
      {
        "paperId": "3bd031e6f2c48c73f3cecfa855b88e208b829cc7",
        "title": "GLOVA: Global and Local Variation-Aware Analog Circuit Design with Risk-Sensitive Reinforcement Learning"
      },
      {
        "paperId": "c4f2e1910f6d2e1c03b01808f4747ee859167c98",
        "title": "AISAD: A User-Friedly AI-Assisted MATLAB Tool for the High-Level Design of \u03a3\u0394 Modulators"
      },
      {
        "paperId": "1054bcc299b3730d3e27cae7ee26dc4dfb291f62",
        "title": "PVTSizing: A TuRBO-RL-Based Batch-Sampling Optimization Framework for PVT-Robust Analog Circuit Synthesis"
      },
      {
        "paperId": "4979ebb1915e7f2f40db1f5c46d897e7d34e4c90",
        "title": "Using Probabilistic Model Rollouts to Boost the Sample Efficiency of Reinforcement Learning for Automated Analog Circuit Sizing"
      },
      {
        "paperId": "e0801a58542b85c8ef84c31ceb3f5b55823ed735",
        "title": "A survey of Reinforcement Learning for Electronic Design Automation"
      },
      {
        "paperId": "60eaa3ab708e25643af4bed4bd757907ec3f466e",
        "title": "On the Use of Artificial Neural Networks for the Automated High-Level Design of \u03a3\u0394 Modulators"
      },
      {
        "paperId": "b3e52c27ba8c2aa66087670436851e2304b2ed19",
        "title": "Design Automation of Analog and Mixed-Signal Circuits Using Neural Networks\u2014A Tutorial Brief"
      },
      {
        "paperId": "ed8b7604684ce0cb2d897ebb08bf383e945b9064",
        "title": "Design and Optimization of Low-Dropout Voltage Regulator Using Relational Graph Neural Network and Reinforcement Learning in Open-Source SKY130 Process"
      },
      {
        "paperId": "2808294b0f2db1be3ea39041e259518aa007826c",
        "title": "DC-Model: A New Method for Assisting the Analog Circuit Optimization"
      },
      {
        "paperId": "98744320777087b942650a73dad0e8cca3b7fc55",
        "title": "Joint Optimization of Sizing and Layout for AMS Designs: Challenges and Opportunities"
      },
      {
        "paperId": "946073a0b4b745a570e894e8fecf2203867e5d0d",
        "title": "APOSTLE: Asynchronously Parallel Optimization for Sizing Analog Transistors using DNN Learning"
      },
      {
        "paperId": "2a297b042702b89c6a0a3864215231c84d0f0e5e",
        "title": "RobustAnalog: Fast Variation-Aware Analog Circuit Design Via Multi-task RL"
      },
      {
        "paperId": "0dc7acd45b460fcd30674b81f5b361944f869a6e",
        "title": "MIT Open Access Articles RobustAnalog: Fast Variation-Aware Analog Circuit Design Via Multi-task RL"
      }
    ],
    "score": 3.4000000000000004
  },
  {
    "id": "06318722ebbac1d9b59e2408ecd3994eadb0ec6b",
    "title": "Deep Reinforcement Learning Task Assignment Based on Domain Knowledge",
    "authors": [
      "Jiayi Liu",
      "Gang Wang",
      "Xiangke Guo",
      "Siyuan Wang",
      "Qiang Fu"
    ],
    "year": 2022,
    "citationCount": 10,
    "abstract": "Deep Reinforcement Learning (DRL) methods are inefficient in the initial strategy exploration process due to the huge state space and action space in large-scale complex scenarios. This is becoming one of the bottlenecks in their application to large-scale game adversarial scenarios. This paper proposes a Safe reinforcement learning combined with Imitation learning for Task Assignment (SITA) method for a representative red-blue game confrontation scenario. Aiming at the problem of difficult sampling of Imitation Learning (IL), this paper combines human knowledge with adversarial rules to build a knowledge rule base; We propose the Imitation Learning with the Decoupled Network (ILDN) pre-training method to solve the problem of excessive initial invalid exploration; In order to reduce invalid exploration and improve the stability in the later stages of training, we incorporate Safe Reinforcement Learning (Safe RL) method after pre-training. Finally, we verified in the digital battlefield that the SITA method has higher training efficiency and strong generalization ability in large-scale complex scenarios.",
    "url": "https://www.semanticscholar.org/paper/06318722ebbac1d9b59e2408ecd3994eadb0ec6b",
    "pdf_url": "https://doi.org/10.1109/ACCESS.2022.3217654",
    "venue": "IEEE Access",
    "publicationDate": "",
    "externalIds": {
      "DBLP": "journals/access/LiuWGWF22",
      "DOI": "10.1109/ACCESS.2022.3217654",
      "CorpusId": 253349357
    },
    "references": [
      {
        "paperId": "713dbc737c1873ebf8dac2b84cbb680cefa9e1ae",
        "title": "Task assignment in ground-to-air confrontation based on multiagent deep reinforcement learning"
      },
      {
        "paperId": "a2259e93de7edef0aba3a8014c1b0edcee459828",
        "title": "Cooperative Multi-Robot Task Allocation with Reinforcement Learning"
      },
      {
        "paperId": "f24b2638d8b38331bd85b94d62e93da1785cdeb5",
        "title": "Multi-UAV Optimal Mission Assignment and Path Planning for Disaster Rescue Using Adaptive Genetic Algorithm and Improved Artificial Bee Colony Method"
      },
      {
        "paperId": "b49270ff7fd124f57ae15d37f27aed69f06f8c48",
        "title": "A novel fuzzy and reverse auction\u2010based algorithm for task allocation with optimal path cost in multi\u2010robot systems"
      },
      {
        "paperId": "6e4d06905e1000202a500b4f4ef7d11382b9b0c6",
        "title": "Reinforced Ant Colony Optimization for Fault Tolerant Task Allocation in Cloud Environments"
      },
      {
        "paperId": "f9f340c8bd0712780148d0f431cd4914a515f4b1",
        "title": "Recent advances in leveraging human guidance for sequential decision-making tasks"
      },
      {
        "paperId": "7da0bcc9c799df7f882c37e1314e1eb9f3c43765",
        "title": "Deep reinforcement learning for inventory control: A roadmap"
      },
      {
        "paperId": "7088c8df2a0ca4bdf05846c7e9d696e65a52d37a",
        "title": "Deep Inverse Reinforcement Learning for Objective Function Identification in Bidding Models"
      },
      {
        "paperId": "71a8cb2387b7fc0baf0380a63e8d7c17c9ec7178",
        "title": "A deep reinforcement learning-based approach for autonomous driving in highway on-ramp merge"
      },
      {
        "paperId": "81a128029f97e2270d3d63e2b466c1f6082d5420",
        "title": "DAPath: Distance-aware knowledge graph reasoning based on deep reinforcement learning"
      },
      {
        "paperId": "83551ac1e6358182a2c0f2ec223fb3c6f736c8e1",
        "title": "Error Bounds of Imitating Policies and Environments for Reinforcement Learning"
      },
      {
        "paperId": "62b38624ce1029b602e8d3212ec7111503d527d3",
        "title": "Robust Behavioral Cloning for Autonomous Vehicles using End-to-End Imitation Learning"
      },
      {
        "paperId": "5e4c574fbdd7df63e333b3c11afd846a38786507",
        "title": "Constrained Cross-Entropy Method for Safe Reinforcement Learning"
      },
      {
        "paperId": "99bc5a19a5aa9f85e2c1ecdc9bf8add7b75dae45",
        "title": "Novel Policy Seeking with Constrained Optimization"
      },
      {
        "paperId": "a68ae60892d48dc452add96a2836d72e6bc53173",
        "title": "Alpha C2\u2013An Intelligent Air Defense Commander Independent of Human Decision-Making"
      },
      {
        "paperId": "e537143dd60294eb6b38dd7965860e5f7a52cc5f",
        "title": "Guided Constrained Policy Optimization for Dynamic Quadrupedal Robot Locomotion"
      },
      {
        "paperId": "4b77327c3645496a4fb11224164f5d2f34ef1c6e",
        "title": "Teaching a humanoid robot to walk faster through Safe Reinforcement Learning"
      },
      {
        "paperId": "d703af257599602db6b05283b9e6786420063c54",
        "title": "An ensemble method for inverse reinforcement learning"
      },
      {
        "paperId": "361c00b22e29d0816ca896513d2c165e26399821",
        "title": "Grandmaster level in StarCraft II using multi-agent reinforcement learning"
      },
      {
        "paperId": "dd4f6823e6f677244f901a429db970e082e06b8d",
        "title": "Object tracking using the particle filter optimised by the improved artificial fish swarm algorithm"
      },
      {
        "paperId": "9f7ae31a9968cf2ddac84020efe89d859788f9ec",
        "title": "Task Assignment of the Improved Contract Net Protocol under a Multi-Agent System"
      },
      {
        "paperId": "3fa50569925cfecc66fed5ec616682ecf3794ad7",
        "title": "Lyapunov-based Safe Policy Optimization for Continuous Control"
      },
      {
        "paperId": "a2bac02786e09a25a80732f738aaaee088d13e43",
        "title": "Low-Level Control of a Quadrotor With Deep Model-Based Reinforcement Learning"
      },
      {
        "paperId": "18d43061bef62bace6b738fed3b1c44eea2d2147",
        "title": "Accelerated Primal-Dual Policy Optimization for Safe Reinforcement Learning"
      },
      {
        "paperId": "c28ec2a40a2c77e20d64cf1c85dc931106df8e83",
        "title": "Overcoming Exploration in Reinforcement Learning with Demonstrations"
      },
      {
        "paperId": "9da4df00780136ea1b9d0885b85f1fd8dfe874fc",
        "title": "Deep-Reinforcement-Learning-Based Optimization for Cache-Enabled Opportunistic Interference Alignment Wireless Networks"
      },
      {
        "paperId": "1bead9000a719cb258bac7320228055aee650d2c",
        "title": "Leveraging Demonstrations for Deep Reinforcement Learning on Robotics Problems with Sparse Rewards"
      },
      {
        "paperId": "449532187c94af3dd3aa55e16d2c50f7854d2199",
        "title": "Trust Region Policy Optimization"
      },
      {
        "paperId": "79ab3c49903ec8cb339437ccf5cf998607fc313e",
        "title": "A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning"
      },
      {
        "paperId": "98f93f0be44b6b9caee53569ff7773d990ce39b4",
        "title": "Task allocation for maximizing reliability of distributed systems: A simulated annealing approach"
      },
      {
        "paperId": "7b96b6743108032ffd583cf0169044e7d09ff792",
        "title": "Multirobot coordination with deep reinforcement learning in complex environments"
      },
      {
        "paperId": "1289e61ef16843c6ed6b82c69c5a3c503988e343",
        "title": "Deep Reinforcement Learning With Application to Air Confrontation Intelligent Decision-Making of Manned/Unmanned Aerial Vehicle Cooperative System"
      },
      {
        "paperId": "df4e223681d8a5de15cf870464a26ba0ba628b17",
        "title": "Low Level Control of a Quadrotor with Deep Model-Based Reinforcement Learning"
      },
      {
        "paperId": null,
        "title": "Air Force Engineering University, in 2007, and the Ph.D. degree from the School of Electronics and Information Engineering,"
      },
      {
        "paperId": "02552a8b40f3a82a5353f596264db71d899a9b4a",
        "title": "Supporting Online Material for Reducing the Dimensionality of Data with Neural Networks"
      },
      {
        "paperId": null,
        "title": "He is currently pursuing the Ph.D. degree with Air Force Engineering University. His research interests include intention recognition"
      },
      {
        "paperId": null,
        "title": "When intercepting a target"
      },
      {
        "paperId": null,
        "title": "If more than one unit is tracking the target and has the same kill probability, the interceptor unit with the most ammunition remaining is assigned \ufb01rst"
      },
      {
        "paperId": null,
        "title": "For Blue\u2019s cruise missiles, priority is given to intercepting them using close-range munitions"
      },
      {
        "paperId": null,
        "title": "When the threat level of the drone reaches 7 or more, send a missile to intercept, enter the observation phase, if not killed, when the interception conditions are met"
      },
      {
        "paperId": null,
        "title": "D E = (cid:110)(cid:16) S i , G ( i ) (cid:17)(cid:111) m i = 1 B. IMITATION LEARNING WITH THE DECOUPLED NETWORK (ILDN)"
      },
      {
        "paperId": null,
        "title": "When the probability of killing is the same, the unit whose sensors are tracking the target is used \ufb01rst to intercept"
      }
    ],
    "cited_by": [
      {
        "paperId": "c2e90666947ddcad37104f2756c65ba14d32ddbb",
        "title": "Intelligent decision and planning for unmanned surface vehicle: A review of machine learning techniques"
      },
      {
        "paperId": "808688e14876a7e0f6e6e42f6270ccd9591108ba",
        "title": "Deep Reinforcement Learning-Based Multi-Robotic Agent Motion Planning"
      },
      {
        "paperId": "2dc2d78926e50b97a3313b6de816e6997ace61f0",
        "title": "Cooperative Multi-Agent Assignment over Stochastic Graphs via Constrained Reinforcement Learning"
      },
      {
        "paperId": "5c101b16826815fa22d2bf1f94dbb1267a8e8409",
        "title": "Research on Task Allocation for Continuous Transmission Line Inspection Based on Reinforcement Learning"
      },
      {
        "paperId": "2a368d4604275a83166151dbc78b1ab23a236b08",
        "title": "A transfer learning model for cognitive electronic reconnaissance of unmanned aerial vehicle: Experiments"
      },
      {
        "paperId": "f8c53b85e2fac2a413aaa7a84a2c1a1415cbc932",
        "title": "Reinforcement Learning for AI NPC Literacy Educational Game"
      },
      {
        "paperId": "da0607f8fe637808d90442149569c97a948d1279",
        "title": "Optimizing Demand Response in Multi-HVAC Systems by RL Control with Comfort Consideration"
      },
      {
        "paperId": "a4cb9c1ab05186100df8764773cb83d44f8c355f",
        "title": "Fast prediction method for dynamic RCS of rotary wing small UAVs"
      },
      {
        "paperId": "6c1edd0d4d323f85a3d4d06501f0ef03b94d62ec",
        "title": "A deep reinforcement learning control strategy to improve the operating flexibility of CHP units under variable load conditions"
      },
      {
        "paperId": "f7c04d9d4d42631746c2676fda3be9d0b9585e2f",
        "title": "AN OPTIMAL MACHINE LEARNING MODEL BASED ON SELECTIVE REINFORCED MARKOV DECISION TO PREDICT WEB BROWSING PATTERNS"
      }
    ],
    "score": 3.333333333333333
  },
  {
    "id": "1f577f6f0785c7967bef4ac6d0ab0370c2815b4d",
    "title": "Occupancy Reward-Driven Exploration with Deep Reinforcement Learning for Mobile Robot System",
    "authors": [
      "A. Kamalova",
      "Suk-Gyu Lee",
      "Soon-H. Kwon"
    ],
    "year": 2022,
    "citationCount": 10,
    "abstract": "This paper investigates the solution to a mobile-robot exploration problem following autonomous driving principles. The exploration task is formulated in this study as a process of building a map while a robot moves in an indoor environment beginning from full uncertainties. The sequence of robot decisions of how to move defines the strategy of the exploration that this paper aims to investigate, applying one of the Deep Reinforcement Learning methods, known as the Deep Deterministic Policy Gradient (DDPG) algorithm. A custom environment is created representing the mapping process with a map visualization, a robot model, and a reward function. The actor-critic network receives and sends input and output data, respectively, to the custom environment. The input is the data from the laser sensor, which is equipped on the robot. The output is the continuous actions of the robot in terms of linear and angular velocities. The training results of this study show the strengths and weaknesses of the DDPG algorithm for the robotic mapping problem. The implementation was developed in MATLAB platform using its corresponding toolboxes. A comparison with another exploration algorithm is also provided.",
    "url": "https://www.semanticscholar.org/paper/1f577f6f0785c7967bef4ac6d0ab0370c2815b4d",
    "pdf_url": "https://doi.org/10.3390/app12189249",
    "venue": "Applied Sciences",
    "publicationDate": "2022-09-15",
    "externalIds": {
      "DOI": "10.3390/app12189249",
      "CorpusId": 252352603
    },
    "references": [
      {
        "paperId": "56e8863838b4dcc4790108cd1e7e680a104a7c30",
        "title": "Machine Learning Algorithms: A Review"
      },
      {
        "paperId": "d70d488f4d26e770807148a0688ce4409971cc9c",
        "title": "A Review of Multi-Sensor Fusion SLAM Systems Based on 3D LIDAR"
      },
      {
        "paperId": "674c0118cab68b107466ec067efa5234043c250b",
        "title": "Robotic Mapping Approach under Illumination-Variant Environments at Planetary Construction Sites"
      },
      {
        "paperId": "0c3411ab18c0ce4e85a7d599845a506a8bbc857b",
        "title": "Development of Autonomous Navigation Performance Criteria and Related Test Methods for Autonomous Mobile Robot in the Outdoor Environment"
      },
      {
        "paperId": "75397aa4447e10b1c2ce36295a9da5bd8be7b034",
        "title": "A review of mobile robot motion planning methods: from classical motion planning workflows to reinforcement learning-based architectures"
      },
      {
        "paperId": "cd2da98b3d37bb6e5227176852cbcac05e15f262",
        "title": "Autonomous mobile robot navigation in uncertain dynamic environments based on deep reinforcement learning"
      },
      {
        "paperId": "dc90ea37246998cb5cb0d921caad25afa322ebea",
        "title": "Collaborative Complete Coverage and Path Planning for Multi-Robot Exploration"
      },
      {
        "paperId": "625182bd20996635bec933ed00d17b700dac6b6f",
        "title": "A Collision Avoidance Method Based on Deep Reinforcement Learning"
      },
      {
        "paperId": "4ef678ab88fdcd0eeaff8b36cc8ecd74bc85115e",
        "title": "Active Mapping and Robot Exploration: A Survey"
      },
      {
        "paperId": "3ae947d4bb453f7a592ce5537bb7aefb5aec1e9b",
        "title": "Multimodal Deep Reinforcement Learning with Auxiliary Task for Obstacle Avoidance of Indoor Mobile Robot"
      },
      {
        "paperId": "b1e8d1d2f3207678276977bb7f28b33714e66a49",
        "title": "Accelerated Sim-to-Real Deep Reinforcement Learning: Learning Collision Avoidance from Human Player"
      },
      {
        "paperId": "79202a6533e4730deb54f7cc6647c2384988cb23",
        "title": "Modular deep reinforcement learning from reward and punishment for robot navigation"
      },
      {
        "paperId": "4141a3b71d306f61cb788467f1805f9c933a0af8",
        "title": "Learning Human-Aware Robot Navigation from Physical Interaction via Inverse Reinforcement Learning"
      },
      {
        "paperId": "dc3b309cca6b14f1ef138083b557b252fe953a56",
        "title": "Frontier Detection and Reachability Analysis for Efficient 2D Graph-SLAM Based Active Exploration"
      },
      {
        "paperId": "98d282579593987aa13c17717b12207a3af86c68",
        "title": "Autonomous Exploration Under Uncertainty via Deep Reinforcement Learning on Graphs"
      },
      {
        "paperId": "62516303058a1322450b58e4cd778ab873b5e531",
        "title": "Object Goal Navigation using Goal-Oriented Semantic Exploration"
      },
      {
        "paperId": "8a47843c2e664e5e7e218e2d891726d023619403",
        "title": "Deep Reinforcement learning for real autonomous mobile robot navigation in indoor environments"
      },
      {
        "paperId": "1a15b576e85e997187570d7c4ac63b3d78c294d4",
        "title": "When Artificial Intelligence and Computational Neuroscience Meet"
      },
      {
        "paperId": "324719217793bf42792efef807baf8a163f6f791",
        "title": "A novel mobile robot navigation method based on deep reinforcement learning"
      },
      {
        "paperId": "05fb3049ccfe037e1baa73cc0022ccda621dd2e7",
        "title": "Mapless Navigation among Dynamics with Social-safety-awareness: a reinforcement learning approach from 2D laser scans"
      },
      {
        "paperId": "f342693f46f1732c39dd1987a473d4496ecf7947",
        "title": "Continuous Control with Deep Reinforcement Learning for Mobile Robot Navigation"
      },
      {
        "paperId": "79223048f8f7815cb5880f8997b5be9f0dd3048b",
        "title": "A fast, complete, point cloud based loop closure for LiDAR odometry and mapping"
      },
      {
        "paperId": "65d850b7067e0c634a3e5351888798c8839c1184",
        "title": "The current state and future outlook of rescue robotics"
      },
      {
        "paperId": "8b835a6dedd55e57c2d5328b94b839faa25faca8",
        "title": "A Brief History of Artificial Intelligence: On the Past, Present, and Future of Artificial Intelligence"
      },
      {
        "paperId": "00fe45b2b97562b196c237e182f40207f3702410",
        "title": "Robotics, Artificial Intelligence, and the Evolving Nature of Work"
      },
      {
        "paperId": "643fb5aeccce0351cac89bb8e446c8d7e48a3d99",
        "title": "The limits and potentials of deep learning for robotics"
      },
      {
        "paperId": "535d184eadf47fa17ce4073b6e2f180783e85300",
        "title": "Curiosity-driven Exploration for Mapless Navigation with Deep Reinforcement Learning"
      },
      {
        "paperId": "cd8eca4378ec55b092900a70458dc24031c3d221",
        "title": "Application of deep reinforcement learning in mobile robot path planning"
      },
      {
        "paperId": "34121542c2d157ee0403c1d55b465c9d38408f50",
        "title": "Autonomous exploration of mobile robots through deep neural networks"
      },
      {
        "paperId": "3ea6113b1dd5f43cb8c6ec79d648b934aad9d697",
        "title": "Neural SLAM"
      },
      {
        "paperId": "799c0e461332570ecde97e13266fecde8476efe3",
        "title": "Virtual-to-real deep reinforcement learning: Continuous control of mobile robots for mapless navigation"
      },
      {
        "paperId": "b99f42939f80e870ff3b901accc24d8063793d63",
        "title": "Deep-learning in Mobile Robotics - from Perception to Control Systems: A Survey on Why and Why not"
      },
      {
        "paperId": "8a8f5ec1d76ab3308434b8fccc2b5e67907226ac",
        "title": "Mobile robots exploration through cnn-based reinforcement learning"
      },
      {
        "paperId": "1a0d50fd4a3e52b25c0a662b687daeb8ea963b4b",
        "title": "Deep reinforcement learning with successor features for navigation across similar environments"
      },
      {
        "paperId": "01dfe1868e8abc090b1485482929f65743e23743",
        "title": "Towards Cognitive Exploration through Deep Reinforcement Learning for Mobile Robots"
      },
      {
        "paperId": "07f1ae1658916d12e32248bad39049d15b0386c6",
        "title": "Deep learning of structured environments for robot search"
      },
      {
        "paperId": "7af7f2f539cd3479faae4c66bbef49b0f66202fa",
        "title": "Target-driven visual navigation in indoor scenes using deep reinforcement learning"
      },
      {
        "paperId": "09c0c993500f5d15b3ccbfbec11bbb445bf51e57",
        "title": "SemanticFusion: Dense 3D semantic mapping with convolutional neural networks"
      },
      {
        "paperId": "116fc10b798e3294b00e92f2b8053d0c89ad9182",
        "title": "A robot exploration strategy based on Q-learning network"
      },
      {
        "paperId": "024006d4c2a89f7acacc6e4438d156525b60a98f",
        "title": "Continuous control with deep reinforcement learning"
      },
      {
        "paperId": "340f48901f72278f6bf78a04ee5b01df208cc508",
        "title": "Human-level control through deep reinforcement learning"
      },
      {
        "paperId": "2319a491378867c7049b3da055c5df60e1671158",
        "title": "Playing Atari with Deep Reinforcement Learning"
      },
      {
        "paperId": "4b965d0ebe5b6362001cb6d6c3b89cefde53f1f5",
        "title": "A review of research in multi-robot systems"
      },
      {
        "paperId": "2f755a9de4c9ca23a7e412e327a4e42c807f16a6",
        "title": "Coordinated multi-robot exploration using a segmentation of the environment"
      },
      {
        "paperId": "8584ac7f54090d9f66a9912ac7cf4b266db077ca",
        "title": "Trajectory Optimization using Reinforcement Learning for Map Exploration"
      },
      {
        "paperId": "f4265f4355c23b68d8b3d88c70d61be3be3b4918",
        "title": "Probalistic Robotics20062D.M. Hutton Computer Science International, UK(Section Editor). Probalistic Robotics. MIT Press, 2006. 668 pp., ISBN: 0\u2010262\u201020162"
      },
      {
        "paperId": "70e81d7a219b212008d20bc891cb43ae81fd00b5",
        "title": "Towards integrated autonomous underwater operations for ocean mapping and monitoring"
      },
      {
        "paperId": "2913c2bf3f92b5ae369400a42b2d27cc5bc05ecb",
        "title": "Deep Learning"
      }
    ],
    "cited_by": [
      {
        "paperId": "0fcdc53b3a3c9f4289e7d9913ba1bdbf99ff4402",
        "title": "CogniPlan: Uncertainty-Guided Path Planning with Conditional Generative Layout Prediction"
      },
      {
        "paperId": "1654e36b1015935548c440784302433a986f0fa3",
        "title": "Population\u2010Based Optimization With Decentralized Method"
      },
      {
        "paperId": "74293451d1af4f72b273537b636f18fa08702b96",
        "title": "Path Planning for Lunar Rovers in Dynamic Environments: An Autonomous Navigation Framework Enhanced by Digital Twin-Based A*-D3QN"
      },
      {
        "paperId": "2e9db0b24eebfa0bbc0695b70d7396f6822d10b4",
        "title": "Multi-agent based DRL with federated learning for data transmission in mobile sensor networks"
      },
      {
        "paperId": "8a9efbdf030542c862e71922c0d0a01ab46f1f7f",
        "title": "A Hierarchical Region-Based Approach for Efficient Multi-Robot Exploration"
      },
      {
        "paperId": "bfa31955144ef893708931a37c12e6ffb595b36a",
        "title": "Reinforcement Learning-Based Optimal Path Planning for Mobile Robot with Obstacles Avoidance"
      },
      {
        "paperId": "6f13b6392b8f31e7c82fa43b63ddef8b76d37537",
        "title": "An Enhanced Deep Q Network Algorithm for Localized Obstacle Avoidance in Indoor Robot Path Planning"
      },
      {
        "paperId": "dd6661167d88e3e85b5ea81911da84f030c831a0",
        "title": "Dynamic path planning via Dueling Double Deep Q-Network (D3QN) with prioritized experience replay"
      },
      {
        "paperId": "3a7baf028efb106f2431965d79600725e0aab536",
        "title": "Signal Novelty Detection as an Intrinsic Reward for Robotics"
      },
      {
        "paperId": "aebbcbe8786fefa80fdea818bf973c3ae3d3bd9c",
        "title": "Multi-Robot Exploration of Unknown Space Using Combined Meta-Heuristic Salp Swarm Algorithm and Deterministic Coordinated Multi-Robot Exploration"
      }
    ],
    "score": 3.333333333333333
  },
  {
    "id": "33e3f13087abd5241d55523140720f5e684b7bee",
    "title": "Generative Planning for Temporally Coordinated Exploration in Reinforcement Learning",
    "authors": [
      "Haichao Zhang",
      "Wei Xu",
      "Haonan Yu"
    ],
    "year": 2022,
    "citationCount": 10,
    "abstract": "Standard model-free reinforcement learning algorithms optimize a policy that generates the action to be taken in the current time step in order to maximize expected future return. While flexible, it faces difficulties arising from the inefficient exploration due to its single step nature. In this work, we present Generative Planning method (GPM), which can generate actions not only for the current step, but also for a number of future steps (thus termed as generative planning). This brings several benefits to GPM. Firstly, since GPM is trained by maximizing value, the plans generated from it can be regarded as intentional action sequences for reaching high value regions. GPM can therefore leverage its generated multi-step plans for temporally coordinated exploration towards high value regions, which is potentially more effective than a sequence of actions generated by perturbing each action at single step level, whose consistent movement decays exponentially with the number of exploration steps. Secondly, starting from a crude initial plan generator, GPM can refine it to be adaptive to the task, which, in return, benefits future explorations. This is potentially more effective than commonly used action-repeat strategy, which is non-adaptive in its form of plans. Additionally, since the multi-step plan can be interpreted as the intent of the agent from now to a span of time period into the future, it offers a more informative and intuitive signal for interpretation. Experiments are conducted on several benchmark environments and the results demonstrated its effectiveness compared with several baseline methods.",
    "url": "https://www.semanticscholar.org/paper/33e3f13087abd5241d55523140720f5e684b7bee",
    "pdf_url": "https://arxiv.org/pdf/2201.09765.pdf",
    "venue": "International Conference on Learning Representations",
    "publicationDate": "2022-01-24",
    "externalIds": {
      "ArXiv": "2201.09765",
      "DBLP": "conf/iclr/ZhangXY22",
      "CorpusId": 246241075
    },
    "references": [
      {
        "paperId": "9776324620082ab89edad134aa259b1ae7bf6f1f",
        "title": "TempoRL: Learning When to Act"
      },
      {
        "paperId": "1508879dae81f73f56ba0cb0e25150d9c5f8f731",
        "title": "TAAC: Temporally Abstract Actor-Critic for Continuous Control"
      },
      {
        "paperId": "3b0b15dcef21d2ac69ba85300c3dbd95f1182f29",
        "title": "Locally Persistent Exploration in Continuous Control Tasks with Sparse Rewards"
      },
      {
        "paperId": "d669358916608af804c20329b7287d02c75b1311",
        "title": "Behavior Priors for Efficient Reinforcement Learning"
      },
      {
        "paperId": "b68b8b980db62308864b2a7d33718182c5f8335b",
        "title": "Accelerating Reinforcement Learning with Learned Skill Priors"
      },
      {
        "paperId": "72260c19441259404ed24003d9e27588fb3613ae",
        "title": "Maximum Entropy Gain Exploration for Long Horizon Multi-goal Reinforcement Learning"
      },
      {
        "paperId": "c6ba8e55f2fed2037378a94a3f8a2b1b09825636",
        "title": "Temporally-Extended \u03b5-Greedy Exploration"
      },
      {
        "paperId": "3a71c306eb6232658c9e5fd48aed1ef3befe5fbe",
        "title": "Planning to Explore via Self-Supervised World Models"
      },
      {
        "paperId": "c6c5155c38d4e4ed3db17ca3ab9bcec3a2e7be4d",
        "title": "Continuous-Discrete Reinforcement Learning for Hybrid Control in Robotics"
      },
      {
        "paperId": "0cc956565c7d249d4197eeb1dbab6523c648b2c9",
        "title": "Dream to Control: Learning Behaviors by Latent Imagination"
      },
      {
        "paperId": "42d0b94c8cf71541e5b52f45ab845b6d47dc4efe",
        "title": "End-to-End Model-Free Reinforcement Learning for Urban Driving Using Implicit Affordances"
      },
      {
        "paperId": "e73a1061e36d6e55ee4d5cf43d0159e26ab59a60",
        "title": "CoverNet: Multimodal Behavior Prediction Using Trajectory Sets"
      },
      {
        "paperId": "c39fb7a46335c23f7529dd6f9f980462fd38653a",
        "title": "Mastering Atari, Go, chess and shogi by planning with a learned model"
      },
      {
        "paperId": "648d2cd200584f790bd6d07ba875eabc21710d6d",
        "title": "Explicit Explore-Exploit Algorithms in Continuous State Spaces"
      },
      {
        "paperId": "cc1d3f10eb4d8a92e2fe09e67f0352d7a0600afd",
        "title": "Learning to Predict Without Looking Ahead: World Models Without Forward Prediction"
      },
      {
        "paperId": "969f6ef56d812fdb3afc7d2a84c59fdfc609dc15",
        "title": "Planning Approximate Exploration Trajectories for Model-Free Reinforcement Learning in Contact-Rich Manipulation"
      },
      {
        "paperId": "80f06b686a4e747a64c3514929667821bdb64783",
        "title": "Regularizing Neural Networks for Future Trajectory Prediction via Inverse Reinforcement Learning"
      },
      {
        "paperId": "db1614b97aec1e0ead9554a038aa0f8dd9a26e30",
        "title": "Exploring Model-based Planning with Policy Networks"
      },
      {
        "paperId": "e0889fcee1acd985af76a3907d5d0029bf260be9",
        "title": "Search on the Replay Buffer: Bridging Planning and Reinforcement Learning"
      },
      {
        "paperId": "5a22ce57b02c8aa446c793435a2235bbe6afbc65",
        "title": "Exploration via Hindsight Goal Generation"
      },
      {
        "paperId": "bc2a0a27ea6c0e7eec0ccf605ed12ecea5d345c8",
        "title": "Driving with Style: Inverse Reinforcement Learning in General-Purpose Planning for Automated Driving"
      },
      {
        "paperId": "7698ce800b7a35dc2951586c1428545ba547d0e8",
        "title": "Autoregressive Policies for Continuous Control Deep Reinforcement Learning"
      },
      {
        "paperId": "7388826b5ee00efe17cb7f19a623d9b5e955ae70",
        "title": "Skew-Fit: State-Covering Self-Supervised Reinforcement Learning"
      },
      {
        "paperId": "766d48ad414065e5e2fa4fef13c8951f28b58bf4",
        "title": "The Natural Language of Actions"
      },
      {
        "paperId": "029341c7f1ce11696a6bc7e7a716b7876010ebc7",
        "title": "Model Learning for Look-ahead Exploration in Continuous Control"
      },
      {
        "paperId": "fea3e63c97c7292dc6fbcb3ffe7131eb54053986",
        "title": "Learning Latent Dynamics for Planning from Pixels"
      },
      {
        "paperId": "9b114351bf97a66ab8cfef4c3b04a0f70503295e",
        "title": "Deep Imitative Models for Flexible Inference, Planning, and Control"
      },
      {
        "paperId": "6ab8aca1f727e379632292e1ec4a24ea2739cf89",
        "title": "Model-Based Active Exploration"
      },
      {
        "paperId": "6a9013a8cdd84e423223f76a903028011c84c4ab",
        "title": "Plan Online, Learn Offline: Efficient Learning and Exploration via Model-Based Control"
      },
      {
        "paperId": "a88d54c168a84ed8a04d2a32be0b5939586b5792",
        "title": "NADPEx: An on-policy temporally consistent exploration method for deep reinforcement learning"
      },
      {
        "paperId": "0f710daa7bbba3350169f0bbb5d24f8db3e5199e",
        "title": "Self-Consistent Trajectory Autoencoder: Hierarchical Reinforcement Learning with Trajectory Embeddings"
      },
      {
        "paperId": "56136aa0b2c347cbcf3d50821f310c4253155026",
        "title": "Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models"
      },
      {
        "paperId": "ce1c28ca2f52a42c6e60d792cd71ba894abc47d5",
        "title": "Multi-Goal Reinforcement Learning: Challenging Robotics Environments and Request for Research"
      },
      {
        "paperId": "4debb99c0c63bfaa97dd433bc2828e4dac81c48b",
        "title": "Addressing Function Approximation Error in Actor-Critic Methods"
      },
      {
        "paperId": "811df72e210e20de99719539505da54762a11c6d",
        "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor"
      },
      {
        "paperId": "ebf0615fc4d98cf1dbe527c79146ce1e50dce9af",
        "title": "CARLA: An Open Urban Driving Simulator"
      },
      {
        "paperId": "600bfe5f0597ebd84898f0c4270ddfb3750594f5",
        "title": "Imagination-Augmented Agents for Deep Reinforcement Learning"
      },
      {
        "paperId": "cf020b27d06efb28f3e5db264aceeec1f397817b",
        "title": "Value Prediction Network"
      },
      {
        "paperId": "3fab4d28be5ff73f42c8b307117aa8d680345ad9",
        "title": "Noisy Networks for Exploration"
      },
      {
        "paperId": "142497432fe179ddb6ffe600c64a837ec6179550",
        "title": "Parameter Space Noise for Exploration"
      },
      {
        "paperId": "225ab689f41cef1dc18237ef5dab059a49950abf",
        "title": "Curiosity-Driven Exploration by Self-Supervised Prediction"
      },
      {
        "paperId": "869f4fc59d74a96098ed46935cb6fd1a537c38ce",
        "title": "Information theoretic MPC for model-based reinforcement learning"
      },
      {
        "paperId": "2d6075ffa9ef3fbfe338bfeb469a50883242bdcd",
        "title": "Learning to Repeat: Fine Grained Action Repetition for Deep Reinforcement Learning"
      },
      {
        "paperId": "39b19ea254b0952f2abd23ad899420749816bb1d",
        "title": "The Predictron: End-To-End Learning and Planning"
      },
      {
        "paperId": "3deecaee4ec1a37de3cb10420eaabff067669e17",
        "title": "Stochastic Neural Networks for Hierarchical Reinforcement Learning"
      },
      {
        "paperId": "e37b999f0c96d7136db07b0185b837d5decd599a",
        "title": "Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates"
      },
      {
        "paperId": "15b26d8cb35d7e795c8832fe08794224ee1e9f84",
        "title": "The Option-Critic Architecture"
      },
      {
        "paperId": "d1e51c7e374dca4465a91300e98bfb27335be463",
        "title": "Watch this: Scalable cost-function learning for path planning in urban environments"
      },
      {
        "paperId": "4ba25cb493ac7a03fc15d3b936257c9a6c689c1d",
        "title": "Strategic Attentive Writer for Learning Macro-Actions"
      },
      {
        "paperId": "0e3cc46583217ec81e87045a4f9ae3478a008227",
        "title": "End to End Learning for Self-Driving Cars"
      },
      {
        "paperId": "846aedd869a00c09b40f1f1f35673cb22bc87490",
        "title": "Mastering the game of Go with deep neural networks and tree search"
      },
      {
        "paperId": "024006d4c2a89f7acacc6e4438d156525b60a98f",
        "title": "Continuous control with deep reinforcement learning"
      },
      {
        "paperId": "438bb3d46e72b177ed1c9b7cd2c11a045644a1f4",
        "title": "Gradient Estimation Using Stochastic Computation Graphs"
      },
      {
        "paperId": "340f48901f72278f6bf78a04ee5b01df208cc508",
        "title": "Human-level control through deep reinforcement learning"
      },
      {
        "paperId": "ca7f25d5b139c684a8d477e954380138dcba3a73",
        "title": "Variational Policy Search via Trajectory Optimization"
      },
      {
        "paperId": "244539f454800697ed663326b7cfba337ca0c2ec",
        "title": "Guided Policy Search"
      },
      {
        "paperId": "2fc26ef1f69677bad6c64f8bb5587f18a1c2e374",
        "title": "Constructing Skill Trees for Reinforcement Learning Agents from Demonstration Trajectories"
      },
      {
        "paperId": "36fb553aa996885017afe3489a8377eceddc08ee",
        "title": "Planning-based prediction for pedestrians"
      },
      {
        "paperId": "0e7638dc16a5e5e9e46c91272bfb9c3dd242ef6d",
        "title": "Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning"
      },
      {
        "paperId": "6f92e052835aa778af1669501b19e1fa0b76bc1c",
        "title": "Learning Macro-Actions in Reinforcement Learning"
      },
      {
        "paperId": "1678bd32846b1aded5b1e80a617170812e80f562",
        "title": "Feudal Reinforcement Learning"
      },
      {
        "paperId": "831edc3d67457db83da40d260e93bfd7559347ae",
        "title": "Dyna, an integrated architecture for learning, planning, and reacting"
      },
      {
        "paperId": "fba442362d7c4373975f71ae9637b51e237ed0d6",
        "title": "Minimum jerk path generation"
      },
      {
        "paperId": "e941b0898691455b85dbe12f551b1241b8cbf7ec",
        "title": "Deep Coherent Exploration for Continuous Control"
      },
      {
        "paperId": "ef4f5a50837a7c1b3e87b9300ffc7ba00d461a0f",
        "title": "AUTO-ENCODING VARIATIONAL BAYES"
      },
      {
        "paperId": null,
        "title": "Universal planning networks: Learning generalizable representations for visuomotor control"
      }
    ],
    "cited_by": [
      {
        "paperId": "574ec4679c9efdaac629b8fd7f01ee07d78b741d",
        "title": "Chunking the Critic: A Transformer-based Soft Actor-Critic with N-Step Returns"
      },
      {
        "paperId": "387b9d21e44c664e2328200c9362570b877f6e31",
        "title": "Select before Act: Spatially Decoupled Action Repetition for Continuous Control"
      },
      {
        "paperId": "6efb032415f6e62d8d5de7a1571f35901e1c6f97",
        "title": "Overcoming Slow Decision Frequencies in Continuous Control: Model-Based Sequence Reinforcement Learning for Model-Free Control"
      },
      {
        "paperId": "55d322ddac05dcf89fc43cc859ca136471877bbd",
        "title": "Knowledge Planning in Large Language Models for Domain-Aligned Counseling Summarization"
      },
      {
        "paperId": "761509e64bf8eb462b615e5fbcea866289ccc8af",
        "title": "FuRL: Visual-Language Models as Fuzzy Rewards for Reinforcement Learning"
      },
      {
        "paperId": "bdcfd95b3d98a33e859de8e4fda69940ff6c51a2",
        "title": "Optimized glycemic control of type 2 diabetes with reinforcement learning: a proof-of-concept trial"
      },
      {
        "paperId": "828e27fd4fcd5e8982032b903950947b12afb6bb",
        "title": "Diffusion Model is an Effective Planner and Data Synthesizer for Multi-Task Reinforcement Learning"
      },
      {
        "paperId": "7f270c9b098727675c9d8b893e362b561d61f27e",
        "title": "Policy Expansion for Bridging Offline-to-Online Reinforcement Learning"
      },
      {
        "paperId": "caa03f47176505fc27e56708c2ce990c5e7abed2",
        "title": "Leveraging Demonstrations with Latent Space Priors"
      },
      {
        "paperId": "c56bc32f12c991f0e1b7d2999a1ec4a842c25709",
        "title": "Model-based Reinforcement Learning with Multi-step Plan Value Estimation"
      }
    ],
    "score": 3.333333333333333
  },
  {
    "id": "23e3f39ede3c45cb8cf456de1f56d7a8b8d4bc2e",
    "title": "A Decision-Making Strategy for Car Following Based on Naturalist Driving Data via Deep Reinforcement Learning",
    "authors": [
      "Wenli Li",
      "Yousong Zhang",
      "Xiaohui Shi",
      "Fanke Qiu"
    ],
    "year": 2022,
    "citationCount": 10,
    "abstract": "To improve the satisfaction and acceptance of automatic driving, we propose a deep reinforcement learning (DRL)-based autonomous car-following (CF) decision-making strategy using naturalist driving data (NDD). This study examines the traits of CF behavior using 1341 pairs of CF events taken from the Next Generation Simulation (NGSIM) data. Furthermore, in order to improve the random exploration of the agent\u2019s action, the dynamic characteristics of the speed-acceleration distribution are established in accordance with NDD. The action\u2019s varying constraints are achieved via a normal distribution 3\u03c3 boundary point-to-fit curve. A multiobjective reward function is designed considering safety, efficiency, and comfort, according to the time headway (THW) probability density distribution. The introduction of a penalty reward in mechanical energy allows the agent to internalize negative experiences. Next, a model of agent-environment interaction for CF decision-making control is built using the deep deterministic policy gradient (DDPG) method, which can explore complicated environments. Finally, extensive simulation experiments validate the effectiveness and accuracy of our proposal, and the driving strategy is learned through real-world driving data, which is better than human data.",
    "url": "https://www.semanticscholar.org/paper/23e3f39ede3c45cb8cf456de1f56d7a8b8d4bc2e",
    "pdf_url": "https://doi.org/10.3390/s22208055",
    "venue": "Italian National Conference on Sensors",
    "publicationDate": "2022-10-01",
    "externalIds": {
      "DBLP": "journals/sensors/LiZSQ22",
      "PubMedCentral": "9608473",
      "DOI": "10.3390/s22208055",
      "CorpusId": 253135874,
      "PubMed": "36298405"
    },
    "references": [
      {
        "paperId": "5d45d8fca50e5dbc3a048ae1673d987dc983949c",
        "title": "Generalized Single-Vehicle-Based Graph Reinforcement Learning for Decision-Making in Autonomous Driving"
      },
      {
        "paperId": "126955d67e7ca80b4e34250601bdc5cdba6580b3",
        "title": "Biased Pressure: Cyclic Reinforcement Learning Model for Intelligent Traffic Signal Control"
      },
      {
        "paperId": "6c0f9fed102f7806aa4941db81cb0fc4598e06c4",
        "title": "An Investigation into the Appropriateness of Car-Following Models in Assessing Autonomous Vehicles"
      },
      {
        "paperId": "a53394c01efd8f1f54b6db344817f738b679064e",
        "title": "Hybrid Car-Following Strategy Based on Deep Deterministic Policy Gradient and Cooperative Adaptive Cruise Control"
      },
      {
        "paperId": "26c944a4808d9aacffdf6bf0d015666c60a4ba36",
        "title": "Driver Characteristics Oriented Autonomous Longitudinal Driving System in Car-Following Situation"
      },
      {
        "paperId": "ee1d276415b9c0b29fe458354bb343e2ebaeb14b",
        "title": "Optimal car-following control for intelligent vehicles using online road-slope approximation method"
      },
      {
        "paperId": "1373206f36def7313836d88cac1544c62f99f514",
        "title": "Cross-Type Transfer for Deep Reinforcement Learning Based Hybrid Electric Vehicle Energy Management"
      },
      {
        "paperId": "ea4ca33c55f7513451db3f30b1f98b18430170c4",
        "title": "Trajectory data-based traffic flow studies: A revisit"
      },
      {
        "paperId": "129983331ca874142a3e8eb2d93d820bdf1f9aca",
        "title": "Deep Reinforcement Learning for Autonomous Driving: A Survey"
      },
      {
        "paperId": "cb7fbc294d9c09244090f39bfbcfaab98433d694",
        "title": "Survey of Deep Reinforcement Learning for Motion Planning of Autonomous Vehicles"
      },
      {
        "paperId": "155c5e8531cb0080def99252964e90327cd71dcf",
        "title": "Learning Robust Control Policies for End-to-End Autonomous Driving From Data-Driven Simulation"
      },
      {
        "paperId": "c69f87f3d5452aba69a5352748ac4e9dde8ab6a4",
        "title": "Reinforcement Learning Model with a Reward Function Based on Human Driving Characteristics"
      },
      {
        "paperId": "f90d2ebdcd30c02cfb4942f8441815e8f741a926",
        "title": "The Car-Following Model Based on Fuzzy Inference Controller"
      },
      {
        "paperId": "5d041daed6c524405f0380fe6e4cf4eed791d0c0",
        "title": "Safe Technology with a Novel Rear Collision Avoidance System of Vehicles"
      },
      {
        "paperId": "ba4114a4d11384ada258187ae5e8bba75f11c4c3",
        "title": "Using advanced adaptive cruise control systems to reduce congestion at sags: An evaluation based on microscopic traffic simulation"
      },
      {
        "paperId": "7edcf91a18fc8fddf3e4d48ae107ac6c54970ce4",
        "title": "Multimodel Approach to Personalized Autonomous Adaptive Cruise Control"
      },
      {
        "paperId": "8d539ce145276fd9ccba770136f6323155e54294",
        "title": "Autonomous Highway Driving using Deep Reinforcement Learning"
      },
      {
        "paperId": "fb1c46d24fadc748020b8d70a52a13e968c1a8d7",
        "title": "Safe, Efficient, and Comfortable Velocity Control based on Reinforcement Learning for Autonomous Driving"
      },
      {
        "paperId": "66c7f79a81c125c3180fa9f77cb94639aef16567",
        "title": "Human-Like Maneuver Decision Using LSTM-CRF Model for On-Road Self-Driving"
      },
      {
        "paperId": "e717bdf2865d155f748d4222f5ca08a2267c0cda",
        "title": "Integrated Longitudinal and Lateral Networked Control System Design for Vehicle Platooning"
      },
      {
        "paperId": "f9634adc35ba6757bd1ffffc102470dad8b0e881",
        "title": "Cooperative Adaptive Cruise Control for Connected Autonomous Vehicles by Factoring Communication-Related Constraints"
      },
      {
        "paperId": "9713139ff53d787e6a6ff8e5ed8ac0130390cf94",
        "title": "Design and Experimental Validation of a Cooperative Adaptive Cruise Control System Based on Supervised Reinforcement Learning"
      },
      {
        "paperId": "591cc5e912f70bb2d1f8712c7e41be1d6e473e10",
        "title": "The Relationship between Different Safety Indicators in Car-following Situations"
      },
      {
        "paperId": "9c77545cd4c1e454edaf7023c7f932bf6280f46a",
        "title": "Driver Behavior Analysis for Advanced Driver Assistance System"
      },
      {
        "paperId": "88ce1015c40b6e61425a16ee31913cf98fe17ef7",
        "title": "Comfort in automated driving: An analysis of preferences for different automated driving styles and their dependence on personality traits"
      },
      {
        "paperId": "dd4053836e7b8ded642cab2e4535cdccfb5378cc",
        "title": "Capturing Car-Following Behaviors by Deep Learning"
      },
      {
        "paperId": "9857a9662545304044897be4ca7a95563f679f9b",
        "title": "How Much Data Are Enough? A Statistical Approach With Case Study on Longitudinal Driving Behavior"
      },
      {
        "paperId": "8db9df2eadea654f128c1887722c677c708e8a47",
        "title": "Deep Reinforcement Learning framework for Autonomous Driving"
      },
      {
        "paperId": "ab2e9398954142727aa78ac68517159906e3fffd",
        "title": "Imitating driver behavior with generative adversarial networks"
      },
      {
        "paperId": "6ddaf7ef913c12013561c7f8e22af59649fe43b3",
        "title": "Combining Deep Reinforcement Learning and Safety Based Control for Autonomous Driving"
      },
      {
        "paperId": "fc79dd5e8ecb174860957c7b36cc1585c33877c9",
        "title": "A Control Strategy of Autonomous Vehicles Based on Deep Reinforcement Learning"
      },
      {
        "paperId": "46d3ce3b4697a75629d6c5507c7a6d9c6e920aa0",
        "title": "Trajectory data reconstruction and simulation-based validation against macroscopic traffic patterns"
      },
      {
        "paperId": "024006d4c2a89f7acacc6e4438d156525b60a98f",
        "title": "Continuous control with deep reinforcement learning"
      },
      {
        "paperId": "d4d04d602bdbbe07ae2dc6ddcf971296609da00a",
        "title": "Autonomous car following: A learning-based approach"
      },
      {
        "paperId": "ff17793c11dc39c739848d48aaa16a804bba6cdd",
        "title": "A Fuzzy Control Based Self-Optimizing PID Model for Autonomous Car Following on Highway"
      },
      {
        "paperId": "6d8bd6c1cf7668f6c12b755fe237069936841c53",
        "title": "Can Results of car-following Model Calibration Based on Trajectory Data be Trusted?"
      },
      {
        "paperId": "bc2d42b35fce17ac9450335ae7157a65f51b7e0d",
        "title": "Human driving data-based design of a vehicle adaptive cruise control algorithm"
      },
      {
        "paperId": "31bef5154feab4dde7e30759e08c36d7001fec9f",
        "title": "DDPG-Based Decision-Making Strategy of Adaptive Cruising for Heavy Vehicles Considering Stability"
      },
      {
        "paperId": "b1b6202a67101fd4d5f177adfd399604306ad352",
        "title": "Comfort Oriented Robust Adaptive Cruise Control in Multi-Lane Traffic Conditions"
      }
    ],
    "cited_by": [
      {
        "paperId": "9484c28cdabd6e5d6b286f9e4f24afdb97199059",
        "title": "Modeling and Analysis of Mixed Traffic Flow Considering Driver Stochasticity and CAV Connectivity Uncertainty"
      },
      {
        "paperId": "fd6b5bf6be4b7894e2ff7d438862bc06d3fd9add",
        "title": "Human-Like Behavior Decision Making in Autonomous Driving Based on Deep Reinforcement Learning"
      },
      {
        "paperId": "fc5c43a7b1fed6ab5d7dfd61dbbdb1d24f414d9e",
        "title": "From Virtual to Reality: A Deep Reinforcement Learning Solution to Implement Autonomous Driving with 3D-LiDAR"
      },
      {
        "paperId": "925c6d54fc2e98ad8cebb32079b9ef6897893bde",
        "title": "Design and Application of Deep Reinforcement Learning Algorithms Based on Unbiased Exploration Strategies for Value Functions"
      },
      {
        "paperId": "bdbcff22b171c61403d5445b2651282c76a9c9cc",
        "title": "A CNN-LSTM Car-Following Model Considering Generalization Ability"
      },
      {
        "paperId": "8a582da973cf7088f12e0463159fb6ab51beed81",
        "title": "Deep Reinforcement Learning Car-Following Model Considering Longitudinal and Lateral Control"
      },
      {
        "paperId": "06fe1590171c767976de82f15e7071842bf13966",
        "title": "Enhancing Longitudinal Velocity Control With Attention Mechanism-Based Deep Deterministic Policy Gradient (DDPG) for Safety and Comfort"
      },
      {
        "paperId": "1e5b32744aa03d6f4cd45783118b334bcc306e9f",
        "title": "Optimizing Longitudinal Velocity Control via Self-Supervised Learning and Deep Deterministic Policy Gradient"
      },
      {
        "paperId": "0b9c3d12ff3cd5f6863126a5fd3db81a1a76a895",
        "title": "A Car-Following Model Integrating Personalized Driving Style Based on the DER-DDPG Deep Reinforcement Learning Algorithm"
      },
      {
        "paperId": "197f980e5cb6149390a7836d3e9a7a5cc8a05c3c",
        "title": "Toward Learning Human-Like, Safe and Comfortable Car-Following Policies With a Novel Deep Reinforcement Learning Approach"
      }
    ],
    "score": 3.333333333333333
  },
  {
    "id": "97fe089acbe9d1f55753cd6b2ad6070e89521f6c",
    "title": "Exposing Surveillance Detection Routes via Reinforcement Learning, Attack Graphs, and Cyber Terrain",
    "authors": [
      "Lanxiao Huang",
      "Tyler Cody",
      "Christopher Redino",
      "Abdul Rahman",
      "A. Kakkar",
      "Deepak Kushwaha",
      "Cheng Wang",
      "Ryan Clark",
      "Dan Radke",
      "P. Beling",
      "E. Bowen"
    ],
    "year": 2022,
    "citationCount": 10,
    "abstract": "Reinforcement learning (RL) operating on attack graphs leveraging cyber terrain principles are used to develop reward and state associated with determination of surveillance detection routes (SDR). This work extends previous efforts on developing RL methods for path analysis within enterprise networks. This work focuses on building SDR where the routes focus on exploring the network services while trying to evade risk. RL is utilized to support the development of these routes by building a reward mechanism that would help in realization of these paths. The RL algorithm is modified to have a novel warm-up phase which decides in the initial exploration which areas of the network are safe to explore based on the rewards and penalty scale factor.",
    "url": "https://www.semanticscholar.org/paper/97fe089acbe9d1f55753cd6b2ad6070e89521f6c",
    "pdf_url": "https://arxiv.org/pdf/2211.03027.pdf",
    "venue": "International Conference on Machine Learning and Applications",
    "publicationDate": "2022-11-06",
    "externalIds": {
      "ArXiv": "2211.03027",
      "DBLP": "journals/corr/abs-2211-03027",
      "DOI": "10.1109/ICMLA55696.2022.00282",
      "CorpusId": 253383895
    },
    "references": [
      {
        "paperId": "9dd33287d04e1845d91a17fecfe8f8cb7d84987b",
        "title": "A Layered Reference Model for Penetration Testing with Reinforcement Learning and Attack Graphs"
      },
      {
        "paperId": "0f10c6cf5a1a70e92908fb94049b56cef737c3a6",
        "title": "Discovering Exfiltration Paths Using Reinforcement Learning with Attack Graphs"
      },
      {
        "paperId": "686c86fd6d269757b0227a9b5693b38ed66fdf9e",
        "title": "Autonomous Penetration Testing Based on Improved Deep Q-Network"
      },
      {
        "paperId": "0089605c398bfa45b5ace3e4b4f2778f71114b75",
        "title": "Deep hierarchical reinforcement agents for automated penetration testing"
      },
      {
        "paperId": "3dc97e148ffecfcdcb47da40a1ee068041871cbd",
        "title": "Crown Jewels Analysis using Reinforcement Learning with Attack Graphs"
      },
      {
        "paperId": "8238d922796a33db56bc21b4217623d18c8e0e25",
        "title": "Using Cyber Terrain in Reinforcement Learning for Penetration Testing"
      },
      {
        "paperId": "68f0104dc958d281e29b040825b169c481ce67ce",
        "title": "Autonomous Security Analysis and Penetration Testing"
      },
      {
        "paperId": "7e0a789e7255687f4907ebe647921498219ba77a",
        "title": "Automated Penetration Testing Using Deep Reinforcement Learning"
      },
      {
        "paperId": "e6979ab4af2b69c0b9fb126705b9bd42f328ec66",
        "title": "Modeling Penetration Testing with Reinforcement Learning Using Capture-the-Flag Challenges: Trade-offs between Model-free Learning and A Priori Knowledge"
      },
      {
        "paperId": "27101c092d812461a5e2b347533807994ac5984a",
        "title": "Reinforcement Learning for Efficient Network Penetration Testing"
      },
      {
        "paperId": "c075ae9aefe19e58a852a1a0bb77d9790898c106",
        "title": "Autonomous Penetration Testing using Reinforcement Learning"
      },
      {
        "paperId": "699dddb5d1061480ad101895d6b1d757cf1df46f",
        "title": "A Reinforcement Learning Approach for Attack Graph Analysis"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "d316c82c12cf4c45f9e85211ef3d1fa62497bff8",
        "title": "High-Dimensional Continuous Control Using Generalized Advantage Estimation"
      },
      {
        "paperId": "340f48901f72278f6bf78a04ee5b01df208cc508",
        "title": "Human-level control through deep reinforcement learning"
      },
      {
        "paperId": "2319a491378867c7049b3da055c5df60e1671158",
        "title": "Playing Atari with Deep Reinforcement Learning"
      },
      {
        "paperId": "d7ba1af3d8f4b6f79fbc2aeba5648cb2d9e0d4dd",
        "title": "Intelligent route surveillance"
      },
      {
        "paperId": "82491639e5534356d16103e16d3d89b3cf04c175",
        "title": "About Penetration Testing"
      },
      {
        "paperId": "1fb916dda3cf42a5aeb91fed6ba72cdc8bdd17c9",
        "title": "Information Security: An Integrated Collection of Essays"
      },
      {
        "paperId": "48d0cc6dec42f475da2208a5f1d8b66917a279a6",
        "title": "The Proposal of Double Agent Architecture using Actor-critic Algorithm for Penetration Testing"
      },
      {
        "paperId": null,
        "title": "Mitre att&ck framework\u00ae"
      },
      {
        "paperId": "50416d832a822452b8325170d6aef4d7d5a696fd",
        "title": "Ontology-based Automation of Penetration Testing"
      },
      {
        "paperId": "169278f3f1a394ab8c1dcc48375f55d8834e10d5",
        "title": "Attack Trees"
      },
      {
        "paperId": "97efafdb4a3942ab3efba53ded7413199f79c054",
        "title": "Reinforcement Learning: An Introduction"
      }
    ],
    "cited_by": [
      {
        "paperId": "ca515c68f3e867c38ac6531328b0852e6a660140",
        "title": "From Capabilities to Performance: Evaluating Key Functional Properties of LLM Architectures in Penetration Testing"
      },
      {
        "paperId": "6884ab080a9ac50c7b06ddafaad39b2f528db589",
        "title": "Leveraging Reinforcement Learning in Red Teaming for Advanced Ransomware Attack Simulations"
      },
      {
        "paperId": "a0a632adca0dac8508ec72d658802670785fd54d",
        "title": "Lessons learned in designing a cyber reinforcement learning competition"
      },
      {
        "paperId": "8c67a79d6e85239b52c247113c0183637f341e78",
        "title": "Discovering Command and Control (C2) Channels on Tor and Public Networks Using Reinforcement Learning"
      },
      {
        "paperId": "709d05e7814eeb1b9d4f006cf5b95786c17018af",
        "title": "Enhancing Exfiltration Path Analysis Using Reinforcement Learning"
      },
      {
        "paperId": "f5e9ca39c80039a0ddfd63dd1985f67942a366ed",
        "title": "Whole Campaign Emulation with Reinforcement Learning for Cyber Test"
      },
      {
        "paperId": "ba8d2d3310fe524609cde58ba74bd2956db83441",
        "title": "ESASCF: Expertise Extraction, Generalization and Reply Framework for Optimized Automation of Network Security Compliance"
      },
      {
        "paperId": "ec57fd4015e2a211a5fc532cf7141c32647bd4ba",
        "title": "Towards operational resilience for AI-based cyber in multi-domain operations"
      },
      {
        "paperId": "751c57213461898bb583b646d33919f8099a0499",
        "title": "Deterring Adversarial Learning in Penetration Testing by Exploiting Domain Adaptation Theory"
      },
      {
        "paperId": "11c479b6ad8f75adf3f473155c8b1ab8428cc20d",
        "title": "GAT-APG: Graph Attention Network-Based Attack Path Generation for Security Simulation"
      }
    ],
    "score": 3.333333333333333
  },
  {
    "id": "6d97b81b3473492cb9986a63886cbb128496010c",
    "title": "No-regret Exploration in Contextual Reinforcement Learning",
    "authors": [
      "Aditya Modi",
      "Ambuj Tewari"
    ],
    "year": 2019,
    "citationCount": 19,
    "abstract": "",
    "url": "https://www.semanticscholar.org/paper/6d97b81b3473492cb9986a63886cbb128496010c",
    "pdf_url": null,
    "venue": "Conference on Uncertainty in Artificial Intelligence",
    "publicationDate": "2019-03-14",
    "externalIds": {
      "MAG": "3090294742",
      "DBLP": "conf/uai/0002T20",
      "CorpusId": 211259433
    },
    "references": [],
    "cited_by": [
      {
        "paperId": "0fe8f2f55046ad0b8d6337f57a78466790923264",
        "title": "Outcome-based Exploration for LLM Reasoning"
      },
      {
        "paperId": "ac11e4a0c6258986714310ccfd2329304414ac0e",
        "title": "On Bits and Bandits: Quantifying the Regret-Information Trade-off"
      },
      {
        "paperId": "5f66ae83d32157ed4b9b5d31c22a874c3a6e231e",
        "title": "Scaling Distributed Multi-task Reinforcement Learning with Experience Sharing"
      },
      {
        "paperId": "ddf61f435014b94b6ba0d4baefcac0c766e6f2a3",
        "title": "Efficient Rate Optimal Regret for Adversarial Contextual MDPs Using Online Function Approximation"
      },
      {
        "paperId": "b72602e822bbba34ffecdf5b48d2eabd8e62c129",
        "title": "Congested Bandits: Optimal Routing via Short-term Resets"
      },
      {
        "paperId": "8a37e4e6858b26b36f6f6b2d72cf4319f06492ec",
        "title": "Eluder-based Regret for Stochastic Contextual MDPs"
      },
      {
        "paperId": "3c9ee60cf7e32e08b6f177ac1d104c3ea74c3c76",
        "title": "Optimism in Face of a Context: Regret Guarantees for Stochastic Contextual MDP"
      },
      {
        "paperId": "054f555d1a414172dfbd60e8bfe71129c2ddf860",
        "title": "Provably Efficient Lifelong Reinforcement Learning with Linear Function Approximation"
      },
      {
        "paperId": "34d7e905cfd95f74c9698d5a60c7f0178e3b491a",
        "title": "Worst-case Performance of Greedy Policies in Bandits with Imperfect Context Observations"
      },
      {
        "paperId": "99b9595ac6a13bf986db40a301e3f11442eec36b",
        "title": "Block Contextual MDPs for Continual Learning"
      },
      {
        "paperId": "40ecec159ed6a797a0cae85783d7a227df760f05",
        "title": "Dynamic neighbourhood optimisation for task allocation using multi-agent"
      },
      {
        "paperId": "8a123b8941d420ee2fa5807bc3690465edfb9a52",
        "title": "TorsionNet: A Reinforcement Learning Approach to Sequential Conformer Search"
      },
      {
        "paperId": "5050392c82b2c59c0b469ff631102058e8f67693",
        "title": "PAC Bounds for Imitation and Model-based Batch Learning of Contextual Markov Decision Processes"
      },
      {
        "paperId": "525a8966c0399ec606c345213c6e2111767e67a6",
        "title": "Sample Complexity of Reinforcement Learning using Linearly Combined Model Ensembles"
      },
      {
        "paperId": "b636e03e6c8497acbe3e1cb78d1ae8e0088deef5",
        "title": "Continuous Control with Contexts, Provably"
      },
      {
        "paperId": "a4af0eb16016c5e1f36bafb7291a8a4932c0f17c",
        "title": "Reinforcement Learning in Feature Space: Matrix Bandit, Kernels, and Regret Bound"
      },
      {
        "paperId": "0e7eb6cb5ff1851d7c9de343c4722adce0a375d1",
        "title": "A Regret Bound for Greedy Partially Observed Stochastic Contextual Bandits"
      },
      {
        "paperId": "0b4a5709b1ea48e0ae2ed64f564d626f43491b69",
        "title": "Unified Algorithms for RL with Decision-Estimation Coefficients: No-Regret, PAC, and Reward-Free Learning"
      },
      {
        "paperId": "a8fec91579e8528fd37cdad20c056fff09776138",
        "title": "Counterfactual Optimism: Rate Optimal Regret for Stochastic Contextual MDPs"
      }
    ],
    "score": 3.1666666666666665
  },
  {
    "id": "807f377de905eda62e4cd2f0797153a59296adbb",
    "title": "Partial Off-policy Learning: Balance Accuracy and Diversity for Human-Oriented Image Captioning",
    "authors": [
      "Jiahe Shi",
      "Yali Li",
      "Shengjin Wang"
    ],
    "year": 2021,
    "citationCount": 12,
    "abstract": "Human-oriented image captioning with both high diversity and accuracy is a challenging task in vision+language modeling. The reinforcement learning (RL) based frameworks promote the accuracy of image captioning, yet seriously hurt the diversity. In contrast, other methods based on variational auto-encoder (VAE) or generative adversarial network (GAN) can produce diverse yet less accurate captions. In this work, we devote our attention to promote the diversity of RL-based image captioning. To be specific, we devise a partial off-policy learning scheme to balance accuracy and diversity. First, we keep the model exposed to varied candidate captions by sampling from the initial state before RL launched. Second, a novel criterion named max-CIDEr is proposed to serve as the reward for promoting diversity. We combine the above-mentioned offpolicy strategy with the on-policy one to moderate the exploration effect, further balancing the diversity and accuracy for human-like image captioning. Experiments show that our method locates the closest to human performance in the diversity-accuracy space, and achieves the highest Pearson correlation as 0.337 with human performance.",
    "url": "https://www.semanticscholar.org/paper/807f377de905eda62e4cd2f0797153a59296adbb",
    "pdf_url": "https://doi.org/10.1109/ICCV48922.2021.00219",
    "venue": "IEEE International Conference on Computer Vision",
    "publicationDate": "2021-10-01",
    "externalIds": {
      "DBLP": "conf/iccv/ShiLW21",
      "DOI": "10.1109/ICCV48922.2021.00219",
      "CorpusId": 244430079
    },
    "references": [],
    "cited_by": [
      {
        "paperId": "7dc594d03c112c2379d8b2c764d4fcf73dd444a9",
        "title": "Augmented Decoding Method Using Semantic Diverse Beam Search for Language Generation Model"
      },
      {
        "paperId": "d21d09ca270dd27baa8c34ba09662c847663a517",
        "title": "Surveying the Landscape of Image Captioning Evaluation: A Comprehensive Taxonomy, Trends and Metrics Analysis"
      },
      {
        "paperId": "0f9f85fa5621b493bbb26688fa9e1c845e7961a1",
        "title": "Enhance Training Objectives for Image Captioning with Decomposed Sequence-level Metric"
      },
      {
        "paperId": "741fd7c27cf94fa38c1983b1991b4e71a80abe59",
        "title": "Policy Learning-Based Image Captioning With Vision Transformer"
      },
      {
        "paperId": "54a86a3042591b9757c292630594b6487a2fc82c",
        "title": "Reinforcement Learning for Generative AI: A Survey"
      },
      {
        "paperId": "5c43607c7f10284003e0072b8632ef7427d3df06",
        "title": "Switching to Discriminative Image Captioning by Relieving a Bottleneck of Reinforcement Learning"
      },
      {
        "paperId": "f13e375726a3d97d56a0fae17d29e0a8a8ed94ba",
        "title": "A Mask-Guided Transformer Network with Topic Token for Remote Sensing Image Captioning"
      },
      {
        "paperId": "bd1c3c46ac592f1fd51c9fc12fd03f1e6888c77e",
        "title": "Variational Transformer: A Framework Beyond the Tradeoff Between Accuracy and Diversity for Image Captioning"
      },
      {
        "paperId": "5afc3968787ef8c43f54724b26e871a6b5a490d6",
        "title": "Surveying the Landscape of Image Captioning Evaluation: A Comprehensive Taxonomy and Novel Ensemble Method"
      },
      {
        "paperId": "297c62e63c3a534d87f1c2b401c4816f50b06d31",
        "title": "UTStyleCap4K: Generating Image Captions with Sentimental Styles"
      },
      {
        "paperId": "b915173b3b87c717d5e6dbc8e323b7f8e8a20601",
        "title": "Multi-Modal Image Captioning"
      },
      {
        "paperId": "2bc415a6985c1e3ea81684692191d1317f516ffc",
        "title": "Long-Tail Classi\ufb01cation for Distinctive Image Captioning: A Simple yet Effective Remedy for Side Effects of Reinforcement Learning"
      }
    ],
    "score": 3.0
  },
  {
    "id": "f14645d3a0740504ee632ab06f045cceaa5297bc",
    "title": "RL-GEP: Symbolic Regression via Gene Expression Programming and Reinforcement Learning",
    "authors": [
      "Hengzhe Zhang",
      "Aimin Zhou"
    ],
    "year": 2021,
    "citationCount": 12,
    "abstract": "Symbolic regression has become a hot topic in recent years due to the surging demand for interpretable machine learning methods. Traditionally, symbolic regression problems are mainly solved by genetic algorithms. Nonetheless, with the development of deep learning, reinforcement learning based symbolic regression methods have received attention gradually. Unfortunately, hardly any of those reinforcement learning based methods have been proven effectively to solve real world regression problems as genetic algorithm based methods. In this paper, we find a general reinforcement learning based symbolic regression method is difficult to solve real world problems since it is hard to balance between exploration and exploitation. To deal with this problem, we propose a hybrid method to use both genetic algorithm and reinforcement learning for solving symbolic regression problems. By doing so, we can combine the advantages of reinforcement learning and genetic algorithm and achieve better performance than using them alone. To validate the effectiveness of the proposed method, we apply the proposed method to ten benchmark datasets. The experimental results show that the proposed method achieves competitive performance compared with several well-known symbolic regression methods on those datasets.",
    "url": "https://www.semanticscholar.org/paper/f14645d3a0740504ee632ab06f045cceaa5297bc",
    "pdf_url": "https://doi.org/10.1109/IJCNN52387.2021.9533735",
    "venue": "IEEE International Joint Conference on Neural Network",
    "publicationDate": "2021-07-18",
    "externalIds": {
      "DBLP": "conf/ijcnn/ZhangZ21a",
      "DOI": "10.1109/IJCNN52387.2021.9533735",
      "CorpusId": 237598105
    },
    "references": [],
    "cited_by": [
      {
        "paperId": "11e2303f0ccde7ffb31508d77dcf92a12fb0a4c8",
        "title": "Botfip-LLM: An enhanced multimodal scientific computing framework leveraging knowledge distillation from large language models"
      },
      {
        "paperId": "11bb77d228a090ff8583788c8c0bea823c4c4ab5",
        "title": "Recent Advances in Symbolic Regression"
      },
      {
        "paperId": "8cef5342ed9ddd79b1e360fd328ad6461112d982",
        "title": "Botfip-LLM: An Enhanced Multimodal Scientific Computing Framework Leveraging Knowledge Distillation from Large Language Models"
      },
      {
        "paperId": "5382a73567fc05df4c4a10b50e6fba5ddcff30c4",
        "title": "Constraining genetic symbolic regression via semantic backpropagation"
      },
      {
        "paperId": "f0412991b7b493a8057135199cc6c99d0d9c3511",
        "title": "Accelerating evolutionary exploration through language model-based transfer learning"
      },
      {
        "paperId": "e2d910bcdc70112e2cc8bbb733fd52a49d296677",
        "title": "MORL4PDEs: Data-driven discovery of PDEs based on multi-objective optimization and reinforcement learning"
      },
      {
        "paperId": "fb26430c13b09d76642be2ca15f5312786ece535",
        "title": "Symbolic Regression for Data Storage with Side Information"
      },
      {
        "paperId": "264cfc933b8d8bd84ffb1c4db4566b925bd2684b",
        "title": "DISCOVER: Deep identification of symbolically concise open-form PDEs via enhanced reinforcement-learning"
      },
      {
        "paperId": "d1b2394b20812eb06669e98e32e7a65b8f449610",
        "title": "Evolvability degeneration in multi-objective genetic programming for symbolic regression"
      },
      {
        "paperId": "6c345679489814094537d17ec592ef0a6dbbe7de",
        "title": "A Reinforcement Learning Approach to Domain-Knowledge Inclusion Using Grammar Guided Symbolic Regression"
      },
      {
        "paperId": "9bdb06583add68a43d89a695ae78c2e53f0ab90b",
        "title": "Adaptive Case Selection for Symbolic Regression in Grammatical Evolution"
      },
      {
        "paperId": "334f50cdfcc11ee627342622ba3a6393febd2629",
        "title": "DISCOVER: Deep identification of symbolic open-form PDEs via enhanced reinforcement-learning"
      }
    ],
    "score": 3.0
  },
  {
    "id": "f715558b65fd4f3c6966505c237d9a622947010b",
    "title": "Sample Efficient Reinforcement Learning Method via High Efficient Episodic Memory",
    "authors": [
      "Dujia Yang",
      "Xiaowei Qin",
      "Xiaodong Xu",
      "Chensheng Li",
      "Guo Wei"
    ],
    "year": 2020,
    "citationCount": 15,
    "abstract": "Reinforcement Learning (RL), especially Deep Reinforcement Learning (DRL), has made great progress in many areas, such as robots, video games and driving. However, sample inefficiency is a big obstacle to the widespread practical application of DRL. Inspired by the decision making in human brain, this problem can be solved by incorporating instance based learning, i.e. episodic memory. Many episodic memory based RL algorithms have emerged recently. However, these algorithms either only replace parametric DRL algorithm with episodic control or incorporate episodic memory in a single component of DRL. In contrast to preview works, this paper proposes a new sample-efficient reinforcement learning architecture which introduces a new episodic memory module and incorporates episodic thought into some key components of DRL: exploration, experience replay and loss function. Taking Deep Q-Network (DQN) algorithm for example, when combined with DQN, our algorithm is called High Efficient Episodic Memory DQN (HE-EMDQN). In HE-EMDQN, a new non-parametric episodic memory module is introduced to help calculate the loss and modify the predicted value for exploration. For the sake of accelerating the sample learning in experience replay, an auxiliary small buffer called percentile best episode replay memory is designed to compose a mixed mini-batch. We show across the testing environments that our algorithm is significantly more powerful and sample-efficient than DQN and the recent episodic memory deep q-network (EMDQN). This work provides a new perspective for other RL algorithms to improve sample efficiency by utilising episodic memory efficiently.",
    "url": "https://www.semanticscholar.org/paper/f715558b65fd4f3c6966505c237d9a622947010b",
    "pdf_url": "https://doi.org/10.1109/ACCESS.2020.3009329",
    "venue": "IEEE Access",
    "publicationDate": "",
    "externalIds": {
      "MAG": "3042840230",
      "DBLP": "journals/access/YangQXLW20",
      "DOI": "10.1109/ACCESS.2020.3009329",
      "CorpusId": 220733803
    },
    "references": [
      {
        "paperId": "d369d15aacb9a0fce14ac5ac060489f9007aabdd",
        "title": "Path Planning via an Improved DQN-Based Learning Policy"
      },
      {
        "paperId": "ddea0e5c27ff91cda7ba50e1231aa7f7d076e58b",
        "title": "Reinforcement Learning, Fast and Slow"
      },
      {
        "paperId": "fe46b8e91a30814c3813d1a1fe3ddfbaddc5ee3c",
        "title": "Random Projection in Neural Episodic Control"
      },
      {
        "paperId": "6a9c324a86cfff5998402845e43e1bba4c42d209",
        "title": "Asynchronous Episodic Deep Deterministic Policy Gradient: Toward Continuous Control in Computationally Complex Environments"
      },
      {
        "paperId": "0278d047bbba31ddcd8813356e4ca2d1854de5a6",
        "title": "Fast deep reinforcement learning using online adjustments from the past"
      },
      {
        "paperId": "fdfeeb14bbde2ab31b18e56b92d362dcd1b14f71",
        "title": "Episodic Curiosity through Reachability"
      },
      {
        "paperId": "7a4acfe83152ad62bcba11f8963106f860af5a04",
        "title": "Sample-Efficient Deep Reinforcement Learning via Episodic Backward Update"
      },
      {
        "paperId": "06b030f6a7d409e02cedd98321af3a675c011a88",
        "title": "Episodic Memory Deep Q-Networks"
      },
      {
        "paperId": "937ef8458f08073b721b1534fe4c1a67bb078ab1",
        "title": "Integrating Episodic Memory into a Reinforcement Learning Agent using Reservoir Sampling"
      },
      {
        "paperId": "b929de5591447e84e077a349b3aeb41a29f3182e",
        "title": "Faster Deep Q-Learning Using Neural Episodic Control"
      },
      {
        "paperId": "c27db32efa8137cbf654902f8f728f338e55cd1c",
        "title": "Mastering the game of Go without human knowledge"
      },
      {
        "paperId": "0ab3f7ecbdc5a33565a234215604a6ca9d155a33",
        "title": "Rainbow: Combining Improvements in Deep Reinforcement Learning"
      },
      {
        "paperId": "9e09cff6cd4c0cfbe9675cd1f0ae67c9c75a8ac9",
        "title": "Control of a Quadrotor With Reinforcement Learning"
      },
      {
        "paperId": "8db9df2eadea654f128c1887722c677c708e8a47",
        "title": "Deep Reinforcement Learning framework for Autonomous Driving"
      },
      {
        "paperId": "37088dec26231bc5a4937054ebc862bb83a3db4d",
        "title": "Neural Episodic Control"
      },
      {
        "paperId": "0fcb2034e31a2bc2f12a2b1363d0d77baf445fdf",
        "title": "#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning"
      },
      {
        "paperId": "85d8b1b3483c7f4db999e7cf6b3e6231954c43dc",
        "title": "Learning to Play in a Day: Faster Deep Reinforcement Learning by Optimality Tightening"
      },
      {
        "paperId": "ba378579fb44007db9f02699889721dcd2b5b3a0",
        "title": "Model-Free Episodic Control"
      },
      {
        "paperId": "dc3e905bfb27d21675ee1720413e007b014b37d3",
        "title": "Safe and Efficient Off-Policy Reinforcement Learning"
      },
      {
        "paperId": "2b10281297ee001a9f3f4ea1aa9bea6b638c27df",
        "title": "OpenAI Gym"
      },
      {
        "paperId": "846aedd869a00c09b40f1f1f35673cb22bc87490",
        "title": "Mastering the game of Go with deep neural networks and tree search"
      },
      {
        "paperId": "4c05d7caa357148f0bbd61720bdd35f0bc05eb81",
        "title": "Dueling Network Architectures for Deep Reinforcement Learning"
      },
      {
        "paperId": "c6170fa90d3b2efede5a2e1660cb23e1c824f2ca",
        "title": "Prioritized Experience Replay"
      },
      {
        "paperId": "3b9732bb07dc99bde5e1f9f75251c6ea5039373e",
        "title": "Deep Reinforcement Learning with Double Q-Learning"
      },
      {
        "paperId": "340f48901f72278f6bf78a04ee5b01df208cc508",
        "title": "Human-level control through deep reinforcement learning"
      },
      {
        "paperId": "2be700f497c1d4ec4905158341a6dcab92a85b6f",
        "title": "Weighted importance sampling for off-policy learning with linear function approximation"
      },
      {
        "paperId": "65438e0ba226c1f97bd8a36333ebc3297b1a32fd",
        "title": "Reinforcement learning in robotics: A survey"
      },
      {
        "paperId": "f82e4ff4f003581330338aaae71f60316e58dd26",
        "title": "The Arcade Learning Environment: An Evaluation Platform for General Agents"
      },
      {
        "paperId": "e2a1acb572c8e9aae9b692234a9738765507a4ed",
        "title": "The hippocampal\u2013striatal axis in learning, prediction and goal-directed behavior"
      },
      {
        "paperId": "9000c3e1a9cce90ad505d044935de35a44127bd3",
        "title": "Decision Making and Reward in Frontal Cortex"
      },
      {
        "paperId": "40d650666e1bba23602297d2e6290795a9bb2048",
        "title": "Play it again: reactivation of waking experience and memory"
      },
      {
        "paperId": "46b77834cf05579c223898d0cdc52c75e7cbb536",
        "title": "Evaluations of pleasurable experiences: The peak-end rule"
      },
      {
        "paperId": "c8e337a12df57783edb75eace2b8d67270a6823c",
        "title": "Hippocampal Contributions to Control: The Third Way"
      },
      {
        "paperId": "187f3f984e6f375178f41827ab90c4e748773fa7",
        "title": "PAC model-free reinforcement learning"
      },
      {
        "paperId": "9501920cd97a6f1cde5f163dbaaa23593910db52",
        "title": "Reward-Motivated Learning: Mesolimbic Activation Precedes Memory Formation"
      },
      {
        "paperId": "2ebf18e7892e660a833152ddc6cf8f1d21a7b881",
        "title": "Why there are complementary learning systems in the hippocampus and neocortex: insights from the successes and failures of connectionist models of learning and memory."
      },
      {
        "paperId": "03b7e51c52084ac1db5118342a00b5fbcfc587aa",
        "title": "Q-learning"
      },
      {
        "paperId": "03854785c693f07eeb1cc64c15c21e6b92699e0c",
        "title": "Simple memory: a theory for archicortex."
      },
      {
        "paperId": "c4f1d14f83ae2fbafeffaa77f1a99233c58f9143",
        "title": "QoE-Oriented Rate Adaptation for DASH With Enhanced Deep Q-Learning"
      },
      {
        "paperId": "ed985d4d3078612cec4889c56b4f1695e95dbff8",
        "title": "Sample-E\ufb03cient Deep Reinforcement Learning via Episodic Backward Update"
      },
      {
        "paperId": null,
        "title": "\u2018\u2018Alphastar: Mastering the real-time strategy game starcraft II,\u2019\u2019"
      },
      {
        "paperId": null,
        "title": "Togelius, Arti\ufb01cialIntelligenceandGames ,vol.2. New York, NY, USA"
      },
      {
        "paperId": "2eae2935841029505e8bae7a3da929d90210e746",
        "title": "The Hippocampus Book"
      },
      {
        "paperId": "97efafdb4a3942ab3efba53ded7413199f79c054",
        "title": "Reinforcement Learning: An Introduction"
      },
      {
        "paperId": "1d0635cda34b8af995313848a0c42bac6efe79ec",
        "title": "Extensions of Lipschitz mappings into Hilbert space"
      },
      {
        "paperId": null,
        "title": "received the B.S. and Ph.D degrees"
      }
    ],
    "cited_by": [
      {
        "paperId": "02d148c52e49ca75c16701729650b74914a04afb",
        "title": "Self-Balancing, Memory Efficient, Dynamic Metric Space Data Maintenance, for Rapid Multi-Kernel Estimation"
      },
      {
        "paperId": "191342b8a0d626d1b723311c82f56b763ef1f707",
        "title": "Implementing an intelligent diagnosis and treatment system for in-hospital cardiac arrest in the Utstein style: a multi-center case study"
      },
      {
        "paperId": "042deff1b1011d3734ac6f1bef35c55e2f22e440",
        "title": "A Survey of Reinforcement Learning for Optimization in Automation"
      },
      {
        "paperId": "ba5f7ad7b6ab59056b259261b54698a5196e7fe7",
        "title": "Concepts at the Interface"
      },
      {
        "paperId": "8eede5336cbd98fdbd345961122c83838f1645d5",
        "title": "A novel robotic grasping method for moving objects based on multi-agent deep reinforcement learning"
      },
      {
        "paperId": "2d2d9c577d398ab3446085318a31c9ee310eaddf",
        "title": "UAV Control Method Combining Reptile Meta-Reinforcement Learning and Generative Adversarial Imitation Learning"
      },
      {
        "paperId": "2868ed43f7e9a192cc7a5a23a9ae343341727dbf",
        "title": "QoE-Aware Adaptive Bitrate Algorithm Based on Subepisodic Deep Reinforcement Learning for DASH"
      },
      {
        "paperId": "31181bf9515ec395f39539bc52f6386389d1934c",
        "title": "Moving beyond content\u2010specific computation in artificial neural networks"
      },
      {
        "paperId": "15d79a5e930fa88114bae59ffa106de5410965de",
        "title": "Sample-efficient deep reinforcement learning with directed associative graph"
      },
      {
        "paperId": "1f6e33fe5bbd6d9e50a17d6d4df31345c764614e",
        "title": "Review on Reinforcement Learning, Research Evolution and Scope of Application"
      },
      {
        "paperId": "b406d3ee1b4f685418612325118e791ac5fa32c4",
        "title": "Sample Efficiency in Sparse Reinforcement Learning: Or Your Money Back"
      },
      {
        "paperId": "2d23c8224262b10106351dcf1999aa373f71a229",
        "title": "Experience Replay Optimization via ESMM for Stable Deep Reinforcement Learning"
      },
      {
        "paperId": "e81f6a3f8fbab6bdf726bb684706f0aab9bb03ee",
        "title": "Contrastive Learning Methods for Deep Reinforcement Learning"
      },
      {
        "paperId": "cea4280342039e0a3b2c9084fb9c9cc20b97d803",
        "title": "D EEP R EINFORCEMENT L EARNING BASED I NSIGHT S ELECTION P OLICY"
      },
      {
        "paperId": "895b3e34dfcd70c2b38f35d93427a240ea941890",
        "title": "Multifunctional Radar Cognitive Jamming Decision Based on Dueling Double Deep Q-Network"
      }
    ],
    "score": 3.0
  },
  {
    "id": "e0d4af45fa3073dbfa9b6e464fa88d52e6f06928",
    "title": "Energy-Efficient Slithering Gait Exploration for a Snake-like Robot based on Reinforcement Learning",
    "authors": [
      "Zhenshan Bing",
      "Christian Lemke",
      "Zhuangyi Jiang",
      "Kai Huang",
      "A. Knoll"
    ],
    "year": 2019,
    "citationCount": 17,
    "abstract": "Similar to their counterparts in nature, the flexible bodies of snake-like robots enhance their movement capability and adaptability in diverse environments. However, this flexibility corresponds to a complex control task involving highly redundant degrees of freedom, where traditional model-based methods usually fail to propel the robots energy-efficiently. In this work, we present a novel approach for designing an energy-efficient slithering gait for a snake-like robot using a model-free reinforcement learning (RL) algorithm. Specifically, we present an RL-based controller for generating locomotion gaits at a wide range of velocities, which is trained using the proximal policy optimization (PPO) algorithm. Meanwhile, a traditional parameterized gait controller is presented and the parameter sets are optimized using the grid search and Bayesian optimization algorithms for the purposes of reasonable comparisons. Based on the analysis of the simulation results, we demonstrate that this RL-based controller exhibits very natural and adaptive movements, which are also substantially more energy-efficient than the gaits generated by the parameterized controller. Videos are shown at\u00a0https://videoviewsite.wixsite.com/rlsnake\u00a0.",
    "url": "https://www.semanticscholar.org/paper/e0d4af45fa3073dbfa9b6e464fa88d52e6f06928",
    "pdf_url": "https://arxiv.org/pdf/1904.07788.pdf",
    "venue": "International Joint Conference on Artificial Intelligence",
    "publicationDate": "2019-04-16",
    "externalIds": {
      "MAG": "2964056357",
      "ArXiv": "1904.07788",
      "DBLP": "journals/corr/abs-1904-07788",
      "DOI": "10.24963/ijcai.2019/785",
      "CorpusId": 119311604
    },
    "references": [
      {
        "paperId": "a2c5a02559fe47aa3508ed6df8e739b63f3a20bf",
        "title": "Learning symmetric and low-energy locomotion"
      },
      {
        "paperId": "bca91c27d811588409a6ce78853c593f64215bda",
        "title": "Towards Optimally Decentralized Multi-Robot Collision Avoidance via Deep Reinforcement Learning"
      },
      {
        "paperId": "e010ba3ff5744604cdbfe44a733e2a98649ee907",
        "title": "Learning Complex Dexterous Manipulation with Deep Reinforcement Learning and Demonstrations"
      },
      {
        "paperId": "4b6bf87aeb5d05aa0bcdda6d7bc1dc7bec93f233",
        "title": "Towards autonomous locomotion: Slithering gait design of a snake-like robot for target observation and tracking"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "9917363277c783a01bff32af1c27fc9b373ad55d",
        "title": "DeepLoco"
      },
      {
        "paperId": "6b6d326e9d2a3154d50608c77031e105963ee5fd",
        "title": "Simplifying Gait Design via Shape Basis Optimization"
      },
      {
        "paperId": "e1c6aeffc3259fe02041906f41563f65c5509c6b",
        "title": "Analysis"
      },
      {
        "paperId": "a7884010158c18e6510b43da9883cfa90b2d6aec",
        "title": "Advanced Robotics"
      },
      {
        "paperId": "0ab09b02811384b525db0c5e0ec073764bdcb113",
        "title": "Experimental determination of control parameter intervals for repeatable gaits in modular snake robots"
      },
      {
        "paperId": "1a632fb89b6b05dc16fbc026d86e390e22ca6ac3",
        "title": "Robots that can adapt like animals"
      },
      {
        "paperId": "e5fafb2f57944f9d3f5e1bab0f6902b6e596a455",
        "title": "Bayesian Gait Optimization for Bipedal Locomotion"
      },
      {
        "paperId": "70850393a5efea8b407f2ba5cdf4eabe88827143",
        "title": "Volume 4"
      },
      {
        "paperId": "65438e0ba226c1f97bd8a36333ebc3297b1a32fd",
        "title": "Reinforcement learning in robotics: A survey"
      },
      {
        "paperId": "b354ee518bfc1ac0d8ac447eece9edb69e92eae1",
        "title": "MuJoCo: A physics engine for model-based control"
      },
      {
        "paperId": "25ab4c0179cd14725326add1ef7c39b2ed41e2c3",
        "title": "Snake Robots: Modelling, Mechatronics, and Control"
      },
      {
        "paperId": "be2beec6d83a41724a68bd9ff2ae2d859d31d027",
        "title": "Using response surfaces and expected improvement to optimize snake robot gait parameters"
      },
      {
        "paperId": "8fe92a109691f91f36023bda4ab32b5585108de8",
        "title": "The mechanics of slithering locomotion"
      },
      {
        "paperId": "0449afd77c51bc63973be7c37a412ca91ac66ad1",
        "title": "A Mobile Robot Platform with Double Angle-Changeable Tracks"
      },
      {
        "paperId": "474d0bb38298ebd659d7b8c6b3de86dd96485707",
        "title": "Online Optimization of Swimming and Crawling in an Amphibious Snake Robot"
      },
      {
        "paperId": "d89e23d3267c75db75e2055951facc2d74f2908b",
        "title": "Automatic Gait Optimization with Gaussian Process Regression"
      },
      {
        "paperId": "bafd636f4511e463bffbbaa144bcfe104f4f959e",
        "title": "An evolutionary approach to gait learning for four-legged robots"
      },
      {
        "paperId": "c280e0d60ce7eb7d47b4bfe24af582df70f29b38",
        "title": "Machine Learning for Fast Quadrupedal Locomotion"
      },
      {
        "paperId": "366238daca69fa82c035aa0fb7d06925e43c3665",
        "title": "Analysis of snake movement forms for realization of snake-like robots"
      },
      {
        "paperId": "927f51e785811093ff0f2f86b623edf19ac27e96",
        "title": "On autonomous robots"
      },
      {
        "paperId": "55f3af4d93b1677c206a7e330b0339d77ca08919",
        "title": "Book review: The Engineering of Knowledge-based Systems Theory and Practice by Avelino J. Gonzales and Douglas D. Dankel (Prentice Hall, 1993)"
      },
      {
        "paperId": "0824e5e1ba1646e058c7f61019ac242a98bf007d",
        "title": "The energetic cost of moving about."
      },
      {
        "paperId": "edeff2cfd3f850e76e82dd4aa450607ffee4b432",
        "title": "Proceedings of the National Academy of Sciences"
      },
      {
        "paperId": "ab3173f09db77e55ad785883fd75403c60057ec1",
        "title": "Learning to exploit passive compliance for energy-efficient gait generation on a compliant humanoid"
      },
      {
        "paperId": null,
        "title": "ACM Transactions on Graphics (TOG)"
      },
      {
        "paperId": null,
        "title": "Nature"
      },
      {
        "paperId": null,
        "title": "In 2014 IEEE International Symposium on Safety"
      },
      {
        "paperId": "455e6d0c09b87fc7d2b6e9ee7c99d69e779cac8c",
        "title": "Automatic Gait Optimisation for Quadruped Robots"
      },
      {
        "paperId": null,
        "title": "pages 1\u20133"
      },
      {
        "paperId": "1c78c3c904c40d1ce046e8523bf30d2274cfdee5",
        "title": "IEEE/RSJ International Conference on Intelligent Robots and Systems"
      },
      {
        "paperId": null,
        "title": "Modeling , analysis , and synthesis of serpentine locomotion with a multilink robotic snake"
      },
      {
        "paperId": null,
        "title": "In Proceedings 1999 IEEE International Conference on Robotics and Automation (Cat"
      },
      {
        "paperId": "de84de57c1f0201eae8f914318723e12a150e15e",
        "title": "Volume 7"
      },
      {
        "paperId": "b8bc3c588ed8489bc5d652deeca7f53f2df25bcb",
        "title": "Power Sources for Small Robots"
      },
      {
        "paperId": "5a12ec2e57040da35ff4f7a3abf702883a730256",
        "title": "Limbless locomotion: learning to crawl with a snake robot"
      },
      {
        "paperId": "89aedea530414eb98c3c9db22de74c3175bd080c",
        "title": "Biologically Inspired Robots: Snake-Like Locomotors and Manipulators"
      }
    ],
    "cited_by": [
      {
        "paperId": "c16dc92d75cebfd6da610b8e59e28c182d0d598d",
        "title": "Footstep reward for energy-efficient quadruped gait generation and transition through deep reinforcement learning"
      },
      {
        "paperId": "4a16faa62968e717100d2cdc5cacf6ddac7b3782",
        "title": "Discontinuous Rod Spanning Motion Control for Snake Robot based on Reinforcement Learning"
      },
      {
        "paperId": "da66164d2fe130ffb5b43a5f701dd4b6197a8235",
        "title": "Efficient Adaptive Matching for Real-Time City Express Delivery"
      },
      {
        "paperId": "a09ac2a213f495474a6ac9fb024f3501d35e74fb",
        "title": "Simulation to Real: Learning Energy-Efficient Slithering Gaits for a Snake-Like Robot"
      },
      {
        "paperId": "499513a54b8bb4a6f76ee22e71b135fe723fba1c",
        "title": "Adaptive Control for a 3D Snake-Like Robot Based on Mutual Supervised Reinforcement Learning"
      },
      {
        "paperId": "d0ff1b95e33c5d2ba0880608114bb0ced03a1275",
        "title": "Creating Better Collision-Free Trajectory for Robot Motion Planning by Linearly Constrained Quadratic Programming"
      },
      {
        "paperId": "fd379cd81f86251452c0b91da173e7495a6225a2",
        "title": "Synergy Emergence in Deep Reinforcement Learning for Full-Dimensional Arm Manipulation"
      },
      {
        "paperId": "ee9e210a8003c475ed85be94bb463d67f81ba0c6",
        "title": "Motion Planning for a Snake Robot using Double Deep Q-Learning"
      },
      {
        "paperId": "856e561beb9e9193f74dd9edd0e6157629793d0c",
        "title": "Perception-Action Coupling Target Tracking Control for a Snake Robot via Reinforcement Learning"
      },
      {
        "paperId": "753689c04980cce226092bcca7fc5660aabfb6d2",
        "title": "Optimally Efficient Locomotion of Snake Robot"
      },
      {
        "paperId": "7e25680ceef7e2c0709711fdff042502975b8712",
        "title": "Energy-efficient and damage-recovery slithering gait design for a snake-like robot based on reinforcement learning and inverse reinforcement learning"
      },
      {
        "paperId": "87082e490d7dca2ab3f7cd981306f0f65ae18baf",
        "title": "Motor Synergy Development in High-Performing Deep Reinforcement Learning Algorithms"
      },
      {
        "paperId": "209f0c9576181a9178b13e42f78ad0a902abf44e",
        "title": "Automatic Snake Gait Generation Using Model Predictive Control"
      },
      {
        "paperId": "918ca8dedcc653fed7a8462349151ce5afb2a7af",
        "title": "A fast non-dominated sorting multi-objective symbiotic organism search algorithm for energy efficient locomotion of snake robot"
      },
      {
        "paperId": "8cd71783cf728f02a11fda73962723941873a64e",
        "title": "Energy-Efficient Gait Optimization of Snake-Like Modular Robots by Using Multiobjective Reinforcement Learning and a Fuzzy Inference System"
      },
      {
        "paperId": "995b1bc6c5237833a3fb1972f36ef0f47acc66ad",
        "title": "Sim-to-Real: Learning Energy-Efficient Slithering Gaits for a Snake-like Robot"
      },
      {
        "paperId": "dc01cfaa8a42db58c0f98900b99362819693467f",
        "title": "Deep Reinforcement Learning for Snake Robot Locomotion"
      }
    ],
    "score": 2.833333333333333
  },
  {
    "id": "f80432fa03c91283cadbf6c262a7bc6e45f4edd3",
    "title": "Discretionary Lane Change Decision Making using Reinforcement Learning with Model-Based Exploration",
    "authors": [
      "Songan Zhang",
      "H. Peng",
      "S. Nageshrao",
      "H. E. Tseng"
    ],
    "year": 2019,
    "citationCount": 17,
    "abstract": "Deep reinforcement learning (DRL) techniques have been used to solve a discretionary lane change decision-making problem and are showing promising results. However, since the input information for the discretionary lane change problem is continuous and can be in high dimension, it is an open challenge for DRL to optimize the exploration-exploitation trade-off. Conventional model-less exploration methods lack a systematic way to incorporate additional engineering or model-based knowledge of our application into consideration and as a result, the training can be inefficient and may dwell on a policy, e.g. lane change strategy that is impractical. In previous related work, many used the rule-based safety check policy to guide the exploration and collect input information data. However, it is not guaranteed to get the optimal policy and the performance is dependent on the safety check policy selected. In this paper, we developed an explicit statistical aggregated environment model using a conditional variational auto-encoder and a model-based exploration strategy leveraging it. The agent is guided to explore with surprise-based intrinsic reward derived from the environment model. The result is compared with annealing epsilon-greedy exploration and with rule-based safety check exploration. We demonstrate that the performance of the developed model-based exploration method is comparable with the best rule-based safety check exploration and much better than the epsilon-greedy exploration.",
    "url": "https://www.semanticscholar.org/paper/f80432fa03c91283cadbf6c262a7bc6e45f4edd3",
    "pdf_url": "https://doi.org/10.1109/ICMLA.2019.00147",
    "venue": "International Conference on Machine Learning and Applications",
    "publicationDate": "2019-12-01",
    "externalIds": {
      "DBLP": "conf/icmla/ZhangPNT19",
      "MAG": "3007923353",
      "DOI": "10.1109/ICMLA.2019.00147",
      "CorpusId": 211226987
    },
    "references": [],
    "cited_by": [
      {
        "paperId": "05e4475331f5970f61300f8828ae276d1c9f7e71",
        "title": "Mixture of Experts Framework Based on Soft Actor-Critic Algorithm for Highway Decision-Making of Connected and Automated Vehicles"
      },
      {
        "paperId": "e6eda7a7e9f3089c39ac88c5a31d8642e4892450",
        "title": "Enhancing Highway Driving: High Automated Vehicle Decision Making in a Complex Multi-Body Simulation Environment"
      },
      {
        "paperId": "63097cc4ef30960e897a0d2399649b3e9ad6c3f3",
        "title": "SECRM-2D: RL-Based Efficient and Comfortable Route-Following Autonomous Driving with Analytic Safety Guarantees"
      },
      {
        "paperId": "31e842f5ea400beadf60531ed0585db50febb48c",
        "title": "An Integrated Model for Autonomous Speed and Lane Change Decision-Making Based on Deep Reinforcement Learning"
      },
      {
        "paperId": "a4f36897e6f4124540cb572295a67467db4a6642",
        "title": "A Hybrid Partitioning Strategy for Backward Reachability of Neural Feedback Loops"
      },
      {
        "paperId": "9f666b51b879a65a8995283e5065a7e11df587fd",
        "title": "Deep Reinforcement Learning Approach for Automated Vehicle Mandatory Lane Changing"
      },
      {
        "paperId": "5429e5255e1586d0bac62288a34ce2523c46adf4",
        "title": "Robust AI Driving Strategy for Autonomous Vehicles"
      },
      {
        "paperId": "449c4f166f2856029687c6e9ba2e8b7de119a33c",
        "title": "Quick Learner Automated Vehicle Adapting its Roadmanship to Varying Traffic Cultures with Meta Reinforcement Learning"
      },
      {
        "paperId": "a7f80674501dc4f9a54bc6a434bf3a7001a797b6",
        "title": "Mobile Reconfigurable Intelligent Surfaces for NOMA Networks: Federated Learning Approaches"
      },
      {
        "paperId": "3201f03dae335c4732ea3c2f9c4a4e68362f30a5",
        "title": "Decision-Making for Complex Scenario using Safe Reinforcement Learning"
      },
      {
        "paperId": "27211f1e5ba90cc936a39c9d23fafff133cd6dd7",
        "title": "Deep Reinforcement Learning and Transportation Research: A Comprehensive Review"
      },
      {
        "paperId": "c142290fccffff0e876fa86c0b14a967bb2ce286",
        "title": "Decision-making for Autonomous Vehicles on Highway: Deep Reinforcement Learning with Continuous Action Horizon"
      },
      {
        "paperId": "5caa713fa87766d8db7813460067d143c786e2d3",
        "title": "Generating Socially Acceptable Perturbations for Efficient Evaluation of Autonomous Vehicles"
      },
      {
        "paperId": "c501fcd7079c6c998ac83126f251e2d7ef506a8e",
        "title": "Safe Reinforcement Learning for Autonomous Vehicles through Parallel Constrained Policy Optimization*"
      },
      {
        "paperId": "658eae5aaecbbde9e56e8561bf0781ebf2d8e989",
        "title": "2019 18th IEEE International Conference On Machine Learning And Applications (ICMLA)"
      },
      {
        "paperId": "f91f117bf765539200e33ce2fbb300b837cb571f",
        "title": "A Human-Like Free-Lane-Change Trajectory Planning and Control Method With Data-Based Behavior Decision"
      },
      {
        "paperId": "98623bd175656d92ec0454722cf64609604861fb",
        "title": "Emergency Vehicle Aware Lane Change Decision Model for Autonomous Vehicles Using Deep Reinforcement Learning"
      }
    ],
    "score": 2.833333333333333
  },
  {
    "id": "117cd9e1dafc577e53e2d46897a784ed1e65996f",
    "title": "Adaptive Exploration Strategy With Multi-Attribute Decision-Making for Reinforcement Learning",
    "authors": [
      "Chunyang Hu",
      "Meng Xu"
    ],
    "year": 2020,
    "citationCount": 14,
    "abstract": "Reinforcement Learning (RL) agents often encounter the bottleneck of the performance when the dilemma of exploration and exploitation arises. In this study, an adaptive exploration strategy with multi-attribute decision-making is proposed to address the trade-off problem between exploration and exploitation. Firstly, the proposed method decomposes a complex task into several sub-tasks and trains each sub-task using the same training method individually. Then, the proposed method uses a multi-attribute decision-making method to develop an action policy integrating the training results of these trained sub-tasks. There are practical advantages to improve learning performance by allowing multiple learners to learn in parallel. An adaptive exploration strategy determines the probability of exploration depending on the information entropy instead of the suffocating work of empirical tuning. Finally, transfer learning extends the applicability of the proposed method. The experiment of the robot migration, the robot confrontation, and the real wheeled mobile robot are used to demonstrate the availability and practicability of the proposed method.",
    "url": "https://www.semanticscholar.org/paper/117cd9e1dafc577e53e2d46897a784ed1e65996f",
    "pdf_url": "https://doi.org/10.1109/ACCESS.2020.2973169",
    "venue": "IEEE Access",
    "publicationDate": "",
    "externalIds": {
      "MAG": "3006486026",
      "DBLP": "journals/access/HuX20",
      "DOI": "10.1109/ACCESS.2020.2973169",
      "CorpusId": 211297828
    },
    "references": [
      {
        "paperId": "8e97d2a54b08ec79ce1a2d9eba820d01698ef4d3",
        "title": "A Fuzzy Adaptive Approach to Decoupled Visual Servoing for a Wheeled Mobile Robot"
      },
      {
        "paperId": "4b80d7b4782c9c3d7de1a7291ce21f424b912b60",
        "title": "Domain Adaptation With Neural Embedding Matching"
      },
      {
        "paperId": "0605b11d96a660a9982525b64e5aa943ced4bcb4",
        "title": "Structured optimal graph based sparse feature extraction for semi-supervised learning"
      },
      {
        "paperId": "85413aed24789f5aa69a8667aecc14fc9bb90354",
        "title": "Incipient winding fault detection and diagnosis for squirrel-cage induction motors equipped on CRH trains."
      },
      {
        "paperId": "5422da4ff5e84514801435591b04ea12c10979c5",
        "title": "Behavior fusion for deep reinforcement learning."
      },
      {
        "paperId": "3b3b76043d26b44e070315d530b7e5b87629dda8",
        "title": "Discriminative low-rank preserving projection for dimensionality reduction"
      },
      {
        "paperId": "c714ca7809fa14b2e87458a2520cdc2588acc11e",
        "title": "A Descriptor System Approach for Estimation of Incipient Faults With Application to High-Speed Railway Traction Devices"
      },
      {
        "paperId": "5e37bd22e0ece158ee090439a7e87712b82d8c18",
        "title": "Adaptive Image-Based Visual Servoing With Temporary Loss of the Visual Signal"
      },
      {
        "paperId": "a50284e90349d90a0ff9cfb45e8bada17e1faee6",
        "title": "Event-Triggered Distributed Control of Nonlinear Interconnected Systems Using Online Reinforcement Learning With Exploration"
      },
      {
        "paperId": "26b90ccf7541fd2cd4235118493e1e49d358c351",
        "title": "StarCraft Micromanagement With Reinforcement Learning and Curriculum Transfer Learning"
      },
      {
        "paperId": "e9e446b2abc2109ac4e9e8b5231269fd02f45c7b",
        "title": "Manifold Regularized Reinforcement Learning"
      },
      {
        "paperId": "705c5c0e1a556b135d23d1f3f8d1d76a51e8e7ca",
        "title": "Safe Exploration Algorithms for Reinforcement Learning Controllers"
      },
      {
        "paperId": "43c0f5ebd97225369e88418b4e957c3d592c0389",
        "title": "Self-Paced Prioritized Curriculum Learning With Coverage Penalty in Deep Reinforcement Learning"
      },
      {
        "paperId": "0705fddf9f22d12c3240c705cc95df89c1dd6827",
        "title": "Hybrid Whale Optimization Algorithm with simulated annealing for feature selection"
      },
      {
        "paperId": "ad75df368c6baf45d279b4fb868476b06b79ba64",
        "title": "A nonlinear-programming methodology for multi-attribute decision-making problem with interval-valued intuitionistic fuzzy soft sets information"
      },
      {
        "paperId": "20372f8be6099e1a869fd7f23dead6ad771027f8",
        "title": "Extending the Peak Bandwidth of Parameters for Softmax Selection in Reinforcement Learning"
      },
      {
        "paperId": "142497432fe179ddb6ffe600c64a837ec6179550",
        "title": "Parameter Space Noise for Exploration"
      },
      {
        "paperId": "225ab689f41cef1dc18237ef5dab059a49950abf",
        "title": "Curiosity-Driven Exploration by Self-Supervised Prediction"
      },
      {
        "paperId": "f848c7aeacee204c319236506a02a2dec7181ee0",
        "title": "Predicting links based on knowledge dissemination in complex network"
      },
      {
        "paperId": "cd63b72f2d25f9c1c2b0b30ba7ee94863375ff1a",
        "title": "Pheromone-Based Planning Strategies in Dyna-Q Learning"
      },
      {
        "paperId": "7dfa698a6a1490bbe5d447ef22cb2452b4f1d303",
        "title": "Multiattribute decision making based on interval-valued intuitionistic fuzzy values and linear programming methodology"
      },
      {
        "paperId": "6e4ef32ce5e7b1bb5d4dc22765264f83bd2e4b6f",
        "title": "Balancing exploration and exploitation in reinforcement learning using a value of information criterion"
      },
      {
        "paperId": "8324d39c6ba78fdfe319055656448020b1881789",
        "title": "KELEA (Kinetic Energy Limiting Electrostatic Attraction) Offers an Alternative Explanation to Existing Concepts Regarding Wave-Particle Duality, Cold Fusion and Superconductivity"
      },
      {
        "paperId": "1b3ad72511eb21aeaa567f10d77d9c72b1901bf3",
        "title": "RL-IAC: An exploration policy for online saliency learning on an autonomous mobile robot"
      },
      {
        "paperId": "775ccbdd4b2b2b19f0166e898f4f42cde75aa888",
        "title": "Convergence Analysis of Two Loss Functions in Soft-Max Regression"
      },
      {
        "paperId": "4b63e34276aa98d5345efa7fe09bb06d8a9d8f52",
        "title": "Deep Exploration via Bootstrapped DQN"
      },
      {
        "paperId": "69e76e16740ed69f4dc55361a3d319ac2f1293dd",
        "title": "Asynchronous Methods for Deep Reinforcement Learning"
      },
      {
        "paperId": "527f73608488578e1313933330ad1d1dd6976fd3",
        "title": "Active transfer learning of matching query results across multiple sources"
      },
      {
        "paperId": "2c9908f2cf68dbb1f585d388c4a97131e7b2ce9e",
        "title": "Evolving Robocode tanks for Evo Robocode"
      },
      {
        "paperId": "2f2dcb63b094057032b85708a7647fecde2f983d",
        "title": "A Simple Scheme for Formation Control Based on Weighted Behavior Learning"
      },
      {
        "paperId": "587512a58e59d93229adc340de821c61244159e1",
        "title": "Analysis and Classification of Sleep Stages Based on Difference Visibility Graphs From a Single-Channel EEG Signal"
      },
      {
        "paperId": "7117b948dda2081c7dcd8f8a7850a8f1de7aaa8c",
        "title": "Fusion of Multiple Behaviors Using Layered Reinforcement Learning"
      },
      {
        "paperId": "26df92db349ef638eda0707647ee18b1f363249f",
        "title": "Unified Behavior Framework for Reactive Robot Control"
      },
      {
        "paperId": "73fece24a04e7c0b6cea80a77d192bd048d6e70c",
        "title": "A phased reinforcement learning algorithm for complex control problems"
      },
      {
        "paperId": "e0f0471bcab5f05948c6f53fbe3770b311fd97f5",
        "title": "The ordered weighted geometric averaging operators"
      },
      {
        "paperId": "1e1d35136b1bf3b13ef6b53f6039f39d9ee820e3",
        "title": "Finite-time Analysis of the Multiarmed Bandit Problem"
      },
      {
        "paperId": "aa27cb09e68992aa0b8015482648f9e281d58ba4",
        "title": "Acquisition of stand-up behavior by a real robot using hierarchical reinforcement learning"
      },
      {
        "paperId": "0699d009dfdf63c880342fd98163002d00eb2c2b",
        "title": "Induced ordered weighted averaging operators"
      },
      {
        "paperId": "28be6c2ed074a7fa63818a1730b04219d8a01c02",
        "title": "The convergence of TD(\u03bb) for general \u03bb"
      },
      {
        "paperId": "3538d9e4fbba523041899a21d4535d016375645c",
        "title": "Decoupled Visual Servoing With Fuzzy Q-Learning"
      },
      {
        "paperId": "9e3136842848c5b5cae948e360ba38b5e5d539a8",
        "title": "Transfer of Reinforcement Learning:The State of the Art"
      },
      {
        "paperId": "992af1e10fdaed19a115d460f2b71ad91c68c8cd",
        "title": "Information Entropy"
      },
      {
        "paperId": "805256745b33eeab46fdd0344d647e55ffecc436",
        "title": "Technical Note: Q-Learning"
      },
      {
        "paperId": "97efafdb4a3942ab3efba53ded7413199f79c054",
        "title": "Reinforcement Learning: An Introduction"
      },
      {
        "paperId": null,
        "title": "Adaptive Exploration Strategy With MADM for RL"
      }
    ],
    "cited_by": [
      {
        "paperId": "ed8ea3d06c173849f02ee8afcf8db07df0f31261",
        "title": "Optimal Energy Management of Plug-In Hybrid Electric Vehicles Through Ensemble Reinforcement Learning With Exploration-to-Exploitation Ratio Control"
      },
      {
        "paperId": "fd7f4547fab2e316dd6dd2f8adffbe97c258a990",
        "title": "Deep reinforcement learning for the rapid on-demand design of mechanical metamaterials with targeted nonlinear deformation responses"
      },
      {
        "paperId": "e5ed5cc77809752deaaea6bcadcc5f7dd3407788",
        "title": "Optimal Energy Management of Plug-in Hybrid Vehicles Through Exploration-to-Exploitation Ratio Control in Ensemble Reinforcement Learning"
      },
      {
        "paperId": "316aa30119e9c8d83f14886e7c1dbd1012f6efc7",
        "title": "Visual servoing with deep reinforcement learning for rotor unmanned helicopter"
      },
      {
        "paperId": "d9b5d26b2068e967598cd636df88e0e75779ed95",
        "title": "\"Deep Reinforcement Learning for Engineering Design Through Topology Optimization of Elementally Discretized Design Domains\""
      },
      {
        "paperId": "8d9f23092fe61b0cdcc1251d29d01fa4bea364b2",
        "title": "Multi-Agent Reinforcement Learning via Adaptive Kalman Temporal Difference and Successor Representation"
      },
      {
        "paperId": "9c421b0d5d04ddd1750006e3230c4b882775bc53",
        "title": "Distributed Hybrid Kalman Temporal Differences for Reinforcement Learning"
      },
      {
        "paperId": "efef29668ae2194290ebbbe6a4d186943ece813a",
        "title": "MM-KTD: Multiple Model Kalman Temporal Differences for Reinforcement Learning"
      },
      {
        "paperId": "0e80650e3275dc87494bd143c51e8a231ac03712",
        "title": "A Hierarchical Decision-Making Method with a Fuzzy Ant Colony Algorithm for Mission Planning of Multiple UAVs"
      },
      {
        "paperId": "0256975b4448b8945fc44ff71b22ddc3ca95731d",
        "title": "A Confrontation Decision-Making Method with Deep Reinforcement Learning and Knowledge Transfer for Multi-Agent System"
      },
      {
        "paperId": "5d7e905be41b883687abcfb67528f10b76785512",
        "title": "A Situation Assessment Method with an Improved Fuzzy Deep Neural Network for Multiple UAVs"
      },
      {
        "paperId": "4f2274bd30934924e66fef999152c61724bbcc55",
        "title": "Comparative Analysis of A3C and PPO Algorithms in Reinforcement Learning: A Survey on General Environments"
      },
      {
        "paperId": "0345396a1fe79830c45e6b634278c892bf5617d2",
        "title": "An Experience Aggregative Reinforcement Learning With Multi-Attribute Decision-Making for Obstacle Avoidance of Wheeled Mobile Robot"
      },
      {
        "paperId": "c2e2cd439d293f077fc196315b150365f4d8e4f4",
        "title": "A Fuzzy Ensemble Method With Deep Learning for Multi-Robot System"
      }
    ],
    "score": 2.8000000000000003
  },
  {
    "id": "46cbd8acfacca5fc74b8d05bb06264aff0d82a4e",
    "title": "Graph-based Cluttered Scene Generation and Interactive Exploration using Deep Reinforcement Learning",
    "authors": [
      "K. N. Kumar",
      "Irfan Essa",
      "Sehoon Ha"
    ],
    "year": 2021,
    "citationCount": 11,
    "abstract": "We introduce a novel method to teach a robotic agent to interactively explore cluttered yet structured scenes, such as kitchen pantries and grocery shelves, by leveraging the physical plausibility of the scene. We propose a novel learning framework to train an effective scene exploration policy to discover hidden objects with minimal interactions. First, we define a novel scene grammar to represent structured clutter. Then we train a Graph Neural Network (GNN) based Scene Generation agent using deep reinforcement learning (deep RL), to manipulate this Scene Grammar to create a diverse set of stable scenes, each containing multiple hidden objects. Given such cluttered scenes, we then train a Scene Exploration agent, using deep RL, to uncover hidden objects by interactively rearranging the scene. We show that our learned agents hide and discover significantly more objects than the baselines. We present quantitative results that prove the generalization capabilities of our agents. We also demonstrate sim-to-real transfer by successfully deploying the learned policy on a real UR10 robot to explore real-world cluttered scenes. The supplemental video can be found at: https://www.youtube.com/watch?v=T2Jo7wwaXss.",
    "url": "https://www.semanticscholar.org/paper/46cbd8acfacca5fc74b8d05bb06264aff0d82a4e",
    "pdf_url": "https://arxiv.org/pdf/2109.10460.pdf",
    "venue": "IEEE International Conference on Robotics and Automation",
    "publicationDate": "2021-09-21",
    "externalIds": {
      "ArXiv": "2109.10460",
      "DBLP": "journals/corr/abs-2109-10460",
      "DOI": "10.1109/icra46639.2022.9811874",
      "CorpusId": 237263367
    },
    "references": [
      {
        "paperId": "c0e18efa236c4c0696a98505d57d3bd8ccffcdad",
        "title": "GPU-based simulation of cloth wrinkles at submillimeter levels"
      },
      {
        "paperId": "cb94486716c59aca2a9aca3e9afed4261cc42530",
        "title": "iGibson, a Simulation Environment for Interactive Tasks in Large Realistic Scenes"
      },
      {
        "paperId": "c35f3f4f344c4f056e89cf0ab84ba96595f62986",
        "title": "Mechanical Search on Shelves using Lateral Access X-RAY"
      },
      {
        "paperId": "5f975172aa9088f24236ccc8fe4bfc01ce6fb9b9",
        "title": "Object Rearrangement Using Learned Implicit Collision Functions"
      },
      {
        "paperId": "126d36f17cae3e5210a6f62e5c6a23ddec0ef350",
        "title": "Scaled-YOLOv4: Scaling Cross Stage Partial Network"
      },
      {
        "paperId": "b4caa67681cbe4973b21e96e69ad7b213b19f28f",
        "title": "Rearrangement: A Challenge for Embodied AI"
      },
      {
        "paperId": "eb6b9bc4ff3e4e2cf1724324d79ce7de43131478",
        "title": "Transporter Networks: Rearranging the Visual World for Robotic Manipulation"
      },
      {
        "paperId": "e965fa8a1f2410d3f535b9f54c322606fc45dcc9",
        "title": "Symbolic Relational Deep Reinforcement Learning based on Graph Neural Networks"
      },
      {
        "paperId": "855a91f77b5e0964756bed23e8051bf4d75a46b0",
        "title": "Meta-Sim2: Unsupervised Learning of Scene Structure for Synthetic Data Generation"
      },
      {
        "paperId": "c93949f74194eed629a394b6e68078e983663aaf",
        "title": "Visuomotor Mechanical Search: Learning to Retrieve Target Objects in Clutter"
      },
      {
        "paperId": "77e124b5ab1c20c690dce5e437a4fbbf4e4878a4",
        "title": "X-Ray: Mechanical Search for an Occluded Object by Minimizing Support of Learned Occupancy Distributions"
      },
      {
        "paperId": "1f3d6f3a51051b9dae95e9fa7c8bdf8ed41fa78a",
        "title": "GeoFusion: Geometric Consistency Informed Scene Estimation in Dense Clutter"
      },
      {
        "paperId": "ca5045c9d9e0bf2e95f6694dff657e28ffcd4f07",
        "title": "Learning by Cheating"
      },
      {
        "paperId": "c0e9306e97094dd5349453ff7a61466b69f1c0d2",
        "title": "SG-VAE: Scene Grammar Variational Autoencoder to Generate New Indoor Scenes"
      },
      {
        "paperId": "cc09f6ab4def060f02b42818ef923cf5cb77b7f4",
        "title": "Object Finding in Cluttered Scenes Using Interactive Perception"
      },
      {
        "paperId": "5d20197923af24b8ed24eee45dac99d6bf2d9773",
        "title": "Deep Reinforcement Learning for Robotic Pushing and Picking in Cluttered Environment"
      },
      {
        "paperId": "5fb9910134a3d7c7c9a14b4f628cd28985e13df4",
        "title": "Split Deep Q-Learning for Robust Object Singulation*"
      },
      {
        "paperId": "f2bb983284a0b8d39aca897b974fd9ee2cca385e",
        "title": "A Deep Learning Approach to Grasping the Invisible"
      },
      {
        "paperId": "99e3da8da39ece922fca46ce9f2301626354baaf",
        "title": "SceneGraphNet: Neural Message Passing for 3D Indoor Scene Augmentation"
      },
      {
        "paperId": "7a6cf9c40e5dabd7c98a7731a9705fb8883024e9",
        "title": "PlanIT"
      },
      {
        "paperId": "dbdb47913244562ff6cd2a70956997cbdeffd6cb",
        "title": "DensePhysNet: Learning Dense Physical Object Representations via Multi-step Dynamic Interactions"
      },
      {
        "paperId": "5daecb135e8f914ad05933fe4a5dffc8c068c340",
        "title": "PoseRBPF: A Rao\u2013Blackwellized Particle Filter for 6-D Object Pose Tracking"
      },
      {
        "paperId": "cab702e6c0ff938086da8f9c477841240ee255a5",
        "title": "Probabilistic Active Filtering for Object Search in Clutter"
      },
      {
        "paperId": "b4a35e548de27b6924e5f2ee41d37238a5c4a1d5",
        "title": "Habitat: A Platform for Embodied AI Research"
      },
      {
        "paperId": "bfa07d449c9b0a153c3991fa3fabc5aa746ea2db",
        "title": "Mechanical Search: Multi-Step Retrieval of a Target Object Occluded by Clutter"
      },
      {
        "paperId": "ea5dd6a3d8f210d05e53a7b6fa5e16f1b115f693",
        "title": "Graph Neural Networks: A Review of Methods and Applications"
      },
      {
        "paperId": "bcffc36185ce86eb2e313df8e914c64f354567de",
        "title": "Human-Centric Indoor Scene Synthesis Using Stochastic Grammar"
      },
      {
        "paperId": "16a931f72e51b8cc4acc2a586ee53fb9b0d8959d",
        "title": "Learning Synergies Between Pushing and Grasping with Self-Supervised Deep Reinforcement Learning"
      },
      {
        "paperId": "b59958ff4eed7059659f35f7f5df5b985e797f4b",
        "title": "Learning to Singulate Objects using a Push Proposal Network"
      },
      {
        "paperId": "9172cd6c253edf7c3a1568e03577db20648ad0c4",
        "title": "Reinforcement Learning with Deep Energy-Based Policies"
      },
      {
        "paperId": "331cf0f4abff3c567744af024c66302b5261e959",
        "title": "Goal-directed robot manipulation through axiomatic scene estimation"
      },
      {
        "paperId": "7c7c94e62e36bb5f353c628cc5a6e4add2d07947",
        "title": "Act to See and See to Act: POMDP planning for objects search in clutter"
      },
      {
        "paperId": "1fa9e326274b74aba5b9973ec057dd679cbab785",
        "title": "Interactive Perception: Leveraging Action in Perception and Perception in Action"
      },
      {
        "paperId": "afcf4dbd2ef300e5c4b35043d4fbe516807cdf7d",
        "title": "Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations"
      },
      {
        "paperId": "492f57ee9ceb61fb5a47ad7aebfec1121887a175",
        "title": "Gated Graph Sequence Neural Networks"
      },
      {
        "paperId": "ecb09b54ea93f0fb25ad5211bfbbf26a5483f59e",
        "title": "The YCB object and Model set: Towards common benchmarks for manipulation research"
      },
      {
        "paperId": "33932140df2fbfb685256006f15d99a7e1a37fed",
        "title": "Creating consistent scene graphs using a probabilistic grammar"
      },
      {
        "paperId": "9dcb39ba6b0e4e14e4090979d11f16eadb0d558e",
        "title": "Interactive singulation of objects from a pile"
      },
      {
        "paperId": "a22cbd162066b01639aefd3deceeadd0170a6afe",
        "title": "Estimating Mass Distribution of Articulated Objects using Non-prehensile Manipulation"
      },
      {
        "paperId": null,
        "title": "Syntactic structures"
      },
      {
        "paperId": "1207aa55c8a2b47a25fc17c18328910daf3feb71",
        "title": "Core knowledge."
      },
      {
        "paperId": null,
        "title": "PyBullet, a Python module for physics simulation for games, robotics and machine learning"
      },
      {
        "paperId": null,
        "title": "A learning framework to train an effective scene exploration agent that discovers hidden objects, along with a scene generation agent that specializes in hiding objects in clutter"
      },
      {
        "paperId": null,
        "title": "Interactive segmentation of articulated objects in 3d"
      }
    ],
    "cited_by": [
      {
        "paperId": "6f41f72a27ec9336c5b588416fa2c919f944d65c",
        "title": "When Embodied AI Meets Industry 5.0: Human-Centered Smart Manufacturing"
      },
      {
        "paperId": "5a43a8b91973777bb78a19b0ed7f0a3011c4da19",
        "title": "Learning-based methods for adaptive informative path planning"
      },
      {
        "paperId": "4514292a233cc0eb7b3b58cfb4ac4ed308305df0",
        "title": "Object-aware interactive perception for tabletop scene exploration"
      },
      {
        "paperId": "e34b82692c4fe6e6614e7cf1b74c8c8722c00808",
        "title": "Effectively Rearranging Heterogeneous Objects on Cluttered Tabletops"
      },
      {
        "paperId": "327d6d3cd84461183214e3d143d73a534dedd3fa",
        "title": "Resolution Complete In-Place Object Retrieval given Known Object Models"
      },
      {
        "paperId": "0af6a63167df299a1556a560d6884ae38eda390d",
        "title": "Cascaded Compositional Residual Learning for Complex Interactive Behaviors"
      },
      {
        "paperId": "b4fb286d535c81b13365a2ea137f4c90cfe05135",
        "title": "Review of Learning-Based Robotic Manipulation in Cluttered Environments"
      },
      {
        "paperId": "242363a89495f122cab20dd1b3f4858c3c4997c4",
        "title": "Scene Graph for Embodied Exploration in Cluttered Scenario"
      },
      {
        "paperId": "b115b1bb4f23d99fcd0d9092df2305a1f8027bcc",
        "title": "Mechanical Search on Shelves with Efficient Stacking and Destacking of Objects"
      },
      {
        "paperId": "8fb6f2d97c26de59209f2755ac0827a8025744b5",
        "title": "Safe, Occlusion-Aware Manipulation for Online Object Reconstruction in Confined Spaces"
      },
      {
        "paperId": "8732c4ab0c47c4f7b0c05bafbbb3311e53aa32ba",
        "title": "Robotics and Autonomous Systems"
      }
    ],
    "score": 2.75
  },
  {
    "id": "48de0dab9255ededce3cdaeac84f039d726f7f3e",
    "title": "Controller exploitation-exploration reinforcement learning architecture for computing near-optimal policies",
    "authors": [
      "Erick Asiain",
      "J. Clempner",
      "A. Poznyak"
    ],
    "year": 2018,
    "citationCount": 19,
    "abstract": "",
    "url": "https://www.semanticscholar.org/paper/48de0dab9255ededce3cdaeac84f039d726f7f3e",
    "pdf_url": "https://doi.org/10.1007/s00500-018-3225-7",
    "venue": "Soft Computing - A Fusion of Foundations, Methodologies and Applications",
    "publicationDate": "2018-05-10",
    "externalIds": {
      "MAG": "2803013487",
      "DBLP": "journals/soco/AsiainCP19",
      "DOI": "10.1007/s00500-018-3225-7",
      "CorpusId": 67176831
    },
    "references": [],
    "cited_by": [
      {
        "paperId": "4fd4cd7ba0c7164ae2ccdaae423cbe27bc0a803b",
        "title": "Learning Deceptive Tactics for Defense and Attack in Bayesian\u2013Markov Stackelberg Security Games"
      },
      {
        "paperId": "342a3e1b9bc9c09fc549efd44bbe825048792901",
        "title": "The Evolution of Reinforcement Learning in Quantitative Finance: A Survey"
      },
      {
        "paperId": "7f1fed6fa54b1f404fea3d3a59bc02a8ac1f7685",
        "title": "Reinforcement Learning in Education 4.0: Open Applications and Deployment Challenges"
      },
      {
        "paperId": "0a99faeff723be5589cc75ba7ef54135ca60fb00",
        "title": "A Bayesian reinforcement learning approach in markov games for computing near-optimal policies"
      },
      {
        "paperId": "96433de43674b7297422db6c21c5490501f08b68",
        "title": "A Review of Deep Reinforcement Learning Approaches for Smart Manufacturing in Industry 4.0 and 5.0 Framework"
      },
      {
        "paperId": "0eca7b6d8506a4c74997a9283fe5fc470bf16dce",
        "title": "Learning attack-defense response in continuous-time discrete-states Stackelberg Security Markov games"
      },
      {
        "paperId": "4a08bca47697a02a05214bd1b0cbb383a6dc1ad3",
        "title": "A Lyapunov approach for stable reinforcement learning"
      },
      {
        "paperId": "8e970cf740c9045ace2945df282460209acb4e22",
        "title": "Reveling misleading information for defenders and attackers in repeated Stackelberg Security Games"
      },
      {
        "paperId": "0008e9f8386759f6da8b48b9f121eb610df5dd81",
        "title": "Behavior Decision of Mobile Robot With a Neurophysiologically Motivated Reinforcement Learning Model"
      },
      {
        "paperId": "249e2997936ec6d5f21f0c412a22d908b0311bee",
        "title": "A Dynamic Mechanism Design for Controllable and Ergodic Markov Games"
      },
      {
        "paperId": "ef7d342ecfa53dd63264b2813f02b42a1ec962c4",
        "title": "Learning machiavellian strategies for manipulation in Stackelberg security games"
      },
      {
        "paperId": "c032841e229d20d293cecf8b2f2bd076b6da9fb5",
        "title": "Novel pricing strategies for revenue maximization and demand learning using an exploration\u2013exploitation framework"
      },
      {
        "paperId": "3293f72e2cbb608dc4917a9ba85ffbde1df6ba5f",
        "title": "A Markovian Stackelberg game approach for computing an optimal dynamic mechanism"
      },
      {
        "paperId": "cab8b35a150839f161d27209057529304ca6644d",
        "title": "Multi-Agent Reinforcement Learning Framework in SDN-IoT for Transient Load Detection and Prevention"
      },
      {
        "paperId": "b55124626caf482746a341646011ee4cc3865460",
        "title": "A nucleus for Bayesian Partially Observable Markov Games: Joint observer and mechanism design"
      },
      {
        "paperId": "ff86d3997c43174d31d10a557d05c4fc79a9a6b3",
        "title": "Traffic-signal control reinforcement learning approach for continuous-time Markov games"
      },
      {
        "paperId": "016df1c1efcc6adc23446a127c6838cd659312b8",
        "title": "Tuning of reinforcement learning parameters applied to SOP using the Scott\u2013Knott method"
      },
      {
        "paperId": "d749ac6d02fa5e00499ecf3f04d1a4a1ec1615da",
        "title": "Controlling Population Diversity of Harris Hawks Optimization Algorithm Using Self-adaptive Clustering Approach"
      },
      {
        "paperId": "60c6c1528135b4ee477e1e2b58058a6e6cdee2c6",
        "title": "An Adaptive Similarity-Measuring-Based CMAB Model for Recommendation System"
      }
    ],
    "score": 2.714285714285714
  },
  {
    "id": "071de005741bd666c7a9ccf40d5ed1d502f5282b",
    "title": "Curiosity-Driven Exploration for Off-Policy Reinforcement Learning Methods*",
    "authors": [
      "Boyao Li",
      "Tao Lu",
      "Jiayi Li",
      "N. Lu",
      "Yinghao Cai",
      "Shuo Wang"
    ],
    "year": 2019,
    "citationCount": 14,
    "abstract": "Deep reinforcement learning (DRL) has achieved remarkable results in many high-dimensional continuous control tasks. However, the RL agent still explores the environment randomly, resulting in low exploration efficiency and learning performance, especially in robotic manipulation tasks with sparse rewards. To address this problem, in this paper, we intro-duce a simplified Intrinsic Curiosity Module (S-ICM) into the off-policy RL methods to encourage the agent to pursue novel and surprising states for improving the exploration competence. This method can be combined with an arbitrary off-policy RL algorithm. We evaluate our approach on three challenging robotic manipulation tasks provided by OpenAI Gym. In our experiments, we combined our method with Deep Deterministic Policy Gradient (DDPG) with and without Hindsight Experience Replay (HER). The empirical results show that our proposed method significantly outperforms vanilla RL algorithms both in sample-efficiency and learning performance.",
    "url": "https://www.semanticscholar.org/paper/071de005741bd666c7a9ccf40d5ed1d502f5282b",
    "pdf_url": "https://doi.org/10.1109/ROBIO49542.2019.8961529",
    "venue": "IEEE International Conference on Robotics and Biomimetics",
    "publicationDate": "2019-12-01",
    "externalIds": {
      "MAG": "3000757491",
      "DBLP": "conf/robio/LiLLLCW19",
      "DOI": "10.1109/ROBIO49542.2019.8961529",
      "CorpusId": 210888174
    },
    "references": [],
    "cited_by": [
      {
        "paperId": "ba52628af1d89b24f4e717fdb7a2dcdc9b32fb55",
        "title": "A Configurable Intrinsic Curiosity Module for a Testbed for Developing Intelligent Swarm UAVs"
      },
      {
        "paperId": "4e1807cab0de688943515b5f58d37ce5fb100305",
        "title": "Hierarchical reinforcement learning with curriculum demonstrations and goal-guided policies for sequential robotic manipulation"
      },
      {
        "paperId": "929eb043e19cc7f2f9e93429f393c39acb7043d6",
        "title": "Mixture of Autoencoder Experts Guidance using Unlabeled and Incomplete Data for Exploration in Reinforcement Learning"
      },
      {
        "paperId": "3d33b6b902f79778270f88bbe76355990deb28dd",
        "title": "An Experience Replay Approach Based on SSIM to Solve the Sparse Reward Problem in Pursuit Evasion Game*"
      },
      {
        "paperId": "a727f9ea6dfceaae6acdd01d51aca2a17caed016",
        "title": "Scheduled Curiosity-Deep Dyna-Q: Efficient Exploration for Dialog Policy Learning"
      },
      {
        "paperId": "d922a1cca6708cc0d31953abd2ae9c0a3e97328c",
        "title": "AHEGC: Adaptive Hindsight Experience Replay With Goal-Amended Curiosity Module for Robot Control"
      },
      {
        "paperId": "da3f9acc52801d2181448f18ddca93d7bf201bb5",
        "title": "Dealing With Sparse Rewards Using Graph Neural Networks"
      },
      {
        "paperId": "da1e404ae194dcbcce198f5335f49f090403bbd7",
        "title": "Exploration in Deep Reinforcement Learning: A Survey"
      },
      {
        "paperId": "808bf26de91c06138d8d7b352f2080779b380c10",
        "title": "Imaginary Hindsight Experience Replay: Curious Model-based Learning for Sparse Reward Tasks"
      },
      {
        "paperId": "1e515c9c4aaca14194112dad3e7301bea2d61930",
        "title": "DIMSAN: Fast Exploration with the Synergy between Density-based Intrinsic Motivation and Self-adaptive Action Noise"
      },
      {
        "paperId": "0ea89ca0929b9323ba3d65bc6bce654d37cdcea4",
        "title": "Hierarchical Learning from Demonstrations for Long-Horizon Tasks"
      },
      {
        "paperId": "cc7785210c03334bebf34452523bc42628a1a19f",
        "title": "Curiosity-Driven Reinforced Learning of Undesired Actions in Autonomous Intelligent Agents"
      },
      {
        "paperId": "0a38ed285c159e07cbf5edaf4b73ea4e045406d2",
        "title": "ACDER: Augmented Curiosity-Driven Experience Replay"
      },
      {
        "paperId": "dfcf99ef01d3429a7d890ef8bdf19293d84c6354",
        "title": "Towards Improving Exploration through Sibling Augmented GFlowNets"
      }
    ],
    "score": 2.333333333333333
  },
  {
    "id": "3f673101c2cac3b47639056e2988e018546c3c90",
    "title": "Zeroth-Order Supervised Policy Improvement",
    "authors": [
      "Hao Sun",
      "Ziping Xu",
      "Yuhang Song",
      "Meng Fang",
      "Jiechao Xiong",
      "Bo Dai",
      "Zhengyou Zhang",
      "Bolei Zhou"
    ],
    "year": 2020,
    "citationCount": 10,
    "abstract": "Despite the remarkable progress made by the policy gradient algorithms in reinforcement learning (RL), sub-optimal policies usually result from the local exploration property of the policy gradient update. In this work, we propose a method referred to as Zeroth-Order Supervised Policy Improvement (ZOSPI) that exploits the estimated value function Q globally while preserves the local exploitation of the policy gradient methods. We prove that with a good function structure, the zeroth-order optimization strategy combining both local and global samplings can find the global minima within a polynomial number of samples. To improve the exploration efficiency in unknown environments, ZOSPI is further combined with bootstrapped Q networks. Different from the standard policy gradient methods, the policy learning of ZOSPI is conducted in a self-supervision manner so that the policy can be implemented with gradient-free non-parametric models besides the neural network approximator. Experiments show that ZOSPI achieves competitive results on MuJoCo locomotion tasks with a remarkable sample efficiency.",
    "url": "https://www.semanticscholar.org/paper/3f673101c2cac3b47639056e2988e018546c3c90",
    "pdf_url": "https://arxiv.org/pdf/2006.06600.pdf",
    "venue": "arXiv.org",
    "publicationDate": "2020-06-11",
    "externalIds": {
      "DBLP": "journals/corr/abs-2006-06600",
      "MAG": "3034269976",
      "ArXiv": "2006.06600",
      "CorpusId": 219573505
    },
    "references": [
      {
        "paperId": "831e7cbafed2dca05db1e7f5ef16d1a7614f44ec",
        "title": "Learning to Reach Goals via Iterated Supervised Learning"
      },
      {
        "paperId": "93b982385d61793e4859728e58d04f0ca6b9f48f",
        "title": "Gradientless Descent: High-Dimensional Zeroth-Order Optimization"
      },
      {
        "paperId": "361c00b22e29d0816ca896513d2c165e26399821",
        "title": "Grandmaster level in StarCraft II using multi-agent reinforcement learning"
      },
      {
        "paperId": "5818237fefb13f9d2e5306679286e4ecb64f54b7",
        "title": "Efficiently avoiding saddle points with zero order methods: No gradients required"
      },
      {
        "paperId": "007fd689a806de99f053a0f5ef90aedb44f9ec7e",
        "title": "Better Exploration with Optimistic Actor-Critic"
      },
      {
        "paperId": "3a42517dda64129c30e553f602b229f0ca033cc0",
        "title": "Escaping Saddle Points for Zeroth-order Non-convex Optimization using Estimated Gradient Descent"
      },
      {
        "paperId": "c92780cd2c90b0393efb5d5b3a49b1ec9503df11",
        "title": "Policy Continuation with Hindsight Inverse Dynamics"
      },
      {
        "paperId": "e7bb4419a88d15fa8e52c1f4f9cfd65ed58c7379",
        "title": "V-MPO: On-Policy Maximum a Posteriori Policy Optimization for Discrete and Continuous Control"
      },
      {
        "paperId": "ffba26a38d4b3c25d2984266f72f72889b2413ff",
        "title": "Learning To Reach Goals Without Reinforcement Learning"
      },
      {
        "paperId": "cc4435c2c1ea079721f48d08d0ae3436599d1533",
        "title": "Striving for Simplicity in Off-policy Deep Reinforcement Learning"
      },
      {
        "paperId": "82b4b03a4659d6e04bd7cbf51d6e08fde1348dbd",
        "title": "Stabilizing Off-Policy Q-Learning via Bootstrapping Error Reduction"
      },
      {
        "paperId": "39deb017bdc07d3c49f6a888153e7a01d8a2acbe",
        "title": "Policy Search by Target Distribution Learning for Continuous Control"
      },
      {
        "paperId": "348ba3ae6bd48dfa66d525075c2e6c39cba7a3f5",
        "title": "Distributional Policy Optimization: An Alternative Approach for Continuous Control"
      },
      {
        "paperId": "ab8b9bf57675a4a7c28cbb1dc176382886d0f332",
        "title": "Q-Learning for Continuous Actions with Cross-Entropy Guided Policies"
      },
      {
        "paperId": "4bdfc8c2c5f2d6d1a6ef51c3252bda70b01a1ebb",
        "title": "Generalized Off-Policy Actor-Critic"
      },
      {
        "paperId": "4600674f5b157a28dfbc5f2d0167354cf142bd3d",
        "title": "Efficient Model-Free Reinforcement Learning Using Gaussian Process"
      },
      {
        "paperId": "5285cb8faada5de8a92a47622950f6cfd476ac1d",
        "title": "Off-Policy Deep Reinforcement Learning without Exploration"
      },
      {
        "paperId": "655cfe96e20675183dc8c2acbab659bce54fd6f5",
        "title": "Relative Entropy Regularized Policy Iteration"
      },
      {
        "paperId": "caeb088b19b0829551ef4bf3da8b1a5c98bf8e73",
        "title": "Exponentially Weighted Imitation Learning for Batched Historical Data"
      },
      {
        "paperId": "ea68ea513461d329d59e8718a8919729de769273",
        "title": "Actor-Expert: A Framework for using Q-learning in Continuous Action Spaces"
      },
      {
        "paperId": "03cc81e98942bdafd994af7a1d1e62a68ff8b682",
        "title": "Is Q-learning Provably Efficient?"
      },
      {
        "paperId": "f802802b3af5a22b79ac65d033ba3cbee33da91b",
        "title": "Randomized Prior Functions for Deep Reinforcement Learning"
      },
      {
        "paperId": "abc8415a73e9056fdd8a7bf529b2c86898f29501",
        "title": "Simple random search provides a competitive approach to reinforcement learning"
      },
      {
        "paperId": "4debb99c0c63bfaa97dd433bc2828e4dac81c48b",
        "title": "Addressing Function Approximation Error in Actor-Critic Methods"
      },
      {
        "paperId": "811df72e210e20de99719539505da54762a11c6d",
        "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor"
      },
      {
        "paperId": "2064020586d5832b55f80a7dffea1fd90a5d94dd",
        "title": "Improving Exploration in Evolution Strategies for Deep Reinforcement Learning via a Population of Novelty-Seeking Agents"
      },
      {
        "paperId": "b1e296e069308cf79c27f7a8cde7cff825d4c080",
        "title": "Stochastic Zeroth-order Optimization in High Dimensions"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "67d10cea808937089d6492acff14ad9ef156e3c5",
        "title": "Minimax Regret Bounds for Reinforcement Learning"
      },
      {
        "paperId": "4ee802a58d32aa049d549d06be440ac947b53987",
        "title": "Evolution Strategies as a Scalable Alternative to Reinforcement Learning"
      },
      {
        "paperId": "9172cd6c253edf7c3a1568e03577db20648ad0c4",
        "title": "Reinforcement Learning with Deep Energy-Based Policies"
      },
      {
        "paperId": "524513b6f4ddca331c33bcc70a9f677fa240cfa3",
        "title": "Q-Prop: Sample-Efficient Policy Gradient with An Off-Policy Critic"
      },
      {
        "paperId": "d431b498465d35c8aa0012c997eee246e1760f33",
        "title": "Episodic Exploration for Deep Deterministic Policies for StarCraft Micromanagement"
      },
      {
        "paperId": "6a43d91c8d883e3463b358571125fa0ec7298b3a",
        "title": "Sample Efficient Actor-Critic with Experience Replay"
      },
      {
        "paperId": "2b10281297ee001a9f3f4ea1aa9bea6b638c27df",
        "title": "OpenAI Gym"
      },
      {
        "paperId": "4b63e34276aa98d5345efa7fe09bb06d8a9d8f52",
        "title": "Deep Exploration via Bootstrapped DQN"
      },
      {
        "paperId": "21a0b0fbdde1aee56fe10e69e897decaf21f43a6",
        "title": "Random Gradient-Free Minimization of Convex Functions"
      },
      {
        "paperId": "024006d4c2a89f7acacc6e4438d156525b60a98f",
        "title": "Continuous control with deep reinforcement learning"
      },
      {
        "paperId": "340f48901f72278f6bf78a04ee5b01df208cc508",
        "title": "Human-level control through deep reinforcement learning"
      },
      {
        "paperId": "449532187c94af3dd3aa55e16d2c50f7854d2199",
        "title": "Trust Region Policy Optimization"
      },
      {
        "paperId": "687d0e59d5c35f022ce4638b3e3a6142068efc94",
        "title": "Deterministic Policy Gradient Algorithms"
      },
      {
        "paperId": "300fd640ebe02bfc87f9c94776b23bb59b330848",
        "title": "Sample Efficient Reinforcement Learning with Gaussian Processes"
      },
      {
        "paperId": "b354ee518bfc1ac0d8ac447eece9edb69e92eae1",
        "title": "MuJoCo: A physics engine for model-based control"
      },
      {
        "paperId": "0067343a36c0292f36e627eb353f751c8a39f99a",
        "title": "Linear Off-Policy Actor-Critic"
      },
      {
        "paperId": "1e045f3447f69d9a7cac18ef23062ea8dd661285",
        "title": "Nonlinear Inverse Reinforcement Learning with Gaussian Processes"
      },
      {
        "paperId": "0cafe2903b097fc042782c359cb231ea34ef7ed3",
        "title": "Near-optimal Regret Bounds for Reinforcement Learning"
      },
      {
        "paperId": "d28421a802569595b77761f83f1880c355dcea45",
        "title": "Gaussian process models for robust regression, classification, and reinforcement learning"
      },
      {
        "paperId": "ba4bcd8c4ebb1427d335c23d50249d370778ae49",
        "title": "Reinforcement learning with Gaussian processes"
      },
      {
        "paperId": "3cc0095b47615c2f47beaca9bd8f675811fb118a",
        "title": "Gaussian Processes in Reinforcement Learning"
      },
      {
        "paperId": "f1a391bab223fc2609717316bec30ae36f8ea448",
        "title": "Natural Actor-Critic"
      },
      {
        "paperId": "a20f0ce0616def7cc9a87446c228906cd5da093b",
        "title": "Policy Gradient Methods for Reinforcement Learning with Function Approximation"
      },
      {
        "paperId": null,
        "title": "\u201cOpenai baselines.\u201d"
      },
      {
        "paperId": "4c915c1eecb217c123a36dc6d3ce52d12c742614",
        "title": "Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning"
      },
      {
        "paperId": "c7dbf5ed7e9b63e104adb4e18bfbc98f5d6afdae",
        "title": "R-MAX - A General Polynomial Time Algorithm for Near-Optimal Reinforcement Learning"
      },
      {
        "paperId": "ac4af1df88e178386d782705acc159eaa0c3904a",
        "title": "Actor-Critic Algorithms"
      },
      {
        "paperId": "d4ca18249446328c86d9da295a21c679aea1ed77",
        "title": "Mixture Density Networks"
      }
    ],
    "cited_by": [
      {
        "paperId": "25e2be02cee41b870f6e322e14cdb7733bed7b38",
        "title": "The Multi-Query Paradox in Zeroth-Order Optimization"
      },
      {
        "paperId": "43d72582f68133ff7316f4fcbd6eeac41845f57a",
        "title": "Evolutionary Reinforcement Learning: A Systematic Review and Future Directions"
      },
      {
        "paperId": "25523c3e70daedcd0f1172844fcbafe30b8933f1",
        "title": "Bridging Evolutionary Algorithms and Reinforcement Learning: A Comprehensive Survey"
      },
      {
        "paperId": "f782d5f569eaa4eb2d66e0cb95bdba0941dbb8b0",
        "title": "Reinforcement Learning in the Era of LLMs: What is Essential? What is needed? An RL Perspective on RLHF, Prompting, and Beyond"
      },
      {
        "paperId": "192363269283a1ffd98d8f44155640fe754f0b85",
        "title": "Evolutionary Game-Based Vertical Handover Strategy for Space-Air-Ground Integrated Network"
      },
      {
        "paperId": "79dbba6bfba5bf20bc564f895c2cc8c8bf6953b4",
        "title": "Optimistic Curiosity Exploration and Conservative Exploitation with Linear Reward Shaping"
      },
      {
        "paperId": "d0e783eeb54893c7a283ccbd044647f8f390b16e",
        "title": "Combining Evolution and Deep Reinforcement Learning for Policy Search: A Survey"
      },
      {
        "paperId": "ca5c8d732e7f6046af1b23b549e08460a1893ade",
        "title": "Plan Better Amid Conservatism: Offline Multi-Agent Reinforcement Learning with Actor Rectification"
      },
      {
        "paperId": "77eeeb73335404ae1ec72f34509c97ff5caf3110",
        "title": "A survey of learning\u2010based robot motion planning"
      },
      {
        "paperId": "06116b70e27d60da93329c2be14ab1101c61175f",
        "title": "Exploit Reward Shifting in Value-Based Deep-RL: Optimistic Curiosity-Based Exploration and Conservative Exploitation via Linear Reward Shaping"
      }
    ],
    "score": 2.0
  },
  {
    "id": "24107405a96a53d4c292b08608300a6c7e457ffe",
    "title": "Stylistic Dialogue Generation via Information-Guided Reinforcement Learning Strategy",
    "authors": [
      "Yixuan Su",
      "Deng Cai",
      "Yan Wang",
      "Simon Baker",
      "A. Korhonen",
      "Nigel Collier",
      "Xiaojiang Liu"
    ],
    "year": 2020,
    "citationCount": 10,
    "abstract": "Stylistic response generation is crucial for building an engaging dialogue system for industrial use. While it has attracted much research interest, existing methods often generate stylistic responses at the cost of the content quality (relevance and fluency). To enable better balance between the content quality and the style, we introduce a new training strategy, know as Information-Guided Reinforcement Learning (IG-RL). In IG-RL, a training model is encouraged to explore stylistic expressions while being constrained to maintain its content quality. This is achieved by adopting reinforcement learning strategy with statistical style information guidance for quality-preserving explorations. Experiments on two datasets show that the proposed approach outperforms several strong baselines in terms of the overall response performance.",
    "url": "https://www.semanticscholar.org/paper/24107405a96a53d4c292b08608300a6c7e457ffe",
    "pdf_url": "https://arxiv.org/pdf/2004.02202.pdf",
    "venue": "arXiv.org",
    "publicationDate": "2020-04-05",
    "externalIds": {
      "MAG": "3014508569",
      "ArXiv": "2004.02202",
      "DBLP": "journals/corr/abs-2004-02202",
      "CorpusId": 214802909
    },
    "references": [
      {
        "paperId": "c922e80acc086654ad2ca03f655ad60c848bdc68",
        "title": "Generating Responses with a Specific Emotion in Dialog"
      },
      {
        "paperId": "9ebe176991382916a07b3cb107b0d2ff27782308",
        "title": "Fine-Grained Sentence Functions for Short-Text Conversation"
      },
      {
        "paperId": "27033b8f72bf8cb7662c9f92b3ccb3c476db7135",
        "title": "Conversing by Reading: Contentful Neural Conversation with On-demand Machine Reading"
      },
      {
        "paperId": "14597394f68463424c52fbb565c61b096434b3e1",
        "title": "Survey on evaluation methods for dialogue systems"
      },
      {
        "paperId": "0f8f3d86f6bc9277fcd4f18b0238a4340ec3402c",
        "title": "Unsupervised Text Style Transfer via Iterative Matching and Translation"
      },
      {
        "paperId": "087a6600d19ac6b90ddcf0ccff1a706882ae407a",
        "title": "An Affect-Rich Neural Conversational Model with Biased Attention and Weighted Cross-Entropy Loss"
      },
      {
        "paperId": "919aa20706b7331d7efe7304d8793a7a3b240894",
        "title": "Automatic Dialogue Generation with Expressed Emotions"
      },
      {
        "paperId": "f1cba8a5a73c8151c2f5cb6edd5bc6a7c03e80fa",
        "title": "Polite Dialogue Generation Without Parallel Data"
      },
      {
        "paperId": "29de7c0fb3c09eaf55b20619bceaeafe72fd87a6",
        "title": "Hierarchical Neural Story Generation"
      },
      {
        "paperId": "f7eda71e2c88cef614a68a816b4ff186f41762b9",
        "title": "MojiTalk: Generating Emotional Responses at Scale"
      },
      {
        "paperId": "a6401e102c03a441992b3e45f7b63eec09d4b89d",
        "title": "A Survey on Dialogue Systems: Recent Advances and New Frontiers"
      },
      {
        "paperId": "b36a5bb1707bb9c70025294b3a310138aae8327a",
        "title": "Automatic differentiation in PyTorch"
      },
      {
        "paperId": "032274e57f7d8b456bd255fe76b909b2c1d7458e",
        "title": "A Deep Reinforced Model for Abstractive Summarization"
      },
      {
        "paperId": "7b221cce8fdbc1105956b27c938730fce8c1fc10",
        "title": "Emotional Chatting Machine: Emotional Conversation Generation with Internal and External Memory"
      },
      {
        "paperId": "6ce1922802169f757bbafc6e087cc274a867c763",
        "title": "Regularizing Neural Networks by Penalizing Confident Output Distributions"
      },
      {
        "paperId": "1b4f9451593d0cc73e8d547d3e8fdbcc4a1fc5cc",
        "title": "University of Pennsylvania Working Papers in Linguistics"
      },
      {
        "paperId": "55289d3feef4bc1e4ff17008120e371eb7f55a24",
        "title": "Conditional Generation and Snapshot Learning in Neural Dialogue Systems"
      },
      {
        "paperId": "1ea75cdb7ce8c4f5f2599165e3698034b4142e08",
        "title": "A Persona-Based Neural Conversation Model"
      },
      {
        "paperId": "c3781d9f49205e9028053e67cf60ffeec3943a44",
        "title": "Dropped pronoun generation for dialogue machine translation"
      },
      {
        "paperId": "23ffaa0fe06eae05817f527a47ac3291077f9e58",
        "title": "Rethinking the Inception Architecture for Computer Vision"
      },
      {
        "paperId": "651e5bcc14f14605a879303e97572a27ea8c7956",
        "title": "A Diversity-Promoting Objective Function for Neural Conversation Models"
      },
      {
        "paperId": "93499a7c7f699b6630a86fad964536f9423bb6d0",
        "title": "Effective Approaches to Attention-based Neural Machine Translation"
      },
      {
        "paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8",
        "title": "Adam: A Method for Stochastic Optimization"
      },
      {
        "paperId": "5b6f048840ded1a27b9830f78f4d48b3ededb3ea",
        "title": "Data-Driven Response Generation in Social Media"
      },
      {
        "paperId": "5d3fa586f94bc8964a39bcedc52a54ef1b1fe39f",
        "title": "Linguistic Style Matching in Social Interaction"
      },
      {
        "paperId": "a20f0ce0616def7cc9a87446c228906cd5da093b",
        "title": "Policy Gradient Methods for Reinforcement Learning with Function Approximation"
      },
      {
        "paperId": "9e2caa39ac534744a180972a30a320ad0ae41ea3",
        "title": "Word Association Norms, Mutual Information, and Lexicography"
      },
      {
        "paperId": "2a3f545b76e23d5b9e5fd98eb2eeaf9fa6438fac",
        "title": "Language style as audience design"
      },
      {
        "paperId": "0d5bf05e2bef53da05ca0eadeaed108c6521fd54",
        "title": "William Labov, Sociolinguistic patterns . (Conduct and Communication, 4.) Philadelphia: University of Pennsylvania Press, 1972."
      },
      {
        "paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992",
        "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
      },
      {
        "paperId": "5c944be3ae04ae450422e935bfb10b4798fc981b",
        "title": "The Error Is the Clue: Breakdown In Human-Machine Interaction"
      },
      {
        "paperId": "d58927f0b298dc709215d02beba9fa8af989c4c8",
        "title": "Style and Sociolinguistic Variation."
      },
      {
        "paperId": "8c0887909276e294ea1ebc387a0b779ddca52fc9",
        "title": "Towards a sociolinguistics of style"
      },
      {
        "paperId": "cfb4592221080deb127de94e8063fb403b13a298",
        "title": "Measuring nominal scale agreement among many raters."
      },
      {
        "paperId": null,
        "title": ") philadelphia: University of pennsylvania press"
      },
      {
        "paperId": null,
        "title": "2019a. Generating responses"
      }
    ],
    "cited_by": [
      {
        "paperId": "24676a1571419f28806c5731e9c91645141517a1",
        "title": "Unveiling Maternity and Infant Care Conversations: A Chinese Dialogue Dataset for Enhanced Parenting Support"
      },
      {
        "paperId": "67f23dec7687692660d8aa1315b9dbc8e1aacf22",
        "title": "Dialogue Agents 101: A Beginner's Guide to Critical Ingredients for Designing Effective Conversational Systems"
      },
      {
        "paperId": "b029cf8f6de2fdaa6a972166d0d377e236cfefb7",
        "title": "Semantic Space Grounded Weighted Decoding for Multi-Attribute Controllable Dialogue Generation"
      },
      {
        "paperId": "e2b804ee819e5d3ad9ea654b894026b309972e0a",
        "title": "Speaker Profiling in Multiparty Conversations"
      },
      {
        "paperId": "540fc52f0d3f386bbac14f968eb40e27d9b9f4ed",
        "title": "Emotional Dialogue Generation Based on Transformer and Conditional Variational Autoencoder"
      },
      {
        "paperId": "530dbd3247bdec8ee9b2513ebbe6b6df5e4d6055",
        "title": "Stylized Knowledge-Grounded Dialogue Generation via Disentangled Template Rewriting"
      },
      {
        "paperId": "deb11436887c1f9d0b2811640f56e2851601a201",
        "title": "Stylistic Retrieval-based Dialogue System with Unparallel Training Data"
      },
      {
        "paperId": "c219a4aae72fd7ea73346cbefcf67c591d6100e8",
        "title": "Emotional Dialogue Generation Based on Conditional Variational Autoencoder and Dual Emotion Framework"
      },
      {
        "paperId": "d645d1b87e4f0bb371fd95fc9cb890d3a6cd8de1",
        "title": "PROTOTYPE-TO-STYLE: Dialogue Generation With Style-Aware Editing on Retrieval Memory"
      },
      {
        "paperId": "9639402ce19df2bf013a67ab18690454c02c0185",
        "title": "Adding SPICE to Life: Speaker Profiling in Multiparty Conversations"
      }
    ],
    "score": 2.0
  },
  {
    "id": "57f2811d8d8da81f2b4ed80097f5e47a49d76d19",
    "title": "WRFMR: A Multi-Agent Reinforcement Learning Method for Cooperative Tasks",
    "authors": [
      "Hui Liu",
      "Zhen Zhang",
      "Dongqing Wang"
    ],
    "year": 2020,
    "citationCount": 10,
    "abstract": "Multi-agent reinforcement learning (MARL) for cooperative tasks has been extensively studied in recent years. The balance of exploration and exploitation is crucial to MARL algorithms\u2019 performance in terms of the learning speed and the quality of the obtained strategy. To this end, we propose an algorithm known as the weighted relative frequency of obtaining the maximal reward (WRFMR), which uses a weight parameter and the action probability to balance exploration and exploitation and accelerate convergence to the optimal joint action. For the WRFMR algorithm, each agent needs to share the state and the immediate reward and does not need to observe the actions of the other agents. Theoretical analysis on the model of WRFMR in cooperative repeated games shows that each optimal joint action is an asymptotically stable critical point if the component action of every optimal joint action is unique. The box-pushing task, the distributed sensor network (DSN) task, and a strategy game known as blood battlefield are used for empirical studies. Both the DSN task and the box-pushing task involve full cooperation, while blood battle comprises both cooperation and competition. The simulation results show that the WRFMR algorithm outperforms the other algorithms regarding the success rate and the learning speed.",
    "url": "https://www.semanticscholar.org/paper/57f2811d8d8da81f2b4ed80097f5e47a49d76d19",
    "pdf_url": "https://doi.org/10.1109/ACCESS.2020.3040985",
    "venue": "IEEE Access",
    "publicationDate": "",
    "externalIds": {
      "MAG": "3108562470",
      "DBLP": "journals/access/LiuZW20a",
      "DOI": "10.1109/ACCESS.2020.3040985",
      "CorpusId": 228093517
    },
    "references": [
      {
        "paperId": "e7c1d9ed8fa5dcc4214fe15d05bcf10f5122cdea",
        "title": "Adaptive inertia weight Bat algorithm with Sugeno-Function fuzzy search"
      },
      {
        "paperId": "81f207c4fb7b6e24f83c87863efc0b30be211c3e",
        "title": "Multi-agent reinforcement learning for modeling and control of thermostatically controlled loads"
      },
      {
        "paperId": "6f3ecb4a3ef24889d9d8e8e6513b838c60c36fa8",
        "title": "EAQR: A Multiagent Q-Learning Algorithm for Coordination of Multiple Agents"
      },
      {
        "paperId": "ffc211476f2e40e79466ffc198c919a97da3bb76",
        "title": "QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "4de770c8569893f5757c056390ffe1ae8fb49b17",
        "title": "Lenient Multi-Agent Deep Reinforcement Learning"
      },
      {
        "paperId": "250bf5e1d40a619080dec553914c2905db6008c7",
        "title": "Value-Decomposition Networks For Cooperative Multi-Agent Learning"
      },
      {
        "paperId": "7c3ece1ba41c415d7e81cfa5ca33a8de66efd434",
        "title": "Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments"
      },
      {
        "paperId": "2ec1fe09d9602a46da6c93876d1ab7f9adc61696",
        "title": "FMRQ\u2014A Multiagent Reinforcement Learning Algorithm for Fully Cooperative Tasks"
      },
      {
        "paperId": "2b292ff89d808fba10579871591a22f1649cd039",
        "title": "Counterfactual Multi-Agent Policy Gradients"
      },
      {
        "paperId": "3ac0fea1e5395cfb0dc1f0ee2b921fe22b23fed0",
        "title": "Stabilising Experience Replay for Deep Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "ab688b92a0fc790c4f3641d0f94a7d61cdf54231",
        "title": "Adaptive Neural Network Control of AUVs With Control Input Nonlinearities Using Reinforcement Learning"
      },
      {
        "paperId": "f2bc398896cb3f16b0c090703c6ce821fdd460f2",
        "title": "Experience Replay for Optimal Control of Nonzero-Sum Game Systems With Unknown Dynamics"
      },
      {
        "paperId": "53bf2e75bbd456a61fff0eb11026c1531ebac1ea",
        "title": "Exponential moving average based multiagent reinforcement learning algorithms"
      },
      {
        "paperId": "942c5f9087b82612fe4163b4f58d6c1ce7a8eaa3",
        "title": "A multi-agent cooperative reinforcement learning model using a hierarchy of consultants, tutors and workers"
      },
      {
        "paperId": "71bdcf69b665c6b7b66c382997f8f39c7da114e8",
        "title": "Clique-based cooperative multiagent reinforcement learning using factor graphs"
      },
      {
        "paperId": "4f03b6348f77eaa078615386bbd81791eb1898d2",
        "title": "Independent learners in abstract traffic scenarios"
      },
      {
        "paperId": "9c6933244bcf31ce8a05a1e4ee0ec6d015416616",
        "title": "Independent reinforcement learners in cooperative Markov games: a survey regarding coordination problems"
      },
      {
        "paperId": "23abd596945e891da917444491a9ec5b3d9c7874",
        "title": "Multi-agent Q-learning with joint state value approximation"
      },
      {
        "paperId": "98cf2b12111a83a02c880c6d818e93d4975c79c7",
        "title": "Learning to compete, coordinate, and cooperate in\u00a0repeated games using reinforcement learning"
      },
      {
        "paperId": "593678c350e955f9fe4e0b1ac7f51a74b026709a",
        "title": "Multi-Agent Learning with Policy Prediction"
      },
      {
        "paperId": "4aece8df7bd59e2fbfedbf5729bba41abc56d870",
        "title": "A Comprehensive Survey of Multiagent Reinforcement Learning"
      },
      {
        "paperId": "3ab073510c98bd03ef4f72781633c61fbe623966",
        "title": "Preprocessing techniques for accelerating the DCOP algorithm ADOPT"
      },
      {
        "paperId": "7bcb1225984a26dfe1a343b180269c6d62c6ec46",
        "title": "Convergence and No-Regret in Multiagent Learning"
      },
      {
        "paperId": "87a6657499edef36443fa5410602512c1cf5291f",
        "title": "Asymmetric multiagent reinforcement learning"
      },
      {
        "paperId": "e1f153c6df86d1ca8ecb9561daddfe7a54f901e7",
        "title": "Online Convex Programming and Generalized Infinitesimal Gradient Ascent"
      },
      {
        "paperId": "53ca66c57406516f05cf0f1ae78511f1e5f4f385",
        "title": "Adaptive policy gradient in multiagent learning"
      },
      {
        "paperId": "8978d7629fd90f0975656412793715961eeeafc7",
        "title": "AWESOME: A general multiagent learning algorithm that converges in self-play and learns a best response against stationary opponents"
      },
      {
        "paperId": "b07e04eb64705f0f1b5e6403320e0230e2eadc4d",
        "title": "A multiagent reinforcement learning algorithm using extended optimal response"
      },
      {
        "paperId": "05e4f9007e75d3e426ee2d6414a852e9cca8bcb2",
        "title": "Multiagent learning using a variable learning rate"
      },
      {
        "paperId": "ba34fd2fd9628d92d2f4dcff73277d15c35bd6cb",
        "title": "Value-function reinforcement learning in Markov games"
      },
      {
        "paperId": "a7c183abb9b044bfbe1f09199ee970ea3a01104f",
        "title": "Nash Convergence of Gradient Dynamics in General-Sum Games"
      },
      {
        "paperId": "38d35d5581e58dca4e9458501e65c1f85ca754d5",
        "title": "The Dynamics of Reinforcement Learning in Cooperative Multiagent Systems"
      },
      {
        "paperId": "7fbf55baccbc5fdc7ded1ba18330605909aef5e5",
        "title": "Markov Games as a Framework for Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "dd10d2767b2ba4d06ea437af71ca59b45a6bbca6",
        "title": "A Gradient-Based Reinforcement Learning Algorithm for Multiple Cooperative Agents"
      },
      {
        "paperId": null,
        "title": "\u2018\u2018Multi-agent differen-tialgraphicalgames:Onlineadaptivelearningsolutionforsynchronization withoptimality,\u2019\u2019"
      },
      {
        "paperId": "af3e8a983ca2d748229d8e106b61a33ded6ed39d",
        "title": "Convergence, Targeted Optimality, and Safety in Multiagent Learning"
      },
      {
        "paperId": "25c1d2b5fbcf184dc9014f835ac4143f8bf8a923",
        "title": "Learn to Coordinate with Generic Non-Stationary Opponents"
      },
      {
        "paperId": null,
        "title": "Electrical and Computer Engineering, University ofTennessee,Knoxville,USA,from2004to2005. ShehasbeenaProfessorwiththeSchoolofElectricalEngineering,QingdaoUniversity,since"
      },
      {
        "paperId": null,
        "title": "WRFMR: MARL Method for Cooperative Tasks"
      }
    ],
    "cited_by": [
      {
        "paperId": "b46624c2aa3594a422ae82cc4fd3244654ed7d0f",
        "title": "Adaptive neural decision tree for EEG based emotion recognition"
      },
      {
        "paperId": "7121ef4ecf80ca34ec4b5a74bb347b458e2aa67a",
        "title": "Rationality-bounded adaptive learning in multi-agent dynamic games"
      },
      {
        "paperId": "4bb2ee95c25944aaa74de84b6a724b9779854f57",
        "title": "A MADDPG-based multi-agent antagonistic algorithm for sea battlefield confrontation"
      },
      {
        "paperId": "191f10c1c91c84437b3bfcd80d513d4aa30bf381",
        "title": "An FPGA-based multi-agent Reinforcement Learning timing synchronizer"
      },
      {
        "paperId": "d16d3bc736c2de76ef56fdf032bb4c3cf49476a1",
        "title": "Aging state prediction for supercapacitors based on heuristic kalman filter optimization extreme learning machine"
      },
      {
        "paperId": "c7303ad335f31bf8154bd84a9440d84ffe61b15c",
        "title": "Experience Weighted Learning in Multiagent Systems"
      },
      {
        "paperId": "83249a4ac9bdd517da7ce406ba0e8819b328dd9b",
        "title": "A reinforcement learning brain storm optimization algorithm (BSO) with learning mechanism"
      },
      {
        "paperId": "708e4b7344d12f02d26b701dcd8bdebd842bf269",
        "title": "Stacked bidirectional LSTM RNN to evaluate the remaining useful life of supercapacitor"
      },
      {
        "paperId": "81b3490b8abc0802ac3f9c155e4938defcc8cc94",
        "title": "A Contextual Reinforcement Learning Approach for Electricity Consumption Forecasting in Buildings"
      },
      {
        "paperId": "6f6b0ce0892aa0591808f533350e621906457d30",
        "title": "A Cooperative Multi-Agent Reinforcement Learning Method Based on Coordination Degree"
      }
    ],
    "score": 2.0
  },
  {
    "id": "bd38cbbb346a347cb5b60ac4a133b3d73cb44e07",
    "title": "Efficient Online Reinforcement Learning with Offline Data",
    "authors": [
      "Philip J. Ball",
      "Laura M. Smith",
      "Ilya Kostrikov",
      "S. Levine"
    ],
    "year": 2023,
    "citationCount": 220,
    "abstract": "Sample efficiency and exploration remain major challenges in online reinforcement learning (RL). A powerful approach that can be applied to address these issues is the inclusion of offline data, such as prior trajectories from a human expert or a sub-optimal exploration policy. Previous methods have relied on extensive modifications and additional complexity to ensure the effective use of this data. Instead, we ask: can we simply apply existing off-policy methods to leverage offline data when learning online? In this work, we demonstrate that the answer is yes; however, a set of minimal but important changes to existing off-policy RL algorithms are required to achieve reliable performance. We extensively ablate these design choices, demonstrating the key factors that most affect performance, and arrive at a set of recommendations that practitioners can readily apply, whether their data comprise a small number of expert demonstrations or large volumes of sub-optimal trajectories. We see that correct application of these simple recommendations can provide a $\\mathbf{2.5\\times}$ improvement over existing approaches across a diverse set of competitive benchmarks, with no additional computational overhead. We have released our code at https://github.com/ikostrikov/rlpd.",
    "url": "https://www.semanticscholar.org/paper/bd38cbbb346a347cb5b60ac4a133b3d73cb44e07",
    "pdf_url": "https://arxiv.org/pdf/2302.02948.pdf",
    "venue": "International Conference on Machine Learning",
    "publicationDate": "2023-02-06",
    "externalIds": {
      "DBLP": "journals/corr/abs-2302-02948",
      "ArXiv": "2302.02948",
      "DOI": "10.48550/arXiv.2302.02948",
      "CorpusId": 256615834
    },
    "references": [
      {
        "paperId": "7f270c9b098727675c9d8b893e362b561d61f27e",
        "title": "Policy Expansion for Bridging Offline-to-Online Reinforcement Learning"
      },
      {
        "paperId": "c91b5677ecb034a6a2ca69509b604965c776d5d6",
        "title": "On Pathologies in KL-Regularized Reinforcement Learning from Expert Demonstrations"
      },
      {
        "paperId": "b2004f4f19bc6ccae8e4afc554f39142870100f5",
        "title": "MoDem: Accelerating Visual Model-Based Reinforcement Learning with Demonstrations"
      },
      {
        "paperId": "6d7e11f7cb280df884e017002d2e4eb30985629e",
        "title": "Leveraging Offline Data in Online Reinforcement Learning"
      },
      {
        "paperId": "2d33f309f7e92c75434a2bb16f70d6ec65ab7d2a",
        "title": "Hybrid RL: Using Both Offline and Online Data Can Make RL Efficient"
      },
      {
        "paperId": "02d9dc238ae825e45e728607c3c83b77d07f4017",
        "title": "Stabilizing Off-Policy Deep Reinforcement Learning from Pixels"
      },
      {
        "paperId": "bbae3200de2d742b2bdcecab51f40a8dccb228cb",
        "title": "Challenges and Opportunities in Offline Reinforcement Learning from Visual Observations"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "62272403114c67a85e6fde9e428334d89e143485",
        "title": "AW-Opt: Learning Robotic Skills with Imitation and Reinforcement at Scale"
      },
      {
        "paperId": "348a855fe01f3f4273bf0ecf851ca688686dbfcc",
        "title": "Offline Reinforcement Learning with Implicit Q-Learning"
      },
      {
        "paperId": "b2d931da61559c528c5d4eadcb939425a2531652",
        "title": "Dropout Q-Functions for Doubly Efficient Reinforcement Learning"
      },
      {
        "paperId": "e06c005e98281af455c454ce2478285f6f3afeca",
        "title": "Mastering Visual Continuous Control: Improved Data-Augmented Reinforcement Learning"
      },
      {
        "paperId": "33b456eb43e5391761540f17a29e598d7595565b",
        "title": "Offline-to-Online Reinforcement Learning via Balanced Replay and Pessimistic Q-Ensemble"
      },
      {
        "paperId": "399806e861a2ef960a81b37b593c2176a728c399",
        "title": "Offline Reinforcement Learning as Anti-Exploration"
      },
      {
        "paperId": "07aa07cb6791f336cb32643d54201eab295b2757",
        "title": "A graph placement methodology for fast chip design"
      },
      {
        "paperId": "31eaf8fe40e07458911ebf4340e0c83f3e7d4c9e",
        "title": "Tactical Optimism and Pessimism for Deep Reinforcement Learning"
      },
      {
        "paperId": "639b555c90d00c920de9119bd504e0ffd14cab6b",
        "title": "OffCon3: What is state of the art anyway?"
      },
      {
        "paperId": "736590f70e7f2dc464c1c62491cfa8adb4d718f3",
        "title": "Randomized Ensembled Double Q-Learning: Learning Fast Without a Model"
      },
      {
        "paperId": "0272b14dd471fe7b81df703af1b71d7600b77215",
        "title": "Accelerating Online Reinforcement Learning with Offline Datasets"
      },
      {
        "paperId": "5e7bc93622416f14e6948a500278bfbe58cd3890",
        "title": "Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems"
      },
      {
        "paperId": "f1697ce4dddb58533d7d3f937fed74807d46edb8",
        "title": "Implementation Matters in Deep RL: A Case Study on PPO and TRPO"
      },
      {
        "paperId": "6568423cfaca7e24c88ea208cb0e67129e43aa9b",
        "title": "Image Augmentation Is All You Need: Regularizing Deep Reinforcement Learning from Pixels"
      },
      {
        "paperId": "a326d9f2d2d351001fece788165dbcbb524da2e4",
        "title": "D4RL: Datasets for Deep Data-Driven Reinforcement Learning"
      },
      {
        "paperId": "78aea5a51feb90e988a4d00302616e56d1be5eb0",
        "title": "Scaling data-driven robotics with reward sketching and batch reinforcement learning"
      },
      {
        "paperId": "12c0751b4f51ed833172a713b7e32390032ead93",
        "title": "Soft Actor-Critic Algorithms and Applications"
      },
      {
        "paperId": "5285cb8faada5de8a92a47622950f6cfd476ac1d",
        "title": "Off-Policy Deep Reinforcement Learning without Exploration"
      },
      {
        "paperId": "eb37e7b76d26b75463df22b2a3aa32b6a765c672",
        "title": "QT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation"
      },
      {
        "paperId": "4debb99c0c63bfaa97dd433bc2828e4dac81c48b",
        "title": "Addressing Function Approximation Error in Actor-Critic Methods"
      },
      {
        "paperId": "811df72e210e20de99719539505da54762a11c6d",
        "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor"
      },
      {
        "paperId": "c28ec2a40a2c77e20d64cf1c85dc931106df8e83",
        "title": "Overcoming Exploration in Reinforcement Learning with Demonstrations"
      },
      {
        "paperId": "e010ba3ff5744604cdbfe44a733e2a98649ee907",
        "title": "Learning Complex Dexterous Manipulation with Deep Reinforcement Learning and Demonstrations"
      },
      {
        "paperId": "33690ff21ef1efb576410e656f2e60c89d0307d6",
        "title": "Deep Reinforcement Learning that Matters"
      },
      {
        "paperId": "1bead9000a719cb258bac7320228055aee650d2c",
        "title": "Leveraging Demonstrations for Deep Reinforcement Learning on Robotics Problems with Sparse Rewards"
      },
      {
        "paperId": "e3b0ea7209731c47b582215c6c67f9c691ad9863",
        "title": "Deep Q-learning From Demonstrations"
      },
      {
        "paperId": "97fb4e3d45bb098e27e0071448b6152217bd35a5",
        "title": "Layer Normalization"
      },
      {
        "paperId": "846aedd869a00c09b40f1f1f35673cb22bc87490",
        "title": "Mastering the game of Go with deep neural networks and tree search"
      },
      {
        "paperId": "4a026fd65af4ba3575e64174de56fee093fa3330",
        "title": "Taming the Noise in Reinforcement Learning via Soft Updates"
      },
      {
        "paperId": "c6170fa90d3b2efede5a2e1660cb23e1c824f2ca",
        "title": "Prioritized Experience Replay"
      },
      {
        "paperId": "3b9732bb07dc99bde5e1f9f75251c6ea5039373e",
        "title": "Deep Reinforcement Learning with Double Q-Learning"
      },
      {
        "paperId": "024006d4c2a89f7acacc6e4438d156525b60a98f",
        "title": "Continuous control with deep reinforcement learning"
      },
      {
        "paperId": "340f48901f72278f6bf78a04ee5b01df208cc508",
        "title": "Human-level control through deep reinforcement learning"
      },
      {
        "paperId": "244539f454800697ed663326b7cfba337ca0c2ec",
        "title": "Guided Policy Search"
      },
      {
        "paperId": "43be33ef48e66d1f293c73af73f2f6753c6c392c",
        "title": "Agnostic System Identification for Model-Based Reinforcement Learning"
      },
      {
        "paperId": "41356b8998dd7ddf89429445320d82a269e3ab14",
        "title": "Tree-Based Batch Mode Reinforcement Learning"
      },
      {
        "paperId": "a9da09d1e63686706d64782e654d69f13fd292ad",
        "title": "Learning from Demonstration"
      },
      {
        "paperId": "09be45e87b5feef7963b6f2f5d3e5b3b86e2e170",
        "title": "Playback Control of Force Teachable Robots"
      },
      {
        "paperId": "bff20fb30adad8d1c173963089df5fc9664304f0",
        "title": "A Markovian Decision Process"
      },
      {
        "paperId": null,
        "title": "E cient deep reinforcement learning requires regulating statistical over tting"
      },
      {
        "paperId": "f6264b11ec0dd9133f1d88a5288a3267a93182f8",
        "title": "What Matters for On-Policy Deep Actor-Critic Methods? A Large-Scale Study"
      },
      {
        "paperId": "12d4edc029f90503e0bb0b1cfe4713d863af9b0f",
        "title": "Co-Adaptation of Algorithmic and Implementational Innovations in Inference-based Deep Reinforcement Learning"
      },
      {
        "paperId": null,
        "title": "JAX: composable transformations of Python+NumPy programs, 2018"
      },
      {
        "paperId": "34f25a8704614163c4095b3ee2fc969b60de4698",
        "title": "Dropout: a simple way to prevent neural networks from overfitting"
      },
      {
        "paperId": "26b8747eb4d7fb4d4fc45707606d5e969b9afb0c",
        "title": "Issues in Using Function Approximation for Reinforcement Learning"
      },
      {
        "paperId": null,
        "title": "Does the proposed workflow around environment-specific design choices lead to reliable performance ?"
      }
    ],
    "cited_by": [
      {
        "paperId": "26eb25c28ede4997af5c25c9dea0ee5847e00d4e",
        "title": "Fixing That Free Lunch: When, Where, and Why Synthetic Data Fails in Model-Based Policy Optimization"
      },
      {
        "paperId": "93ee5b548708ac07aaab6c3125f974cb574a2281",
        "title": "Robust Policy Expansion for Offline-to-Online RL under Diverse Data Corruption"
      },
      {
        "paperId": "779e0e9480cbeb689b7d7b9576af3cc4c6e8b523",
        "title": "Mash, Spread, Slice! Learning to Manipulate Object States via Visual Spatial Progress"
      },
      {
        "paperId": "a68008d17f84a69828f6f59694f77ca2cd7c7f53",
        "title": "Functional Critic Modeling for Provably Convergent Off-Policy Actor-Critic"
      },
      {
        "paperId": "c3fb25a2052832330b2851087f8a71559f125de2",
        "title": "Activation Function Design Sustains Plasticity in Continual Learning"
      },
      {
        "paperId": "0084a3b604df9d475e46e3124a6c3a3f4fc02a0f",
        "title": "Actor-Critic without Actor"
      },
      {
        "paperId": "c61186e5ffe02954118d97bf845e1d1af431c34d",
        "title": "Residual Off-Policy RL for Finetuning Behavior Cloning Policies"
      },
      {
        "paperId": "2083f2f7cf24d23793c0ba4495a407a4717f8db7",
        "title": "World4RL: Diffusion World Models for Policy Refinement with Reinforcement Learning for Robotic Manipulation"
      },
      {
        "paperId": "506ee2f4b5128a3d657cd3924699be40640afc9f",
        "title": "Lift What You Can: Green Online Learning with Heterogeneous Ensembles"
      },
      {
        "paperId": "543f87b15b888cb0a2381c5b36141256ad5ec84e",
        "title": "LLM-Guided Task- and Affordance-Level Exploration in Reinforcement Learning"
      },
      {
        "paperId": "ba1f70ea8347a229dd083d92c442b9eb3d2f89b6",
        "title": "Uncertainty-Based Smooth Policy Regularisation for Reinforcement Learning with Few Demonstrations"
      },
      {
        "paperId": "de8cba398fd0619ab07e6dcdfd6e5e8ff980aa81",
        "title": "SHaRe-RL: Structured, Interactive Reinforcement Learning for Contact-Rich Industrial Assembly Tasks"
      },
      {
        "paperId": "75a85fa71377ac3ad2ee514cc9750df19a4d2ba5",
        "title": "Solving Robotics Tasks with Prior Demonstration via Exploration-Efficient Deep Reinforcement Learning"
      },
      {
        "paperId": "3cdc2cf475e49537dde52b1ff95120916010e5d8",
        "title": "Offline-to-online reinforcement learning with efficient unconstrained fine-tuning."
      },
      {
        "paperId": "583335e322504d280e7292ed5a18159fcb6723a0",
        "title": "Adaptive Scaling of Policy Constraints for Offline Reinforcement Learning"
      },
      {
        "paperId": "c9e58c7983b70e0ea042ccdf3a89355580a9c532",
        "title": "Multi-Agent Reinforcement Learning in Intelligent Transportation Systems: A Comprehensive Survey"
      },
      {
        "paperId": "68fb126c402af3a3e3dc5904a3f72b1df4916070",
        "title": "Deep Sensorimotor Control by Imitating Predictive Models of Human Motion"
      },
      {
        "paperId": "804039e844664365265dca969ae612d32a992503",
        "title": "On-Policy RL Meets Off-Policy Experts: Harmonizing Supervised Fine-Tuning and Reinforcement Learning via Dynamic Weighting"
      },
      {
        "paperId": "a176836af1c4eae6765152d13e966bc3e9e698ec",
        "title": "SLA-MORL: SLA-Aware Multi-Objective Reinforcement Learning for HPC Resource Optimization"
      },
      {
        "paperId": "5fb99fd13a583f56ea37d283845fb49573bbb4d3",
        "title": "Scaling DRL for Decision Making: A Survey on Data, Network, and Training Budget Strategies"
      },
      {
        "paperId": "d03aa1cfcac80fbe246c6eff1d6af957684696c1",
        "title": "Generative Large-Scale Pre-trained Models for Automated Ad Bidding Optimization"
      },
      {
        "paperId": "9ec36122d6012f9bf1d886ed43c07c8e0490a424",
        "title": "Balancing Expressivity and Robustness: Constrained Rational Activations for Reinforcement Learning"
      },
      {
        "paperId": "5c94c89546b4e497300c05a72b652e61620521a1",
        "title": "Step-DAD: Semi-Amortized Policy-Based Bayesian Experimental Design"
      },
      {
        "paperId": "d5d223fa70cce2d34beb467aeaddbea51e8cd0ff",
        "title": "Relative Entropy Pathwise Policy Optimization"
      },
      {
        "paperId": "42128c5088dbb95a9068a128981eb0a2292739cd",
        "title": "Adaptability in Multi-Agent Reinforcement Learning: A Framework and Unified Review"
      },
      {
        "paperId": "8bede6b5fe990c815b1695a02862c6e012b95038",
        "title": "Penalizing Infeasible Actions and Reward Scaling in Reinforcement Learning with Offline Data"
      },
      {
        "paperId": "a98587acde8e1df366d7c4964b5a08ed979aefb2",
        "title": "Behavioral Exploration: Learning to Explore via In-Context Adaptation"
      },
      {
        "paperId": "b38af7b162f9c4bef03975f3832410b2456718fd",
        "title": "EXPO: Stable Reinforcement Learning with Expressive Policies"
      },
      {
        "paperId": "eafd15421ffb38deb1e5e3a54cf9ba2c07082cea",
        "title": "Reinforcement Learning with Action Chunking"
      },
      {
        "paperId": "0028aa0972d80f1f6bd48b8c5dfa24e871ef3261",
        "title": "SimLauncher: Launching Sample-Efficient Real-world Robotic Reinforcement Learning via Simulation Pre-training"
      },
      {
        "paperId": "7d291643bf9bfa74e3f1e8b014103dc81c85e361",
        "title": "Blending Supervised and Reinforcement Fine-Tuning with Prefix Sampling"
      },
      {
        "paperId": "d0a7ae1c648c05a3694fc1dae986b4555a319a77",
        "title": "Offline to Online Reinforcement Learning for Optimizing FACTS Setpoints"
      },
      {
        "paperId": "d341989bb259c35dc723c386ab45636a78613046",
        "title": "Parallels Between VLA Model Post-Training and Human Motor Learning: Progress, Challenges, and Trends"
      },
      {
        "paperId": "c2246ebb9a217c51a441b74bdd8e7e4141c6090c",
        "title": "Compliant Residual DAgger: Improving Real-World Contact-Rich Manipulation with Human Corrections"
      },
      {
        "paperId": "85ba08c7fbc43032704a25ee00f07363dacf27da",
        "title": "Steering Your Diffusion Policy with Latent Space Reinforcement Learning"
      },
      {
        "paperId": "1472a5bd7dedeee8c3306108118f467cb5c82cbc",
        "title": "Touch begins where vision ends: Generalizable policies for contact-rich manipulation"
      },
      {
        "paperId": "37483be6933d29635e6a5483ba3f1ee2b3dcafc8",
        "title": "MOORL: A Framework for Integrating Offline-Online Reinforcement Learning"
      },
      {
        "paperId": "6e19a3ed0f582faa74d3909aaf74d8997100739f",
        "title": "Reinforcement Learning via Implicit Imitation Guidance"
      },
      {
        "paperId": "51086329921c07bc10158dac823fcb3dbe70d070",
        "title": "A Smooth Sea Never Made a Skilled SAILOR: Robust Imitation via Learning to Search"
      },
      {
        "paperId": "877ba5c1c17a49c649315174abc120c9b2fc21b0",
        "title": "SLAC: Simulation-Pretrained Latent Action Space for Whole-Body Real-World RL"
      },
      {
        "paperId": "34d8817c85fd7f351e75a6bcbdd851a559877300",
        "title": "Trajectory First: A Curriculum for Discovering Diverse Policies"
      },
      {
        "paperId": "966423188dcdb22dcb29bfc08890f32b7791e83a",
        "title": "Agentic Episodic Control"
      },
      {
        "paperId": "face928230ea69c69e92f3f451a014f4304f2ef1",
        "title": "Bigger, Regularized, Categorical: High-Capacity Value Functions are Efficient Multi-Task Learners"
      },
      {
        "paperId": "501027fad9afcd2f8703a04f403d54e9b16ef2be",
        "title": "Composite Flow Matching for Reinforcement Learning with Shifted-Dynamics Data"
      },
      {
        "paperId": "cbbcc17e6c6bd3dc06fe55a9f4d4f2ee10fe6852",
        "title": "Scaling Offline RL via Efficient and Expressive Shortcut Models"
      },
      {
        "paperId": "0d2904745bec787d7504546101bdd935cfca0f98",
        "title": "Deep Actor-Critics with Tight Risk Certificates"
      },
      {
        "paperId": "c3f2c71e6aa8987b10f8d614ffbc1331b37b68bf",
        "title": "Learning to Trust Bellman Updates: Selective State-Adaptive Regularization for Offline RL"
      },
      {
        "paperId": "4fc36fdf827996a8f87cf013fb1798c2507cb573",
        "title": "URPlanner: A Universal Paradigm for Collision-Free Robotic Motion Planning Based on Deep Reinforcement Learning"
      },
      {
        "paperId": "78bc4554d365476493edd27c9bf9cc1f9fb2df2c",
        "title": "Learning the Contact Manifold for Accurate Pose Estimation During Peg-in-Hole Insertion of Complex Geometries"
      },
      {
        "paperId": "4a3e88d203564e547f5fb3f3d816a0b381492eae",
        "title": "VLA-RL: Towards Masterful and General Robotic Manipulation with Scalable Reinforcement Learning"
      },
      {
        "paperId": "15004bf94e73056d0fe3c661b56a36db917247b9",
        "title": "Automatic Reward Shaping from Confounded Offline Data"
      },
      {
        "paperId": "962eedb6cf3bd41f6481eb35030f7a2046521e4e",
        "title": "Improving the Data-efficiency of Reinforcement Learning by Warm-starting with LLM"
      },
      {
        "paperId": "9fa625528073fee005cf2d0fd2b2e9699a1af926",
        "title": "ReWiND: Language-Guided Rewards Teach Robot Policies without New Demonstrations"
      },
      {
        "paperId": "80fd717cccabc6c43bb8eabe79662058bff14371",
        "title": "What Matters for Batch Online Reinforcement Learning in Robotics?"
      },
      {
        "paperId": "bc4b173d49b4dcac8ebd6329fa5b48db954e50f1",
        "title": "SIMPLEMIX: Frustratingly Simple Mixing of Off- and On-policy Data in Language Model Preference Learning"
      },
      {
        "paperId": "5b6c5854dab618d3fed8b9d8c31f18bc6ebe493f",
        "title": "Implicit Neural-Representation Learning for Elastic Deformable-Object Manipulations"
      },
      {
        "paperId": "5a83b1754fc4acb4cbafed806ef432656ff62cd2",
        "title": "Fine-Tuning without Performance Degradation"
      },
      {
        "paperId": "a0f1d8ca443ea8e9e1572bea44b3eba7c7c354a5",
        "title": "HGAT and Multi-Agent RL-Based Method for Multi-Intersection Traffic Signal Control"
      },
      {
        "paperId": "a4bb8259b4c0178934d66bbd937e95eeb5b2fa4f",
        "title": "Vision intelligence-conditioned reinforcement learning for precision assembly"
      },
      {
        "paperId": "e54f2a86192ea5437e64107d6f4f3534b702ad8c",
        "title": "Next-Future: Sample-Efficient Policy Learning for Robotic-Arm Tasks"
      },
      {
        "paperId": "28e5d7362160fb7ea857bd9ee800bb3f857c5672",
        "title": "Enhanced Penalty-based Bidirectional Reinforcement Learning Algorithms"
      },
      {
        "paperId": "e8952ebc3ba4678d8c6192eb8c3c51ee032258d0",
        "title": "HACTS: a Human-As-Copilot Teleoperation System for Robot Learning"
      },
      {
        "paperId": "1a43cad99478b3f981b89fc757a481afa5433741",
        "title": "Robust Quantum Control using Reinforcement Learning from Demonstration"
      },
      {
        "paperId": "8453d2687f06f41a99d310845a9b47c917436aec",
        "title": "Robot Policy Transfer with Online Demonstrations: An Active Reinforcement Learning Approach"
      },
      {
        "paperId": "887ffb482d574cc053213f14da7a80006170e72d",
        "title": "Refined Policy Distillation: From VLA Generalists to RL Experts"
      },
      {
        "paperId": "56b695cbbd2e5784b25878328d0b5e57a0926591",
        "title": "A comparison of visual representations for real-world reinforcement learning in the context of vacuum gripping"
      },
      {
        "paperId": "462396a1cbef6a3b71fed11fd0e2da7c6d877984",
        "title": "Active Robot Curriculum Learning from Online Human Demonstrations"
      },
      {
        "paperId": "45439384b065b56f5b56140a1b40d1ca14d6ec31",
        "title": "A2Perf: Real-World Autonomous Agents Benchmark"
      },
      {
        "paperId": "33f24a2ac0248a72627b1ea42651e5b0700f34b6",
        "title": "Multi-Stage Manipulation with Demonstration-Augmented Reward, Policy, and World Model Learning"
      },
      {
        "paperId": "add637341443b4980988cd2096074f06019d6341",
        "title": "Subtask-Aware Visual Reward Learning from Segmented Demonstrations"
      },
      {
        "paperId": "95da947734f064da1be1f44b2903b0f3152ff214",
        "title": "Uncertainty Comes for Free: Human-in-the-Loop Policies with Diffusion Models"
      },
      {
        "paperId": "696188ba6b12122ba8b154dbeac7917097c013d6",
        "title": "Efficient Reinforcement Learning by Guiding Generalist World Models with Non-Curated Data"
      },
      {
        "paperId": "4831cf54b947bdb262ae3efeaa6909e271525fb4",
        "title": "Hyperspherical Normalization for Scalable Deep Reinforcement Learning"
      },
      {
        "paperId": "19b460d0065662f331c06ea05dbe8218d277e346",
        "title": "MILE: Model-Based Intervention Learning"
      },
      {
        "paperId": "406af78910c3aadacf43ea3286d912e452815eb0",
        "title": "Score-Based Diffusion Policy Compatible with Reinforcement Learning via Optimal Transport"
      },
      {
        "paperId": "e1f707f6f78024bf18ab812b3899f068f0f6bf10",
        "title": "Enhancing Offline Model-Based RL via Active Model Selection: A Bayesian Optimization Perspective"
      },
      {
        "paperId": "b8e3bd96ef6862e6a7af4dd41a72866630bb7b75",
        "title": "Balancing optimism and pessimism in offline-to-online learning"
      },
      {
        "paperId": "c0b68738f3e46e7b9dacbe566f0c31104a78b616",
        "title": "Active Advantage-Aligned Online Reinforcement Learning with Offline Data"
      },
      {
        "paperId": "bc753d43f8ecf716b78399b77b3eca1e5df40ed7",
        "title": "Deep Reinforcement Learning based Triggering Function for Early Classifiers of Time Series"
      },
      {
        "paperId": "84fc714f22535f82c3fbfa28f2883292d2a02167",
        "title": "ConRFT: A Reinforced Fine-tuning Method for VLA Models via Consistency Policy"
      },
      {
        "paperId": "8dcc0a142aefec2d7addb91775e5d58ab4a6771f",
        "title": "Flow Q-Learning"
      },
      {
        "paperId": "289f55b66bd9dc7eebbd9f9d8a54b7f0b5705117",
        "title": "Rapidly Adapting Policies to the Real World via Simulation-Guided Fine-Tuning"
      },
      {
        "paperId": "c5c5a0d4cfe702e2c432789bb0034efdec7af678",
        "title": "Learning from Suboptimal Data in Continuous Control via Auto-Regressive Soft Q-Network"
      },
      {
        "paperId": "e861a51688b6fc086854161c50eef9b04cf11e02",
        "title": "Langevin Soft Actor-Critic: Efficient Exploration through Uncertainty-Driven Critic Learning"
      },
      {
        "paperId": "6f1172eaa5e2a4e7623fccf1acaa3224b367ccc5",
        "title": "Capability-Aware Shared Hypernetworks for Flexible Heterogeneous Multi-Robot Coordination"
      },
      {
        "paperId": "049b75803aea2ac63e17fc4e645aaba03bb3aced",
        "title": "SR-Reward: Taking The Path More Traveled"
      },
      {
        "paperId": "cb52ed456c02472f7de5df58645fde7c2e8e8613",
        "title": "Optimistic Critic Reconstruction and Constrained Fine-Tuning for General Offline-to-Online RL"
      },
      {
        "paperId": "466b7a6e2b12330b6bb23f4af64732b4e77ca794",
        "title": "Policy Decorator: Model-Agnostic Online Refinement for Large Policy Model"
      },
      {
        "paperId": "3c801df470245d404c61a0eb79645e1aeca08c82",
        "title": "Stealing That Free Lunch: Exposing the Limits of Dyna-Style Reinforcement Learning"
      },
      {
        "paperId": "d4cc92cb197f65e3adfa39cb162acf82ecc30459",
        "title": "Stabilizing Reinforcement Learning in Differentiable Multiphysics Simulation"
      },
      {
        "paperId": "dff2c605fbd859f6c3049721194e3af3083a0927",
        "title": "RLDG: Robotic Generalist Policy Distillation via Reinforcement Learning"
      },
      {
        "paperId": "c2c0e1ec3e006ebd6533aae98131128dc1358f0d",
        "title": "Efficient Online Reinforcement Learning Fine-Tuning Need Not Retain Offline Data"
      },
      {
        "paperId": "f0096167c046fbe15e8dbe75356f7d876c04f600",
        "title": "Ameliorating Learning Stability by Regularizing Soft Actor-Critic"
      },
      {
        "paperId": "5c601530f798283890635475e80f57a5e5f69642",
        "title": "ManiSkill-HAB: A Benchmark for Low-Level Manipulation in Home Rearrangement Tasks"
      },
      {
        "paperId": "72efc47152909885e372ad71f22477e3b6d53b67",
        "title": "Policy Agnostic RL: Offline RL and Online RL Fine-Tuning of Any Class and Backbone"
      },
      {
        "paperId": "6148f01b9f0a0f7569157f279107a4f20a492adb",
        "title": "Disentangled Task Representation Learning for Offline Meta Reinforcement Learning"
      },
      {
        "paperId": "8bc2c84200ae7857953679d650e9fafa65cbbbf6",
        "title": "Accelerating Proximal Policy Optimization Learning Using Task Prediction for Solving Environments with Delayed Rewards"
      },
      {
        "paperId": "936473caaa81a255837daec429745f50fe9b43e8",
        "title": "Beyond The Rainbow: High Performance Deep Reinforcement Learning On A Desktop PC"
      },
      {
        "paperId": "2302faac0a41449bfa92f1edbf041ea83ae872be",
        "title": "So You Think You Can Scale Up Autonomous Robot Data Collection?"
      },
      {
        "paperId": "d229676d9ef80fb265cac03ebd2f001b78d950cc",
        "title": "Teaching Embodied Reinforcement Learning Agents: Informativeness and Diversity of Language Use"
      },
      {
        "paperId": "0e1a3c83fa7184211ee331a5fece022424bbe65d",
        "title": "Scalable Reinforcement Post-Training Beyond Static Human Prompts: Evolving Alignment via Asymmetric Self-Play"
      },
      {
        "paperId": "df6c20aee8d31bbfd024d2051e728990bd73f2c1",
        "title": "Adaptive Policy Regularization for Offline-to-Online Reinforcement Learning in HVAC Control"
      },
      {
        "paperId": "38646e980a9665b44bd14f0559d6ecfdfc4ce22c",
        "title": "Offline-to-Online Multi-Agent Reinforcement Learning with Offline Value Function Memory and Sequential Exploration"
      },
      {
        "paperId": "b718711020853736dd1020e2101aeae6a3a963fa",
        "title": "On-Robot Reinforcement Learning with Goal-Contrastive Rewards"
      },
      {
        "paperId": "9eb49d092908be1cd2056ae87351dbfb3d6c838d",
        "title": "O2OAT: Efficient Offline-to-Online Reinforcement Learning with Adaptive Transition Strategy"
      },
      {
        "paperId": "b2203c382daacadf88c02c0a635c466e777e4ff7",
        "title": "SAMG: Offline-to-Online Reinforcement Learning via State-Action-Conditional Offline Model Guidance"
      },
      {
        "paperId": "3775bd14050cd41b32e518d2f34a85125680b2dd",
        "title": "Leveraging Skills from Unlabeled Prior Data for Efficient Online Exploration"
      },
      {
        "paperId": "428524ee288cc9cd942be246b3984f382f4e5b8f",
        "title": "Solving Continual Offline RL through Selective Weights Activation on Aligned Spaces"
      },
      {
        "paperId": "6f6e7a84bbc63dada1e9472482d0e94454a28223",
        "title": "Offline-to-online Reinforcement Learning for Image-based Grasping with Scarce Demonstrations"
      },
      {
        "paperId": "d740f46d5eaeca56c5dc29917f8e61426c7d6101",
        "title": "Online Reinforcement Learning with Passive Memory"
      },
      {
        "paperId": "95e2e406f160664b0ef6326772bdf43e8e96975b",
        "title": "Traversability-Aware Legged Navigation by Learning from Real-World Visual Data"
      },
      {
        "paperId": "697487c80d9aac7e59454634831c6b321906f993",
        "title": "MAD-TD: Model-Augmented Data stabilizes High Update Ratio RL"
      },
      {
        "paperId": "6116f4ce4ec1b3cda988e7b289eda6daea75f479",
        "title": "ManiSkill3: GPU Parallelized Robotics Simulation and Rendering for Generalizable Embodied AI"
      },
      {
        "paperId": "67e621e2869b3ace4cff7e94c08d903304d3f70c",
        "title": "Continuously Improving Mobile Manipulation with Autonomous Real-World RL"
      },
      {
        "paperId": "3844004bf8e2a5cf8b4f812f462de02634b68c59",
        "title": "Task-agnostic Pre-training and Task-guided Fine-tuning for Versatile Diffusion Planner"
      },
      {
        "paperId": "979935ec9e7a8812028786cc7324ed03a37da48e",
        "title": "SoloParkour: Constrained Reinforcement Learning for Visual Locomotion from Privileged Experience"
      },
      {
        "paperId": "cd3e162970393d1d53a602735e1ae0296f16dd1e",
        "title": "The Role of Deep Learning Regularizations on Actors in Offline RL"
      },
      {
        "paperId": "843d8906ab712b14fb4590404c939d3319aa98e6",
        "title": "Goal-Reaching Policy Learning from Non-Expert Observations via Effective Subgoal Guidance"
      },
      {
        "paperId": "1faf55bb3f3ac058c489f0f7776bbdefe8439671",
        "title": "Enhancing Sample Efficiency and Exploration in Reinforcement Learning through the Integration of Diffusion Models and Proximal Policy Optimization"
      },
      {
        "paperId": "e596c98260ec4096eaeb491eb75f91a8339fcf48",
        "title": "Diffusion Policy Policy Optimization"
      },
      {
        "paperId": "042deff1b1011d3734ac6f1bef35c55e2f22e440",
        "title": "A Survey of Reinforcement Learning for Optimization in Automation"
      },
      {
        "paperId": "da99c68367f3892231e219abd01f46a4210a9a56",
        "title": "Unsupervised-to-Online Reinforcement Learning"
      },
      {
        "paperId": "a8b94ecd35c2eaeedb9fc1244a5726ec90805580",
        "title": "Hybrid Training for Enhanced Multi-task Generalization in Multi-agent Reinforcement Learning"
      },
      {
        "paperId": "e5d6cca71ea0fb216a25f86e96d3480886fdba27",
        "title": "D5RL: Diverse Datasets for Data-Driven Deep Reinforcement Learning"
      },
      {
        "paperId": "24c432b83bf30da211babaca2402d21a51388d95",
        "title": "Hybrid Reinforcement Learning Breaks Sample Size Barriers in Linear MDPs"
      },
      {
        "paperId": "374b00384c80b76d0d1134ca1b5bd3c24a1134a4",
        "title": "PLANRL: A Motion Planning and Imitation Learning Framework to Bootstrap Reinforcement Learning"
      },
      {
        "paperId": "93b41399a5d733db856ade26d9947d4c49b77850",
        "title": "Jacta: A Versatile Planner for Learning Dexterous and Whole-body Manipulation"
      },
      {
        "paperId": "eaf7355764a2126b864215b6e3bf408eb315fe02",
        "title": "Language-Conditioned Offline RL for Multi-Robot Navigation"
      },
      {
        "paperId": "d8f75adfc48528833a6fc5fa0dcc6e856016a243",
        "title": "From Imitation to Refinement - Residual Rl for Precise Assembly"
      },
      {
        "paperId": "db36d71a93f0da577db779dc65672fe7d4c18ccf",
        "title": "Energy-Guided Diffusion Sampling for Offline-to-Online Reinforcement Learning"
      },
      {
        "paperId": "256ff3d45f69cceec7bc100552fed9def405026d",
        "title": "Green Screen Augmentation Enables Scene Generalisation in Robotic Manipulation"
      },
      {
        "paperId": "25908892225280f00bbef3fe555e2683ed50fd72",
        "title": "Continuous Control with Coarse-to-fine Reinforcement Learning"
      },
      {
        "paperId": "6e82030c30b553cca4bb0347e9542e9ef6c1942e",
        "title": "HiLMa-Res: A General Hierarchical Framework via Residual RL for Combining Quadrupedal Locomotion and Manipulation"
      },
      {
        "paperId": "e7c70877bc10d7d430f67489da78b227ac08a17a",
        "title": "Normalization and effective learning rates in reinforcement learning"
      },
      {
        "paperId": "ee9e4546012e53ba40d929569090d9354033a3ae",
        "title": "Model-based Offline Reinforcement Learning with Lower Expectile Q-Learning"
      },
      {
        "paperId": "69bf7b644b4ca13f98d4d842a83ca61c3e87e8ab",
        "title": "Data-Driven Reinforcement Learning for Optimal Motor Control in Washing Machines"
      },
      {
        "paperId": "3565ed1d884fcd6b7d20f9d410c6bbb871f90b0c",
        "title": "EXTRACT: Efficient Policy Learning by Extracting Transferrable Robot Skills from Offline Data"
      },
      {
        "paperId": "a34a3f7645d261685af62e30ee31263b7ddc6083",
        "title": "Optimal Energy Scheduling of a Microgrid based on Offline-to-Online Deep Reinforcement Learning"
      },
      {
        "paperId": "26c1e857c64036767abcd4ca422d60545779ee4a",
        "title": "Efficient Offline Reinforcement Learning: The Critic is Critical"
      },
      {
        "paperId": "c18768fc623a57b54a5cccbc7a5b1c0805ecf36b",
        "title": "Hybrid Reinforcement Learning from Offline Observation Alone"
      },
      {
        "paperId": "ed7d213de959004feab13b1f713a6116a9dfa320",
        "title": "ATraDiff: Accelerating Online Reinforcement Learning with Imaginary Trajectories"
      },
      {
        "paperId": "1a4238a1dd6068f59b3daa8923a6612a2f98cd12",
        "title": "DEER: A Delay-Resilient Framework for Reinforcement Learning with Variable Delays"
      },
      {
        "paperId": "8be97edf8093965360d952ba3983f46300a8a6c8",
        "title": "RL in Latent MDPs is Tractable: Online Guarantees via Off-Policy Evaluation"
      },
      {
        "paperId": "e825e6325dfb5b93066817e7cc6b226ba7d3b799",
        "title": "Bayesian Design Principles for Offline-to-Online Reinforcement Learning"
      },
      {
        "paperId": "e1ba406f477df54d747a77b40781b07f837ff33c",
        "title": "Offline-Boosted Actor-Critic: Adaptively Blending Optimal Historical Behaviors in Deep Off-Policy RL"
      },
      {
        "paperId": "cdf9897ed964180631e5b3752efde29369642e6e",
        "title": "Bigger, Regularized, Optimistic: scaling for compute and sample-efficient continuous control"
      },
      {
        "paperId": "9c20ea01a0787f4b458d92081d62328d8ba6817b",
        "title": "Which Experiences Are Influential for RL Agents? Efficiently Estimating The Influence of Experiences"
      },
      {
        "paperId": "0979e3fe644878519f2c168b26136b8d56a75af7",
        "title": "An Efficient Learning Control Framework With Sim-to-Real for String-Type Artificial Muscle-Driven Robotic Systems"
      },
      {
        "paperId": "8532475afc69e55782bfb2da6fbe00c8537fcc08",
        "title": "Ensemble Successor Representations for Task Generalization in Offline-to-Online Reinforcement Learning"
      },
      {
        "paperId": "0fecc3a349cf4bc0f430cb8b9a69e8540ee10905",
        "title": "Reverse Forward Curriculum Learning for Extreme Sample and Demonstration Efficiency in Reinforcement Learning"
      },
      {
        "paperId": "a8ad39fc162c238b5c126a2d350d00dd7ab1ba87",
        "title": "Overcoming Knowledge Barriers: Online Imitation Learning from Visual Observation with Pretrained World Models"
      },
      {
        "paperId": "55905d61566de1db192b8f7b27c93483d212eb1c",
        "title": "Redundant Space Manipulator Autonomous Guidance for In-Orbit Servicing via Deep Reinforcement Learning"
      },
      {
        "paperId": "59516436839bb0dd90eee34e913bb9306f383619",
        "title": "REBEL: Reinforcement Learning via Regressing Relative Rewards"
      },
      {
        "paperId": "f036c481cb86c96228c66e2e3e9778ba4730435c",
        "title": "Dataset Reset Policy Optimization for RLHF"
      },
      {
        "paperId": "ed8246ebe105c0b52441fb5e25efbc3260354605",
        "title": "Sequential Decision Making with Expert Demonstrations under Unobserved Heterogeneity"
      },
      {
        "paperId": "033b96503d85082445fc724b91d5ab252934418f",
        "title": "Demonstration Guided Multi-Objective Reinforcement Learning"
      },
      {
        "paperId": "d429e4078f35be84308138158bad5b2c19a1eca8",
        "title": "Learning Off-policy with Model-based Intrinsic Motivation For Active Online Exploration"
      },
      {
        "paperId": "1a8254800afd2de64eb3451d7db1a56ad4c637e1",
        "title": "Bootstrapping Reinforcement Learning with Imitation for Vision-Based Agile Flight"
      },
      {
        "paperId": "384b0c06c5dcdb986876868781e67bf1f57469a9",
        "title": "Dissecting Deep RL with High Update Ratios: Combatting Value Divergence"
      },
      {
        "paperId": "400abf91d03681d27dfbe6263928e6d55664d914",
        "title": "Reconciling Reality through Simulation: A Real-to-Sim-to-Real Approach for Robust Manipulation"
      },
      {
        "paperId": "bccba48dc80c48e873964eb2750ff8ce9b95d662",
        "title": "A Case for Validation Buffer in Pessimistic Actor-Critic"
      },
      {
        "paperId": "dcf39ce4ea7c1d073dcd0dbb131a0b90a4bd2977",
        "title": "Overestimation, Overfitting, and Plasticity in Actor-Critic: the Bitter Lesson of Reinforcement Learning"
      },
      {
        "paperId": "598fdcc2550c63104c94728491af6b6f008e2314",
        "title": "Hybrid Inverse Reinforcement Learning"
      },
      {
        "paperId": "40c52f7a1df09143f7cc4d686e3d78e9e0f77f21",
        "title": "Fine-tuning Reinforcement Learning Models is Secretly a Forgetting Mitigation Problem"
      },
      {
        "paperId": "5aec9a5815dd428de29887182d0c90635b998e28",
        "title": "Deep Exploration with PAC-Bayes"
      },
      {
        "paperId": "3c1fba46a4272d160df80178799c41b065f88296",
        "title": "Adaptive Q-Aid for Conditional Supervised Learning in Offline Reinforcement Learning"
      },
      {
        "paperId": "fc4d4a25aac973b516c0bf890e36660fe228e79a",
        "title": "SERL: A Software Suite for Sample-Efficient Robotic Reinforcement Learning"
      },
      {
        "paperId": "1014bf18c16799577bf5899ad5b7a0e4972c99a5",
        "title": "Efficient Sparse-Reward Goal-Conditioned Reinforcement Learning with a High Replay Ratio and Regularization"
      },
      {
        "paperId": "71781e2011b174678cbdba1145e7ca49c94c892c",
        "title": "Stable Online and Offline Reinforcement Learning for Antibody CDRH3 Design"
      },
      {
        "paperId": "65731c720d2575ced7fa5134d408cffdbfce37ca",
        "title": "Replay across Experiments: A Natural Extension of Off-Policy RL"
      },
      {
        "paperId": "596bd637e6bab16058759ebc28aba2dbc5e19fed",
        "title": "Reinforcement Learning from Diffusion Feedback: Q* for Image Search"
      },
      {
        "paperId": "738b8695e52756833aaf4b25cf230c44f87e6e0c",
        "title": "RLIF: Interactive Imitation Learning as Reinforcement Learning"
      },
      {
        "paperId": "b19682f1a6f1cbd023166476d6f33e5a61ef7571",
        "title": "Offline Data Enhanced On-Policy Policy Gradient with Provable Guarantees"
      },
      {
        "paperId": "05603c65d813105eb136e867abfc412ea1735cf0",
        "title": "Accelerating Exploration with Unlabeled Prior Data"
      },
      {
        "paperId": "7c8c126558a986638e0548ad67252936ba496092",
        "title": "Uni-O4: Unifying Online and Offline Deep Reinforcement Learning with Multi-Step On-Policy Optimization"
      },
      {
        "paperId": "00f776a588e13e4c9278e91f7c10c536f6dc8c2d",
        "title": "Imitation Bootstrapped Reinforcement Learning"
      },
      {
        "paperId": "4f8a2a75b9465a0019fda0836b10cf3a52c9debc",
        "title": "Adapt On-the-Go: Behavior Modulation for Single-Life Robot Deployment"
      },
      {
        "paperId": "3d3bbaf203e55e681ed1c6f59181abacff2b068c",
        "title": "On the Theory of Risk-Aware Agents: Bridging Actor-Critic and Economics"
      },
      {
        "paperId": "e3b365cc2ea3a6c82eaf9428a5e7a716aa54da72",
        "title": "Unsupervised Behavior Extraction via Random Intent Priors"
      },
      {
        "paperId": "90df9c970aa98aa49bcf79e252ab26b1bb6889c5",
        "title": "Bootstrap Your Own Skills: Learning to Solve New Tasks with Large Language Model Guidance"
      },
      {
        "paperId": "8ed854f2329195009708bc560e8da4381fec0c39",
        "title": "Offline Retraining for Online RL: Decoupled Policy Learning to Mitigate Exploration Bias"
      },
      {
        "paperId": "60bbf59000ee42a30c03a50e151c45aae2c283fc",
        "title": "PolyTask: Learning Unified Policies through Behavior Distillation"
      },
      {
        "paperId": "ebe9258d1a5b6b212c2a7b9738a2f18f7b1f1dbe",
        "title": "Planning to Go Out-of-Distribution in Offline-to-Online Reinforcement Learning"
      },
      {
        "paperId": "9f923edcb6b8281d178c398b72f22190ba8247bf",
        "title": "Understanding, Predicting and Better Resolving Q-Value Divergence in Offline-RL"
      },
      {
        "paperId": "b208f4e13a30c4e575c8f084f2e438b88c51034f",
        "title": "Reinforcement Learning with Foundation Priors: Let Embodied Agent Efficiently Learn on Its Own"
      },
      {
        "paperId": "a6513a213269116444f779fb9136799f66718778",
        "title": "Consistency Models as a Rich and Efficient Policy Class for Reinforcement Learning"
      },
      {
        "paperId": "0de370a501f465a7b61a3cb4e9794e31cf0c528a",
        "title": "H2O+: An Improved Framework for Hybrid Offline-and-Online RL with Dynamics Gaps"
      },
      {
        "paperId": "643fbb1aaaa6d1875ab3c641f37209bd32eea71a",
        "title": "REBOOT: Reuse Data for Bootstrapping Efficient Real-World Dexterous Manipulation"
      },
      {
        "paperId": "ed2aacc2f147967f6306fbc316ee71d1a6e51047",
        "title": "Autonomy 2.0: The Quest for Economies of Scale"
      },
      {
        "paperId": "c99db0656d62e185486da6bcba6485f182307811",
        "title": "ReLU to the Rescue: Improve Your On-Policy Actor-Critic with Positive Advantages"
      },
      {
        "paperId": "1b5ff30f0f6bbf48373a691315b67208a0e1b38d",
        "title": "Cross-Domain Policy Adaptation via Value-Guided Data Filtering"
      },
      {
        "paperId": "2bd9c93edb1ac68ea8d45f6252209a1682bec757",
        "title": "PROTO: Iterative Policy Regularized Offline-to-Online Reinforcement Learning"
      },
      {
        "paperId": "1b6341b05471bb961b4e76467444d7b70c66bbb5",
        "title": "Revisiting the Minimalist Approach to Offline Reinforcement Learning"
      },
      {
        "paperId": "081390fc1bc34dca2a191266a0cd3e9f815c33df",
        "title": "What can online reinforcement learning with function approximation benefit from general coverage conditions?"
      },
      {
        "paperId": "a6f9fb141034a87ff9d627dc8a3ef31d0790c6ed",
        "title": "IDQL: Implicit Q-Learning as an Actor-Critic Method with Diffusion Policies"
      },
      {
        "paperId": "099c2f6509391246152fbb5c2cd8757dc164ed65",
        "title": "FastRLAP: A System for Learning High-Speed Driving via Deep RL and Autonomous Practicing"
      },
      {
        "paperId": "c40452c6d4337b76822ab62a762daa8f0b2acb6f",
        "title": "Selective Reincarnation: Offline-to-Online Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "5aeef5fc2533f8deeefb73688040279acad67e96",
        "title": "Bridging Imitation and Online Reinforcement Learning: An Optimistic Tale"
      },
      {
        "paperId": "90889ff0ce930528e65b1b761bf9bcf9195e5051",
        "title": "Synthetic Experience Replay"
      },
      {
        "paperId": "f5e1993f3f505e8fbb9cac9231285c8c9f1712a7",
        "title": "Cal-QL: Calibrated Offline RL Pre-Training for Efficient Online Fine-Tuning"
      },
      {
        "paperId": "2ebd5df74980a37370b0bcdf16deff958289c041",
        "title": "Foundation Models for Decision Making: Problems, Methods, and Opportunities"
      },
      {
        "paperId": "672ec9fa4ddb5b6bfc46c61c5b2f4bdfa1aa8ed9",
        "title": "Dual RL: Unification and New Methods for Reinforcement and Imitation Learning"
      },
      {
        "paperId": "6d7e11f7cb280df884e017002d2e4eb30985629e",
        "title": "Leveraging Offline Data in Online Reinforcement Learning"
      },
      {
        "paperId": "06da8bb52a89c69641fd8e0f0d699d8d5f279084",
        "title": "Mava: a research library for distributed multi-agent reinforcement learning in JAX"
      },
      {
        "paperId": "58b30fe15c8fe3603f3f032ed28de6df606aabe8",
        "title": "Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications"
      },
      {
        "paperId": "0a176ff2fcf08e852903a5bfa158e53a67c0b71a",
        "title": "Overcoming Knowledge Barriers: Online Imitation Learning from Observation with Pretrained World Models"
      },
      {
        "paperId": "aa42b63924737bce4e168c6450a960382450b506",
        "title": "Leveraging Offline Data in Linear Latent Bandits"
      },
      {
        "paperId": "a9b16e7729436387b1e51bd4095434ea6ffd9f43",
        "title": "Double Gumbel Q-Learning"
      },
      {
        "paperId": "b5a4303a1b0c8c822cc24bb9f2b08dae1246a836",
        "title": "Provable bene\ufb01ts of general coverage conditions in ef\ufb01cient online reinforcement learning with function approximation"
      },
      {
        "paperId": "67e07e0064f88503ded910ebf693b9c476209e07",
        "title": "Distributional Reinforcement Learning with Dual Expectile-Quantile Regression"
      },
      {
        "paperId": "ab83df0afa15fb5fcce8addc634314ea3a187055",
        "title": "Reduce, Reuse, Recycle: Selective Reincarnation in Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "0daca7fce024b8f207fd9be447030700f04eec80",
        "title": "FastRLAP: A System for Learning High-Speed Driving via Deep RL and Autonomous Practicing"
      },
      {
        "paperId": "7ae2c602a163d0afd05bfbd7bd1f8820133a227e",
        "title": "Interactive Imitation Learning as Reinforcement Learning"
      },
      {
        "paperId": "cce859de2e5b9a6a254b652cb0c2a34d1224b6b5",
        "title": "A UTOMATIC F INE -T UNED O FFLINE - TO -O NLINE R EINFORCEMENT L EARNING VIA I NCREASED S IMPLE M OVING A VERAGE Q-VALUE"
      },
      {
        "paperId": "307d05e85fe59377d000cff89df0d57863bca3bf",
        "title": "Accelerating Proximal Policy Optimization Learning Using Task Prediction for Solving Games with Delayed Rewards"
      },
      {
        "paperId": "8e22f1aba4e62044fe5bb9f1157213b53ede0251",
        "title": "Enhancing Data and Algorithm in Offline Reinforcement Learning"
      },
      {
        "paperId": "8564e17fa4ec36c687de1355390222c3e6e2d47a",
        "title": "LLM Alignment Through Successive Policy Re-weighting (SPR)"
      },
      {
        "paperId": "149daa710da4f7eeed40dab51ff1b27494dd924a",
        "title": "Safe and Efficient Offline Reinforcement Learning: The Critic is Critical"
      },
      {
        "paperId": "932c7b4467ad38670302e1621f1536c768ee7d63",
        "title": "Stabilizing Reinforcement Learning in Differentiable Simulation of Deformables"
      },
      {
        "paperId": "3eefc196a35195e6da9f1dcd5a77508d7c60523c",
        "title": "Reward-Guided Prompt Evolving in Reinforcement Learning for LLMs"
      }
    ],
    "score": 110.0
  },
  {
    "id": "1b1efa2f9731ab3801c46bfc877695d41e437406",
    "title": "An Online Reinforcement Learning-Based Energy Management Strategy for Microgrids With Centralized Control",
    "authors": [
      "Qing-ran Meng",
      "Sheharyar Hussain",
      "Fengzhang Luo",
      "Zhongguan Wang",
      "Xiaolong Jin"
    ],
    "year": 2025,
    "citationCount": 62,
    "abstract": "To address the issue of significant unpredictability and intermittent nature of renewable energy sources, particularly wind and solar power, this paper introduces a novel optimization model based on online reinforcement learning. Initially, an energy management optimization model is designed to achieve plan adherence and minimize energy storage (ES) operation costs, taking into account the inherent challenges of wind power-photovoltaic energy storage systems (WPESS). An online reinforcement learning framework is employed, which defines various state variables, action variables, and reward functions for the energy management optimization model. The state-action-reward-state-action (SARSA) algorithm is applied to learn the joint scheduling strategy for the microgrid system, utilizing its iterative exploration mechanisms and interaction with the environment. This strategy aims to accomplish the goals of effective power tracking and reduction of storage charging and discharging. The proposed method's effectiveness is validated using a residential community with electric vehicle (EV) charging loads as a test case. Numerical analyses demonstrate that the approach is not reliant on traditional mathematical models and adapts well to the uncertainties and complex constraints of the WPESS, maintaining a low-cost requirement while achieving computational efficiency significantly higher than that of the model predictive control (MPC) and deep Q-network (DQN) algorithm.",
    "url": "https://www.semanticscholar.org/paper/1b1efa2f9731ab3801c46bfc877695d41e437406",
    "pdf_url": "https://doi.org/10.1109/tia.2024.3430264",
    "venue": "IEEE transactions on industry applications",
    "publicationDate": "2025-01-01",
    "externalIds": {
      "DOI": "10.1109/tia.2024.3430264",
      "CorpusId": 271352158
    },
    "references": [],
    "cited_by": [
      {
        "paperId": "48a587b9207c81b91184df686ca47064fc9cdf63",
        "title": "Robust single-layer predictive control for grid-tied solar systems with integrated maximum power point tracking and power flow optimization"
      },
      {
        "paperId": "8a34090b03da6782155d5e8ff9ca44d1153333c2",
        "title": "Adaptive fuzzy-recurrent neural network tuned fractional-order distributed control for robust frequency regulation in multi-microgrid systems"
      },
      {
        "paperId": "3a57f2a0f0a15980fa15d30d46c80d1389610f0c",
        "title": "ZTFed-MAS2S: A Zero-Trust Federated Learning Framework with Verifiable Privacy and Trust-Aware Aggregation for Wind Power Data Imputation"
      },
      {
        "paperId": "33b732d3d7e5b285c622d4934476a732e7f5a535",
        "title": "Distributed collaborative event-triggering mechanism based optimal power regulator design for multiarea integrated energy systems with communication constraints"
      },
      {
        "paperId": "8239ebabcdec2a6f14b01daeeb086f508fc40e31",
        "title": "A Review of Optimization Strategies for Energy Management in Microgrids"
      },
      {
        "paperId": "b1b8abf3862f980799c7617b312f82d02db5dfdf",
        "title": "Constraint-Guided Prediction Refinement via Deterministic Diffusion Trajectories"
      },
      {
        "paperId": "6177e1ec1ebee6ace2f8c2033c39c064d3479989",
        "title": "Generalized distributed learning-based control for microgrid clusters with spatiotemporal uncertainty"
      },
      {
        "paperId": "1dfd0445cfea8a56e8784d454f1f4678e64c99fb",
        "title": "A Lyapunov Drift-Plus-Penalty Method Tailored for Reinforcement Learning with Queue Stability"
      },
      {
        "paperId": "5696566c5533f6db7e0e644ef0b63de8f898b81a",
        "title": "A GUI Application for Real-Time Home Energy Management Using Uncertainty-Aware Proximal Policy Optimization with Energy Storage, Electric Vehicle, and Renewables"
      },
      {
        "paperId": "71cd62a89d6efeeee1b2bad53f784a0fc29ede5d",
        "title": "Combining meta reinforcement learning with neural plasticity mechanisms for improved AI performance"
      },
      {
        "paperId": "b25def8b8e252175678873297e63be32a169812c",
        "title": "Sand Cat Swarm Optimization and Attention\u2010Based Graph Convolutional Neural Network for Energy Management Analysis of Grid\u2010Connected Hybrid Wind\u2010Microturbine\u2010Photovoltaic\u2010Electric Vehicle Systems"
      },
      {
        "paperId": "c026270b44fd03e393cf3d1e950012d5d78f8842",
        "title": "Integrated DDPG-PSO Energy Management Systems for Enhanced Battery Cycling and Efficient Grid Utilization"
      },
      {
        "paperId": "d6bfc40ac3994f4a0015af771ff14e4bbc116a48",
        "title": "Efficient Utilization of Energy in IoT Devices Using Machine Learning Algorithms"
      },
      {
        "paperId": "815881dc893d099ed70cd6d0c3c32786d6e16688",
        "title": "An Adaptive Scheduling Method for Standalone Microgrids Based on Deep Q-Network and Particle Swarm Optimization"
      },
      {
        "paperId": "46c6e00d6eef7c844e40e0c4c13bf0f74722aa22",
        "title": "Parametric Study of Adaptive Reinforcement Learning for Battery Operations in Microgrids"
      },
      {
        "paperId": "243cee8d8a7dad3ab0a2527dc5215dfe25cd7057",
        "title": "Dynamic Operating System Scheduling Using Double DQN: A Reinforcement Learning Approach to Task Optimization"
      },
      {
        "paperId": "7c3dda4ffa45a8fb6905055790dd886676764e10",
        "title": "Research on Cross-Circuitry Fault Identification Method for AC/DC Transmission System Based on Blind Signal Separation Algorithm"
      },
      {
        "paperId": "7d8b1ca40c120f4d2d9cb14135505a26e31308c4",
        "title": "Reinforcement Learning-Based Optimal Power Flow Control in Islanded Microgrids"
      },
      {
        "paperId": "346c419394c45537c269cc68a4d2a8195538cf74",
        "title": "Global progress towards the Coal: Tracking coal reserves, coal prices, electricity from coal, carbon emissions and coal phase-out"
      },
      {
        "paperId": "a70821af392523711bea9215c232057ea7d6f1a9",
        "title": "Co-optimization of Demand Response Aggregators and distribution system operator for resilient operation using machine learning based wind generation forecasting: A bilevel approach"
      },
      {
        "paperId": "ceba75c85c2a9ab3d242872ac138fea5baeef318",
        "title": "Nash-Stackelberg-Nash three-layer mixed game optimal control strategy for multi-integrated energy systems considering multiple uncertainties"
      },
      {
        "paperId": "6623a117f1b1b5c11ebc06d7be93c6368eae7417",
        "title": "A Preference-Based Online Reinforcement Learning With Embedded Communication Failure Solutions in Smart Grid"
      },
      {
        "paperId": "927b68d614da791f626c7800eb3dfc7afca938fd",
        "title": "Massive Coordination of Distributed Energy Resources in VPP: A Mean Field RL-Based Bi-Level Optimization Approach"
      },
      {
        "paperId": "1fde3bdfba6660d116ea1e22c609c3a414d7d865",
        "title": "Microgrid system for electric vehicle charging stations integrated with renewable energy sources using a hybrid DOA\u2013SBNN approach"
      },
      {
        "paperId": "8541b467b896e9f5e94f4494e79099d34e977614",
        "title": "A two-layer economic resilience model for distribution network restoration after natural disasters"
      },
      {
        "paperId": "a578b13da72f2f80f1f26591edddb9fe24231b7d",
        "title": "Efficient framework for energy management of microgrid installed in Aljouf region considering renewable energy and electric vehicles"
      },
      {
        "paperId": "5a7271cbce0fad515986c1a2adc371a1461100b9",
        "title": "Integration of smart buildings with high penetration of storage systems in isolated 100\u00a0% renewable microgrids"
      },
      {
        "paperId": "a903b962cebad46f7dfb8f41fcad4955240a2897",
        "title": "Transient biomass-SOFC-energy storage hybrid system for microgrids peak shaving based on optimized regulation strategy"
      },
      {
        "paperId": "648e1e50dbbc01c5022977c3c0e0a239fd91b0ed",
        "title": "Transactive Energy Management for Efficient Scheduling and Storage Utilization in a Grid-connected Renewable Energy-based Microgrid"
      },
      {
        "paperId": "65de8509e67a7f95c121b46450df9bc64cd58827",
        "title": "Real-Time Simulation Testbed for Energy Management and Control of Renewable Energy-Based Microgrids"
      },
      {
        "paperId": "8cf5840f6769b6c8035d2f658e91e1415d6d296a",
        "title": "Modeling of calculations on the performance optimization of a double-flash geothermal renewable energy-based combined heat/power plant coupled by transcritical carbon dioxide rankine cycle"
      },
      {
        "paperId": "dda97b1138f1335f63a98f5b2da1f3370747ad44",
        "title": "Optimizing Hydrogen-Integrated Micro-CHP Systems with Demand Response for Sustainable Energy Management"
      },
      {
        "paperId": "a28df0694350021f168a09fc1ec84ea979b0f0ca",
        "title": "A fuzzy-predictive current control with real-time hardware for PEM fuel cell systems"
      },
      {
        "paperId": "adbca3bdd02a07aabd882c3223c2ece64345b3e4",
        "title": "Optimizing Microgrid Systems with Energy Storage Using Advanced Machine Learning Techniques"
      },
      {
        "paperId": "f5e3edf7ab6d54aad9d0d41cda406b3620881c59",
        "title": "Impact of electric vehicles and wave energy systems on OPF of power networks using hybrid Osprey-PSO approach"
      },
      {
        "paperId": "c4506e2c9b3eabce2e9fa6e5084262f5f4371582",
        "title": "A multi-energy inertia-based coordinated voltage and frequency regulation in isolated hybrid power system using PI-TISMC"
      },
      {
        "paperId": "1d26a8f57e3d0b60db2cb4c9b98b6fc488361fc9",
        "title": "Optimal Energy Storage Allocation for Combined Wind-PV-EVs-ES System Based on Improved Triangulation Topology Aggregation Optimizer"
      },
      {
        "paperId": "2dd3c2e538efac163b23962b17abfa69104c5bcb",
        "title": "Innovative Energy Solutions: Evaluating Reinforcement Learning Algorithms for Battery Storage Optimization in Residential Settings"
      },
      {
        "paperId": "1639bb6dab81800e8a8cdf2e053669c574ffeb24",
        "title": "Resiliency-informed optimal scheduling of smart distribution network with urban distributed photovoltaic: A stochastic P-robust optimization"
      },
      {
        "paperId": "c4c859a58b0cb6e11672b171172bb767cfa426ef",
        "title": "An innovative 11-level multilevel inverter topology with rotating trapezoidal SPWM for industrial and renewable applications"
      },
      {
        "paperId": "f266a0f36b056e349f159abff08e5c090e5d6496",
        "title": "Design and development of different adaptive MPPT controllers for renewable energy systems: a comprehensive analysis"
      },
      {
        "paperId": "9d2f31fe2c026f3039b108982822c3c82af94b12",
        "title": "Online demand peak shaving with machine\u2010learned advice in digital twins"
      },
      {
        "paperId": "832130b457bf73863c649842226a3a528bbc7ff7",
        "title": "Real-time energy management simulation for enhanced integration of renewable energy resources in DC microgrids"
      },
      {
        "paperId": "d22d085661fd94515ce28e5badaf9d97169bfa5c",
        "title": "Operation of coupled power-gas networks with hydrogen refueling stations, responsive demands and storage systems: a novel stochastic framework"
      },
      {
        "paperId": "b1b9b3085286d611bd232c8987dab274632d70ed",
        "title": "Optimal energy management via day-ahead scheduling considering renewable energy and demand response in smart grids."
      },
      {
        "paperId": "89419d13fd57985839332cb206af07382ccf7274",
        "title": "Bridgeless modified Cuk\u2013SEPIC power factor correction converter for E-bicycle applications"
      },
      {
        "paperId": "8be5a1c356de368d842b58f3c8da9d788d237d5d",
        "title": "A novel development of advanced control approach for battery-fed electric vehicle systems"
      },
      {
        "paperId": "053b151f3d5b40c86a3fef874ed6f49f44569564",
        "title": "Finite control set model predictive current control for three phase grid connected inverter with common mode voltage suppression"
      },
      {
        "paperId": "fc3d6113ca90731d09689201603616d81fe0adbc",
        "title": "A hybrid deep learning approach to solve optimal power flow problem in hybrid renewable energy systems"
      },
      {
        "paperId": "5aef0e2e1c7e1eaecf459be4ed4b73c334293fe4",
        "title": "Machine learning-based energy management and power forecasting in grid-connected microgrids with multiple distributed energy sources"
      },
      {
        "paperId": "ecf51ad30f4f9ab8f54c3bb3c12fc2647c33c690",
        "title": "Chaotic self-adaptive sine cosine multi-objective optimization algorithm to solve microgrid optimal energy scheduling problems"
      },
      {
        "paperId": "97c1c357dde887751ae2be6d49159bc511c7609a",
        "title": "A novel development of wide voltage supply DC\u2013DC converter for fuel stack application with PSO-ANFIS MPPT controller"
      },
      {
        "paperId": "cf267efc17b2fdf8002fac6fe16dca28feeeb0e9",
        "title": "Revolutionizing photovoltaic consumption and electric vehicle charging: A novel approach for residential distribution systems"
      },
      {
        "paperId": "4427ac6b015c41c1f6055a23cd20ed725e6b4ccb",
        "title": "A high-efficiency poly-input boost DC\u2013DC converter for energy storage and electric vehicle applications"
      },
      {
        "paperId": "ce6d03414a8998d695b614680185a1cfef2eb7f4",
        "title": "Enhancing On-Grid Renewable Energy Systems: Optimal Configuration and Diverse Design Strategies"
      },
      {
        "paperId": "e5d8036c173ae45e2f2cfbd5038476132173b7c3",
        "title": "Data-driven study/optimization of a solar power and cooling generation system in a transient operation mode and proposing a novel multi-turbine modification concept to reduce the sun's intermittent effect"
      },
      {
        "paperId": "3987cfce9a50b83779ef259261a164212324e321",
        "title": "A Universal Source DC\u2013DC Boost Converter for PEMFC\u2010Fed EV Systems With Optimization\u2010Based MPPT Controller"
      },
      {
        "paperId": "2a0ab88ae407e477bccaf8a37344300265c3d270",
        "title": "Performance of an electromagnetic energy harvester with linear and nonlinear springs for low base accelerations"
      },
      {
        "paperId": "bbbb96db2e1e07889a576578ff4380ac6778a6f0",
        "title": "Analysing the impact of the different pricing policies on PV-battery systems: A Dutch case study of a residential microgrid"
      },
      {
        "paperId": "f47ce2549de2e59f15690568ece2446254bcff77",
        "title": "Adaptive PPO With Multi-Armed Bandit Clipping and Meta-Control for Robust Power Grid Operation Under Adversarial Attacks"
      },
      {
        "paperId": "0352eb07856b8abca6cfaf99deb624e67eee8a5a",
        "title": "Stabilization of interconnected nonlinear systems with multiplicative measurement noise: A decentralized aperiodic intermittent control strategy"
      },
      {
        "paperId": "d6fbc6b6ad15b9fe724ff5752c89253273c7d202",
        "title": "Boosting energy markets in grid with a focus on battery storage, including flexible loads and electric vehicles, via a two-stage optimization framework"
      }
    ],
    "score": 62.0
  },
  {
    "id": "08e84c939b88fc50aaa74ef76e202e61a1ad940b",
    "title": "StepCoder: Improve Code Generation with Reinforcement Learning from Compiler Feedback",
    "authors": [
      "Shihan Dou",
      "Yan Liu",
      "Haoxiang Jia",
      "Limao Xiong",
      "Enyu Zhou",
      "Junjie Shan",
      "Caishuang Huang",
      "Wei Shen",
      "Xiaoran Fan",
      "Zhiheng Xi",
      "Yuhao Zhou",
      "Tao Ji",
      "Rui Zheng",
      "Qi Zhang",
      "Xuanjing Huang",
      "Tao Gui"
    ],
    "year": 2024,
    "citationCount": 61,
    "abstract": "The advancement of large language models (LLMs) has significantly propelled the field of code generation. Previous work integrated reinforcement learning (RL) with compiler feedback for exploring the output space of LLMs to enhance code generation quality. However, the lengthy code generated by LLMs in response to complex human requirements makes RL exploration a challenge. Also, since the unit tests may not cover the complicated code, optimizing LLMs by using these unexecuted code snippets is ineffective. To tackle these challenges, we introduce StepCoder, a novel RL framework for code generation, consisting of two main components: CCCS addresses the exploration challenge by breaking the long sequences code generation task into a Curriculum of Code Completion Subtasks, while FGO only optimizes the model by masking the unexecuted code segments to provide Fine-Grained Optimization. In addition, we furthermore construct the APPS+ dataset for RL training, which is manually verified to ensure the correctness of unit tests. Experimental results show that our method improves the ability to explore the output space and outperforms state-of-the-art approaches in corresponding benchmarks. Our dataset APPS+ and StepCoder are available online.",
    "url": "https://www.semanticscholar.org/paper/08e84c939b88fc50aaa74ef76e202e61a1ad940b",
    "pdf_url": "https://arxiv.org/pdf/2402.01391.pdf",
    "venue": "arXiv.org",
    "publicationDate": "2024-02-02",
    "externalIds": {
      "ArXiv": "2402.01391",
      "DBLP": "journals/corr/abs-2402-01391",
      "DOI": "10.48550/arXiv.2402.01391",
      "CorpusId": 267406244
    },
    "references": [
      {
        "paperId": "1f2a20a6efaf83214861dddae4a38a83ae18fe32",
        "title": "DeepSeek-Coder: When the Large Language Model Meets Programming - The Rise of Code Intelligence"
      },
      {
        "paperId": "44b506d9619b5f957dc2b5588801138f343c0308",
        "title": "Let's reward step by step: Step-Level reward model as the Navigators for Reasoning"
      },
      {
        "paperId": "73b54d0875c8c9b8f0120a29f8d7a924166a1e68",
        "title": "Loose lips sink ships: Mitigating Length Bias in Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "e26888285436bc7998e5c95102a9beb60144be5e",
        "title": "Textbooks Are All You Need II: phi-1.5 technical report"
      },
      {
        "paperId": "0b0debb710366cdff461938c80763eace1651af6",
        "title": "Code Llama: Open Foundation Models for Code"
      },
      {
        "paperId": "dd18782960f9ee4c66b79e1518b342ad3f8d19e7",
        "title": "WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct"
      },
      {
        "paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
        "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "paperId": "454c8fef2957aa2fb13eb2c7a454393a2ee83805",
        "title": "WizardCoder: Empowering Code Large Language Models with Evol-Instruct"
      },
      {
        "paperId": "be8db99310602d66bba64bcf41a572c45816fbfc",
        "title": "Let's Verify Step by Step"
      },
      {
        "paperId": "08a80cb34d785258c770acecd302ab41ead46eed",
        "title": "WizardLM: Empowering Large Pre-Trained Language Models to Follow Complex Instructions"
      },
      {
        "paperId": "bafe023fb072045dc0cd50316382a61c8dcb9fae",
        "title": "CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X"
      },
      {
        "paperId": "1bf21dabbdfc81fd4f9e92b1201ecce744cabb6a",
        "title": "SantaCoder: don't reach for the stars!"
      },
      {
        "paperId": "e65b346d442e9962a4276dc1c1af2956d9d5f1eb",
        "title": "Self-Instruct: Aligning Language Models with Self-Generated Instructions"
      },
      {
        "paperId": "6d7b8a478801bd9d21df82d5f33ae6eced90da5e",
        "title": "Solving math word problems with process- and outcome-based feedback"
      },
      {
        "paperId": "06ea568379211ffa07d9605f66f26f6f736ea5e0",
        "title": "PanGu-Coder: Program Synthesis with Function-Level Language Modeling"
      },
      {
        "paperId": "7a1069dafaeb484e22f2473d5545f1e45ce30656",
        "title": "Compilable Neural Code Generation with Compiler Feedback"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "da1e404ae194dcbcce198f5335f49f090403bbd7",
        "title": "Exploration in Deep Reinforcement Learning: A Survey"
      },
      {
        "paperId": "5cbe278b65a81602a864184bbca37de91448a5f5",
        "title": "Competition-level code generation with AlphaCode"
      },
      {
        "paperId": "12075ea34f5fbe32ec5582786761ab34d401209b",
        "title": "Exploration in Deep Reinforcement Learning: From Single-Agent to Multiagent Domain"
      },
      {
        "paperId": "a38e0f993e4805ba8a9beae4c275c91ffcec01df",
        "title": "Program Synthesis with Large Language Models"
      },
      {
        "paperId": "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269",
        "title": "Evaluating Large Language Models Trained on Code"
      },
      {
        "paperId": "1ccd031f28dccfb226f6c0c588c93a97a50bf95f",
        "title": "Measuring Coding Challenge Competence With APPS"
      },
      {
        "paperId": "870ff1dde0c103c3d90be51880f984628e77a8d6",
        "title": "CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"
      },
      {
        "paperId": "a4c6cb385a90441061218f50abe2871d5e384a2e",
        "title": "IntelliCode compose: code generation using transformer"
      },
      {
        "paperId": "5d4ebbaa1ef8feeac885e2869f45c0276c18834f",
        "title": "Learning Montezuma's Revenge from a Single Demonstration"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "9862caed8ee93321c78b0196e0b7eef516b545ba",
        "title": "Reverse Curriculum Generation for Reinforcement Learning"
      },
      {
        "paperId": "f0a79cb16c3e53e9c536b65ec7b2157db1d352f1",
        "title": "Program Synthesis"
      },
      {
        "paperId": "6fab0b3b321988cedd0a017c1dad997f6d4da930",
        "title": "Playing Atari Games with Deep Reinforcement Learning and Human Checkpoint Replay"
      },
      {
        "paperId": "024006d4c2a89f7acacc6e4438d156525b60a98f",
        "title": "Continuous control with deep reinforcement learning"
      },
      {
        "paperId": "d316c82c12cf4c45f9e85211ef3d1fa62497bff8",
        "title": "High-Dimensional Continuous Control Using Generalized Advantage Estimation"
      },
      {
        "paperId": "340f48901f72278f6bf78a04ee5b01df208cc508",
        "title": "Human-level control through deep reinforcement learning"
      },
      {
        "paperId": "a20f0ce0616def7cc9a87446c228906cd5da093b",
        "title": "Policy Gradient Methods for Reinforcement Learning with Function Approximation"
      },
      {
        "paperId": "a4b522d7d55c0ed38c825c4fb9fe28c14d659c0a",
        "title": "A critique of cyclomatic complexity as a software metric"
      },
      {
        "paperId": "2e5dc4b028e670d1b9ac0b4e15580cdb7197cb68",
        "title": "Exploration in Deep Reinforcement Learning: A Comprehensive Survey"
      },
      {
        "paperId": "4c915c1eecb217c123a36dc6d3ce52d12c742614",
        "title": "Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning"
      },
      {
        "paperId": "ac4af1df88e178386d782705acc159eaa0c3904a",
        "title": "Actor-Critic Algorithms"
      },
      {
        "paperId": null,
        "title": "2022. Coderl: Mastering code generation through pre-trained models and deep reinforcement learning"
      },
      {
        "paperId": null,
        "title": "this setting, we fine-tune models only based on the APPS+ dataset and evaluate them on MBPP"
      },
      {
        "paperId": null,
        "title": "OpenCompass Contributors"
      },
      {
        "paperId": null,
        "title": "Findings of the Association for Computational Lin-guistics"
      }
    ],
    "cited_by": [
      {
        "paperId": "cde07fd6bff078e40d267b3ec4718e397e3ca3e5",
        "title": "Humanline: Online Alignment as Perceptual Loss"
      },
      {
        "paperId": "07529b3b3df643faf2a8461100bc548f9b3feef9",
        "title": "Reinforcement Learning-Guided Chain-of-Draft for Token-Efficient Code Generation"
      },
      {
        "paperId": "0dc8652aa3b615c7f712e3f6eb418700ff07eadd",
        "title": "Translating to a Low-Resource Language with Compiler Feedback: A Case Study on Cangjie"
      },
      {
        "paperId": "802f50f2b8117892d03637098dc55e136fa03275",
        "title": "One More Glance with Sharp Eyes: Rethinking Lightweight Captioning as a Practical Visual Specialist"
      },
      {
        "paperId": "8ed4a4d278a8898e19b31e2af9e6d7275a5ae888",
        "title": "Alignment with Fill-In-the-Middle for Enhancing Code Generation"
      },
      {
        "paperId": "da5ea86634d182a39025d81944580b22b116ca90",
        "title": "AR$^2$: Adversarial Reinforcement Learning for Abstract Reasoning in Large Language Models"
      },
      {
        "paperId": "90d32a876eb054b17a4d6cf1cec26750cac40ef6",
        "title": "A.S.E: A Repository-Level Benchmark for Evaluating Security in AI-Generated Code"
      },
      {
        "paperId": "37f8a18c74565ffbbbc20fa5214b72c00b4618d7",
        "title": "From Intent to Execution: Multimodal Chain-of-Thought Reinforcement Learning for Precise CAD Code Generation"
      },
      {
        "paperId": "1bb5eb4dc18adb86453bdc6655ef6e2af7149652",
        "title": "Training Long-Context, Multi-Turn Software Engineering Agents with Reinforcement Learning"
      },
      {
        "paperId": "5458d07ae90e746e54a1f77906ae2258c42605e7",
        "title": "Repair-R1: Better Test Before Repair"
      },
      {
        "paperId": "6ed6b3a3ec405a56892a1b7f36de1d3106654be5",
        "title": "ChemDFM-R: An Chemical Reasoner LLM Enhanced with Atomized Chemical Knowledge"
      },
      {
        "paperId": "5341e0e01bdd27613d95f78d97148e1b98d05178",
        "title": "ParaStudent: Generating and Evaluating Realistic Student Code by Teaching LLMs to Struggle"
      },
      {
        "paperId": "0d9895b4f3b18974b466a59d21369124bc8a5327",
        "title": "D-LiFT: Improving LLM-based Decompiler Backend via Code Quality-driven Fine-tuning"
      },
      {
        "paperId": "2c96c423d0cd481953e50ded37fb04921af02f5e",
        "title": "Co-Evolving LLM Coder and Unit Tester via Reinforcement Learning"
      },
      {
        "paperId": "c1bdb020123ca036e119c8273c8d570c01d24921",
        "title": "Improving LLM-Generated Code Quality with GRPO"
      },
      {
        "paperId": "14f409c6c3661c7dd487a859a98b94c8f0b15a99",
        "title": "CRScore++: Reinforcement Learning with Verifiable Tool and AI Feedback for Code Review"
      },
      {
        "paperId": "2d3b5a90976b89ebd16efda15801df999f0a6599",
        "title": "Afterburner: Reinforcement Learning Facilitates Self-Improving Code Efficiency Optimization"
      },
      {
        "paperId": "0311f5740c78a5dff9890a97b4be59068bbc3d8b",
        "title": "Training Language Models to Generate Quality Code with Program Analysis Feedback"
      },
      {
        "paperId": "af27c981129db6dcf1a6040d998c6db099bf79f6",
        "title": "SPA-RL: Reinforcing LLM Agents via Stepwise Progress Attribution"
      },
      {
        "paperId": "89619dea003a808abf5f2ffe6a39138d59125f09",
        "title": "ShIOEnv: A CLI Behavior-Capturing Environment Enabling Grammar-Guided Command Synthesis for Dataset Curation"
      },
      {
        "paperId": "f7bda1b5ffcd601a09e72f0d089a0fcdcc64de5c",
        "title": "Pass@K Policy Optimization: Solving Harder Reinforcement Learning Problems"
      },
      {
        "paperId": "6b0201470fe132402315c88932271d33e5028311",
        "title": "SuperCoder: Assembly Program Superoptimization with Large Language Models"
      },
      {
        "paperId": "f7e492dfd3de3a9b9f92bdb9921d75ab2fa3929b",
        "title": "CRPE: Expanding The Reasoning Capability of Large Language Model for Code Generation"
      },
      {
        "paperId": "1c1be69d7cd3c3360f5b8af5f66055234fc0506f",
        "title": "Sailing by the Stars: A Survey on Reward Models and Learning Strategies for Learning from Rewards"
      },
      {
        "paperId": "1871906b30e8a0aec7df934214ac45a369872ca4",
        "title": "Advancing Large Language Models in Code Generation: Usaco Benchmark and Bug Mitigation Insights"
      },
      {
        "paperId": "a1798f3bf97e9dbb7ec24e7236681bdeaaebe4c4",
        "title": "Fixing Large Language Models' Specification Misunderstanding for Better Code Generation"
      },
      {
        "paperId": "80bf448279b1ff06afce2f6f9012e5623dc42395",
        "title": "Integrating Symbolic Execution into the Fine-Tuning of Code-Generating LLMs"
      },
      {
        "paperId": "08ca9b6b1a903035c090155ce9414de99ca3279b",
        "title": "Themisto: Jupyter-Based Runtime Benchmark"
      },
      {
        "paperId": "ea9277a0d22811f5a8bc4b4b4f51df58da966719",
        "title": "FeedbackEval: A Benchmark for Evaluating Large Language Models in Feedback-Driven Code Repair Tasks"
      },
      {
        "paperId": "031ed8286daf421cc7b9dd2322e921bc50c652f4",
        "title": "GiFT: Gibbs Fine-Tuning for Code Generation"
      },
      {
        "paperId": "98bdf937d7eb831ff9a2a9b363a4e682638ee366",
        "title": "ACECODER: Acing Coder RL via Automated Test-Case Synthesis"
      },
      {
        "paperId": "e2bd895ffbe527ba9cf038fc86990c059ee33a04",
        "title": "Enhancing Code LLMs with Reinforcement Learning in Code Generation: A Survey"
      },
      {
        "paperId": "76a7fa5d74e99b50dabd3f29c0c8d35b5f268193",
        "title": "From Triumph to Uncertainty: The Journey of Software Engineering in the AI Era"
      },
      {
        "paperId": "ec0c7ac46111707c49b0ff8e7a7c635818112656",
        "title": "Trading Devil RL: Backdoor attack via Stock market, Bayesian Optimization and Reinforcement Learning"
      },
      {
        "paperId": "821619923bc0bda29a197a4e472fa40afe3bdc2b",
        "title": "LLM-based Interactive Code Generation: Empirical Evaluation"
      },
      {
        "paperId": "ae2fa2af2da2c3e96e87af00887b4938e48be79c",
        "title": "Preference Optimization for Reasoning with Pseudo Feedback"
      },
      {
        "paperId": "5cf38eb5325f196d3c9a931fede9ed2ecee82f28",
        "title": "DSTC: Direct Preference Learning with Only Self-Generated Tests and Code to Improve Code LMs"
      },
      {
        "paperId": "02c6f69935f57340bd55d2d7575f6d2c900ad3f0",
        "title": "PerfCodeGen: Improving Performance of LLM Generated Code with Execution Feedback"
      },
      {
        "paperId": "a35961ade69e670e0b125ca32dc178c7b07ba737",
        "title": "Multi-Programming Language Sandbox for LLMs"
      },
      {
        "paperId": "b8ee853b2e58e3ca4a2a2add2a58eefb34ef40fb",
        "title": "FALCON: Feedback-driven Adaptive Long/short-term memory reinforced Coding Optimization system"
      },
      {
        "paperId": "a38ab0d55aea5afd931a6054a6cfd2bb7916cedc",
        "title": "Process Supervision-Guided Policy Optimization for Code Generation"
      },
      {
        "paperId": "585e95a43f4ceb3b9fdd8408b7b0b5df468c1030",
        "title": "RLEF: Grounding Code LLMs in Execution Feedback with Reinforcement Learning"
      },
      {
        "paperId": "b51d1946a6184946c93809d06942aa410c384203",
        "title": "Towards a Unified View of Preference Learning for Large Language Models: A Survey"
      },
      {
        "paperId": "16f7bf04560f15d051b143d216c04113dcc3e1e2",
        "title": "MR-Adopt: Automatic Deduction of Input Transformation Function for Metamorphic Testing"
      },
      {
        "paperId": "ecf239ef3f2633efa5b1ad6bf1cc011a0c1c7cf0",
        "title": "Large Language Model for Verilog Generation with Code-Structure-Guided Reinforcement Learning"
      },
      {
        "paperId": "697775b02833f4e48c47161948f2b5a53fae60ef",
        "title": "STALL+: Boosting LLM-based Repository-level Code Completion with Static Analysis"
      },
      {
        "paperId": "af3648fdec8b22f69f35714811f20a4c34997892",
        "title": "Unlock the Correlation between Supervised Fine-Tuning and Reinforcement Learning in Training Code Large Language Models"
      },
      {
        "paperId": "3cc79d49ca07ba0907ab7ea7b723600b674b9a4d",
        "title": "Meta-Designing Quantum Experiments with Language Models"
      },
      {
        "paperId": "c8b18682965ff9dccc0130dab3d679f78cefa617",
        "title": "A Survey on Large Language Models for Code Generation"
      },
      {
        "paperId": "7924cbe48fe2583058c076926873ea4027bf72a9",
        "title": "ReflectionCoder: Learning from Reflection Sequence for Enhanced One-off Code Generation"
      },
      {
        "paperId": "be992d914389d9e4462df1911e6ede7645b66f86",
        "title": "RLSF: Fine-tuning LLMs via Symbolic Feedback"
      },
      {
        "paperId": "48bbe68f01431f595c43a2e8ddcd92ecefd07a55",
        "title": "CC2Vec: Combining Typed Tokens with Contrastive Learning for Effective Code Clone Detection"
      },
      {
        "paperId": "bd598f1b113891cbaccd5b7b96008a9bf2759712",
        "title": "The Larger the Better? Improved LLM Code-Generation via Budget Reallocation"
      },
      {
        "paperId": "acb66323a032d1244de4b621bc944c2b039f5ac3",
        "title": "CodeS: Natural Language to Code Repository via Multi-Layer Sketch"
      },
      {
        "paperId": "5eac2a40422a7085cb6f03285ad08210b6f6744b",
        "title": "OpenCodeInterpreter: Integrating Code Generation with Execution and Refinement"
      },
      {
        "paperId": "8a6a72afbcc1080212d105f8de02239c8718ef35",
        "title": "Chain-of-Thought in Neural Code Generation: From and for Lightweight Language Models"
      },
      {
        "paperId": "54a86a3042591b9757c292630594b6487a2fc82c",
        "title": "Reinforcement Learning for Generative AI: A Survey"
      },
      {
        "paperId": "2d269a8b8cd99889efadd041993a35e71bf2c1c2",
        "title": "Sailing AI by the Stars: A Survey of Learning from Rewards in Post-Training and Test-Time Scaling of Large Language Models"
      },
      {
        "paperId": "6578806662072300f211220c21aecc09c911dc51",
        "title": "Uncertainty-Aware Iterative Preference Optimization for Enhanced LLM Reasoning"
      },
      {
        "paperId": "e4d69d017f69634ef99567638e36ba7f5cf5fca2",
        "title": "Adversarial RL for Hard-Negative Code Generation"
      },
      {
        "paperId": "fdd5d03e7fb6e2994dd149c21a5c4def0068a644",
        "title": "Teaching Models to Reason about Vision-Based Code Generation using GRPO"
      }
    ],
    "score": 61.0
  },
  {
    "id": "5bac7d00035bc1e246a34f9ee3152b290f97bb92",
    "title": "Supervised Pretraining Can Learn In-Context Reinforcement Learning",
    "authors": [
      "Jonathan Lee",
      "Annie Xie",
      "Aldo Pacchiano",
      "Yash Chandak",
      "Chelsea Finn",
      "Ofir Nachum",
      "E. Brunskill"
    ],
    "year": 2023,
    "citationCount": 110,
    "abstract": "Large transformer models trained on diverse datasets have shown a remarkable ability to learn in-context, achieving high few-shot performance on tasks they were not explicitly trained to solve. In this paper, we study the in-context learning capabilities of transformers in decision-making problems, i.e., reinforcement learning (RL) for bandits and Markov decision processes. To do so, we introduce and study Decision-Pretrained Transformer (DPT), a supervised pretraining method where the transformer predicts an optimal action given a query state and an in-context dataset of interactions, across a diverse set of tasks. This procedure, while simple, produces a model with several surprising capabilities. We find that the pretrained transformer can be used to solve a range of RL problems in-context, exhibiting both exploration online and conservatism offline, despite not being explicitly trained to do so. The model also generalizes beyond the pretraining distribution to new tasks and automatically adapts its decision-making strategies to unknown structure. Theoretically, we show DPT can be viewed as an efficient implementation of Bayesian posterior sampling, a provably sample-efficient RL algorithm. We further leverage this connection to provide guarantees on the regret of the in-context algorithm yielded by DPT, and prove that it can learn faster than algorithms used to generate the pretraining data. These results suggest a promising yet simple path towards instilling strong in-context decision-making abilities in transformers.",
    "url": "https://www.semanticscholar.org/paper/5bac7d00035bc1e246a34f9ee3152b290f97bb92",
    "pdf_url": "https://arxiv.org/pdf/2306.14892.pdf",
    "venue": "Neural Information Processing Systems",
    "publicationDate": "2023-06-26",
    "externalIds": {
      "DBLP": "conf/nips/0002XPCFNB23",
      "ArXiv": "2306.14892",
      "DOI": "10.48550/arXiv.2306.14892",
      "CorpusId": 259262142
    },
    "references": [
      {
        "paperId": "253c900b0569694d57e8f2904e330b51ae740fd8",
        "title": "A Mechanism for Sample-Efficient In-Context Learning for Sparse Retrieval Tasks"
      },
      {
        "paperId": "f197bf0fc2f228483f6af3285000d54d8d97f9eb",
        "title": "Voyager: An Open-Ended Embodied Agent with Large Language Models"
      },
      {
        "paperId": "01706dd038b959e92a93f3141bb98be8f1f048f0",
        "title": "Hyper-Decision Transformer for Efficient Online Policy Adaptation"
      },
      {
        "paperId": "da3aca9d7b50da823f669c983edeb60445720fe0",
        "title": "The Learnability of In-Context Learning"
      },
      {
        "paperId": "2ebd5df74980a37370b0bcdf16deff958289c041",
        "title": "Foundation Models for Decision Making: Problems, Methods, and Opportunities"
      },
      {
        "paperId": "d98b5c1d0f9a4e39dc79ea7a3f74e54789df5e13",
        "title": "Structured State Space Models for In-Context Reinforcement Learning"
      },
      {
        "paperId": "86cf30c4378b8b8a0da7888edc8c383182f93c8a",
        "title": "Approximate Thompson Sampling via Epistemic Neural Networks"
      },
      {
        "paperId": "bfe6fd05f09647b001c7eb6e333a95c881c88344",
        "title": "Human-Timescale Adaptation in an Open-Ended Task Space"
      },
      {
        "paperId": "525d93a382f6e7873b5d8a2e0713eb3dff7fb250",
        "title": "Transformers learn in-context by gradient descent"
      },
      {
        "paperId": "fd1cf28a2b8caf2fe29af5e7fa9191cecfedf84d",
        "title": "RT-1: Robotics Transformer for Real-World Control at Scale"
      },
      {
        "paperId": "93fdf5cf598aefb0335f001039e83494dc721c3a",
        "title": "General-Purpose In-Context Learning by Meta-Learning Transformers"
      },
      {
        "paperId": "15c820a41247ab28424abdb87dafade36a3b5e64",
        "title": "Learning Options via Compression"
      },
      {
        "paperId": "7aa801b907b59b8ee4cfb1296d9dac22c5164c5d",
        "title": "What learning algorithm is in-context learning? Investigations with linear models"
      },
      {
        "paperId": "860bc4f071f35d6d8529a52c2c1858d030779a6a",
        "title": "In-context Reinforcement Learning with Algorithm Distillation"
      },
      {
        "paperId": "834c8c95ff1129eb197bfdfa18f6bdf3c11c205c",
        "title": "Dichotomy of Control: Separating What You Can Control from What You Cannot"
      },
      {
        "paperId": "c90a99eeb57019732a6cc996bb9eaf13faedf00f",
        "title": "In-context Learning and Induction Heads"
      },
      {
        "paperId": "de32da8f5c6a50a6c311e9357ba16aa7d05a1bc9",
        "title": "What Can Transformers Learn In-Context? A Case Study of Simple Function Classes"
      },
      {
        "paperId": "b1ec347c64d06fa17cd25e6e8b4ed4c908f0fb0b",
        "title": "Transformer Neural Processes: Uncertainty-Aware Meta Learning Via Sequence Modeling"
      },
      {
        "paperId": "15cbccf71d1cd3f886ae9b0f3cc001d14577d264",
        "title": "Prompting Decision Transformer for Few-Shot Policy Generalization"
      },
      {
        "paperId": "01d4cc6e7c89f42ad1fc27b57439c9b6c2797fb8",
        "title": "Behavior Transformers: Cloning k modes with one stone"
      },
      {
        "paperId": "511e6559df79b5b7cc3fa69ae31ef1c3badce048",
        "title": "When does return-conditioned supervised learning work for offline reinforcement learning?"
      },
      {
        "paperId": "51606f776a3b53a08d5a7e726f9ad771d52e2506",
        "title": "Multi-Game Decision Transformers"
      },
      {
        "paperId": "5b353418cf914dccf85cfefcbdda892b600fdc6e",
        "title": "Why So Pessimistic? Estimating Uncertainties for Offline RL through Ensembles, and Why Their Independence Matters"
      },
      {
        "paperId": "5922f437512158970c417f4413bface021df5f78",
        "title": "A Generalist Agent"
      },
      {
        "paperId": "b799c782f168b0a02ebab9e50ff38ded1bc79aee",
        "title": "Optimistic posterior sampling for reinforcement learning: worst-case regret bounds"
      },
      {
        "paperId": "1fafaccebc4a74898a74c606f846318c4c2c7536",
        "title": "On the Effect of Pretraining Corpora on In-context Learning by a Large-scale Language Model"
      },
      {
        "paperId": "146e9e1238ff6caf18f0bd936ffcfbe1e65d2afd",
        "title": "Data Distributional Properties Drive Emergent In-Context Learning in Transformers"
      },
      {
        "paperId": "b73266c0b61484107d1771491256adbae7d7bb58",
        "title": "Learning to Induce Causal Structure"
      },
      {
        "paperId": "f4df78183261538e718066331898ee5cad7cad05",
        "title": "Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?"
      },
      {
        "paperId": "82938e991a4094022bc190714c5033df4c35aaf2",
        "title": "Retrieval-Augmented Reinforcement Learning"
      },
      {
        "paperId": "af46b5ee6d0c1aada1c482d53018a50909aa4c90",
        "title": "Impact of Pretraining Term Frequencies on Few-Shot Reasoning"
      },
      {
        "paperId": "10bd4160b44803ada6a3d2e366c44b7e2a4ffe90",
        "title": "An Explanation of In-context Learning as Implicit Bayesian Inference"
      },
      {
        "paperId": "7cf64265882f7129b127ce0e27ff7bca9173aa58",
        "title": "The Context"
      },
      {
        "paperId": "61f371768cdc093828f432660e22f7a17f22e2af",
        "title": "Offline Meta-Reinforcement Learning with Online Self-Supervision"
      },
      {
        "paperId": "e3db459e8f2c5c2c9ce8087ba79a15b05c7346bf",
        "title": "Bayesian decision-making under misspecified priors with applications to meta-learning"
      },
      {
        "paperId": "f864d4d2267abba15eb43db54f58286aef78292b",
        "title": "Offline Reinforcement Learning as One Big Sequence Modeling Problem"
      },
      {
        "paperId": "c1ad5f9b32d80f1c65d67894e5b8c2fdf0ae4500",
        "title": "Decision Transformer: Reinforcement Learning via Sequence Modeling"
      },
      {
        "paperId": "91edb1532effc929e785812f54d601e43c6515f2",
        "title": "On the Optimality of Batch Policy Optimization Algorithms"
      },
      {
        "paperId": "0bcc734246586966596b1aa22efac2565224ebee",
        "title": "Bridging Offline Reinforcement Learning and Imitation Learning: A Tale of Pessimism"
      },
      {
        "paperId": "245682e8b3fa76f4a3e2991b5497577af95cbb3f",
        "title": "COMBO: Conservative Offline Model-Based Policy Optimization"
      },
      {
        "paperId": "9f8a70e9188a913317e97ba1874c8f763fd13c04",
        "title": "Is Pessimism Provably Efficient for Offline RL?"
      },
      {
        "paperId": "17908c7db26985704c00dd4521932f25c43dbe17",
        "title": "Offline Meta-Reinforcement Learning with Advantage Weighting"
      },
      {
        "paperId": "2a9336f92bcdc650c5257ec0cc1b4cd272f5ed1a",
        "title": "Decoupling Exploration and Exploitation for Meta-Reinforcement Learning without Sacrifices"
      },
      {
        "paperId": "034b2e3d957df5405783f2c5695a8c2bd87d6334",
        "title": "FLAMBE: Structural Complexity and Representation Learning of Low Rank MDPs"
      },
      {
        "paperId": "28db20a81eec74a50204686c3cf796c42a020d2e",
        "title": "Conservative Q-Learning for Offline Reinforcement Learning"
      },
      {
        "paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
        "title": "Language Models are Few-Shot Learners"
      },
      {
        "paperId": "5e7bc93622416f14e6948a500278bfbe58cd3890",
        "title": "Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems"
      },
      {
        "paperId": "0881655dcdf891f529ebe7ac18301e138a5e265b",
        "title": "Keep Doing What Worked: Behavioral Modelling Priors for Offline Reinforcement Learning"
      },
      {
        "paperId": "3c8a456509e6c0805354bd40a35e3f2dbf8069b1",
        "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library"
      },
      {
        "paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b",
        "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
      },
      {
        "paperId": "361e953f792a585496834ee14216b94d0ce9ae74",
        "title": "VariBAD: A Very Good Method for Bayes-Adaptive Deep RL via Meta-Learning"
      },
      {
        "paperId": "9be492858863c8c7c24be1ecb75724de5086bd8e",
        "title": "Behavior Regularized Offline Reinforcement Learning"
      },
      {
        "paperId": "76eca1e9f4dbe0b4533cb01994ea51cfd9f29610",
        "title": "A Model-based Approach for Sample-efficient Multi-task Reinforcement Learning"
      },
      {
        "paperId": "82b4b03a4659d6e04bd7cbf51d6e08fde1348dbd",
        "title": "Stabilizing Off-Policy Q-Learning via Bootstrapping Error Reduction"
      },
      {
        "paperId": "9a3c9a0ac460c7891d03d56146f2d566c7e0fb08",
        "title": "Meta reinforcement learning as task inference"
      },
      {
        "paperId": "4625628163a2ee0e6cd320cd7a14b4ccded2a631",
        "title": "Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables"
      },
      {
        "paperId": "5285cb8faada5de8a92a47622950f6cfd476ac1d",
        "title": "Off-Policy Deep Reinforcement Learning without Exploration"
      },
      {
        "paperId": "c456941a270cb2040eed4abfb39150508caf920c",
        "title": "ProMP: Proximal Meta-Policy Search"
      },
      {
        "paperId": "f802802b3af5a22b79ac65d033ba3cbee33da91b",
        "title": "Randomized Prior Functions for Deep Reinforcement Learning"
      },
      {
        "paperId": "944bd3b472c8a30163bbfc1b5cbab8545693c3e0",
        "title": "Learning to Adapt in Dynamic, Real-World Environments through Meta-Reinforcement Learning"
      },
      {
        "paperId": "68c108795deef06fa929d1f6e96b75dbf7ce8531",
        "title": "Meta-Reinforcement Learning of Structured Exploration Strategies"
      },
      {
        "paperId": "e496de121efe29e147175545e2f37245a468cca1",
        "title": "Near-Optimal Regret Bounds for Thompson Sampling"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "7e9c1e0d247b20a0683f4797d9ea248c3b53d424",
        "title": "A Simple Neural Attentive Meta-Learner"
      },
      {
        "paperId": "c0cba3fbc696102ba38c9d754a1f22d9e9eb6366",
        "title": "A Tutorial on Thompson Sampling"
      },
      {
        "paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
        "title": "Attention is All you Need"
      },
      {
        "paperId": "6d18c5fbd2eb1808b006f5d3433a7735e88f49d2",
        "title": "Ensemble Sampling"
      },
      {
        "paperId": "c889d6f98e6d79b89c3a6adf8a921f88fa6ba518",
        "title": "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks"
      },
      {
        "paperId": "282a380fb5ac26d99667224cef8c630f6882704f",
        "title": "Learning to reinforcement learn"
      },
      {
        "paperId": "954b01151ff13aef416d27adc60cd9a076753b1a",
        "title": "RL$^2$: Fast Reinforcement Learning via Slow Reinforcement Learning"
      },
      {
        "paperId": "4b63e34276aa98d5345efa7fe09bb06d8a9d8f52",
        "title": "Deep Exploration via Bootstrapped DQN"
      },
      {
        "paperId": "62e11a7ad4096521b8ba1a30c1994648197330d1",
        "title": "One-shot learning of manipulation skills with online dynamics adaptation and neural network priors"
      },
      {
        "paperId": "789783016fb708abbc061790612ebe91273c05d3",
        "title": "(More) Efficient Reinforcement Learning via Posterior Sampling"
      },
      {
        "paperId": "28cf1bd6110e734e20fc63f727d0d5bba612b921",
        "title": "Learning to Optimize via Posterior Sampling"
      },
      {
        "paperId": "6bea71fa6deb19c67e9586428f8f240e789fb3df",
        "title": "Improved Algorithms for Linear Stochastic Bandits"
      },
      {
        "paperId": "1e1d35136b1bf3b13ef6b53f6039f39d9ee820e3",
        "title": "Finite-time Analysis of the Multiarmed Bandit Problem"
      },
      {
        "paperId": "48cce5ee49facf75eeb12832c387452424b645dd",
        "title": "A Bayesian Framework for Reinforcement Learning"
      },
      {
        "paperId": "552f45f64e64486ad273126e4f0695a6c044a015",
        "title": "Using Options for Knowledge Transfer in Reinforcement Learning"
      },
      {
        "paperId": "d130325c41947a41a55a4431e9e8e15be89da8ea",
        "title": "Learning a synaptic learning rule"
      },
      {
        "paperId": "ee2cd1d17f833d3c157a1016a778c7c22af555a2",
        "title": "ON THE LIKELIHOOD THAT ONE UNKNOWN PROBABILITY EXCEEDS ANOTHER IN VIEW OF THE EVIDENCE OF TWO SAMPLES"
      },
      {
        "paperId": "a131c44951b7ace0892dd830dd0a040b99ed0803",
        "title": "Transformers as Algorithms: Generalization and Implicit Model Selection in In-context Learning"
      },
      {
        "paperId": "d27edce419e1c731c0aeb9e1842d1e022b2cc6ab",
        "title": "Offline Meta Reinforcement Learning - Identifiability Challenges and Effective Data Collection Strategies"
      },
      {
        "paperId": "e3fc5b5627af62ee6981a02090cf6bae368202d7",
        "title": "Stable-Baselines3: Reliable Reinforcement Learning Implementations"
      },
      {
        "paperId": "2cef71e693c3c1e6cc48e9387d9e3bec1ad1b5bb",
        "title": "FOCAL: Efficient Fully-Offline Meta-Reinforcement Learning via Distance Metric Learning and Behavior Regularization"
      },
      {
        "paperId": "2919e4ff0ab126e8bf00492f00ce7363288cb1df",
        "title": "Provably Good Batch Off-Policy Reinforcement Learning Without Great Exploration"
      },
      {
        "paperId": "9405cc0d6169988371b2755e573cc28650d14dfe",
        "title": "Language Models are Unsupervised Multitask Learners"
      },
      {
        "paperId": null,
        "title": "Off-policy policy gradient with state distribution correction"
      },
      {
        "paperId": null,
        "title": "Miniworld: Minimalistic 3d environment for rl and robotics research, 2018"
      },
      {
        "paperId": "0e658618c9dad4d70dd7dcd5c519185ec4f845f5",
        "title": "In Advances in Neural Information Processing Systems"
      },
      {
        "paperId": "e8a5e7f0d466b453cbae5d6f7c8fb269fe64b025",
        "title": "Metalearning"
      },
      {
        "paperId": "97efafdb4a3942ab3efba53ded7413199f79c054",
        "title": "Reinforcement Learning: An Introduction"
      }
    ],
    "cited_by": [
      {
        "paperId": "a300d633112e9636a54c3e5f9abc43cd54cf85d4",
        "title": "When Greedy Wins: Emergent Exploitation Bias in Meta-Bandit LLM Training"
      },
      {
        "paperId": "fe7ad449b6ca304b4d837ea3214c923a06016cb6",
        "title": "Safe In-Context Reinforcement Learning"
      },
      {
        "paperId": "bae701051804ba96c19d57e1538cd51ee1c71d42",
        "title": "In-Context Compositional Q-Learning for Offline Reinforcement Learning"
      },
      {
        "paperId": "eb25d57e87a9001a9a7161dabd502c0ad038dcd4",
        "title": "Towards Monotonic Improvement in In-Context Reinforcement Learning"
      },
      {
        "paperId": "d9c6fa9f1c47b7cb844fa9e23071684113f6f562",
        "title": "Context and Diversity Matter: The Emergence of In-Context Learning in World Models"
      },
      {
        "paperId": "297a5ec4a8c7f888e415365a1627b26d2be7151a",
        "title": "In-Context Learning can Perform Continual Learning Like Humans"
      },
      {
        "paperId": "93f68c21235b13499d45dd287197fe4d635a794b",
        "title": "Inferring Causal Protein Signaling Networks with Reinforcement Learning via Artificial Bee Colony Neural Architecture Search"
      },
      {
        "paperId": "a30b790080d5650c502b1d71c477c8841854dd92",
        "title": "In-Context Decision Making for Optimizing Complex AutoML Pipelines"
      },
      {
        "paperId": "18e60270c1bdb9a8be74437f20e05be807fb11b4",
        "title": "Enhancing Privacy in Decentralized Min-Max Optimization: A Differentially Private Approach"
      },
      {
        "paperId": "b497f87b96a54f597c979e773013b2c71c8ffa6b",
        "title": "In-Context Reinforcement Learning via Communicative World Models"
      },
      {
        "paperId": "6231684a60b26b9108fe1384d7bb367bb4d6f624",
        "title": "The calculus of variations of the Transformer on the hyperspherical tangent bundle"
      },
      {
        "paperId": "6685f3657dcacf6dd136f435dfd21ad016941168",
        "title": "Interaction as Intelligence: Deep Research With Human-AI Partnership"
      },
      {
        "paperId": "a909afbb749ae3fed379e4a95f8b408a58cbcec4",
        "title": "Flows and Diffusions on the Neural Manifold"
      },
      {
        "paperId": "a98587acde8e1df366d7c4964b5a08ed979aefb2",
        "title": "Behavioral Exploration: Learning to Explore via In-Context Adaptation"
      },
      {
        "paperId": "c0770b9e2c5fc42eff550a1d86de81baa35a3754",
        "title": "CooT: Learning to Coordinate In-Context with Coordination Transformers"
      },
      {
        "paperId": "ca0308d36a273f7aad09773b589bc17bfd9402a7",
        "title": "RL-Selector: Reinforcement Learning-Guided Data Selection via Redundancy Assessment"
      },
      {
        "paperId": "5bb48a9a583470a26d144438627cea7cf62568bf",
        "title": "From Memories to Maps: Mechanisms of In-Context Reinforcement Learning in Transformers"
      },
      {
        "paperId": "42f22e6ee8742947f969bb41e92e4c6b0e910133",
        "title": "How Far Can LLMs Improve from Experience? Measuring Test-Time Learning Ability in LLMs with Human Comparison"
      },
      {
        "paperId": "6c0db2bb65dd758a899f0a0661c5f0981814ab12",
        "title": "Provably Learning from Language Feedback"
      },
      {
        "paperId": "17c1d6f70601892410fc38e019bd5a412a82cd38",
        "title": "Foundation Models for Causal Inference via Prior-Data Fitted Networks"
      },
      {
        "paperId": "55586284afcecbd4e86f2a96b957be315a819b00",
        "title": "CausalPFN: Amortized Causal Effect Estimation via In-Context Learning"
      },
      {
        "paperId": "75bdcc561077f71f9c82ca86bbb38807a00821a1",
        "title": "Can In-Context Reinforcement Learning Recover From Reward Poisoning Attacks?"
      },
      {
        "paperId": "7dd12727f73fb87580fa31c506a5f9b1649d319f",
        "title": "Mixture-of-Experts Meets In-Context Reinforcement Learning"
      },
      {
        "paperId": "2aeeabbdc17fa5d1102a6aeeaf4e4582f3fce7ff",
        "title": "Scalable In-Context Q-Learning"
      },
      {
        "paperId": "8574585dd51db51a7a283afb655cc7fb5e790a32",
        "title": "Filtering Learning Histories Enhances In-Context Reinforcement Learning"
      },
      {
        "paperId": "7b86f6dd09db91b96ce407b8bc2534f0dc811309",
        "title": "Reward Is Enough: LLMs Are In-Context Reinforcement Learners"
      },
      {
        "paperId": "d0f73b259bedc1721bbd77b43d40605c6bd88dc0",
        "title": "Adversarially Pretrained Transformers may be Universally Robust In-Context Learners"
      },
      {
        "paperId": "d24b443e6d401ea831fce8278e450d1a0da26fe6",
        "title": "Zero-Shot Adaptation of Behavioral Foundation Models to Unseen Dynamics"
      },
      {
        "paperId": "962eedb6cf3bd41f6481eb35030f7a2046521e4e",
        "title": "Improving the Data-efficiency of Reinforcement Learning by Warm-starting with LLM"
      },
      {
        "paperId": "fd2efe2ef4ddd5b62d4103450c8596a1206edda2",
        "title": "Toward Efficient Exploration by Large Language Model Agents"
      },
      {
        "paperId": "8913f68977f6e320c24b2dd6f10beb9fea365cdc",
        "title": "Text-to-Decision Agent: Offline Meta-Reinforcement Learning from Natural Language Supervision"
      },
      {
        "paperId": "c9a1683036449708ec32d574ef151c7e2e2e604b",
        "title": "Free Random Projection for In-Context Reinforcement Learning"
      },
      {
        "paperId": "6706693ea9e8d24aca80d9bc1f7e90991d20f0e2",
        "title": "Flow to Learn: Flow Matching on Neural Network Parameters"
      },
      {
        "paperId": "8b3107bbd2d02585a9ab36312a70128c2cfc72f3",
        "title": "Distilling Reinforcement Learning Algorithms for In-Context Model-Based Planning"
      },
      {
        "paperId": "59f85416d7e5d10905d51536e68133c354bea2ed",
        "title": "Yes, Q-learning Helps Offline In-Context RL"
      },
      {
        "paperId": "bb146358872cf242e97d891bbf6994bc1faae2fe",
        "title": "Training a Generally Curious Agent"
      },
      {
        "paperId": "3c962a775f2e348068af3d19856c598254094e49",
        "title": "On the Robustness of Transformers against Context Hijacking for Linear Classification"
      },
      {
        "paperId": "7708ade38e47ee2df261d0b583fc3c18e9efaa1f",
        "title": "Towards Large-Scale In-Context Reinforcement Learning by Meta-Training in Randomized Worlds"
      },
      {
        "paperId": "13b1c6df671525f0135976b5a8bc04046aa41ebb",
        "title": "Vintix: Action Model via In-Context Reinforcement Learning"
      },
      {
        "paperId": "1c6b8d7557fd647304166fd39c9d41c724c0d64c",
        "title": "An Adaptable Budget Planner for Enhancing Budget-Constrained Auto-Bidding in Online Advertising"
      },
      {
        "paperId": "4cdaf8e88654bde21e8f80d6a10c2e1ce2754b95",
        "title": "Horizon Generalization in Reinforcement Learning"
      },
      {
        "paperId": "2e459745774062f1c930b645b0adc22d43e9bca9",
        "title": "ICLR: In-Context Learning of Representations"
      },
      {
        "paperId": "812f20b087dd927512963fee56268419462b7efc",
        "title": "A Research Agenda for Usability and Generalisation in Reinforcement Learning"
      },
      {
        "paperId": "662a6fd8aa9aa1d19873a2c3f010b7fa3208519a",
        "title": "REGENT: A Retrieval-Augmented Generalist Agent That Can Act In-Context in New Environments"
      },
      {
        "paperId": "36526c0b8cca6b46b946f0583a704904bdaca599",
        "title": "Adult Glioma Segmentation in Sub-Saharan Africa using Transfer Learning on Stratified Finetuning Data"
      },
      {
        "paperId": "572a4e3e4645a14e7c70b004e624ef474d719425",
        "title": "LMAct: A Benchmark for In-Context Imitation Learning with Long Multimodal Demonstrations"
      },
      {
        "paperId": "6fa388a146fda3b448bb35e077d2f7f5fc0d40fd",
        "title": "HVAC-DPT: A Decision Pretrained Transformer for HVAC Control"
      },
      {
        "paperId": "faf5f373bd9944028664ea3e7da2d6a1fe3bf335",
        "title": "BALROG: Benchmarking Agentic LLM and VLM Reasoning On Games"
      },
      {
        "paperId": "a05a03089e1f8b7aa7dd0e2d140ca03dd04dcc56",
        "title": "AMAGO-2: Breaking the Multi-Task Barrier in Meta-Reinforcement Learning with Transformers"
      },
      {
        "paperId": "65a552ec49693b9264874dc3afa74fc60ff012f0",
        "title": "Pre-Training and Fine-Tuning Transformer for Brain Network Classification"
      },
      {
        "paperId": "5ae4d98ffa8a28af04dbebb2c9359173492f4cca",
        "title": "N-Gram Induction Heads for In-Context RL: Improving Stability and Reducing Data Needs"
      },
      {
        "paperId": "ee29f87a383fd39fd809c675b88c2db99d74b8e4",
        "title": "A Large Recurrent Action Model: xLSTM enables Fast Inference for Robotics Tasks"
      },
      {
        "paperId": "ea1f6767a48ea504113fa26066c7e6dcdbe557e4",
        "title": "Random Policy Enables In-Context Reinforcement Learning within Trust Horizons"
      },
      {
        "paperId": "bc22992620495487ffba2e7c37065423a348119d",
        "title": "Personalized Adaptation via In-Context Preference Learning"
      },
      {
        "paperId": "bfd68f35ce6d60c9f5bea426b7c86db396fe3959",
        "title": "Transformers as Game Players: Provable In-context Game-playing Capabilities of Pre-trained Models"
      },
      {
        "paperId": "109afbe5a11b7cb05a31a2fcdee1d1588b5342b3",
        "title": "Retrieval-Augmented Decision Transformer: External Memory for In-context RL"
      },
      {
        "paperId": "b4bf79bb40715686251636966bf8f50808e2e077",
        "title": "EVOLvE: Evaluating and Optimizing LLMs For Exploration"
      },
      {
        "paperId": "40cdc29bea804c2ad11d69392a0ade3028310690",
        "title": "LLMs Are In-Context Bandit Reinforcement Learners"
      },
      {
        "paperId": "4677efb1c148212f2e1bbd2f4c764e46f9f77d76",
        "title": "ReLIC: A Recipe for 64k Steps of In-Context Reinforcement Learning for Embodied AI"
      },
      {
        "paperId": "c440f83782eae49ef6b2de3e47c041e365fc3771",
        "title": "Multi-Obstacle Path Planning using Deep Reinforcement Learning"
      },
      {
        "paperId": "43ef905b6c99abc8417f63a046b39f01c8003052",
        "title": "One-shot World Models Using a Transformer Trained on a Synthetic Prior"
      },
      {
        "paperId": "2097b656b25c9291548fa082b3c433eb81856c76",
        "title": "Problem Solving Through Human-AI Preference-Based Cooperation"
      },
      {
        "paperId": "763815444e8b5e4e98232767524b5085f35d69a0",
        "title": "In-Context Exploiter for Extensive-Form Games"
      },
      {
        "paperId": "29ed78ea73d88dbc2c7f89368bd9e8db7882913a",
        "title": "NOLO: Navigate Only Look Once"
      },
      {
        "paperId": "d4f5c6b4ffc60ae04289a40fb0dcce136a122511",
        "title": "Multi-Step Reasoning with Large Language Models, a Survey"
      },
      {
        "paperId": "51faa18088f7120d969ecf9714b906eeaa70697b",
        "title": "Mental Modeling of Reinforcement Learning Agents by Language Models"
      },
      {
        "paperId": "115aa901ec1cdadc956c5448eb20aa782d3f7944",
        "title": "Efficient Sequential Decision Making with Large Language Models"
      },
      {
        "paperId": "b47507f12a8b7d26f78a484e32a08d61d3f87358",
        "title": "BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack"
      },
      {
        "paperId": "07b11891be6b132c80c39542b76cb2d0b0e95248",
        "title": "XLand-100B: A Large-Scale Multi-Task Dataset for In-Context Reinforcement Learning"
      },
      {
        "paperId": "bc03dba20fa8c17dbb092a1e521e2a1cdae5a8c0",
        "title": "Medical Vision Generalist: Unifying Medical Imaging Tasks in Context"
      },
      {
        "paperId": "15a666fcd1557394d922b7ff2cabdbd77b914936",
        "title": "Pretraining Decision Transformers with Reward Prediction for In-Context Multi-task Structured Bandit Learning"
      },
      {
        "paperId": "6797227f672b1f60e6d62a3985ad59cd072eb737",
        "title": "Evidence of Learned Look-Ahead in a Chess-Playing Neural Network"
      },
      {
        "paperId": "447970d109e43d6d75aeaed928163a34d03e9bcd",
        "title": "Maneuver-Conditioned Decision Transformer for Tactical in-Flight Decision-Making"
      },
      {
        "paperId": "b04c769c92f2bc58c5f9e6cf220480bae26405d6",
        "title": "In-Context Decision Transformer: Reinforcement Learning via Hierarchical Chain-of-Thought"
      },
      {
        "paperId": "3c79194ec98038f3af4c29d67b360bb610e1d996",
        "title": "Decision Mamba: Reinforcement Learning via Hybrid Selective Sequence Modeling"
      },
      {
        "paperId": "cf5fb966719eec6e75ad5f75f4ecf8d7e8723bec",
        "title": "From Words to Actions: Unveiling the Theoretical Underpinnings of LLM-Driven Autonomous Systems"
      },
      {
        "paperId": "055a4aa6f5009ccf55112085bd76dd8068a84b07",
        "title": "Benchmarking General Purpose In-Context Learning"
      },
      {
        "paperId": "3fca4f2105695f3113c2ea099e58c648205d3938",
        "title": "Understanding the Training and Generalization of Pretrained Transformer for Sequential Decision Making"
      },
      {
        "paperId": "28c19931f26622911455aa94c005ff963cffc009",
        "title": "Preparing for Black Swans: The Antifragility Imperative for Machine Learning"
      },
      {
        "paperId": "35c9daf3eebfd9fbb6b59d5476f8c5f2915f2dad",
        "title": "What is it for a Machine Learning Model to Have a Capability?"
      },
      {
        "paperId": "7342258d37dabfce1754ae2baa97c151ab7c45df",
        "title": "Experimental Design for Active Transductive Inference in Large Language Models"
      },
      {
        "paperId": "ed8246ebe105c0b52441fb5e25efbc3260354605",
        "title": "Sequential Decision Making with Expert Demonstrations under Unobserved Heterogeneity"
      },
      {
        "paperId": "c44471e846846bde281779405a3b5c132fd60b00",
        "title": "Survey on Large Language Model-Enhanced Reinforcement Learning: Concept, Taxonomy, and Methods"
      },
      {
        "paperId": "d492c004b74dcbec9d5c0732d95bbd3588e30451",
        "title": "Can large language models explore in-context?"
      },
      {
        "paperId": "cd8e1f4a32e1070741a0348254dc8fc0348cff07",
        "title": "SplAgger: Split Aggregation for Meta-Reinforcement Learning"
      },
      {
        "paperId": "ea3152821ce9ddd005429378073197feb6684398",
        "title": "In-Context Learning of a Linear Transformer Block: Benefits of the MLP Component and One-Step GD Initialization"
      },
      {
        "paperId": "968dc4fae222137e115d5d771ea0095002b5fc76",
        "title": "Towards an Information Theoretic Framework of Context-Based Offline Meta-Reinforcement Learning"
      },
      {
        "paperId": "78955791f82147a3719fdb46a493922a86644e2c",
        "title": "Self-supervised Pretraining for Decision Foundation Model: Formulation, Pipeline and Challenges"
      },
      {
        "paperId": "972e9f467b9f4b35859bb3af30e6ad9c56ea1240",
        "title": "In-Context Reinforcement Learning for Variable Action Spaces"
      },
      {
        "paperId": "af69e9c4953780d8e68119b1d3e8fdd23bda5f20",
        "title": "Emergence of In-Context Reinforcement Learning from Noise Distillation"
      },
      {
        "paperId": "317c7e5bb76ae979f916abdd3cd48b40aa431e11",
        "title": "XLand-MiniGrid: Scalable Meta-Reinforcement Learning Environments in JAX"
      },
      {
        "paperId": "cb759f110bf074f3a9bdb5bae049cc6bf0ce5f46",
        "title": "Can a Transformer Represent a Kalman Filter?"
      },
      {
        "paperId": "fa1e3a5c9f6e269d2a1ddbfe768a85b19ad94cf8",
        "title": "Generalization to New Sequential Decision Making Tasks with In-Context Learning"
      },
      {
        "paperId": "d899e26d0e7c0ffcf6149fb21e6561c613f97a10",
        "title": "Transformers are Provably Optimal In-context Estimators for Wireless Communications"
      },
      {
        "paperId": "f6b12800ee20b8fd2e61da05cd2c7ef698c23d95",
        "title": "Rethinking Decision Transformer via Hierarchical Reinforcement Learning"
      },
      {
        "paperId": "736dbbb1b7aea6a126bc62e32be43018a04976f0",
        "title": "Transformers as Decision Makers: Provable In-Context Reinforcement Learning via Supervised Pretraining"
      },
      {
        "paperId": "a8282d1378cf872817d3291f5e7edb6fbdf0121a",
        "title": "Cross-Episodic Curriculum for Transformer Agents"
      },
      {
        "paperId": "d3ca116177369bf6fbe27de64506a2f401aca996",
        "title": "Reason for Future, Act for Now: A Principled Framework for Autonomous LLM Agents with Provable Sample Efficiency"
      },
      {
        "paperId": "5185a88711818f11633bae71f15902278885e57d",
        "title": "PASTA: Pretrained Action-State Transformer Agents"
      },
      {
        "paperId": "e418bddc14666671c4df6a9747f39f0f522a1bad",
        "title": "Large Language Models as General Pattern Machines"
      },
      {
        "paperId": "237e85a482b41c59012f468d7c0e573e550605c8",
        "title": "Mental Modelling of Reinforcement Learning Agents by Language Models"
      },
      {
        "paperId": "12971251a5f526b220e41903d87b3d209ec30d6a",
        "title": "Text-to-Decision Agent: Learning Generalist Policies from Natural Language Supervision"
      },
      {
        "paperId": "2517c035aeffdc8f5039a23f29322e890dec426c",
        "title": "Language Models Are Good Tabular Learners"
      },
      {
        "paperId": "9e667f4f8134b7d3fc4a0daba15de53cc81b314a",
        "title": "From a Binary Feature Matrix to Correlation Analysis: A Dual-Paradigm Classification of Global Robotics Research Objectives"
      },
      {
        "paperId": "16612f46bb702149fe8984084f7df627b9c79254",
        "title": "Experimental Design for Active Transductive Inference in Large Language Models"
      },
      {
        "paperId": "25b86b9d6db6f1f684c2faef4dec3fa1f891c473",
        "title": "SAD: State-Action Distillation for In-Context Reinforcement Learning under Random Policies"
      },
      {
        "paperId": "8e7a305eff3738ee8153ff713f9dee6cb3114071",
        "title": "Opponent Modeling with In-context Search"
      },
      {
        "paperId": "e23dd2439961cb582210b686ae07998e91695b41",
        "title": "Towards Safety in Multi-agent Reinforcement Learning through Security and Privacy by Design"
      },
      {
        "paperId": "7bb74afd7dbea38265c52ddf7dffead4099f00b0",
        "title": "Towards General-Purpose In-Context Learning Agents"
      },
      {
        "paperId": "f2866ca95b940f8c7da1901c6817674343b2938b",
        "title": "Dora The Explorer: Learning Explorative Policies for Language Model RL-Finetuning"
      }
    ],
    "score": 55.0
  },
  {
    "id": "043582a1ed2d2b7d08a804bafe9db188e9a65d96",
    "title": "Neural Network Model-Based Reinforcement Learning Control for AUV 3-D Path Following",
    "authors": [
      "D. Ma",
      "Xi Chen",
      "Weihao Ma",
      "Huarong Zheng",
      "Fengzhong Qu"
    ],
    "year": 2024,
    "citationCount": 51,
    "abstract": "Autonomous underwater vehicles (AUVs) have become important tools in the ocean exploration and have drawn considerable attention. Precise control for AUVs is the prerequisite to effectively execute underwater tasks. However, the classical control methods such as model predictive control (MPC) rely heavily on the dynamics model of the controlled system which is difficult to obtain for AUVs. To address this issue, a new reinforcement learning (RL) framework for AUV path-following control is proposed in this article. Specifically, we propose a novel actor-model-critic (AMC) architecture integrating a neural network model with the traditional actor-critic architecture. The neural network model is designed to learn the state transition function to explore the spatio-temporal change patterns of the AUV as well as the surrounding environment. Based on the AMC architecture, a RL-based controller agent named ModelPPO is constructed to control the AUV. With the required sailing speed achieved by a traditional proportional-integral (PI) controller, ModelPPO can control the rudder and elevator fins so that the AUV follows the desired path. Finally, a simulation platform is built to evaluate the performance of the proposed method that is compared with MPC and other RL-based methods. The obtained results demonstrate that the proposed method can achieve better performance than other methods, which demonstrate the great potential of the advanced artificial intelligence methods in solving the traditional motion control problems for intelligent vehicles.",
    "url": "https://www.semanticscholar.org/paper/043582a1ed2d2b7d08a804bafe9db188e9a65d96",
    "pdf_url": "https://doi.org/10.1109/TIV.2023.3282681",
    "venue": "IEEE Transactions on Intelligent Vehicles",
    "publicationDate": "2024-01-01",
    "externalIds": {
      "DBLP": "journals/tiv/MaCMZQ24",
      "DOI": "10.1109/TIV.2023.3282681",
      "CorpusId": 259819914
    },
    "references": [],
    "cited_by": [
      {
        "paperId": "a0470554689b9c18cfc17e2a0b72a509aaca356f",
        "title": "Neuroadaptive fault-tolerant control for Intervention-AUV based on neural interval observer and optimal iterative control"
      },
      {
        "paperId": "f6c96ce4f7d41cbc0ba17fb1233af1aac70e005b",
        "title": "Sample-efficient planning-control framework for autonomous underwater vehicle docking using lightweight Cross Q-Learning"
      },
      {
        "paperId": "b47e5f4d2d314c99e561c61dac76de4db8394880",
        "title": "Underwater glider persistent coverage using deep reinforcement learning for ocean observation"
      },
      {
        "paperId": "28059631cf1e7777c4c85314e3edfbb04a1ea813",
        "title": "A multi-AUV adaptive collaborative target coverage algorithm for unknown environment"
      },
      {
        "paperId": "33b5584533541ac83437574f7279bd8d244fccdd",
        "title": "Reinforcement learning approaches in the motion systems of autonomous underwater vehicles"
      },
      {
        "paperId": "1a6f41c044f05b8de62a59ce02487cf6a84c7e63",
        "title": "How to Perform Energy-Balanced Underwater Data Collection in AUV-Aided UASNs: A Social Welfare-Based Node Clustering Approach"
      },
      {
        "paperId": "0fdc9967f9e869b43920a28ec0a9b20f8e879f1d",
        "title": "Ocean Diviner: A Diffusion-Augmented Reinforcement Learning Framework for AUV Robust Control in Underwater Tasks"
      },
      {
        "paperId": "080e512715c4efd13ab1330ec55f9ed57b6a5dfc",
        "title": "A DRL-Based Path Following and Obstacle Avoidance Method for USV in Water Areas with Environmental Disturbances"
      },
      {
        "paperId": "818939441895f87f16ab45f6444f6f5b17e8d045",
        "title": "Control of Marine Robots in the Era of Data-Driven Intelligence"
      },
      {
        "paperId": "f049af2a3b926e252fa9a72e1c017048f4b3bccb",
        "title": "Geometric line-of-sight guidance law with exponential switching sliding mode control for marine vehicles\u2019 path following"
      },
      {
        "paperId": "0819c89eebf06e8e97633b3facf032f5af60d43f",
        "title": "Enhancing Robust Adaptive Dynamic Positioning of Full-Actuated Surface Vessels: Reinforcement Learning Approach for Unknown Hydrodynamics"
      },
      {
        "paperId": "b5da1315b354ce68431bf8e6492d7ec25df825c0",
        "title": "Underwater Multiple AUV Cooperative Target Tracking Based on Minimal Reward Participation-Embedded MARL"
      },
      {
        "paperId": "c7e5da26a4775e2edc30a146f139a43f6deadcff",
        "title": "Event-Triggered Adaptive Backstepping Control of Underactuated AUVs with Input Saturation"
      },
      {
        "paperId": "cb3f6e19df2cdc3083dc546445b34326dfe5f071",
        "title": "A Trajectory Tracking Control Method for 6 DoF UUV Based on Event Triggering Mechanism"
      },
      {
        "paperId": "3681c861aaf4506b12b4354e6e45617fa0ba215b",
        "title": "A Review of Path Following, Trajectory Tracking, and Formation Control for Autonomous Underwater Vehicles"
      },
      {
        "paperId": "6a26fb3aa945c006ad9179e1b7ded32fbb54be6a",
        "title": "A novel reward-shaping-based soft actor\u2013critic for random trajectory tracking of AUVs"
      },
      {
        "paperId": "abffbb89e9aaf010f00bea523ed874223e20c065",
        "title": "Lyapunov-Based Model Predictive Visual Servo Control of an Underwater Vehicle-Manipulator System"
      },
      {
        "paperId": "c708e4da4337117cbf865499dd488887b343b283",
        "title": "Backstepping Command Filter Control for Electromechanical Servo Systems with Unknown Dynamics Based on Reinforcement Learning"
      },
      {
        "paperId": "59c09dde2927240ee121784cbc268b4aacf31957",
        "title": "Adaptive Control for A Full-Order Unmanned Undersea System Based on Reinforcement Learning"
      },
      {
        "paperId": "be7e9f75f2d4bfb9e80e9be965f1145843100325",
        "title": "Model-Based Offline Reinforcement Learning for AUV Path-Following Under Unknown Ocean Currents with Limited Data"
      },
      {
        "paperId": "cb093196aeed6a7336ee852a25e91a48630d3626",
        "title": "Real-time NMPC for three-dimensional trajectory tracking control of AUV with disturbances"
      },
      {
        "paperId": "66a08e46207036a7a13972f5e01752df0be4c62b",
        "title": "An expert-demonstrated soft actor\u2013critic based adaptive trajectory tracking control of Autonomous Underwater Vehicle with Long Short-Term Memory"
      },
      {
        "paperId": "0c88a679a36293f85a78214f70a280f7e1e6bb8e",
        "title": "Adaptive predefined-time specific performance control for underactuated multi-AUVs: An edge computing-based optimized RL method"
      },
      {
        "paperId": "b00c37511d11d8bf4dd9ae309f3217aa7a71fe6d",
        "title": "End-to-end autonomous underwater vehicle path following control method based on improved soft actor-critic for deep space exploration"
      },
      {
        "paperId": "72ff58e50b78beecf7e730c9e840f35232807f8b",
        "title": "Research on Autonomous Navigation and Control Algorithm of Intelligent Robot based on Reinforcement Learning"
      },
      {
        "paperId": "6b7e813995e5fb3199f4eb0e22bff3c2b25811fa",
        "title": "Learning and Sampling-Based Informative Path Planning for AUVs in Ocean Current Fields"
      },
      {
        "paperId": "a31e4238939ecd7dacbc9a5b3b41c918282c35d4",
        "title": "Adaptive fuzzy quantized state feedback control for AUVs with model uncertainty"
      },
      {
        "paperId": "f9adff867488fc3fc10ffe3b1c29693eb1102fd0",
        "title": "Underwater Target Tracking Based on Hierarchical Software-Defined Multi-AUV Reinforcement Learning: A Multi-AUV Advantage-Attention Actor-Critic Approach"
      },
      {
        "paperId": "d17dffd0b486196c91a763a830d9b74e24f5df4b",
        "title": "P-V Plane Based Active Fault-Tolerant Control Trajectory Tracking for Dish-Shaped AUV Against Complete Failure of Multiple Propellers"
      },
      {
        "paperId": "7d3687ef84ad86ab43e4533bd13d8abebe564fc3",
        "title": "A Control Method for Path Following of AUVs Considering Multiple Factors Under Ocean Currents"
      },
      {
        "paperId": "0fce1cef0d6fa53fc3345bf677ac3542cd6820ae",
        "title": "Hybrid offline-online reinforcement learning for obstacle avoidance in autonomous underwater vehicles"
      },
      {
        "paperId": "fb3edc83e61ae5f651ef3d6d1d49bc05e24b8208",
        "title": "Path planning for Multi-USV target coverage in complex environments"
      },
      {
        "paperId": "3b403329c7465432d808479f9da2e61727458537",
        "title": "Research on obstacle avoidance of underactuated autonomous underwater vehicle based on offline reinforcement learning"
      },
      {
        "paperId": "b6bb406961774c55ceda7e5c060f918167d8cdab",
        "title": "Reinforcement Learning-based AUV Trajectory Tracking and Obstacle Avoidance Control in Wave Disturbance Environments"
      },
      {
        "paperId": "e85ddfb8ac550c671c9948f3b57924d73fbfe5cf",
        "title": "Enhanced three-dimensional trajectory tracking control for AUVs in variable operating conditions using FMPC-FTTSMC"
      },
      {
        "paperId": "00ab8ca2d6c003b99ee47b9fd69944e356b2df71",
        "title": "Research on PID Motion Control System of Underwater Robot Based on Q-Learning"
      },
      {
        "paperId": "570031385c1b425028f9511dc5af82e20d943b45",
        "title": "Hybrid Physics-Learning Model Based Predictive Control for Trajectory Tracking of Unmanned Surface Vehicles"
      },
      {
        "paperId": "05356cee1095853c84da343e087c91d0eab99975",
        "title": "Integration of Deep Sequence Learning-Based Virtual GPS Model and EKF for AUV Navigation"
      },
      {
        "paperId": "b1fdc0942588612cc85cc7d4112a7418410e48d7",
        "title": "Adaptive tracking control of underactuated AUV with historical navigation information and piecewise weighted fractional order integration"
      },
      {
        "paperId": "3dec395a5f73378f116b3fe565dfbcbc22aef281",
        "title": "Research on Autonomous Underwater Vehicles Path Planning Method and Its Obstacle Avoidance Solution"
      },
      {
        "paperId": "1b67a2fd826cc342ab874b03e6c1fcb678b9659f",
        "title": "Input-Quantized Output Feedback Control for Autonomous Underwater Vehicles with Tunnel Prescribed Performance"
      },
      {
        "paperId": "e48012c6aa3173984ac8421b53443b4f97a275b5",
        "title": "Neuroadaptive Trajectory Tracking Control for an Underwater Robot via Backstepping Strategy"
      },
      {
        "paperId": "4826fa39fc97e3d29ade6bfc70374253e2ca0471",
        "title": "Physical-Informed Neural Network for MPC-Based Trajectory Tracking of Vehicles With Noise Considered"
      },
      {
        "paperId": "66c6be6282c0bf8d2a6856be1b37c8f0ec133f1a",
        "title": "Structured Light Vision Based Pipeline Tracking and 3D Reconstruction Method for Underwater Vehicle"
      },
      {
        "paperId": "d92956cc5b89393394ceafdf0ee681b50b273f84",
        "title": "Vision-Based Underwater Inspection With Portable Autonomous Underwater Vehicle: Development, Control, and Evaluation"
      },
      {
        "paperId": "096c347578110e4b1f0656cb53f3a94a0c7090af",
        "title": "Learning-Based Discontinuous Path Following Control for a Biomimetic Underwater Vehicle"
      },
      {
        "paperId": "26cadfe510aaa36ce9b545174d20f14c1496482e",
        "title": "Adaptive finite-time tracking control for heterogeneous AUV systems with intermittent communication: A two-layer observer-controller strategy"
      },
      {
        "paperId": "b69a103fe2dfc2d2a3794f6ac1b5032fef359720",
        "title": "Collision avoidance method for intelligent ships in inland complex waterways based on MVO-QSD models"
      },
      {
        "paperId": "3096bf7886a94d6dd9cb92cdc164b56330593b2b",
        "title": "Ocean Diviner: A Diffusion-Augmented Reinforcement Learning for AUV Robust Control in the Underwater Tasks"
      },
      {
        "paperId": "1f31c29176032e5cf3a7036dcb556f133561353f",
        "title": "An overview of Unmanned Surface Vehicles: Methods, practices, and applications"
      },
      {
        "paperId": "82560ce716102ec191a7aec6b8b96ef27eb3f982",
        "title": "Hierarchical dynamic trajectory planning for autonomous underwater vehicles: Algorithms and experiments"
      }
    ],
    "score": 51.0
  },
  {
    "id": "139a84c6fc4887ce2374489d79af0df9e1e7e4d6",
    "title": "Craftax: A Lightning-Fast Benchmark for Open-Ended Reinforcement Learning",
    "authors": [
      "Michael Matthews",
      "Michael Beukman",
      "Benjamin Ellis",
      "Mikayel Samvelyan",
      "Matthew Jackson",
      "Samuel Coward",
      "Jakob Foerster"
    ],
    "year": 2024,
    "citationCount": 47,
    "abstract": "Benchmarks play a crucial role in the development and analysis of reinforcement learning (RL) algorithms. We identify that existing benchmarks used for research into open-ended learning fall into one of two categories. Either they are too slow for meaningful research to be performed without enormous computational resources, like Crafter, NetHack and Minecraft, or they are not complex enough to pose a significant challenge, like Minigrid and Procgen. To remedy this, we first present Craftax-Classic: a ground-up rewrite of Crafter in JAX that runs up to 250x faster than the Python-native original. A run of PPO using 1 billion environment interactions finishes in under an hour using only a single GPU and averages 90% of the optimal reward. To provide a more compelling challenge we present the main Craftax benchmark, a significant extension of the Crafter mechanics with elements inspired from NetHack. Solving Craftax requires deep exploration, long term planning and memory, as well as continual adaptation to novel situations as more of the world is discovered. We show that existing methods including global and episodic exploration, as well as unsupervised environment design fail to make material progress on the benchmark. We believe that Craftax can for the first time allow researchers to experiment in a complex, open-ended environment with limited computational resources.",
    "url": "https://www.semanticscholar.org/paper/139a84c6fc4887ce2374489d79af0df9e1e7e4d6",
    "pdf_url": "https://arxiv.org/pdf/2402.16801.pdf",
    "venue": "International Conference on Machine Learning",
    "publicationDate": "2024-02-26",
    "externalIds": {
      "DBLP": "conf/icml/MatthewsBESJCF24",
      "ArXiv": "2402.16801",
      "DOI": "10.48550/arXiv.2402.16801",
      "CorpusId": 268032272
    },
    "references": [
      {
        "paperId": "cbffb28cde9b3ef2d95e28c9a9f3acba13bf28b8",
        "title": "JaxUED: A simple and useable UED library in Jax"
      },
      {
        "paperId": "317c7e5bb76ae979f916abdd3cd48b40aa431e11",
        "title": "XLand-MiniGrid: Scalable Meta-Reinforcement Learning Environments in JAX"
      },
      {
        "paperId": "bfbb219ac6d16743c1fe0cde9c327cd9ccbf3980",
        "title": "minimax: Efficient Baselines for Autocurricula in JAX"
      },
      {
        "paperId": "fb6bad6038eb4288d0e410239ae2ab5622384b12",
        "title": "JaxMARL: Multi-Agent RL Environments and Algorithms in JAX"
      },
      {
        "paperId": "8bb7cecd1bc3fa470aa882158e7553705b1a6141",
        "title": "Minigrid & Miniworld: Modular & Customizable Reinforcement Learning Environments for Goal-Oriented Tasks"
      },
      {
        "paperId": "713ebabceaa61d68be611757ffe00ce794dbf691",
        "title": "Jumanji: a Diverse Suite of Scalable Reinforcement Learning Environments in JAX"
      },
      {
        "paperId": "d16feddfeca2617ca2127b7d134ceb78ce8a8b40",
        "title": "A Study of Global and Episodic Bonuses for Exploration in Contextual MDPs"
      },
      {
        "paperId": "a67cbdeac8ee612a2e059ac9ce01c64bc2f0bf4a",
        "title": "Pgx: Hardware-Accelerated Parallel Game Simulators for Reinforcement Learning"
      },
      {
        "paperId": "79e1221ba7c4bb06884c3ab0d5945c6ea8aaecee",
        "title": "SMACv2: An Improved Benchmark for Cooperative Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "522ac9eb08bb0c5a422700bb254ea1c44e9157de",
        "title": "Discovered Policy Optimisation"
      },
      {
        "paperId": "0351a103915d2f10d1915b4892a988d9b15df406",
        "title": "Exploration via Elliptical Episodic Bonuses"
      },
      {
        "paperId": "0bbc63e3ec6814ec536bb317e785425e93b2a94f",
        "title": "Grounding Aleatoric Uncertainty for Unsupervised Environment Design"
      },
      {
        "paperId": "bfe49426a90213a1591e125d299b13bbfafa8c0a",
        "title": "Insights From the NeurIPS 2021 NetHack Challenge"
      },
      {
        "paperId": "e016b35c422e94b302f9f6d0508b47469aa0b189",
        "title": "Evolving Curricula with Regret-Based Environment Design"
      },
      {
        "paperId": "fdb8ccebef9f544f44cd9b88085d8ec5458b38ab",
        "title": "Replay-Guided Adversarial Environment Design"
      },
      {
        "paperId": "43ea4f5d999d35d4fc6c544eedbc100d8c3a5e00",
        "title": "MiniHack the Planet: A Sandbox for Open-Ended Reinforcement Learning Research"
      },
      {
        "paperId": "8e128a1b2efb0ddf688902ade4405d22d5b61eec",
        "title": "Benchmarking the Spectrum of Agent Capabilities"
      },
      {
        "paperId": "13263ec6b83e59442aafe1b45b7554bbba57b680",
        "title": "Brax - A Differentiable Physics Engine for Large Scale Rigid Body Simulation"
      },
      {
        "paperId": "93b2788fb1f2aed0e545d9f9d7dca1c05a63208a",
        "title": "Emergent Complexity and Zero-shot Transfer via Unsupervised Environment Design"
      },
      {
        "paperId": "822bdb6e8c39e272ebfee127666e032bd3aa0107",
        "title": "The NetHack Learning Environment"
      },
      {
        "paperId": "616ac6772508e72084360158078058dfa8bda7e7",
        "title": "First return, then explore"
      },
      {
        "paperId": "8d814620a1ca77e745bc8a33b96b86148f2804fe",
        "title": "Leveraging Procedural Generation to Benchmark Reinforcement Learning"
      },
      {
        "paperId": "8e79f3bfb2b1fc3cfdf188a6aef046185a72170a",
        "title": "How Does Learning Rate Decay Help Modern Neural Networks"
      },
      {
        "paperId": "3606c313dccd21f784bedf5759152a3677f5719d",
        "title": "Behaviour Suite for Reinforcement Learning"
      },
      {
        "paperId": "05d94cfe5768ebdcc1ecd8cb81de694e0d3e2f5d",
        "title": "MinAtar: An Atari-Inspired Testbed for Thorough and Reproducible Reinforcement Learning Experiments"
      },
      {
        "paperId": "82055eed2ba0d7156a54c586249742c848e5d565",
        "title": "The StarCraft Multi-Agent Challenge"
      },
      {
        "paperId": "6a505dbfb89cf05344457bf85b2e8307af5c4ad0",
        "title": "The Hanabi Challenge: A New Frontier for AI Research"
      },
      {
        "paperId": "c48ca266c1e16f9adc5fb7770afd95a0feec8753",
        "title": "Paired Open-Ended Trailblazer (POET): Endlessly Generating Increasingly Complex and Diverse Learning Environments and Their Solutions"
      },
      {
        "paperId": "4cb3fd057949624aa4f0bbe7a6dcc8777ff04758",
        "title": "Exploration by Random Network Distillation"
      },
      {
        "paperId": "811df72e210e20de99719539505da54762a11c6d",
        "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor"
      },
      {
        "paperId": "3b290ffa1f4f8226e326f00984acecdfbe9e28bf",
        "title": "Revisiting the Arcade Learning Environment: Evaluation Protocols and Open Problems for General Agents"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "32ceb28e45a445df4d89df281bb0e3ab5aab1a2a",
        "title": "Domain randomization for transferring deep neural networks from simulation to the real world"
      },
      {
        "paperId": "8499a250422a3c66357367c8d5fa504de5424c59",
        "title": "Intrinsic Motivation and Automatic Curricula via Asymmetric Self-Play"
      },
      {
        "paperId": "bd6b6291c3c14551cf9f2aa0e04e2e33c86b800e",
        "title": "The Malmo Platform for Artificial Intelligence Experimentation"
      },
      {
        "paperId": "6e90fd78e8a3b98af3954aae5209703aa966603e",
        "title": "Unifying Count-Based Exploration and Intrinsic Motivation"
      },
      {
        "paperId": "2b10281297ee001a9f3f4ea1aa9bea6b638c27df",
        "title": "OpenAI Gym"
      },
      {
        "paperId": "ac3ee98020251797c2b401e1389461df88e52e62",
        "title": "Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling"
      },
      {
        "paperId": "b354ee518bfc1ac0d8ac447eece9edb69e92eae1",
        "title": "MuJoCo: A physics engine for model-based control"
      },
      {
        "paperId": "f82e4ff4f003581330338aaae71f60316e58dd26",
        "title": "The Arcade Learning Environment: An Evaluation Platform for General Agents"
      },
      {
        "paperId": "8b033b13b7e6269eae9a70f6573b7eafc3991d98",
        "title": "Evolutionary Robotics and the Radical Envelope-of-Noise Hypothesis"
      },
      {
        "paperId": "2980dfe5c99658dc3e508d9d6e1d7f26e6fc8934",
        "title": "A possibility for implementing curiosity and boredom in model-building neural controllers"
      },
      {
        "paperId": "e3fc5b5627af62ee6981a02090cf6bae368202d7",
        "title": "Stable-Baselines3: Reliable Reinforcement Learning Implementations"
      },
      {
        "paperId": null,
        "title": "Open Ended Learning Team et al"
      },
      {
        "paperId": null,
        "title": "JAX: composable transformations of Python+NumPy programs"
      },
      {
        "paperId": null,
        "title": "Open-endedness: The last grand challenge you\u2019ve never heard of. While open-endedness could be a force for discovering intelligence, it could also be a component of AI"
      },
      {
        "paperId": "fae8bbf868681b83d91b2fec6c840d4d2b32005b",
        "title": "Intrinsic Motivation and Reinforcement Learning"
      },
      {
        "paperId": "fb144a1d31aec3b2bece6a59bd11a876a9fafb34",
        "title": "Exploiting Open-Endedness to Solve Problems Through the Search for Novelty"
      },
      {
        "paperId": "abafd744e85dbc3122c5463e12f9edcab6770a11",
        "title": "This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination. IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION 1 Intrinsic Motivation Systems for Autonomous Mental Development"
      },
      {
        "paperId": null,
        "title": "We used PPO hyperparameters found in an earlier hyperparameter sweep and did a small search over learning rate { 0 . 0002 , 0 . 0003 }"
      },
      {
        "paperId": null,
        "title": "Priori-tized level replay"
      }
    ],
    "cited_by": [
      {
        "paperId": "3b3e3d991af437f7c9412548223a87e15472b1a7",
        "title": "Goal-Guided Efficient Exploration via Large Language Model in Reinforcement Learning"
      },
      {
        "paperId": "a0968099802a94fc51cbd2f8d983cffc12fd9eec",
        "title": "A Q-Learning Novelty Search Strategy for Evaluating Robustness of Deep Reinforcement Learning in Open-World Environments"
      },
      {
        "paperId": "d015b9a3bf45b7812bdf7d81d9256d643d0c6625",
        "title": "Scalable Option Learning in High-Throughput Environments"
      },
      {
        "paperId": "5b5ad142fcfeace5f9ccafd2eb47ef6bbaf54254",
        "title": "PuzzleJAX: A Benchmark for Reasoning and Learning"
      },
      {
        "paperId": "a0132748f31498c569220b32d32400f7ad5d092b",
        "title": "An Efficient Open World Environment for Multi-Agent Social Learning"
      },
      {
        "paperId": "e4555ecff63afc78924c225b0ed407013f96e41f",
        "title": "NiceWebRL: a Python library for human subject experiments with reinforcement learning environments"
      },
      {
        "paperId": "e09bc30a6b5bcbf0c461ba9c14a185a2fca673b9",
        "title": "CrafterDojo: A Suite of Foundation Models for Building Open-Ended Embodied Agents in Crafter"
      },
      {
        "paperId": "9724c737c8d4023b84a184383e1c8422fc4ca7bc",
        "title": "The Yokai Learning Environment: Tracking Beliefs Over Space and Time"
      },
      {
        "paperId": "5fb99fd13a583f56ea37d283845fb49573bbb4d3",
        "title": "Scaling DRL for Decision Making: A Survey on Data, Network, and Training Budget Strategies"
      },
      {
        "paperId": "1400daa39a5d2d7350e931395c99708da3b6bf83",
        "title": "Benchmarking Partial Observability in Reinforcement Learning with a Suite of Memory-Improvable Domains"
      },
      {
        "paperId": "7a7b1d278b7ea8eb6b00418618df0a5afab9260b",
        "title": "How Should We Meta-Learn Reinforcement Learning Algorithms?"
      },
      {
        "paperId": "e3028886b75ae5aa4bcc5ff0709ba3655e0c7c63",
        "title": "Preemptive Solving of Future Problems: Multitask Preplay in Humans and Machines"
      },
      {
        "paperId": "6db6769e76aa430dc1a4127593ac89213232f5b8",
        "title": "Ludax: A GPU-Accelerated Domain Specific Language for Board Games"
      },
      {
        "paperId": "5e93dbd203dc8997cfd604937ec0fe9d78528049",
        "title": "Learning Instruction-Following Policies through Open-Ended Instruction Relabeling with Large Language Models"
      },
      {
        "paperId": "5d3865a0d524da972d36a22b240938e14e0292f4",
        "title": "Deep RL Needs Deep Behavior Analysis: Exploring Implicit Planning by Model-Free Agents in Open-Ended Environments"
      },
      {
        "paperId": "ac9fbe3af000a2f09099cf282afdac0aeb68e859",
        "title": "Large Language Models for Planning: A Comprehensive and Systematic Survey"
      },
      {
        "paperId": "80049e1d5a4c1f339aae5c1c06569ce4e604fc64",
        "title": "An Optimisation Framework for Unsupervised Environment Design"
      },
      {
        "paperId": "1d530f8aa0c93c68997e70b3066c49bf59f218ae",
        "title": "CrafText Benchmark: Advancing Instruction Following in Complex Multimodal Open-Ended World"
      },
      {
        "paperId": "38da0c5560477435e843be33cf844fdd6dfb0a15",
        "title": "Plasticine: Accelerating Research in Plasticity-Motivated Deep Reinforcement Learning"
      },
      {
        "paperId": "07831c8cc525a2100f20442a1e30f81bfd4b2493",
        "title": "Cross-environment Cooperation Enables Zero-shot Multi-agent Coordination"
      },
      {
        "paperId": "8f709642ed82dfb7e354fc5ad466ec6dc2d730c0",
        "title": "POPGym Arcade: Parallel Pixelated POMDPs"
      },
      {
        "paperId": "c4ad7adb65edeb1dea36998928248c39fa0a10a7",
        "title": "Uncovering Untapped Potential in Sample-Efficient World Model Agents"
      },
      {
        "paperId": "2b4833b78189268d500a34b463f09d6f6d6c5ee9",
        "title": "Improving Transformer World Models for Data-Efficient RL"
      },
      {
        "paperId": "a58ccedf1ab35ff7dcba7273c0707fd3ea06af04",
        "title": "Adam on Local Time: Addressing Nonstationarity in RL with Relative Adam Timesteps"
      },
      {
        "paperId": "48a3a4e0618ac9dce4355e63585a7a737a400d4c",
        "title": "Can a MISL Fly? Analysis and Ingredients for Mutual Information Skill Learning"
      },
      {
        "paperId": "3a229767eb807ed5c17f48b0a2970b259dd831cd",
        "title": "Mixtures of Experts for Scaling up Neural Networks in Order Execution"
      },
      {
        "paperId": "688c324664b8bf6147281f2771e9b17edeabf45d",
        "title": "Kinetix: Investigating the Training of General Agents through Open-Ended Physics-Based Control Tasks"
      },
      {
        "paperId": "9941cbe01c08b11eb62ba449e834b3f404f23077",
        "title": "EconoJax: A Fast & Scalable Economic Simulation in Jax"
      },
      {
        "paperId": "3c8e250026551ad5b8960de77343d13ce9050676",
        "title": "SimBa: Simplicity Bias for Scaling Up Parameters in Deep Reinforcement Learning"
      },
      {
        "paperId": "aa64075f0c7a0977508a5dcb6a3c319952afcc20",
        "title": "Assessing the Zero-Shot Capabilities of LLMs for Action Evaluation in RL"
      },
      {
        "paperId": "219392cd30fe3acab8218614476e791b1ad11846",
        "title": "JaxLife: An Open-Ended Agentic Simulator"
      },
      {
        "paperId": "1b659f619be575bbe6e2fdac00460c9c9c7fcf23",
        "title": "No Regrets: Investigating and Improving Regret Approximations for Curriculum Discovery"
      },
      {
        "paperId": "0c4db6a4830e38d873382f45a187f87ac30030ed",
        "title": "Scaling, Control and Generalization in Reinforcement Learning Level Generators"
      },
      {
        "paperId": "f7f63c70a7580a32016e4db5a1a6be21d4ff58ef",
        "title": "NAVIX: Scaling MiniGrid Environments with JAX"
      },
      {
        "paperId": "5ecf53ab083f72f10421225a7dde25eb51cb6b22",
        "title": "Can Learned Optimization Make Reinforcement Learning Less Difficult?"
      },
      {
        "paperId": "ee5212ffa1c7aababb908c33cd146a3635e677a1",
        "title": "Simplifying Deep Temporal Difference Learning"
      },
      {
        "paperId": "fe1250c8af48711a38e3b998a952ff228df829ee",
        "title": "Craftium: Bridging Flexibility and Efficiency for Rich 3D Single- and Multi-Agent Environments"
      },
      {
        "paperId": "ca9c4cddb7529ed1f464ca118de1fa1d91c4b82c",
        "title": "RobocupGym: A challenging continuous control benchmark in Robocup"
      },
      {
        "paperId": "b5718931570960c896868bd094f22ef3dbb9d281",
        "title": "The Overcooked Generalisation Challenge"
      },
      {
        "paperId": "68acf01150ddb8d88ee9b49459273d2c0a5190c7",
        "title": "A Batch Sequential Halving Algorithm without Performance Degradation"
      },
      {
        "paperId": "769d8fdf6520c52e8767ee6d54f6417cf6e7904e",
        "title": "RLeXplore: Accelerating Research in Intrinsically-Motivated Reinforcement Learning"
      },
      {
        "paperId": "317c7e5bb76ae979f916abdd3cd48b40aa431e11",
        "title": "XLand-MiniGrid: Scalable Meta-Reinforcement Learning Environments in JAX"
      },
      {
        "paperId": "886280cada60ec81a7f9d998d6aa7a19238bb400",
        "title": "AGaLiTe: Approximate Gated Linear Transformers for Online Reinforcement Learning"
      },
      {
        "paperId": "713ebabceaa61d68be611757ffe00ce794dbf691",
        "title": "Jumanji: a Diverse Suite of Scalable Reinforcement Learning Environments in JAX"
      },
      {
        "paperId": "abcfdf85b669324ed631638e2335441a30d5ce73",
        "title": "An Efficient Open World Benchmark for Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "a8a75247345091ee5e5eebb072fced073b7b5697",
        "title": "Autotelic LLM-based exploration for goal-conditioned RL"
      },
      {
        "paperId": "16ec8aa4e632182a9ba9bf125e7be228b37fed4d",
        "title": "PokerOWL: A Multi-Agent Poker Environment for Implementing and Evaluating Open-World Learning"
      }
    ],
    "score": 47.0
  },
  {
    "id": "26662adf92cacf0810a14faa514360f270e97b53",
    "title": "A Lightweight Reinforcement-Learning-Based Real-Time Path-Planning Method for Unmanned Aerial Vehicles",
    "authors": [
      "Meng Xi",
      "Huiao Dai",
      "Jingyi He",
      "Wenjie Li",
      "Jiabao Wen",
      "Shuai Xiao",
      "Jiachen Yang"
    ],
    "year": 2024,
    "citationCount": 44,
    "abstract": "The unmanned aerial vehicles (UAVs) are competent to perform a variety of applications, possessing great potential and promise. The deep neural networks (DNN) technology has enabled the UAV-assisted paradigm, accelerated the construction of smart cities, and propelled the development of the Internet of Things (IoT). UAVs play an increasingly important role in various applications, such as surveillance, environmental monitoring, emergency rescue, and supplies delivery, for which a robust path-planning technique is the foundation and prerequisite. However, existing methods lack comprehensive consideration of the complicated urban environment and do not provide an overall assessment of the robustness and generalization. Meanwhile, due to the resource constraints and hardware limitations of UAVs, the complexity of deploying the network needs to be reduced. This article proposes a lightweight, reinforcement-learning-based (RL-based) real-time path-planning method for UAVs, adaptive SAC (ASAC) algorithm, which optimizing the training process, network architecture, and algorithmic models. First of all, we establish a framework of global training and local adaptation, where the structured environment model is constructed for interaction, and local dynamically varying information aids in improving generalization. Second, ASAC introduces a cross-layer connection approach that passes the original state information into the higher layers to avoid feature loss and improve learning efficiency. Finally, we propose an adaptive temperature coefficient, which flexibly adjusts the exploration probability of UAVs with the training phase and experience data accumulation. In addition, a series of comparison experiments have been conducted in conjunction with practical application requirements, and the results have fully proved the favorable superiority of ASAC.",
    "url": "https://www.semanticscholar.org/paper/26662adf92cacf0810a14faa514360f270e97b53",
    "pdf_url": "https://doi.org/10.1109/JIOT.2024.3350525",
    "venue": "IEEE Internet of Things Journal",
    "publicationDate": "2024-06-15",
    "externalIds": {
      "DBLP": "journals/iotj/XiDHLWXY24",
      "DOI": "10.1109/JIOT.2024.3350525",
      "CorpusId": 266796903
    },
    "references": [],
    "cited_by": [
      {
        "paperId": "8e4a7cd007fa0ee6739e9d78b207fb1bb5d85562",
        "title": "Efficient Autonomy: Autonomous Driving of Retrofitted Electric Vehicles via Enhanced Transformer Modeling"
      },
      {
        "paperId": "87e23570ee3e368c9ab1759a1400a5361bc4bde2",
        "title": "Maximize the Value of Goal-Driven UAV Network Operations Based on Network Intelligence: A Comprehensive Review"
      },
      {
        "paperId": "672c00d4790b0e42a71da98e701a061122142cdf",
        "title": "A UAV-UGV Cooperative System: Patrolling and Energy Management for Urban Monitoring"
      },
      {
        "paperId": "1c170fa697e42d9ca035009355ce03dc3a59e5ce",
        "title": "Enhanced Q learning and deep reinforcement learning for unmanned combat intelligence planning in adversarial environments"
      },
      {
        "paperId": "30ba9d20a1ebac6ff89ff3fb5df6fb3989f685a4",
        "title": "Noise-driven enhancement for exploration: Deep reinforcement learning for UAV autonomous navigation in complex environments"
      },
      {
        "paperId": "98cab04f4c6c4f32c8c317a463c1b126667435f5",
        "title": "DOAMC: A Novel Path Planning Algorithm for Dynamic Obstacle Avoidance in Mapless Robot Navigation"
      },
      {
        "paperId": "7004d5a771501aa3c250b5816637f4c1dfed2321",
        "title": "Comparative Evaluation of Reinforcement Learning Algorithms for Multi-Agent Unmanned Aerial Vehicle Path Planning in 2D and 3D Environments"
      },
      {
        "paperId": "25772777c888f09b66178a10e1f9326793a0045c",
        "title": "Energy-Efficient Deployment and Offloading Strategy in a Multi-AAV-Assisted MEC System"
      },
      {
        "paperId": "4adb9bb8537d7618ee343c8b921e812ce13fbd2b",
        "title": "Secure and Resilient IoMT Node Deployment: Enhancing Privacy and Threat Mitigation with 3D Voronoi Diagrams and a PSO-GA Hybrid Algorithm in Healthcare Networks"
      },
      {
        "paperId": "31ab7a83858d859da1650b0d9ab658c7f9203f6e",
        "title": "DWT-CPLnet: A New Intrusion Disturbance Identification Paradigm for Optical Fiber Sensing Network in Open Environments"
      },
      {
        "paperId": "9d47b8aeb3c0119f91e4eefe7e2e468df914e0f4",
        "title": "Scalable multi-agent path planning via wavelet-based KAN and enhanced feature extraction"
      },
      {
        "paperId": "98a59b42f72d46da7b1141c2b4d325879c99fdac",
        "title": "An Improved Grey Wolf Optimizer Inspired by Advanced Cooperative Predation for UAV Shortest Path Planning"
      },
      {
        "paperId": "7aa3e063dbb08b51060a04dc36a62f26966b5194",
        "title": "Emerging trends and strategic opportunities in tiny machine learning: A comprehensive thematic analysis"
      },
      {
        "paperId": "bf942c469d259cebbb4d32991099ac85ab5c08e5",
        "title": "Using large language models for semantic interoperability: A systematic literature review"
      },
      {
        "paperId": "6c1f2ea52ca59a4c384462ce859ab9fd838516e1",
        "title": "Path planning algorithm for logistics autonomous vehicles at Cainiao stations based on multi-sensor data fusion"
      },
      {
        "paperId": "01d5fc778802ea54f77832bb35ca6a7e944b8c5a",
        "title": "Multi-Drone-Truck Collaborative Delivery with En Route Operations: A Hierarchical MARL-Based Approach"
      },
      {
        "paperId": "5f7491adc3746c3077d426e82c61e2fa623b3934",
        "title": "An Improved MATD3-Based Algorithm for Cooperative Autonomous Obstacle Avoidance and Trajectory Planning in Multi-UAV Systems"
      },
      {
        "paperId": "3e34ed58e04fbfd28c2d5dfee242075a1a962253",
        "title": "Toward Energy-Efficient Spike-Based Deep Reinforcement Learning With Temporal Coding"
      },
      {
        "paperId": "8891cb1a9f6a85c2cf1db0e573a720b8be25bc17",
        "title": "AGCo-MATA: Air-Ground Collaborative Multi-Agent Task Allocation in Mobile Crowdsensing"
      },
      {
        "paperId": "d68905b03d7dcd0cdcb2f96908456b9282f1b331",
        "title": "Lightweight AI for Drones: A Survey"
      },
      {
        "paperId": "51a0cb7adf02bc97cd14b751bafcd67978236446",
        "title": "Unmanned aerial vehicle routing based on frog-leaping optimization algorithm"
      },
      {
        "paperId": "7977d09e4a8c00ff82857d17a4f09532f902d0bf",
        "title": "Facilitating Multi-UAVs application for rescue in complex 3D sea wind offshore environment: A scalable Multi-UAVs collaborative path planning method based on improved coatis optimization algorithm"
      },
      {
        "paperId": "6ae4846e109f41418163cf6bd68b4488d08c530c",
        "title": "Graphics Processing Unit-enabled Path Planning based on Global Evolutionary Dynamic Programming and Local Genetic Algorithm Optimization"
      },
      {
        "paperId": "3f69486780dd001e774696531a98b47642510b43",
        "title": "Explainable and Data-Efficient Deep Learning for Enhanced Attack Detection in IIoT Ecosystem"
      },
      {
        "paperId": "b6c361e99b9411daf0fbd7438fcb998374cac4a7",
        "title": "Distributed Unmanned Aerial Vehicle Cluster Testing Method Based on Deep Reinforcement Learning"
      },
      {
        "paperId": "da047bcd92b6a0d4e075604a9427ee230ad02f8a",
        "title": "Z-Score Experience Replay in Off-Policy Deep Reinforcement Learning"
      },
      {
        "paperId": "53ea51ca4688076940f893d243962d74dbd499ac",
        "title": "Novel Energy-Aware 3D UAV Path Planning and Collision Avoidance Using Receding Horizon and Optimization-Based Control"
      },
      {
        "paperId": "c35facaaaebcc25aaaf9a58d7bd37afe5ceb7a21",
        "title": "Curvature index of image samples used to evaluate the interpretability informativeness"
      },
      {
        "paperId": "5c92c3f103591d4ce73f59f1f4e4d7348cb2fae9",
        "title": "A dynamic analysis of UAV -based IoT networks for efficient communication using falcon optimization approach"
      },
      {
        "paperId": "4b9623ba4d89c699b482fd299e494e6fa8829aee",
        "title": "Deep Reinforcement Learning for UAV-Based SDWSN Data Collection"
      },
      {
        "paperId": "eb7f727de72669dec76e1f3e1b3e591a4fd30256",
        "title": "A Butterfly Algorithm That Combines Chaos Mapping and Fused Particle Swarm Optimization for UAV Path Planning"
      },
      {
        "paperId": "e5de2a51d6869c1ed4dd3dabc11a16b1f2b6f332",
        "title": "Joint Codebook Selection and MCS Adaptation for MmWave eMBB Services Based on Deep Reinforcement Learning"
      },
      {
        "paperId": "a60f46cc7b51968c9c6d1fa9545fe75773d3ccdf",
        "title": "SP-MSResNet: A Multiscale Residual Network With Strip Pooling Module for Intrusion Pattern Recognition"
      },
      {
        "paperId": "d476b47b74b451c97cffbcb66a8b975c0863508f",
        "title": "A Soft Actor-Critic Based Reinforcement Learning Approach for Motion Planning of UAVs Using Depth Images"
      },
      {
        "paperId": "8e62833f1cdd6f2dfda2e6ff167c3d15e25f1fef",
        "title": "Spatial interpolation of global DEM using federated deep learning"
      },
      {
        "paperId": "1f0e0edb66f659a6088f3b3f2a0142be7dbbb0dc",
        "title": "Lyapunov-Guided Deep Reinforcement Learning for Semantic-Aware AoI Minimization in UAV-Assisted Wireless Networks"
      },
      {
        "paperId": "ffc79cc003059648a56e97a57b9f8136bda9dce3",
        "title": "Enhancing Transportation Management in Marine Internet of Vessels: A 5G Broadcasting-Centric Framework Leveraging Federated Learning"
      },
      {
        "paperId": "991382289712caa6ff3db2824eea6586771fb893",
        "title": "High-Precision Underwater Perception and Path Planning of AUVs Based on Quantum-Enhanced"
      },
      {
        "paperId": "4cf6110f748dc540e6aad7b5a553e34ba0d1f3c7",
        "title": "FlightBench: Benchmarking Learning-Based Methods for Ego-Vision-Based Quadrotors Navigation"
      },
      {
        "paperId": "31b3f93465dd04aad226d1b11706e6bee3dfcade",
        "title": "Explainable Federated Medical Image Analysis Through Causal Learning and Blockchain"
      },
      {
        "paperId": "09aa408a240f95ebba47be1005785966f0ec7e7e",
        "title": "RadDQN: A Deep Q Learning-Based Architecture for Finding Time-Efficient Minimum Radiation Exposure Pathway"
      },
      {
        "paperId": "6ce2d7e3cfbe6fda9af25ce06cdce760982e5ccf",
        "title": "An Air-Ground Unmanned Swarm Collaborative Area Search Strategy Based on the Learning Wolf Pack Algorithm"
      },
      {
        "paperId": "0bea3b0d17dd2df754ed07419fdf330b7d7a29c0",
        "title": "Autonomous UAV last-mile delivery in urban environments: A survey on deep learning and reinforcement learning solutions"
      },
      {
        "paperId": "9d3f00eb7e1beef16a1fbb1fbb5d5fbf1bebf374",
        "title": "A Joint Network of Super-Resolution and Active Learning for Agricultural Land Classification"
      }
    ],
    "score": 44.0
  },
  {
    "id": "914eaadede7a95116362cd6982321f93044b3b19",
    "title": "Multi-USV Task Planning Method Based on Improved Deep Reinforcement Learning",
    "authors": [
      "Jing Zhang",
      "Jian-Lin Ren",
      "Yani Cui",
      "Delong Fu",
      "Jingyu Cong"
    ],
    "year": 2024,
    "citationCount": 41,
    "abstract": "A safe and reliable task planning method is a prerequisite for the collaborative execution of ocean observation data collection tasks by multiple unmanned surface vessels (multi-USVs). Deep reinforcement learning (DRL) combines the powerful nonlinear function-fitting capabilities of deep neural networks with the decision-making and control abilities of reinforcement learning, providing a novel approach to solving the multi-USV task planning problem. However, when applied to the field of multi-USV task planning, it faces challenges, such as a vast exploration space, extended training times, and unstable training process. To this end, this article proposes a multi-USV task planning method based on improved DRL. The proposed method draws on the idea of a value decomposition network, breaking down the multi-USV task planning problem into two subproblems: 1) task allocation and 2) autonomous collision avoidance. Different state spaces, action spaces, and reward functions are designed for the various subproblems. Based on this, a deep neural network is used to map the state space of each subproblem to the action space of each USV, and the generated strategy of the deep neural network is assessed based on the corresponding reward function. This successfully integrates task allocation and path planning into a comprehensive task planning framework. Deep neural networks consist of the Actor networks and the Critic networks. During the training phase of the Critic network, different methods are used to train different Critic networks to improve the convergence speed of the algorithm. An improved temporal difference error method is specifically applied to train the Critic network for evaluating autonomous collision avoidance strategies, resulting in improving the autonomous collision avoidance ability of USVs. At the same time, to improve the efficiency of task allocation, hierarchical mechanisms and regional division mechanisms are introduced to construct subsystem task planning models, which further decompose the task planning problem. A combination of successor features and an improved temporal difference error method is specifically applied to train another Critic network for evaluating the subsystem\u2019s task allocation schemes and collaborative motion trajectories, aiming to enhance the allocation efficiency of the subsystems. Furthermore, transfer learning is employed to merge the subsystem task planning, using it as a constraint to direct the exploration and assessment of both the cluster task allocation schemes and the cluster collaborative motion trajectories. This enables rapid and accurate learning for task allocation within the multi-USV cluster. During the training phase of the Actor network, the introduction of the experience replay method and target network technique is employed to enhance the proximal policy optimization algorithm. This facilitates distributed joint training of the Actor network, thereby improving the accuracy of the algorithm. Simulation results validate the effectiveness and superiority of this method.",
    "url": "https://www.semanticscholar.org/paper/914eaadede7a95116362cd6982321f93044b3b19",
    "pdf_url": "https://doi.org/10.1109/JIOT.2024.3363044",
    "venue": "IEEE Internet of Things Journal",
    "publicationDate": "2024-05-15",
    "externalIds": {
      "DBLP": "journals/iotj/ZhangRCFC24",
      "DOI": "10.1109/JIOT.2024.3363044",
      "CorpusId": 267535997
    },
    "references": [],
    "cited_by": [
      {
        "paperId": "229db90ebcf13499c8c0ad166aa15988149d532c",
        "title": "Deep reinforcement learning for collision avoidance in unmanned surface vehicles: State-of-the-art"
      },
      {
        "paperId": "89c82eceb7cee89f50f04ab167f5d8b511e775a6",
        "title": "Dynamic Task Allocation for UAV Swarms in Maritime Rescue Scenarios Based on PG-MAPPO"
      },
      {
        "paperId": "6ed465cab90563afe9d55c54a070cb0a5463d317",
        "title": "Learning-Based Transmission Scheduling Over USV-Oriented Maritime Communication Networks for Tracking Maintenance"
      },
      {
        "paperId": "1390c714a71d6e67cfabca9b9b2904e009ccdfc5",
        "title": "Dynamic Programming of Complex Tasks Based on Regretting Greedy Algorithm"
      },
      {
        "paperId": "b6f2c046f10a7c995dcfcf1ba5109ca8315c4d3d",
        "title": "Towards Autonomous Coordination of Two I-AUVs in Submarine Pipeline Assembly"
      },
      {
        "paperId": "b6cd478d1dbbfc5fc3b948debeb477176d33609c",
        "title": "A Novel Hybrid Enhanced Particle Swarm Optimization for UAV Path Planning"
      },
      {
        "paperId": "32b7fd3aba32dab7b1cfc3fd4815998ac69fca9f",
        "title": "A bibliometric analysis of the development of autonomous ships in inland waterway transport"
      },
      {
        "paperId": "78ed7eccc56aee667b683da5bcdea038c8335654",
        "title": "Time-varying formation trajectory tracking scheme for underactuated USVs based on adaptive sliding mode control with improved guidance strategy"
      },
      {
        "paperId": "9d39808505949eb37639d81d964f87c53e0f1ff1",
        "title": "Progressive deep reinforcement learning for intelligent collision avoidance in unmanned surface vehicles"
      },
      {
        "paperId": "aa97e71ae2becd5f8eb68191098af2d5f35fe40b",
        "title": "An Optimal Cooperative Hunting Strategy for Dynamic Water Surface Target"
      },
      {
        "paperId": "b9224e065c35c49b2d6092219374a519308585b1",
        "title": "Analysis of the research status of path planning for unmanned surface vehicles"
      },
      {
        "paperId": "aa0d9416c0892de8fb716dd65b1a1f68d04e1105",
        "title": "Embodied Intelligence: The Key to Unblocking Generalized Artificial Intelligence"
      },
      {
        "paperId": "56aa78eb50f019cbf4befc80c9aaf5bc36643c68",
        "title": "Research on Path Planning Multiple Mobile Robots Based on the LAPGWO Algorithm"
      },
      {
        "paperId": "c2e90666947ddcad37104f2756c65ba14d32ddbb",
        "title": "Intelligent decision and planning for unmanned surface vehicle: A review of machine learning techniques"
      },
      {
        "paperId": "2e3ac8c355c1ca1c1b1c9132eea7914001e27c5b",
        "title": "Adaptive collision avoidance strategy for USVs in perception-limited environments using dynamic priority guidance"
      },
      {
        "paperId": "5314b38bbc4dd828b4d8d6dbb4106379ce3cdec2",
        "title": "Distributed Event\u2010Triggered Optimization for Multiple USVs With Coupled Objective Functions"
      },
      {
        "paperId": "1762ca1044f4115b0a3dea7a16f83ac562f519c3",
        "title": "Game-Theoretic Cooperative Task Allocation for Multiple-Mobile-Robot Systems"
      },
      {
        "paperId": "31003551e2e3889b23c721bfe9e22a6d379b14a2",
        "title": "Hybrid path planning framework to integrate improved A*-DWA algorithms for enhancing path safety and efficiency"
      },
      {
        "paperId": "1843d452aad3f70440d99ab62fd76f9f7219137a",
        "title": "Cooperative formation and navigation methods for distributed unmanned surface vehicle systems with sensing information"
      },
      {
        "paperId": "258a2b612e2248466032627261a3615cb949c7d1",
        "title": "Human Implicit Preference-Based Policy Fine-tuning for Multi-Agent Reinforcement Learning in USV Swarm"
      },
      {
        "paperId": "2167f29910c85a39c07e023834f1bcb8c36db3e0",
        "title": "Design and Field Test of Collision Avoidance Method With Prediction for USVs: A Deep Deterministic Policy Gradient Approach"
      },
      {
        "paperId": "9f92c19cf2dcb1f44909dc23fb3d3306fb4ee03c",
        "title": "A port water navigation solution based on priority sampling SAC: Taking Yantai port environment as an example"
      },
      {
        "paperId": "31efa42cd7affa8c530b484cee72af1137bd9d31",
        "title": "Survey of AI-driven routing protocols in underwater acoustic networks for enhanced communication efficiency"
      },
      {
        "paperId": "20e219bb8481edaba09cf80c28c8650cae10217e",
        "title": "Adaptive quantum multi-objective parrot optimizer for task allocation of multi-UUV systems in underwater search"
      },
      {
        "paperId": "74426f826ee92ac0995fbfc9c26268803622ce69",
        "title": "Enhancing USV Station Keeping via Improved Maximum Entropy Deep Reinforcement Learning"
      },
      {
        "paperId": "c5a86f004c5c0753be7bbfe43f6ee8260110772a",
        "title": "Trends in Transfer Learning on Reinforcement Learning for Mobility Control"
      },
      {
        "paperId": "481ffa5d3e24638a0f4a53a4c856598a3e868660",
        "title": "An Improved Fruit Fly Optimization Algorithm for Disassembly Lines Requiring Multiskilled Workers"
      },
      {
        "paperId": "4daf6fec4ccebc32c9a8c17048084f55487c28bd",
        "title": "Formation Control of a Multi-Unmanned Surface Vessel System: A Bibliometric Analysis"
      },
      {
        "paperId": "d8f7f22373c58fa2a160bad754b78b672c81eb33",
        "title": "An efficient algorithm for data transmission certainty in IIoT sensing network: A priority-based approach"
      },
      {
        "paperId": "f854974f6ed786fc0f359bcf8e4b3fe2b02ecbda",
        "title": "An intelligent algorithm for energy efficiency optimization in software-defined wireless sensor networks for 5G communications"
      },
      {
        "paperId": "417cc565a3788f6def9a322f9056c0a9c3065ea2",
        "title": "Optimization of network topology robustness in IoTs: A systematic review"
      },
      {
        "paperId": "ce5db330b1c1f877bc3de95b5de16929ad3cf8ae",
        "title": "Receive wireless sensor data through IoT gateway using web client based on border gateway protocol"
      },
      {
        "paperId": "48f2a09cdcaf2295d17e76f9f3561c677a270955",
        "title": "Efficient and secure signcryption-based data aggregation for Internet of Drone-based drone-to-ground station communication"
      },
      {
        "paperId": "3a2e272635f75667edf23fb768f5c0cfa34e7b8a",
        "title": "ETFC: Energy-efficient and deadline-aware task scheduling in fog computing"
      },
      {
        "paperId": "212724fba5ebed0ca42b2f364ae232b29652250d",
        "title": "Cloud center energy consumption control for predictability in neural fuzzy systems"
      },
      {
        "paperId": "30ccd0c2a13355238d0027512222e454d623e53d",
        "title": "Experimental Advances in Airfoil Dynamic Stall and Transition Phenomena"
      },
      {
        "paperId": "85317b755e78bf515bd3b34ff88948587f0bccb7",
        "title": "Deep Reinforcement Learning of group consciousness for multi-robot pathfinding"
      },
      {
        "paperId": "46ce8e19dbfea2e94bfe8b528f93866c534d3300",
        "title": "Heterogeneous Multi-Agent Task Planning Method in Complex Marine Environment"
      },
      {
        "paperId": "9b4280d727d06ec5ef15d28c723a1db1ef6871d3",
        "title": "Path Planning Method for Unmanned Surface Vehicle Under High Temporal and Spatial Dynamic Characteristics of Marine Environment"
      },
      {
        "paperId": "aea2b9043bf8101eed08b89a8d04787d66e08157",
        "title": "Efficient Multi-Object Recognition Using GMM Segmentation Feature Fusion Approach"
      },
      {
        "paperId": "69216a6930a2801b3d23b566e2fe21ead39a6158",
        "title": "Novel Approach for Intrusion Detection Attacks on Small Drones Using ConvLSTM Model"
      }
    ],
    "score": 41.0
  },
  {
    "id": "7d6168fbd3ed72f9098573007f4b8c2ec9e576b9",
    "title": "Training Large Language Models for Reasoning through Reverse Curriculum Reinforcement Learning",
    "authors": [
      "Zhiheng Xi",
      "Wenxiang Chen",
      "Boyang Hong",
      "Senjie Jin",
      "Rui Zheng",
      "Wei He",
      "Yiwen Ding",
      "Shichun Liu",
      "Xin Guo",
      "Junzhe Wang",
      "Honglin Guo",
      "Wei Shen",
      "Xiaoran Fan",
      "Yuhao Zhou",
      "Shihan Dou",
      "Xiao Wang",
      "Xinbo Zhang",
      "Peng Sun",
      "Tao Gui",
      "Qi Zhang",
      "Xuanjing Huang"
    ],
    "year": 2024,
    "citationCount": 39,
    "abstract": "In this paper, we propose R$^3$: Learning Reasoning through Reverse Curriculum Reinforcement Learning (RL), a novel method that employs only outcome supervision to achieve the benefits of process supervision for large language models. The core challenge in applying RL to complex reasoning is to identify a sequence of actions that result in positive rewards and provide appropriate supervision for optimization. Outcome supervision provides sparse rewards for final results without identifying error locations, whereas process supervision offers step-wise rewards but requires extensive manual annotation. R$^3$ overcomes these limitations by learning from correct demonstrations. Specifically, R$^3$ progressively slides the start state of reasoning from a demonstration's end to its beginning, facilitating easier model exploration at all stages. Thus, R$^3$ establishes a step-wise curriculum, allowing outcome supervision to offer step-level signals and precisely pinpoint errors. Using Llama2-7B, our method surpasses RL baseline on eight reasoning tasks by $4.1$ points on average. Notebaly, in program-based reasoning on GSM8K, it exceeds the baseline by $4.2$ points across three backbone models, and without any extra data, Codellama-7B + R$^3$ performs comparable to larger models or closed-source models.",
    "url": "https://www.semanticscholar.org/paper/7d6168fbd3ed72f9098573007f4b8c2ec9e576b9",
    "pdf_url": "https://arxiv.org/pdf/2402.05808.pdf",
    "venue": "International Conference on Machine Learning",
    "publicationDate": "2024-02-08",
    "externalIds": {
      "DBLP": "conf/icml/XiCHJZHDLGWGSFZ24",
      "ArXiv": "2402.05808",
      "DOI": "10.48550/arXiv.2402.05808",
      "CorpusId": 267547500
    },
    "references": [
      {
        "paperId": "7c16ef4e3c13265307c3569cc8f8ec5b0f7b0991",
        "title": "Secrets of RLHF in Large Language Models Part II: Reward Modeling"
      },
      {
        "paperId": "288e64e8adb23d81e291a2cb51e3a56b315023b7",
        "title": "OVM, Outcome-supervised Value Models for Planning in Mathematical Reasoning"
      },
      {
        "paperId": "b272513916b45c8517d289d7abee4a53e6832187",
        "title": "ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving"
      },
      {
        "paperId": "f42f61a547c5996be6aee175145b0d74e6324dff",
        "title": "Navigate through Enigmatic Labyrinth A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future"
      },
      {
        "paperId": "77b1f1c6d1658d120456b9046667cf009ceb39ce",
        "title": "MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models"
      },
      {
        "paperId": "2d4ee2195cf1932eb6b2d940c241f4f4443c0a6e",
        "title": "Design of Chain-of-Thought in Math Problem Solving"
      },
      {
        "paperId": "a3dd7d33dfaa9e02e43d92e900cba01f52d8c4b9",
        "title": "MAmmoTH: Building Math Generalist Models through Hybrid Instruction Tuning"
      },
      {
        "paperId": "74b4b993babe99bc5f5c589c27fef0f1baba606b",
        "title": "Making Large Language Models Better Reasoners with Alignment"
      },
      {
        "paperId": "d00735241af700d21762d2f3ca00d920241a15a4",
        "title": "Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models"
      },
      {
        "paperId": "0b0debb710366cdff461938c80763eace1651af6",
        "title": "Code Llama: Open Foundation Models for Code"
      },
      {
        "paperId": "dd18782960f9ee4c66b79e1518b342ad3f8d19e7",
        "title": "WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct"
      },
      {
        "paperId": "182c7b40ff7560a5545764814338f55a2098e441",
        "title": "Reinforced Self-Training (ReST) for Language Modeling"
      },
      {
        "paperId": "1dbd58bd8768ba0dada2e7c84aa2fe0b9f418ebc",
        "title": "Solving Challenging Math Word Problems Using GPT-4 Code Interpreter with Code-based Self-Verification"
      },
      {
        "paperId": "548278897d46a54958909bb23bcaecf63e24fadf",
        "title": "Secrets of RLHF in Large Language Models Part I: PPO"
      },
      {
        "paperId": "f021aebbc4c8d38f55470ad11bfb1a2c59b788a7",
        "title": "BoardgameQA: A Dataset for Natural Language Reasoning with Contradictory Information"
      },
      {
        "paperId": "be8db99310602d66bba64bcf41a572c45816fbfc",
        "title": "Let's Verify Step by Step"
      },
      {
        "paperId": "0744580e75a74357e466a57082c85cb42f548aa9",
        "title": "The CoT Collection: Improving Zero-shot and Few-shot Learning of Language Models via Chain-of-Thought Fine-Tuning"
      },
      {
        "paperId": "9a9b1e2968302eb882870537d4af6e2c722dfd1a",
        "title": "Self-Polish: Enhance Reasoning in Large Language Models via Problem Refinement"
      },
      {
        "paperId": "c3ed333a37a6d9a0fcf1dad3106a114f66a45b99",
        "title": "WebCPM: Interactive Web Search for Chinese Long-form Question Answering"
      },
      {
        "paperId": "ef018d9fad6167cfddb7d6654c5422df1e953730",
        "title": "Self-Evaluation Guided Beam Search for Reasoning"
      },
      {
        "paperId": "163b4d6a79a5b19af88b8585456363340d9efd04",
        "title": "GPT-4 Technical Report"
      },
      {
        "paperId": "3d68522abfadfc8ee6b7ec9edaaf91f1b2f38e5e",
        "title": "Large Language Models Can Be Easily Distracted by Irrelevant Context"
      },
      {
        "paperId": "6845bea94b2fb17d4377b3bb2bd10f73a959f9cc",
        "title": "Reasoning with Language Model Prompting: A Survey"
      },
      {
        "paperId": "6d7b8a478801bd9d21df82d5f33ae6eced90da5e",
        "title": "Solving math word problems with process- and outcome-based feedback"
      },
      {
        "paperId": "7d645a3fd276918374fd9483fd675c28e46506d1",
        "title": "Galactica: A Large Language Model for Science"
      },
      {
        "paperId": "6d994b4f5a46cd14e8f09f1e9e49120546b15e31",
        "title": "CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning"
      },
      {
        "paperId": "e7ad08848d5d7c5c47673ffe0da06af443643bda",
        "title": "Large Language Models are Zero-Shot Reasoners"
      },
      {
        "paperId": "0286b2736a114198b25fb5553c671c33aed5d477",
        "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "da1e404ae194dcbcce198f5335f49f090403bbd7",
        "title": "Exploration in Deep Reinforcement Learning: A Survey"
      },
      {
        "paperId": "1b6e810ce0afd0dd093f789d2b2742d047e316d5",
        "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models"
      },
      {
        "paperId": "2f3efe44083af91cef562c1a3451eee2f8601d22",
        "title": "WebGPT: Browser-assisted question-answering with human feedback"
      },
      {
        "paperId": "3dc7dc1bea9a4f70c02b6759a0bda7aca0005a9e",
        "title": "A General Language Assistant as a Laboratory for Alignment"
      },
      {
        "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
        "title": "Training Verifiers to Solve Math Word Problems"
      },
      {
        "paperId": "0eada7dcf0f92adf668253461435ade904284b7b",
        "title": "Trajectory-based Split Hindsight Reverse Curriculum Learning"
      },
      {
        "paperId": "4698fc4712f0212c8a3810fd67b41ee8b8896aba",
        "title": "Generate & Rank: A Multi-task Framework for Math Word Problems"
      },
      {
        "paperId": "76e9e2ec3de437ffb30d8b7b629f7fe3e61de5c2",
        "title": "On the Opportunities and Risks of Foundation Models"
      },
      {
        "paperId": "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269",
        "title": "Evaluating Large Language Models Trained on Code"
      },
      {
        "paperId": "13c4e5a6122f3fa2663f63e49537091da6532f35",
        "title": "Are NLP Models really able to Solve Simple Math Word Problems?"
      },
      {
        "paperId": "053b1d7b97eb2c91fc3921d589c160b0923c70b1",
        "title": "Learning to summarize from human feedback"
      },
      {
        "paperId": "725264948d7b6946259af5b8d966e996b9570f99",
        "title": "DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters"
      },
      {
        "paperId": "e0645432d259a1428fd56f26986bb20012707770",
        "title": "Rethinking Defeasible Reasoning: A Scalable Approach"
      },
      {
        "paperId": "5d4ebbaa1ef8feeac885e2869f45c0276c18834f",
        "title": "Learning Montezuma's Revenge from a Single Demonstration"
      },
      {
        "paperId": "a6bfb1b817062e0176490139f9f49c5eea00afcb",
        "title": "On a Flexible Representation for Defeasible Reasoning Variants"
      },
      {
        "paperId": "ce1c28ca2f52a42c6e60d792cd71ba894abc47d5",
        "title": "Multi-Goal Reinforcement Learning: Challenging Robotics Environments and Request for Research"
      },
      {
        "paperId": "c28ec2a40a2c77e20d64cf1c85dc931106df8e83",
        "title": "Overcoming Exploration in Reinforcement Learning with Demonstrations"
      },
      {
        "paperId": "cbd569036fc72ae7ff747350b91816440282596b",
        "title": "Seq2SQL: Generating Structured Queries from Natural Language using Reinforcement Learning"
      },
      {
        "paperId": "7ad66cba3b7e3abae7ef33122588512a146f7f77",
        "title": "A Survey on Multi-Task Learning"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "9862caed8ee93321c78b0196e0b7eef516b545ba",
        "title": "Reverse Curriculum Generation for Reinforcement Learning"
      },
      {
        "paperId": "6d431f835c06afdea45dff6b24486bf301ebdef0",
        "title": "An Overview of Multi-Task Learning in Deep Neural Networks"
      },
      {
        "paperId": "471f9742b4e32d8ee68f9ee493768ff0466a231d",
        "title": "Automatic Goal Generation for Reinforcement Learning Agents"
      },
      {
        "paperId": "5ded2b8c64491b4a67f6d39ce473d4b9347a672e",
        "title": "A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference"
      },
      {
        "paperId": "636a79420d838eabe4af7fb25d6437de45ab64e8",
        "title": "RACE: Large-scale ReAding Comprehension Dataset From Examinations"
      },
      {
        "paperId": "017b07fe36e8d43965b2125f6170a97c9d747fca",
        "title": "Data-efficient Deep Reinforcement Learning for Dexterous Manipulation"
      },
      {
        "paperId": "6fab0b3b321988cedd0a017c1dad997f6d4da930",
        "title": "Playing Atari Games with Deep Reinforcement Learning and Human Checkpoint Replay"
      },
      {
        "paperId": "dc0adbb27267f1ae88dc5c9982ec7acb5124b95f",
        "title": "Exploration from Demonstration for Interactive Reinforcement Learning"
      },
      {
        "paperId": "69e76e16740ed69f4dc55361a3d319ac2f1293dd",
        "title": "Asynchronous Methods for Deep Reinforcement Learning"
      },
      {
        "paperId": "f04df4e20a18358ea2f689b4c129781628ef7fc1",
        "title": "A large annotated corpus for learning natural language inference"
      },
      {
        "paperId": "523b4ce1c2a1336962444abc1dec215756c2f3e6",
        "title": "Approximately Optimal Approximate Reinforcement Learning"
      },
      {
        "paperId": "e811e771f5950d86eafe50655c0d1e5b571e19b6",
        "title": "The Unreliability of Explanations in Few-Shot In-Context Learning"
      },
      {
        "paperId": "122a4c6cf1952feff84648c06000bc93d9709637",
        "title": "Defeasible Reasoning"
      },
      {
        "paperId": null,
        "title": "Dynamic programming and optimal control: Volume I , volume 4"
      },
      {
        "paperId": null,
        "title": "Reinforcement learning, bit by bit. Found"
      },
      {
        "paperId": null,
        "title": "9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference"
      },
      {
        "paperId": null,
        "title": "Alpaca: A strong, replicable instruction-following model"
      },
      {
        "paperId": null,
        "title": "DeepMimic: Example-Guided Deep Reinforcement Learning of Physics-Based Character Skills"
      },
      {
        "paperId": null,
        "title": "program-aided language models"
      },
      {
        "paperId": null,
        "title": "A large-scale adversarial dataset for grounded common-sense inference"
      },
      {
        "paperId": "c35fb9c7f10a60488ded4a1bb520e68c9ec957a5",
        "title": "Failure Modes of Learning Reward Models for LLMs and other Sequence Models"
      }
    ],
    "cited_by": [
      {
        "paperId": "761600ac0c503e7d19969a19e788bb1e91828f8e",
        "title": "Thinking Sparks!: Emergent Attention Heads in Reasoning Models During Post Training"
      },
      {
        "paperId": "714abcc8b1c6527844a5f8ed691f1a85e7fb25a7",
        "title": "VCRL: Variance-based Curriculum Reinforcement Learning for Large Language Models"
      },
      {
        "paperId": "7d99c6fb68ef2de8dc2a1c67937d86b4a9e8f817",
        "title": "LLM-based Agents Suffer from Hallucinations: A Survey of Taxonomy, Methods, and Directions"
      },
      {
        "paperId": "30da58c425d4e2a5e3c0b774cf8302d2fcf9ce51",
        "title": "AgentGym-RL: Training LLM Agents for Long-Horizon Decision Making through Multi-Turn Reinforcement Learning"
      },
      {
        "paperId": "5d8aff62a93f0bb2a9f0c4943f9e0bc19a602130",
        "title": "Can Large Models Teach Student Models to Solve Mathematical Problems Like Human Beings? A Reasoning Distillation Method via Multi-LoRA Interaction"
      },
      {
        "paperId": "1ae10be82a6c09b6d53b6932d764f6f34bbeb9e4",
        "title": "EvoCoT: Overcoming the Exploration Bottleneck in Reinforcement Learning"
      },
      {
        "paperId": "bf4b0c26fd62c334d5a6eaac7e2e01bc54d2f316",
        "title": "IRGPT: Understanding Real-world Infrared Image with Bi-cross-modal Curriculum on Large-scale Benchmark"
      },
      {
        "paperId": "92857d75fddee456ec2a3b14e05020eac89eb308",
        "title": "From Reasoning to Super-Intelligence: A Search-Theoretic Perspective"
      },
      {
        "paperId": "a7d42f60c7db599b3500d31f1353b83c841a3c39",
        "title": "From Data-Centric to Sample-Centric: Enhancing LLM Reasoning via Progressive Optimization"
      },
      {
        "paperId": "2868932ec97c80d9a17376ee5dacda7e3121e269",
        "title": "RL for Reasoning by Adaptively Revealing Rationales"
      },
      {
        "paperId": "bad49375ec986ca3d4a6afa228340c474deeab49",
        "title": "FreePRM: Training Process Reward Models Without Ground Truth Process Labels"
      },
      {
        "paperId": "87918c80365e2ca82611dcd6313915aa4c13ded8",
        "title": "From Large AI Models to Agentic AI: A Tutorial on Future Intelligent Communications"
      },
      {
        "paperId": "02bc41796acfbba89c01785ee8635ea960f67f3e",
        "title": "Token-Importance Guided Direct Preference Optimization"
      },
      {
        "paperId": "859caa7e5a365c5c13d5fb0c614ac8d151126e29",
        "title": "UFT: Unifying Supervised and Reinforcement Fine-Tuning"
      },
      {
        "paperId": "be1985ec9f0bce3b45c81e7b28b93acbaf7e20eb",
        "title": "Self-Evolving Curriculum for LLM Reasoning"
      },
      {
        "paperId": "6462d1fd642d1a64646d53e338a2b94c6b52e96d",
        "title": "MARGE: Improving Math Reasoning for LLMs with Guided Exploration"
      },
      {
        "paperId": "f0870be65f480a57d28f792f89246cae90431a5f",
        "title": "Improving RL Exploration for LLM Reasoning through Retrospective Replay"
      },
      {
        "paperId": "4e0016dd49b44e07c50f8adea6ea8c21bc4c0b1e",
        "title": "DUMP: Automated Distribution-Level Curriculum Learning for RL-based LLM Post-training"
      },
      {
        "paperId": "1738bfdda961ddc9d31b39f3dd38b1f425199e94",
        "title": "Right Question is Already Half the Answer: Fully Unsupervised LLM Reasoning Incentivization"
      },
      {
        "paperId": "9e57c154c4a5d6a64012c66dd08084d030f68a07",
        "title": "LEMMA: Learning from Errors for MatheMatical Advancement in LLMs"
      },
      {
        "paperId": "62cf5a720909a0f82854bf4a7508a7e2b46ed4ca",
        "title": "SOLAR: Scalable Optimization of Large-scale Architecture for Reasoning"
      },
      {
        "paperId": "e145627eb1501d0ef0439757c8b556f77afdfe43",
        "title": "Improving Retrospective Language Agents via Joint Policy Gradient Optimization"
      },
      {
        "paperId": "6b34d9f4a91670a265ce51ce4be71cdbf8e15d05",
        "title": "LLM Post-Training: A Deep Dive into Reasoning Large Language Models"
      },
      {
        "paperId": "8df6708751a84287e43dc40a62b33a8a165b5abe",
        "title": "Improving Value-based Process Verifier via Structural Prior Injection"
      },
      {
        "paperId": "4a7e1870fa76329da369211ab0630ce4960402f6",
        "title": "The Lessons of Developing Process Reward Models in Mathematical Reasoning"
      },
      {
        "paperId": "5bd36b2c701056984a6353edc8ebdd3a3864bc34",
        "title": "Beyond Examples: High-level Automated Reasoning Paradigm in In-Context Learning via MCTS"
      },
      {
        "paperId": "019bb0ba4beee75d92d80cbb9d79cca055f2c496",
        "title": "Understanding Chain-of-Thought in LLMs through Information Theory"
      },
      {
        "paperId": "78d9c14c845c5ead1a3a839a3463d0e4e2cb30e2",
        "title": "Enhancing the Reasoning Ability of Multimodal Large Language Models via Mixed Preference Optimization"
      },
      {
        "paperId": "e409ce659b1aaa70d6c6ea2c4891e50322138b7a",
        "title": "Mitigating Tail Narrowing in LLM Self-Improvement via Socratic-Guided Sampling"
      },
      {
        "paperId": "10f278d20b235a4ce3c552da9c4642de3a98b483",
        "title": "Distill Visual Chart Reasoning Ability from LLMs to MLLMs"
      },
      {
        "paperId": "03bae9c8d33a312987dc0b14fcfc7d31890a8829",
        "title": "Autoregressive Multi-trait Essay Scoring via Reinforcement Learning with Scoring-aware Multiple Rewards"
      },
      {
        "paperId": "e13362d862a844a14d9d43f80ef12118d06621a4",
        "title": "Enhancing Decision-Making for LLM Agents via Step-Level Q-Value Models"
      },
      {
        "paperId": "ef2550da955a926b249b110d02e1602b9f033f1c",
        "title": "Step-Controlled DPO: Leveraging Stepwise Error for Enhanced Mathematical Reasoning"
      },
      {
        "paperId": "3072ad4982cd916606ac88ea1883d4d725c4eda4",
        "title": "AgentGym: Evolving Large Language Model-based Agents across Diverse Environments"
      },
      {
        "paperId": "ed502eae116b7ce63147d210d86bfde6bb04ba68",
        "title": "Exploring the Compositional Deficiency of Large Language Models in Mathematical Reasoning Through Trap Problems"
      },
      {
        "paperId": "3f573716cadfe0bf5c548e2dd384ef2012beb4eb",
        "title": "Masked Thought: Simply Masking Partial Reasoning Steps Can Improve Mathematical Reasoning Learning of Language Models"
      },
      {
        "paperId": "fa9ddaa3c15543e0abcadba8671a198816f0c910",
        "title": "DORA: Dynamic Optimization Prompt for Continuous Reflection of LLM-based Agent"
      },
      {
        "paperId": "65332c006c114a9eb8f1a848f4aad7214bba6514",
        "title": "Flow of Reasoning: Efficient Training of LLM Policy with Divergent Thinking"
      },
      {
        "paperId": "98867f10f0ee0914667a6b92700e8dc8c8929ae3",
        "title": "F LOW OF R EASONING : T RAINING LLM S FOR D IVER - GENT P ROBLEM S OLVING WITH M INIMAL E XAMPLES"
      }
    ],
    "score": 39.0
  },
  {
    "id": "a694a3fb49f3c6e63e2fa5ddb72fe3aa490592f1",
    "title": "Toward Enhanced Reinforcement Learning-Based Resource Management via Digital Twin: Opportunities, Applications, and Challenges",
    "authors": [
      "Nan Cheng",
      "Xiucheng Wang",
      "Zan Li",
      "Zhisheng Yin",
      "Tom H. Luan",
      "Xuemin Shen"
    ],
    "year": 2024,
    "citationCount": 21,
    "abstract": "This article presents a digital twin (DT)-enhanced reinforcement learning (RL) framework aimed at optimizing performance and reliability in network resource management, since the traditional RL methods face several unified challenges when applied to physical networks, including limited exploration efficiency, slow convergence, poor long-term performance, and safety concerns during the exploration phase. To deal with the above challenges, a comprehensive DT-based framework is proposed to enhance the convergence speed and performance for unified RL-based resource management. The proposed framework provides safe action exploration, more accurate estimates of long-term returns, faster training convergence, higher convergence performance, and real-time adaptation to varying network conditions. Then, two case studies on ultra-reliable and low-latency communication (URLLC) services and multiple unmanned aerial vehicles (UAV) network are presented, demonstrating improvements of the proposed framework in performance, convergence speed, and training cost reduction both on traditional RL and neural network based Deep RL (DRL). Finally, the article identifies and explores some of the research challenges and open issues in this rapidly evolving field.",
    "url": "https://www.semanticscholar.org/paper/a694a3fb49f3c6e63e2fa5ddb72fe3aa490592f1",
    "pdf_url": "https://arxiv.org/pdf/2406.07857.pdf",
    "venue": "IEEE Network",
    "publicationDate": "2024-06-12",
    "externalIds": {
      "DBLP": "journals/network/ChengWLYLS25",
      "ArXiv": "2406.07857",
      "DOI": "10.1109/MNET.2024.3438543",
      "CorpusId": 270391435
    },
    "references": [
      {
        "paperId": "263154bef578a67e9a5749475fbc13cb749dc0a8",
        "title": "Dynamic Neural Network-Based Resource Management for Mobile Edge Computing in 6G Networks"
      },
      {
        "paperId": "d9b6f33701ccc0cec789b28835faa5284a4adcae",
        "title": "Imperfect Digital Twin Assisted Low Cost Reinforcement Training for Multi-UAV Networks"
      },
      {
        "paperId": "2460c7093c5dd78653b7c19a836334090e989256",
        "title": "Holistic Network Virtualization and Pervasive Network Intelligence for 6G"
      },
      {
        "paperId": "5793550ec7f5eb63086b8b7ae26514015ab7603c",
        "title": "A Dynamic Hierarchical Framework for IoT-Assisted Digital Twin Synchronization in the Metaverse"
      },
      {
        "paperId": "ecf5dc817fd6326e943b759c889d1285e673b24a",
        "title": "Digital Twin-Assisted Efficient Reinforcement Learning for Edge Task Scheduling"
      },
      {
        "paperId": "da1e404ae194dcbcce198f5335f49f090403bbd7",
        "title": "Exploration in Deep Reinforcement Learning: A Survey"
      },
      {
        "paperId": "de1a97b38cd5d8e4b2d5154c92aedf3c42b9452f",
        "title": "The Paradigm of Digital Twin Communications"
      },
      {
        "paperId": "a9a9ffce6ca18c41acc0bb08dccfbc55db597438",
        "title": "Machine Learning for 6G Wireless Networks: Carrying Forward Enhanced Bandwidth, Massive Access, and Ultrareliable/Low-Latency Service"
      },
      {
        "paperId": "6b5e8a917cebbacc4937f6d033b12a7a5de683ca",
        "title": "Single and Multi-Agent Deep Reinforcement Learning for AI-Enabled Wireless Networks: A Tutorial"
      },
      {
        "paperId": "f8492a321d66c381637b693a24af994af41b3cdf",
        "title": "Transfer Learning in Deep Reinforcement Learning: A Survey"
      },
      {
        "paperId": "2bc19ef65806db28f2c54aea436371387a4b6e27",
        "title": "Resource Management at the Network Edge: A Deep Reinforcement Learning Approach"
      },
      {
        "paperId": "1f9bb9e2edb6c8721e85c0c1141ddb45c9d9fb99",
        "title": "Multi-Agent Reinforcement Learning-Based Resource Allocation for UAV Networks"
      },
      {
        "paperId": "1cb6edbedc4a1ac5c32f61a435a23264e42a9071",
        "title": "Towards Sample Efficient Reinforcement Learning"
      },
      {
        "paperId": "97efafdb4a3942ab3efba53ded7413199f79c054",
        "title": "Reinforcement Learning: An Introduction"
      }
    ],
    "cited_by": [
      {
        "paperId": "2138d3bb41cc0bde43f2a0994cc61b271df61944",
        "title": "Accelerating Reinforcement Learning Algorithms Convergence using Pre-trained Large Language Models as Tutors With Advice Reusing"
      },
      {
        "paperId": "183c61234329b88c1d649f4e800f8c8d6555df4f",
        "title": "Edge General Intelligence Through World Models and Agentic AI: Fundamentals, Solutions, and Challenges"
      },
      {
        "paperId": "3dc180970574809198855e1170f779aafca57de6",
        "title": "Hybrid Model-Based and Data-Driven DRL Optimization Approach: A Novel DRL Design to Accelerate Convergence"
      },
      {
        "paperId": "18f24771459b75b7fdce4274b1cbe2fae8cfae8c",
        "title": "Latent Diffusion Model Based Denoising Receiver for 6G Semantic Communication: From Stochastic Differential Theory to Application"
      },
      {
        "paperId": "e8b6930ea2c993fa57c4c4fc15346d15dac60c41",
        "title": "Digital Twin Network-Based 6G Self-Evolution"
      },
      {
        "paperId": "0f27950d0339e3586c335b4d17fd79baa48f88ce",
        "title": "Digital twin-driven self-adaptive reconfiguration planning method of smart manufacturing systems using game theory and deep Q-network for industry 5.0"
      },
      {
        "paperId": "71cd62a89d6efeeee1b2bad53f784a0fc29ede5d",
        "title": "Combining meta reinforcement learning with neural plasticity mechanisms for improved AI performance"
      },
      {
        "paperId": "42cb8236b88fd562e3cd06b2e3eda691fe9d4147",
        "title": "ACDV: Adaptive Content Delivery for Vehicular Digital Twin Networks"
      },
      {
        "paperId": "47955327f2b35e2e0028faeba595526211959f99",
        "title": "Digital Twin-Assisted Adaptive Federated Multi-Agent DRL with GenAI for Optimized Resource Allocation in IoV Networks"
      },
      {
        "paperId": "e05052df6fa1c9eedde2aa1c4724526275270d90",
        "title": "Synergizing AI and Digital Twins for Next-Generation Network Optimization, Forecasting, and Security"
      },
      {
        "paperId": "cb517195576ddd8a397c08a2897c2a0070d9d985",
        "title": "GenAI-Enhanced Federated Multiagent DRL for Digital-Twin-Assisted IoV Networks"
      },
      {
        "paperId": "22e26d13f21d435095c33b2c52f7b3a5222d1ff3",
        "title": "Provable Performance Bounds for Digital Twin-driven Deep Reinforcement Learning in Wireless Networks: A Novel Digital-Twin Bisimulation Metric"
      },
      {
        "paperId": "f0f6ec56fed4d6c70ac3bd27cff716039d93341d",
        "title": "Continual Reinforcement Learning for Digital Twin Synchronization Optimization"
      },
      {
        "paperId": "495860d10654b2282191864db0474c9a0819d20d",
        "title": "QoE-oriented Communication Service Provision for Annotation Rendering in Mobile Augmented Reality"
      },
      {
        "paperId": "5fa6137a4066eadad90ebef76ca8a634ce4737f5",
        "title": "Hybrid Machine Learning-Based 3-Dimensional UAV Node Localization for UAV-Assisted Wireless Networks"
      },
      {
        "paperId": "da7db72525b296765f474466ccd12b9b6ec6aa7c",
        "title": "AGV-Assisted Data Collection Strategies in Industrial IoT: A Value of Information Perspective"
      },
      {
        "paperId": "e41cf52fbc5692e1b37151048a59439813be3bf6",
        "title": "Digital Twin in Industries: A Comprehensive Survey"
      },
      {
        "paperId": "43c88764827b5353d88be5c3965c8bc8d0fb350d",
        "title": "Advancing Experimental Platforms for UAV Communications: Insights from AERPAW\u2019S Digital Twin"
      },
      {
        "paperId": "5457ec19d013f59a4fbbc24d655239877fb1ab8e",
        "title": "Trapezoidal Gradient Descent for Effective Reinforcement Learning in Spiking Networks"
      },
      {
        "paperId": "3ebfff2aa6a33481ac1a7949fd31c78942f5321d",
        "title": "Constructing and Evaluating Digital Twins: An Intelligent Framework for DT Development"
      },
      {
        "paperId": "20c17985b88f0f26c30b725c3c69b3860a1f859d",
        "title": "Digital Twin With Soft Actor-Critic Reinforcement Learning for Transitioning From Industry 4.0 to 5.0"
      }
    ],
    "score": 21.0
  },
  {
    "id": "25db1b77bc330476c3cf6ce43236404c578b4372",
    "title": "Model-Bellman Inconsistency for Model-based Offline Reinforcement Learning",
    "authors": [
      "Yihao Sun",
      "Jiajin Zhang",
      "Chengxing Jia",
      "Hao-Chu Lin",
      "Junyin Ye",
      "Yangze Yu"
    ],
    "year": 2023,
    "citationCount": 42,
    "abstract": "",
    "url": "https://www.semanticscholar.org/paper/25db1b77bc330476c3cf6ce43236404c578b4372",
    "pdf_url": null,
    "venue": "International Conference on Machine Learning",
    "publicationDate": "",
    "externalIds": {
      "DBLP": "conf/icml/SunZJLYY23",
      "CorpusId": 260846642
    },
    "references": [],
    "cited_by": [
      {
        "paperId": "37483be6933d29635e6a5483ba3f1ee2b3dcafc8",
        "title": "MOORL: A Framework for Integrating Offline-Online Reinforcement Learning"
      },
      {
        "paperId": "a552bef347f7992870a96682ba7534a60468c018",
        "title": "Hybrid Cross-domain Robust Reinforcement Learning"
      },
      {
        "paperId": "e5106930a3b7dde40623701cbf5918dbd558b53c",
        "title": "SOReL and TOReL: Two Methods for Fully Offline Reinforcement Learning"
      },
      {
        "paperId": "c3f2c71e6aa8987b10f8d614ffbc1331b37b68bf",
        "title": "Learning to Trust Bellman Updates: Selective State-Adaptive Regularization for Offline RL"
      },
      {
        "paperId": "6abfd973391e24fd55b47218e40b457c3c9b7595",
        "title": "Policy-Driven World Model Adaptation for Robust Offline Model-based Reinforcement Learning"
      },
      {
        "paperId": "9da58a29f476f560dda373976e820a369fba9568",
        "title": "Temporal Distance-aware Transition Augmentation for Offline Model-based Reinforcement Learning"
      },
      {
        "paperId": "ad258b6e72e9c70418dd7c58026d4d0cf66a5fca",
        "title": "Video-Enhanced Offline Reinforcement Learning: A Model-Based Approach"
      },
      {
        "paperId": "66e649ae0236bd90c84c3fe71210a07258653ec8",
        "title": "Taming OOD Actions for Offline Reinforcement Learning: An Advantage-Based Approach"
      },
      {
        "paperId": "459c0fa2605be2c3e4febc7f95b6178d9760809a",
        "title": "Uncertainty-aware Latent Safety Filters for Avoiding Out-of-Distribution Failures"
      },
      {
        "paperId": "552dbdb461d77915f71713e5a69912a045e9b613",
        "title": "VIPO: Value Function Inconsistency Penalized Offline Reinforcement Learning"
      },
      {
        "paperId": "089ff72ad5b2c25a1a4f96d1d207530b94946a0a",
        "title": "NeoRL-2: Near Real-World Benchmarks for Offline Reinforcement Learning with Extended Realistic Scenarios"
      },
      {
        "paperId": "e1f707f6f78024bf18ab812b3899f068f0f6bf10",
        "title": "Enhancing Offline Model-Based RL via Active Model Selection: A Bayesian Optimization Perspective"
      },
      {
        "paperId": "69dcde212b9cbfd0d6354184747e1d2d5f4443da",
        "title": "Model-Based Offline Reinforcement Learning with Reliability-Guaranteed Sequence Modeling"
      },
      {
        "paperId": "6c2c31e9dbaff4697886d71b156b0d1a861264f5",
        "title": "Behavior-Regularized Diffusion Policy Optimization for Offline Reinforcement Learning"
      },
      {
        "paperId": "106e497d33d084df10addfe14292638a9e8b6cc9",
        "title": "Dual Alignment Maximin Optimization for Offline Model-based RL"
      },
      {
        "paperId": "f77daccdaff11cd60e12fa26e49231caad665046",
        "title": "Conservative reward enhancement through the nearest neighbor integration in model-based Offline Policy Optimization"
      },
      {
        "paperId": "4f3367c6fd56a3a2575b0108d8a27b949af100cb",
        "title": "Maximization Operator Uncertainty Penalty for Model-Based Offline Reinforcement Learning"
      },
      {
        "paperId": "df8274ae7e12c541d417601b9e85f455f08b42ea",
        "title": "A Synthetic Data-enhanced Differential Privacy Reinforcement Learning Policy Optimization Method"
      },
      {
        "paperId": "c594010e0dbe6fac94caebb0578e3f22d1fe95bd",
        "title": "Doubly Mild Generalization for Offline Reinforcement Learning"
      },
      {
        "paperId": "d319d19973a15c3e9e9dd6875720b4d7e73599fa",
        "title": "Constrained Latent Action Policies for Model-Based Offline Reinforcement Learning"
      },
      {
        "paperId": "bd35201507bbf3a304d5b854c10d5d00fbc6f758",
        "title": "Offline Reinforcement Learning with OOD State Correction and OOD Action Suppression"
      },
      {
        "paperId": "efdc2ba255b5d0ab258f74669fbb22fe5182d7d1",
        "title": "Practical Probabilistic Model-Based Reinforcement Learning by Integrating Dropout Uncertainty and Trajectory Sampling"
      },
      {
        "paperId": "753989375fed1c872d6f367de3320f37490a01cc",
        "title": "Bayes Adaptive Monte Carlo Tree Search for Offline Model-based Reinforcement Learning"
      },
      {
        "paperId": "ea04ceb7f6c05972354b53df1cda302403a5751f",
        "title": "SAMBO-RL: Shifts-aware Model-based Offline Reinforcement Learning"
      },
      {
        "paperId": "e83c0c3bf031ead0547bea7ad5242e842169790c",
        "title": "Offline Model-Based Reinforcement Learning with Anti-Exploration"
      },
      {
        "paperId": "6ec0c8668d0cd595fcc2c6a780b312c815c223a4",
        "title": "Diversification of Adaptive Policy for Effective Offline Reinforcement Learning"
      },
      {
        "paperId": "de8485113b257928b0c76a0727161355056271a4",
        "title": "Model-Based Offline Reinforcement Learning for Autonomous Delivery of Guidewire"
      },
      {
        "paperId": "e11a3afedfbf4fdda3699fcc28713d9b94afa939",
        "title": "BECAUSE: Bilinear Causal Representation for Generalizable Offline Model-based Reinforcement Learning"
      },
      {
        "paperId": "ee9e4546012e53ba40d929569090d9354033a3ae",
        "title": "Model-based Offline Reinforcement Learning with Lower Expectile Q-Learning"
      },
      {
        "paperId": "03006dbe170d0f98b7cc4b94ab5ebb95dfffbcfa",
        "title": "Deterministic Uncertainty Propagation for Improved Model-Based Offline Reinforcement Learning"
      },
      {
        "paperId": "45b5149363c6f7c22c5faa2789c79586ffc08b22",
        "title": "Offline Reinforcement Learning With Behavior Value Regularization"
      },
      {
        "paperId": "e4983b7b77b4b879d0f65505a09c7441cc8938a5",
        "title": "Compositional Conservatism: A Transductive Approach in Offline Reinforcement Learning"
      },
      {
        "paperId": "34f1480a2da58eb124c1d1724ece43c1e5c3b9ea",
        "title": "The Edge-of-Reach Problem in Offline Model-Based Reinforcement Learning"
      },
      {
        "paperId": "5aec9a5815dd428de29887182d0c90635b998e28",
        "title": "Deep Exploration with PAC-Bayes"
      },
      {
        "paperId": "d5e79c37c5a1ac71307053d743f7f66b0cd317e4",
        "title": "MICRO: Model-Based Offline Reinforcement Learning with a Conservative Bellman Operator"
      },
      {
        "paperId": "3cec3154470c3ed16a8fa50c6421c9b45f6a4792",
        "title": "Reward-Consistent Dynamics Models are Strongly Generalizable for Offline Reinforcement Learning"
      },
      {
        "paperId": "2995fdd2073562effa7bcfed17c2928527f575c2",
        "title": "Neural Stochastic Differential Equations for Uncertainty-Aware Offline RL"
      },
      {
        "paperId": "1433c709492dc79b0c58a5090f543d374990a000",
        "title": "Cross-Domain Offline Policy Adaptation with Optimal Transport and Dataset Constraint"
      },
      {
        "paperId": "b8f3ef5753c06ac70f65b26388fda033b9fe7888",
        "title": "Information-Directed Pessimism for Offline Reinforcement Learning"
      },
      {
        "paperId": "82cbf4ce6db5c5d7ec6ffdbcfd3147e6c8f33df5",
        "title": "Policy Learning from Tutorial Books via Understanding, Rehearsing and Introspecting"
      },
      {
        "paperId": "900a5e6fac28c8f3180a5c80499634da92832bda",
        "title": "O\ufb04ine Reinforcement Learning without Regularization and Pessimism"
      },
      {
        "paperId": "76677be2ef40ab21bd982e5b41fe9fbbab034128",
        "title": "High-Fidelity Data-Driven Dynamics Model for Reinforcement Learning-based Magnetic Control in HL-3 Tokamak"
      }
    ],
    "score": 21.0
  },
  {
    "id": "4e98282f5f3f1a388b8d95380473d4ef4878266e",
    "title": "DrM: Mastering Visual Reinforcement Learning through Dormant Ratio Minimization",
    "authors": [
      "Guowei Xu",
      "Ruijie Zheng",
      "Yongyuan Liang",
      "Xiyao Wang",
      "Zhecheng Yuan",
      "Tianying Ji",
      "Yu Luo",
      "Xiaoyu Liu",
      "Jiaxin Yuan",
      "Pu Hua",
      "Shuzhen Li",
      "Yanjie Ze",
      "Hal Daum'e",
      "Furong Huang",
      "Huazhe Xu"
    ],
    "year": 2023,
    "citationCount": 39,
    "abstract": "Visual reinforcement learning (RL) has shown promise in continuous control tasks. Despite its progress, current algorithms are still unsatisfactory in virtually every aspect of the performance such as sample efficiency, asymptotic performance, and their robustness to the choice of random seeds. In this paper, we identify a major shortcoming in existing visual RL methods that is the agents often exhibit sustained inactivity during early training, thereby limiting their ability to explore effectively. Expanding upon this crucial observation, we additionally unveil a significant correlation between the agents' inclination towards motorically inactive exploration and the absence of neuronal activity within their policy networks. To quantify this inactivity, we adopt dormant ratio as a metric to measure inactivity in the RL agent's network. Empirically, we also recognize that the dormant ratio can act as a standalone indicator of an agent's activity level, regardless of the received reward signals. Leveraging the aforementioned insights, we introduce DrM, a method that uses three core mechanisms to guide agents' exploration-exploitation trade-offs by actively minimizing the dormant ratio. Experiments demonstrate that DrM achieves significant improvements in sample efficiency and asymptotic performance with no broken seeds (76 seeds in total) across three continuous control benchmark environments, including DeepMind Control Suite, MetaWorld, and Adroit. Most importantly, DrM is the first model-free algorithm that consistently solves tasks in both the Dog and Manipulator domains from the DeepMind Control Suite as well as three dexterous hand manipulation tasks without demonstrations in Adroit, all based on pixel observations.",
    "url": "https://www.semanticscholar.org/paper/4e98282f5f3f1a388b8d95380473d4ef4878266e",
    "pdf_url": "https://arxiv.org/pdf/2310.19668.pdf",
    "venue": "International Conference on Learning Representations",
    "publicationDate": "2023-10-30",
    "externalIds": {
      "DBLP": "conf/iclr/XuZLWYJL0YHLZDH24",
      "ArXiv": "2310.19668",
      "DOI": "10.48550/arXiv.2310.19668",
      "CorpusId": 264796749
    },
    "references": [
      {
        "paperId": "9d288a7c846b55e63b87f081ed15572ed89e4731",
        "title": "COPlanner: Plan to Roll Out Conservatively but to Explore Optimistically for Model-Based RL"
      },
      {
        "paperId": "f817e4d16a2300f5dc8d408c50442b999f0db890",
        "title": "RL-ViGen: A Reinforcement Learning Benchmark for Visual Generalization"
      },
      {
        "paperId": "0a806876e419de58b650aacbf218e23f589327ad",
        "title": "CEM: Constrained Entropy Maximization for Task-Agnostic Safe Exploration"
      },
      {
        "paperId": "9b0e89b3770005ad1f79b0d95f951f442f83de23",
        "title": "TACO: Temporal Latent Action-Driven Contrastive Loss for Visual Reinforcement Learning"
      },
      {
        "paperId": "433eb27e9853d276bf119c70b027b4c7fbc7dafd",
        "title": "Seizing Serendipity: Exploiting the Value of Past Success in Off-Policy Actor-Critic"
      },
      {
        "paperId": "b9e4ea7ff34a304cc0e7bfaa638eaa850391b676",
        "title": "Accelerating Reinforcement Learning with Value-Conditional State Entropy Exploration"
      },
      {
        "paperId": "d364f8cb20daf958ebca53a807e79082eff98e78",
        "title": "Bigger, Better, Faster: Human-level Atari with human-level efficiency"
      },
      {
        "paperId": "b9c9338e247570bbcea0d947b5fbe18a19116ceb",
        "title": "Deep Reinforcement Learning with Plasticity Injection"
      },
      {
        "paperId": "542905f5fc96bce7572f6ded7f56aedfa62270c1",
        "title": "Understanding plasticity in neural networks"
      },
      {
        "paperId": "6c282567e9c452f81416214933b9b1e45ab3add4",
        "title": "The Dormant Neuron Phenomenon in Deep Reinforcement Learning"
      },
      {
        "paperId": "f2d952a183dfb0a1e031b8a3f535d9f8423d7a6e",
        "title": "Mastering Diverse Domains through World Models"
      },
      {
        "paperId": "02d9dc238ae825e45e728607c3c83b77d07f4017",
        "title": "Stabilizing Off-Policy Deep Reinforcement Learning from Pixels"
      },
      {
        "paperId": "69b80ce5ab6c2263b145b1eaa23664145938f351",
        "title": "The Primacy Bias in Deep Reinforcement Learning"
      },
      {
        "paperId": "0a818d426c813b7a4758c959d38ece5f2cc11476",
        "title": "Understanding and Preventing Capacity Loss in Reinforcement Learning"
      },
      {
        "paperId": "d46eef8b0d46cfdebefe9941a0f60aeeed31ded0",
        "title": "Temporal Difference Learning for Model Predictive Control"
      },
      {
        "paperId": "399bbcecf674a55fb1a91b178a5be4a8999bb0ac",
        "title": "The Importance of Non-Markovianity in Maximum State Entropy Exploration"
      },
      {
        "paperId": "b2d931da61559c528c5d4eadcb939425a2531652",
        "title": "Dropout Q-Functions for Doubly Efficient Reinforcement Learning"
      },
      {
        "paperId": "e06c005e98281af455c454ce2478285f6f3afeca",
        "title": "Mastering Visual Continuous Control: Improved Data-Augmented Reinforcement Learning"
      },
      {
        "paperId": "42cc5f45d59a694813e990b29631be4e32bc6453",
        "title": "MADE: Exploration via Maximizing Deviation from Explored Regions"
      },
      {
        "paperId": "6056961e0e4b5fb8bdce7aa64a8499e5a38da7df",
        "title": "UCB Momentum Q-learning: Correcting the bias without forgetting"
      },
      {
        "paperId": "7fc04f8031598da6510bce4a7e5b4722305ca5b0",
        "title": "State Entropy Maximization with Random Encoders for Efficient Exploration"
      },
      {
        "paperId": "736590f70e7f2dc464c1c62491cfa8adb4d718f3",
        "title": "Randomized Ensembled Double Q-Learning: Learning Fast Without a Model"
      },
      {
        "paperId": "600aaa3918feb25a690f69931446c8ace2863d8e",
        "title": "Generalization in Reinforcement Learning by Soft Data Augmentation"
      },
      {
        "paperId": "63b4843f3789838331168315ea6fa406f0f973b1",
        "title": "Implicit Under-Parameterization Inhibits Data-Efficient Deep Reinforcement Learning"
      },
      {
        "paperId": "243467598d7db59aa7467763f5863a0f1ae0b6b4",
        "title": "Batch Exploration With Examples for Scalable Robotic Reinforcement Learning"
      },
      {
        "paperId": "b44bb1762640ed72091fd5f5fdc20719a6dc24af",
        "title": "Mastering Atari with Discrete World Models"
      },
      {
        "paperId": "17985b57240bfaea02a6098a7a34e71e780180eb",
        "title": "Decoupling Representation Learning from Reinforcement Learning"
      },
      {
        "paperId": "5e06f64e91f4f48665c7407ea6eac74f93d5d0d0",
        "title": "On the model-based stochastic value gradient for continuous reinforcement learning"
      },
      {
        "paperId": "576c7b003427d6906e3b58a0139b4d83301909fd",
        "title": "Fast active learning for pure exploration in reinforcement learning"
      },
      {
        "paperId": "7c4356ec0dca6e6df0af7a882e2cd1571c8bf3dc",
        "title": "Data-Efficient Reinforcement Learning with Self-Predictive Representations"
      },
      {
        "paperId": "669f07843065e471974080608a3cbee876d13cef",
        "title": "Adaptive Reward-Free Exploration"
      },
      {
        "paperId": "ea915ada7638914c44654864a07453ec5c45c622",
        "title": "Push-motivation-based emotional arousal: A research study in a coastal destination"
      },
      {
        "paperId": "3a71c306eb6232658c9e5fd48aed1ef3befe5fbe",
        "title": "Planning to Explore via Self-Supervised World Models"
      },
      {
        "paperId": "744139d65c3bf6da6a6acd384a32d94a06f44f62",
        "title": "Reinforcement Learning with Augmented Data"
      },
      {
        "paperId": "6568423cfaca7e24c88ea208cb0e67129e43aa9b",
        "title": "Image Augmentation Is All You Need: Regularizing Deep Reinforcement Learning from Pixels"
      },
      {
        "paperId": "9efb64f20ab1f157ca9f4050d4aaacf6c3f9b2b2",
        "title": "CURL: Contrastive Unsupervised Representations for Reinforcement Learning"
      },
      {
        "paperId": "086159600bede14e00f96043c733d4f3b45855aa",
        "title": "Never Give Up: Learning Directed Exploration Strategies"
      },
      {
        "paperId": "90f9ef793c1e4000ee523e1f8cea501fb8806fa0",
        "title": "Reward-Free Exploration for Reinforcement Learning"
      },
      {
        "paperId": "0cc956565c7d249d4197eeb1dbab6523c648b2c9",
        "title": "Dream to Control: Learning Behaviors by Latent Imagination"
      },
      {
        "paperId": "0bc855f84668b35cb65618d996d09f6e434d28c9",
        "title": "Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning"
      },
      {
        "paperId": "538dbb608312b0a107cf98ccbaca2e0f563e2a61",
        "title": "On Warm-Starting Neural Network Training"
      },
      {
        "paperId": "e7bb4419a88d15fa8e52c1f4f9cfd65ed58c7379",
        "title": "V-MPO: On-Policy Maximum a Posteriori Policy Optimization for Discrete and Continuous Control"
      },
      {
        "paperId": "69d1ee8a99f55e9228f33fdb3a0339541ad1201c",
        "title": "Stochastic Latent Actor-Critic: Deep Reinforcement Learning with a Latent Variable Model"
      },
      {
        "paperId": "cfec54f9208dcbb2c558aa7ff45f57be2533d046",
        "title": "Efficient Exploration via State Marginal Matching"
      },
      {
        "paperId": "adab275554eb745cdc21fd6c526ec55a5b2ef362",
        "title": "Provably Efficient Maximum Entropy Exploration"
      },
      {
        "paperId": "4da82fda1b07d7b780751f56d503dccdfe8fb7b6",
        "title": "Empirical Design"
      },
      {
        "paperId": "fea3e63c97c7292dc6fbcb3ffe7131eb54053986",
        "title": "Learning Latent Dynamics for Planning from Pixels"
      },
      {
        "paperId": "4cb3fd057949624aa4f0bbe7a6dcc8777ff04758",
        "title": "Exploration by Random Network Distillation"
      },
      {
        "paperId": "a9a3ed69c94a3e1c08ef1f833d9199f57736238b",
        "title": "DeepMind Control Suite"
      },
      {
        "paperId": "e010ba3ff5744604cdbfe44a733e2a98649ee907",
        "title": "Learning Complex Dexterous Manipulation with Deep Reinforcement Learning and Demonstrations"
      },
      {
        "paperId": "3fab4d28be5ff73f42c8b307117aa8d680345ad9",
        "title": "Noisy Networks for Exploration"
      },
      {
        "paperId": "142497432fe179ddb6ffe600c64a837ec6179550",
        "title": "Parameter Space Noise for Exploration"
      },
      {
        "paperId": "225ab689f41cef1dc18237ef5dab059a49950abf",
        "title": "Curiosity-Driven Exploration by Self-Supervised Prediction"
      },
      {
        "paperId": "12f67fb182bc934fc95ce97acff553d83e2ca72e",
        "title": "Count-Based Exploration with Neural Density Models"
      },
      {
        "paperId": "0fcb2034e31a2bc2f12a2b1363d0d77baf445fdf",
        "title": "#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning"
      },
      {
        "paperId": "6e90fd78e8a3b98af3954aae5209703aa966603e",
        "title": "Unifying Count-Based Exploration and Intrinsic Motivation"
      },
      {
        "paperId": "317cd4522b1f4a6f889743578143bb8823623f8b",
        "title": "VIME: Variational Information Maximizing Exploration"
      },
      {
        "paperId": "024006d4c2a89f7acacc6e4438d156525b60a98f",
        "title": "Continuous control with deep reinforcement learning"
      },
      {
        "paperId": "2a25f57a5ac627e5f7a2e4502c24bfa51cfdc1cf",
        "title": "2010 Special Issue: Parameter-exploring policy gradients"
      },
      {
        "paperId": "8ac68370386cb90e72189dd4d0b487bb3da19665",
        "title": "Exploring parameter space in reinforcement learning"
      },
      {
        "paperId": "103f6fe35033f9327611ddafde74a2b544072980",
        "title": "Using Confidence Bounds for Exploitation-Exploration Trade-offs"
      },
      {
        "paperId": "116d7798c1123cf7fad4176e98f58fd49de4f8f1",
        "title": "Planning and Acting in Partially Observable Stochastic Domains"
      },
      {
        "paperId": "bff20fb30adad8d1c173963089df5fc9664304f0",
        "title": "A Markovian Decision Process"
      },
      {
        "paperId": "33d84b1531f88d2bd2e516e1574f22e139133065",
        "title": "Sample-Efficient Reinforcement Learning by Breaking the Replay Ratio Barrier"
      },
      {
        "paperId": null,
        "title": "Arousal theory"
      },
      {
        "paperId": null,
        "title": "0 200 400 600 800 1000 E p i s o d e R e w a r d Figure algorithms maximizing the difference between frames, it would likely struggle with tasks that require minimal motion"
      },
      {
        "paperId": null,
        "title": "We introduce a mechanism that periodically perturbs the model weights of the agent, effectively reducing the dormant ratio and hence accelerating skill acquisition"
      },
      {
        "paperId": null,
        "title": "Control policy with autocorrelated noise in reinforcement learning for robotics"
      }
    ],
    "cited_by": [
      {
        "paperId": "26a2dc9c0973e3d596aa2a6caf93f482d94ac4b6",
        "title": "HERMES: Human-to-Robot Embodied Learning from Multi-Source Motion Data for Mobile Dexterous Manipulation"
      },
      {
        "paperId": "3cd9fd0ef2ba215cfc46fe80e36cccbccab1e52b",
        "title": "Revisiting Bisimulation Metric for Robust Representations in Reinforcement Learning"
      },
      {
        "paperId": "e1f6d4f020721052e0980c0f265f1415d1aec562",
        "title": "Multimodal Visual Transformer for Sim2real Transfer in Visual Reinforcement Learning"
      },
      {
        "paperId": "eae3d4d11f8a72d0e2a95e571fe17858cef13c2b",
        "title": "A Forget-and-Grow Strategy for Deep Reinforcement Learning Scaling in Continuous Control"
      },
      {
        "paperId": "92d93ac08c2c5513ba5a0a95c4ab3d3bb8899b36",
        "title": "Residual Reward Models for Preference-based Reinforcement Learning"
      },
      {
        "paperId": "c28d1f25d8a778d09bb04620bead6fe8cfe05e75",
        "title": "Network Sparsity Unlocks the Scaling Potential of Deep Reinforcement Learning"
      },
      {
        "paperId": "6efb0c7c785ab0e735436ee32daef0a854c76be7",
        "title": "The Courage to Stop: Overcoming Sunk Cost Fallacy in Deep Reinforcement Learning"
      },
      {
        "paperId": "5bd486625ed6bb7b552e2c2369c5dca9061b0e66",
        "title": "Measure gradients, not activations! Enhancing neuronal activity in deep reinforcement learning"
      },
      {
        "paperId": "347b7e5c3deaced566daecf979ee8a2bda4f4f8f",
        "title": "Zero-Shot Sim-to-Real Reinforcement Learning for Fruit Harvesting"
      },
      {
        "paperId": "e4c11b4f6eaaa079856d81f39be8924e5bd1ba40",
        "title": "Merging and Disentangling Views in Visual Reinforcement Learning for Robotic Manipulation"
      },
      {
        "paperId": "3786a19a53187b08e95f3cb10397ecfc5379c36c",
        "title": "Understanding and Exploiting Plasticity for Non-stationary Network Resource Adaptation"
      },
      {
        "paperId": "94a0cb55474450fb92f4e46de2ed5e22f972381d",
        "title": "Rethinking Bimanual Robotic Manipulation: Learning with Decoupled Interaction Framework"
      },
      {
        "paperId": "4831cf54b947bdb262ae3efeaa6909e271525fb4",
        "title": "Hyperspherical Normalization for Scalable Deep Reinforcement Learning"
      },
      {
        "paperId": "6f7cb5ce9f2696221be4db0eff38bb6446a93fb9",
        "title": "Value-Based Deep RL Scales Predictably"
      },
      {
        "paperId": "ddb639409ebdf479c85b32055d075d2c7afc8acc",
        "title": "UnrealZoo: Enriching Photo-realistic Virtual Worlds for Embodied AI"
      },
      {
        "paperId": "e4fef8d5864c5468100ca167639ef3fa374c0442",
        "title": "MaxInfoRL: Boosting exploration in reinforcement learning through information gain maximization"
      },
      {
        "paperId": "35f6d83bb69a8040069e8646c47101cc9c988c65",
        "title": "Robots Pre-train Robots: Manipulation-Centric Robotic Representation from Large-Scale Robot Datasets"
      },
      {
        "paperId": "4614720c4ede3e6704428300e8a895b68bb9f37e",
        "title": "Prioritized Generative Replay"
      },
      {
        "paperId": "02affc488b7775b40c028685660ccf50010f0a85",
        "title": "MENTOR: Mixture-of-Experts Network with Task-Oriented Perturbation for Visual Reinforcement Learning"
      },
      {
        "paperId": "59c401dd3b13fd16f6f91931701bd0c66b855d96",
        "title": "Exploring Visual Reinforcement Learning for Sample-Efficient Robotic Manipulation"
      },
      {
        "paperId": "697487c80d9aac7e59454634831c6b321906f993",
        "title": "MAD-TD: Model-Augmented Data stabilizes High Update Ratio RL"
      },
      {
        "paperId": "1b7a9bdcdab269551a6439ed8108eb2ecbedc29f",
        "title": "Neuroplastic Expansion in Deep Reinforcement Learning"
      },
      {
        "paperId": "851c588dba8c7e241aca5c036779640bb7646c3f",
        "title": "Generalizing Consistency Policy to Visual RL with Prioritized Proximal Experience Regularization"
      },
      {
        "paperId": "59184c4e52b3f5478bdad1be320dc24cd51f8c17",
        "title": "Improving Deep Reinforcement Learning by Reducing the Chain Effect of Value and Policy Churn"
      },
      {
        "paperId": "bef2c07fa936cbe05aedfee946fed76e5840bf40",
        "title": "Visual Reinforcement Learning Using Dynamic State Representation for Continuous Motion Control"
      },
      {
        "paperId": "7b3e07ceb9d7da26effa24fdf13bd23121c07117",
        "title": "Pretrained Visual Representations in Reinforcement Learning"
      },
      {
        "paperId": "9bb60b570d9dcb9adb820e3c7cd7974a2c756882",
        "title": "Make-An-Agent: A Generalizable Policy Network Generator with Behavior-Prompted Diffusion"
      },
      {
        "paperId": "a2de2a8605ebf240d12ccae8d617371cde3802fa",
        "title": "UVIS: Unsupervised Video Instance Segmentation"
      },
      {
        "paperId": "f1fc35e72b66f13c42d5e52fb24bc525b71fea2d",
        "title": "A Study of Plasticity Loss in On-Policy Deep Reinforcement Learning"
      },
      {
        "paperId": "e1ba406f477df54d747a77b40781b07f837ff33c",
        "title": "Offline-Boosted Actor-Critic: Adaptively Blending Optimal Historical Behaviors in Deep Off-Policy RL"
      },
      {
        "paperId": "0ac2e7e336fe6ae5a8217d2dd1f1c5e571894ed3",
        "title": "Higher Replay Ratio Empowers Sample-Efficient Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "0e6face6f76726ef6b0a7905fe5052cff2e2c5d1",
        "title": "Diverse Feature Learning by Self-distillation and Reset"
      },
      {
        "paperId": "06df2005b1244940c711eb4819be41cc26e7eed7",
        "title": "ACE : Off-Policy Actor-Critic with Causality-Aware Entropy Regularization"
      },
      {
        "paperId": "600fce8b16708755a331ba7122b9a0600f8890e1",
        "title": "PRISE: LLM-Style Sequence Compression for Learning Temporal Action Abstractions in Control"
      },
      {
        "paperId": "3703bc8a0001319fc34bc7ac25005c98294ce6c5",
        "title": "Premier-TACO is a Few-Shot Policy Learner: Pretraining Multitask Representation via Temporal Action-Driven Contrastive Loss"
      },
      {
        "paperId": "3d3bbaf203e55e681ed1c6f59181abacff2b068c",
        "title": "On the Theory of Risk-Aware Agents: Bridging Actor-Critic and Economics"
      },
      {
        "paperId": "0a486db0e2167bf1be7cdbc546bae3ba25df828c",
        "title": "Game-Theoretic Robust Reinforcement Learning Handles Temporally-Coupled Perturbations"
      },
      {
        "paperId": "b22c77aa6b545ed7eebae91ffc48ae596d5088b8",
        "title": "Learning and Transferring Better with Depth Information in Visual Reinforcement Learning"
      },
      {
        "paperId": "8aa7ad3e6dca112e004077c2e92c1fcbfc746a4e",
        "title": "The Dormant Neuron Phenomenon in Multi-Agent Reinforcement Learning Value Factorization"
      }
    ],
    "score": 19.5
  },
  {
    "id": "abeb46288d537f98f76b979040a547ee81216377",
    "title": "R1-Reward: Training Multimodal Reward Model Through Stable Reinforcement Learning",
    "authors": [
      "Yi-Fan Zhang",
      "Xingyu Lu",
      "Xiao Hu",
      "Chaoyou Fu",
      "Bin Wen",
      "Tianke Zhang",
      "Changyi Liu",
      "Kaiyu Jiang",
      "Kaibing Chen",
      "Kaiyu Tang",
      "Haojie Ding",
      "Jiankang Chen",
      "Fan Yang",
      "Zhang Zhang",
      "Tingting Gao",
      "Liang Wang"
    ],
    "year": 2025,
    "citationCount": 19,
    "abstract": "Multimodal Reward Models (MRMs) play a crucial role in enhancing the performance of Multimodal Large Language Models (MLLMs). While recent advancements have primarily focused on improving the model structure and training data of MRMs, there has been limited exploration into the effectiveness of long-term reasoning capabilities for reward modeling and how to activate these capabilities in MRMs. In this paper, we explore how Reinforcement Learning (RL) can be used to improve reward modeling. Specifically, we reformulate the reward modeling problem as a rule-based RL task. However, we observe that directly applying existing RL algorithms, such as Reinforce++, to reward modeling often leads to training instability or even collapse due to the inherent limitations of these algorithms. To address this issue, we propose the StableReinforce algorithm, which refines the training loss, advantage estimation strategy, and reward design of existing RL methods. These refinements result in more stable training dynamics and superior performance. To facilitate MRM training, we collect 200K preference data from diverse datasets. Our reward model, R1-Reward, trained using the StableReinforce algorithm on this dataset, significantly improves performance on multimodal reward modeling benchmarks. Compared to previous SOTA models, R1-Reward achieves a $8.4\\%$ improvement on the VL Reward-Bench and a $14.3\\%$ improvement on the Multimodal Reward Bench. Moreover, with more inference compute, R1-Reward's performance is further enhanced, highlighting the potential of RL algorithms in optimizing MRMs.",
    "url": "https://www.semanticscholar.org/paper/abeb46288d537f98f76b979040a547ee81216377",
    "pdf_url": "https://arxiv.org/pdf/2505.02835.pdf",
    "venue": "arXiv.org",
    "publicationDate": "2025-05-05",
    "externalIds": {
      "ArXiv": "2505.02835",
      "DBLP": "journals/corr/abs-2505-02835",
      "DOI": "10.48550/arXiv.2505.02835",
      "CorpusId": 278339678
    },
    "references": [
      {
        "paperId": "cddf14e5b97090111d3fa814c9aec60e2bf24b8a",
        "title": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models"
      },
      {
        "paperId": "f5d194b600e4e4021564f3647afde07308ac41d3",
        "title": "VLM-R1: A Stable and Generalizable R1-style Large Vision-Language Model"
      },
      {
        "paperId": "fda89fd26e40edcb29439c9d759610dc2241cd2c",
        "title": "MME-Unify: A Comprehensive Benchmark for Unified Multimodal Understanding and Generation Models"
      },
      {
        "paperId": "5b08060d9efee18b235b4d49bb1f58898f3fc24f",
        "title": "Inference-Time Scaling for Generalist Reward Modeling"
      },
      {
        "paperId": "a3cdf5d2d5c53370dd6173b509438481e32a0419",
        "title": "Video-R1: Reinforcing Video Reasoning in MLLMs"
      },
      {
        "paperId": "de23d38bc2604dcf334dcc46aff217eb6bcd1fe1",
        "title": "Understanding R1-Zero-Like Training: A Critical Perspective"
      },
      {
        "paperId": "ee8703feb9ec4c1fd133cb653a26631fc9847ed0",
        "title": "Judge Anything: MLLM as a Judge Across Any Modality"
      },
      {
        "paperId": "dd4cfde3e135f799a9a71b4f57e13a29de89f7e3",
        "title": "DAPO: An Open-Source LLM Reinforcement Learning System at Scale"
      },
      {
        "paperId": "ae6f30f15a699b2b8cba027e6d89fa8c4340c19f",
        "title": "Tapered Off-Policy REINFORCE: Stable and efficient reinforcement learning for LLMs"
      },
      {
        "paperId": "24519912c263a84321d5bf1126eddc24a2e1863e",
        "title": "Aligning Multimodal LLM with Human Preference: A Survey"
      },
      {
        "paperId": "68e63d7210a9191cd79412af3cca5dd858ab52ca",
        "title": "LMM-R1: Empowering 3B LMMs with Strong Reasoning Abilities Through Two-Stage Rule-Based RL"
      },
      {
        "paperId": "942328473407147f609761dc22128a503713df58",
        "title": "Vision-R1: Incentivizing Reasoning Capability in Multimodal Large Language Models"
      },
      {
        "paperId": "35c639be8599d72834a76b89746eba8fa2544d28",
        "title": "R1-Omni: Explainable Omni-Multimodal Emotion Recognition with Reinforcement Learning"
      },
      {
        "paperId": "2464e287e6f92738fd5627819597172e8674e36b",
        "title": "Visual-RFT: Visual Reinforcement Fine-Tuning"
      },
      {
        "paperId": "446c3356b95a29dca4b908533c088ce63f2275cb",
        "title": "Logic-RL: Unleashing LLM Reasoning with Rule-Based Reinforcement Learning"
      },
      {
        "paperId": "c44b60bc8671cc783256035c3f41b3fa84083379",
        "title": "Multimodal RewardBench: Holistic Evaluation of Reward Models for Vision Language Models"
      },
      {
        "paperId": "bb6426f40b7a5323423826afa0485fd940ec3c78",
        "title": "MM-RLHF: The Next Step Forward in Multimodal LLM Alignment"
      },
      {
        "paperId": "0fb1b32196e837049cbaf4850a2ee10c5cec764f",
        "title": "Long-VITA: Scaling Large Multi-modal Models to 1 Million Tokens with Leading Short-Context Accuracy"
      },
      {
        "paperId": "d5172b0ae32f7902e66401df75bd708c6606eef5",
        "title": "DAMA: Data- and Model-aware Alignment of Multi-modal LLMs"
      },
      {
        "paperId": "e23379a9752f57732d311f7a97af2c69af6fae7b",
        "title": "Process Reinforcement through Implicit Rewards"
      },
      {
        "paperId": "a99dee9602e21a71526b9681d8dba37c55b66941",
        "title": "SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training"
      },
      {
        "paperId": "f56928470abbe177dd6f7527523b54962fd465ed",
        "title": "Baichuan-Omni-1.5 Technical Report"
      },
      {
        "paperId": "34471a2fa18ea22efad5287cf4aeb18542c98a9b",
        "title": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "paperId": "54e1a79ef688b8f6462b6265fc803d9c3e90a72a",
        "title": "InternLM-XComposer2.5-Reward: A Simple Yet Effective Multi-Modal Reward Model"
      },
      {
        "paperId": "b4d0f1237431cbc73db16685771790541c8c246a",
        "title": "MiniMax-01: Scaling Foundation Models with Lightning Attention"
      },
      {
        "paperId": "8839700fe75a6e78f12cd73bcafed4c54cfcd19b",
        "title": "LLaVA-Mini: Efficient Image and Video Large Multimodal Models with One Vision Token"
      },
      {
        "paperId": "4f62ac743e2ed33e682a9c6f5f7875723d9a7025",
        "title": "VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction"
      },
      {
        "paperId": "dad0c0d62c7d4afd19353b5ed48581e495cc8e27",
        "title": "VL-RewardBench: A Challenging Benchmark for Vision-Language Generative Reward Models"
      },
      {
        "paperId": "b6b7a7fe30623f06a627d3ddbe34f40cb96a982f",
        "title": "Self-Generated Critiques Boost Reward Modeling for Language Models"
      },
      {
        "paperId": "05f01ee0687b03d8ab3527ca26380dca692d7a5c",
        "title": "MME-Survey: A Comprehensive Survey on Evaluation of Multimodal LLMs"
      },
      {
        "paperId": "7796f56c7b9c152ac9573c8bb34f716aa78b4e6d",
        "title": "Skywork-Reward: Bag of Tricks for Reward Modeling in LLMs"
      },
      {
        "paperId": "97cf25e40098b5a53493363e26e29beb782d31bf",
        "title": "LLLaVA-Critic: Learning to Evaluate Multimodal Models"
      },
      {
        "paperId": "9113b84814c92244813b8a0f49e6cefb73236254",
        "title": "Uncertainty-aware Reward Model: Teaching Reward Models to Know What is Unknown"
      },
      {
        "paperId": "a22b23b65cdbc0335b872ed7c7a7206c710f8997",
        "title": "Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution"
      },
      {
        "paperId": "9a2c37ab4e38652f0d50dc9818e06d35e646e2b0",
        "title": "MME-RealWorld: Could Your Multimodal LLM Challenge High-Resolution Real-World Scenarios that are Difficult for Humans?"
      },
      {
        "paperId": "9dc337bd18bc0201839942931b12aff8ec1c93f5",
        "title": "Show-o: One Single Transformer to Unify Multimodal Understanding and Generation"
      },
      {
        "paperId": "62a82afa1b2adadb68b4667ede9faab0613a6cd9",
        "title": "VITA: Towards Open-Source Interactive Omni Multimodal LLM"
      },
      {
        "paperId": "1a71f7b216b710b936da666027014adb83af8e7a",
        "title": "LLaVA-OneVision: Easy Visual Task Transfer"
      },
      {
        "paperId": "0bf85ecae19a6f64c5beea5ee7109cff5c0f4a42",
        "title": "WildVision: Evaluating Vision-Language Models in the Wild with Human Preferences"
      },
      {
        "paperId": "7e9157b84155d75d1300e373375daef12d838f28",
        "title": "Beyond LLaVA-HD: Diving into High-Resolution Large Multimodal Models"
      },
      {
        "paperId": "f590d8926dd12345a3bd22253461850f5ca4b3ed",
        "title": "HelpSteer2: Open-source dataset for training top-performing reward models"
      },
      {
        "paperId": "db54861e0b31b0b341bbc9655f30dca98201ffca",
        "title": "RLAIF-V: Open-Source AI Feedback Leads to Super GPT-4V Trustworthiness"
      },
      {
        "paperId": "32112b798f70faab00e14806f51d46058cf5e597",
        "title": "Chameleon: Mixed-Modal Early-Fusion Foundation Models"
      },
      {
        "paperId": "887306ee7ebd9eebd7faea24106ec8e8f1a50987",
        "title": "How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites"
      },
      {
        "paperId": "aef34d7831e93f071e1752ffd9a59c1baa99d78a",
        "title": "Debiasing Multimodal Large Language Models"
      },
      {
        "paperId": "f8a642fbb51e0b0ae4774781309545d15d6d9b11",
        "title": "Aligning Modalities in Vision Large Language Models via Preference Fine-tuning"
      },
      {
        "paperId": "1036f6dfe75af06fbcdb3447dbe9be8613bf857c",
        "title": "MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark"
      },
      {
        "paperId": "35b142ea69598e6241f0011312128031df55895c",
        "title": "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models"
      },
      {
        "paperId": "9dad4d7c6da4f3bb56a24742998c76a2629ac9fd",
        "title": "Benchmarking Large Multimodal Models against Common Corruptions"
      },
      {
        "paperId": "6a33e58ef961a3a0a5657518b2be86395eb7c8d0",
        "title": "Intern VL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks"
      },
      {
        "paperId": "2b14d9e190022e388476ebb24eb1a84349ca0de4",
        "title": "Silkie: Preference Distillation for Large Visual Language Models"
      },
      {
        "paperId": "844bb298d49ef4a07b5d4929dfdfd170f6a1d5f5",
        "title": "Aligning Large Multimodal Models with Factually Augmented RLHF"
      },
      {
        "paperId": "0d1c76d45afa012ded7ab741194baf142117c495",
        "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "e0642272d01afd867c090c7beddd37218616fcfd",
        "title": "Reinforcement learning in robotic applications: a comprehensive survey"
      },
      {
        "paperId": "a2e43270a9b1421e452c2975e5163e2a216abeac",
        "title": "A Survey of Deep Reinforcement Learning in Video Games"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "8b191d96db9248c8284d6146976ca3b5c4858f00",
        "title": "REINFORCE++: A Simple and Efficient Approach for Aligning Large Language Models"
      },
      {
        "paperId": "3462eb6ba2b358ed95afca845e57f22cd0d79a8e",
        "title": "Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Multimodal Models"
      },
      {
        "paperId": "9deda73da446cbb21550abaa62b41f5a3acd92fc",
        "title": "OpenRLHF: An Easy-to-use, Scalable and High-performance RLHF Framework"
      },
      {
        "paperId": null,
        "title": "Llama3 Team"
      },
      {
        "paperId": null,
        "title": ": Open frontier-class multimodal llms"
      },
      {
        "paperId": null,
        "title": "An effective process reward model"
      },
      {
        "paperId": null,
        "title": "Introducing openai o1-preview"
      }
    ],
    "cited_by": [
      {
        "paperId": "555b532aa0adaa8cab4b6842411e1089a6f6f8d3",
        "title": "TempSamp-R1: Effective Temporal Sampling with Reinforcement Fine-Tuning for Video LLMs"
      },
      {
        "paperId": "06df7400cba1ef6947f72d2dae130eb74c1ac1ca",
        "title": "BaseReward: A Strong Baseline for Multimodal Reward Model"
      },
      {
        "paperId": "c99eb27ac56a3d201e8402b7264704e8a09dfc16",
        "title": "Rethinking Reasoning Quality in Large Language Models through Enhanced Chain-of-Thought via RL"
      },
      {
        "paperId": "eb6ef63df104c1b35bbc2400f00285b3414400b2",
        "title": "SimpleTIR: End-to-End Reinforcement Learning for Multi-Turn Tool-Integrated Reasoning"
      },
      {
        "paperId": "9f8960098f94581749cb00845dacfcce9982ead5",
        "title": "Kwai Keye-VL 1.5 Technical Report"
      },
      {
        "paperId": "56ddff358894ddf3144de57d25a101aa3ccccdcf",
        "title": "LLaVA-Critic-R1: Your Critic Model is Secretly a Strong Policy Model"
      },
      {
        "paperId": "a2b7b7670eb8b27ded1e8a0e34399d9951a2c581",
        "title": "VideoRewardBench: Comprehensive Evaluation of Multimodal Reward Models for Video Understanding"
      },
      {
        "paperId": "76067dc410f475261780a185060c5eaf029d1912",
        "title": "Med-RewardBench: Benchmarking Reward Models and Judges for Medical Multimodal Large Language Models"
      },
      {
        "paperId": "de684792f883b66091e8d92ff461af1fe592f04a",
        "title": "Thyme: Think Beyond Images"
      },
      {
        "paperId": "fcf275305eb7c0863517fa992f9b967d1756fae8",
        "title": "Audio-Thinker: Guiding Audio Language Model When and How to Think via Reinforcement Learning"
      },
      {
        "paperId": "6b3e64ab07bdb533d26bc4a78dad189e02a9bcb8",
        "title": "Cooper: Co-Optimizing Policy and Reward Models in Reinforcement Learning for Large Language Models"
      },
      {
        "paperId": "0859b3bfc28a8dcce29648bacb369dc0957d37dc",
        "title": "GRAIL:Learning to Interact with Large Knowledge Graphs for Retrieval Augmented Reasoning"
      },
      {
        "paperId": "fa478eeab535811afe25ae0a5ad01b3d3058e775",
        "title": "VRPRM: Process Reward Modeling via Visual Reasoning"
      },
      {
        "paperId": "44c717ce0ec31ff828c06abca417fea83421d256",
        "title": "Reason-SVG: Hybrid Reward RL for Aha-Moments in Vector Graphics Generation"
      },
      {
        "paperId": "843422813fb73d5e677ac49e962f76c47843f9b1",
        "title": "Writing-Zero: Bridge the Gap Between Non-verifiable Tasks and Verifiable Rewards"
      },
      {
        "paperId": "c463a2add169cbbb446be091cea099491eb85afe",
        "title": "SAM-R1: Leveraging SAM for Reward Feedback in Multimodal Segmentation via Reinforcement Learning"
      },
      {
        "paperId": "55baf937c13205e826711eddf60b02da7dc74250",
        "title": "Reinforcement Fine-Tuning Powers Reasoning Capability of Multimodal Large Language Models"
      },
      {
        "paperId": "d4fb9f6d3e45c0f27c8b3d2ce2a3330251c45bde",
        "title": "Divide-Fuse-Conquer: Eliciting \"Aha Moments\" in Multi-Scenario Games"
      },
      {
        "paperId": "00b39319f2dab85e7e338201a675f8c6dca2dfe8",
        "title": "Writing-Zero: Bridge the Gap Between Non-verifiable Problems and Verifiable Rewards"
      }
    ],
    "score": 19.0
  },
  {
    "id": "4a3e88d203564e547f5fb3f3d816a0b381492eae",
    "title": "VLA-RL: Towards Masterful and General Robotic Manipulation with Scalable Reinforcement Learning",
    "authors": [
      "Guanxing Lu",
      "Wenkai Guo",
      "Chubin Zhang",
      "Yuheng Zhou",
      "Hao Jiang",
      "Zifeng Gao",
      "Yansong Tang",
      "Ziwei Wang"
    ],
    "year": 2025,
    "citationCount": 18,
    "abstract": "Recent high-capacity vision-language-action (VLA) models have demonstrated impressive performance on a range of robotic manipulation tasks by imitating human demonstrations. However, exploiting offline data with limited visited states will cause execution failure in out-of-distribution scenarios. Intuitively, an exploration-based method that improves on online collected data at test time could address this limitation. We present VLA-RL, an algorithmic and systematic framework that leverages online reinforcement learning (RL) to improve pretrained auto-regressive VLAs in downstream tasks. Within a unified perspective, we first introduce a trajectory-level RL formulation for auto-regressive VLA training, which models general robotic manipulation trajectory as multi-modal multi-turn conversation. To address the challenge of sparse rewards, we fine-tune a pretrained vision-language model as a robotic process reward model, which is trained on pseudo reward labels annotated on automatically extracted task segments. To scale up, we identify several implementation findings that improve the stability and efficiency including curriculum selection strategy, GPU-balanced vectorized environments, batch decoding, and critic warmup. VLA-RL enables OpenVLA-7B to surpass the strongest finetuned baseline by 4.5% on 40 challenging robotic manipulation tasks in LIBERO, and even matches the performance of advanced commercial models such as $\\pi_0$-FAST. Notably, we observe that VLA-RL benefits from increased test-time optimization, indicating an early spark of inference scaling laws in robotics.",
    "url": "https://www.semanticscholar.org/paper/4a3e88d203564e547f5fb3f3d816a0b381492eae",
    "pdf_url": "https://arxiv.org/pdf/2505.18719.pdf",
    "venue": "arXiv.org",
    "publicationDate": "2025-05-24",
    "externalIds": {
      "ArXiv": "2505.18719",
      "DBLP": "journals/corr/abs-2505-18719",
      "DOI": "10.48550/arXiv.2505.18719",
      "CorpusId": 278904856
    },
    "references": [
      {
        "paperId": "03d77ddb975ec3a7a0943dbbad701fb7bc0b2791",
        "title": "UniVG-R1: Reasoning Guided Universal Visual Grounding with Reinforcement Learning"
      },
      {
        "paperId": "1d03586baa32b3d6ff657a180053821543e11abb",
        "title": "RAGEN: Understanding Self-Evolution in LLM Agents via Multi-Turn Reinforcement Learning"
      },
      {
        "paperId": "d981ce332586e6a29f595cbdfa9347cf425e5cd0",
        "title": "Open-Reasoner-Zero: An Open Source Approach to Scaling Up Reinforcement Learning on the Base Model"
      },
      {
        "paperId": "de23d38bc2604dcf334dcc46aff217eb6bcd1fe1",
        "title": "Understanding R1-Zero-Like Training: A Critical Perspective"
      },
      {
        "paperId": "dd4cfde3e135f799a9a71b4f57e13a29de89f7e3",
        "title": "DAPO: An Open-Source LLM Reinforcement Learning System at Scale"
      },
      {
        "paperId": "ffc09c7971df578929ea3764d06e4ab62886421f",
        "title": "Improving Vision-Language-Action Model with Online Reinforcement Learning"
      },
      {
        "paperId": "34471a2fa18ea22efad5287cf4aeb18542c98a9b",
        "title": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "paperId": "3880f5bad862ba1b18f4f8ec060038b326b118ed",
        "title": "FAST: Efficient Action Tokenization for Vision-Language-Action Models"
      },
      {
        "paperId": "4a7e1870fa76329da369211ab0630ce4960402f6",
        "title": "The Lessons of Developing Process Reward Models in Mathematical Reasoning"
      },
      {
        "paperId": "88aa6b1f37d1fd8e0a40499ce9bb87873f03aaa8",
        "title": "Qwen2.5 Technical Report"
      },
      {
        "paperId": "dff2c605fbd859f6c3049721194e3af3083a0927",
        "title": "RLDG: Robotic Generalist Policy Distillation via Reinforcement Learning"
      },
      {
        "paperId": "52d0be520845f45cac46ddcf3764ac84ff04625c",
        "title": "Ponder & Press: Advancing Visual GUI Agent towards General Computer Control"
      },
      {
        "paperId": "134a66cc64344b9ef913e0f25c2fab6f4ff92c94",
        "title": "ATP-LLaVA: Adaptive Token Pruning for Large Vision Language Models"
      },
      {
        "paperId": "fce6e97893c652bc441f510eaab12043f93fa924",
        "title": "GRAPE: Generalizing Robot Policy via Preference Alignment"
      },
      {
        "paperId": "6a7c29829227bfd65ae0ffec294a874bb9ea0871",
        "title": "T\u00dcLU 3: Pushing Frontiers in Open Language Model Post-Training"
      },
      {
        "paperId": "ef0c2a66b7bc82a2f90ac298310c064a49da8213",
        "title": "\u03c00: A Vision-Language-Action Flow Model for General Robot Control"
      },
      {
        "paperId": "8440692409f544c6eb3e4193fb391a1dc7e8be4d",
        "title": "Precise and Dexterous Robotic Manipulation via Human-in-the-Loop Reinforcement Learning"
      },
      {
        "paperId": "e9ae8e12ba18430ab501c8b1f70ec5c66a92ba98",
        "title": "Data Scaling Laws in Imitation Learning for Robotic Manipulation"
      },
      {
        "paperId": "f2144e875ca6270cd3fc4f4e177c2c07a338cb62",
        "title": "FLaRe: Achieving Masterful and Adaptive Robot Policies with Large-Scale Reinforcement Learning Fine-Tuning"
      },
      {
        "paperId": "4f5aa34d2e6758f78dc4a731cc2eed26f83de3b4",
        "title": "Deep Reinforcement Learning for Robotics: A Survey of Real-World Successes"
      },
      {
        "paperId": "8292083dd8f6ae898ea0ee54a6b97997d1a51c9d",
        "title": "Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters"
      },
      {
        "paperId": "d133c7bc08ba0e50d974adac78b3ede743f0e8f9",
        "title": "Hierarchical Memory for Long Video QA"
      },
      {
        "paperId": "b9f39a73b67707a95b3a5eaacc7e28af808b455e",
        "title": "PoliFormer: Scaling On-Policy RL with Transformers Results in Masterful Navigators"
      },
      {
        "paperId": "d3c5f4d6a90c158fccc7df5c8e6644b47d5786f6",
        "title": "VoCo-LLaMA: Towards Vision Compression with Large Language Models"
      },
      {
        "paperId": "8f9ceb5ffad8e7a066dfc9d9aaa5153b714740ee",
        "title": "OpenVLA: An Open-Source Vision-Language-Action Model"
      },
      {
        "paperId": "f9cfedb64717235ede7e5921c76f94acb3899ef9",
        "title": "Flash-VStream: Memory-Based Real-Time Understanding for Long Video Streams"
      },
      {
        "paperId": "1d2753d74025e7a71594506623be81f18b073adb",
        "title": "Octo: An Open-Source Generalist Robot Policy"
      },
      {
        "paperId": "ee5070fe52fd17da9a89d3f342fb07cc9ae51afe",
        "title": "DROID: A Large-Scale In-The-Wild Robot Manipulation Dataset"
      },
      {
        "paperId": "1a8254800afd2de64eb3451d7db1a56ad4c637e1",
        "title": "Bootstrapping Reinforcement Learning with Imitation for Vision-Based Agile Flight"
      },
      {
        "paperId": "40c52f7a1df09143f7cc4d686e3d78e9e0f77f21",
        "title": "Fine-tuning Reinforcement Learning Models is Secretly a Forgetting Mitigation Problem"
      },
      {
        "paperId": "fc4d4a25aac973b516c0bf890e36660fe228e79a",
        "title": "SERL: A Software Suite for Sample-Efficient Robotic Reinforcement Learning"
      },
      {
        "paperId": "de4c362339afc7e070bd4250de3dcb06b32a46fc",
        "title": "ThinkBot: Embodied Instruction Following with Thought Chain Reasoning"
      },
      {
        "paperId": "f90595f99a0c66d2bb6d0f230f17c7cd8c58f44d",
        "title": "Qwen-Audio: Advancing Universal Audio Understanding via Unified Large-Scale Audio-Language Models"
      },
      {
        "paperId": "00f776a588e13e4c9278e91f7c10c536f6dc8c2d",
        "title": "Imitation Bootstrapped Reinforcement Learning"
      },
      {
        "paperId": "ef7d31137ef06c5be8c2824ecc5af6ce3358cc8f",
        "title": "Open X-Embodiment: Robotic Learning Datasets and RT-X Models"
      },
      {
        "paperId": "83b90f4a0ae4cc214eb3cc140ccfef9cd99fac05",
        "title": "Efficient Memory Management for Large Language Model Serving with PagedAttention"
      },
      {
        "paperId": "148e95859248878a0695a31ef6165614a01df631",
        "title": "RoboAgent: Generalization and Efficiency in Robot Manipulation via Semantic Augmentations and Action Chunking"
      },
      {
        "paperId": "182c7b40ff7560a5545764814338f55a2098e441",
        "title": "Reinforced Self-Training (ReST) for Language Modeling"
      },
      {
        "paperId": "91206346edbe28abb606d7b3425cd455d4019d4f",
        "title": "Scaling Relationship on Learning Mathematical Reasoning with Large Language Models"
      },
      {
        "paperId": "38939304bb760473141c2aca0305e44fbe04e6e8",
        "title": "RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control"
      },
      {
        "paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
        "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
      },
      {
        "paperId": "c8ef5b72b608a9e51b506aed28fb41fce7e13a86",
        "title": "RH20T: A Comprehensive Robotic Dataset for Learning Diverse Skills in One-Shot"
      },
      {
        "paperId": "98cfd7b1b29453c4e82536f5afdc6ddc58bbb1b3",
        "title": "LIBERO: Benchmarking Knowledge Transfer for Lifelong Robot Learning"
      },
      {
        "paperId": "be8db99310602d66bba64bcf41a572c45816fbfc",
        "title": "Let's Verify Step by Step"
      },
      {
        "paperId": "0d1c76d45afa012ded7ab741194baf142117c495",
        "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
      },
      {
        "paperId": "2b1121a97cb7385f553aeed57451e276f174ff67",
        "title": "Causal Policy Gradient for Whole-Body Mobile Manipulation"
      },
      {
        "paperId": "a0e7c31d723608e03f30fc92ffc2a604a7a039da",
        "title": "PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel"
      },
      {
        "paperId": "5a9cb1b3dc4655218b3deeaf4a2417a9a8cd0891",
        "title": "DINOv2: Learning Robust Visual Features without Supervision"
      },
      {
        "paperId": "35aba190f28b5c39df333c06ca21f46bd4845eba",
        "title": "Sigmoid Loss for Language Image Pre-Training"
      },
      {
        "paperId": "bdba3bd30a49ea4c5b20b43dbd8f0eb59e9d80e2",
        "title": "Diffusion Policy: Visuomotor Policy Learning via Action Diffusion"
      },
      {
        "paperId": "bd38cbbb346a347cb5b60ac4a133b3d73cb44e07",
        "title": "Efficient Online Reinforcement Learning with Offline Data"
      },
      {
        "paperId": "fd1cf28a2b8caf2fe29af5e7fa9191cecfedf84d",
        "title": "RT-1: Robotics Transformer for Real-World Control at Scale"
      },
      {
        "paperId": "65fc1f1c567801fee3788974e753cdbf934f07e9",
        "title": "Video PreTraining (VPT): Learning to Act by Watching Unlabeled Online Videos"
      },
      {
        "paperId": "5e86a1e80cd7a84a5ff316f59345f00c402bddb5",
        "title": "Reincarnating Reinforcement Learning: Reusing Prior Computation to Accelerate Progress"
      },
      {
        "paperId": "f3a63a840185fa8c5e0db4bbe12afa7d3de7d029",
        "title": "Jump-Start Reinforcement Learning"
      },
      {
        "paperId": "23dd78e424d32f6a48660dcd67ce994b8a7db8be",
        "title": "STaR: Bootstrapping Reasoning With Reasoning"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "1d803f07e4591bd67c358eef715bcd443e821894",
        "title": "BC-Z: Zero-Shot Task Generalization with Robotic Imitation Learning"
      },
      {
        "paperId": "62272403114c67a85e6fde9e428334d89e143485",
        "title": "AW-Opt: Learning Robotic Skills with Imitation and Reinforcement at Scale"
      },
      {
        "paperId": "6bae20930eaa0d9d489317f3b3b1aaaf18205ef8",
        "title": "Bridge Data: Boosting Generalization of Robotic Skills with Cross-Domain Datasets"
      },
      {
        "paperId": "a8ca46b171467ceb2d7652fbfb67fe701ad86092",
        "title": "LoRA: Low-Rank Adaptation of Large Language Models"
      },
      {
        "paperId": "c1ad5f9b32d80f1c65d67894e5b8c2fdf0ae4500",
        "title": "Decision Transformer: Reinforcement Learning via Sequence Modeling"
      },
      {
        "paperId": "3e85d208b1b927fdb69ecf8336c70995818aaebd",
        "title": "MT-Opt: Continuous Multi-Task Robotic Reinforcement Learning at Scale"
      },
      {
        "paperId": "9faecf3e18a833f2d49b030d591cc2ded0b54336",
        "title": "Towards Continual Reinforcement Learning: A Review and Perspectives"
      },
      {
        "paperId": "f8492a321d66c381637b693a24af994af41b3cdf",
        "title": "Transfer Learning in Deep Reinforcement Learning: A Survey"
      },
      {
        "paperId": "0272b14dd471fe7b81df703af1b71d7600b77215",
        "title": "Accelerating Online Reinforcement Learning with Offline Datasets"
      },
      {
        "paperId": "2d85f63a193cd88741c8398ceb98c55e1e89387d",
        "title": "The Ingredients of Real-World Robotic Reinforcement Learning"
      },
      {
        "paperId": "5dab673e999a2ea7be23d57467fab4276c6e7ede",
        "title": "Never Stop Learning: The Effectiveness of Fine-Tuning in Robotic Reinforcement Learning"
      },
      {
        "paperId": "8c54e8575e7c17a4097838305915e6e7b00fd4af",
        "title": "Relay Policy Learning: Solving Long-Horizon Tasks via Imitation and Reinforcement Learning"
      },
      {
        "paperId": "3e519d85cdcefdd1d2ad89829d6ad445695d8c58",
        "title": "RoboNet: Large-Scale Multi-Robot Learning"
      },
      {
        "paperId": "320b227027030fc291de2896fc3c6da49d7614be",
        "title": "Solving Rubik's Cube with a Robot Hand"
      },
      {
        "paperId": "78aea5a51feb90e988a4d00302616e56d1be5eb0",
        "title": "Scaling data-driven robotics with reward sketching and batch reinforcement learning"
      },
      {
        "paperId": "cda470bede832f2965e594f9bdee79d6973a91e9",
        "title": "ROBOTURK: A Crowdsourcing Platform for Robotic Skill Learning through Imitation"
      },
      {
        "paperId": "8b83a0fd1337b7e5dbac87e137e4c140866dd8be",
        "title": "Robot Learning in Homes: Improving Generalization and Reducing Dataset Bias"
      },
      {
        "paperId": "eb37e7b76d26b75463df22b2a3aa32b6a765c672",
        "title": "QT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation"
      },
      {
        "paperId": "d356a5603f14c7a6873272774782d7812871f952",
        "title": "Reinforcement and Imitation Learning for Diverse Visuomotor Skills"
      },
      {
        "paperId": "f83a207712fd4cf41aded79e9e6c4345ba879128",
        "title": "Ray: A Distributed Framework for Emerging AI Applications"
      },
      {
        "paperId": "e010ba3ff5744604cdbfe44a733e2a98649ee907",
        "title": "Learning Complex Dexterous Manipulation with Deep Reinforcement Learning and Demonstrations"
      },
      {
        "paperId": "1bead9000a719cb258bac7320228055aee650d2c",
        "title": "Leveraging Demonstrations for Deep Reinforcement Learning on Robotics Problems with Sparse Rewards"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "e3b0ea7209731c47b582215c6c67f9c691ad9863",
        "title": "Deep Q-learning From Demonstrations"
      },
      {
        "paperId": "f03b4ff1b4943691cec703b508c0a91f2d97a881",
        "title": "Supersizing self-supervision: Learning to grasp from 50K tries and 700 robot hours"
      },
      {
        "paperId": "d316c82c12cf4c45f9e85211ef3d1fa62497bff8",
        "title": "High-Dimensional Continuous Control Using Generalized Advantage Estimation"
      },
      {
        "paperId": "467568f1777bc51a15a5100516cd4fe8de62b9ab",
        "title": "Transfer Learning for Reinforcement Learning Domains: A Survey"
      },
      {
        "paperId": "9deda73da446cbb21550abaa62b41f5a3acd92fc",
        "title": "OpenRLHF: An Easy-to-use, Scalable and High-performance RLHF Framework"
      },
      {
        "paperId": "df4928b9636ecc7944d097f2bfb493008a0ec138",
        "title": "Plan, Posture and Go: Towards Open-Vocabulary Text-to-Motion Generation"
      },
      {
        "paperId": "5b7ef5b8604e56d2616b4f55320c633a541b3dfc",
        "title": "Imitating Shortest Paths in Simulation Enables Effective Navigation and Manipulation in the Real World"
      },
      {
        "paperId": "9a21003ba1334e2fa7fc48178f37e02027e91936",
        "title": "Investigating Multi-task Pretraining and Generalization in Reinforcement Learning"
      },
      {
        "paperId": "c5f8d083ea2a49d921f4f74d16ba6923a504dd08",
        "title": "Imitation and Reinforcement Learning for Motor Primitives with Perceptual Coupling"
      },
      {
        "paperId": "9b608a6c80271cc996f22f584f9da4c43e354f45",
        "title": "Welcome to the Era of Experience"
      }
    ],
    "cited_by": [
      {
        "paperId": "492a99e7c25ed0f96ad65c2cdc88cbb2b3dfafe3",
        "title": "VLA-RFT: Vision-Language-Action Reinforcement Fine-tuning with Verified Rewards in World Simulators"
      },
      {
        "paperId": "50edc85668afdda254ce4ba7703aa10f41a77fcb",
        "title": "World-Env: Leveraging World Model as a Virtual Environment for VLA Post-Training"
      },
      {
        "paperId": "1e8305bd658da8763216f9521b6e8a45e9ab47ef",
        "title": "ReLAM: Learning Anticipation Model for Rewarding Visual Robotic Manipulation"
      },
      {
        "paperId": "cf403503b4200ffc4af8e95077735445d7baa214",
        "title": "VLA-Reasoner: Empowering Vision-Language-Action Models with Reasoning via Online Monte Carlo Tree Search"
      },
      {
        "paperId": "cf3e01ec29ce6d6ec3446646327bd6b54fea9d6f",
        "title": "Pure Vision Language Action (VLA) Models: A Comprehensive Survey"
      },
      {
        "paperId": "2e3d2cb75be2d8939387f9173b68cd81315ec694",
        "title": "Agentic Reinforcement Learning with Implicit Step Rewards"
      },
      {
        "paperId": "a3d88cddfa87cc7b66438920443e6224711b454b",
        "title": "RoboSeek: You Need to Interact with Your Objects"
      },
      {
        "paperId": "a78271d62892d11dfe7e68851ea4392007b4c5e9",
        "title": "Dual-Actor Fine-Tuning of VLA Models: A Talk-and-Tweak Human-in-the-Loop Approach"
      },
      {
        "paperId": "bca7f4dd4db559d6772b470d9fe5391e3608cc8c",
        "title": "SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning"
      },
      {
        "paperId": "5380fe9137f1ffec91b06fe4db60a083759a34fd",
        "title": "F1: A Vision-Language-Action Model Bridging Understanding and Generation to Actions"
      },
      {
        "paperId": "8cc3498c93e6003bbc0203e761a67021bf8a988e",
        "title": "Balancing Signal and Variance: Adaptive Offline RL Post-Training for VLA Flow Models"
      },
      {
        "paperId": "4f157904d3f624c018d09dec7d9e95278a2e6f9e",
        "title": "Survey of Vision-Language-Action Models for Embodied Manipulation"
      },
      {
        "paperId": "bf5950355f8bec8175b5082340ab35cc81314fae",
        "title": "Embodied-R1: Reinforced Embodied Reasoning for General Robotic Manipulation"
      },
      {
        "paperId": "dce821cddbf6a3eb435de04d9353681925aaaecf",
        "title": "Large VLM-based Vision-Language-Action Models for Robotic Manipulation: A Survey"
      },
      {
        "paperId": "9a5071bacb2bcb98ddd95076dbd446240b4da2c6",
        "title": "Reinforcement Learning in Vision: A Survey"
      },
      {
        "paperId": "956c03803fca72e647a19f26fef50d0177505e07",
        "title": "Learning to See and Act: Task-Aware View Planning for Robotic Manipulation"
      },
      {
        "paperId": "e2641056395b1ea7b75245812389e9faa9c0b284",
        "title": "RLRC: Reinforcement Learning-based Recovery for Compressed Vision-Language-Action Models"
      },
      {
        "paperId": "9fa625528073fee005cf2d0fd2b2e9699a1af926",
        "title": "ReWiND: Language-Guided Rewards Teach Robot Policies without New Demonstrations"
      }
    ],
    "score": 18.0
  },
  {
    "id": "1a73038804052a40c12aae696848ece2168f6da7",
    "title": "On the Importance of Exploration for Generalization in Reinforcement Learning",
    "authors": [
      "Yiding Jiang",
      "J. Z. Kolter",
      "R. Raileanu"
    ],
    "year": 2023,
    "citationCount": 32,
    "abstract": "Existing approaches for improving generalization in deep reinforcement learning (RL) have mostly focused on representation learning, neglecting RL-specific aspects such as exploration. We hypothesize that the agent's exploration strategy plays a key role in its ability to generalize to new environments. Through a series of experiments in a tabular contextual MDP, we show that exploration is helpful not only for efficiently finding the optimal policy for the training environments but also for acquiring knowledge that helps decision making in unseen environments. Based on these observations, we propose EDE: Exploration via Distributional Ensemble, a method that encourages exploration of states with high epistemic uncertainty through an ensemble of Q-value distributions. Our algorithm is the first value-based approach to achieve state-of-the-art on both Procgen and Crafter, two benchmarks for generalization in RL with high-dimensional observations. The open-sourced implementation can be found at https://github.com/facebookresearch/ede .",
    "url": "https://www.semanticscholar.org/paper/1a73038804052a40c12aae696848ece2168f6da7",
    "pdf_url": "https://arxiv.org/pdf/2306.05483.pdf",
    "venue": "Neural Information Processing Systems",
    "publicationDate": "2023-06-08",
    "externalIds": {
      "DBLP": "conf/nips/JiangKR23",
      "ArXiv": "2306.05483",
      "DOI": "10.48550/arXiv.2306.05483",
      "CorpusId": 259129530
    },
    "references": [
      {
        "paperId": "76cf947057333b9ecb6e83c066d4f0dbcc278d3e",
        "title": "Adversarial Style Transfer for Robust Policy Optimization in Deep Reinforcement Learning"
      },
      {
        "paperId": "0acdcca3eb56ad23959fe5367ddbbf9623384785",
        "title": "Reducing Variance in Temporal-Difference Value Estimation via Ensemble of Deep Networks"
      },
      {
        "paperId": "50aae79f43e4499183241bd66787177fa7751901",
        "title": "Learning Dynamics and Generalization in Reinforcement Learning"
      },
      {
        "paperId": "4bceb9f2162d6931ccf704002d71fcc813d36689",
        "title": "Disentangling Epistemic and Aleatoric Uncertainty in Reinforcement Learning"
      },
      {
        "paperId": "69b80ce5ab6c2263b145b1eaa23664145938f351",
        "title": "The Primacy Bias in Deep Reinforcement Learning"
      },
      {
        "paperId": "e016b35c422e94b302f9f6d0508b47469aa0b189",
        "title": "Evolving Curricula with Regret-Based Environment Design"
      },
      {
        "paperId": "5be0c196a8884eb87fa67cb51284547fd2aa85e1",
        "title": "Improving Zero-shot Generalization in Offline Reinforcement Learning using Generalized Similarity Functions"
      },
      {
        "paperId": "44164c068499fbe387a1765104d69a8cbc5f0327",
        "title": "Procedural Generalization by Planning with Self-Supervised World Models"
      },
      {
        "paperId": "fdb8ccebef9f544f44cd9b88085d8ec5458b38ab",
        "title": "Replay-Guided Adversarial Environment Design"
      },
      {
        "paperId": "8e128a1b2efb0ddf688902ade4405d22d5b61eec",
        "title": "Benchmarking the Spectrum of Agent Capabilities"
      },
      {
        "paperId": "558ca2e8c7eb56edd77a52b084e6cc24dffe5bcd",
        "title": "Deep Reinforcement Learning at the Edge of the Statistical Precipice"
      },
      {
        "paperId": "e06c005e98281af455c454ce2478285f6f3afeca",
        "title": "Mastering Visual Continuous Control: Improved Data-Augmented Reinforcement Learning"
      },
      {
        "paperId": "ef24c31e07443b5d08dfbc822bff06acb7bc7cbd",
        "title": "Why Generalization in RL is Difficult: Epistemic POMDPs and Implicit Partial Observability"
      },
      {
        "paperId": "390e248117f8bafad8b47b8e799352980c2f3f70",
        "title": "Stabilizing Deep Q-Learning with ConvNets and Vision Transformers under Data Augmentation"
      },
      {
        "paperId": "8fac9e3a0eba8ea559a4e6006d9596c8ebe77bf4",
        "title": "Learning Task Informed Abstractions"
      },
      {
        "paperId": "a54d3a4b732d2945f3830321c38576e23e062485",
        "title": "Measuring Sample Efficiency and Generalization in Reinforcement Learning Benchmarks: NeurIPS 2020 Procgen Benchmark"
      },
      {
        "paperId": "de887347a073106b33f7ebbb5cbd6cffa94a4d37",
        "title": "Learning One Representation to Optimize All Rewards"
      },
      {
        "paperId": "09f36087d9dae1ca5ccc1d75bb326954ec30239a",
        "title": "Robust Deep Reinforcement Learning via Multi-View Information Bottleneck"
      },
      {
        "paperId": "8272d2f9412e9151c023011205227859a5021177",
        "title": "Decoupling Value and Policy for Generalization in Reinforcement Learning"
      },
      {
        "paperId": "3b9c93876303d2a0f96d44e2392f54c7cc61b376",
        "title": "Efficient Scheduling of Data Augmentation for Deep Reinforcement Learning"
      },
      {
        "paperId": "60056a89051545aae35d721adc62677f2ea3ee05",
        "title": "Adversarially Guided Actor-Critic"
      },
      {
        "paperId": "a188c3b58e71657ecfcddc37359aca51213e2187",
        "title": "Rank the Episodes: A Simple Approach for Exploration in Procedurally-Generated Environments"
      },
      {
        "paperId": "7428f65393c19a6ca6381693767cb4f643a49a5c",
        "title": "Contrastive Behavioral Similarity Embeddings for Generalization in Reinforcement Learning"
      },
      {
        "paperId": "296aec3d9d045670eebc648f74b8f0a79d0b7192",
        "title": "When Is Generalizable Reinforcement Learning Tractable?"
      },
      {
        "paperId": "600aaa3918feb25a690f69931446c8ace2863d8e",
        "title": "Generalization in Reinforcement Learning by Soft Data Augmentation"
      },
      {
        "paperId": "9e380323c2a6db51450253e3ce5ed72bddbe64a0",
        "title": "Instance based Generalization in Reinforcement Learning"
      },
      {
        "paperId": "e4a46c64aafbef0406e9cfa90dd9c43e3e07598c",
        "title": "One Solution is Not All You Need: Few-Shot Extrapolation via Structured MaxEnt RL"
      },
      {
        "paperId": "3caf26a81f8b8be27b8ebc09bdb93416b4c40fe6",
        "title": "Improving Generalization in Reinforcement Learning with Mixture Regularization"
      },
      {
        "paperId": "2dd28b14430b88fefa48402d2adda860d7e55ff8",
        "title": "Measuring Visual Generalization in Continuous Control from Pixels"
      },
      {
        "paperId": "6999fd72868c4044e852c43a040a87a43d03ab3a",
        "title": "Prioritized Level Replay"
      },
      {
        "paperId": "b44bb1762640ed72091fd5f5fdc20719a6dc24af",
        "title": "Mastering Atari with Discrete World Models"
      },
      {
        "paperId": "17985b57240bfaea02a6098a7a34e71e780180eb",
        "title": "Decoupling Representation Learning from Reinforcement Learning"
      },
      {
        "paperId": "2a9336f92bcdc650c5257ec0cc1b4cd272f5ed1a",
        "title": "Decoupling Exploration and Exploitation for Meta-Reinforcement Learning without Sacrifices"
      },
      {
        "paperId": "324effa5a7c737257b675f85bd93d777fc486878",
        "title": "SUNRISE: A Simple Unified Framework for Ensemble Learning in Deep Reinforcement Learning"
      },
      {
        "paperId": "05c82617cdaa16c9dc17c32f3cb5ed4a7182b13e",
        "title": "Automatic Data Augmentation for Generalization in Deep Reinforcement Learning"
      },
      {
        "paperId": "518b827e340c26582b5093401283a4f5cff605b9",
        "title": "Learning Invariant Representations for Reinforcement Learning without Reconstruction"
      },
      {
        "paperId": "4a4ef182464b5ed53561d6eed0c173935c054f88",
        "title": "Task-agnostic Exploration in Reinforcement Learning"
      },
      {
        "paperId": "27a7df880e9c4ccd87cb88cccb131e2b4687567f",
        "title": "Deep Reinforcement and InfoMax Learning"
      },
      {
        "paperId": "43cfafb95bbfc2974437c517a6d63107ecfca12d",
        "title": "The Impact of Non-stationarity on Generalisation in Deep Reinforcement Learning"
      },
      {
        "paperId": "3f01a03a284ad2d1745e6d0d378384bcab49bd3e",
        "title": "Sample Complexity of Asynchronous Q-Learning: Sharper Analysis and Variance Reduction"
      },
      {
        "paperId": "ca96857a584bab75c5bf8225357a6197fb5892b3",
        "title": "Visual Transfer for Reinforcement Learning via Wasserstein Domain Confusion"
      },
      {
        "paperId": "c6ba8e55f2fed2037378a94a3f8a2b1b09825636",
        "title": "Temporally-Extended \u03b5-Greedy Exploration"
      },
      {
        "paperId": "bbf52b4a14c033f801670fa3faa31222e0d6f181",
        "title": "Invariant Policy Optimization: Towards Stronger Generalization in Reinforcement Learning"
      },
      {
        "paperId": "822bdb6e8c39e272ebfee127666e032bd3aa0107",
        "title": "The NetHack Learning Environment"
      },
      {
        "paperId": "744139d65c3bf6da6a6acd384a32d94a06f44f62",
        "title": "Reinforcement Learning with Augmented Data"
      },
      {
        "paperId": "2b2735cffb0d2321a456363880ff5671e80df4cb",
        "title": "On Bonus Based Exploration Methods In The Arcade Learning Environment"
      },
      {
        "paperId": "6568423cfaca7e24c88ea208cb0e67129e43aa9b",
        "title": "Image Augmentation Is All You Need: Regularizing Deep Reinforcement Learning from Pixels"
      },
      {
        "paperId": "6509c28c459e07021a2505258715442b2ff0d72b",
        "title": "Reinforcement Learning Generalization with Surprise Minimization"
      },
      {
        "paperId": "9651e1987a39d100dc0f6696a2077199e5075ea2",
        "title": "Agent57: Outperforming the Atari Human Benchmark"
      },
      {
        "paperId": "66e27719c87c807abc9fbf9afe3682ebfb9cc69e",
        "title": "Interference and Generalization in Temporal Difference Learning"
      },
      {
        "paperId": "54e1a1f3a60ce51eae5e27e1724294032cc60929",
        "title": "Invariant Causal Prediction for Block MDPs"
      },
      {
        "paperId": "bebe8ffb0c357ac0c7eea2556f817b03ee22b570",
        "title": "RIDE: Rewarding Impact-Driven Exploration for Procedurally-Generated Environments"
      },
      {
        "paperId": "2d0e625fca0efc079bb523baf1ff78c46ff4916e",
        "title": "Rotation, Translation, and Cropping for Zero-Shot Generalization"
      },
      {
        "paperId": "384ce8a6d36ef3102a71b8d32ebabafad26b7da7",
        "title": "No-Regret Exploration in Goal-Oriented Reinforcement Learning"
      },
      {
        "paperId": "675770f77cd913abc7b7f0466f281cb6c4b383f7",
        "title": "Observational Overfitting in Reinforcement Learning"
      },
      {
        "paperId": "8d814620a1ca77e745bc8a33b96b86148f2804fe",
        "title": "Leveraging Procedural Generation to Benchmark Reinforcement Learning"
      },
      {
        "paperId": "2c3a1a088ef51548264197ed8882f42e0ad73a9b",
        "title": "Generalization in Reinforcement Learning with Selective Noise Injection and Information Bottleneck"
      },
      {
        "paperId": "dc05886db1e6f17f4489d867477b38fe13e31783",
        "title": "Network Randomization: A Simple Technique for Generalization in Deep Reinforcement Learning"
      },
      {
        "paperId": "6ddf83bee5ca7a0542964e389a98adc1ed4a6838",
        "title": "Deep Ensembles: A Loss Landscape Perspective"
      },
      {
        "paperId": "d6285ff3dcb15c1da84dcbc98141be10ef0e8dd1",
        "title": "Estimating Risk and Uncertainty in Deep Reinforcement Learning"
      },
      {
        "paperId": "f6b218453170edcbb51e49dd44ba2f83af53ef92",
        "title": "Distributional Reinforcement Learning for Efficient Exploration"
      },
      {
        "paperId": "7388826b5ee00efe17cb7f19a623d9b5e955ae70",
        "title": "Skew-Fit: State-Covering Self-Supervised Reinforcement Learning"
      },
      {
        "paperId": "16da857c08186f2a923da1cfaf88a8a16507d3f9",
        "title": "Obstacle Tower: A Generalization Challenge in Vision, Control, and Planning"
      },
      {
        "paperId": "ef2bc452812d6005ab0a66af6c3f97b6b0ba837e",
        "title": "Quantifying Generalization in Reinforcement Learning"
      },
      {
        "paperId": "f8a54ba839f6194198b4886097169a53905fbb37",
        "title": "Uncertainty in Neural Networks: Approximately Bayesian Ensembling"
      },
      {
        "paperId": "4cb3fd057949624aa4f0bbe7a6dcc8777ff04758",
        "title": "Exploration by Random Network Distillation"
      },
      {
        "paperId": "567eb0ab96fb7f2869e9c4c204da9f2cf422d946",
        "title": "Generalization and Regularization in DQN"
      },
      {
        "paperId": "caea502325b6a82b1b437c62585992609b5aa542",
        "title": "Assessing Generalization in Deep Reinforcement Learning"
      },
      {
        "paperId": "9f67b3edc67a35c884bd532a5e73fa3a7f3660d8",
        "title": "Count-Based Exploration with the Successor Representation"
      },
      {
        "paperId": "60fc8a885083f74c7a0aea829c81a92f2107e4d1",
        "title": "Illuminating Generalization in Deep Reinforcement Learning through Procedural Level Generation"
      },
      {
        "paperId": "a622be547caf0b1223626de5e69377c20ae11265",
        "title": "A Dissection of Overfitting and Generalization in Continuous Reinforcement Learning"
      },
      {
        "paperId": "041d99f442dc22cf51118dc992095be9aa0972e0",
        "title": "A Study on Overfitting in Deep Reinforcement Learning"
      },
      {
        "paperId": "b72e488b48e464c7633418873f1cf9fbf3453ff2",
        "title": "Gotta Learn Fast: A New Benchmark for Generalization in RL"
      },
      {
        "paperId": "1485c5ac93b3f01a5f3ef7824eb428c8bff39ba3",
        "title": "UCB EXPLORATION VIA Q-ENSEMBLES"
      },
      {
        "paperId": "601a2d349fc26d7b82f905e924e2f91b0ac4b310",
        "title": "Distributed Prioritized Experience Replay"
      },
      {
        "paperId": "80196cdfcd0c6ce2953bf65a7f019971e2026386",
        "title": "IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures"
      },
      {
        "paperId": "fe3e91e40a950c6b6601b8f0a641884774d949ae",
        "title": "Distributional Reinforcement Learning with Quantile Regression"
      },
      {
        "paperId": "0ab3f7ecbdc5a33565a234215604a6ca9d155a33",
        "title": "Rainbow: Combining Improvements in Deep Reinforcement Learning"
      },
      {
        "paperId": "3b290ffa1f4f8226e326f00984acecdfbe9e28bf",
        "title": "Revisiting the Arcade Learning Environment: Evaluation Protocols and Open Problems for General Agents"
      },
      {
        "paperId": "bdf6572b67a6c5d8aacd39e1826db2c5c8f85716",
        "title": "The Uncertainty Bellman Equation and Exploration"
      },
      {
        "paperId": "c1f4ef741242d629d1f56e442a09a7ba29595a0e",
        "title": "A Distributional Perspective on Reinforcement Learning"
      },
      {
        "paperId": "3fab4d28be5ff73f42c8b307117aa8d680345ad9",
        "title": "Noisy Networks for Exploration"
      },
      {
        "paperId": "225ab689f41cef1dc18237ef5dab059a49950abf",
        "title": "Curiosity-Driven Exploration by Self-Supervised Prediction"
      },
      {
        "paperId": "0dff37d661a0c629ba846f43a62687679ebb8506",
        "title": "Exploration-Exploitation in MDPs with Options"
      },
      {
        "paperId": "67d10cea808937089d6492acff14ad9ef156e3c5",
        "title": "Minimax Regret Bounds for Reinforcement Learning"
      },
      {
        "paperId": "ff7bcaa4556cb13fc7bf03e477172493546172cd",
        "title": "What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?"
      },
      {
        "paperId": "cadb96f49f3c13c86dfe285b5c75c655391ad1c3",
        "title": "Towards Generalization and Simplicity in Continuous Control"
      },
      {
        "paperId": "802168a81571dde28f5ddb94d84677bc007afa7b",
        "title": "Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles"
      },
      {
        "paperId": "250b20c201df2dd3fb425a95f9be61ed8b80afda",
        "title": "Curiosity Search: Producing Generalists by Encouraging Individuals to Continually Explore and Acquire Skills throughout Their Lifetime"
      },
      {
        "paperId": "6e90fd78e8a3b98af3954aae5209703aa966603e",
        "title": "Unifying Count-Based Exploration and Intrinsic Motivation"
      },
      {
        "paperId": "4b63e34276aa98d5345efa7fe09bb06d8a9d8f52",
        "title": "Deep Exploration via Bootstrapped DQN"
      },
      {
        "paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d",
        "title": "Deep Residual Learning for Image Recognition"
      },
      {
        "paperId": "c6170fa90d3b2efede5a2e1660cb23e1c824f2ca",
        "title": "Prioritized Experience Replay"
      },
      {
        "paperId": "f4aa31b7ae2e03ee5a6b94f34cb3b6a554230aef",
        "title": "Sample Complexity of Episodic Fixed-Horizon Reinforcement Learning"
      },
      {
        "paperId": "909f736182963dcee2cb2e8c567e2069192b4749",
        "title": "Bootstrapped Thompson Sampling and Deep Exploration"
      },
      {
        "paperId": "04955600df47c66a055591d927c024e6e9c72c61",
        "title": "Contextual Markov Decision Processes"
      },
      {
        "paperId": "1389772b8a0f9c7fc43057f9da41a7d0ebf0308b",
        "title": "Generalization and Exploration via Randomized Value Functions"
      },
      {
        "paperId": "2319a491378867c7049b3da055c5df60e1671158",
        "title": "Playing Atari with Deep Reinforcement Learning"
      },
      {
        "paperId": "789783016fb708abbc061790612ebe91273c05d3",
        "title": "(More) Efficient Reinforcement Learning via Posterior Sampling"
      },
      {
        "paperId": "ab867c140d2947511979c87e7ae580d9d3f0aeab",
        "title": "An Empirical Evaluation of Thompson Sampling"
      },
      {
        "paperId": "79ab3c49903ec8cb339437ccf5cf998607fc313e",
        "title": "A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning"
      },
      {
        "paperId": "4282669947ca340de9f93a9a2a38007fe542195d",
        "title": "Near-Bayesian exploration in polynomial time"
      },
      {
        "paperId": "8d2820ac17ff3cedf59f173b16b98872848bf3ad",
        "title": "Exploration-exploitation tradeoff using variance estimates in multi-armed bandits"
      },
      {
        "paperId": "237a1cf18ed83bb3ad852b34f443c6c1ff3336c1",
        "title": "An analysis of model-based Interval Estimation for Markov Decision Processes"
      },
      {
        "paperId": "427ce4ce5f64295f39c8714de6ff09fb3bb7d187",
        "title": "Robust Reinforcement Learning"
      },
      {
        "paperId": "1e1d35136b1bf3b13ef6b53f6039f39d9ee820e3",
        "title": "Finite-time Analysis of the Multiarmed Bandit Problem"
      },
      {
        "paperId": "a20f0ce0616def7cc9a87446c228906cd5da093b",
        "title": "Policy Gradient Methods for Reinforcement Learning with Function Approximation"
      },
      {
        "paperId": "65b37ae4010e17d1fa7d53094bb6167caa0eb2f5",
        "title": "Finite-Sample Convergence Rates for Q-Learning and Indirect Algorithms"
      },
      {
        "paperId": "03b7e51c52084ac1db5118342a00b5fbcfc587aa",
        "title": "Q-learning"
      },
      {
        "paperId": "2980dfe5c99658dc3e508d9d6e1d7f26e6fc8934",
        "title": "A possibility for implementing curiosity and boredom in model-building neural controllers"
      },
      {
        "paperId": "bff20fb30adad8d1c173963089df5fc9664304f0",
        "title": "A Markovian Decision Process"
      },
      {
        "paperId": "ee2cd1d17f833d3c157a1016a778c7c22af555a2",
        "title": "ON THE LIKELIHOOD THAT ONE UNKNOWN PROBABILITY EXCEEDS ANOTHER IN VIEW OF THE EVIDENCE OF TWO SAMPLES"
      },
      {
        "paperId": "7365d4b97e38ca7fcd8ac3db194854b59def1d42",
        "title": "Feature-Attending Recurrent Modules for Generalization in Reinforcement Learning"
      },
      {
        "paperId": "13043fb963122bef8aff4d0a9c24eda0ebb9d8fb",
        "title": "An Investigation into the Open World Survival Game Crafter"
      },
      {
        "paperId": "8d15f17ea8f807efe8801d236b7218b6659ac1d9",
        "title": "NovelD: A Simple yet Effective Exploration Criterion"
      },
      {
        "paperId": "e75fb417b54a6eae589ff382874de09d7f58a3de",
        "title": "Open-Ended Learning Leads to Generally Capable Agents"
      },
      {
        "paperId": "42edbc3c29af476c27f102b3de9f04e56b5c642d",
        "title": "A Survey of Generalisation in Deep Reinforcement Learning"
      },
      {
        "paperId": "818826f356444f3daa3447755bf63f171f39ec47",
        "title": "Active Learning Literature Survey"
      },
      {
        "paperId": "4c915c1eecb217c123a36dc6d3ce52d12c742614",
        "title": "Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning"
      },
      {
        "paperId": "97efafdb4a3942ab3efba53ded7413199f79c054",
        "title": "Reinforcement Learning: An Introduction"
      },
      {
        "paperId": "701d221508f65926edf214d07b43f7fc38131665",
        "title": "The Bayesian approach to global optimization"
      },
      {
        "paperId": null,
        "title": "A study of off-policy learning in environments with procedural content generation"
      },
      {
        "paperId": null,
        "title": "Procgen scores on train levels after training on 25M environment steps. The mean and standard deviation are computed using 5 runs with different seeds"
      }
    ],
    "cited_by": [
      {
        "paperId": "121357fd3c0f9fd056154e3beb5a0eb42a74ee38",
        "title": "Complexity-Driven Policy Optimization"
      },
      {
        "paperId": "c0c23c58fc3925ab5a4a144ad839240a8765fbed",
        "title": "Towards Agents That Know When They Don't Know: Uncertainty as a Control Signal for Structured Reasoning"
      },
      {
        "paperId": "a98587acde8e1df366d7c4964b5a08ed979aefb2",
        "title": "Behavioral Exploration: Learning to Explore via In-Context Adaptation"
      },
      {
        "paperId": "30c3606f7472aae170195b6cb5ce139dcf8dd43f",
        "title": "Zero-Shot Context Generalization in Reinforcement Learning from Few Training Contexts"
      },
      {
        "paperId": "845e7b768dd3b846be8e2aa5a001d57fcc635a90",
        "title": "Uncertainty Prioritized Experience Replay"
      },
      {
        "paperId": "f0712cb9128462739a9931a1b08a38d46e3a7d88",
        "title": "Generative Adversarial Imitation Learning Method Based on TD3-SAC Hybrid Algorithm for Robot Motion Control"
      },
      {
        "paperId": "3b9356b4bafad8d15c819ee57aa68c822f605a73",
        "title": "How Ensembles of Distilled Policies Improve Generalisation in Reinforcement Learning"
      },
      {
        "paperId": "71cd62a89d6efeeee1b2bad53f784a0fc29ede5d",
        "title": "Combining meta reinforcement learning with neural plasticity mechanisms for improved AI performance"
      },
      {
        "paperId": "0784d836945cf06ea18faef7dcfae1545f596ee7",
        "title": "Trade-Offs in Navigation Problems Using Value-Based Methods"
      },
      {
        "paperId": "beeda97aa293ec880099d9ca0086b1e64b56365a",
        "title": "On Generalization Across Environments In Multi-Objective Reinforcement Learning"
      },
      {
        "paperId": "aa37bbbdd95343c8f7bd61d6e13164d767eaa330",
        "title": "Ensemble RL through Classifier Models: Enhancing Risk-Return Trade-offs in Trading Strategies"
      },
      {
        "paperId": "e7550a9038577db013910f78397d518656317e80",
        "title": "Adaptive Data Exploitation in Deep Reinforcement Learning"
      },
      {
        "paperId": "48ea067b2c84680117ef5fc3bc01eb0e66907e66",
        "title": "Tackling Uncertainties in Multi-Agent Reinforcement Learning through Integration of Agent Termination Dynamics"
      },
      {
        "paperId": "0e0fd0f46e2fa18b62ffa0dc64a7470561fff213",
        "title": "TransferLight: Zero-Shot Traffic Signal Control on any Road-Network"
      },
      {
        "paperId": "7699f27f4e926f109fdb819d38687cb62f1e528c",
        "title": "From Mystery to Mastery: Failure Diagnosis for Improving Manipulation Policies"
      },
      {
        "paperId": "b7af43849da34b4eb84df6b6167f6cc6adb8f4ed",
        "title": "Efficient Diversity-based Experience Replay for Deep Reinforcement Learning"
      },
      {
        "paperId": "33ad38686a08a25d176cd613b7c12355df7f783d",
        "title": "Improving Generalization on the ProcGen Benchmark with Simple Architectural Changes and Scale"
      },
      {
        "paperId": "01fc2697f974738a2d4316d24c197512c3857d4e",
        "title": "Exploration Implies Data Augmentation: Reachability and Generalisation in Contextual MDPs"
      },
      {
        "paperId": "c6e3e5f794233da2b27ea5dd4af19e62b95c863f",
        "title": "DigiRL: Training In-The-Wild Device-Control Agents with Autonomous Reinforcement Learning"
      },
      {
        "paperId": "83852eac1d5686096b34785b5acd541456825896",
        "title": "Improving Policy Optimization via \u03b5-Retrain"
      },
      {
        "paperId": "fc803bc8be2d388a87a36f821b4cf564a536c41a",
        "title": "Explore-Go: Leveraging Exploration for Generalisation in Deep Reinforcement Learning"
      },
      {
        "paperId": "ed77da2cd99b424dd644da6026c415ed11cd1968",
        "title": "Optimal Gait Control for a Tendon-driven Soft Quadruped Robot by Model-based Reinforcement Learning"
      },
      {
        "paperId": "d21a1fbeb53db40f9dc641e1b2c48cbaf36def08",
        "title": "AUTONODE: A Neuro-Graphic Self-Learnable Engine for Cognitive GUI Automation"
      },
      {
        "paperId": "2a3c0ffbe70b4aa4b5d118cc0da365fe2a04f262",
        "title": "Deal, or no deal (or who knows)? Forecasting Uncertainty in Conversations using Large Language Models"
      },
      {
        "paperId": "5db93d8d91e85b096da59125f490e2ed6db16a1a",
        "title": "Energy-Efficient and Real-Time Sensing for Federated Continual Learning via Sample-Driven Control"
      },
      {
        "paperId": "a5187bbf4bf8bd905dbbc70a60d606ca54408d51",
        "title": "Diverse Projection Ensembles for Distributional Reinforcement Learning"
      },
      {
        "paperId": "c99db0656d62e185486da6bcba6485f182307811",
        "title": "ReLU to the Rescue: Improve Your On-Policy Actor-Critic with Positive Advantages"
      },
      {
        "paperId": "618b5d75751c6448f6b9f065b22e4b2abd9ceac9",
        "title": "Learning generalizable agents via self-supervised exploration"
      },
      {
        "paperId": "ed3a98458ff2e638aea87fe6bcbe5505af5e8101",
        "title": "Policy Optimization using Horizon Regularized Advantage to Improve Generalization in Reinforcement Learning"
      },
      {
        "paperId": "91386f36723796258bbe1f11692f221e1b4561d5",
        "title": "A Replaceable Curiosity-Driven Candidate Agent Exploration Approach for Task-Oriented Dialog Policy Learning"
      },
      {
        "paperId": "7d5b55c8facc8eca1972c5d220791831dba65ad9",
        "title": "Improving Policy Optimization via \ud835\udc3f -Retrain"
      },
      {
        "paperId": "bf242d3a3fcb500e2ab873b7205c35d3dcbd603f",
        "title": "Learning Robust Representations for Visual Reinforcement Learning via Task-Relevant Mask Sampling"
      }
    ],
    "score": 16.0
  },
  {
    "id": "79f923d6575bd8253e2f0b70813caa61a870ccee",
    "title": "Entropy-regularized Diffusion Policy with Q-Ensembles for Offline Reinforcement Learning",
    "authors": [
      "Ruoqing Zhang",
      "Ziwei Luo",
      "Jens Sj\u00f6lund",
      "Thomas B. Sch\u00f6n",
      "Per Mattsson"
    ],
    "year": 2024,
    "citationCount": 16,
    "abstract": "This paper presents advanced techniques of training diffusion policies for offline reinforcement learning (RL). At the core is a mean-reverting stochastic differential equation (SDE) that transfers a complex action distribution into a standard Gaussian and then samples actions conditioned on the environment state with a corresponding reverse-time SDE, like a typical diffusion policy. We show that such an SDE has a solution that we can use to calculate the log probability of the policy, yielding an entropy regularizer that improves the exploration of offline datasets. To mitigate the impact of inaccurate value functions from out-of-distribution data points, we further propose to learn the lower confidence bound of Q-ensembles for more robust policy improvement. By combining the entropy-regularized diffusion policy with Q-ensembles in offline RL, our method achieves state-of-the-art performance on most tasks in D4RL benchmarks. Code is available at https://github.com/ruoqizzz/Entropy-Regularized-Diffusion-Policy-with-QEnsemble.",
    "url": "https://www.semanticscholar.org/paper/79f923d6575bd8253e2f0b70813caa61a870ccee",
    "pdf_url": "https://arxiv.org/pdf/2402.04080.pdf",
    "venue": "Neural Information Processing Systems",
    "publicationDate": "2024-02-06",
    "externalIds": {
      "DBLP": "journals/corr/abs-2402-04080",
      "ArXiv": "2402.04080",
      "DOI": "10.48550/arXiv.2402.04080",
      "CorpusId": 267500191
    },
    "references": [
      {
        "paperId": "c95244ea3ec82de5adc96fd3b96c0f59f5d1b1b2",
        "title": "Diffusion Models for Reinforcement Learning: A Survey"
      },
      {
        "paperId": "2aae2dd79ef30022da59d8c33f0b3afa55d9c20d",
        "title": "Efficient Diffusion Policies for Offline Reinforcement Learning"
      },
      {
        "paperId": "a6f9fb141034a87ff9d627dc8a3ef31d0790c6ed",
        "title": "IDQL: Implicit Q-Learning as an Actor-Critic Method with Diffusion Policies"
      },
      {
        "paperId": "9b8b9c4f742998d3730cc13b30313a95fc0077fd",
        "title": "Refusion: Enabling Large-Size Realistic Image Restoration with Latent-Space Diffusion Models"
      },
      {
        "paperId": "e701e4c02a32da186d25b08373ada12d83b73b3d",
        "title": "Scaling Robot Learning with Semantically Imagined Experience"
      },
      {
        "paperId": "9a01fc428d195a9c5ea2005dc2943a650d59aa76",
        "title": "GenAug: Retargeting behaviors to unseen situations via Generative Augmentation"
      },
      {
        "paperId": "c9217c50acce84e417bbc13a4ecaa06db0c77026",
        "title": "Image Restoration with Mean-Reverting Stochastic Differential Equations"
      },
      {
        "paperId": "2cbea7615ebecea2c414d8fbad47d5d258a5c3b4",
        "title": "Diffusion Policies as an Expressive Policy Class for Offline Reinforcement Learning"
      },
      {
        "paperId": "a6f6fef883b0d21eb095caf80b5f61eadec873ef",
        "title": "Speech Enhancement and Dereverberation With Diffusion-Based Generative Models"
      },
      {
        "paperId": "5b353418cf914dccf85cfefcbdda892b600fdc6e",
        "title": "Why So Pessimistic? Estimating Uncertainties for Offline RL through Ensembles, and Why Their Independence Matters"
      },
      {
        "paperId": "9695824d7a01fad57ba9c01d7d76a519d78d65e7",
        "title": "Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding"
      },
      {
        "paperId": "3ebdd3db0dd91069fa0cd31cbf8308b60b1b565e",
        "title": "Planning with Diffusion for Flexible Behavior Synthesis"
      },
      {
        "paperId": "ebf62718b2e86dda0e0bfaaad5774663c66c6512",
        "title": "Speech Enhancement with Score-Based Generative Models in the Complex STFT Domain"
      },
      {
        "paperId": "23abe79046d7f7b430d5d21b6a93598d0aa1b9c2",
        "title": "Pessimistic Bootstrapping for Uncertainty-Driven Offline Reinforcement Learning"
      },
      {
        "paperId": "c10075b3746a9f3dd5811970e93c8ca3ad39b39d",
        "title": "High-Resolution Image Synthesis with Latent Diffusion Models"
      },
      {
        "paperId": "348a855fe01f3f4273bf0ecf851ca688686dbfcc",
        "title": "Offline Reinforcement Learning with Implicit Q-Learning"
      },
      {
        "paperId": "9560080a2c32682bd1c1a9850a54ca6163f1956e",
        "title": "Uncertainty-Based Offline Reinforcement Learning with Diversified Q-Ensemble"
      },
      {
        "paperId": "c879b25308026d6538e52b27bcf4fd3cb60855f3",
        "title": "A Minimalist Approach to Offline Reinforcement Learning"
      },
      {
        "paperId": "c1ad5f9b32d80f1c65d67894e5b8c2fdf0ae4500",
        "title": "Decision Transformer: Reinforcement Learning via Sequence Modeling"
      },
      {
        "paperId": "64ea8f180d0682e6c18d1eb688afdb2027c02794",
        "title": "Diffusion Models Beat GANs on Image Synthesis"
      },
      {
        "paperId": "de18baa4964804cf471d85a5a090498242d2e79f",
        "title": "Improved Denoising Diffusion Probabilistic Models"
      },
      {
        "paperId": "9f8a70e9188a913317e97ba1874c8f763fd13c04",
        "title": "Is Pessimism Provably Efficient for Offline RL?"
      },
      {
        "paperId": "633e2fbfc0b21e959a244100937c5853afca4853",
        "title": "Score-Based Generative Modeling through Stochastic Differential Equations"
      },
      {
        "paperId": "014576b866078524286802b1d0e18628520aa886",
        "title": "Denoising Diffusion Implicit Models"
      },
      {
        "paperId": "57ffba5ea5e602b5365752e19a6f85a716db1d8b",
        "title": "Bandit Algorithms"
      },
      {
        "paperId": "5c126ae3421f05768d8edd97ecd44b1364e2c99a",
        "title": "Denoising Diffusion Probabilistic Models"
      },
      {
        "paperId": "28db20a81eec74a50204686c3cf796c42a020d2e",
        "title": "Conservative Q-Learning for Offline Reinforcement Learning"
      },
      {
        "paperId": "dea0f1c5949f8d898b9b6ff68226a781558e413c",
        "title": "MOPO: Model-based Offline Policy Optimization"
      },
      {
        "paperId": "309c2c5ee60e725244da09180f913cd8d4b8d4e9",
        "title": "MOReL : Model-Based Offline Reinforcement Learning"
      },
      {
        "paperId": "5e7bc93622416f14e6948a500278bfbe58cd3890",
        "title": "Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems"
      },
      {
        "paperId": "a326d9f2d2d351001fece788165dbcbb524da2e4",
        "title": "D4RL: Datasets for Deep Data-Driven Reinforcement Learning"
      },
      {
        "paperId": "26d6386ca5a60509a9454b2def7c9163230405ad",
        "title": "Institutional Review Board"
      },
      {
        "paperId": "5d5e01193e36a77c836fe8ee3ebb13d57520d050",
        "title": "Learning to Combat Compounding-Error in Model-Based Reinforcement Learning"
      },
      {
        "paperId": "82b4b03a4659d6e04bd7cbf51d6e08fde1348dbd",
        "title": "Stabilizing Off-Policy Q-Learning via Bootstrapping Error Reduction"
      },
      {
        "paperId": "12c0751b4f51ed833172a713b7e32390032ead93",
        "title": "Soft Actor-Critic Algorithms and Applications"
      },
      {
        "paperId": "5285cb8faada5de8a92a47622950f6cfd476ac1d",
        "title": "Off-Policy Deep Reinforcement Learning without Exploration"
      },
      {
        "paperId": "d431aab3184c8e7e38e5a18a150c7db90fade77b",
        "title": "Diffusion Models"
      },
      {
        "paperId": "5b01eaef54a653ba03ddd5a978690380fbc19bfc",
        "title": "Diversity is All You Need: Learning Skills without a Reward Function"
      },
      {
        "paperId": "811df72e210e20de99719539505da54762a11c6d",
        "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "9172cd6c253edf7c3a1568e03577db20648ad0c4",
        "title": "Reinforcement Learning with Deep Energy-Based Policies"
      },
      {
        "paperId": "4b63e34276aa98d5345efa7fe09bb06d8a9d8f52",
        "title": "Deep Exploration via Bootstrapped DQN"
      },
      {
        "paperId": "69e76e16740ed69f4dc55361a3d319ac2f1293dd",
        "title": "Asynchronous Methods for Deep Reinforcement Learning"
      },
      {
        "paperId": "a25fbcbbae1e8f79c4360d26aa11a3abf1a11972",
        "title": "A Survey on Transfer Learning"
      },
      {
        "paperId": "57a23a111b40528af0cb1ba86f90d75e8d11a21d",
        "title": "Exact numerical simulation of the Ornstein-Uhlenbeck process and its integral."
      },
      {
        "paperId": "c7a5128b45edb4db9105ec5167210b887617ddf2",
        "title": "Reverse-time diffusion equation models"
      },
      {
        "paperId": "ef4f5a50837a7c1b3e87b9300ffc7ba00d461a0f",
        "title": "AUTO-ENCODING VARIATIONAL BAYES"
      },
      {
        "paperId": "8969d85db0abfc2e2739b752d38280745102eb26",
        "title": "Mish: A Self Regularized Non-Monotonic Activation Function"
      },
      {
        "paperId": null,
        "title": "A method for stochastic optimization"
      },
      {
        "paperId": "5c65d095600d6c647426fa3bc45031b208882d5f",
        "title": "Batch Reinforcement Learning"
      },
      {
        "paperId": "2a65434d43ffa6554eaf14b728780919ad4f33eb",
        "title": "Modeling Purposeful Adaptive Behavior with the Principle of Maximum Causal Entropy"
      },
      {
        "paperId": null,
        "title": "the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm"
      }
    ],
    "cited_by": [
      {
        "paperId": "9f33494db13e19be75387598d1d1e15285400b6d",
        "title": "Unleashing Flow Policies with Distributional Critics"
      },
      {
        "paperId": "037559ed8e640d69a1f65516a9fe3c03b4378461",
        "title": "Stabilizing Holistic Semantics in Diffusion Bridge for Image Inpainting"
      },
      {
        "paperId": "85ba08c7fbc43032704a25ee00f07363dacf27da",
        "title": "Steering Your Diffusion Policy with Latent Space Reinforcement Learning"
      },
      {
        "paperId": "f1102284c141fc24d70716452f59f697c1aeefa7",
        "title": "Diffusion Guidance Is a Controllable Policy Improvement Operator"
      },
      {
        "paperId": "cbbcc17e6c6bd3dc06fe55a9f4d4f2ee10fe6852",
        "title": "Scaling Offline RL via Efficient and Expressive Shortcut Models"
      },
      {
        "paperId": "d9174ab271e74c1f148ff5e692e7dc7612135aca",
        "title": "Learning Generalizable Robot Policy with Human Demonstration Video as a Prompt"
      },
      {
        "paperId": "681e64ff41861d1603952ceb9ce23774d7e7b1e1",
        "title": "Real-Time Diffusion Policies for Games: Enhancing Consistency Policies with Q-Ensembles"
      },
      {
        "paperId": "2bd000c4c49a0e7620f90e0734372d6408293038",
        "title": "Towards Better Sample Efficiency in Multi-Agent Reinforcement Learning via Exploration"
      },
      {
        "paperId": "aa37bbbdd95343c8f7bd61d6e13164d767eaa330",
        "title": "Ensemble RL through Classifier Models: Enhancing Risk-Return Trade-offs in Trading Strategies"
      },
      {
        "paperId": "6c2c31e9dbaff4697886d71b156b0d1a861264f5",
        "title": "Behavior-Regularized Diffusion Policy Optimization for Offline Reinforcement Learning"
      },
      {
        "paperId": "8dcc0a142aefec2d7addb91775e5d58ab4a6771f",
        "title": "Flow Q-Learning"
      },
      {
        "paperId": "8a5b2ac09ad6a5d3af0e1aad6bec222fa615a948",
        "title": "Entropy Regularized Task Representation Learning for Offline Meta-Reinforcement Learning"
      },
      {
        "paperId": "1ca137888bba74eaba2f99a89405e6e9bce5229d",
        "title": "The Exploration-Exploitation Dilemma Revisited: An Entropy Perspective"
      },
      {
        "paperId": "dc73bb8ef683c7e62adc02a5e57012be9116040a",
        "title": "Photo-Realistic Image Restoration in the Wild with Controlled Vision-Language Models"
      },
      {
        "paperId": "afe75806e0fa5da4f96dcc2e70976d7dfd60d4db",
        "title": "P ROVABLE C AUSAL S TATE R EPRESENTATION UNDER A SYNCHRONOUS D IFFUSION M ODEL FOR POMDP"
      },
      {
        "paperId": "931d4c47287ac8d70de4d6f3fb4cc491c82eba78",
        "title": "Real-Time Diffusion"
      }
    ],
    "score": 16.0
  },
  {
    "id": "ff87e30cb969d40b1d5c8ddc8932427793467f29",
    "title": "Balance Reward and Safety Optimization for Safe Reinforcement Learning: A Perspective of Gradient Manipulation",
    "authors": [
      "Shangding Gu",
      "Bilgehan Sel",
      "Yuhao Ding",
      "Lu Wang",
      "Qingwei Lin",
      "Ming Jin",
      "Alois Knoll"
    ],
    "year": 2024,
    "citationCount": 16,
    "abstract": "Ensuring the safety of Reinforcement Learning (RL) is crucial for its deployment in real-world applications. Nevertheless, managing the trade-off between reward and safety during exploration presents a significant challenge. Improving reward performance through policy adjustments may adversely affect safety performance. In this study, we aim to address this conflicting relation by leveraging the theory of gradient manipulation. Initially, we analyze the conflict between reward and safety gradients. Subsequently, we tackle the balance between reward and safety optimization by proposing a soft switching policy optimization method, for which we provide convergence analysis. Based on our theoretical examination, we provide a safe RL framework to overcome the aforementioned challenge, and we develop a Safety-MuJoCo Benchmark to assess the performance of safe RL algorithms. Finally, we evaluate the effectiveness of our method on the Safety-MuJoCo Benchmark and a popular safe benchmark, Omnisafe. Experimental results demonstrate that our algorithms outperform several state-of-the-art baselines in terms of balancing reward and safety optimization.",
    "url": "https://www.semanticscholar.org/paper/ff87e30cb969d40b1d5c8ddc8932427793467f29",
    "pdf_url": "https://arxiv.org/pdf/2405.01677.pdf",
    "venue": "AAAI Conference on Artificial Intelligence",
    "publicationDate": "2024-03-24",
    "externalIds": {
      "DBLP": "journals/corr/abs-2405-01677",
      "ArXiv": "2405.01677",
      "DOI": "10.1609/aaai.v38i19.30102",
      "CorpusId": 268712261
    },
    "references": [
      {
        "paperId": "1eb5f9b321e66f0c01184e675676e329c5d06349",
        "title": "Gradient-Adaptive Pareto Optimization for Constrained Reinforcement Learning"
      },
      {
        "paperId": "cb071e609fe6c82ccc906c878c535098b773e266",
        "title": "OmniSafe: An Infrastructure for Accelerating Safe Reinforcement Learning Research"
      },
      {
        "paperId": "263622e8036274973677888724a53a500e8a91d8",
        "title": "Safe multi-agent reinforcement learning for multi-robot control"
      },
      {
        "paperId": "f173a8652eeda3fa3ede5cdf5a982ae9c9eff0b7",
        "title": "A human-centered safe robot reinforcement learning framework with interactive behaviors"
      },
      {
        "paperId": "3ffa359555ab15e7e2c379daec73fbcf5bafa458",
        "title": "Constrained Update Projection Approach to Safe Policy Optimization"
      },
      {
        "paperId": "1981fdfc760b875512568e2417791ee66f1936b0",
        "title": "Constrained Reinforcement Learning for Vehicle Motion Planning with Topological Reachability Analysis"
      },
      {
        "paperId": "0c373d032f0833f3d1a773237f854170ea0c1b8d",
        "title": "Generalized Data Weighting via Class-level Gradient Manipulation"
      },
      {
        "paperId": "b7e531104c95eb67ce6c20d6dc1318e8bf837bf8",
        "title": "Conflict-Averse Gradient Descent for Multi-task Learning"
      },
      {
        "paperId": "700e73bf63837c555c40a6d67d919cb8154c52a0",
        "title": "CRPO: A New Approach for Safe Reinforcement Learning with Convergence Guarantee"
      },
      {
        "paperId": "eecc04e4751ef623ecd9f9e69e9601c9431152d2",
        "title": "Balancing Constraints and Rewards with Meta-Gradient D4PG"
      },
      {
        "paperId": "9ec0bd60df62b997bb371e65d254434f8851ea9b",
        "title": "Projection-Based Constrained Policy Optimization"
      },
      {
        "paperId": "129983331ca874142a3e8eb2d93d820bdf1f9aca",
        "title": "Deep Reinforcement Learning for Autonomous Driving: A Survey"
      },
      {
        "paperId": "449c5660d637741f7aa7ff42549c32b43c9968bf",
        "title": "Gradient Surgery for Multi-Task Learning"
      },
      {
        "paperId": "f7f8f05eb2798272fc3a61443d45f2aa47e65135",
        "title": "On the Theory of Policy Gradient Methods: Optimality, Approximation, and Distribution Shift"
      },
      {
        "paperId": "dc3a72a5b4b3b76805e851cd32fc207622062ad3",
        "title": "Temporal Logic Guided Safe Reinforcement Learning Using Control Barrier Functions"
      },
      {
        "paperId": "3fa50569925cfecc66fed5ec616682ecf3794ad7",
        "title": "Lyapunov-based Safe Policy Optimization for Continuous Control"
      },
      {
        "paperId": "65fb1b37c41902793ac65db3532a6e51631a9aff",
        "title": "A Lyapunov-based Approach to Safe Reinforcement Learning"
      },
      {
        "paperId": "8bf1aa5dcae7183c5bd634da7e39bef8909306d4",
        "title": "Safe Reinforcement Learning via Formal Methods: Toward Safe Control Through Proof and Learning"
      },
      {
        "paperId": "ede9f86e18d39993589963205d56de7a926c7196",
        "title": "Learning-Based Model Predictive Control for Safe Exploration"
      },
      {
        "paperId": "3ebb600a7a4279f1d1438f8aa8c60b90aa16ec8d",
        "title": "On the Fenchel Duality between Strong Convexity and Lipschitz Continuous Gradient"
      },
      {
        "paperId": "846aedd869a00c09b40f1f1f35673cb22bc87490",
        "title": "Mastering the game of Go with deep neural networks and tree search"
      },
      {
        "paperId": "da3e22c1f3cd7f7f68421f7295e56cfa0642baf4",
        "title": "Safe Exploration for Optimization with Gaussian Processes"
      },
      {
        "paperId": "449532187c94af3dd3aa55e16d2c50f7854d2199",
        "title": "Trust Region Policy Optimization"
      },
      {
        "paperId": "65438e0ba226c1f97bd8a36333ebc3297b1a32fd",
        "title": "Reinforcement learning in robotics: A survey"
      },
      {
        "paperId": "b354ee518bfc1ac0d8ac447eece9edb69e92eae1",
        "title": "MuJoCo: A physics engine for model-based control"
      },
      {
        "paperId": "0b14178e7d79ac426d0a39700e1ac8b2c6f2e752",
        "title": "Convex optimization"
      },
      {
        "paperId": "a20f0ce0616def7cc9a87446c228906cd5da093b",
        "title": "Policy Gradient Methods for Reinforcement Learning with Function Approximation"
      },
      {
        "paperId": "f27202f17d7399b5ac78b10ce72aa361baae4bf3",
        "title": "On the Convergence of Stochastic Multi-Objective Gradient Manipulation and Beyond"
      },
      {
        "paperId": "4d0f6a6ffcd6ab04732ff76420fd9f8a7bb649c3",
        "title": "Benchmarking Safe Exploration in Deep Reinforcement Learning"
      },
      {
        "paperId": "270fdc648c7c9047950c7cdb1c186da8373391ee",
        "title": "Combining Trust Region and Line Search Techniques"
      },
      {
        "paperId": null,
        "title": "2022. Safe learning in robotics: From learning-based control to safe reinforcement learning"
      },
      {
        "paperId": null,
        "title": "2022b. A review of safe reinforcement learning: Methods, theory and applications"
      }
    ],
    "cited_by": [
      {
        "paperId": "2ec7d7a5caa49c58443c0206b2c870b4dd9d5a6e",
        "title": "SPiDR: A Simple Approach for Zero-Shot Safety in Sim-to-Real Transfer"
      },
      {
        "paperId": "0b977b2f95e3b16ee102ab114057a0af6cdb6d25",
        "title": "Robust Observer Design for the Longitudinal Dynamics of a Fixed-Wing Aircraft"
      },
      {
        "paperId": "e92dccc8157803c69643839f90c113b40ac9449e",
        "title": "Rectified Robust Policy Optimization for Model-Uncertain Constrained Reinforcement Learning without Strong Duality"
      },
      {
        "paperId": "b8560e6ba7b58590e68f087f8fb09be4558f7bd1",
        "title": "Proactive Constrained Policy Optimization with Preemptive Penalty"
      },
      {
        "paperId": "459e6283bf432d4523dab9198c5ce351088ec406",
        "title": "CrossPlatformStack: Enabling High Availability and Safe Deployment for Products Across Meta Services"
      },
      {
        "paperId": "4dca655bab8294c4951ee4e6b39ce7eb07cbb887",
        "title": "Safe Continual Domain Adaptation after Sim2Real Transfer of Reinforcement Learning Policies in Robotics"
      },
      {
        "paperId": "0eb4656d4b4ccbb10ab3a4fadd3f97c71ac5dab6",
        "title": "Robust Gymnasium: A Unified Modular Benchmark for Robust Reinforcement Learning"
      },
      {
        "paperId": "af66187d02889719bd1dd410794413c6db9c2f3a",
        "title": "Don't Trade Off Safety: Diffusion Regularization for Constrained Offline RL"
      },
      {
        "paperId": "93b629ca05e8229adda7fe620344da6542a7f482",
        "title": "Learning-Enhanced Safeguard Control for High-Relative-Degree Systems: Robust Optimization under Disturbances and Faults"
      },
      {
        "paperId": "d5226e15bec4f42d1efa4d3bc7ba8c90000f5e95",
        "title": "Flipping-based Policy for Chance-Constrained Markov Decision Processes"
      },
      {
        "paperId": "d0ddb64bc11d08084f47ccc518b742bdae8b268f",
        "title": "Robust Position Estimation using Range Measurements from Transmitters with Inaccurate Positions"
      },
      {
        "paperId": "bd7e3ed91488d4cc32d2845565919f0fc1ef2537",
        "title": "Optimal Transport-Assisted Risk-Sensitive Q-Learning"
      },
      {
        "paperId": "1eb8cf490e8bc3170d9577653b0df3a93d723364",
        "title": "Enhancing Efficiency of Safe Reinforcement Learning via Sample Manipulation"
      },
      {
        "paperId": "d1f769b04c53e7197654559fa386259467d30ab5",
        "title": "Skin-in-the-Game: Decision Making via Multi-Stakeholder Alignment in LLMs"
      },
      {
        "paperId": "28c19931f26622911455aa94c005ff963cffc009",
        "title": "Preparing for Black Swans: The Antifragility Imperative for Machine Learning"
      },
      {
        "paperId": "fa3e5931807f0d124ad05d9113fa37a50aaca1e7",
        "title": "A Survey of Safe Reinforcement Learning Methods in Robotics"
      }
    ],
    "score": 16.0
  },
  {
    "id": "aedd5c4bc11d8faf01e8456c91a183f2f76e5778",
    "title": "ExploRLLM: Guiding Exploration in Reinforcement Learning with Large Language Models",
    "authors": [
      "Runyu Ma",
      "Jelle Luijkx",
      "Zlatan Ajanovi\u0107",
      "Jens Kober"
    ],
    "year": 2024,
    "citationCount": 15,
    "abstract": "In robot manipulation, Reinforcement Learning (RL) often suffers from low sample efficiency and uncertain convergence, especially in large observation and action spaces. Foundation Models (FMs) offer an alternative, demonstrating promise in zero-shot and few-shot settings. However, they can be unreliable due to limited physical and spatial understanding. We introduce ExploRLLM, a method that combines the strengths of both paradigms. In our approach, FMs improve RL convergence by generating policy code and efficient representations, while a residual RL agent compensates for the FMs' limited physical understanding. We show that Explorllm outperforms both policies derived from FMs and RL baselines in table-top manipulation tasks. Additionally, real-world experiments show that the policies exhibit promising zero-shot sim-to-real transfer. Supplementary material is available at https://explorllm.github.io.",
    "url": "https://www.semanticscholar.org/paper/aedd5c4bc11d8faf01e8456c91a183f2f76e5778",
    "pdf_url": "https://arxiv.org/pdf/2403.09583.pdf",
    "venue": "IEEE International Conference on Robotics and Automation",
    "publicationDate": "2024-03-14",
    "externalIds": {
      "DBLP": "journals/corr/abs-2403-09583",
      "ArXiv": "2403.09583",
      "DOI": "10.1109/ICRA55743.2025.11127622",
      "CorpusId": 268385512
    },
    "references": [
      {
        "paperId": "f74b8cff57c3591f63497ad21791b546803c89f1",
        "title": "Engine Agnostic Graph Environments for Robotics (EAGERx): A Graph-Based Framework for Sim2real Robot Learning"
      },
      {
        "paperId": "4cbf53465e2af3aa4f0079167a0474fc713f3ce0",
        "title": "RLingua: Improving Reinforcement Learning Sample Efficiency in Robotic Manipulations With Large Language Models"
      },
      {
        "paperId": "b156004675ad3aa5e39a56928afc530aec191044",
        "title": "LLMs Can't Plan, But Can Help Planning in LLM-Modulo Frameworks"
      },
      {
        "paperId": "6ca16c1c2c60ceda87242c8f8e522d12cc4a13bc",
        "title": "Eureka: Human-Level Reward Design via Coding Large Language Models"
      },
      {
        "paperId": "51b7614501e702c7f5fcb03a1b27fc26c1c2c199",
        "title": "Intrinsic Language-Guided Exploration for Complex Long-Horizon Robotic Manipulation Tasks"
      },
      {
        "paperId": "6b4da2023c3a11aea6e3ccb9ab13e594833c47eb",
        "title": "Self-Refined Large Language Model as Automated Reward Function Designer for Deep Reinforcement Learning in Robotics"
      },
      {
        "paperId": "38939304bb760473141c2aca0305e44fbe04e6e8",
        "title": "RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control"
      },
      {
        "paperId": "67188a50e1d8a601896f1217451b99f646af4ac8",
        "title": "Towards A Unified Agent with Foundation Models"
      },
      {
        "paperId": "1cd8373490efc2d74c2796f4b2aa27c7d4415ec9",
        "title": "VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models"
      },
      {
        "paperId": "94bcf0390d5acb1b92323bd15cc1dc311314122c",
        "title": "Language to Rewards for Robotic Skill Synthesis"
      },
      {
        "paperId": "163b4d6a79a5b19af88b8585456363340d9efd04",
        "title": "GPT-4 Technical Report"
      },
      {
        "paperId": "38fe8f324d2162e63a967a9ac6648974fc4c66f3",
        "title": "PaLM-E: An Embodied Multimodal Language Model"
      },
      {
        "paperId": "d318e0169f649656c71f02a1f84194a734fe1962",
        "title": "Reward Design with Language Models"
      },
      {
        "paperId": "89e184d2bc830af568e439db9476caa0c047e11a",
        "title": "Guiding Pretraining in Reinforcement Learning with Large Language Models"
      },
      {
        "paperId": "0b58f4ec8cbf6f63fb65b7e3c368cf511eadecd3",
        "title": "Grounding Large Language Models in Interactive Environments with Online Reinforcement Learning"
      },
      {
        "paperId": "a39d9570216f30dca2c37ffffd2b20ff4160849c",
        "title": "PARTNR: Pick and place Ambiguity Resolving by Trustworthy iNteractive leaRning"
      },
      {
        "paperId": "0ec485d265e6c98d6661bc8a9a71469152a09abe",
        "title": "Interactive Imitation Learning in Robotics: A Survey"
      },
      {
        "paperId": "c03fa01fbb9c77fe3d10609ba5f1dee33a723867",
        "title": "ProgPrompt: Generating Situated Robot Task Plans using Large Language Models"
      },
      {
        "paperId": "91deaf9d324c8feafc189da0da03e60a60287bca",
        "title": "Code as Policies: Language Model Programs for Embodied Control"
      },
      {
        "paperId": "f3cf71c51b882fe3111d71c4bf104297d38197f8",
        "title": "Inner Monologue: Embodied Reasoning through Planning with Language Models"
      },
      {
        "paperId": "e7ad08848d5d7c5c47673ffe0da06af443643bda",
        "title": "Large Language Models are Zero-Shot Reasoners"
      },
      {
        "paperId": "cb5e3f085caefd1f3d5e08637ab55d39e61234fc",
        "title": "Do As I Can, Not As I Say: Grounding Language in Robotic Affordances"
      },
      {
        "paperId": "ada81a4de88a6ce474df2e2446ad11fea480616e",
        "title": "Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language"
      },
      {
        "paperId": "92a8f7f09f3705cb5a6009a42220a6f01ea084e8",
        "title": "Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents"
      },
      {
        "paperId": "69ee9b3a915951cc84b74599a3a2699a66d4004f",
        "title": "CLIPort: What and Where Pathways for Robotic Manipulation"
      },
      {
        "paperId": "cf9b8da26d9b92e75ba49616ed2a1033f59fce14",
        "title": "Open-vocabulary Object Detection via Vision and Language Knowledge Distillation"
      },
      {
        "paperId": "6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4",
        "title": "Learning Transferable Visual Models From Natural Language Supervision"
      },
      {
        "paperId": "eb6b9bc4ff3e4e2cf1724324d79ce7de43131478",
        "title": "Transporter Networks: Rearranging the Visual World for Robotic Manipulation"
      },
      {
        "paperId": "0ee121834ddd93f93a8f5d7303b6a151270dfa03",
        "title": "Proximal Policy Optimization in StarCraft"
      },
      {
        "paperId": "12c0751b4f51ed833172a713b7e32390032ead93",
        "title": "Soft Actor-Critic Algorithms and Applications"
      },
      {
        "paperId": "ae4d32f05cf40e4cc01c69d7787149a258c95eda",
        "title": "Residual Reinforcement Learning for Robot Control"
      },
      {
        "paperId": "811df72e210e20de99719539505da54762a11c6d",
        "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "65438e0ba226c1f97bd8a36333ebc3297b1a32fd",
        "title": "Reinforcement learning in robotics: A survey"
      },
      {
        "paperId": "6aeeedf23ba3a903325dc175a020bd7776194ca2",
        "title": "Position: LLMs Can't Plan, But Can Help Planning in LLM-Modulo Frameworks"
      },
      {
        "paperId": "e3fc5b5627af62ee6981a02090cf6bae368202d7",
        "title": "Stable-Baselines3: Reliable Reinforcement Learning Implementations"
      },
      {
        "paperId": "97efafdb4a3942ab3efba53ded7413199f79c054",
        "title": "Reinforcement Learning: An Introduction"
      },
      {
        "paperId": null,
        "title": "We introduce a prompting method for LLM-based exploration using hierarchical language-model programs, leading to faster convergence"
      },
      {
        "paperId": null,
        "title": "Cluster ("
      },
      {
        "paperId": null,
        "title": "We propose ExploRLLM, which employs an RL agent with a) residual action and observation spaces derived from affordances identified by FMs and b) uses an LLM to guide exploration"
      }
    ],
    "cited_by": [
      {
        "paperId": "543f87b15b888cb0a2381c5b36141256ad5ec84e",
        "title": "LLM-Guided Task- and Affordance-Level Exploration in Reinforcement Learning"
      },
      {
        "paperId": "2fa350b566feb088de0b97057986ecb56ec0f360",
        "title": "$Agent^2$: An Agent-Generates-Agent Framework for Reinforcement Learning Automation"
      },
      {
        "paperId": "00e57938cf04d7fd36752bbfbeb738a41f556f41",
        "title": "Curriculum-Based Multi-Tier Semantic Exploration via Deep Reinforcement Learning"
      },
      {
        "paperId": "2138d3bb41cc0bde43f2a0994cc61b271df61944",
        "title": "Accelerating Reinforcement Learning Algorithms Convergence using Pre-trained Large Language Models as Tutors With Advice Reusing"
      },
      {
        "paperId": "00a9a61a7a2b07241110a368014783e4fc5264b8",
        "title": "Knowledge Integration for Physics-informed Symbolic Regression Using Pre-trained Large Language Models"
      },
      {
        "paperId": "25caf432faf629085f4db9c9614086a57ddd233b",
        "title": "Training-free Generation of Temporally Consistent Rewards from VLMs"
      },
      {
        "paperId": "290790ee57b4065326a4d31887f6bc164bc7d316",
        "title": "Intelligent Robot Control and Uncertainty Analysis Integrating Reinforcement Learning and Large Language Models"
      },
      {
        "paperId": "ada560d9a6d0eec4ead89f9282b19b91e2b012e0",
        "title": "APEX: Empowering LLMs with Physics-Based Task Planning for Real-time Insight"
      },
      {
        "paperId": "984adb3206db3dadf68bd13c4c93b7f049e28bff",
        "title": "LLM-based Interactive Imitation Learning for Robotic Manipulation"
      },
      {
        "paperId": "b49cbccd88cacf9a7514c80d33cf5c42b80edc1a",
        "title": "Towards Autonomous Reinforcement Learning for Real-World Robotic Manipulation With Large Language Models"
      },
      {
        "paperId": "f44852b83bf680c44e5705b0a98bbe16df99d391",
        "title": "Towards a Reward-Free Reinforcement Learning Framework for Vehicle Control"
      },
      {
        "paperId": "d1d0e5ff34fa5daf3de70e24dfbfdfa919cb49c5",
        "title": "An Overview and Discussion on Using Large Language Models for Implementation Generation of Solutions to Open-Ended Problems"
      },
      {
        "paperId": "84c9be49064d0df6bf601734225ca1da68ec531a",
        "title": "Task-Oriented Hierarchical Object Decomposition for Visuomotor Control"
      },
      {
        "paperId": "369bd8479daa33df68567e93e00a52155320ed88",
        "title": "Zero-shot Model-based Reinforcement Learning using Large Language Models"
      },
      {
        "paperId": "1ed6f9c35bc23965d5451ba39f197fefbe20710f",
        "title": "Language-Model-Assisted Bi-Level Programming for Reward Learning from Internet Videos"
      }
    ],
    "score": 15.0
  },
  {
    "id": "e0bf76edd8e6b793b81c16a915ede48e377ebab6",
    "title": "Efficient Reinforcement Learning with Large Language Model Priors",
    "authors": [
      "Xue Yan",
      "Yan Song",
      "Xidong Feng",
      "Mengyue Yang",
      "Haifeng Zhang",
      "H. Ammar",
      "Jun Wang"
    ],
    "year": 2024,
    "citationCount": 15,
    "abstract": "In sequential decision-making (SDM) tasks, methods like reinforcement learning (RL) and heuristic search have made notable advances in specific cases. However, they often require extensive exploration and face challenges in generalizing across diverse environments due to their limited grasp of the underlying decision dynamics. In contrast, large language models (LLMs) have recently emerged as powerful general-purpose tools, due to their capacity to maintain vast amounts of domain-specific knowledge. To harness this rich prior knowledge for efficiently solving complex SDM tasks, we propose treating LLMs as prior action distributions and integrating them into RL frameworks through Bayesian inference methods, making use of variational inference and direct posterior sampling. The proposed approaches facilitate the seamless incorporation of fixed LLM priors into both policy-based and value-based RL frameworks. Our experiments show that incorporating LLM-based action priors significantly reduces exploration and optimization complexity, substantially improving sample efficiency compared to traditional RL techniques, e.g., using LLM priors decreases the number of required samples by over 90% in offline learning scenarios.",
    "url": "https://www.semanticscholar.org/paper/e0bf76edd8e6b793b81c16a915ede48e377ebab6",
    "pdf_url": "https://arxiv.org/pdf/2410.07927.pdf",
    "venue": "arXiv.org",
    "publicationDate": "2024-10-10",
    "externalIds": {
      "DBLP": "journals/corr/abs-2410-07927",
      "ArXiv": "2410.07927",
      "DOI": "10.48550/arXiv.2410.07927",
      "CorpusId": 273233492
    },
    "references": [
      {
        "paperId": "3707939a856655fcabf0acd5cba1a1009987b439",
        "title": "Generative Verifiers: Reward Modeling as Next-Token Prediction"
      },
      {
        "paperId": "8292083dd8f6ae898ea0ee54a6b97997d1a51c9d",
        "title": "Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters"
      },
      {
        "paperId": "c19769ffab690f9ba42abbf7e680453bd484cc1a",
        "title": "Exchangeable Sequence Models Quantify Uncertainty Over Latent Concepts"
      },
      {
        "paperId": "f66b6049946a10080d1f2a7ee6a40e5cca3ee6a0",
        "title": "Large Language Monkeys: Scaling Inference Compute with Repeated Sampling"
      },
      {
        "paperId": "8deb5fd40e310dc4feb27f7db7019e734b44631b",
        "title": "LLM Critics Help Catch LLM Bugs"
      },
      {
        "paperId": "7934c3e46773e3e7c7e5bcc9fe6cb74a0975e823",
        "title": "Stochastic Q-learning for Large Discrete Action Spaces"
      },
      {
        "paperId": "0317c55e77147a2f3bba943c4afddee5f5daa19b",
        "title": "Probabilistic Inference in Language Models via Twisted Sequential Monte Carlo"
      },
      {
        "paperId": "b42713664a72410307839fe44ec51aef8d69c943",
        "title": "How Can LLM Guide RL? A Value-Based Approach"
      },
      {
        "paperId": "b392122a48d8b3212ee17074ff65f6b9df5c36c7",
        "title": "Q-Probe: A Lightweight Approach to Reward Maximization for Language Models"
      },
      {
        "paperId": "d5c5cd269dbba04f7ec67192061c3ae22ebde72e",
        "title": "Reflect-RL: Two-Player Online RL Fine-Tuning for LMs"
      },
      {
        "paperId": "9f22f66cacac8eebc5728cd0a28513b5e75a9a58",
        "title": "Large Language Models Play StarCraft II: Benchmarks and A Chain of Summarization Approach"
      },
      {
        "paperId": "4ba57555bef02f988f2ed3bab2f102733dc55221",
        "title": "Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations"
      },
      {
        "paperId": "f2e1c7a7456a6196889950a58b6b8e46e6b94f80",
        "title": "Distilled Self-Critique of LLMs with Synthetic Data: a Bayesian Perspective"
      },
      {
        "paperId": "5195e20de717b2e4af595e275be45499b18c1d51",
        "title": "Ask more, know better: Reinforce-Learned Prompt Questions for Decision Making with Large Language Models"
      },
      {
        "paperId": "1effda8ce21573ed864eadfdc7b233aac39b8fe6",
        "title": "Amortizing intractable inference in large language models"
      },
      {
        "paperId": "e8df1cf6742b50a15500b8dd3dde3942e9c91418",
        "title": "Alphazero-like Tree-Search can Guide Large Language Model Decoding and Training"
      },
      {
        "paperId": "5fc1a3a49e8f1d106118b69d1d6be3b6caa23da0",
        "title": "Qwen Technical Report"
      },
      {
        "paperId": "9fbde8d0315f52e598c5b4a1409ed2aee215ace9",
        "title": "ProAgent: Building Proactive Cooperative Agents with Large Language Models"
      },
      {
        "paperId": "5e4597eb21a393b23e473cf66cb5ae8b27cab03e",
        "title": "ExpeL: LLM Agents Are Experiential Learners"
      },
      {
        "paperId": "be8db99310602d66bba64bcf41a572c45816fbfc",
        "title": "Let's Verify Step by Step"
      },
      {
        "paperId": "0d1c76d45afa012ded7ab741194baf142117c495",
        "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
      },
      {
        "paperId": "f197bf0fc2f228483f6af3285000d54d8d97f9eb",
        "title": "Voyager: An Open-Ended Embodied Agent with Large Language Models"
      },
      {
        "paperId": "5dbffedcabe3fa43060ebbe2b1789500edfd871f",
        "title": "Reasoning with Language Model is Planning with World Model"
      },
      {
        "paperId": "2f3822eb380b5e753a6d579f31dfc3ec4c4a0820",
        "title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models"
      },
      {
        "paperId": "70ece7b4ba8f3b67f5a797daed544fb6a0b627bf",
        "title": "A Latent Space Theory for Emergent Abilities in Large Language Models"
      },
      {
        "paperId": "5278a8eb2ba2429d4029745caf4e661080073c81",
        "title": "Generative Agents: Interactive Simulacra of Human Behavior"
      },
      {
        "paperId": "b626560f19f815808a289ef5c24a17c57320da70",
        "title": "MathPrompter: Mathematical Reasoning using Large Language Models"
      },
      {
        "paperId": "0b58f4ec8cbf6f63fb65b7e3c368cf511eadecd3",
        "title": "Grounding Large Language Models in Interactive Environments with Online Reinforcement Learning"
      },
      {
        "paperId": "29bd550d0ab53296790ceba31dfe0a06754bcdde",
        "title": "Large Language Models Are Latent Variable Models: Explaining and Finding Good Demonstrations for In-Context Learning"
      },
      {
        "paperId": "bd2ff852e86d16df09376f2dfdc934c533bb04a2",
        "title": "Residual Skill Policies: Learning an Adaptable Skill-based Action Space for Reinforcement Learning for Robotics"
      },
      {
        "paperId": "c4fff3bdbbdcff01545f1f53ec6290b3556c41ac",
        "title": "Large Language Models can Implement Policy Iteration"
      },
      {
        "paperId": "db36139c646ee6cf0a7a652488cf1df725ad5dca",
        "title": "Quantum decision making in automatic driving"
      },
      {
        "paperId": "a5cea6716378949a2b73f0401237d29791a6ee6c",
        "title": "Offline RL for Natural Language Generation with Implicit Language Q Learning"
      },
      {
        "paperId": "cf18a9f5a334e574f1d1f6ffdd64b6dac11fe9be",
        "title": "RL with KL penalties is better viewed as Bayesian inference"
      },
      {
        "paperId": "5f19ae1135a9500940978104ec15a5b8751bc7d2",
        "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models"
      },
      {
        "paperId": "68f141724814839d556a989646194be88641b143",
        "title": "Scaling Language Models: Methods, Analysis & Insights from Training Gopher"
      },
      {
        "paperId": "d6e783bce3b8e3ad082c2757235c34cb86c4e653",
        "title": "Safe Learning in Robotics: From Learning-Based Control to Safe Reinforcement Learning"
      },
      {
        "paperId": "b6c4a96e09b9f11e7c70e7f1fbe3f3971b92762d",
        "title": "FUDGE: Controlled Text Generation With Future Discriminators"
      },
      {
        "paperId": "c7fa68b000db6bca4ba10ec42565497f7df91f02",
        "title": "Monte Carlo Tree Search: a review of recent modifications and applications"
      },
      {
        "paperId": "398a0625e8707a0b41ac58eaec51e8feb87dd7cb",
        "title": "ALFWorld: Aligning Text and Embodied Environments for Interactive Learning"
      },
      {
        "paperId": "28db20a81eec74a50204686c3cf796c42a020d2e",
        "title": "Conservative Q-Learning for Offline Reinforcement Learning"
      },
      {
        "paperId": "43f2ad297941db230c089ba353efc3f281ab678c",
        "title": "5\u5206\u3067\u5206\u304b\u308b!? \u6709\u540d\u8ad6\u6587\u30ca\u30ca\u30e1\u8aad\u307f\uff1aJacob Devlin et al. : BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding"
      },
      {
        "paperId": "361c00b22e29d0816ca896513d2c165e26399821",
        "title": "Grandmaster level in StarCraft II using multi-agent reinforcement learning"
      },
      {
        "paperId": "0dd71add741dd207968f58f6e5160f24705ed556",
        "title": "Dialogue Generation: From Imitation Learning to Inverse Reinforcement Learning"
      },
      {
        "paperId": "ef2bc452812d6005ab0a66af6c3f97b6b0ba837e",
        "title": "Quantifying Generalization in Reinforcement Learning"
      },
      {
        "paperId": "89daae27e7df4a418b9610d307ce3df0e30fc8a2",
        "title": "TextWorld: A Learning Environment for Text-based Games"
      },
      {
        "paperId": "6ecc4b1ab05f3ec12484a0ea36abfd6271c5c5ba",
        "title": "Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review"
      },
      {
        "paperId": "811df72e210e20de99719539505da54762a11c6d",
        "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "059a1297a61afc812de48edede631229608dc513",
        "title": "Survey of Model-Based Reinforcement Learning: Applications on Robotics"
      },
      {
        "paperId": "1158ca943de5bf7f9eae3f0dd2a341beeeebcc23",
        "title": "AlphaGo, Deep Learning, and the Future of the Human Microscopist."
      },
      {
        "paperId": "846aedd869a00c09b40f1f1f35673cb22bc87490",
        "title": "Mastering the game of Go with deep neural networks and tree search"
      },
      {
        "paperId": "3b2aff88ee03e82993c066c3e698d51da62d5496",
        "title": "Deep Reinforcement Learning in Large Discrete Action Spaces"
      },
      {
        "paperId": "2319a491378867c7049b3da055c5df60e1671158",
        "title": "Playing Atari with Deep Reinforcement Learning"
      },
      {
        "paperId": "c5a301d332608db368e2cc306b55ca3e47615c31",
        "title": "Power-steering control architecture for automatic driving"
      },
      {
        "paperId": "8c11def05d9701db9a78589e2289e12f8262b928",
        "title": "Model predictive control: Theory and practice - A survey"
      },
      {
        "paperId": "965406129f7ce1dc5acd1374493e7d091a17cc6e",
        "title": "True Knowledge Comes from Practice: Aligning Large Language Models with Embodied Environments via Reinforcement Learning"
      },
      {
        "paperId": "8f7f1431cc26f6332e6c7bc02c622008b78ee2c8",
        "title": "Entropy-Regularized Token-Level Policy Optimization for Large Language Models"
      },
      {
        "paperId": "f4ab7d2e91f6533989d1a3e65b18319985ad3750",
        "title": "An Efficient End-to-End Training Approach for Zero-Shot Human-AI Coordination"
      },
      {
        "paperId": null,
        "title": "Increasing brain-llm alignment via information-theoretic compression"
      }
    ],
    "cited_by": [
      {
        "paperId": "61e1a5bb6dd6bcc48e4fc8fe051f935465d32610",
        "title": "Memory-Driven Self-Improvement for Decision Making with Large Language Models"
      },
      {
        "paperId": "11f54df10e66e61957c0109f8c81d2d90cba3da5",
        "title": "Guide: Generalized-Prior and Data Encoders for DAG Estimation"
      },
      {
        "paperId": "3b3e3d991af437f7c9412548223a87e15472b1a7",
        "title": "Goal-Guided Efficient Exploration via Large Language Model in Reinforcement Learning"
      },
      {
        "paperId": "63505f132df7ef525b898b5ec2d11d814258737b",
        "title": "LLM-Driven Policy Diffusion: Enhancing Generalization in Offline Reinforcement Learning"
      },
      {
        "paperId": "36baccf77336559bdc37853566f1e4121c66f9e8",
        "title": "Memento: Fine-tuning LLM Agents without Fine-tuning LLMs"
      },
      {
        "paperId": "3b23264893f410d84d07f31a564ecda516f184ff",
        "title": "Large Language Model Enhanced Multi-UAV Direct Cross-boundary Maritime Data Collection Scheme"
      },
      {
        "paperId": "7636c3209e4fa1bc500351501ffbc82c2fc86333",
        "title": "SoftPipe: A Soft-Guided Reinforcement Learning Framework for Automated Data Preparation"
      },
      {
        "paperId": "e1938a3fbd505d9666ed252b191aaf96ffd5f0d7",
        "title": "Multi-Agent Collaboration via Evolving Orchestration"
      },
      {
        "paperId": "d686f3c2284ba5fbbe0c7ef7579b97b4975ea0db",
        "title": "DialogXpert: Driving Intelligent and Emotion-Aware Conversations through Online Value-Based Reinforcement Learning with LLM Priors"
      },
      {
        "paperId": "210c44d337547f3d9e1a8baf6b5ca4c6a21f4dd4",
        "title": "HCRMP: A LLM-Hinted Contextual Reinforcement Learning Framework for Autonomous Driving"
      },
      {
        "paperId": "33ef686c36a1d1d994de170005020f8a2d10c3f8",
        "title": "Cache-Efficient Posterior Sampling for Reinforcement Learning with LLM-Derived Priors Across Discrete and Continuous Domains"
      },
      {
        "paperId": "fd2efe2ef4ddd5b62d4103450c8596a1206edda2",
        "title": "Toward Efficient Exploration by Large Language Model Agents"
      },
      {
        "paperId": "ee5272d4fcf6dfec4f8b984bee7f8c9eba62e745",
        "title": "Optimizing Case-Based Reasoning System for Functional Test Script Generation with Large Language Models"
      },
      {
        "paperId": "1025b6b6c0db1dbd07a13afeb9d10a5b949da2aa",
        "title": "Average-Reward Soft Actor-Critic"
      },
      {
        "paperId": "bd7237ad2db9786beffbe59bdfb06432da2adc1d",
        "title": "Moral Alignment for LLM Agents"
      }
    ],
    "score": 15.0
  },
  {
    "id": "5f5e9ec8bcc5e83eefecc0565bdc004f929f4723",
    "title": "Self-Supervised Neuron Segmentation with Multi-Agent Reinforcement Learning",
    "authors": [
      "Yinda Chen",
      "Wei Huang",
      "Shenglong Zhou",
      "Qi Chen",
      "Zhiwei Xiong"
    ],
    "year": 2023,
    "citationCount": 28,
    "abstract": "The performance of existing supervised neuron segmentation methods is highly dependent on the number of accurate annotations, especially when applied to large scale electron microscopy (EM) data. By extracting semantic information from unlabeled data, self-supervised methods can improve the performance of downstream tasks, among which the mask image model (MIM) has been widely used due to its simplicity and effectiveness in recovering original information from masked images. However, due to the high degree of structural locality in EM images, as well as the existence of considerable noise, many voxels contain little discriminative information, making MIM pretraining inefficient on the neuron segmentation task. To overcome this challenge, we propose a decision-based MIM that utilizes reinforcement learning (RL) to automatically search for optimal image masking ratio and masking strategy. Due to the vast exploration space, using single-agent RL for voxel prediction is impractical. Therefore, we treat each input patch as an agent with a shared behavior policy, allowing for multi-agent collaboration. Furthermore, this multi-agent model can capture dependencies between voxels, which is beneficial for the downstream segmentation task. Experiments conducted on representative EM datasets demonstrate that our approach has a significant advantage over alternative self-supervised methods on the task of neuron segmentation. Code is available at https://github.com/ydchen0806/dbMiM.",
    "url": "https://www.semanticscholar.org/paper/5f5e9ec8bcc5e83eefecc0565bdc004f929f4723",
    "pdf_url": "https://arxiv.org/pdf/2310.04148.pdf",
    "venue": "International Joint Conference on Artificial Intelligence",
    "publicationDate": "2023-08-01",
    "externalIds": {
      "ArXiv": "2310.04148",
      "DBLP": "journals/corr/abs-2310-04148",
      "DOI": "10.24963/ijcai.2023/68",
      "CorpusId": 260862422
    },
    "references": [
      {
        "paperId": "f1aa892eb6c00bf83c94f86abe7a59f0e3f1549e",
        "title": "Learning Multiscale Consistency for Self-Supervised Electron Microscopy Instance Segmentation"
      },
      {
        "paperId": "9b11f5e8b40b109cb774e29e5cf5a5baa8beeed8",
        "title": "Generative Text-Guided 3D Vision-Language Pretraining for Unified Medical Image Segmentation"
      },
      {
        "paperId": "a135632a05cc1f3311859fdebcd1350b4e9e1ee7",
        "title": "AdaMAE: Adaptive Masking for Efficient Spatiotemporal Learning with Masked Autoencoders"
      },
      {
        "paperId": "ff0ea71fa0e4804ca835fa4ffbf4a520684d3292",
        "title": "A Unified Deep Learning Framework for ssTEM Image Restoration"
      },
      {
        "paperId": "0dd0cc85e6585ef2c9c66aa63ecb0a49bad94980",
        "title": "Biological Instance Segmentation with a Superpixel-Guided Graph"
      },
      {
        "paperId": "eb8348f95373bb6c391ef55417fb32b348b892c1",
        "title": "Learning to Model Pixel-Embedded Affinity for Homogeneous Instance Segmentation"
      },
      {
        "paperId": "defb94d732c2d366953f38e715cba046a728a986",
        "title": "Semi-Supervised Neuron Segmentation via Reinforced Consistency Learning"
      },
      {
        "paperId": "1457c1ea57ce45ff79d59b689efe376c42540dda",
        "title": "Masked Autoencoders As Spatiotemporal Learners"
      },
      {
        "paperId": "b1fc7d96d732f99510658c73a8d9da3fd7b25923",
        "title": "MultiMAE: Multi-modal Multi-task Masked Autoencoders"
      },
      {
        "paperId": "4990f7542f0600e0501a7e7a931b32eb7cb804d5",
        "title": "VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training"
      },
      {
        "paperId": "008a428e049003fe768068a0f1fa1416af5c4982",
        "title": "Masked Feature Prediction for Self-Supervised Visual Pre-Training"
      },
      {
        "paperId": "44f4975b026c7c9ab934ead385afae080b20d66b",
        "title": "SelectAugment: Hierarchical Deterministic Sample Selection for Data Augmentation"
      },
      {
        "paperId": "076a8e778f2e9efb3c2fd45fed534ae9e6035f1b",
        "title": "Self-Supervised Pre-Training of Swin Transformers for 3D Medical Image Analysis"
      },
      {
        "paperId": "6351ebb4a3287f5f3e1273464b3b91e5df5a16d7",
        "title": "Masked Autoencoders Are Scalable Vision Learners"
      },
      {
        "paperId": "6ea88feb2e6d47dfef9f00bfbff4f11c82d38fc1",
        "title": "SimCVD: Simple Contrastive Voxel-Wise Representation Distillation for Semi-Supervised Medical Image Segmentation"
      },
      {
        "paperId": "ad4a0938c48e61b7827869e4ac3baffd0aefab35",
        "title": "Emerging Properties in Self-Supervised Vision Transformers"
      },
      {
        "paperId": "e2738deb16c4abfbcccb22de22ab849549e5e621",
        "title": "Local Patch AutoAugment With Multi-Agent Collaboration"
      },
      {
        "paperId": "7519a1e9e7371df79bd8a21cee871feb0ec597a5",
        "title": "UNETR: Transformers for 3D Medical Image Segmentation"
      },
      {
        "paperId": "7f58f9a582b719a80499f29562090091bf230ac9",
        "title": "Local shape descriptors for neuron segmentation"
      },
      {
        "paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a",
        "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
      },
      {
        "paperId": "38f93092ece8eee9771e61c1edaf11b1293cae1b",
        "title": "Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning"
      },
      {
        "paperId": "7af72a461ed7cda180e7eab878efd5f35d79bbf4",
        "title": "A Simple Framework for Contrastive Learning of Visual Representations"
      },
      {
        "paperId": "7d668ed2d5b383c7ff9641974b2bc0c84f43e251",
        "title": "Adversarial AutoAugment"
      },
      {
        "paperId": "0d12da3dbccafb67bb9cdf3be1dd90c113983cee",
        "title": "Iteratively-Refined Interactive 3D Medical Image Segmentation With Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "ff6f3f36693002328c7e1627b31152698c0c999d",
        "title": "Fast and Accurate Electron Microscopy Image Registration with 3D Convolution"
      },
      {
        "paperId": "37680090f534ae1dfdc4e7312cf6de8d7b22177a",
        "title": "Large Scale Image Segmentation with Structured Loss Based Deep Learning for Connectome Reconstruction"
      },
      {
        "paperId": "2e71d40d5a53b7f71eb12a3f5315be4e6a8a6900",
        "title": "Neuron Segmentation With High-Level Biological Priors"
      },
      {
        "paperId": "b907537c602b95948da809f7d4aff4bc959d8ba1",
        "title": "Superhuman Accuracy on the SNEMI3D Connectomics Challenge"
      },
      {
        "paperId": "229cf8ddb9b5dbdf3b8bb60c9fbcf83759347810",
        "title": "A Complete Electron Microscopy Volume of the Brain of Adult Drosophila melanogaster"
      },
      {
        "paperId": "8d1f2bbaba29eb12a534385afa4f90e0956a12dd",
        "title": "Multicut brings automated neurite segmentation closer to human performance"
      },
      {
        "paperId": "0772905d40b9afa3dc087a88184f09f3b3e1464f",
        "title": "Learning to Communicate with Deep Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "69e76e16740ed69f4dc55361a3d319ac2f1293dd",
        "title": "Asynchronous Methods for Deep Reinforcement Learning"
      },
      {
        "paperId": "e07e61de05d30a348cb7b4cb38aede541f80941c",
        "title": "Crowdsourcing the creation of image segmentation algorithms for connectomics"
      },
      {
        "paperId": "37288fe76a864233214b6a5e4cee29923e9c36b9",
        "title": "Saturated Reconstruction of a Volume of Neocortex"
      },
      {
        "paperId": "6364fdaa0a0eccd823a779fcdd489173f938e91a",
        "title": "U-Net: Convolutional Networks for Biomedical Image Segmentation"
      },
      {
        "paperId": "ef4f4cadea21b4d6d7a4c82c251d879db34d35a5",
        "title": "Machine Learning of Hierarchical Clustering to Segment 2D and 3D Images"
      },
      {
        "paperId": "feacb4cf21eaf068197f80b164827db888ddd28d",
        "title": "Convolutional Networks Can Learn to Generate Affinity Graphs for Image Segmentation"
      },
      {
        "paperId": "07ef1b52b9fbb25cec28f65d030e44237c8f3f5b",
        "title": "Maximin affinity learning of image segmentation"
      },
      {
        "paperId": "bc6dff14a130c57a91d5a21339c23471faf1d46f",
        "title": "Et al"
      },
      {
        "paperId": "e8b12467bdc20bde976750b8a28decdb33246d1d",
        "title": "Histograms of oriented gradients for human detection"
      },
      {
        "paperId": "7fbf55baccbc5fdc7ded1ba18330605909aef5e5",
        "title": "Markov Games as a Framework for Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "79822b4b1b172ad56c24002e8d3610c5b2a1f84f",
        "title": "Local"
      },
      {
        "paperId": "5a7e67f42c36f22096a4d294fe32045b3df140f0",
        "title": "Unified Deep Learning"
      },
      {
        "paperId": "f203f52c0f4151b89f0a1797c6be750b64af0a02",
        "title": "Self Pre-training with Masked Autoencoders for Medical Image Analysis"
      },
      {
        "paperId": "4d90f6a0c79f62bf9a607766c9c0e7a8d36215b9",
        "title": "MCMAE: Masked Convolution Meets Masked Autoencoders"
      },
      {
        "paperId": null,
        "title": "IEEE Transactions on Medical Imaging"
      },
      {
        "paperId": null,
        "title": "Artif"
      },
      {
        "paperId": null,
        "title": "pages 1597\u2013 1607"
      },
      {
        "paperId": null,
        "title": "Thorsten Beier"
      },
      {
        "paperId": null,
        "title": "Raimund"
      },
      {
        "paperId": "1b4cf91d2136f3e492c86caa7228120ed57da2e3",
        "title": "Nature Methods"
      },
      {
        "paperId": null,
        "title": "In 2005 IEEE computer society conference on computer vision and pattern recognition (CVPR\u201905)"
      },
      {
        "paperId": "4d98ce60f4f8ed822503b8d13b0605f8c5d74ca7",
        "title": "In Advances in Neural Information Processing Systems"
      },
      {
        "paperId": null,
        "title": "In Machine learning proceedings 1994"
      }
    ],
    "cited_by": [
      {
        "paperId": "a45a150681860758e088d9e1e7e7ceea18f29817",
        "title": "Dual form Complementary Masking for Domain-Adaptive Image Segmentation"
      },
      {
        "paperId": "bfef7b3170f085fefa01e280a3413798793a1004",
        "title": "DVP-MVS++: Synergize Depth-Normal-Edge and Harmonized Visibility Prior for Multi-View Stereo"
      },
      {
        "paperId": "7804121d64a9ded601e675f8db51ce5f3660d308",
        "title": "SED-MVS: Segmentation-Driven and Edge-Aligned Deformation Multi-View Stereo with Depth Restoration and Occlusion Constraint"
      },
      {
        "paperId": "96744f5f09798405bb0c26195459d31b8c7d92ab",
        "title": "Medical image segmentation with an emphasis on prior convolution and channel multi-branch attention"
      },
      {
        "paperId": "e6051794e30f2bec0a25b0da44b7355ff5bc2869",
        "title": "Mask autoencoder for enhanced image reconstruction with position coding offset and combined masking"
      },
      {
        "paperId": "eac2c2e79605378d015c681e031e644ef4eb8a72",
        "title": "A novel scene coupling semantic mask network for remote sensing image segmentation"
      },
      {
        "paperId": "99790717b91668485751f20c0b3fa086c6c2ff6a",
        "title": "Mask Factory: Towards High-quality Synthetic Data Generation for Dichotomous Image Segmentation"
      },
      {
        "paperId": "1c46bc0b36a874d6c57b58b741bec5b05a3c6616",
        "title": "Class Probability Space Regularization for semi-supervised semantic segmentation"
      },
      {
        "paperId": "2825883144aae681c2126d1415d5c360b48130d8",
        "title": "A Vehicle Classification Method Based on Machine Learning"
      },
      {
        "paperId": "4a606468ad3f9723edb4f4ed4c914c1a98e82c00",
        "title": "Efficient In-Context Medical Segmentation with Meta-driven Visual Prompt Selection"
      },
      {
        "paperId": "e6f96f4a535e16d3ad2adb3ebd083a4a6c94b441",
        "title": "Advancing precise diagnosis of nasopharyngeal carcinoma through endoscopy-based radiomics analysis"
      },
      {
        "paperId": "4383242be5bdfb30ffa84e58cc252acfb58d4878",
        "title": "Research on Adverse Drug Reaction Prediction Model Combining Knowledge Graph Embedding and Deep Learning"
      },
      {
        "paperId": "e7d9c427e2710a41cb86eb620b74607933739957",
        "title": "Cross-dimension Affinity Distillation for 3D EM Neuron Segmentation"
      },
      {
        "paperId": "28b73de91fbcc1134f32621c4ddb446f076ee3d2",
        "title": "TokenUnify: Scaling Up Autoregressive Pretraining for Neuron Segmentation"
      },
      {
        "paperId": "1f3b4dbdc595bdb120a82a8ffd566d5da425c563",
        "title": "Research on Image Recognition Technology Based on Multimodal Deep Learning"
      },
      {
        "paperId": "c2ed9c1988f1f05754db7d3ac72ea04a37369cd0",
        "title": "E-commerce Webpage Recommendation Scheme Base on Semantic Mining and Neural Networks"
      },
      {
        "paperId": "f26f48e62f1fbd3381943b01c21ba96c6e99fa5f",
        "title": "BIMCV-R: A Landmark Dataset for 3D CT Text-Image Retrieval"
      },
      {
        "paperId": "d4c1e8350689ff91ca44ec2ab6c8a3b09c46153f",
        "title": "Rumor Detection with a novel graph neural network approach"
      },
      {
        "paperId": "9f461a98fcf5e53768466d75d0d726fc462474af",
        "title": "Image Captioning in news report scenario"
      },
      {
        "paperId": "0a33dd6906623abbe4eff2e2cbb11fe3b28bf4fc",
        "title": "EAGLE: an edge-aware gradient localization enhanced loss for CT image reconstruction"
      },
      {
        "paperId": "92674372262c340efd7ecac1a868b1ab7cb7c060",
        "title": "Immunotherapy efficacy prediction through a feature re-calibrated 2.5D neural network"
      },
      {
        "paperId": "0d87d85e9e2dcc91b338c65e3bfd7126c66b80cb",
        "title": "ETP: Learning Transferable ECG Representations via ECG-Text Pre-Training"
      },
      {
        "paperId": "b52fcc17e11d8bed51c0f4beaebd7534b26995bf",
        "title": "Domain Adaptive Synapse Detection with Weak Point Annotations"
      },
      {
        "paperId": "f1aa892eb6c00bf83c94f86abe7a59f0e3f1549e",
        "title": "Learning Multiscale Consistency for Self-Supervised Electron Microscopy Instance Segmentation"
      },
      {
        "paperId": "2044ab82dcb2c11ef660bd51d40130fe182f98d3",
        "title": "Zeroth-Order Optimization Meets Human Feedback: Provable Learning via Ranking Oracles"
      },
      {
        "paperId": "3d27a91ef6a90b8a778cbd3b28490821b31a9fa6",
        "title": "End-To-End Training and Testing Gamification Framework to Learn Human Highway Driving"
      },
      {
        "paperId": "fd2f8bae1fd3d7e5c14c8df823a0017921e761b5",
        "title": "Confidence Trigger Detection: Accelerating Real-Time Tracking-by-Detection Systems"
      },
      {
        "paperId": "cee6408adcb18ad6c8f750684de0d1323d068970",
        "title": "TokenUnify: Scalable Autoregressive Visual Pre-training with Mixture Token Prediction"
      }
    ],
    "score": 14.0
  },
  {
    "id": "d60df0754df6ccb14c563f07f865f391da3cba2d",
    "title": "An OCBA-Based Method for Efficient Sample Collection in Reinforcement Learning",
    "authors": [
      "Kuo Li",
      "Xinze Jin",
      "Qing-Shan Jia",
      "Dongchun Ren",
      "Huaxia Xia"
    ],
    "year": 2024,
    "citationCount": 13,
    "abstract": "This work focuses on the sample collection in reinforcement learning (RL), where the interaction with the environment is typically time-consuming and extravagantly expensive. In order to collect samples in a more valuable way, we propose a confidence-based sampling strategy based on the optimal computing budget allocation algorithm (OCBA), which actively allocates the computing efforts to actions with different predictive uncertainties. We estimate the uncertainty with ensembles and generalize them from tabular representations to function approximations. The OCBA-based sampling strategy could be easily integrated into various off-policy RL algorithms, where we take Q-learning, DQN, and SAC as examples to show the incorporation. Besides, we provide the theoretical analysis towards convergence and evaluate the algorithms experimentally. According to the experiments, the incorporated algorithms obtain remarkable gains compared with modern ensemble-based RL algorithms. Note to Practitioners\u2014Reinforcement learning is a powerful tool for handling sequential decision-making problems, e.g., autonomous driving and robotics control, where the behaviors typically have a long-term effect on future events. However, although RL achieves human-level control in some tasks, it severely suffers from low sample efficiency. Therefore, implementing RL in some practical areas, e.g., healthcare and rescue, is extremely hard due to the requirement of massive samples. This work aims to enhance the exploration of RL by incorporating OCBA, which provides an asymptotically optimal data-collection strategy for simulation-based optimization. Based on ensemble-based uncertainty estimation and OCBA-based action selection, the incorporated RL algorithms show competitive performance on many benchmarks and significantly reduce the sampling efforts during iterations.",
    "url": "https://www.semanticscholar.org/paper/d60df0754df6ccb14c563f07f865f391da3cba2d",
    "pdf_url": "https://doi.org/10.1109/TASE.2023.3282257",
    "venue": "IEEE Transactions on Automation Science and Engineering",
    "publicationDate": "2024-07-01",
    "externalIds": {
      "DBLP": "journals/tase/LiJJRX24",
      "DOI": "10.1109/TASE.2023.3282257",
      "CorpusId": 259426320
    },
    "references": [
      {
        "paperId": "a9eebb54c1d55b45d52169121c1e48c55014931a",
        "title": "Scalable Autonomous Separation Assurance With Heterogeneous Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "c12dff5ce0c189dab6307781ac2133625aff85d6",
        "title": "Visuomotor Reinforcement Learning for Multirobot Cooperative Navigation"
      },
      {
        "paperId": "7767a158eee410b665d29b5baefd80869b2fff55",
        "title": "Decentralized Multi-Agent Reinforcement Learning: An Off-Policy Method"
      },
      {
        "paperId": "2e55fa159c46506c0556fc8e65f653247fa34827",
        "title": "An Optimal Computing Budget Allocation Tree Policy for Monte Carlo Tree Search"
      },
      {
        "paperId": "5cbb80db7c48ff7c5a2276c6631ac2e9488a143a",
        "title": "Review on ranking and selection: A new perspective"
      },
      {
        "paperId": "324effa5a7c737257b675f85bd93d777fc486878",
        "title": "SUNRISE: A Simple Unified Framework for Ensemble Learning in Deep Reinforcement Learning"
      },
      {
        "paperId": "84243c3a8e88cacc2b9194a3e58f25f0686dc9f6",
        "title": "False data injection against state estimation in power systems with multiple cooperative attackers."
      },
      {
        "paperId": "0a0866ec7180bbf87b1c87ed48bd4fa00574b814",
        "title": "Soft Actor-Critic for Discrete Action Settings"
      },
      {
        "paperId": "7d74c483561c985226d5c4ad498631f976ef681f",
        "title": "Coarse-to-Fine UAV Target Tracking With Deep Reinforcement Learning"
      },
      {
        "paperId": "4ee70fb32981f84f9dddc57bd59a69e677c91759",
        "title": "Benchmarking Model-Based Reinforcement Learning"
      },
      {
        "paperId": "db1614b97aec1e0ead9554a038aa0f8dd9a26e30",
        "title": "Exploring Model-based Planning with Policy Networks"
      },
      {
        "paperId": "b81f0adde561b6c1e27490fa253d528071bb6428",
        "title": "Advancing Constrained Ranking and Selection With Regression in Partitioned Domains"
      },
      {
        "paperId": "12c0751b4f51ed833172a713b7e32390032ead93",
        "title": "Soft Actor-Critic Algorithms and Applications"
      },
      {
        "paperId": "03cc81e98942bdafd994af7a1d1e62a68ff8b682",
        "title": "Is Q-learning Provably Efficient?"
      },
      {
        "paperId": "56136aa0b2c347cbcf3d50821f310c4253155026",
        "title": "Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models"
      },
      {
        "paperId": "4debb99c0c63bfaa97dd433bc2828e4dac81c48b",
        "title": "Addressing Function Approximation Error in Actor-Critic Methods"
      },
      {
        "paperId": "50df19aff9e4a68fedfc7dad3fca48a060fc9085",
        "title": "Fully Decentralized Multi-Agent Reinforcement Learning with Networked Agents"
      },
      {
        "paperId": "1485c5ac93b3f01a5f3ef7824eb428c8bff39ba3",
        "title": "UCB EXPLORATION VIA Q-ENSEMBLES"
      },
      {
        "paperId": "27dfecb6bb0308c7484e13dcaefd5eeebba677d3",
        "title": "Model-Ensemble Trust-Region Policy Optimization"
      },
      {
        "paperId": "811df72e210e20de99719539505da54762a11c6d",
        "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor"
      },
      {
        "paperId": "c6affe793e68e1bc8ca954d276d942d357f35650",
        "title": "Ranking and Selection as Stochastic Control"
      },
      {
        "paperId": "0ab3f7ecbdc5a33565a234215604a6ca9d155a33",
        "title": "Rainbow: Combining Improvements in Deep Reinforcement Learning"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "3fab4d28be5ff73f42c8b307117aa8d680345ad9",
        "title": "Noisy Networks for Exploration"
      },
      {
        "paperId": "7c3ece1ba41c415d7e81cfa5ca33a8de66efd434",
        "title": "Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments"
      },
      {
        "paperId": "225ab689f41cef1dc18237ef5dab059a49950abf",
        "title": "Curiosity-Driven Exploration by Self-Supervised Prediction"
      },
      {
        "paperId": "8db9df2eadea654f128c1887722c677c708e8a47",
        "title": "Deep Reinforcement Learning framework for Autonomous Driving"
      },
      {
        "paperId": "9172cd6c253edf7c3a1568e03577db20648ad0c4",
        "title": "Reinforcement Learning with Deep Energy-Based Policies"
      },
      {
        "paperId": "802168a81571dde28f5ddb94d84677bc007afa7b",
        "title": "Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles"
      },
      {
        "paperId": "c89f6de5aaf0f9f196888f8a06c470c9c4cff9a0",
        "title": "Incorporation of Optimal Computing Budget Allocation for Ordinal Optimization Into Learning Automata"
      },
      {
        "paperId": "69e76e16740ed69f4dc55361a3d319ac2f1293dd",
        "title": "Asynchronous Methods for Deep Reinforcement Learning"
      },
      {
        "paperId": "846aedd869a00c09b40f1f1f35673cb22bc87490",
        "title": "Mastering the game of Go with deep neural networks and tree search"
      },
      {
        "paperId": "4c05d7caa357148f0bbd61720bdd35f0bc05eb81",
        "title": "Dueling Network Architectures for Deep Reinforcement Learning"
      },
      {
        "paperId": "c6170fa90d3b2efede5a2e1660cb23e1c824f2ca",
        "title": "Prioritized Experience Replay"
      },
      {
        "paperId": "3b9732bb07dc99bde5e1f9f75251c6ea5039373e",
        "title": "Deep Reinforcement Learning with Double Q-Learning"
      },
      {
        "paperId": "024006d4c2a89f7acacc6e4438d156525b60a98f",
        "title": "Continuous control with deep reinforcement learning"
      },
      {
        "paperId": "f5f323e62acb75f785e00b4c90ace16f1690076f",
        "title": "Deep Recurrent Q-Learning for Partially Observable MDPs"
      },
      {
        "paperId": "340f48901f72278f6bf78a04ee5b01df208cc508",
        "title": "Human-level control through deep reinforcement learning"
      },
      {
        "paperId": "449532187c94af3dd3aa55e16d2c50f7854d2199",
        "title": "Trust Region Policy Optimization"
      },
      {
        "paperId": "2b9c08bc8b18358c074d6f8c105ad6158e97e489",
        "title": "On Upper-Confidence Bound Policies for Switching Bandit Problems"
      },
      {
        "paperId": "644a079073969a92674f69483c4a85679d066545",
        "title": "Double Q-learning"
      },
      {
        "paperId": "dc1ae648345583efa4d2cbcde4edf71d105c0af7",
        "title": "An actor-critic algorithm with function approximation for discounted cost constrained Markov decision processes"
      },
      {
        "paperId": "c958734949eaeac84bb554bb542fb053ea0dd087",
        "title": "A Knowledge-Gradient Policy for Sequential Information Collection"
      },
      {
        "paperId": "a67f8cbe851f99051f213ffed79474e29acf9978",
        "title": "New Two-Stage and Sequential Procedures for Selecting the Best Simulated System"
      },
      {
        "paperId": "89d5670c7fc763402f65cb6f8aac77486785cccd",
        "title": "Simulation Budget Allocation for Further Enhancing the Efficiency of Ordinal Optimization"
      },
      {
        "paperId": "50529de8a9d1431b2d47c74f8a6256457dc59863",
        "title": "Computing efforts allocation for ordinal optimization and discrete event simulation"
      },
      {
        "paperId": "b90b5b0cf05bc63f768f55322381f5cfbee6ce1c",
        "title": "Bayesian Q-Learning"
      },
      {
        "paperId": "e851bdcaeeb82c95e1bc61e8445d240cae545386",
        "title": "Bayesian Analysis For Simulation Input And Output"
      },
      {
        "paperId": "03b7e51c52084ac1db5118342a00b5fbcfc587aa",
        "title": "Q-learning"
      },
      {
        "paperId": "e02d1d482ea7a51ede5a0babd45ab3d4344a8e13",
        "title": "On the Theory of Apportionment"
      },
      {
        "paperId": "ee2cd1d17f833d3c157a1016a778c7c22af555a2",
        "title": "ON THE LIKELIHOOD THAT ONE UNKNOWN PROBABILITY EXCEEDS ANOTHER IN VIEW OF THE EVIDENCE OF TWO SAMPLES"
      },
      {
        "paperId": "8ae21cc3bc48fe2d2a35cce70436c44a6a44259d",
        "title": "Resilient output regulation in heterogeneous networked systems under Byzantine agents"
      },
      {
        "paperId": "d9e3faf253985f01e062eb62e2df26f11bbb48b5",
        "title": "Ranking and Selection: Efficient Simulation Budget Allocation"
      },
      {
        "paperId": null,
        "title": "Simulation-Based Optimization . Berlin, Germany"
      },
      {
        "paperId": "5ace8f396dad193ca6d84a6423a0038ada4cf1b2",
        "title": "Statistical Inference"
      },
      {
        "paperId": "49ad614ab5a27ef4d1f6a66dff701bf4cf5fe9da",
        "title": "Sequential Sampling to Myopically Maximize the Expected Value of Information"
      },
      {
        "paperId": null,
        "title": "\u201cConvergence of Q-learning: A simple proof,\u201d"
      },
      {
        "paperId": "97efafdb4a3942ab3efba53ded7413199f79c054",
        "title": "Reinforcement Learning: An Introduction"
      },
      {
        "paperId": "4e9d797427cd56be90932d6092fc3b6282dfb96f",
        "title": "On the Convergence of Stochastic Iterative Dynamic Programming Algorithms"
      },
      {
        "paperId": null,
        "title": "On-Line Q-Learning Using Connec-tionist Systems , vol. 37. Princeton, NJ, USA"
      },
      {
        "paperId": null,
        "title": "Driving Department. Group"
      },
      {
        "paperId": null,
        "title": "received the B.E. degree in automation and the Ph.D. degree in control science and engineering from Tsinghua University, Beijing, China, in 2002 and 2006, respectively"
      },
      {
        "paperId": null,
        "title": "received the automation from Nankai University and the Ph.D. degree in"
      }
    ],
    "cited_by": [
      {
        "paperId": "3580a93c24dfe64c2856d85ea96df67d76180bc3",
        "title": "On Efficient Sampling for Multi-Agent Reinforcement Learning in Supply Demand Matching"
      },
      {
        "paperId": "3f2813b31d67831cc71f3c4de31b344001bc2db2",
        "title": "Preference-Based Multi-Objective Reinforcement Learning"
      },
      {
        "paperId": "86d02d6954da6b862f8a69253de08386299061a4",
        "title": "Sample Efficient Robot Learning in Supervised Effect Prediction Tasks"
      },
      {
        "paperId": "c89735e59d2c1a982a48b018b8191a20d93f8742",
        "title": "ISFORS-MIX: Multi-agent reinforcement learning with Importance-Sampling-Free Off-policy learning and Regularized-Softmax Mixing network"
      },
      {
        "paperId": "ef81323c49ba559805c0d25956d04224b70362ff",
        "title": "An OCBA-Based Information Sharing Method for Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "230fb7791edd01d86facadaba1d8723a2b9381a1",
        "title": "On Efficient Sampling for Reinforcement Learning with Multiple Constraints*"
      },
      {
        "paperId": "4be269b6c51f01740d1fdd897ccc65bc47de5e15",
        "title": "On Efficient Multi-Agent Reinforcement Learning for Large Scale Supply Demand Matching"
      },
      {
        "paperId": "8623fe8e65755b76595c5ab2211e427b47b63f5d",
        "title": "On Efficient Sampling in Supervisory Reinforcement Learning"
      },
      {
        "paperId": "fcdd39b2b81a56465b9801c9cc567bfe24970e8f",
        "title": "On Sampling Efficiency Optimization in Constrained Reinforcement Learning*"
      },
      {
        "paperId": "39b5e390f004e0e6fc78f4d244973e3c27ad2804",
        "title": "On Efficient Sampling in Offline Reinforcement Learning"
      },
      {
        "paperId": "b2445d0ff46f3f6c46db70ddcfb3038941222598",
        "title": "Multi-Agent Reinforcement Learning With Decentralized Distribution Correction"
      },
      {
        "paperId": "be676fabb8bbff233faa4391bf93378fed9d30bc",
        "title": "Fractional-Order Optimal Control and FIOV-MASAC Reinforcement Learning for Combating Malware Spread in Internet of Vehicles"
      },
      {
        "paperId": "90e33f41ed02abed6eec968fe441d667bf6f4da7",
        "title": "Content Caching for IoT Devices by Using Self-Feedback Adversarial Semi-Bandits Learning"
      }
    ],
    "score": 13.0
  },
  {
    "id": "03c7f9a6b743eb1c9127b8d4552147b2b1a0c1e1",
    "title": "Demonstration-Guided Reinforcement Learning with Efficient Exploration for Task Automation of Surgical Robot",
    "authors": [
      "Tao Huang",
      "Kai Chen",
      "Bin Li",
      "Yunhui Liu",
      "Qingxu Dou"
    ],
    "year": 2023,
    "citationCount": 26,
    "abstract": "Task automation of surgical robot has the potentials to improve surgical efficiency. Recent reinforcement learning (RL) based approaches provide scalable solutions to surgical automation, but typically require extensive data collection to solve a task if no prior knowledge is given. This issue is known as the exploration challenge, which can be alleviated by providing expert demonstrations to an RL agent. Yet, how to make effective use of demonstration data to improve exploration efficiency still remains an open challenge. In this work, we introduce Demonstration-guided EXploration (DEX), an efficient reinforcement learning algorithm that aims to overcome the exploration problem with expert demonstrations for surgical automation. To effectively exploit demonstrations, our method estimates expert-like behaviors with higher values to facilitate productive interactions, and adopts non-parametric regression to enable such guidance at states unobserved in demonstration data. Extensive experiments on 10 surgical manipulation tasks from SurRoL, a comprehensive surgical simulation platform, demonstrate significant improvements in the exploration efficiency and task success rates of our method. Moreover, we also deploy the learned policies to the da Vinci Research Kit (dVRK) platform to show the effectiveness on the real robot. Code is available at https://github.com/med-air/DEX.",
    "url": "https://www.semanticscholar.org/paper/03c7f9a6b743eb1c9127b8d4552147b2b1a0c1e1",
    "pdf_url": "https://arxiv.org/pdf/2302.09772.pdf",
    "venue": "IEEE International Conference on Robotics and Automation",
    "publicationDate": "2023-02-20",
    "externalIds": {
      "ArXiv": "2302.09772",
      "DBLP": "conf/icra/HuangCLLD23",
      "DOI": "10.1109/ICRA48891.2023.10160327",
      "CorpusId": 257038664
    },
    "references": [
      {
        "paperId": "0a28625a743790b52accb6d332094b68dedd8902",
        "title": "SAVAnet: Surgical Action-Driven Visual Attention Network for Autonomous Endoscope Control"
      },
      {
        "paperId": "c91b5677ecb034a6a2ca69509b604965c776d5d6",
        "title": "On Pathologies in KL-Regularized Reinforcement Learning from Expert Demonstrations"
      },
      {
        "paperId": "ec5f3cad41691ec5fc12cb6a87f72cee5b4c3e5f",
        "title": "Learning intraoperative organ manipulation with context-based reinforcement learning"
      },
      {
        "paperId": "c35831b0adef60e4497815fae077ff31137c73e1",
        "title": "Dexterous Imitation Made Easy: A Learning-Based Framework for Efficient Dexterous Manipulation"
      },
      {
        "paperId": "229ca1d46d63ccac4b82e5cca5c4dd676aaa870d",
        "title": "Autonomous robotic laparoscopic surgery for intestinal anastomosis"
      },
      {
        "paperId": "943d0dd652837ec01e2bc25911589b5659939803",
        "title": "Inverse Reinforcement Learning Intra-Operative Path Planning for Steerable Needle"
      },
      {
        "paperId": "5e39c030eb7e3ad516cbe1bdcd7b8c4a9e51a6a9",
        "title": "The Surprising Effectiveness of Representation Learning for Visual Imitation"
      },
      {
        "paperId": "274e32e992bb5ce70feb1a474ea9a4cc7886dad8",
        "title": "Model-free reinforcement learning from expert demonstrations: a survey"
      },
      {
        "paperId": "8d030a65d304587c0d235629f1e9d434c3ad2062",
        "title": "Learning from Demonstrations for Autonomous Soft-tissue Retraction *"
      },
      {
        "paperId": "751801d8242bd0319d277c758e6c5a8fb4008bf6",
        "title": "Cooperative Assistance in Robotic Surgery through Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "3f1b30c5a58d5ee72b3105b78d187e7d3f84e0ac",
        "title": "Safe Reinforcement Learning using Formal Verification for Tissue Retraction in Autonomous Robotic-Assisted Surgery"
      },
      {
        "paperId": "6f4ca4a3fd6071787083d067cf420e468e930d62",
        "title": "SurRoL: An Open-source Reinforcement Learning Centered and dVRK Compatible Platform for Surgical Robot Learning"
      },
      {
        "paperId": "558ca2e8c7eb56edd77a52b084e6cc24dffe5bcd",
        "title": "Deep Reinforcement Learning at the Edge of the Statistical Precipice"
      },
      {
        "paperId": "97f38da992b4974d57cf6ef74706e12aabdc7ced",
        "title": "Autonomous Needle Manipulation for Robotic Surgical Suturing Based on Skills Learned from Demonstration"
      },
      {
        "paperId": "259b4f5ed43fda5dd3510821b40fac13021e7605",
        "title": "Hierarchical Few-Shot Imitation with Skill Transition Models"
      },
      {
        "paperId": "9f2e34581ca03160e8fd8b770203a5e7c2a902d3",
        "title": "RRL: Resnet as representation for Reinforcement Learning"
      },
      {
        "paperId": "783d0c9afd94309e3b29d17bcf20b2699f23244f",
        "title": "Robotic Surgery With Lean Reinforcement Learning"
      },
      {
        "paperId": "1e066cabcd8a03a2262955274f457bcf1dcd7a25",
        "title": "Accelerating Surgical Robotics Research: A Review of 10 Years With the da Vinci Research Kit"
      },
      {
        "paperId": "362cc80481b288874af0428107ab31e955dcf09f",
        "title": "Offline Reinforcement Learning with Fisher Divergence Critic Regularization"
      },
      {
        "paperId": "400811ee31020a3f002551476dac25973e13035e",
        "title": "How to train your robot with deep reinforcement learning: lessons we have learned"
      },
      {
        "paperId": "a5f103845d5d23098cb0afc3b1d6f2808c0b0ab2",
        "title": "Bimanual Regrasping for Suture Needles using Reinforcement Learning for Rapid Motion Planning"
      },
      {
        "paperId": "342291cbac7ec5d9d36b23dbf005759f6ad8a0e4",
        "title": "Soft Tissue Simulation Environment to Learn Manipulation Tasks in Autonomous Robotic Surgery*"
      },
      {
        "paperId": "d591e9d71641e8c1e74c5d48354dc4403932aa99",
        "title": "Autonomous Tissue Retraction in Robotic Assisted Minimally Invasive Surgery \u2013 A Feasibility Study"
      },
      {
        "paperId": "ea46d2f21b3bb2e43549b2e6bd210b8e5b162a99",
        "title": "Collaborative Suturing: A Reinforcement Learning Approach to Automate Hand-off Task in Suturing for Surgical Robots"
      },
      {
        "paperId": "0272b14dd471fe7b81df703af1b71d7600b77215",
        "title": "Accelerating Online Reinforcement Learning with Offline Datasets"
      },
      {
        "paperId": "5f8048bed5bc373434aa2ac4c6071c22e4891046",
        "title": "Collaborative Robot-Assisted Endovascular Catheterization with Generative Adversarial Imitation Learning"
      },
      {
        "paperId": "dbee258c79dbd5ac899a2f460dd95a408a663305",
        "title": "Reinforcement Learning Based Manipulation Skill Transferring for Robot-assisted Minimally Invasive Surgery"
      },
      {
        "paperId": "6568423cfaca7e24c88ea208cb0e67129e43aa9b",
        "title": "Image Augmentation Is All You Need: Regularizing Deep Reinforcement Learning from Pixels"
      },
      {
        "paperId": "1ee2cf270b6f6db61e11e96f7d73f7a106a724eb",
        "title": "Optical Coherence Tomography-Guided Robotic Ophthalmic Microsurgery via Reinforcement Learning from Demonstration"
      },
      {
        "paperId": "9efb64f20ab1f157ca9f4050d4aaacf6c3f9b2b2",
        "title": "CURL: Contrastive Unsupervised Representations for Reinforcement Learning"
      },
      {
        "paperId": "9be492858863c8c7c24be1ecb75724de5086bd8e",
        "title": "Behavior Regularized Offline Reinforcement Learning"
      },
      {
        "paperId": "b668ba0900ddcdacd0a07ff9983172f525c3c4d6",
        "title": "Goal-conditioned Imitation Learning"
      },
      {
        "paperId": "b4e69b0172d69c80f83366c296b6222805360445",
        "title": "SQIL: Imitation Learning via Reinforcement Learning with Sparse Rewards"
      },
      {
        "paperId": "e4254a7bb2997a5949f2dcad981c647f45d8ef98",
        "title": "Manipulating Soft Tissues by Deep Reinforcement Learning for Autonomous Robotic Surgery"
      },
      {
        "paperId": "302ef104572b00eb271c3169921e0b097ea28fd1",
        "title": "A New Tensioning Method using Deep Reinforcement Learning for Surgical Pattern Cutting"
      },
      {
        "paperId": "f0df16721c94e57d60d1b4ca53c7755caf22626b",
        "title": "Robot-Assisted Training in Laparoscopy Using Deep Reinforcement Learning"
      },
      {
        "paperId": "12c0751b4f51ed833172a713b7e32390032ead93",
        "title": "Soft Actor-Critic Algorithms and Applications"
      },
      {
        "paperId": "5285cb8faada5de8a92a47622950f6cfd476ac1d",
        "title": "Off-Policy Deep Reinforcement Learning without Exploration"
      },
      {
        "paperId": "64ecc055b5b84d5f0843ad3eef98deedee4bba85",
        "title": "Dexterous Manipulation with Deep Reinforcement Learning: Efficient, General, and Low-Cost"
      },
      {
        "paperId": "62d76acb8eb4a8890b7fba143843a908a247531f",
        "title": "Discriminator-Actor-Critic: Addressing Sample Inefficiency and Reward Bias in Adversarial Imitation Learning"
      },
      {
        "paperId": "6838cac0edc58fceb6b708767f49dce52f14d598",
        "title": "Learning 2D Surgical Camera Motion From Demonstrations"
      },
      {
        "paperId": "1b9ce6abc0f3024b88fcd4dbd0c10cf5bcf7d38d",
        "title": "DeepMimic"
      },
      {
        "paperId": "8d6adaa16ed0af9935a1130a305c85e8bdf8780d",
        "title": "An Algorithmic Perspective on Imitation Learning"
      },
      {
        "paperId": "d356a5603f14c7a6873272774782d7812871f952",
        "title": "Reinforcement and Imitation Learning for Diverse Visuomotor Skills"
      },
      {
        "paperId": "4debb99c0c63bfaa97dd433bc2828e4dac81c48b",
        "title": "Addressing Function Approximation Error in Actor-Critic Methods"
      },
      {
        "paperId": "811df72e210e20de99719539505da54762a11c6d",
        "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor"
      },
      {
        "paperId": "608298f059b806a3c2e75b09ef619bd029321d5e",
        "title": "Using intermittent synchronization to compensate for rhythmic body motion during autonomous surgical cutting and debridement"
      },
      {
        "paperId": "c28ec2a40a2c77e20d64cf1c85dc931106df8e83",
        "title": "Overcoming Exploration in Reinforcement Learning with Demonstrations"
      },
      {
        "paperId": "e010ba3ff5744604cdbfe44a733e2a98649ee907",
        "title": "Learning Complex Dexterous Manipulation with Deep Reinforcement Learning and Demonstrations"
      },
      {
        "paperId": "1bead9000a719cb258bac7320228055aee650d2c",
        "title": "Leveraging Demonstrations for Deep Reinforcement Learning on Robotics Problems with Sparse Rewards"
      },
      {
        "paperId": "35f7b928a5ed86b3a480a71846c3dfb19f3104fd",
        "title": "Robot Autonomy for Surgery"
      },
      {
        "paperId": "429ed4c9845d0abd1f8204e1d7705919559bc2a2",
        "title": "Hindsight Experience Replay"
      },
      {
        "paperId": "07be721c0268da8baf08db09da0dea8213b7bdcd",
        "title": "Multilateral surgical pattern cutting in 2D orthotropic gauze with deep reinforcement learning policies for tensioning"
      },
      {
        "paperId": "e3b0ea7209731c47b582215c6c67f9c691ad9863",
        "title": "Deep Q-learning From Demonstrations"
      },
      {
        "paperId": "4ab53de69372ec2cd2d90c126b6a100165dc8ed1",
        "title": "Generative Adversarial Imitation Learning"
      },
      {
        "paperId": "3b9732bb07dc99bde5e1f9f75251c6ea5039373e",
        "title": "Deep Reinforcement Learning with Double Q-Learning"
      },
      {
        "paperId": "024006d4c2a89f7acacc6e4438d156525b60a98f",
        "title": "Continuous control with deep reinforcement learning"
      },
      {
        "paperId": "7ff7a763c7b09459524329d689d953d43b740d04",
        "title": "Learning by observation for surgical subtasks: Multilateral cutting of 3D viscoelastic and 2D Orthotropic Tissue Phantoms"
      },
      {
        "paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8",
        "title": "Adam: A Method for Stochastic Optimization"
      },
      {
        "paperId": "29b5efbeb766820d719a03203045356eca87eb5d",
        "title": "A Review of Camera Viewpoint Automation in Robotic and Laparoscopic Surgery"
      },
      {
        "paperId": "358c709654551e37ebff1425b6d41c054bfb11f0",
        "title": "Smart Tissue Anastomosis Robot (STAR): A Vision-Guided Robotics System for Laparoscopic Suturing"
      },
      {
        "paperId": "244539f454800697ed663326b7cfba337ca0c2ec",
        "title": "Guided Policy Search"
      },
      {
        "paperId": "4e5dfb0b1e54412e799eb0e86d552956cc3a5f54",
        "title": "A survey of robot learning from demonstration"
      },
      {
        "paperId": "c8221c054459e37edbf313668523d667fe5c1536",
        "title": "Maximum Entropy Inverse Reinforcement Learning"
      },
      {
        "paperId": "6b502694a8e23d3b9ab08fa7a0c6d5e82bd1f066",
        "title": "Locally Weighted Learning"
      },
      {
        "paperId": null,
        "title": "\u201cAmp: Adversarial motion priors for stylized physics-based character control,\u201d"
      },
      {
        "paperId": null,
        "title": "Integrating behavior cloning and reinforcement learning for improved performance in dense and sparse reward environments"
      },
      {
        "paperId": "941ba185f01b1a0a27453fd178aa5f010510ee8b",
        "title": "Learning Robust Rewards with Adverserial Inverse Reinforcement Learning"
      },
      {
        "paperId": null,
        "title": "Openai baselines"
      },
      {
        "paperId": "1f4731d5133cb96ab30e08bf39dffa874aebf487",
        "title": "A Framework for Behavioural Cloning"
      }
    ],
    "cited_by": [
      {
        "paperId": "b67c2660efa6d53983bc58611909675639383d94",
        "title": "Visuomotor Grasping with World Models for Surgical Robots"
      },
      {
        "paperId": "31d69fe352508b532e2a8013f421269a7fcada61",
        "title": "A Systematic Review of Task Automation in Surgical Robotics"
      },
      {
        "paperId": "d4be57ab5d1d693ff14aaa5dbaba5393773b265a",
        "title": "Sim4EndoR: A Reinforcement Learning Centered Simulation Platform for Task Automation of Endovascular Robotics"
      },
      {
        "paperId": "772b4c1c7530f5ecbefc03577e173231d042ffd2",
        "title": "dARt Vinci: Egocentric Data Collection for Surgical Robot Learning at Scale"
      },
      {
        "paperId": "fddd061ddfd54176cb7ba3f8bf2fa69d02330333",
        "title": "Diffusion Stabilizer Policy for Automated Surgical Robot Manipulations"
      },
      {
        "paperId": "fddc43d4cfaf46f2e0c7886832db7bf50f6fadcd",
        "title": "Realistic Surgical Simulation from Monocular Videos"
      },
      {
        "paperId": "47e7974f7a12d6be799a1f978cb2c143d7437d7d",
        "title": "FF-SRL: High Performance GPU-Based Surgical Simulation For Robot Learning"
      },
      {
        "paperId": "7d7069ec9bae7a3eb4522cca196c90e378f37067",
        "title": "SurgIRL: Towards Life-Long Learning for Surgical Automation by Incremental Reinforcement Learning"
      },
      {
        "paperId": "ca09eae7d0ef0a74ff7ec75c0bc7918c2a891c3e",
        "title": "Surgical Task Automation Using Actor-Critic Frameworks and Self-Supervised Imitation Learning"
      },
      {
        "paperId": "8998ae447ebec960f121832117526b7b11747334",
        "title": "Rethinking RGB-D Fusion for Semantic Segmentation in Surgical Datasets"
      },
      {
        "paperId": "f344e82d0d4f5e97c3d6176fd4b953a8c6c72d45",
        "title": "SurgicAI: A Hierarchical Platform for Fine-Grained Surgical Policy Learning and Benchmarking"
      },
      {
        "paperId": "320a68d93dbc3da7d75bb3183c071423647564e2",
        "title": "World Models for General Surgical Grasping"
      },
      {
        "paperId": "feb6f6a2f7de192baf2ae67e6cdb40c20f530393",
        "title": "Multi-objective Cross-task Learning via Goal-conditioned GPT-based Decision Transformers for Surgical Robot Task Automation"
      },
      {
        "paperId": "58abf857592ef8ae067459d50c245b67c9c29050",
        "title": "Lens Capsule Tearing in Cataract Surgery using Reinforcement Learning"
      },
      {
        "paperId": "b8fa03bdcf341019b5d00f665021a0d8087a65d2",
        "title": "SimEndoGS: Efficient Data-driven Scene Simulation using Robotic Surgery Videos via Physics-embedded 3D Gaussians"
      },
      {
        "paperId": "94734477b0a642dea4482869e400d1528cfaef08",
        "title": "Efficient Physically-based Simulation of Soft Bodies in Embodied Environment for Surgical Robot"
      },
      {
        "paperId": "e3090c285306e5be311a12c11ec4d4a78f83512b",
        "title": "General-purpose foundation models for increased autonomy in robot-assisted surgery"
      },
      {
        "paperId": "f4f9dda3a53f078eac7878a89322450609d6df10",
        "title": "Signal Temporal Logic-Guided Apprenticeship Learning"
      },
      {
        "paperId": "b7a1618056cb1452bb3aefb15014e85d3bb8339a",
        "title": "Leveraging Optimal Transport for Enhanced Offline Reinforcement Learning in Surgical Robotic Environments"
      },
      {
        "paperId": "89931304bd4b7faf6ff8e3ff1579f7151938d15a",
        "title": "Autonomous Soft Tissue Retraction Using Demonstration-Guided Reinforcement Learning"
      },
      {
        "paperId": "4c1ae44f053a09a79c942b32cc0e0e2cd2cbef3b",
        "title": "Value-Informed Skill Chaining for Policy Learning of Long-Horizon Tasks with Surgical Robot"
      },
      {
        "paperId": "566c345cdee27efce945671fe214fdc1037b2ceb",
        "title": "Teach Me How to Learn: A Perspective Review towards User-centered Neuro-symbolic Learning for Robotic Surgical Systems"
      },
      {
        "paperId": "b242e4c1525abb07a02d47284190160d01476ef8",
        "title": "Learning Needle Pick-and-Place Without Expert Demonstrations"
      },
      {
        "paperId": "0e30c1ae0a5e2675a16e16ba5f2794a244be5e15",
        "title": "Human-in-the-Loop Embodied Intelligence With Interactive Simulation Environment for Surgical Robot Learning"
      },
      {
        "paperId": "2936a5cbed369ce05cdab66addfafc84cdf62230",
        "title": "\u673a\u5668\u4eba\u8d85\u58f0\u6210\u50cf\uff1a\u6280\u672f\u8fdb\u5c55\u4e0e\u5e94\u7528\u524d\u666f"
      },
      {
        "paperId": "12a4f3d56c83ef949780afffa473b0320e022fdc",
        "title": "Overleaf Example"
      }
    ],
    "score": 13.0
  },
  {
    "id": "7bd4edf878976d329f326f3a12675a66cbc075e9",
    "title": "Constrained reinforcement learning with statewise projection: a control barrier function approach",
    "authors": [
      "Xinze Jin",
      "Kuo Li",
      "Qing-Shan Jia"
    ],
    "year": 2024,
    "citationCount": 13,
    "abstract": "",
    "url": "https://www.semanticscholar.org/paper/7bd4edf878976d329f326f3a12675a66cbc075e9",
    "pdf_url": "https://doi.org/10.1007/s11432-023-3872-9",
    "venue": "Science China Information Sciences",
    "publicationDate": "2024-02-19",
    "externalIds": {
      "DBLP": "journals/chinaf/JinLJ24",
      "DOI": "10.1007/s11432-023-3872-9",
      "CorpusId": 267965675
    },
    "references": [],
    "cited_by": [
      {
        "paperId": "3580a93c24dfe64c2856d85ea96df67d76180bc3",
        "title": "On Efficient Sampling for Multi-Agent Reinforcement Learning in Supply Demand Matching"
      },
      {
        "paperId": "e0fd394b557921c7052c4133e890a27ef5140a09",
        "title": "Enhancing Safety in Model\u2010Based Reinforcement Learning With High\u2010Order Control Barrier Functions"
      },
      {
        "paperId": "ef81323c49ba559805c0d25956d04224b70362ff",
        "title": "An OCBA-Based Information Sharing Method for Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "5b1f4c504c3dc5aa77cb666c4271c0bbf6698fc2",
        "title": "Two-Phase on-Line Joint Scheduling for Welfare Maximization of Charging Station"
      },
      {
        "paperId": "bffeceb955b0c444c5b9c817732cadc44265f54c",
        "title": "Large-Scale Data Center Cooling Control via Sample-Efficient Reinforcement Learning"
      },
      {
        "paperId": "230fb7791edd01d86facadaba1d8723a2b9381a1",
        "title": "On Efficient Sampling for Reinforcement Learning with Multiple Constraints*"
      },
      {
        "paperId": "4be269b6c51f01740d1fdd897ccc65bc47de5e15",
        "title": "On Efficient Multi-Agent Reinforcement Learning for Large Scale Supply Demand Matching"
      },
      {
        "paperId": "8623fe8e65755b76595c5ab2211e427b47b63f5d",
        "title": "On Efficient Sampling in Supervisory Reinforcement Learning"
      },
      {
        "paperId": "fcdd39b2b81a56465b9801c9cc567bfe24970e8f",
        "title": "On Sampling Efficiency Optimization in Constrained Reinforcement Learning*"
      },
      {
        "paperId": "39b5e390f004e0e6fc78f4d244973e3c27ad2804",
        "title": "On Efficient Sampling in Offline Reinforcement Learning"
      },
      {
        "paperId": "b6af09722063318e9b890760aedbe2672c3489a2",
        "title": "Learning Future Representation with Synthetic Observations for Sample-efficient Reinforcement Learning"
      },
      {
        "paperId": "b2445d0ff46f3f6c46db70ddcfb3038941222598",
        "title": "Multi-Agent Reinforcement Learning With Decentralized Distribution Correction"
      },
      {
        "paperId": "e3dbf6ef5caa2a3689e86ba6344d21d83c1223ce",
        "title": "Stochastic Frank-Wolfe Algorithm for Constrained Bilevel Optimization With Improved Per-Iteration Complexity"
      }
    ],
    "score": 13.0
  },
  {
    "id": "c90aa0f206c6fd41c490c142f63f7ba046cae6b7",
    "title": "Provable and Practical: Efficient Exploration in Reinforcement Learning via Langevin Monte Carlo",
    "authors": [
      "Haque Ishfaq",
      "Qingfeng Lan",
      "Pan Xu",
      "A. Mahmood",
      "Doina Precup",
      "Anima Anandkumar",
      "K. Azizzadenesheli"
    ],
    "year": 2023,
    "citationCount": 24,
    "abstract": "We present a scalable and effective exploration strategy based on Thompson sampling for reinforcement learning (RL). One of the key shortcomings of existing Thompson sampling algorithms is the need to perform a Gaussian approximation of the posterior distribution, which is not a good surrogate in most practical settings. We instead directly sample the Q function from its posterior distribution, by using Langevin Monte Carlo, an efficient type of Markov Chain Monte Carlo (MCMC) method. Our method only needs to perform noisy gradient descent updates to learn the exact posterior distribution of the Q function, which makes our approach easy to deploy in deep RL. We provide a rigorous theoretical analysis for the proposed method and demonstrate that, in the linear Markov decision process (linear MDP) setting, it has a regret bound of $\\tilde{O}(d^{3/2}H^{3/2}\\sqrt{T})$, where $d$ is the dimension of the feature mapping, $H$ is the planning horizon, and $T$ is the total number of steps. We apply this approach to deep RL, by using Adam optimizer to perform gradient updates. Our approach achieves better or similar results compared with state-of-the-art deep RL algorithms on several challenging exploration tasks from the Atari57 suite.",
    "url": "https://www.semanticscholar.org/paper/c90aa0f206c6fd41c490c142f63f7ba046cae6b7",
    "pdf_url": "https://arxiv.org/pdf/2305.18246.pdf",
    "venue": "International Conference on Learning Representations",
    "publicationDate": "2023-05-29",
    "externalIds": {
      "DBLP": "journals/corr/abs-2305-18246",
      "ArXiv": "2305.18246",
      "DOI": "10.48550/arXiv.2305.18246",
      "CorpusId": 258959015
    },
    "references": [
      {
        "paperId": "ec2aecde467988f67b6d190479cf4b7ae7806b8c",
        "title": "Posterior Sampling with Delayed Feedback for Reinforcement Learning with Linear Function Approximation"
      },
      {
        "paperId": "1311944e1ac408fd4a7829b254f25a6560b66e07",
        "title": "Maximize to Explore: One Objective Function Fusing Estimation, Planning, and Exploration"
      },
      {
        "paperId": "d6931619b7cfebf59ae978359deb6b6e99737f46",
        "title": "Regularization and Variance-Weighted Regression Achieves Minimax Optimality in Linear MDPs: Theory and Practice"
      },
      {
        "paperId": "c863579d508a67d29a79fe61bdb2f12ff40b55d2",
        "title": "Nearly Minimax Optimal Reinforcement Learning for Linear Markov Decision Processes"
      },
      {
        "paperId": "b61c60d75f25c8b8cf23131f82d6efaced2ad3cc",
        "title": "Bilinear Exponential Family of MDPs: Frequentist Regret Bound with Tractable Exploration and Planning"
      },
      {
        "paperId": "7b6b72bfe8ba1a3ab07dc641c163676a8ae5e59d",
        "title": "A Provably Efficient Model-Free Posterior Sampling Method for Episodic Reinforcement Learning"
      },
      {
        "paperId": "10b28bd8e9a7c54d063a1647dd8f38dae48cea22",
        "title": "Making Linear MDPs Practical via Contrastive Representation Learning"
      },
      {
        "paperId": "59c2a40761b12aef01dcb8b5e10065733551f331",
        "title": "Langevin Monte Carlo for Contextual Bandits"
      },
      {
        "paperId": "85cd1064f7b82d52c1864d35223d5f45144721ff",
        "title": "From Dirichlet to Rubin: Optimistic Exploration in RL without Bonuses"
      },
      {
        "paperId": "867d3905b7a527ba46e03c454db55fa55c709389",
        "title": "Cliff Diving: Exploring Reward Surfaces in Reinforcement Learning Environments"
      },
      {
        "paperId": "b799c782f168b0a02ebab9e50ff38ded1bc79aee",
        "title": "Optimistic posterior sampling for reinforcement learning: worst-case regret bounds"
      },
      {
        "paperId": "dfee8970f6a4fa3e56b9dc7feb29ac721d04bb2a",
        "title": "Feel-Good Thompson Sampling for Contextual Bandits and Reinforcement Learning"
      },
      {
        "paperId": "80438f2ef228776b22118915f1a27e2cf4a0f4e0",
        "title": "Tianshou: a Highly Modularized Deep Reinforcement Learning Library"
      },
      {
        "paperId": "e106cfe18ee89d1121ceec2c8927f3bbc1bbf78c",
        "title": "Stochastic gradient Langevin dynamics with adaptive drifts"
      },
      {
        "paperId": "014e5f875578cbc6de620e47a0666056461f9aa5",
        "title": "Randomized Exploration for Reinforcement Learning with General Value Function Approximation"
      },
      {
        "paperId": "bf661aefbade5958c81ddefbb87e96d5b4b48729",
        "title": "Provably Correct Optimization and Exploration with Non-linear Policies"
      },
      {
        "paperId": "e2381b66f552eacfb5f8ec3bdf6bb98c5e1e50f7",
        "title": "Near-Optimal Randomized Exploration for Tabular Markov Decision Processes"
      },
      {
        "paperId": "489666f4c11787b679b36238dee95b63248ed60a",
        "title": "Training Larger Networks for Deep Reinforcement Learning"
      },
      {
        "paperId": "0bfc0ce3c40d83f24510ea80e2c3675413e3afa5",
        "title": "Nearly Minimax Optimal Reinforcement Learning for Linear Mixture Markov Decision Processes"
      },
      {
        "paperId": "350904da231fa0c28e8a6b9358198366db9a2f9b",
        "title": "Revisiting Rainbow: Promoting more insightful and inclusive deep reinforcement learning research"
      },
      {
        "paperId": "14701a7dcb91c2afe139add048412ec26ac9c1ac",
        "title": "Faster Convergence of Stochastic Gradient Langevin Dynamics for Non-Log-Concave Sampling"
      },
      {
        "paperId": "af6de8ee0c8bf4639763b73f205b51dacd490627",
        "title": "D2RL: Deep Dense Architectures in Reinforcement Learning"
      },
      {
        "paperId": "fa5853fdef7d2f6bb68203d187ddacbbddc63a8b",
        "title": "High-Dimensional Probability: An Introduction with Applications in Data Science"
      },
      {
        "paperId": "2b1730a4ee3ab1a832b8a513b1eea7394cc71451",
        "title": "Stochastic Gradient Langevin Dynamics Algorithms with Adaptive Drifts"
      },
      {
        "paperId": "1b395381e2e1078c1a62b72514eb50865941291b",
        "title": "On Worst-case Regret of Linear Thompson Sampling"
      },
      {
        "paperId": "175a9a3f0bb4f31fa235386aff52ad18c67275d3",
        "title": "Model-Based Reinforcement Learning with Value-Targeted Regression"
      },
      {
        "paperId": "60b1e6bd3614086009f9dfcc95f44897f0480d55",
        "title": "Reinforcement Learning with General Value Function Approximation: Provably Efficient Approach via Bounded Eluder Dimension"
      },
      {
        "paperId": "1b60170819cc9a775d98e9047a081a09e3ecb671",
        "title": "A Finite Time Analysis of Two Time-Scale Actor Critic Methods"
      },
      {
        "paperId": "2b2735cffb0d2321a456363880ff5671e80df4cb",
        "title": "On Bonus Based Exploration Methods In The Arcade Learning Environment"
      },
      {
        "paperId": "5c5f76af16e9fc7f5b65df390b0d4cc6d64e6f20",
        "title": "Learning Near Optimal Policies with Low Inherent Bellman Error"
      },
      {
        "paperId": "893edccfd3fe1ed58626e3b1af9bd5105d34051f",
        "title": "On Thompson Sampling with Langevin Algorithms"
      },
      {
        "paperId": "d0cf6bc0d44e2af3135d238a54c58dcd5c2d4e84",
        "title": "Provably Efficient Exploration in Policy Optimization"
      },
      {
        "paperId": "33b3324a5c81edb56990e9323bb1ca5040adb7ea",
        "title": "A Finite-Time Analysis of Q-Learning with Neural Network Function Approximation"
      },
      {
        "paperId": "57e72da5765157f72e216054f64280dbf3f8d865",
        "title": "Learning with Good Feature Representations in Bandits and in RL with a Generative Model"
      },
      {
        "paperId": "1fce49b7f2973e3576b7bf19d4b62f72c9c2eb20",
        "title": "Comments on the Du-Kakade-Wang-Yang Lower Bounds"
      },
      {
        "paperId": "86687ad06378954f57cc01922b0369d97e75fd19",
        "title": "Frequentist Regret Bounds for Randomized Least-Squares Value Iteration"
      },
      {
        "paperId": "3606c313dccd21f784bedf5759152a3677f5719d",
        "title": "Behaviour Suite for Reinforcement Learning"
      },
      {
        "paperId": "d423fa6cd0f4d088941d9fe4bebd834d0137c9b9",
        "title": "Provably Efficient Reinforcement Learning with Linear Function Approximation"
      },
      {
        "paperId": "855f943165013e0897b9f2bc54f6748ba08aef2a",
        "title": "Stochastic Gradient and Langevin Processes"
      },
      {
        "paperId": "e5e440562aa293b43cd95a5b52dd705fbf88115b",
        "title": "Worst-Case Regret Bounds for Exploration via Randomized Value Functions"
      },
      {
        "paperId": "a4af0eb16016c5e1f36bafb7291a8a4932c0f17c",
        "title": "Reinforcement Learning in Feature Space: Matrix Bandit, Kernels, and Regret Bound"
      },
      {
        "paperId": "eeddb10e32c501770a6f63beb25f1fa5e402cb1b",
        "title": "Sampling from Non-Log-Concave Distributions via Variance-Reduced Gradient Langevin Dynamics"
      },
      {
        "paperId": "12cd30cab8281567f5a1e8e3145641336fbb819c",
        "title": "Sample-Optimal Parametric Q-Learning Using Linearly Additive Features"
      },
      {
        "paperId": "a02ed13d218b62ae2045ae822db94cde3c5f94a2",
        "title": "Where Did My Optimum Go?: An Empirical Analysis of Gradient Descent Optimization in Policy Gradient Methods"
      },
      {
        "paperId": "4cb3fd057949624aa4f0bbe7a6dcc8777ff04758",
        "title": "Exploration by Random Network Distillation"
      },
      {
        "paperId": "03cc81e98942bdafd994af7a1d1e62a68ff8b682",
        "title": "Is Q-learning Provably Efficient?"
      },
      {
        "paperId": "d85623ffae865f9ef386644dd02d0ea2d6a8c8de",
        "title": "Implicit Quantile Networks for Distributional Reinforcement Learning"
      },
      {
        "paperId": "f802802b3af5a22b79ac65d033ba3cbee33da91b",
        "title": "Randomized Prior Functions for Deep Reinforcement Learning"
      },
      {
        "paperId": "3f5eefd759da6e85feb55134f5ad7b1f4af8ee3d",
        "title": "Efficient Exploration Through Bayesian Deep Q-Networks"
      },
      {
        "paperId": "fe3e91e40a950c6b6601b8f0a641884774d949ae",
        "title": "Distributional Reinforcement Learning with Quantile Regression"
      },
      {
        "paperId": "6f7cdcf98f559d93f33c8b6f0f376f0c8c8cfab2",
        "title": "Global Convergence of Langevin Dynamics Based Algorithms for Nonconvex Optimization"
      },
      {
        "paperId": "c1f4ef741242d629d1f56e442a09a7ba29595a0e",
        "title": "A Distributional Perspective on Reinforcement Learning"
      },
      {
        "paperId": "3fab4d28be5ff73f42c8b307117aa8d680345ad9",
        "title": "Noisy Networks for Exploration"
      },
      {
        "paperId": "142497432fe179ddb6ffe600c64a837ec6179550",
        "title": "Parameter Space Noise for Exploration"
      },
      {
        "paperId": "a441728f9fd6af1946368240162a72c2028c8cb1",
        "title": "Deep Exploration via Randomized Value Functions"
      },
      {
        "paperId": "67d10cea808937089d6492acff14ad9ef156e3c5",
        "title": "Minimax Regret Bounds for Reinforcement Learning"
      },
      {
        "paperId": "12f67fb182bc934fc95ce97acff553d83e2ca72e",
        "title": "Count-Based Exploration with Neural Density Models"
      },
      {
        "paperId": "a0ebf91e3d32980c73022011236d8a7a172c4894",
        "title": "Variance Reduction in Stochastic Gradient Langevin Dynamics"
      },
      {
        "paperId": "0fcb2034e31a2bc2f12a2b1363d0d77baf445fdf",
        "title": "#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning"
      },
      {
        "paperId": "809d399f604d5494a01910d8a1fb8b6aefc3da1e",
        "title": "Linear Thompson Sampling Revisited"
      },
      {
        "paperId": "88909a57da9a43ceb52aae8424b1f348dba99cab",
        "title": "Why is Posterior Sampling Better than Optimism for Reinforcement Learning?"
      },
      {
        "paperId": "6e90fd78e8a3b98af3954aae5209703aa966603e",
        "title": "Unifying Count-Based Exploration and Intrinsic Motivation"
      },
      {
        "paperId": "4b63e34276aa98d5345efa7fe09bb06d8a9d8f52",
        "title": "Deep Exploration via Bootstrapped DQN"
      },
      {
        "paperId": "0423bbead494ea145f572e612b2251a6c97a63ec",
        "title": "Preconditioned Stochastic Gradient Langevin Dynamics for Deep Neural Networks"
      },
      {
        "paperId": "e55cfc9f3fbca6f4c519df0a73adeb0c5bce8974",
        "title": "On the Convergence of Stochastic Gradient MCMC Algorithms with High-Order Integrators"
      },
      {
        "paperId": "c6170fa90d3b2efede5a2e1660cb23e1c824f2ca",
        "title": "Prioritized Experience Replay"
      },
      {
        "paperId": "3b9732bb07dc99bde5e1f9f75251c6ea5039373e",
        "title": "Deep Reinforcement Learning with Double Q-Learning"
      },
      {
        "paperId": "340f48901f72278f6bf78a04ee5b01df208cc508",
        "title": "Human-level control through deep reinforcement learning"
      },
      {
        "paperId": "c6db2e81610c722d5b51fd7febb87bcaa387d86d",
        "title": "Theoretical guarantees for approximate sampling from smooth and log\u2010concave densities"
      },
      {
        "paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8",
        "title": "Adam: A Method for Stochastic Optimization"
      },
      {
        "paperId": "aaee41270a6397024a68188dbf61614d87f891fb",
        "title": "Consistency and Fluctuations For Stochastic Gradient Langevin Dynamics"
      },
      {
        "paperId": "981ce6b655cc06416ff6bf7fac8c6c2076fd7fac",
        "title": "Identifying and attacking the saddle point problem in high-dimensional non-convex optimization"
      },
      {
        "paperId": "1389772b8a0f9c7fc43057f9da41a7d0ebf0308b",
        "title": "Generalization and Exploration via Randomized Value Functions"
      },
      {
        "paperId": "044f8706a3b078cfe9c5d1448e42293b33ccf26a",
        "title": "Analysis and Geometry of Markov Diffusion Operators"
      },
      {
        "paperId": "789783016fb708abbc061790612ebe91273c05d3",
        "title": "(More) Efficient Reinforcement Learning via Posterior Sampling"
      },
      {
        "paperId": "1abe50532f9359e90947224f8588bde00e8fe790",
        "title": "I. INTRODUCTION"
      },
      {
        "paperId": "f26f1a3c034b96514fc092dee99acacedd9c380b",
        "title": "Thompson Sampling for Contextual Bandits with Linear Payoffs"
      },
      {
        "paperId": "f82e4ff4f003581330338aaae71f60316e58dd26",
        "title": "The Arcade Learning Environment: An Evaluation Platform for General Agents"
      },
      {
        "paperId": "6bea71fa6deb19c67e9586428f8f240e789fb3df",
        "title": "Improved Algorithms for Linear Stochastic Bandits"
      },
      {
        "paperId": "aeed631d6a84100b5e9a021ec1914095c66de415",
        "title": "Bayesian Learning via Stochastic Gradient Langevin Dynamics"
      },
      {
        "paperId": "3b0821ff22fdffc95b0caae1f9660773eb54dc52",
        "title": "Handbook of Markov Chain Monte Carlo"
      },
      {
        "paperId": "644a079073969a92674f69483c4a85679d066545",
        "title": "Double Q-learning"
      },
      {
        "paperId": "9506b3c0ca4ccf611f27199df81259ca486a8745",
        "title": "\u5f15\u8a00 (Introduction)"
      },
      {
        "paperId": "0cafe2903b097fc042782c359cb231ea34ef7ed3",
        "title": "Near-optimal Regret Bounds for Reinforcement Learning"
      },
      {
        "paperId": "237a1cf18ed83bb3ad852b34f443c6c1ff3336c1",
        "title": "An analysis of model-based Interval Estimation for Markov Decision Processes"
      },
      {
        "paperId": "2818b562aff51d12b08ee6144a8682416343e8f3",
        "title": "Langevin Diffusions and Metropolis-Hastings Algorithms"
      },
      {
        "paperId": "1e1d35136b1bf3b13ef6b53f6039f39d9ee820e3",
        "title": "Finite-time Analysis of the Multiarmed Bandit Problem"
      },
      {
        "paperId": "48cce5ee49facf75eeb12832c387452424b645dd",
        "title": "A Bayesian Framework for Reinforcement Learning"
      },
      {
        "paperId": "0dffd5c04f5830deadd4fba16ee1575abc5ee051",
        "title": "Exponential convergence of Langevin distributions and their discrete approximations"
      },
      {
        "paperId": "721f54f6fa32f5f02c5124a2b73ce5f4280b4eaf",
        "title": "Matrix analysis"
      },
      {
        "paperId": "ee2cd1d17f833d3c157a1016a778c7c22af555a2",
        "title": "ON THE LIKELIHOOD THAT ONE UNKNOWN PROBABILITY EXCEEDS ANOTHER IN VIEW OF THE EVIDENCE OF TWO SAMPLES"
      },
      {
        "paperId": "e02f1d1c368553e0a49317e453ee9cb91f6296a1",
        "title": "Online RL in Linearly q\u03c0-Realizable MDPs Is as Easy as in Linear MDPs If You Learn What to Ignore"
      },
      {
        "paperId": "4c4d41a977e6d8d82b6cc19ac28e50de3c814fd8",
        "title": "HyperDQN: A Randomized Exploration Method for Deep Reinforcement Learning"
      },
      {
        "paperId": null,
        "title": "Let A \u2208 Rd\u00d7d be a positive definite matrix where its largest eigenvalue \u03bbmax(A) \u2264 \u03bb"
      },
      {
        "paperId": "4ecdd8776a75c5fe904ac2b24215b0f3ee3ec4c1",
        "title": "On Approximate Thompson Sampling with Langevin Algorithms"
      },
      {
        "paperId": null,
        "title": "Covering numbers and self-normalized processes Lemma C.8"
      },
      {
        "paperId": null,
        "title": "Inequalities for summations Lemma"
      },
      {
        "paperId": null,
        "title": "DQN Zoo: Reference implementations of DQN-based agents, 2020"
      },
      {
        "paperId": "3c993f1501177a4b5aa2a89e91b1b2ed7d358928",
        "title": "Subsampled Stochastic Variance-Reduced Gradient Langevin Dynamics"
      },
      {
        "paperId": null,
        "title": "st}) and \u03c6therm(st"
      },
      {
        "paperId": null,
        "title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude"
      },
      {
        "paperId": "97efafdb4a3942ab3efba53ded7413199f79c054",
        "title": "Reinforcement Learning: An Introduction"
      },
      {
        "paperId": "987312496fb243bb56518fb9ecfa051e9df8f35d",
        "title": "Brownian dynamics as smart Monte Carlo simulation"
      },
      {
        "paperId": "267d384a9a5b348cd6073e49f8b5bc288a21712f",
        "title": "Handbook of Mathematical Functions with Formulas, Graphs,"
      },
      {
        "paperId": "9443908296fa4f90975647e59534fc6fd2e71cdc",
        "title": "\u039b LANGEVIN"
      }
    ],
    "cited_by": [
      {
        "paperId": "76c9ba86f00603c8562498ed78f91083513b62e6",
        "title": "Priors Matter: Addressing Misspecification in Bayesian Deep Q-Learning"
      },
      {
        "paperId": "fc450c5aa4c85224e9944e65daaccc971470113d",
        "title": "Thompson Sampling in Online RLHF with General Function Approximation"
      },
      {
        "paperId": "2b99f1383a719b8ffc6c4a8b5b2b7cba12a6ec32",
        "title": "Universal Value-Function Uncertainties"
      },
      {
        "paperId": "fd2efe2ef4ddd5b62d4103450c8596a1206edda2",
        "title": "Toward Efficient Exploration by Large Language Model Agents"
      },
      {
        "paperId": "fe01586174cdc2a5786c83248f9f460d4ea2ab43",
        "title": "CAE: Repurposing the Critic as an Explorer in Deep Reinforcement Learning"
      },
      {
        "paperId": "326f3b22fb91661048ea24da2e47a78f091430c8",
        "title": "IL-SOAR : Imitation Learning with Soft Optimistic Actor cRitic"
      },
      {
        "paperId": "7846a633d03299acc1843f4137b4910a9cda5366",
        "title": "Muti-Fidelity Prediction and Uncertainty Quantification with Laplace Neural Operators for Parametric Partial Differential Equations"
      },
      {
        "paperId": "e861a51688b6fc086854161c50eef9b04cf11e02",
        "title": "Langevin Soft Actor-Critic: Efficient Exploration through Uncertainty-Driven Critic Learning"
      },
      {
        "paperId": "1617d8895aa0db738077a07e883ceeb2329b6d24",
        "title": "Concurrent Learning with Aggregated States via Randomized Least Squares Value Iteration"
      },
      {
        "paperId": "023ac9657725b307f7460471b2f3caac20948cfd",
        "title": "Isoperimetry is All We Need: Langevin Posterior Sampling for RL with Sublinear Regret"
      },
      {
        "paperId": "0eff31517a78ee87e7d38f5efb70004949c61f6e",
        "title": "Upper and Lower Bounds for Distributionally Robust Off-Dynamics Reinforcement Learning"
      },
      {
        "paperId": "d06737f9395e592f35ef251e09bea1c18037b096",
        "title": "Random Latent Exploration for Deep Reinforcement Learning"
      },
      {
        "paperId": "39d2839aa4c3d8c0e64553891fe98ba261703154",
        "title": "More Efficient Randomized Exploration for Reinforcement Learning via Approximate Sampling"
      },
      {
        "paperId": "4c6db31f46558a56af2b3d2830d1306a8a94c714",
        "title": "Computationally Efficient RL under Linear Bellman Completeness for Deterministic Dynamics"
      },
      {
        "paperId": "ed8246ebe105c0b52441fb5e25efbc3260354605",
        "title": "Sequential Decision Making with Expert Demonstrations under Unobserved Heterogeneity"
      },
      {
        "paperId": "dd089f7e75f8e7fc12b0e8051b328fa6779d9632",
        "title": "Prior-dependent analysis of posterior sampling reinforcement learning with function approximation"
      },
      {
        "paperId": "f2adbfe8450ddb406f4082b6c75006eb0a09523c",
        "title": "Distributionally Robust Off-Dynamics Reinforcement Learning: Provable Efficiency with Linear Function Approximation"
      },
      {
        "paperId": "7c8ed72733835684c86420e168b2cb5515d42115",
        "title": "Q-Star Meets Scalable Posterior Sampling: Bridging Theory and Practice via HyperAgent"
      },
      {
        "paperId": "10fe645c0ffc41edaed4ab5bb425660d9a70002d",
        "title": "Accelerating Approximate Thompson Sampling with Underdamped Langevin Monte Carlo"
      },
      {
        "paperId": "94324b139ed86982a60e380eed381a94141355a6",
        "title": "Finite-Time Frequentist Regret Bounds of Multi-Agent Thompson Sampling on Sparse Hypergraphs"
      },
      {
        "paperId": "8dc78151c3567e8820bd1176adad2bdb1dcd8f1f",
        "title": "Langevin DQN"
      },
      {
        "paperId": "db59b22ae9556458d98107297736f640997c31be",
        "title": "Robust exploration with adversary via Langevin Monte Carlo"
      },
      {
        "paperId": "916fe86b549659df30f73d409803e59a71c6a59b",
        "title": "Bayesian Ensembles for Exploration in Deep Q-Learning"
      },
      {
        "paperId": "47049bc0c666acddba1bb1fa09e3e4b6a4cae7c7",
        "title": "HyperAgent: A Simple, Scalable, Efficient and Provable Reinforcement Learning Framework for Complex Environments"
      }
    ],
    "score": 12.0
  },
  {
    "id": "53db22a9d4ae77dd8218ba867184898adc84d1d1",
    "title": "Sample Efficient Offline-to-Online Reinforcement Learning",
    "authors": [
      "Siyuan Guo",
      "Lixin Zou",
      "Hechang Chen",
      "B. Qu",
      "Haotian Chi",
      "Philip S. Yu",
      "Yi Chang"
    ],
    "year": 2024,
    "citationCount": 12,
    "abstract": "Offline reinforcement learning (RL) makes it possible to train the agents entirely from a previously collected dataset. However, constrained by the quality of the offline dataset, offline RL agents typically have limited performance and cannot be directly deployed. Thus, it is desirable to further finetune the pretrained offline RL agents via online interactions with the environment. Existing offline-to-online RL algorithms suffer from the low sample efficiency issue, due to two inherent challenges, i.e., exploration limitation and distribution shift. To this end, we propose a sample-efficient offline-to-online RL algorithm via Optimistic Exploration and Meta Adaptation (OEMA). Specifically, we first propose an optimistic exploration strategy according to the principle of optimism in the face of uncertainty. This allows agents to sufficiently explore the environment in a stable manner. Moreover, we propose a meta learning based adaptation method, which can reduce the distribution shift and accelerate the offline-to-online adaptation process. We empirically demonstrate that OEMA improves the sample efficiency on D4RL benchmark. Besides, we provide in-depth analyses to verify the effectiveness of both optimistic exploration and meta adaptation.",
    "url": "https://www.semanticscholar.org/paper/53db22a9d4ae77dd8218ba867184898adc84d1d1",
    "pdf_url": "https://doi.org/10.1109/TKDE.2023.3302804",
    "venue": "IEEE Transactions on Knowledge and Data Engineering",
    "publicationDate": "2024-03-01",
    "externalIds": {
      "DBLP": "journals/tkde/GuoZCQCYC24",
      "DOI": "10.1109/TKDE.2023.3302804",
      "CorpusId": 267524234
    },
    "references": [],
    "cited_by": [
      {
        "paperId": "aec5c98eb7fd986b2f69ee9944fcccf19fd7eab0",
        "title": "An Improved Stable Fine-Tuning framework for offline-to-online reinforcement learning"
      },
      {
        "paperId": "75855becab033bad3b7dd8610d729762607227a8",
        "title": "An offline-to-online reinforcement learning framework with trajectory-guided exploration for industrial process control"
      },
      {
        "paperId": "18530ef1b25c4225d5caffa3ca330c934810782d",
        "title": "Decision\u2010Making Problem for Two\u2010Player Markov Game: Perspective of Feedback Control"
      },
      {
        "paperId": "3cdc2cf475e49537dde52b1ff95120916010e5d8",
        "title": "Offline-to-online reinforcement learning with efficient unconstrained fine-tuning."
      },
      {
        "paperId": "a57d9e1bbe510c7dd8411e30252b02e87b903570",
        "title": "Dynamic Compressing Prompts for Efficient Inference of Large Language Models"
      },
      {
        "paperId": "90d1b91904f20a056719ffaeffe5ef84238b3649",
        "title": "DRDT3: Diffusion-Refined Decision Test-Time Training Model"
      },
      {
        "paperId": "76782de39572ae1063bc72b53f5f1f2974891bb6",
        "title": "Robustness Evaluation of Offline Reinforcement Learning for Robot Control Against Action Perturbations"
      },
      {
        "paperId": "ea372d30521abee28843b64df13f3364bf55cec9",
        "title": "Safe Imitation Learning-based Optimal Energy Storage Systems Dispatch in Distribution Networks"
      },
      {
        "paperId": "7257d3d3488d3d670237d1814b8775b8c48cf08c",
        "title": "Selective imitation for efficient online reinforcement learning with pre-collected data"
      },
      {
        "paperId": "033b96503d85082445fc724b91d5ab252934418f",
        "title": "Demonstration Guided Multi-Objective Reinforcement Learning"
      },
      {
        "paperId": "7d83372c7c66fb2f64c25b1776d59739239743bd",
        "title": "Temporal Logic Specification-Conditioned Decision Transformer for Offline Safe Reinforcement Learning"
      },
      {
        "paperId": "8fb9daf1dca69d8196454b31fcd6c5736b32ac31",
        "title": "Enhancing Robustness of Graph Neural Networks on Social Media with Explainable Inverse Reinforcement Learning"
      }
    ],
    "score": 12.0
  },
  {
    "id": "fbc0b5e1b822796d7ae97268def2e0993b5da644",
    "title": "Algorithm Discovery With LLMs: Evolutionary Search Meets Reinforcement Learning",
    "authors": [
      "Anja Surina",
      "Amin Mansouri",
      "Lars Quaedvlieg",
      "Amal Seddas",
      "Maryna Viazovska",
      "Emmanuel Abbe",
      "Caglar Gulcehre"
    ],
    "year": 2025,
    "citationCount": 12,
    "abstract": "Discovering efficient algorithms for solving complex problems has been an outstanding challenge in mathematics and computer science, requiring substantial human expertise over the years. Recent advancements in evolutionary search with large language models (LLMs) have shown promise in accelerating the discovery of algorithms across various domains, particularly in mathematics and optimization. However, existing approaches treat the LLM as a static generator, missing the opportunity to update the model with the signal obtained from evolutionary exploration. In this work, we propose to augment LLM-based evolutionary search by continuously refining the search operator - the LLM - through reinforcement learning (RL) fine-tuning. Our method leverages evolutionary search as an exploration strategy to discover improved algorithms, while RL optimizes the LLM policy based on these discoveries. Our experiments on combinatorial optimization tasks demonstrate that integrating RL with evolutionary search accelerates the discovery of superior algorithms, showcasing the potential of RL-enhanced evolutionary strategies for algorithm design.",
    "url": "https://www.semanticscholar.org/paper/fbc0b5e1b822796d7ae97268def2e0993b5da644",
    "pdf_url": "https://arxiv.org/pdf/2504.05108.pdf",
    "venue": "arXiv.org",
    "publicationDate": "2025-04-07",
    "externalIds": {
      "ArXiv": "2504.05108",
      "DBLP": "journals/corr/abs-2504-05108",
      "DOI": "10.48550/arXiv.2504.05108",
      "CorpusId": 277621098
    },
    "references": [
      {
        "paperId": "ffa99ecba6035e38c68c4b7495d43dba6738b93f",
        "title": "NeoBERT: A Next-Generation BERT"
      },
      {
        "paperId": "3bf54817fb52f161fe6f30f13320470f18ea7db0",
        "title": "Amplifying human performance in combinatorial competitive programming"
      },
      {
        "paperId": "40e8af970329135ec95057d73e239dab805ad128",
        "title": "The Llama 3 Herd of Models"
      },
      {
        "paperId": "603d3f90fc40f79ff51258f0295de3ec5107f73e",
        "title": "AI models collapse when trained on recursively generated data"
      },
      {
        "paperId": "899f6c5ee479aa81d66e94812e3fa84a06a8cf7d",
        "title": "Discovering Preference Optimization Algorithms with and for Large Language Models"
      },
      {
        "paperId": "07d05f5e230ee5613bc287ab92d5452cc3af99b0",
        "title": "Iterative Reasoning Preference Optimization"
      },
      {
        "paperId": "860ba78f9789bbfc99c299b18558ca19430d8fea",
        "title": "LLM-SR: Scientific Equation Discovery via Programming with Large Language Models"
      },
      {
        "paperId": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
        "title": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      },
      {
        "paperId": "46566ef7e51987cd101bf2b275c650cb3be21995",
        "title": "Human Alignment of Large Language Models through Online Preference Optimisation"
      },
      {
        "paperId": "be7a88babf78512b545f585517704cb597388cbc",
        "title": "ReEvo: Large Language Models as Hyper-Heuristics with Reflective Evolution"
      },
      {
        "paperId": "a2e128be33b85f8f91515e205bf0237ecc0546cc",
        "title": "Evolution of Heuristics: Towards Efficient Automatic Algorithm Design Using Large Language Model"
      },
      {
        "paperId": "d32ba88571141ed0ebe7aeefbaa4ccaf8cda7be3",
        "title": "Mathematical discoveries from program search with large language models"
      },
      {
        "paperId": "48362b169a235ca650918c489c8cea4c597da645",
        "title": "Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models"
      },
      {
        "paperId": "6a4719871c5d51ad2cd140561652500df06409bd",
        "title": "Algorithm Evolution Using Large Language Model"
      },
      {
        "paperId": "1c259361caa85c2d95a7d04e5e42fa98693da85b",
        "title": "Large Language Models as Evolutionary Optimizers"
      },
      {
        "paperId": "6ca16c1c2c60ceda87242c8f8e522d12cc4a13bc",
        "title": "Eureka: Human-Level Reward Design via Coding Large Language Models"
      },
      {
        "paperId": "3c2f14a28ba5e826c63c294f4cca01a8217a217e",
        "title": "Neural Combinatorial Optimization with Heavy Decoder: Toward Large Scale Generalization"
      },
      {
        "paperId": "cb3968152f7d93f53d24b00279a90d5071ddc85a",
        "title": "Understanding the Effects of RLHF on LLM Generalisation and Diversity"
      },
      {
        "paperId": "30cf1bab6cc3124015779c3dcde5b0fa7543c20d",
        "title": "NeuralGLS: learning to guide local search with graph convolutional network for the traveling salesman problem"
      },
      {
        "paperId": "7fe071ea76e49bc3e573beb53f07721630954247",
        "title": "Promptbreeder: Self-Referential Self-Improvement Via Prompt Evolution"
      },
      {
        "paperId": "860c8de4fdac38695ff6860dd15312f1079c6117",
        "title": "Beyond Reverse KL: Generalizing Direct Preference Optimization with Diverse Divergence Constraints"
      },
      {
        "paperId": "f8a2dca1e8fe56e698984c077f7ff58d8ca867e9",
        "title": "Large Language Models as Optimizers"
      },
      {
        "paperId": "182c7b40ff7560a5545764814338f55a2098e441",
        "title": "Reinforced Self-Training (ReST) for Language Modeling"
      },
      {
        "paperId": "6eb46737bf0ef916a7f906ec6a8da82a45ffb623",
        "title": "Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback"
      },
      {
        "paperId": "713ebabceaa61d68be611757ffe00ce794dbf691",
        "title": "Jumanji: a Diverse Suite of Scalable Reinforcement Learning Environments in JAX"
      },
      {
        "paperId": "0d1c76d45afa012ded7ab741194baf142117c495",
        "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
      },
      {
        "paperId": "442ab95eb9cfbc03bb17a27b52313b5d25eaa738",
        "title": "Discovering faster matrix multiplication algorithms with reinforcement learning"
      },
      {
        "paperId": "4baac6b8fa7731352004bc45d2ba2b6bbd04a4e7",
        "title": "Evolution through Large Models"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "1b6e810ce0afd0dd093f789d2b2742d047e316d5",
        "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models"
      },
      {
        "paperId": "a8ca46b171467ceb2d7652fbfb67fe701ad86092",
        "title": "LoRA: Low-Rank Adaptation of Large Language Models"
      },
      {
        "paperId": "e3f453ee3d2e084cb0f24769ead763844dbc0661",
        "title": "Efficient Active Search for Combinatorial Optimization Problems"
      },
      {
        "paperId": "d568557a2c75e879373fd6b62e020cf86037a6c7",
        "title": "Deep Policy Dynamic Programming for Vehicle Routing Problems"
      },
      {
        "paperId": "50eec8da1acc52a7c4a0a2c527d7696a41bbdc22",
        "title": "Generalize a Small Pre-trained Model to Arbitrarily Large TSP Instances"
      },
      {
        "paperId": "246a68fcf2bcd1db8f0184129b63123d1f98eaa3",
        "title": "POMO: Policy Optimization with Multiple Optima for Reinforcement Learning"
      },
      {
        "paperId": "053b1d7b97eb2c91fc3921d589c160b0923c70b1",
        "title": "Learning to summarize from human feedback"
      },
      {
        "paperId": "57ffba5ea5e602b5365752e19a6f85a716db1d8b",
        "title": "Bandit Algorithms"
      },
      {
        "paperId": "7d45afc1f12cba92a66da4e83a96380ac0a15997",
        "title": "Learning the travelling salesperson problem requires rethinking generalization"
      },
      {
        "paperId": "5e7bc93622416f14e6948a500278bfbe58cd3890",
        "title": "Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems"
      },
      {
        "paperId": "b19729b27a1b4c24b52f87308c907653300afa7f",
        "title": "Dota 2 with Large Scale Deep Reinforcement Learning"
      },
      {
        "paperId": "04636d0edc2c6822af5119bef83efdc63b65dc14",
        "title": "Neural Large Neighborhood Search for the Capacitated Vehicle Routing Problem"
      },
      {
        "paperId": "361c00b22e29d0816ca896513d2c165e26399821",
        "title": "Grandmaster level in StarCraft II using multi-agent reinforcement learning"
      },
      {
        "paperId": "f598a8afec169c435e48ad19356c3b768f8ce7a7",
        "title": "An Efficient Graph Convolutional Network Technique for the Travelling Salesman Problem"
      },
      {
        "paperId": "ebd487d064811871edba8f91abaca73332229c04",
        "title": "Knowledge-guided local search for the vehicle routing problem"
      },
      {
        "paperId": "cf4aa38ae31b43fd07abe13b4ffdb265babb7be1",
        "title": "The Curious Case of Neural Text Degeneration"
      },
      {
        "paperId": "69cb36923959f409220d2cf40e40c6c276f18d07",
        "title": "New applications of the polynomial method: The cap set conjecture and beyond"
      },
      {
        "paperId": "f2b840c3c14b9f05106589c1be0a2dd4a494c0ba",
        "title": "Learning to Perform Local Rewriting for Combinatorial Optimization"
      },
      {
        "paperId": "ce4f001c1d8ddb9a95cf54e14240ef02c44bd329",
        "title": "Attention, Learn to Solve Routing Problems!"
      },
      {
        "paperId": "d07284a6811f1b2745d91bdb06b040b57f226882",
        "title": "Decoupled Weight Decay Regularization"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "d7878c2044fb699e0ce0cad83e411824b1499dc8",
        "title": "Neural Combinatorial Optimization with Reinforcement Learning"
      },
      {
        "paperId": "769ef3d5021cd71c37d2c403f231a53d1accf786",
        "title": "An overview of gradient descent optimization algorithms"
      },
      {
        "paperId": "846aedd869a00c09b40f1f1f35673cb22bc87490",
        "title": "Mastering the game of Go with deep neural networks and tree search"
      },
      {
        "paperId": "c6170fa90d3b2efede5a2e1660cb23e1c824f2ca",
        "title": "Prioritized Experience Replay"
      },
      {
        "paperId": "9653d5c2c7844347343d073bbedd96e05d52f69b",
        "title": "Pointer Networks"
      },
      {
        "paperId": "340f48901f72278f6bf78a04ee5b01df208cc508",
        "title": "Human-level control through deep reinforcement learning"
      },
      {
        "paperId": "6e0a00ddb630f121f6e35e4d2bbcc0c5e30f62c1",
        "title": "Guided Local Search"
      },
      {
        "paperId": "051edcf23021de14f8dd30f967198307d6d9a6e5",
        "title": "The traveling salesman problem"
      },
      {
        "paperId": "96f50d4fdec8aaec1d3486223fcd3e5e3d0098d2",
        "title": "Guided local search and its application to the traveling salesman problem"
      },
      {
        "paperId": "a89dc7bf47b17b54f3998616160047a3ae50b481",
        "title": "OR-Library: Distributing Test Problems by Electronic Mail"
      },
      {
        "paperId": "2c62b5fe3a8c7c4c5ee00b700b04b2d12a0bb9c7",
        "title": "Lower bounds and reduction procedures for the bin packing problem"
      },
      {
        "paperId": "c61134ada9f0e3f3373d635c31a8b3caa37f9977",
        "title": "Genetic algorithms and Machine Learning"
      },
      {
        "paperId": "052b1d8ce63b07fec3de9dbb583772d860b7c769",
        "title": "Learning representations by back-propagating errors"
      },
      {
        "paperId": "dd5061631a4d11fa394f4421700ebf7e78dcbc59",
        "title": "Optimization by Simulated Annealing"
      },
      {
        "paperId": "5ff22e4e167401f3275dd126e9b5dff6992b55fd",
        "title": "Gradient Theory of Optimal Flight Paths"
      },
      {
        "paperId": "060a1239ba7a6b3d210384d6a95fc243bcd8c8ad",
        "title": "Can Large Language Models Invent Algorithms to Improve Themselves?"
      },
      {
        "paperId": "7d1fb8ba683189d6d0fa2df0488bba2de1770355",
        "title": "Jumanji: a Diverse Suite of Scalable Reinforcement Learning Environments in JAX"
      },
      {
        "paperId": "8d17234680db76f99efd22fbcb169f45d2d79d93",
        "title": "Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers"
      },
      {
        "paperId": "168526097c6c1e8a2010ce9d2ec434319a5fb948",
        "title": "Learning a Latent Search Space for Routing Problems using Variational Autoencoders"
      },
      {
        "paperId": "9e78c07733bf4470b64922aff85a0dbf1e07e9ab",
        "title": "RL Unplugged: A Collection of Benchmarks for Offline Reinforcement Learning"
      },
      {
        "paperId": null,
        "title": "elkai: A python library for solving travelling salesman problems"
      },
      {
        "paperId": "0c0a778e6fdf7e36b1750c533dcc916f86608607",
        "title": "A Survey on Context Learning"
      },
      {
        "paperId": "1c46943103bd7b7a2c7be86859995a4144d1938b",
        "title": "Visualizing Data using t-SNE"
      },
      {
        "paperId": "a13fcba4fab4713d5acb2d970eddc6b148d2596d",
        "title": "A Survey of Parallel Genetic Algorithms"
      },
      {
        "paperId": "7bea855e19fd13461590e4f2d44bbf7b807ce3e3",
        "title": "A bitter lesson."
      },
      {
        "paperId": "97efafdb4a3942ab3efba53ded7413199f79c054",
        "title": "Reinforcement Learning: An Introduction"
      },
      {
        "paperId": "cc1ba7592f61a92b2581fce4504349453be52ef5",
        "title": "Distributed genetic algorithms for function optimization"
      },
      {
        "paperId": "09f15a930dd9f9e004e48e39543f9decb20e50f4",
        "title": "Approximation Algorithms for Bin-Packing \u2014 An Updated Survey"
      },
      {
        "paperId": "9cbc6bc1bec461f1bb7ddc4b68509ae9a4c6ef6b",
        "title": "Theory of Error-correcting Codes"
      },
      {
        "paperId": null,
        "title": "max_balance_bonus: Maximum bonus for a perfectly balanced bin"
      },
      {
        "paperId": null,
        "title": "Accelerate: Training and inference at scale made simple, efficient and adaptable"
      },
      {
        "paperId": null,
        "title": "urgency_trend_weight: Weight to emphasize urgency trends in the bin allocation strategy"
      },
      {
        "paperId": null,
        "title": "time_weight"
      },
      {
        "paperId": null,
        "title": "Hugging Face"
      },
      {
        "paperId": null,
        "title": "Granite 3.0 language models"
      },
      {
        "paperId": null,
        "title": "real_time_optimization_step: Adjustment factor in real-time optimization"
      }
    ],
    "cited_by": [
      {
        "paperId": "896061a2a65a701f74c3cda9a00a7a85d4a1aba8",
        "title": "Recursive Self-Aggregation Unlocks Deep Thinking in Large Language Models"
      },
      {
        "paperId": "f727446279a030a572a77205746a2e3e87bd480c",
        "title": "AutoEP: LLMs-Driven Automation of Hyperparameter Evolution for Metaheuristic Algorithms"
      },
      {
        "paperId": "c3e6fd0fdd93e33abf551837effd93440facd439",
        "title": "GeoEvolve: Automating Geospatial Model Discovery via Multi-Agent Large Language Models"
      },
      {
        "paperId": "7cada3bf16747950e37e7c59689749a6600ca9f1",
        "title": "Large Language Models and Operations Research: A Structured Survey"
      },
      {
        "paperId": "7457ebb00722bfec20b904bb0100dadc61abfb7f",
        "title": "Data-Driven Discovery of Interpretable Kalman Filter Variants through Large Language Models and Genetic Programming"
      },
      {
        "paperId": "7f5b2ddc683b7b4be98d327ae006a85c0b6081c5",
        "title": "MiGrATe: Mixed-Policy GRPO for Adaptation at Test-Time"
      },
      {
        "paperId": "1c93729fa65d9a7523a3b4fb471295f46310214f",
        "title": "Fine-tuning Large Language Model for Automated Algorithm Design"
      },
      {
        "paperId": "ed32e5bb11a9e5e2f03ea805782e8670a6e10efd",
        "title": "AlphaEvolve: A coding agent for scientific and algorithmic discovery"
      },
      {
        "paperId": "9084c23b292b2cb02de6f63b040cad5bb09024af",
        "title": "LLM-Meta-SR: In-Context Learning for Evolving Selection Operators in Symbolic Regression"
      },
      {
        "paperId": "0131fa23b98f67b6a8a358d85b66fd3544d5b480",
        "title": "CALM: Co-evolution of Algorithms and Language Model for Automatic Heuristic Design"
      },
      {
        "paperId": "48f00745feca6545a293d9dbaee0f4c189121542",
        "title": "Adaptive Self-Improvement for Smarter Energy Systems using Agentic Policy Search"
      },
      {
        "paperId": "c9d3305778a5d01d7634bf64dc51788b088ae01f",
        "title": "LLM-Meta-SR: Learning to Evolve Selection Operators for Symbolic Regression"
      }
    ],
    "score": 12.0
  },
  {
    "id": "97c7066b53b208b24001ad0dcc49a851fcf13bfd",
    "title": "DIME:Diffusion-Based Maximum Entropy Reinforcement Learning",
    "authors": [
      "Onur Celik",
      "Zechu Li",
      "Denis Blessing",
      "Ge Li",
      "Daniel Palanicek",
      "Jan Peters",
      "G. Chalvatzaki",
      "Gerhard Neumann"
    ],
    "year": 2025,
    "citationCount": 12,
    "abstract": "Maximum entropy reinforcement learning (MaxEnt-RL) has become the standard approach to RL due to its beneficial exploration properties. Traditionally, policies are parameterized using Gaussian distributions, which significantly limits their representational capacity. Diffusion-based policies offer a more expressive alternative, yet integrating them into MaxEnt-RL poses challenges-primarily due to the intractability of computing their marginal entropy. To overcome this, we propose Diffusion-Based Maximum Entropy RL (DIME). \\emph{DIME} leverages recent advances in approximate inference with diffusion models to derive a lower bound on the maximum entropy objective. Additionally, we propose a policy iteration scheme that provably converges to the optimal diffusion policy. Our method enables the use of expressive diffusion-based policies while retaining the principled exploration benefits of MaxEnt-RL, significantly outperforming other diffusion-based methods on challenging high-dimensional control benchmarks. It is also competitive with state-of-the-art non-diffusion based RL methods while requiring fewer algorithmic design choices and smaller update-to-data ratios, reducing computational complexity.",
    "url": "https://www.semanticscholar.org/paper/97c7066b53b208b24001ad0dcc49a851fcf13bfd",
    "pdf_url": "https://arxiv.org/pdf/2502.02316.pdf",
    "venue": "arXiv.org",
    "publicationDate": "2025-02-04",
    "externalIds": {
      "DBLP": "journals/corr/abs-2502-02316",
      "ArXiv": "2502.02316",
      "DOI": "10.48550/arXiv.2502.02316",
      "CorpusId": 276107334
    },
    "references": [
      {
        "paperId": "f265242fd76a20d393d492297507021f0c340681",
        "title": "Underdamped Diffusion Bridges with Applications to Sampling"
      },
      {
        "paperId": "739b5ccd9d84b8b36f95a989c292df7468013dc2",
        "title": "End-To-End Learning of Gaussian Mixture Priors for Diffusion Sampler"
      },
      {
        "paperId": "e861a51688b6fc086854161c50eef9b04cf11e02",
        "title": "Langevin Soft Actor-Critic: Efficient Exploration through Uncertainty-Driven Critic Learning"
      },
      {
        "paperId": "16733982a60e0446853f2268d1ac6b9b9cc3f75e",
        "title": "Sequential Controlled Langevin Diffusions"
      },
      {
        "paperId": "501226462aa57db2b4a86ad4e5425eac3687c298",
        "title": "Learned Reference-based Diffusion Sampling for multi-modal distributions"
      },
      {
        "paperId": "a72661ca3c4de880152e69c7e20383645eb20056",
        "title": "TOP-ERL: Transformer-based Off-Policy Episodic Reinforcement Learning"
      },
      {
        "paperId": "929ef9e22b9cb4cb6f169f22aa3c3d968890095e",
        "title": "Diffusion-DICE: In-Sample Diffusion Guidance for Offline Reinforcement Learning"
      },
      {
        "paperId": "7c0bbc6f2b4a195cbf31a121c2331551e1e1c12a",
        "title": "Variational Distillation of Diffusion Policies into Mixture of Experts"
      },
      {
        "paperId": "2628640e02d44ffbcaf35fe554432a7f9c2350ea",
        "title": "Learning Multimodal Behaviors from Scratch with Diffusion Policy Gradient"
      },
      {
        "paperId": "5e4d83bae388ba067a25815eb7776bbd12b56f24",
        "title": "Diffusion-based Reinforcement Learning via Q-weighted Variational Policy Optimization"
      },
      {
        "paperId": "cdf9897ed964180631e5b3752efde29369642e6e",
        "title": "Bigger, Regularized, Optimistic: scaling for compute and sample-efficient continuous control"
      },
      {
        "paperId": "88f0b92bdb29505055b73b751722da0d2cd3f0ab",
        "title": "Diffusion Actor-Critic with Entropy Regulator"
      },
      {
        "paperId": "435d459eee226fc08e6d00b08f2e61b10f40502f",
        "title": "S2AC: Energy-Based Reinforcement Learning with Stein Soft Actor Critic"
      },
      {
        "paperId": "9f0f134bc53f9aea130f1a561acb5fa6b08ae4ce",
        "title": "Iterated Denoising Energy Matching for Sampling from Boltzmann Densities"
      },
      {
        "paperId": "6a91263e4bd079499c58e2488bda5a561e69beeb",
        "title": "Learning a Diffusion Model Policy from Rewards via Q-Score Matching"
      },
      {
        "paperId": "3d3bbaf203e55e681ed1c6f59181abacff2b068c",
        "title": "On the Theory of Risk-Aware Agents: Bridging Actor-Critic and Economics"
      },
      {
        "paperId": "5a475fcb5fcee91437929545fbccb26c1d6464f1",
        "title": "Diffusion Generative Flow Samplers: Improving learning signals through partial trajectory optimization"
      },
      {
        "paperId": "a6513a213269116444f779fb9136799f66718778",
        "title": "Consistency Models as a Rich and Efficient Policy Class for Reinforcement Learning"
      },
      {
        "paperId": "4bc358371da87f033e42d71cc0895efac965910a",
        "title": "Transport meets Variational Inference: Controlled Monte Carlo Diffusions"
      },
      {
        "paperId": "d174e9d35a9d6d899acbc661e05a937a659ffc42",
        "title": "Improved sampling via learned diffusions"
      },
      {
        "paperId": "2aae2dd79ef30022da59d8c33f0b3afa55d9c20d",
        "title": "Efficient Diffusion Policies for Offline Reinforcement Learning"
      },
      {
        "paperId": "fe9fe9f15f24fbbb19b62bcd9a3418511a699b84",
        "title": "Policy Representation via Diffusion Probability Model for Reinforcement Learning"
      },
      {
        "paperId": "a6f9fb141034a87ff9d627dc8a3ef31d0790c6ed",
        "title": "IDQL: Implicit Q-Learning as an Actor-Critic Method with Diffusion Policies"
      },
      {
        "paperId": "1334a47e8f4e4ffd04ff534329d76a5e5cc16f46",
        "title": "Goal-Conditioned Imitation Learning using Score-based Diffusion Policies"
      },
      {
        "paperId": "bdba3bd30a49ea4c5b20b43dbd8f0eb59e9d80e2",
        "title": "Diffusion Policy: Visuomotor Policy Learning via Action Diffusion"
      },
      {
        "paperId": "3ee60ee899941c7056fc10e1b69e71bac0f11c97",
        "title": "Denoising Diffusion Samplers"
      },
      {
        "paperId": "5f8c041cb8985ee922625b5e65bece10e646f31f",
        "title": "An optimal control perspective on diffusion-based generative modeling"
      },
      {
        "paperId": "0f1402c536cc3cbbcb73b06f96289e50a34ca3cf",
        "title": "Offline Reinforcement Learning via High-Fidelity Generative Behavior Modeling"
      },
      {
        "paperId": "8d10253815e7f307e71fb108f983a1e22d7d1c9d",
        "title": "Langevin Diffusion Variational Inference"
      },
      {
        "paperId": "8290dfdfa59103da5871fa6f2defcb5a9a9b1e9d",
        "title": "Score-Based Diffusion meets Annealed Importance Sampling"
      },
      {
        "paperId": "2cbea7615ebecea2c414d8fbad47d5d258a5c3b4",
        "title": "Diffusion Policies as an Expressive Policy Class for Offline Reinforcement Learning"
      },
      {
        "paperId": "2f4c451922e227cbbd4f090b74298445bbd900d0",
        "title": "Elucidating the Design Space of Diffusion-Based Generative Models"
      },
      {
        "paperId": "faafdd8133e068eab317646eed07c0e5ec68f486",
        "title": "MyoSuite: A Contact-rich Simulation Suite for Musculoskeletal Motor Control"
      },
      {
        "paperId": "3ebdd3db0dd91069fa0cd31cbf8308b60b1b565e",
        "title": "Planning with Diffusion for Flexible Behavior Synthesis"
      },
      {
        "paperId": "69b80ce5ab6c2263b145b1eaa23664145938f351",
        "title": "The Primacy Bias in Deep Reinforcement Learning"
      },
      {
        "paperId": "713223e6a06773523195d1f3a9472f0fce940aea",
        "title": "Path Integral Sampler: a stochastic control approach for sampling"
      },
      {
        "paperId": "c930632c662231f768338c7424300b8ba5390c25",
        "title": "Bayesian learning via neural Schr\u00f6dinger\u2013F\u00f6llmer flows"
      },
      {
        "paperId": "558ca2e8c7eb56edd77a52b084e6cc24dffe5bcd",
        "title": "Deep Reinforcement Learning at the Edge of the Statistical Precipice"
      },
      {
        "paperId": "633e2fbfc0b21e959a244100937c5853afca4853",
        "title": "Score-Based Generative Modeling through Stochastic Differential Equations"
      },
      {
        "paperId": "8ba600c169f0d2422625822223976bce562eabe1",
        "title": "dm_control: Software and Tasks for Continuous Control"
      },
      {
        "paperId": "5c126ae3421f05768d8edd97ecd44b1364e2c99a",
        "title": "Denoising Diffusion Probabilistic Models"
      },
      {
        "paperId": "5e7bc93622416f14e6948a500278bfbe58cd3890",
        "title": "Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems"
      },
      {
        "paperId": "fa2d7908c5eac437b8f3f1000640632a283b7da2",
        "title": "Theoretical guarantees for sampling and inference in generative models with latent diffusions"
      },
      {
        "paperId": "d383d11c97d5702a10656f85009fd24ecd75c0ba",
        "title": "CrossQ: Batch Normalization in Deep Reinforcement Learning for Greater Sample Efficiency and Simplicity"
      },
      {
        "paperId": "12c0751b4f51ed833172a713b7e32390032ead93",
        "title": "Soft Actor-Critic Algorithms and Applications"
      },
      {
        "paperId": "089eb7d7e6524ed3038ee8287670c0ee88bb80d7",
        "title": "Efficient Gradient-Free Variational Inference using Policy Search"
      },
      {
        "paperId": "ccf8dd6f5602d0c2be46eab1bd0d04424aa060ef",
        "title": "Latent Space Policies for Hierarchical Reinforcement Learning"
      },
      {
        "paperId": "811df72e210e20de99719539505da54762a11c6d",
        "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor"
      },
      {
        "paperId": "c1f4ef741242d629d1f56e442a09a7ba29595a0e",
        "title": "A Distributional Perspective on Reinforcement Learning"
      },
      {
        "paperId": "2e7d1e21409a90e66106722506aeb434ee7a18f3",
        "title": "A unified view of entropy-regularized Markov decision processes"
      },
      {
        "paperId": "9172cd6c253edf7c3a1568e03577db20648ad0c4",
        "title": "Reinforcement Learning with Deep Energy-Based Policies"
      },
      {
        "paperId": "0fa88943665de1176b0fc6de4ed7469b40cdb08c",
        "title": "Learning to Draw Samples: With Application to Amortized MLE for Generative Adversarial Learning"
      },
      {
        "paperId": "729b18d8d91035f4bb84bf2e61b0517824e5d31b",
        "title": "Auxiliary Deep Generative Models"
      },
      {
        "paperId": "2cc3fbf8e36667fa2bec9ea047076b79304e2eee",
        "title": "The Variational Gaussian Process"
      },
      {
        "paperId": "f31ac36adbd24c43dcd28397081702e98e026b34",
        "title": "Hierarchical Variational Models"
      },
      {
        "paperId": "2dcef55a07f8607a819c21fe84131ea269cc2e3c",
        "title": "Deep Unsupervised Learning using Nonequilibrium Thermodynamics"
      },
      {
        "paperId": "872bae24c109f7c30e052ac218b17a8b028d08a0",
        "title": "A Connection Between Score Matching and Denoising Autoencoders"
      },
      {
        "paperId": "aeed631d6a84100b5e9a021ec1914095c66de415",
        "title": "Bayesian Learning via Stochastic Gradient Langevin Dynamics"
      },
      {
        "paperId": "7a7a23f2c39f9b1526bc8853c6c71a5b7f89e68c",
        "title": "Robot trajectory optimization using approximate inference"
      },
      {
        "paperId": "c8221c054459e37edbf313668523d667fe5c1536",
        "title": "Maximum Entropy Inverse Reinforcement Learning"
      },
      {
        "paperId": "17ae320e0b24057afc75b2ed3c90f7962a7171e0",
        "title": "Cover"
      },
      {
        "paperId": "9966e890f2eedb4577e11b9d5a66380a4d9341fe",
        "title": "Estimation of Non-Normalized Statistical Models by Score Matching"
      },
      {
        "paperId": "fbcd7ecb6743e8a210feea539c0667ecb0ea9f48",
        "title": "An Auxiliary Variational Method"
      },
      {
        "paperId": "3ce93c1a772061161948a6728716f6f4d752809b",
        "title": "Sequential Monte Carlo samplers"
      },
      {
        "paperId": "c7a5128b45edb4db9105ec5167210b887617ddf2",
        "title": "Reverse-time diffusion equation models"
      },
      {
        "paperId": "ef4f5a50837a7c1b3e87b9300ffc7ba00d461a0f",
        "title": "AUTO-ENCODING VARIATIONAL BAYES"
      },
      {
        "paperId": null,
        "title": "Dynamical theories of Brownian motion , volume 101"
      },
      {
        "paperId": null,
        "title": "Applied stochastic differential equations , volume 10"
      },
      {
        "paperId": "5c65d095600d6c647426fa3bc45031b208882d5f",
        "title": "Batch Reinforcement Learning"
      },
      {
        "paperId": "2a65434d43ffa6554eaf14b728780919ad4f33eb",
        "title": "Modeling Purposeful Adaptive Behavior with the Principle of Maximum Causal Entropy"
      },
      {
        "paperId": "97efafdb4a3942ab3efba53ded7413199f79c054",
        "title": "Reinforcement Learning: An Introduction"
      },
      {
        "paperId": "f5fdc3e66379c0320cf9cc5493dd97cd59673f82",
        "title": "A stochastic control approach to reciprocal diffusion processes"
      },
      {
        "paperId": null,
        "title": "Time reversal of diffu-sions"
      },
      {
        "paperId": null,
        "title": "Schr\u00a8odinger-f\u00a8ollmer sampler: sampling without ergodicity"
      },
      {
        "paperId": null,
        "title": "Con-trastive energy prediction for exact energy-guided diffusion sampling in offline reinforcement learning"
      },
      {
        "paperId": null,
        "title": "Diffusion-Based Maximum sample efficiency and simplicity"
      }
    ],
    "cited_by": [
      {
        "paperId": "52b86384972bfdab2c2b411df3a2f645c9387b09",
        "title": "SAC Flow: Sample-Efficient Reinforcement Learning of Flow-Based Policies via Velocity-Reparameterized Sequential Modeling"
      },
      {
        "paperId": "83d0a2a5de8df1c86e4c6d242a4b11f3005f67e4",
        "title": "Reinforcement Learning with Discrete Diffusion Policies for Combinatorial Action Spaces"
      },
      {
        "paperId": "768f51ed227b1d019b583e87bd9ed11255f19081",
        "title": "Trust Region Constrained Measure Transport in Path Space for Stochastic Optimal Control and Inference"
      },
      {
        "paperId": "da504273a315e6bd279ad2d8b1051b98cd3f7857",
        "title": "One-Step Flow Policy Mirror Descent"
      },
      {
        "paperId": "a80127b993d989ac231177af63a0af12c6d1fe11",
        "title": "Curriculum Learning Guided Musculoskeletal Grasping Task"
      },
      {
        "paperId": "d5d223fa70cce2d34beb467aeaddbea51e8cd0ff",
        "title": "Relative Entropy Pathwise Policy Optimization"
      },
      {
        "paperId": "07854082c37061fee267f1860a9e1fe8059a1a40",
        "title": "Enhanced DACER Algorithm with High Diffusion Efficiency"
      },
      {
        "paperId": "7cbdcaad93669033b8adaaca14f1d17a7ca3f09c",
        "title": "GenPO: Generative Diffusion Models Meet On-Policy Reinforcement Learning"
      },
      {
        "paperId": "574ec4679c9efdaac629b8fd7f01ee07d78b741d",
        "title": "Chunking the Critic: A Transformer-based Soft Actor-Critic with N-Step Returns"
      },
      {
        "paperId": "739b5ccd9d84b8b36f95a989c292df7468013dc2",
        "title": "End-To-End Learning of Gaussian Mixture Priors for Diffusion Sampler"
      },
      {
        "paperId": "9ed524c29a0f93d0f7c5a992f8175befb0230b22",
        "title": "Efficient Online Reinforcement Learning for Diffusion Policy"
      },
      {
        "paperId": "90d1b91904f20a056719ffaeffe5ef84238b3649",
        "title": "DRDT3: Diffusion-Refined Decision Test-Time Training Model"
      }
    ],
    "score": 12.0
  },
  {
    "id": "81fdf2b3db2b3e7be90b51866a31b73587eecd30",
    "title": "A Novel Reinforcement Learning-Based Robust Control Strategy for a Quadrotor",
    "authors": [
      "Hean Hua",
      "Yongchun Fang"
    ],
    "year": 2023,
    "citationCount": 23,
    "abstract": "In this article, a novel reinforcement learning (RL)-based robust control approach is proposed for quadrotors, which guarantees efficient learning and satisfactory tracking performance by simultaneously evaluating the RL and the baseline method in training. Different from existing works, the key novelty is to design a practice-reliable RL control framework for quadrotors in a two-part cooperative manner. In the first part, based on the hierarchical property, a new robust integral of the signum of the error (RISE) design is proposed to ensure asymptotic convergence, which includes the nonlinear and the disturbance rejection terms. In the second part, a one-actor-dual-critic (OADC) learning framework is proposed, where the designed switching logic in the first part works as a benchmark to guide the learning. Specifically, the two critics independently evaluate the RL policy and the switching logic simultaneously, which are utilized for policy update, only when both are positive, corresponding to the remarkable actor-better exploration actions. The asymptotic RISE controller, together with the two critics in OADC learning framework, guarantees accurate judgment on every exploration. On this basis, the satisfactory performance of the RL policy is guaranteed by the actor-better exploration based learning while the chattering problem arisen from the switching logic is addressed completely. Plenty of comparative experimental tests are presented to illustrate the superior performance of the proposed RL controller in terms of tracking accuracy and robustness.",
    "url": "https://www.semanticscholar.org/paper/81fdf2b3db2b3e7be90b51866a31b73587eecd30",
    "pdf_url": "https://doi.org/10.1109/TIE.2022.3165288",
    "venue": "IEEE transactions on industrial electronics (1982. Print)",
    "publicationDate": "2023-03-01",
    "externalIds": {
      "DBLP": "journals/tie/HuaF23",
      "DOI": "10.1109/TIE.2022.3165288",
      "CorpusId": 248133874
    },
    "references": [],
    "cited_by": [
      {
        "paperId": "7b384716af09ab9e3bf2ae0a79e791361fc9fe78",
        "title": "Enhancing Robustness of Locomotion Policy for Quadrupedal Robot With Deep Disturbance Observer"
      },
      {
        "paperId": "06fc078c287cb3123c3201b450abdd2a90191170",
        "title": "A Second-Order LADRC-Based Control Strategy for Quadrotor UAVs Using a Modified Crayfish Optimization Algorithm and Fuzzy Logic"
      },
      {
        "paperId": "a12501ed7b22e5380261b3ae6ee3ade4ff8b7e82",
        "title": "Robust Tracking Control of Underactuated UAVs Based on Zero-Sum Differential Games"
      },
      {
        "paperId": "a0f6aeb2f14848912295b9b824b66d8bf6d1bfa5",
        "title": "HO2RL: A Novel Hybrid Offline-and-Online Reinforcement Learning Method for Active Pantograph Control"
      },
      {
        "paperId": "3bff2864865d7b5c8ad95f9c1c4f08d41e6aac5e",
        "title": "Deep Reinforcement Learning-Based Hierarchical Motion Planning Strategy for Multirotors"
      },
      {
        "paperId": "ef0e41b9bf9fcab08553d7fe73cb38c50b382772",
        "title": "Hierarchical Optimization Design for Autonomous Flight of Vision-Based Quadrotor Using Reinforcement Learning"
      },
      {
        "paperId": "07fecdd0d2f99b9373865304b3b686dff098d18b",
        "title": "Event\u2010Triggered Trajectory Tracking Control for Quadrotor UAVs Subject to External Disturbances"
      },
      {
        "paperId": "278c57336f5f3c7bae078093ad67e1f9e290e2dc",
        "title": "Robust Finite\u2010Time Trajectory Tracking Control for Quadrotor UAVs With Uncertainties, External Disturbances, and Input Saturation"
      },
      {
        "paperId": "b825ccc15901acf32dcf6905a498d3b82cd23dcc",
        "title": "A Reinforcement Learning Path Following Strategy for Snake Robots Based on Transferable Constrained-Residual Gait Generator"
      },
      {
        "paperId": "e124dfff1de10ab6b548ea5c0c4fd61d818b768b",
        "title": "Target Tracking for Quadrotors based on Deep Reinforcement Learning"
      },
      {
        "paperId": "a43574185cfb494c4390843eef04fe721810d950",
        "title": "Robust Reinforcement Learning Control for a Quadrotor with Disturbance Compensation"
      },
      {
        "paperId": "69f2762b24dcc98f449552639da7aa6866223e4b",
        "title": "High-Speed Trajectory Tracking Control for Quadrotors via Deep Reinforcement Learning"
      },
      {
        "paperId": "98464873068bf49c8e01fe085c36e2cfa7778511",
        "title": "Robust control for affine nonlinear systems under the reinforcement learning framework"
      },
      {
        "paperId": "0043c7f2bfdfad4aebb0b3bfd0701d76ef9552cf",
        "title": "Historical and Current Landscapes of Autonomous Quadrotor Control: An Early-Career Researchers\u2019 Guide"
      },
      {
        "paperId": "e3874b6918a457b86e2445a369d5d77aa47dbc6d",
        "title": "A Model-Free Online Learning Control for Attitude Tracking of Quadrotors"
      },
      {
        "paperId": "54c528575b62ba71e13e3a6ade654648bc53210b",
        "title": "A Nonlinear Trajectory Tracking Control Strategy for Quadrotor With Suspended Payload Based on Force Sensor"
      },
      {
        "paperId": "16989aa06d4831c3b1c48df5a7b75582bdcc74a9",
        "title": "Impact-Angle Constraint Guidance and Control Strategies Based on Deep Reinforcement Learning"
      },
      {
        "paperId": "d68055876123ada48d6e0807d9b212a3f7d7dbe8",
        "title": "Data-Driven Safe Formation Control for Multi-Agent Systems and Its Applications in Multi-UGV Systems"
      },
      {
        "paperId": "b09aae75cf789926b579ced6aa03db3b7362057e",
        "title": "Critic-Only Learning Based Tracking Control for Uncertain Nonlinear Systems with Prescribed Performance"
      },
      {
        "paperId": "cce1245ba1ec154120b3b256faf7bf28f769b505",
        "title": "A Novel Guided Deep Reinforcement Learning Tracking Control Strategy for Multirotors"
      },
      {
        "paperId": "761abec2430039dc2ac25a4978a0fdefa915c8c1",
        "title": "Event-Driven Robust Geometric Control for a Nano Unmanned Helicopter"
      },
      {
        "paperId": "4ebff0854dc12c300bdd55bccfcbc1a2b1b472c2",
        "title": "Event-Triggered Deep Learning Control of Quadrotors for Trajectory Tracking"
      },
      {
        "paperId": "bc4d7fa3f5f5cb4aaa83175d9847fcf353f4e9bf",
        "title": "Sustainable Robotic Joints 4D Printing with Variable Stiffness Using Reinforcement Learning"
      }
    ],
    "score": 11.5
  },
  {
    "id": "e4fef8d5864c5468100ca167639ef3fa374c0442",
    "title": "MaxInfoRL: Boosting exploration in reinforcement learning through information gain maximization",
    "authors": [
      "Bhavya Sukhija",
      "Stelian Coros",
      "Andreas Krause",
      "Pieter Abbeel",
      "Carmelo Sferrazza"
    ],
    "year": 2024,
    "citationCount": 11,
    "abstract": "Reinforcement learning (RL) algorithms aim to balance exploiting the current best strategy with exploring new options that could lead to higher rewards. Most common RL algorithms use undirected exploration, i.e., select random sequences of actions. Exploration can also be directed using intrinsic rewards, such as curiosity or model epistemic uncertainty. However, effectively balancing task and intrinsic rewards is challenging and often task-dependent. In this work, we introduce a framework, MaxInfoRL, for balancing intrinsic and extrinsic exploration. MaxInfoRL steers exploration towards informative transitions, by maximizing intrinsic rewards such as the information gain about the underlying task. When combined with Boltzmann exploration, this approach naturally trades off maximization of the value function with that of the entropy over states, rewards, and actions. We show that our approach achieves sublinear regret in the simplified setting of multi-armed bandits. We then apply this general formulation to a variety of off-policy model-free RL methods for continuous state-action spaces, yielding novel algorithms that achieve superior performance across hard exploration problems and complex scenarios such as visual control tasks.",
    "url": "https://www.semanticscholar.org/paper/e4fef8d5864c5468100ca167639ef3fa374c0442",
    "pdf_url": "https://arxiv.org/pdf/2412.12098.pdf",
    "venue": "International Conference on Learning Representations",
    "publicationDate": "2024-12-16",
    "externalIds": {
      "DBLP": "journals/corr/abs-2412-12098",
      "ArXiv": "2412.12098",
      "DOI": "10.48550/arXiv.2412.12098",
      "CorpusId": 274789119
    },
    "references": [
      {
        "paperId": "25c6aefef579c4a4ea079d1c4f0c0cb63196a794",
        "title": "NeoRL: Efficient Exploration for Nonepisodic RL"
      },
      {
        "paperId": "75850000ac9e056ce604c1bcd7631a3ae73d0458",
        "title": "HumanoidBench: Simulated Humanoid Benchmark for Whole-Body Locomotion and Manipulation"
      },
      {
        "paperId": "1675bce7fca4eb2171f68755e79c399060087f23",
        "title": "Probabilistic Inference in Reinforcement Learning Done Right"
      },
      {
        "paperId": "4e98282f5f3f1a388b8d95380473d4ef4878266e",
        "title": "DrM: Mastering Visual Reinforcement Learning through Dormant Ratio Minimization"
      },
      {
        "paperId": "31fcbacd4874a772e77ac21e4ec7d0dff132ec4e",
        "title": "Submodular Reinforcement Learning"
      },
      {
        "paperId": "3cdf1b5b4c758d56c44c229ce83ed5af4141318d",
        "title": "Anytime Model Selection in Linear Bandits"
      },
      {
        "paperId": "e66d55565f6cd6336f92be01b4efc7e6a1eb2381",
        "title": "Optimistic Active Exploration of Dynamical Systems"
      },
      {
        "paperId": "8037c0795aa73f7a5f4be1d452cccbbad70254a2",
        "title": "Optimal Exploration for Model-Based RL in Nonlinear Systems"
      },
      {
        "paperId": "b9e4ea7ff34a304cc0e7bfaa638eaa850391b676",
        "title": "Accelerating Reinforcement Learning with Value-Conditional State Entropy Exploration"
      },
      {
        "paperId": "864f5ec6ba95774747bbdc3f172b8dc36a5f0b91",
        "title": "Minimax-Bayes Reinforcement Learning"
      },
      {
        "paperId": "f2d952a183dfb0a1e031b8a3f535d9f8423d7a6e",
        "title": "Mastering Diverse Domains through World Models"
      },
      {
        "paperId": "fd1cf28a2b8caf2fe29af5e7fa9191cecfedf84d",
        "title": "RT-1: Robotics Transformer for Real-World Control at Scale"
      },
      {
        "paperId": "ea14465601f45bf50a148563c73c4d6e1971dcb1",
        "title": "Redeeming Intrinsic Rewards via Constrained Optimization"
      },
      {
        "paperId": "c64a99ec780d5e1c6ba242a82e72e96eb0c3ef8a",
        "title": "An Information-Theoretic Perspective on Intrinsic Motivation in Reinforcement Learning: A Survey"
      },
      {
        "paperId": "b4888a20a9910e292448a51bf4248cc5b60e2af3",
        "title": "A Walk in the Park: Learning to Walk in 20 Minutes With Model-Free Reinforcement Learning"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "512fb8acde1bce3d0239c146694f1b5852fba0f6",
        "title": "GoSafeOpt: Scalable safe exploration for global optimization of dynamical systems"
      },
      {
        "paperId": "4545358c54c90dc5d5eb8f11a3c610b3eda88b55",
        "title": "Discovering and Achieving Goals via World Models"
      },
      {
        "paperId": "b2d931da61559c528c5d4eadcb939425a2531652",
        "title": "Dropout Q-Functions for Doubly Efficient Reinforcement Learning"
      },
      {
        "paperId": "e06c005e98281af455c454ce2478285f6f3afeca",
        "title": "Mastering Visual Continuous Control: Improved Data-Augmented Reinforcement Learning"
      },
      {
        "paperId": "7ff21c6ca43afd1ec99ee958ce2f9e1da40ec881",
        "title": "Epistemic Neural Networks"
      },
      {
        "paperId": "183c998ad987aad377193593bcf50591e0da787c",
        "title": "Bayesian Bellman Operators"
      },
      {
        "paperId": "7e1b6ff0525e61c75c636982b400a3a9576b10e0",
        "title": "Task-Agnostic Exploration via Policy Gradient of a Non-Parametric State Entropy Estimate"
      },
      {
        "paperId": "7fc04f8031598da6510bce4a7e5b4722305ca5b0",
        "title": "State Entropy Maximization with Random Encoders for Efficient Exploration"
      },
      {
        "paperId": "736590f70e7f2dc464c1c62491cfa8adb4d718f3",
        "title": "Randomized Ensembled Double Q-Learning: Learning Fast Without a Model"
      },
      {
        "paperId": "27208f21014d9d948871dd5ccb138ea5becc4e34",
        "title": "On Information Gain and Regret Bounds in Gaussian Process Bandits"
      },
      {
        "paperId": "6be61525ee8b21c3bef6564df17b435fc4f84282",
        "title": "Action and Perception as Divergence Minimization"
      },
      {
        "paperId": "57ffba5ea5e602b5365752e19a6f85a716db1d8b",
        "title": "Bandit Algorithms"
      },
      {
        "paperId": "458d4f3d398da068493c63687e285b691514dff5",
        "title": "Information Theoretic Regret Bounds for Online Nonlinear Control"
      },
      {
        "paperId": "712e3a8b0291413ee44f27058853cfd1e5dad7b6",
        "title": "Active Learning for Nonlinear System Identification with Guarantees"
      },
      {
        "paperId": "3a07e0157a0c8b6321496b01c299319cadd5ec15",
        "title": "Efficient Model-Based Reinforcement Learning through Optimistic Policy Search and Planning"
      },
      {
        "paperId": "3a71c306eb6232658c9e5fd48aed1ef3befe5fbe",
        "title": "Planning to Explore via Self-Supervised World Models"
      },
      {
        "paperId": "6568423cfaca7e24c88ea208cb0e67129e43aa9b",
        "title": "Image Augmentation Is All You Need: Regularizing Deep Reinforcement Learning from Pixels"
      },
      {
        "paperId": "007fd689a806de99f053a0f5ef90aedb44f9ec7e",
        "title": "Better Exploration with Optimistic Actor-Critic"
      },
      {
        "paperId": "ffb3886a253ff927bcc46b78e00409893865a68e",
        "title": "Dynamics-Aware Unsupervised Discovery of Skills"
      },
      {
        "paperId": "87a40a2d506fec06d67dc8f14ce2c708a789a2b4",
        "title": "Self-Supervised Exploration via Disagreement"
      },
      {
        "paperId": "7388826b5ee00efe17cb7f19a623d9b5e955ae70",
        "title": "Skew-Fit: State-Covering Self-Supervised Reinforcement Learning"
      },
      {
        "paperId": "bb0ee42d406f2361fee89cf1274073185a0e9eec",
        "title": "Learning agile and dynamic motor skills for legged robots"
      },
      {
        "paperId": "12c0751b4f51ed833172a713b7e32390032ead93",
        "title": "Soft Actor-Critic Algorithms and Applications"
      },
      {
        "paperId": "4cb3fd057949624aa4f0bbe7a6dcc8777ff04758",
        "title": "Exploration by Random Network Distillation"
      },
      {
        "paperId": "a25b645c3d24f91164230a0ac5bb2d4ec88c1538",
        "title": "Information-Directed Exploration for Deep Reinforcement Learning"
      },
      {
        "paperId": "563dac292e449d5b602719845bcda3787c075c15",
        "title": "Variational Bayesian Reinforcement Learning with Regret Bounds"
      },
      {
        "paperId": "3aadab924520c58be81781aafd51e6807e9c4576",
        "title": "Visual Reinforcement Learning with Imagined Goals"
      },
      {
        "paperId": "f802802b3af5a22b79ac65d033ba3cbee33da91b",
        "title": "Randomized Prior Functions for Deep Reinforcement Learning"
      },
      {
        "paperId": "6ecc4b1ab05f3ec12484a0ea36abfd6271c5c5ba",
        "title": "Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review"
      },
      {
        "paperId": "4debb99c0c63bfaa97dd433bc2828e4dac81c48b",
        "title": "Addressing Function Approximation Error in Actor-Critic Methods"
      },
      {
        "paperId": "5b01eaef54a653ba03ddd5a978690380fbc19bfc",
        "title": "Diversity is All You Need: Learning Skills without a Reward Function"
      },
      {
        "paperId": "a9a3ed69c94a3e1c08ef1f833d9199f57736238b",
        "title": "DeepMind Control Suite"
      },
      {
        "paperId": "c27db32efa8137cbf654902f8f728f338e55cd1c",
        "title": "Mastering the game of Go without human knowledge"
      },
      {
        "paperId": "daea16e16370c9d141ce6bd1b42ee3e5287bf275",
        "title": "Learning Unknown Markov Decision Processes: A Thompson Sampling Approach"
      },
      {
        "paperId": "64b6057f7273d6620fc71f62bea14b01d8b771a9",
        "title": "Deep Kernels for Optimizing Locomotion Controllers"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "11026fc42c4d30f9ea91ec8c32f8d75768b70f6d",
        "title": "Boltzmann Exploration Done Right"
      },
      {
        "paperId": "225ab689f41cef1dc18237ef5dab059a49950abf",
        "title": "Curiosity-Driven Exploration by Self-Supervised Prediction"
      },
      {
        "paperId": "ba0fcccdea31b6d39857796032b0d2dd3b83e7b8",
        "title": "On Kernelized Multi-armed Bandits"
      },
      {
        "paperId": "802168a81571dde28f5ddb94d84677bc007afa7b",
        "title": "Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles"
      },
      {
        "paperId": "6e90fd78e8a3b98af3954aae5209703aa966603e",
        "title": "Unifying Count-Based Exploration and Intrinsic Motivation"
      },
      {
        "paperId": "2b10281297ee001a9f3f4ea1aa9bea6b638c27df",
        "title": "OpenAI Gym"
      },
      {
        "paperId": "3f1c092d89e3cb2bc93ad5515bd0e1c67068b4bb",
        "title": "Automatic LQR tuning based on Gaussian process global optimization"
      },
      {
        "paperId": "a6b82abf3bdc0a190bf21e290db70ac1091c9ff8",
        "title": "Bayesian optimization with safety constraints: safe and automatic parameter tuning in robotics"
      },
      {
        "paperId": "3b9732bb07dc99bde5e1f9f75251c6ea5039373e",
        "title": "Deep Reinforcement Learning with Double Q-Learning"
      },
      {
        "paperId": "024006d4c2a89f7acacc6e4438d156525b60a98f",
        "title": "Continuous control with deep reinforcement learning"
      },
      {
        "paperId": "2470fcf0f89082de874ac9133ccb3a8667dd89a8",
        "title": "Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models"
      },
      {
        "paperId": "e2471b05fd995ac764782f7213009e04e15afb72",
        "title": "Bayesian optimization for learning gaits under uncertainty"
      },
      {
        "paperId": "1e3154b10b872c100b86181ac2931c8e26f67912",
        "title": "Theory of Disagreement-Based Active Learning"
      },
      {
        "paperId": "f770614e497f456cfbe310bf7fd5223a4c28edd7",
        "title": "Learning to Optimize via Information-Directed Sampling"
      },
      {
        "paperId": "2319a491378867c7049b3da055c5df60e1671158",
        "title": "Playing Atari with Deep Reinforcement Learning"
      },
      {
        "paperId": "d01e3414ca706eda917576d947ece811b5cbcdde",
        "title": "Empowerment - an Introduction"
      },
      {
        "paperId": "f446319e3aee33eababc533d65c693f8d95267c2",
        "title": "The true sample complexity of active learning"
      },
      {
        "paperId": "0c8413ab8de0c1b8f2e86402b8d737d94371610f",
        "title": "Information-Theoretic Regret Bounds for Gaussian Process Optimization in the Bandit Setting"
      },
      {
        "paperId": "0cafe2903b097fc042782c359cb231ea34ef7ed3",
        "title": "Near-optimal Regret Bounds for Reinforcement Learning"
      },
      {
        "paperId": "c8221c054459e37edbf313668523d667fe5c1536",
        "title": "Maximum Entropy Inverse Reinforcement Learning"
      },
      {
        "paperId": "a301edf25f11440569f6f81af5902a4532b1dcb8",
        "title": "Near-Optimal Sensor Placements in Gaussian Processes: Theory, Efficient Algorithms and Empirical Studies"
      },
      {
        "paperId": "17ae320e0b24057afc75b2ed3c90f7962a7171e0",
        "title": "Cover"
      },
      {
        "paperId": "6cc0699a4a99132b7aecddb7f2e37d731e36bb43",
        "title": "Empowerment: a universal agent-centric measure of control"
      },
      {
        "paperId": "8ca86e941da7254613a5d03dd7a6c36886fadc1d",
        "title": "Gaussian Processes for Machine Learning (Adaptive Computation and Machine Learning)"
      },
      {
        "paperId": "dc649486b881e672eea6546da48c46e1f98daf32",
        "title": "Near-Optimal Reinforcement Learning in Polynomial Time"
      },
      {
        "paperId": "8f79332b1361e9eaa9da3327f83f57dcac5cd11d",
        "title": "Bayesian Experimental Design: A Review"
      },
      {
        "paperId": "a9cd8efe9184dddb1bedbbec3a356c4dfb22fe63",
        "title": "Markov Decision Processes: Discrete Stochastic Dynamic Programming"
      },
      {
        "paperId": "2980dfe5c99658dc3e508d9d6e1d7f26e6fc8934",
        "title": "A possibility for implementing curiosity and boredom in model-building neural controllers"
      },
      {
        "paperId": "9d3af1c5fd5f7fb51467b4308952bc4da8285396",
        "title": "Paper"
      },
      {
        "paperId": "ff5ff13ba3155cb24d8ff88607b816f8174c6bad",
        "title": "On a Measure of the Information Provided by an Experiment"
      },
      {
        "paperId": "f692622228db1a48b09e391ed2917135fb2b5980",
        "title": "Information-Directed Sampling - Frequentist Analysis and Applications"
      },
      {
        "paperId": "e3fc5b5627af62ee6981a02090cf6bae368202d7",
        "title": "Stable-Baselines3: Reliable Reinforcement Learning Implementations"
      },
      {
        "paperId": null,
        "title": "JAXRL: Implementations of Reinforcement Learning algorithms in JAX, 10"
      },
      {
        "paperId": null,
        "title": "the simplified setting of stochastic multi-armed bandits in continuous spaces"
      },
      {
        "paperId": null,
        "title": "where a normalized intrinsic reward is added the extrinsic reward"
      },
      {
        "paperId": null,
        "title": "Nonlinear control , volume 406"
      },
      {
        "paperId": "c7dbf5ed7e9b63e104adb4e18bfbc98f5d6afdae",
        "title": "R-MAX - A General Polynomial Time Algorithm for Near-Optimal Reinforcement Learning"
      },
      {
        "paperId": "97efafdb4a3942ab3efba53ded7413199f79c054",
        "title": "Reinforcement Learning: An Introduction"
      },
      {
        "paperId": null,
        "title": "The same as D ISAGREEMENT but with curiosity as the intrinsic reward"
      }
    ],
    "cited_by": [
      {
        "paperId": "b69f7136ed671c53da6588d29bec17407ed6e2e9",
        "title": "Leveraging Temporally Extended Behavior Sharing for Multi-task Reinforcement Learning"
      },
      {
        "paperId": "309fdeb3ca8d8f60a66390ace2290731c067595a",
        "title": "Wonder Wins Ways: Curiosity-Driven Exploration through Multi-Agent Contextual Calibration"
      },
      {
        "paperId": "4228fb3c5713be37dbe2ae0ffc036969087a48f2",
        "title": "Divide, Discover, Deploy: Factorized Skill Learning with Symmetry and Style Priors"
      },
      {
        "paperId": "7f5e32ca30063b29da66d78b565ff35ed8274cd2",
        "title": "Making AI Tutors Empathetic and Conscious: A Needs-Driven Pathway to Synthetic Machine Consciousness"
      },
      {
        "paperId": "7a7b1d278b7ea8eb6b00418618df0a5afab9260b",
        "title": "How Should We Meta-Learn Reinforcement Learning Algorithms?"
      },
      {
        "paperId": "84bfcfd64f59a55e6517e3f51d57de7b2bf5890f",
        "title": "Epistemically-guided forward-backward exploration"
      },
      {
        "paperId": "37c4bea3d5af535566b42d50a1c6153f10b478ab",
        "title": "Learning to Explore in Diverse Reward Settings via Temporal-Difference-Error Maximization"
      },
      {
        "paperId": "0a035cf137c16b3447a910d9cd15186e2c64894a",
        "title": "Learning What Matters Now: A Dual-Critic Context-Aware RL Framework for Priority-Driven Information Gain"
      },
      {
        "paperId": "abc453355f196859ba6ba1d6c64295e776e23d6f",
        "title": "Optimistic critics can empower small actors"
      },
      {
        "paperId": "760389d05f01f9acc7258b8ba00797d835f25044",
        "title": "DISCOVER: Automated Curricula for Sparse-Reward Reinforcement Learning"
      },
      {
        "paperId": "47fccd1b5e3faa0e9184c8b401c3b3486717e697",
        "title": "Imagine, Verify, Execute: Memory-Guided Agentic Exploration with Vision-Language Models"
      }
    ],
    "score": 11.0
  },
  {
    "id": "7da3eebf5c2f481bc38ae6115cec3fc71dc87f2b",
    "title": "Randomized Exploration in Cooperative Multi-Agent Reinforcement Learning",
    "authors": [
      "Hao-Lun Hsu",
      "Weixin Wang",
      "Miroslav Pajic",
      "Pan Xu"
    ],
    "year": 2024,
    "citationCount": 11,
    "abstract": "We present the first study on provably efficient randomized exploration in cooperative multi-agent reinforcement learning (MARL). We propose a unified algorithm framework for randomized exploration in parallel Markov Decision Processes (MDPs), and two Thompson Sampling (TS)-type algorithms, CoopTS-PHE and CoopTS-LMC, incorporating the perturbed-history exploration (PHE) strategy and the Langevin Monte Carlo exploration (LMC) strategy, respectively, which are flexible in design and easy to implement in practice. For a special class of parallel MDPs where the transition is (approximately) linear, we theoretically prove that both CoopTS-PHE and CoopTS-LMC achieve a $\\widetilde{\\mathcal{O}}(d^{3/2}H^2\\sqrt{MK})$ regret bound with communication complexity $\\widetilde{\\mathcal{O}}(dHM^2)$, where $d$ is the feature dimension, $H$ is the horizon length, $M$ is the number of agents, and $K$ is the number of episodes. This is the first theoretical result for randomized exploration in cooperative MARL. We evaluate our proposed method on multiple parallel RL environments, including a deep exploration problem (i.e., $N$-chain), a video game, and a real-world problem in energy systems. Our experimental results support that our framework can achieve better performance, even under conditions of misspecified transition models. Additionally, we establish a connection between our unified framework and the practical application of federated learning.",
    "url": "https://www.semanticscholar.org/paper/7da3eebf5c2f481bc38ae6115cec3fc71dc87f2b",
    "pdf_url": "https://arxiv.org/pdf/2404.10728.pdf",
    "venue": "Neural Information Processing Systems",
    "publicationDate": "2024-04-16",
    "externalIds": {
      "DBLP": "conf/nips/HsuWP024",
      "ArXiv": "2404.10728",
      "DOI": "10.48550/arXiv.2404.10728",
      "CorpusId": 269157020
    },
    "references": [],
    "cited_by": [
      {
        "paperId": "15fd9b42e6f855dd7572c86923afe9eaee83070a",
        "title": "Game-Theoretic Understandings of Multi-Agent Systems with Multiple Objectives"
      },
      {
        "paperId": "5b576bebf7d18d8dc2ed90ef53cab0a8a283d1a6",
        "title": "Exploration by Random Reward Perturbation"
      },
      {
        "paperId": "95148ebccb6062606261ce17199b3326fa601620",
        "title": "Regret-Optimal Q-Learning with Low Cost for Single-Agent and Federated Reinforcement Learning"
      },
      {
        "paperId": "e38a1ae037aa9350fbfcbd07d8ef2e9b7fa093ea",
        "title": "A Reward-driven Automated Webshell Malicious-code Generator for Red-teaming"
      },
      {
        "paperId": "0d1754f3569a92d1fb4602d3acc45e293b59d7ef",
        "title": "Cooperative Multiagent Learning and Exploration With Min\u2013Max Intrinsic Motivation"
      },
      {
        "paperId": "936e5fccd634faf82845d1012dece1e2318d9f40",
        "title": "Inverse Reinforcement Learning with Dynamic Reward Scaling for LLM Alignment"
      },
      {
        "paperId": "dab91fc8eddb34879b1014b69945dc6575ba1742",
        "title": "Gap-Dependent Bounds for Federated Q-learning"
      },
      {
        "paperId": "023ac9657725b307f7460471b2f3caac20948cfd",
        "title": "Isoperimetry is All We Need: Langevin Posterior Sampling for RL with Sublinear Regret"
      },
      {
        "paperId": "0eff31517a78ee87e7d38f5efb70004949c61f6e",
        "title": "Upper and Lower Bounds for Distributionally Robust Off-Dynamics Reinforcement Learning"
      },
      {
        "paperId": "39d2839aa4c3d8c0e64553891fe98ba261703154",
        "title": "More Efficient Randomized Exploration for Reinforcement Learning via Approximate Sampling"
      },
      {
        "paperId": "db59b22ae9556458d98107297736f640997c31be",
        "title": "Robust exploration with adversary via Langevin Monte Carlo"
      }
    ],
    "score": 11.0
  },
  {
    "id": "2e6534fbeb932fc058b9f865c0b25a3f201a0704",
    "title": "Learning State-Specific Action Masks for Reinforcement Learning",
    "authors": [
      "Ziyi Wang",
      "Xinran Li",
      "Luoyang Sun",
      "Haifeng Zhang",
      "Hualin Liu",
      "Jun Wang"
    ],
    "year": 2024,
    "citationCount": 11,
    "abstract": "Efficient yet sufficient exploration remains a critical challenge in reinforcement learning (RL), especially for Markov Decision Processes (MDPs) with vast action spaces. Previous approaches have commonly involved projecting the original action space into a latent space or employing environmental action masks to reduce the action possibilities. Nevertheless, these methods often lack interpretability or rely on expert knowledge. In this study, we introduce a novel method for automatically reducing the action space in environments with discrete action spaces while preserving interpretability. The proposed approach learns state-specific masks with a dual purpose: (1) eliminating actions with minimal influence on the MDP and (2) aggregating actions with identical behavioral consequences within the MDP. Specifically, we introduce a novel concept called Bisimulation Metrics on Actions by States (BMAS) to quantify the behavioral consequences of actions within the MDP and design a dedicated mask model to ensure their binary nature. Crucially, we present a practical learning procedure for training the mask model, leveraging transition data collected by any RL policy. Our method is designed to be plug-and-play and adaptable to all RL policies, and to validate its effectiveness, an integration into two prominent RL algorithms, DQN and PPO, is performed. Experimental results obtained from Maze, Atari, and \u03bcRTS2 reveal a substantial acceleration in the RL learning process and noteworthy performance improvements facilitated by the introduced approach.",
    "url": "https://www.semanticscholar.org/paper/2e6534fbeb932fc058b9f865c0b25a3f201a0704",
    "pdf_url": "https://doi.org/10.3390/a17020060",
    "venue": "Algorithms",
    "publicationDate": "2024-01-30",
    "externalIds": {
      "DBLP": "journals/algorithms/WangLSZLW24",
      "DOI": "10.3390/a17020060",
      "CorpusId": 267449328
    },
    "references": [
      {
        "paperId": "75e7668e65a72149689762978245a1c6dd94628a",
        "title": "Hierarchical Reinforcement Learning for Crude Oil Supply Chain Scheduling"
      },
      {
        "paperId": "368e049909a909baa7fb98d92ba770982f3fe8b5",
        "title": "Goal-Conditioned Action Space Reduction for Deformable Object Manipulation"
      },
      {
        "paperId": "832d9fda09f1c36c4eda2010295ef68cdf95ad37",
        "title": "Effective and Stable Role-Based Multi-Agent Collaboration by Structural Information Principles"
      },
      {
        "paperId": "890d8fdc790e216bf466e16ec254f6a3b7c3a36b",
        "title": "AlphaStar: an integrated application of reinforcement learning algorithms"
      },
      {
        "paperId": "6b98e16fc63687b234c77fd546b3d12b99de98e9",
        "title": "Deep Reinforcement Learning-Based Approach for Autonomous Power Flow Control Using Only Topology Changes"
      },
      {
        "paperId": "96dc2240cfd0b7c5c03b1478c5b3f41711b8f080",
        "title": "Proposal-Free Temporal Action Detection via Global Segmentation Mask Learning"
      },
      {
        "paperId": "1774bebf8ae786d83a63bcd13333ed7a078b3b08",
        "title": "Reinforcement Learning in Factored Action Spaces using Tensor Decompositions"
      },
      {
        "paperId": "9c477fc2d1fbe27abad912b3001b3c176bf83667",
        "title": "Tesseract: Tensorised Actors for Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "4e93cea327e1420078f09d3377e4ff3e51eade5a",
        "title": "LASER: Learning a Latent Action Space for Efficient Reinforcement Learning"
      },
      {
        "paperId": "5685abf9e7bb2c16449ae1eb181051e503602a55",
        "title": "Reinforcement Learning based Recommender Systems: A Survey"
      },
      {
        "paperId": "681bbcf763fd7c59869853bedfe6b9f02a3364e6",
        "title": "Towards Playing Full MOBA Games with Deep Reinforcement Learning"
      },
      {
        "paperId": "cff6566e92e71c8fcc5cfa5d16eef34e95b1a1f3",
        "title": "PLAS: Latent Action Space for Offline Reinforcement Learning"
      },
      {
        "paperId": "5f72c4f7a0991e7035ccc87b69410f4b3e6244ce",
        "title": "Masked Contrastive Representation Learning for Reinforcement Learning"
      },
      {
        "paperId": "371d71685313314da1fd83c0c6fcaef188b59986",
        "title": "Jointly-Learned State-Action Embedding for Efficient Reinforcement Learning"
      },
      {
        "paperId": "5764095b0186a3fc3832c1052aa14996a5927edc",
        "title": "RODE: Learning Roles to Decompose Multi-Agent Tasks"
      },
      {
        "paperId": "40812530e307c249fca9fa1214f0b14eb4c072c2",
        "title": "Reinforcement Learning in Dynamic Task Scheduling: A Review"
      },
      {
        "paperId": "83b7d091fa44906d4cc873fdfaa4ba3d1858e7bb",
        "title": "A Closer Look at Invalid Action Masking in Policy Gradient Algorithms"
      },
      {
        "paperId": "518b827e340c26582b5093401283a4f5cff605b9",
        "title": "Learning Invariant Representations for Reinforcement Learning without Reconstruction"
      },
      {
        "paperId": "9efb64f20ab1f157ca9f4050d4aaacf6c3f9b2b2",
        "title": "CURL: Contrastive Unsupervised Representations for Reinforcement Learning"
      },
      {
        "paperId": "3f37a4a4ca3ff3c9a2cc9bae2138db628dd7ecc4",
        "title": "Action Space Shaping in Deep Reinforcement Learning"
      },
      {
        "paperId": "5e8350467d7d19e1d5f991be23f7b4826a9303a8",
        "title": "ROMA: Multi-Agent Reinforcement Learning with Emergent Roles"
      },
      {
        "paperId": "b19729b27a1b4c24b52f87308c907653300afa7f",
        "title": "Dota 2 with Large Scale Deep Reinforcement Learning"
      },
      {
        "paperId": "361c00b22e29d0816ca896513d2c165e26399821",
        "title": "Grandmaster level in StarCraft II using multi-agent reinforcement learning"
      },
      {
        "paperId": "4c54efd3b9ce84f4268b165f5896c0692a50bcd1",
        "title": "Learning Action Representations for Reinforcement Learning"
      },
      {
        "paperId": "54e0f2c7ddc0a92d4f7f1076141088bca65675d7",
        "title": "Discretizing Continuous Action Space for On-Policy Optimization"
      },
      {
        "paperId": "eed8cae46eb28311e88f6fc41f788528ca2d0f00",
        "title": "State Representation Learning for Control: An Overview"
      },
      {
        "paperId": "699d3449b82a635d4fbdf3b0946d31b4d78f0cea",
        "title": "Learning to Factor Policies and Action-Value Functions: Factored Action Space Representations for Deep Reinforcement learning"
      },
      {
        "paperId": "1a0912bb76777469295bb2c059faee907e7f3258",
        "title": "Mask R-CNN"
      },
      {
        "paperId": "bd6b6291c3c14551cf9f2aa0e04e2e33c86b800e",
        "title": "The Malmo Platform for Artificial Intelligence Experimentation"
      },
      {
        "paperId": "3b2aff88ee03e82993c066c3e698d51da62d5496",
        "title": "Deep Reinforcement Learning in Large Discrete Action Spaces"
      },
      {
        "paperId": "2020aca3838a0e8a723761e74899b183d6b56f30",
        "title": "Equivalence notions and model minimization in Markov decision processes"
      },
      {
        "paperId": "9bf5b3499e25b1c0baf1429e3b17b2a50e93326e",
        "title": "The future of PID control"
      },
      {
        "paperId": "5c8fe9a0412a078e30eb7e5eeb0068655b673e86",
        "title": "A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise"
      },
      {
        "paperId": "88fe421dc58c7e099de79265405e7f0961192731",
        "title": "Theory of linear and integer programming"
      },
      {
        "paperId": "ca9a2d326b9de48c095a6cb5912e1990d2c5ab46",
        "title": "Towards a Unified Theory of State Abstraction for MDPs"
      },
      {
        "paperId": "7ef50e96eba960fad1878e7754b6266f83a80c6d",
        "title": "The future of PID control"
      }
    ],
    "cited_by": [
      {
        "paperId": "aa52afc6c070817dc11290f906e7476d0a89ec07",
        "title": "Large Language Model Integration with Reinforcement Learning to Augment Decision-Making in Autonomous Cyber Operations"
      },
      {
        "paperId": "e5fb4dd24d54edadb83965d4c425b0ecbbf152a4",
        "title": "Planning-Query-Guided Model Generation for Model-Based Deformable Object Manipulation"
      },
      {
        "paperId": "dd7530fd9559e897dd34b702146fb32212742e9a",
        "title": "A Comparative Evaluation of Teacher-Guided Reinforcement Learning Techniques for Autonomous Cyber Operations"
      },
      {
        "paperId": "93986955153347219ef0ab2f877c880315ef49b9",
        "title": "Incentivizing Cooperation for Handover Strategies in LEO Constellations via Fairness-guided MARL"
      },
      {
        "paperId": "a218e2f3f7b59aaaacccffd466e158c062d0784d",
        "title": "Action Space Reduction Strategies for Reinforcement Learning in Autonomous Driving"
      },
      {
        "paperId": "89619dea003a808abf5f2ffe6a39138d59125f09",
        "title": "ShIOEnv: A CLI Behavior-Capturing Environment Enabling Grammar-Guided Command Synthesis for Dataset Curation"
      },
      {
        "paperId": "808b0a49e9971b9d2606bf1580edf2fc7649c2db",
        "title": "TRM-A2C Planning Method for Mega-Constellation Region Observation Mission"
      },
      {
        "paperId": "68e2adf7dace2f55b42fc3728259215bc873128d",
        "title": "Perspective on the ethics of AI at the intersection of nutrition and behaviour change"
      },
      {
        "paperId": "e79d227a6211e7938a225c78171c27083e063c17",
        "title": "A Review of Reinforcement Learning for Semantic Communications"
      },
      {
        "paperId": "4e6baff7492ed31883eac9fe72efd2f3980afa36",
        "title": "A Deep Reinforcement Learning Optimization Method Considering Network Node Failures"
      },
      {
        "paperId": "ab10ad80dfb3774fb63647620260370fa8af823b",
        "title": "A Mixed-Reality-Augmented Deep Reinforcement Learning Approach for Multi-Robot Safe Motion Generation in Human\u2013Robot Collaborative Manufacturing Cells"
      }
    ],
    "score": 11.0
  },
  {
    "id": "01936f6df3c760d23df237d8d15cb7faadce9520",
    "title": "Design of an Adaptive Robust PI Controller for DC/DC Boost Converter Using Reinforcement-Learning Technique and Snake Optimization Algorithm",
    "authors": [
      "S. Ghamari",
      "Mojtaba Hajihosseini",
      "D. Habibi",
      "Asma Aziz"
    ],
    "year": 2024,
    "citationCount": 11,
    "abstract": "The DC/DC Boost converter exhibits a non-minimum phase system with a right half-plane zero structure, posing significant challenges for the design of effective control approaches. This article presents the design of a robust Proportional-Integral (PI) controller for this converter with an online adaptive mechanism based on the Reinforcement-Learning (RL) strategy. Classical PI controllers are simple and easy to build, but they need to be more robust against a wide range of disturbances and more adaptable to operational parameters. To address these issues, the RL adaptive strategy is used to optimize the performance of the PI controller. Some of the main advantages of the RL are lower sensitivity to error, more reliable results through the collection of data from the environment, an ideal model behavior within a specific context, and better frequency matching in real-time applications. Random exploration, nevertheless, can result in disastrous outcomes and surprising performance in real-world settings. Therefore, we opt for the Deterministic Policy Gradient (DPG) technique, which employs a deterministic action function as opposed to a stochastic one. DPG combines the benefits of actor-critics, deep Q-networks, and the deterministic policy gradient method. In addition, this method adopts the Snake Optimization (SO) algorithm to optimize the initial condition of gains, yielding more reliable results with faster dynamics. The SO method is known for its disciplined and nature-inspired approach, which results in faster decision-making and greater accuracy compared to other optimization algorithms. A structure using a hardware setup with CONTROLLINO MAXI Automation is built, which offers a more cost-effective and precise measurement method. Finally, the results achieved by simulations and experiments demonstrate the robustness of this approach.",
    "url": "https://www.semanticscholar.org/paper/01936f6df3c760d23df237d8d15cb7faadce9520",
    "pdf_url": "https://doi.org/10.1109/ACCESS.2024.3440580",
    "venue": "IEEE Access",
    "publicationDate": "",
    "externalIds": {
      "DBLP": "journals/access/GhamariHHA24",
      "DOI": "10.1109/ACCESS.2024.3440580",
      "CorpusId": 271810013
    },
    "references": [
      {
        "paperId": "044ff4502600abeb5d045ea13c63b1145367bab0",
        "title": "Active equalization control method for battery pack based on Double-DQN"
      },
      {
        "paperId": "356f5fd6693a7bbc16701d2d774545036e2f9e20",
        "title": "Intelligent Control of Power Converters using Reinforcement Learning"
      },
      {
        "paperId": "1b2896de2c29464fe15ef1c5b8d6ef214f9a0157",
        "title": "Robust adaptive backstepping control of H\u2010bridge inverter based on type\u20102 fuzzy optimization of parameters"
      },
      {
        "paperId": "5915412cb402770fc4a137bdd0b4403ac5802360",
        "title": "A Comprehensive Survey on Feature Selection with Grasshopper Optimization Algorithm"
      },
      {
        "paperId": "5e8c783037745b973e04f73a803a8a54d2fcfaab",
        "title": "Stability-Oriented Design of Model Predictive Control for DC/DC Boost Converter"
      },
      {
        "paperId": "03c54dbf3883e99fc3d657c5763beef91e1aa739",
        "title": "A novel cat swarm optimization\u2010based fuzzy PI controller for improving dynamic response of converter"
      },
      {
        "paperId": "902a5d7ae306ea02d1396d6450b6a9028cff1c3c",
        "title": "Dynamic Neural-Based Model Predictive Voltage Controller for an Interleaved Boost Converter With Adaptive Constraint Tuning"
      },
      {
        "paperId": "1d5c3421833d9195464daddfc09d3579dc4cda91",
        "title": "Design and Implementation of Fuzzy sliding mode control (FSMC) approach for a Modified Negative Output Luo DC-DC Converter with its comparative analysis"
      },
      {
        "paperId": "0c73572c449b1095f4cf602716c6fc635c57708c",
        "title": "Intelligent power grid energy supply forecasting and economic operation management using the snake optimizer algorithm with Bigur-attention model"
      },
      {
        "paperId": "6afeee79a75db45723908a6a6cc7287436b98a60",
        "title": "Reinforcement Learning-based Control of a Buck Converter: A Comparative Study of DQN and DDPG Algorithms"
      },
      {
        "paperId": "e58a9751631a35c1c6b812c26f21f530779bdd29",
        "title": "An Adaptive Model Based on Data-driven Approach for FCS-MPC Forming Converter in Microgrid"
      },
      {
        "paperId": "311059c42c8f4754b953ca22a93f276117a80163",
        "title": "A Compact Snake Optimization Algorithm in the Application of WKNN Fingerprint Localization"
      },
      {
        "paperId": "fa374ae71493520396ff3f69945aa691b399b74a",
        "title": "A Systematic Review of the Whale Optimization Algorithm: Theoretical Foundation, Improvements, and Hybridizations"
      },
      {
        "paperId": "71aacb2e0979222385df0bf651949ce933adecef",
        "title": "Adaptive backstepping controller design for DC/DC buck converter optimised by grey wolf algorithm"
      },
      {
        "paperId": "c6e0a6e66df75e7f718c20f3401bd6b95e65664c",
        "title": "Design of a novel robust type\u20102 fuzzy\u2010based adaptive backstepping controller optimized with antlion algorithm for buck converter"
      },
      {
        "paperId": "2313ad1f0b6a0411adaa9b30b6e55ee5f10ad808",
        "title": "Deep Reinforcement Learning with Importance Weighted A3C for QoE enhancement in Video Delivery Services"
      },
      {
        "paperId": "05d50e67ddd658430f4813b34f09a92898e72929",
        "title": "Design of a novel robust adaptive neural network\u2010based fractional\u2010order proportional\u2010integrated\u2010derivative controller on DC/DC Boost converter"
      },
      {
        "paperId": "fca74b323de2cd83761efe82c08fa033ee4b9957",
        "title": "A Variable Self-Tuning Horizon Mechanism for Generalized Dynamic Predictive Control on DC/DC Boost Converters Feeding CPLs"
      },
      {
        "paperId": "f69948bb8f2de440cf9c1fc746f7533ed9581316",
        "title": "Voltage control of DC-DC converters through direct control of power switches using reinforcement learning"
      },
      {
        "paperId": "66853c461e02b915b86b304a341d386f917a8142",
        "title": "An improved heuristic mechanism ant colony optimization algorithm for solving path planning"
      },
      {
        "paperId": "147f54a316dee6e098abc1cab8c55f65777c651f",
        "title": "Design of a novel robust adaptive cascade controller for DC\u2010DC buck\u2010boost converter optimized with neural network and fractional\u2010order PID strategies"
      },
      {
        "paperId": "ddb343beac6cdc56000791a6eda64fc7bf8def8e",
        "title": "Designs of Particle-Swarm-Optimization-Based Intelligent PID Controllers and DC/DC Buck Converters for PEM Fuel-Cell-Powered Four-Wheeled Automated Guided Vehicle"
      },
      {
        "paperId": "ae07b20a239e0a9c8e8a4a2c4da74566635510b2",
        "title": "A Survey of Algorithms, Applications and Trends for Particle Swarm Optimization"
      },
      {
        "paperId": "c5798cc0d231a885e08484e371ccde6482ed1431",
        "title": "Adaptive Model Predictive Control of an Interleaved Boost Converter Using Real-Time Updated Model"
      },
      {
        "paperId": "f1ed68067fb6bab0a02af382b76431a15f4cb9ab",
        "title": "BEESO: Multi-strategy Boosted Snake-Inspired Optimizer for Engineering Applications"
      },
      {
        "paperId": "bd8588a02729179069c528da8a55234422982df5",
        "title": "A novel adaptive neuro linear quadratic regulator (ANLQR) controller design on DC\u2010DC buck converter"
      },
      {
        "paperId": "383c002e32416dbc3898026ffefe3ebb5ff5bdcc",
        "title": "Lyapunov-based adaptive PID controller design for buck converter"
      },
      {
        "paperId": "ae159d795f276dc2772a5984860065619d0c27e2",
        "title": "Design of a robust adaptive self-tuning regulator controller on single-phase full-bridge grid-connected inverter"
      },
      {
        "paperId": "b77397dd22fc14024d2daabdc50372624c02be1c",
        "title": "Harris Hawks Optimization Algorithm: Variants and Applications"
      },
      {
        "paperId": "6cb50816371b7702780943209b85c0e3ed7f63e5",
        "title": "Generalised model predictive controller design for A DC\u2013DC non\u2010inverting buck\u2013boost converter optimised with a novel identification technique"
      },
      {
        "paperId": "21dd2dd049378c36cbb573fec97b35936cb91628",
        "title": "Performance Comparison of Optimization Algorithm Tuned PID Controllers in Positive Output Re-Lift Luo Converter Operation for Electric Vehicle Applications"
      },
      {
        "paperId": "42d47f90813b4273b94777982e5bdf7a608319d9",
        "title": "Self\u2010tuning regulator adaptive controller design for DC\u2010DC boost converter with a novel robust improved identification method"
      },
      {
        "paperId": "27bc594215fda7db7b5f23bea92c9227caedcb92",
        "title": "Metaheuristic algorithms for PID controller parameters tuning: review, approaches and open problems"
      },
      {
        "paperId": "f82da6414e91e698de4d2b1d86b440485d657483",
        "title": "A systematic review of meta-heuristic algorithms in IoT based application"
      },
      {
        "paperId": "36cbf37930a0f0ddc61124eca9d40c6806e534ea",
        "title": "A New Meta-Heuristics Data Clustering Algorithm Based on Tabu Search and Adaptive Search Memory"
      },
      {
        "paperId": "6eed34757d8430142a8e7afa61c0ebd404174096",
        "title": "Reinforcement Learning Approach to Autonomous PID Tuning"
      },
      {
        "paperId": "36190f351e7b2610b4549f3f1a5f3ec8f93d2a19",
        "title": "Fractional\u2010order fuzzy PID controller design on buck converter with antlion optimization algorithm"
      },
      {
        "paperId": "d325aa9305ec92a75511dff3c5b2cdcc8b6c3953",
        "title": "Deep Reinforcement Learning with Shallow Controllers: An Experimental Application to PID Tuning"
      },
      {
        "paperId": "d1ff3d3098f7a249e34efce67534c19a4b8df168",
        "title": "Prioritized Replay Dueling DDQN Based Grid-Edge Control of Community Energy Storage System"
      },
      {
        "paperId": "74753de00b401df93f6f202a8103d9f376f1157a",
        "title": "A Novel Sample-Efficient Deep Reinforcement Learning with Episodic Policy Transfer for PID-Based Control in Robotic Catheter System"
      },
      {
        "paperId": "674191d5a65cbfd110a3d1fc3a845e973fca10a6",
        "title": "Optimal Policy Characterization Enhanced Actor-Critic Approach for Electric Vehicle Charging Scheduling in a Power Distribution Network"
      },
      {
        "paperId": "c5fd8bfb50c40c0455fd2a5e828ac1dbd875db67",
        "title": "Modeling and stability issues of voltage-source converter dominated power systems: A review"
      },
      {
        "paperId": "e160f26f7c901828f39a6b1acc6148357e532528",
        "title": "Elephant herding algorithm-based optimal PI controller for LVRT enhancement of wind energy conversion systems"
      },
      {
        "paperId": "c5ba9196ee30969b16a5f40e4cca89c00165bdd7",
        "title": "Design of a Reinforcement Learning PID controller"
      },
      {
        "paperId": "6592046b956d978db7ace1598c2559f49a43c747",
        "title": "Optimal PID and Antiwindup Control Design as a Reinforcement Learning Problem"
      },
      {
        "paperId": "f1ea8b9e517c0a9b067a69253da68f74f48cc26a",
        "title": "Ant Lion Optimizer: A Comprehensive Survey of Its Variants and Applications"
      },
      {
        "paperId": "6fa998f464fcfffd5b58eb2f260063ca9cfd0fd4",
        "title": "Deterministic policy gradient adaptive dynamic programming for model-free optimal control"
      },
      {
        "paperId": "d98914404b2185abf9ff7770ec3918d5f506bdac",
        "title": "DC/DC Power Converter Control-Based Deep Machine Learning Techniques: Real-Time Implementation"
      },
      {
        "paperId": "b233f730770c839fcdb9694e9ff4171e0a54102b",
        "title": "A Survey of Using Swarm Intelligence Algorithms in IoT"
      },
      {
        "paperId": "cf068a2e9d22d9f64015e7a8248e2e9ba634ec9e",
        "title": "Survey of Swarm Intelligence Algorithms"
      },
      {
        "paperId": "88a4101a6f12987dbb4e4c727d8ee84976c5728c",
        "title": "Salp swarm algorithm: a comprehensive survey"
      },
      {
        "paperId": "5ae28f80d35969fcfab356620a213e020eaf7b39",
        "title": "A survey on new generation metaheuristic algorithms"
      },
      {
        "paperId": "ea604ad04e93e12217f2cd36a5dc6d121ec05edb",
        "title": "Robust Adaptive Controller Design for DC-DC SEPIC Converter In Photo Voltaic Application"
      },
      {
        "paperId": null,
        "title": "\u2018\u2018Increasing PEM fuel cell performance via fuzzy-logic controlled cascaded DC\u2013DC boost converter,\u2019\u2019"
      },
      {
        "paperId": null,
        "title": "\u2018\u2018Extracted"
      },
      {
        "paperId": "e7478cb7f3af9076d645062ae0071e5ee6e51ab4",
        "title": "RBF Neural Network-based Adaptive PID Controller for Active Suspension"
      },
      {
        "paperId": null,
        "title": "\u2018\u2018Apower-aware taskschedulerforenergyharvesting-basedwearablebiomedicalsystemsusingsnakeoptimizer,\u2019\u2019"
      },
      {
        "paperId": null,
        "title": "\u2018\u2018PerformanceanalysisofPIDcontrollerandfuzzylogiccontrollerforDC\u2013DCboostconverter"
      },
      {
        "paperId": null,
        "title": "\u2018\u2018Comparisonofvariousreinforcementlearning environmentsinthecontextofcontinuumrobotcontrol"
      },
      {
        "paperId": null,
        "title": "\u2018\u2018Model-based control strategy for the three-phase n-level CHB multilevel converter"
      },
      {
        "paperId": null,
        "title": "\u2018\u2018Modelpredictivecontroland linear control of DC\u2013DC boost converter in low voltage DC microgrid: An experimentalcomparativestudy"
      },
      {
        "paperId": null,
        "title": "\u2018\u2018Interval type-2 fuzzy-logic-based constantswitchingfrequencycontrolofasliding-mode-controlledDC\u2013DC boostconverter,\u2019\u2019"
      },
      {
        "paperId": null,
        "title": "\u2018\u2018Discrete-timeadaptivePIDcurrentcontroller forwindboostconverter,\u2019\u2019"
      },
      {
        "paperId": "eef11bf261fbbb39b04d0cd5614793c5d4a451d4",
        "title": "Deterministic policy gradient: Convergence analysis"
      },
      {
        "paperId": null,
        "title": "\u2018\u2018Snakeoptimizer:Anovelmeta-heuristicoptimizationalgorithm"
      },
      {
        "paperId": "3ec028bd090dc109cb061771aa14caad293344d8",
        "title": "Adaptive Neural Network Linear Parameter-Varying Control of Shipboard Direct Current Microgrids"
      },
      {
        "paperId": null,
        "title": "\u2018\u2018A novel adaptive PID controllerdesignforaPEMfuelcellusingstochasticgradientdescentwithmomentumenhancedbywhaleoptimizer,\u2019\u2019"
      },
      {
        "paperId": "484e619ae6905e5b102e8e61042510fa3d4bba8c",
        "title": "Grid-Forming Converters: Control Approaches, Grid-Synchronization, and Future Trends\u2014A Review"
      },
      {
        "paperId": "f0d4ddf7167d03d0992844bc29f4c1586f37841e",
        "title": "Metaheuristic Algorithms on Feature Selection: A Survey of One Decade of Research (2009-2019)"
      },
      {
        "paperId": "3d19c2a27729f9c4ce585b38516108c81992b923",
        "title": "Crow Search Algorithm: Theory, Recent Advances, and Applications"
      },
      {
        "paperId": "315d17c941d569736057fc329e6b069be96eedf7",
        "title": "Influence of Initializing Krill Herd Algorithm With Low-Discrepancy Sequences"
      },
      {
        "paperId": "d6cf37ee25289855993fcb46fc7c531cdddda3dd",
        "title": "Metaheuristic Algorithms: A Comprehensive Review"
      },
      {
        "paperId": "f686307db030d7de520e30fd84451d00bbb68fce",
        "title": "Power Electronics Converters Applications And Design"
      },
      {
        "paperId": null,
        "title": "Advanced DC/DC Converters . New York, NY, USA"
      },
      {
        "paperId": null,
        "title": "He is currently pursuing the Ph.D.degreeinpowerengineeringwiththeSchool ofEngineering,DeakinUniversity,Australia.From2015to2020,hewasaResearchAssistant withShirazUniversity,Iran.Then"
      },
      {
        "paperId": null,
        "title": "\u2018\u2018Optimizedmodel basedcontrollerwithmodelplantmismatchforNMPmitigationinboostconverter,\u2019\u2019"
      },
      {
        "paperId": null,
        "title": "\u2018\u2018Impacts of metaheuristic and swarm intelligence approach in optimization"
      },
      {
        "paperId": null,
        "title": "\u2018\u2018Using"
      },
      {
        "paperId": null,
        "title": "\u2018\u2018Reinforcementlearning-basedadaptiveoptimalfuzzyMPPTcontrolforvariablespeedwindturbine,\u2019\u2019"
      },
      {
        "paperId": null,
        "title": "ResearcherwiththeFacultyofElectricalEngineeringandComputing(FER), UniversityofZagreb"
      },
      {
        "paperId": null,
        "title": "andmodelingenergystoragesystemsforpowersystemapplications"
      },
      {
        "paperId": null,
        "title": "Telstra Research Laboratories, Flinders University, Intelligent Pixels Inc"
      },
      {
        "paperId": null,
        "title": "\u2018\u2018Slidingmodecontrolofquadraticboostconvertersbasedon min-type control strategy,\u2019\u2019"
      }
    ],
    "cited_by": [
      {
        "paperId": "12cf2ce0ae855fcfc0d3ef0d0113eeaaa0484c53",
        "title": "Design of a Robust Adaptive Cascade Fractional-Order Proportional\u2013Integral\u2013Derivative Controller Enhanced by Reinforcement Learning Algorithm for Speed Regulation of Brushless DC Motor in Electric Vehicles"
      },
      {
        "paperId": "a16cc6b87fa93149982b48c3c1e1ebfae42ea9ea",
        "title": "Sandpiper Optimized PI\u2010Controlled Triple\u2010Core Integrated Boost Converter for Solar\u2010Powered BLDC Motor Driven Electric Vehicle"
      },
      {
        "paperId": "c4d5cb7408dd6f9e60adbda8475c0068141d807e",
        "title": "Design of a novel robust adaptive fractional-order model predictive controller for boost converter using grey wolf optimization algorithm"
      },
      {
        "paperId": "0d2fdc1c028cf8e1b06ff67dfb4d43c933de662c",
        "title": "The Voltage Regulation of Boost Converters via a Hybrid DQN-PI Control Strategy Under Large-Signal Disturbances"
      },
      {
        "paperId": "7ba2c6b221c0372c46c8b38971ac9e5b217ea754",
        "title": "Advancing Stability in Robot Manipulators: A Review of Recent Progress and Parameters"
      },
      {
        "paperId": "6ca0feb944694952eb9d1e995d8b49556ca21a62",
        "title": "Optimized Voltage Feed-Forward Control for Photovoltaic-Battery DC Microgrid Using Enhanced Grey Wolf Algorithm"
      },
      {
        "paperId": "3dd69b53481141c0921c54beb283ac489fe98319",
        "title": "Optimization of PID Controllers Using Groupers and Moray Eels Optimization with Dual-Stream Multi-Dependency Graph Neural Networks for Enhanced Dynamic Performance"
      },
      {
        "paperId": "66364ee1b60454b744243f540536342c48b2fa77",
        "title": "Hybrid Power Conversion System Using PWM Rectification and Advanced Bidirectional DC-DC Control for Efficient Load Applications"
      },
      {
        "paperId": "249ee0ea4159789304f8f33533fa6e1c42fe7d92",
        "title": "Artificial Intelligence and Digital Twin Technologies for Power Converter Control in Transportation Applications: A Review"
      },
      {
        "paperId": "4c106e821e4f3b713b31b2950d5683de5fa2f7cd",
        "title": "Genetic Algorithm-based Model Predictive Control for Partial Shading Mitigation on PV Array"
      },
      {
        "paperId": "001de071185f248d4dac25495e1169082a99371a",
        "title": "Robust Cascade Pid-Based Controller Design for Brushless Dc Motor Using Antlion Optimization Algorithm"
      }
    ],
    "score": 11.0
  },
  {
    "id": "c225c6e24526c160bbceaaa36b447df9d8e95f13",
    "title": "Safe Reinforcement Learning in Autonomous Driving With Epistemic Uncertainty Estimation",
    "authors": [
      "Zhengran Zhang",
      "Qi Liu",
      "Yanjie Li",
      "Ke Lin",
      "Linyu Li"
    ],
    "year": 2024,
    "citationCount": 11,
    "abstract": "Safety is one of the critical challenges in the autonomous driving task. Recent works address the safety by implementing a safe reinforcement learning (safe RL) mechanism. However, most approaches make conservative decisions without knowing the confidence of the actions, which ultimately causes traffic congestion and low travel efficiency. This paper proposes an uncertainty-augmented Lagrangian safe reinforcement algorithm (Lag-U) to improve exploration and safety performance for autonomous driving. First, epistemic uncertainty is introduced into safe RL by using deep ensemble. We use the estimated epistemic uncertainty to encourage exploration and to learn a risk-sensitive policy by adaptively modifying safety constraints. Second, we facilitate an intervention assurance to choose safer actions based on the quantified epistemic uncertainty during deployment. Experimental results prove that the proposed method outperforms other safe RL baselines. The trained vehicle can make a decent trade-off between high efficiency and avoiding risks, thus preventing ultra-conservative policy.",
    "url": "https://www.semanticscholar.org/paper/c225c6e24526c160bbceaaa36b447df9d8e95f13",
    "pdf_url": "https://doi.org/10.1109/TITS.2024.3397700",
    "venue": "IEEE transactions on intelligent transportation systems (Print)",
    "publicationDate": "2024-10-01",
    "externalIds": {
      "DBLP": "journals/tits/ZhangLLLL24",
      "DOI": "10.1109/TITS.2024.3397700",
      "CorpusId": 269945817
    },
    "references": [],
    "cited_by": [
      {
        "paperId": "50f0c65126cfc65b80bb04575ac64df54bbcafdb",
        "title": "Group Confident Policy Optimization"
      },
      {
        "paperId": "97bc21aa191fab2f8ea11ee19608eef0d80ac97d",
        "title": "A Connected-Automated Vehicles-Based Dynamic Speed Limit Control Strategy for Improving Safety and Efficiency of Freeway Tunnels: An Augmented Lagrange Safe Reinforcement Learning Framework"
      },
      {
        "paperId": "ca9d1cc7f577d3e1fa404c6c8c7864b198de2bc5",
        "title": "Simulating Vegetation Potential and Quantifying Uncertainty for Precision Forestation in Arid Regions"
      },
      {
        "paperId": "6cddab0e099ce5df22fd49b94a7a326056b6ebc5",
        "title": "Two-Stage AL-iLQR-Based Trajectory Planning for Special-Shaped Curb Cleaning of Sweeper"
      },
      {
        "paperId": "73c73f5ceaa7241f9aa54eefda579858f859b913",
        "title": "Uncertainty-Aware Safety-Critical Decision and Control for Autonomous Vehicles at Unsignalized Intersections"
      },
      {
        "paperId": "c8d8513bcde9c9225baa0eef14ecdd399cb96d0a",
        "title": "A Survey of Reinforcement Learning-Based Motion Planning for Autonomous Driving: Lessons Learned from a Driving Task Perspective"
      },
      {
        "paperId": "7017ea7db81044674425567b69f23416edd51b9e",
        "title": "Hybrid Action Based Reinforcement Learning for Multi-Objective Compatible Autonomous Driving"
      },
      {
        "paperId": "e6f87481917449600f901c2b9a15be7d7728fbea",
        "title": "PlatoonSim: Autonomous Truck Platooning Simulation Framework"
      },
      {
        "paperId": "4b9f8735de6c38c5372e8d7d37aa08d5babc88f4",
        "title": "Adaptive Levels of Automation Adjustment with Reinforcement Learning and Fuzzy Logic"
      },
      {
        "paperId": "195ef03429ccb1879ade7e6114dc954619d02243",
        "title": "Knowledge Transfer from Simple to Complex: A Safe and Efficient Reinforcement Learning Framework for Autonomous Driving Decision-Making"
      },
      {
        "paperId": "c5fb88997e4cfa3950084e18da75382cd3c65720",
        "title": "F ROM S IMPLE TO C OMPLEX : K NOWLEDGE T RANSFER IN S AFE AND E FFICIENT R EINFORCEMENT L EARNING FOR A UTONOMOUS D RIVING"
      }
    ],
    "score": 11.0
  },
  {
    "id": "c734971c6000e3f2769ab5165d00816af80dd76f",
    "title": "In-context Exploration-Exploitation for Reinforcement Learning",
    "authors": [
      "Zhenwen Dai",
      "Federico Tomasi",
      "Sina Ghiassian"
    ],
    "year": 2024,
    "citationCount": 10,
    "abstract": "In-context learning is a promising approach for online policy learning of offline reinforcement learning (RL) methods, which can be achieved at inference time without gradient optimization. However, this method is hindered by significant computational costs resulting from the gathering of large training trajectory sets and the need to train large Transformer models. We address this challenge by introducing an In-context Exploration-Exploitation (ICEE) algorithm, designed to optimize the efficiency of in-context policy learning. Unlike existing models, ICEE performs an exploration-exploitation trade-off at inference time within a Transformer model, without the need for explicit Bayesian inference. Consequently, ICEE can solve Bayesian optimization problems as efficiently as Gaussian process biased methods do, but in significantly less time. Through experiments in grid world environments, we demonstrate that ICEE can learn to solve new RL tasks using only tens of episodes, marking a substantial improvement over the hundreds of episodes needed by the previous in-context learning method.",
    "url": "https://www.semanticscholar.org/paper/c734971c6000e3f2769ab5165d00816af80dd76f",
    "pdf_url": "https://arxiv.org/pdf/2403.06826.pdf",
    "venue": "International Conference on Learning Representations",
    "publicationDate": "2024-03-11",
    "externalIds": {
      "ArXiv": "2403.06826",
      "DBLP": "conf/iclr/DaiTG24",
      "DOI": "10.48550/arXiv.2403.06826",
      "CorpusId": 268363726
    },
    "references": [
      {
        "paperId": "ba9bff4deca65c2fb7131065ef81570687a6916c",
        "title": "BayesO: A Bayesian optimization framework in Python"
      },
      {
        "paperId": "eb971944bccf9793ac463c3e2f4d4251d4e8e071",
        "title": "Do Large Language Models Know What They Don't Know?"
      },
      {
        "paperId": "253c900b0569694d57e8f2904e330b51ae740fd8",
        "title": "A Mechanism for Sample-Efficient In-Context Learning for Sparse Retrieval Tasks"
      },
      {
        "paperId": "bfe6fd05f09647b001c7eb6e333a95c881c88344",
        "title": "Human-Timescale Adaptation in an Open-Ended Task Space"
      },
      {
        "paperId": "860bc4f071f35d6d8529a52c2c1858d030779a6a",
        "title": "In-context Reinforcement Learning with Algorithm Distillation"
      },
      {
        "paperId": "51606f776a3b53a08d5a7e726f9ad771d52e2506",
        "title": "Multi-Game Decision Transformers"
      },
      {
        "paperId": "5922f437512158970c417f4413bface021df5f78",
        "title": "A Generalist Agent"
      },
      {
        "paperId": "f4df78183261538e718066331898ee5cad7cad05",
        "title": "Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?"
      },
      {
        "paperId": "3c4831e493dc7a362359cdb1883e8c16d084c55b",
        "title": "A History of Meta-gradient: Gradient Methods for Meta-learning"
      },
      {
        "paperId": "eb92a453cf982126fa2125d4c8915352a52af54d",
        "title": "Online Decision Transformer"
      },
      {
        "paperId": "f8befa0bc3442979ff19e070f7c6b16d66a776c5",
        "title": "Bootstrapped Meta-Learning"
      },
      {
        "paperId": "61f371768cdc093828f432660e22f7a17f22e2af",
        "title": "Offline Meta-Reinforcement Learning with Online Self-Supervision"
      },
      {
        "paperId": "f864d4d2267abba15eb43db54f58286aef78292b",
        "title": "Offline Reinforcement Learning as One Big Sequence Modeling Problem"
      },
      {
        "paperId": "c1ad5f9b32d80f1c65d67894e5b8c2fdf0ae4500",
        "title": "Decision Transformer: Reinforcement Learning via Sequence Modeling"
      },
      {
        "paperId": "3544650f12a05cf4ed3bf2f7e22fc5c02fcabf50",
        "title": "Pretrained Transformers as Universal Computation Engines"
      },
      {
        "paperId": "17908c7db26985704c00dd4521932f25c43dbe17",
        "title": "Offline Meta-Reinforcement Learning with Advantage Weighting"
      },
      {
        "paperId": "759ae1234d46e2d1399ce9d642724738a766ed22",
        "title": "Meta-Gradient Reinforcement Learning with an Objective Discovered Online"
      },
      {
        "paperId": "0272b14dd471fe7b81df703af1b71d7600b77215",
        "title": "Accelerating Online Reinforcement Learning with Offline Datasets"
      },
      {
        "paperId": "5e7bc93622416f14e6948a500278bfbe58cd3890",
        "title": "Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems"
      },
      {
        "paperId": "020bb2ba5f3923858cd6882ba5c5a44ea8041ab6",
        "title": "Meta-Learning in Neural Networks: A Survey"
      },
      {
        "paperId": "0b8c259e7fabb1c49c907af9a8ff63d8d38ebcff",
        "title": "A Self-Tuning Actor-Critic Algorithm"
      },
      {
        "paperId": "0881655dcdf891f529ebe7ac18301e138a5e265b",
        "title": "Keep Doing What Worked: Behavioral Modelling Priors for Offline Reinforcement Learning"
      },
      {
        "paperId": "63dc500cfe0d3b4bd15a022dd237cc130ebc7d8c",
        "title": "Discovery of Useful Questions as Auxiliary Tasks"
      },
      {
        "paperId": "5285cb8faada5de8a92a47622950f6cfd476ac1d",
        "title": "Off-Policy Deep Reinforcement Learning without Exploration"
      },
      {
        "paperId": "37b3d9ab049c5671fed29f56cacee858d98c2ea8",
        "title": "Unsupervised Learning via Meta-Learning"
      },
      {
        "paperId": "b93317f61c6ed99542da9d1d691ded9732c16c1c",
        "title": "Unsupervised Meta-Learning for Reinforcement Learning"
      },
      {
        "paperId": "2a49a71c9d40051a03c4445fe49025bc75d9eeb6",
        "title": "Meta-Gradient Reinforcement Learning"
      },
      {
        "paperId": "35271d36cb20bf8d716e79c9dd15d738d955a931",
        "title": "On Learning Intrinsic Rewards for Policy Gradient Methods"
      },
      {
        "paperId": "b80991d12b41a5a68dc14dd87b692c0f903ceb9c",
        "title": "Some Considerations on Learning to Explore via Meta-Reinforcement Learning"
      },
      {
        "paperId": "7e9c1e0d247b20a0683f4797d9ea248c3b53d424",
        "title": "A Simple Neural Attentive Meta-Learner"
      },
      {
        "paperId": "5c57bb5630835a05eb1c3d0df3e12d6180d75de2",
        "title": "One-Shot Imitation Learning"
      },
      {
        "paperId": "b8ff7e02ffa1577d125acd3e998e8ce76a9059dc",
        "title": "Learned Optimizers that Scale and Generalize"
      },
      {
        "paperId": "c889d6f98e6d79b89c3a6adf8a921f88fa6ba518",
        "title": "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks"
      },
      {
        "paperId": "282a380fb5ac26d99667224cef8c630f6882704f",
        "title": "Learning to reinforcement learn"
      },
      {
        "paperId": "29c887794eed2ca9462638ff853e6fe1ab91d5d8",
        "title": "Optimization as a Model for Few-Shot Learning"
      },
      {
        "paperId": "71683e224ab91617950956b5005ed0439a733a71",
        "title": "Learning to learn by gradient descent by gradient descent"
      },
      {
        "paperId": "340f48901f72278f6bf78a04ee5b01df208cc508",
        "title": "Human-level control through deep reinforcement learning"
      },
      {
        "paperId": "3346e72b31399527d037d6ed091a2ab96225e0fd",
        "title": "Preliminary"
      },
      {
        "paperId": "41356b8998dd7ddf89429445320d82a269e3ab14",
        "title": "Tree-Based Batch Mode Reinforcement Learning"
      },
      {
        "paperId": "282001869bd502c7917db8b32b75593addfbbc68",
        "title": "Neural Fitted Q Iteration - First Experiences with a Data Efficient Neural Reinforcement Learning Method"
      },
      {
        "paperId": "a20f0ce0616def7cc9a87446c228906cd5da093b",
        "title": "Policy Gradient Methods for Reinforcement Learning with Function Approximation"
      },
      {
        "paperId": "d7986fc1c9026180468023f3dcc1d7c5f6379b8f",
        "title": "Simple Principles of Metalearning"
      },
      {
        "paperId": "9cd8193a66cf53143cbba6ccb0c7b9c2ebf2452b",
        "title": "Self-improving reactive agents based on reinforcement learning, planning and teaching"
      },
      {
        "paperId": "a91635f8d0e7fb804efd1c38d9c24ee952ba7076",
        "title": "Learning to predict by the methods of temporal differences"
      },
      {
        "paperId": "c7629a4d7e1c87fd1ea73850bcb800538fd0aa4b",
        "title": "VariBAD: Variational Bayes-Adaptive Deep RL via Meta-Learning"
      },
      {
        "paperId": "d27edce419e1c731c0aeb9e1842d1e022b2cc6ab",
        "title": "Offline Meta Reinforcement Learning - Identifiability Challenges and Effective Data Collection Strategies"
      },
      {
        "paperId": "53a58f0f4a286c42b543f2ec29ba6b5e55befdf3",
        "title": "Appendix to: BOTORCH: A Framework for Efficient Monte-Carlo Bayesian Optimization"
      },
      {
        "paperId": "05ea5e3bbcc22ccf57f9b458ee7d72f9c51682b1",
        "title": "RL Unplugged: A Suite of Benchmarks for Of\ufb02ine Reinforcement Learning"
      },
      {
        "paperId": "44b6da0cd36c0fa2f7d3485c6dc0b6d2fbe379bb",
        "title": "Learning how to learn"
      },
      {
        "paperId": "5c65d095600d6c647426fa3bc45031b208882d5f",
        "title": "Batch Reinforcement Learning"
      },
      {
        "paperId": "5dbe244846bfedfd687be0cdaba19befcd96c8f6",
        "title": "Conference Paper"
      },
      {
        "paperId": "97efafdb4a3942ab3efba53ded7413199f79c054",
        "title": "Reinforcement Learning: An Introduction"
      }
    ],
    "cited_by": [
      {
        "paperId": "fe7ad449b6ca304b4d837ea3214c923a06016cb6",
        "title": "Safe In-Context Reinforcement Learning"
      },
      {
        "paperId": "bc21851d7bac221333699043a0b7f7e6cc009539",
        "title": "Towards Provable Emergence of In-Context Reinforcement Learning"
      },
      {
        "paperId": "485de643643640b916eb18db1b7028f8441e924f",
        "title": "CoEx -- Co-evolving World-model and Exploration"
      },
      {
        "paperId": "c0770b9e2c5fc42eff550a1d86de81baa35a3754",
        "title": "CooT: Learning to Coordinate In-Context with Coordination Transformers"
      },
      {
        "paperId": "7dd12727f73fb87580fa31c506a5f9b1649d319f",
        "title": "Mixture-of-Experts Meets In-Context Reinforcement Learning"
      },
      {
        "paperId": "2aeeabbdc17fa5d1102a6aeeaf4e4582f3fce7ff",
        "title": "Scalable In-Context Q-Learning"
      },
      {
        "paperId": "7b86f6dd09db91b96ce407b8bc2534f0dc811309",
        "title": "Reward Is Enough: LLMs Are In-Context Reinforcement Learners"
      },
      {
        "paperId": "fd2efe2ef4ddd5b62d4103450c8596a1206edda2",
        "title": "Toward Efficient Exploration by Large Language Model Agents"
      },
      {
        "paperId": "59f85416d7e5d10905d51536e68133c354bea2ed",
        "title": "Yes, Q-learning Helps Offline In-Context RL"
      },
      {
        "paperId": "1e315965d42c0e32321327738e81e3fe6a81b81f",
        "title": "Why is prompting hard? Understanding prompts on binary sequence predictors"
      }
    ],
    "score": 10.0
  },
  {
    "id": "ed8ea3d06c173849f02ee8afcf8db07df0f31261",
    "title": "Optimal Energy Management of Plug-In Hybrid Electric Vehicles Through Ensemble Reinforcement Learning With Exploration-to-Exploitation Ratio Control",
    "authors": [
      "Bin Shuai",
      "Min Hua",
      "Yanfei Li",
      "Shijin Shuai",
      "Hongming Xu",
      "Quan Zhou"
    ],
    "year": 2025,
    "citationCount": 10,
    "abstract": "Reinforcement learning (RL) has demonstrated its advantages in the intelligent control of many vehicle systems. However, controlling the exploration-to-exploitation (E2E) ratio of RL for the best performance in real-world operations is a great challenge. To obtain the optimal E2E ratio for managing the energy flow of a plug-in hybrid electric vehicle (PHEV) in real-world driving, this paper proposes an ensemble learning scheme based on two independent Q-learning agents working back-to-back competitively. At each sampling time, these agents generate two candidate control actions based on state observation and their control policies. Three decay functions, including the widely-used exponential decay and two new decay methods i.e., reciprocal function-base decay and step-based decay, are introduced to formulate one-dimensional look-up tables for E2E control. Then the PHEV control action will be selected from the candidate actions by a learning automata module(LAM) developed in this paper. The combinations of the three decay methods and three ensemble strategies with maximum-based, random-based, and weighted-based methods are investigated with the considerations of their energy efficiency, real-time performance, and robustness. Experiments are carried out on software-in-loop and hardware-in-the-loop platforms to demonstrate the energy-saving potentials. By implementing the ensemble learning scheme based on the weighted-based ensemble method in the control of the studied PHEV, up to 1.04% of energy can be saved under the predefined real-world driving cycles compared to the conventional Q-learning scheme.",
    "url": "https://www.semanticscholar.org/paper/ed8ea3d06c173849f02ee8afcf8db07df0f31261",
    "pdf_url": "https://doi.org/10.1109/TIV.2024.3377215",
    "venue": "IEEE Transactions on Intelligent Vehicles",
    "publicationDate": "2025-04-01",
    "externalIds": {
      "DBLP": "journals/tiv/ShuaiHLSXZ25",
      "DOI": "10.1109/TIV.2024.3377215",
      "CorpusId": 268583530
    },
    "references": [],
    "cited_by": [
      {
        "paperId": "9e574c41c1abf086324a02740d8b4a61e61953cd",
        "title": "Efficient Energy Management of Plug-In Hybrid Electric Vehicles Through Ensemble With In-Target Minimization Q-Learning"
      },
      {
        "paperId": "9be4781f218685eb416153888472448a47141dc2",
        "title": "Adversarial Swarm Learning for Robust Equivalent Consumption Minimization of Electrified Vehicles"
      },
      {
        "paperId": "2110eba258d204f4189d146760c72f5d031c04e4",
        "title": "A Combined Energy Management Strategy for Heavy-Duty Trucks Based on Global Traffic Information Optimization"
      },
      {
        "paperId": "77e9456499a7d4a86d0a9fe3ace7597ce858834b",
        "title": "Explicit Nonlinear Control for Optimal Trajectory Tracking of Autonomous Vehicles"
      },
      {
        "paperId": "853599c8e206bdb0cb515684b056b1c743b76b9e",
        "title": "Crested Porcupine Optimizer for Enhanced Power Management in Plug-In Hybrid Electric Vehicles"
      },
      {
        "paperId": "3e65ce2ac764fbbef0f9de7c0f61d6f2de04f6c2",
        "title": "Distributional Soft Actor-Critic with Harmonic Gradient for Safe and Efficient Autonomous Driving in Multi-Lane Scenarios"
      },
      {
        "paperId": "a448aadec2abe2bc898d7898a398db852dd3d137",
        "title": "Reinforcement learning-based energy management for hybrid electric vehicles: A comprehensive up-to-date review on methods, challenges, and research gaps"
      },
      {
        "paperId": "83e0eaacc431ef7c680e296a41527e29d36ae43b",
        "title": "Constrained Optimal Fuel Consumption of HEVs under Observational Noise"
      },
      {
        "paperId": "d1a6626bd4c4e3bdfcb2395df06d393788b961c5",
        "title": "Enhancing electric vehicle charging efficiency at the aggregator level: A deep-weighted ensemble model for wholesale electricity price forecasting"
      },
      {
        "paperId": "288d124ff289227ad34e04c456cbddd1cba431bb",
        "title": "Communication-Efficient MARL for Platoon Stability and Energy-Efficiency Co-Optimization in Cooperative Adaptive Cruise Control of CAVs"
      }
    ],
    "score": 10.0
  },
  {
    "id": "dcbafb5ec43572d6ca170d86569d09a5dd40a8f8",
    "title": "Excluding the Irrelevant: Focusing Reinforcement Learning through Continuous Action Masking",
    "authors": [
      "Roland Stolz",
      "Hanna Krasowski",
      "Jakob Thumm",
      "Michael Eichelbeck",
      "Philipp Gassert",
      "Matthias Althoff"
    ],
    "year": 2024,
    "citationCount": 10,
    "abstract": "Continuous action spaces in reinforcement learning (RL) are commonly defined as multidimensional intervals. While intervals usually reflect the action boundaries for tasks well, they can be challenging for learning because the typically large global action space leads to frequent exploration of irrelevant actions. Yet, little task knowledge can be sufficient to identify significantly smaller state-specific sets of relevant actions. Focusing learning on these relevant actions can significantly improve training efficiency and effectiveness. In this paper, we propose to focus learning on the set of relevant actions and introduce three continuous action masking methods for exactly mapping the action space to the state-dependent set of relevant actions. Thus, our methods ensure that only relevant actions are executed, enhancing the predictability of the RL agent and enabling its use in safety-critical applications. We further derive the implications of the proposed methods on the policy gradient. Using proximal policy optimization (PPO), we evaluate our methods on four control tasks, where the relevant action set is computed based on the system dynamics and a relevant state set. Our experiments show that the three action masking methods achieve higher final rewards and converge faster than the baseline without action masking.",
    "url": "https://www.semanticscholar.org/paper/dcbafb5ec43572d6ca170d86569d09a5dd40a8f8",
    "pdf_url": "https://arxiv.org/pdf/2406.03704.pdf",
    "venue": "Neural Information Processing Systems",
    "publicationDate": "2024-06-06",
    "externalIds": {
      "DBLP": "conf/nips/StolzKTEGA24",
      "ArXiv": "2406.03704",
      "DOI": "10.48550/arXiv.2406.03704",
      "CorpusId": 270286022
    },
    "references": [
      {
        "paperId": "0c102b2cabf3cf644bb53ad113043f9e899e8eb4",
        "title": "Pure-Past Action Masking"
      },
      {
        "paperId": "d99586f79e3d76a94b3c5abf42e5927702febbd8",
        "title": "Scalable Computation of Robust Control Invariant Sets of Nonlinear Systems"
      },
      {
        "paperId": "a3b7390bfb5fab18787e13a7f7f62b76a8b3e7d9",
        "title": "No Prior Mask: Eliminate Redundant Action for Deep Reinforcement Learning"
      },
      {
        "paperId": "9e6081161cd6569c75ebc36cba3d0db1e544f365",
        "title": "Enhancement of Distribution Network Resilience: A Multi-Buffer Invalid-Action-Mask Double Q-Network Approach for Distribution Network Restoration"
      },
      {
        "paperId": "816227a7e7d6d681695268087ea61204aebe668d",
        "title": "Autonomous Decision Making with Reinforcement Learning in Multi-UAV Air Combat"
      },
      {
        "paperId": "b4f3493466a69093c943cb51a13693a0717c1cac",
        "title": "Exploring the Use of Invalid Action Masking in Reinforcement Learning: A Comparative Study of On-Policy and Off-Policy Algorithms in Real-Time Strategy Games"
      },
      {
        "paperId": "47d180d6ad8e43b1ddccd208026158ae26ba4e2f",
        "title": "Reinforcement learning algorithms: A brief survey"
      },
      {
        "paperId": "aee25ec357bfb8b4422e04630bc46c8f5a03a695",
        "title": "A Survey on Deep Reinforcement Learning Algorithms for Robotic Manipulation"
      },
      {
        "paperId": "9be31cb657bd092ae0e124a9eeb7e21a1fe1facc",
        "title": "Global path planning algorithm based on double DQN for multi-tasks amphibious unmanned surface vehicle"
      },
      {
        "paperId": "b1fb5611b7e0e92f1f9d2f269e6e7ce8b4d1cedb",
        "title": "Provably Safe Reinforcement Learning: Conceptual Analysis, Survey, and Benchmarking"
      },
      {
        "paperId": "525614d6ddabff376949d4feb4a8c0a736c50673",
        "title": "Fuzzy Action-Masked Reinforcement Learning Behavior Planning for Highly Automated Driving"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "b8baed505dc57a6c4d677255c362ec5483111dfe",
        "title": "Computationally Efficient Safe Reinforcement Learning for Power Systems"
      },
      {
        "paperId": "0b17d0a7bb2f12afb5d00804ec4e03f4705b2813",
        "title": "On the co-NP-completeness of the zonotope containment problem"
      },
      {
        "paperId": "5a1b92aa50797a7c1e99b8840ff01aad66038596",
        "title": "Sim-to-Real Transfer in Deep Reinforcement Learning for Robotics: a Survey"
      },
      {
        "paperId": "becd04c7b38bb6041c1cd8a0fdb2dce3bd724b97",
        "title": "Safe Reinforcement Learning for Autonomous Lane Changing Using Set-Based Prediction"
      },
      {
        "paperId": "83b7d091fa44906d4cc873fdfaa4ba3d1858e7bb",
        "title": "A Closer Look at Invalid Action Masking in Policy Gradient Algorithms"
      },
      {
        "paperId": "b2a0d1383d5fe34ed283b3300b1e9f359dab8fcb",
        "title": "Constrained polynomial zonotopes"
      },
      {
        "paperId": "f3f75a24c22a2d01f4ac4915761a25be5d78bf6f",
        "title": "Multivariate Normal Distribution"
      },
      {
        "paperId": "c6c5155c38d4e4ed3db17ca3ab9bcec3a2e7be4d",
        "title": "Continuous-Discrete Reinforcement Learning for Hybrid Control in Robotics"
      },
      {
        "paperId": "a2e43270a9b1421e452c2975e5163e2a216abeac",
        "title": "A Survey of Deep Reinforcement Learning in Video Games"
      },
      {
        "paperId": "292bbd1287a0674cd9e3e79224e768ca557dcf81",
        "title": "Mastering Complex Control in MOBA Games with Deep Reinforcement Learning"
      },
      {
        "paperId": "4cdf2fad22afc865999747336c7399fe422e6e8e",
        "title": "Optuna: A Next-generation Hyperparameter Optimization Framework"
      },
      {
        "paperId": "1076d94770ab8824a9003d3c6a267b676dbd3712",
        "title": "Linear Encodings for Polytope Containment Problems"
      },
      {
        "paperId": "6667fdb86a2d06d4895d7f8cb6ac82c314e65e16",
        "title": "Invariant, viability and discriminating kernel under-approximation via zonotope scaling: poster abstract"
      },
      {
        "paperId": "08fa8d94f7d004547204f0743a24976c52cff796",
        "title": "High-level Decision Making for Safe and Reasonable Autonomous Lane Changing using Reinforcement Learning"
      },
      {
        "paperId": "8bf1aa5dcae7183c5bd634da7e39bef8909306d4",
        "title": "Safe Reinforcement Learning via Formal Methods: Toward Safe Control Through Proof and Learning"
      },
      {
        "paperId": "4debb99c0c63bfaa97dd433bc2828e4dac81c48b",
        "title": "Addressing Function Approximation Error in Actor-Critic Methods"
      },
      {
        "paperId": "811df72e210e20de99719539505da54762a11c6d",
        "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "1cbf4e970fb4d4d5556b66c298bccca8a9510c62",
        "title": "Combining zonotopes and support functions for efficient reachability analysis of linear systems"
      },
      {
        "paperId": "fbeb715b912f5b80b06855c71b8abdfc6ccafe34",
        "title": "Scalable Safety-Preserving Robust Control Synthesis for Continuous-Time Linear Systems"
      },
      {
        "paperId": "b354ee518bfc1ac0d8ac447eece9edb69e92eae1",
        "title": "MuJoCo: A physics engine for model-based control"
      },
      {
        "paperId": "1499f206b576914668ce4d01829f401ef57e4cf8",
        "title": "A tutorial on geometric programming"
      },
      {
        "paperId": "b94a21b58ab6d67d787c43a623c7381b8a1aa45d",
        "title": "The Image Computation Problem in Hybrid Systems Model Checking"
      },
      {
        "paperId": "668b1277fbece28c4841eeab1c97e4ebd0079700",
        "title": "Pattern Recognition and Machine Learning"
      },
      {
        "paperId": "0220cd362ba6d07e9023b5f7e097a31afda8c086",
        "title": "Hit-and-run from a corner"
      },
      {
        "paperId": "3ac80231762dc66e98054607fd4d4ced1be50c62",
        "title": "An adaptive numerical cubature algorithm for simplices"
      },
      {
        "paperId": "a20f0ce0616def7cc9a87446c228906cd5da093b",
        "title": "Policy Gradient Methods for Reinforcement Learning with Function Approximation"
      },
      {
        "paperId": "e3fc5b5627af62ee6981a02090cf6bae368202d7",
        "title": "Stable-Baselines3: Reliable Reinforcement Learning Implementations"
      },
      {
        "paperId": "b7c1bdf7d9c92f7d38aeb2ca106189bffc903f5c",
        "title": "Reinforcement learning - an introduction, 2nd Edition"
      },
      {
        "paperId": null,
        "title": "The Truncated Normal Distribution . Department of Scientific Computing Website, Florida State University, Tallahassee"
      },
      {
        "paperId": "e9d99e67723dabcdb553ee30e97e0a731b37a274",
        "title": "Reachability Analysis and its Application to the Safety Assessment of Autonomous Cars"
      },
      {
        "paperId": "c8557a70ecdeec83f70954c5f169393c7f04fc9e",
        "title": "Introduction to linear optimization"
      },
      {
        "paperId": "78809eb2b01ee81b5b832caab51ab57d09a2ef09",
        "title": "Hit-and-Run Methods"
      }
    ],
    "cited_by": [
      {
        "paperId": "17688e6e0d7228c908bd0f79ea57e6c8fd77e72b",
        "title": "Learning to Interact in World Latent for Team Coordination"
      },
      {
        "paperId": "74497495766451eee0d10d3d97c45e5bf1c6048b",
        "title": "Safe Reinforcement Learning using Action Projection: Safeguard the Policy or the Environment?"
      },
      {
        "paperId": "50598ba407f9b3bf57fad7d8e460869c947a7514",
        "title": "Beyond Fixed Morphologies: Learning Graph Policies with Trust Region Compensation in Variable Action Spaces"
      },
      {
        "paperId": "6b75bfb07e46674c1469da86ccb89ab7aa7d8ffb",
        "title": "Increasing Interaction Fidelity: Training Routines for Biomechanical Models in HCI"
      },
      {
        "paperId": "8ddbd6dccc2674c5dad2b962465014bd8c73f705",
        "title": "Deep Reinforcement Learning for Zero-Shot Coverage Path Planning With Mobile Robots"
      },
      {
        "paperId": "a218e2f3f7b59aaaacccffd466e158c062d0784d",
        "title": "Action Space Reduction Strategies for Reinforcement Learning in Autonomous Driving"
      },
      {
        "paperId": "c7547531757e21cf43dc0e2fc529db178351ef76",
        "title": "Leveraging Analytic Gradients in Provably Safe Reinforcement Learning"
      },
      {
        "paperId": "f5d84becc86618ff52ab7c99a33b0126d4dd6f72",
        "title": "Embodied Escaping: End-to-End Reinforcement Learning for Robot Navigation in Narrow Environment"
      },
      {
        "paperId": "e3966d64b8dfa20b3fec9da0dd7f1534f72ceff2",
        "title": "CAMEL: Continuous Action Masking Enabled by Large Language Models for Reinforcement Learning"
      },
      {
        "paperId": "46b6a615baf77306611f77ecf6dfe35d611eab59",
        "title": "Directed Exploration in Reinforcement Learning from Linear Temporal Logic"
      }
    ],
    "score": 10.0
  },
  {
    "id": "492f441bc6fdbb5f4b9273197ae563126439abeb",
    "title": "Learning and Repair of Deep Reinforcement Learning Policies from Fuzz-Testing Data",
    "authors": [
      "Martin Tappler",
      "Andrea Pferscher",
      "B. Aichernig",
      "Bettina K\u00f6nighofer"
    ],
    "year": 2024,
    "citationCount": 10,
    "abstract": "Reinforcement learning from demonstrations (RLfD) is a promising approach to improve the exploration efficiency of reinforcement learning (RL) by learning from expert demonstrations in addition to interactions with the environment. In this paper, we propose a framework that combines techniques from search-based testing with RLfD with the goal to raise the level of dependability of RL policies and to reduce human engineering effort. Within our framework, we provide methods for efficiently training, evaluating, and repairing RL policies. Instead of relying on the costly collection of demonstrations from (human) experts, we automatically compute a diverse set of demonstrations via search-based fuzzing methods and use the fuzz demonstrations for RLfD. To evaluate the safety and robustness of the trained RL agent, we search for safety-critical scenarios in the black-box environment. Finally, when unsafe behavior is detected, we compute demonstrations through fuzz testing that represent safe behavior and use them to repair the policy. Our experiments show that our framework is able to efficiently learn high-performing and safe policies without requiring any expert knowledge.",
    "url": "https://www.semanticscholar.org/paper/492f441bc6fdbb5f4b9273197ae563126439abeb",
    "pdf_url": "https://doi.org/10.1145/3597503.3623311",
    "venue": "International Conference on Software Engineering",
    "publicationDate": "2024-02-06",
    "externalIds": {
      "DBLP": "conf/icse/TapplerPAK24",
      "DOI": "10.1145/3597503.3623311",
      "CorpusId": 267523750
    },
    "references": [
      {
        "paperId": "8bb7cecd1bc3fa470aa882158e7553705b1a6141",
        "title": "Minigrid & Miniworld: Modular & Customizable Reinforcement Learning Environments for Goal-Oriented Tasks"
      },
      {
        "paperId": "899f28625ba1166ae000633215851605d948b9da",
        "title": "Testing of Deep Reinforcement Learning Agents with Surrogate Models"
      },
      {
        "paperId": "5d8939023cfdaa18d0ef00b62914da00adba3e63",
        "title": "Towards mutation testing of Reinforcement Learning systems"
      },
      {
        "paperId": "ee2e080407e112abb764631405df5832934eeb57",
        "title": "Self-Adaptive Imitation Learning: Learning Tasks with Delayed Rewards from Sub-optimal Demonstrations"
      },
      {
        "paperId": "b58cee34a6fe2ce8a909de80848eb7f210f85236",
        "title": "A Search-Based Testing Approach for Deep Reinforcement Learning Agents"
      },
      {
        "paperId": "aee2222a2f75bbdcd86ef54fce58f2a4ecf20106",
        "title": "Search-Based Testing of Reinforcement Learning"
      },
      {
        "paperId": "14aa7aa1249075940b8a460d01e8ae821f40ab32",
        "title": "Constraint Sampling Reinforcement Learning: Incorporating Expertise For Faster Learning"
      },
      {
        "paperId": "274e32e992bb5ce70feb1a474ea9a4cc7886dad8",
        "title": "Model-free reinforcement learning from expert demonstrations: a survey"
      },
      {
        "paperId": "42a525c2f7ce08be82c363ebc5bcf6e88e8bf569",
        "title": "Automatic Program Repair"
      },
      {
        "paperId": "0f1382cb004b4834cc3ca7824a61d0d6b86a5763",
        "title": "Pretraining Representations for Data-Efficient Reinforcement Learning"
      },
      {
        "paperId": "c05843ee87fd30ab09eb746accb6f44a3e2f52b7",
        "title": "Sasha"
      },
      {
        "paperId": "5a14b037a5d73e48fc723c40328769d5561fd372",
        "title": "Induction and Exploitation of Subgoal Automata for Reinforcement Learning"
      },
      {
        "paperId": "44323ec1ea9c1432966ea0d5c2ef78f4b8e2a619",
        "title": "A Framework for Learning From Demonstration With Minimal Human Effort"
      },
      {
        "paperId": "1ca5bb12c0d46f9e074e74bd7e08845ebfebf3d5",
        "title": "Convolutional neural network: a review of models, methodologies and applications to object detection"
      },
      {
        "paperId": "157491897eb73fe9507b73ee1d7a84ad0c08f2a1",
        "title": "Reinforcement Learning with Non-Markovian Rewards"
      },
      {
        "paperId": "f0a53f2516ef6c62277fcea3b25e4dbbdd8a0aa2",
        "title": "Reinforcement Learning from Imperfect Demonstrations under Soft Expert Guidance"
      },
      {
        "paperId": "361c00b22e29d0816ca896513d2c165e26399821",
        "title": "Grandmaster level in StarCraft II using multi-agent reinforcement learning"
      },
      {
        "paperId": "42c62c03b68cf7d8eeef9337640564bcacb0174f",
        "title": "Towards Interpreting Recurrent Neural Networks through Probabilistic Abstraction"
      },
      {
        "paperId": "f6d0d9785ba4444d10a84d01bdd01f0bf9871152",
        "title": "Joint Inference of Reward Machines and Policies for Reinforcement Learning"
      },
      {
        "paperId": "9a8c0b88409657095b9b388fb3c1af8bc0e26d3b",
        "title": "DeepHunter: a coverage-guided fuzz testing framework for deep neural networks"
      },
      {
        "paperId": "a034070a193177d2840f8e15b51ebbc0c87ecba5",
        "title": "Learning a Behavior Model of Hybrid Systems Through Combining Model-Based Testing and Machine Learning (Full Version)"
      },
      {
        "paperId": "3ce5a172a96008cbdc5ffedf4572b783301fd468",
        "title": "The role of artificial intelligence in achieving the Sustainable Development Goals"
      },
      {
        "paperId": "bcdb21ca1703fc6f62df420626e36d138480a6a1",
        "title": "Action Robust Reinforcement Learning and Applications in Continuous Control"
      },
      {
        "paperId": "73b5c8b2e4d291a556c63c29b2033b63c0eeb10a",
        "title": "Active deep Q-learning with demonstration"
      },
      {
        "paperId": "c979efe1f0a8b0b343ea332368e5b51dc153c522",
        "title": "Policy Optimization with Demonstrations"
      },
      {
        "paperId": "0735bc67a2149d5c6f38a2e97b2ef8678666d577",
        "title": "TensorFuzz: Debugging Neural Networks with Coverage-Guided Fuzzing"
      },
      {
        "paperId": "1cb6edbedc4a1ac5c32f61a435a23264e42a9071",
        "title": "Towards Sample Efficient Reinforcement Learning"
      },
      {
        "paperId": "705bbc4dcd475f9230863771da6596e1f677a92d",
        "title": "Playing hard exploration games by watching YouTube"
      },
      {
        "paperId": "c37e0d93d19efbd8bb50d1a4d92979793f4341d2",
        "title": "Reinforcement Learning from Imperfect Demonstrations"
      },
      {
        "paperId": "e010ba3ff5744604cdbfe44a733e2a98649ee907",
        "title": "Learning Complex Dexterous Manipulation with Deep Reinforcement Learning and Demonstrations"
      },
      {
        "paperId": "d2ca23d17ace237c2cc6c4f0df8fdd7e0ed4c439",
        "title": "DeepTest: Automated Testing of Deep-Neural-Network-Driven Autonomous Cars"
      },
      {
        "paperId": "e3b0ea7209731c47b582215c6c67f9c691ad9863",
        "title": "Deep Q-learning From Demonstrations"
      },
      {
        "paperId": "8f3b80ddc0dd62e6c3369fabb1715990c29e9b9a",
        "title": "Learning without Forgetting"
      },
      {
        "paperId": "846aedd869a00c09b40f1f1f35673cb22bc87490",
        "title": "Mastering the game of Go with deep neural networks and tree search"
      },
      {
        "paperId": "3b9732bb07dc99bde5e1f9f75251c6ea5039373e",
        "title": "Deep Reinforcement Learning with Double Q-Learning"
      },
      {
        "paperId": "340f48901f72278f6bf78a04ee5b01df208cc508",
        "title": "Human-level control through deep reinforcement learning"
      },
      {
        "paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8",
        "title": "Adam: A Method for Stochastic Optimization"
      },
      {
        "paperId": "6212ebb6372a7a53e0e5b69c0243a371fa434d93",
        "title": "Boosted Bellman Residual Minimization Handling Expert Demonstrations"
      },
      {
        "paperId": "56afe4f59c25a2bfb3dc5509fbde43f032390d24",
        "title": "Learning Nondeterministic Mealy Machines"
      },
      {
        "paperId": "05607111cf79330d56164a10d351dbf94e2cfa44",
        "title": "SemFix: Program repair via semantic analysis"
      },
      {
        "paperId": "9db77cb43da46de6b0c5350348d74c949112e1e1",
        "title": "Cross-domain transfer for reinforcement learning"
      },
      {
        "paperId": "f3e10675b2ef79d8431b8011f909ee0d05e92d92",
        "title": "Incremental multi-step Q-learning"
      },
      {
        "paperId": null,
        "title": "DeepSynth:AutomataSynthesisforAuto-maticTaskSegmentationinDeepReinforcementLearning"
      },
      {
        "paperId": null,
        "title": "IntegratingBehaviorCloningandReinforcement LearningforImprovedPerformanceinDenseandSparseRewardEnvironments"
      },
      {
        "paperId": null,
        "title": "LearningRewardMachinesforPartiallyObservableReinforcementLearning"
      },
      {
        "paperId": null,
        "title": "Learning and Repair of Deep Reinforcement Learning Policies from Fuzz-Testing Data II using Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": null,
        "title": "Super Mario Bros for OpenAI Gym"
      },
      {
        "paperId": null,
        "title": "Learning-BasedTesting:RecentProgressandFutureProspects"
      },
      {
        "paperId": "97efafdb4a3942ab3efba53ded7413199f79c054",
        "title": "Reinforcement Learning: An Introduction"
      },
      {
        "paperId": null,
        "title": "2023. Supplementary Material for \"Learning and Repair of Deep Reinforcement Learning Policies from Fuzz-Testing Data\""
      },
      {
        "paperId": null,
        "title": "Proceedings of the 36th International Conference on Machine Learning (Proceedings of Machine Learning Research, Vol. 97)"
      },
      {
        "paperId": null,
        "title": "applicable license agreement with IEEE. Restrictions apply"
      }
    ],
    "cited_by": [
      {
        "paperId": "a2580264fb70c5f90bbcafeaec07fab5967c6b04",
        "title": "Testing reinforcement learning systems: A comprehensive review"
      },
      {
        "paperId": "fffb9ec96305ebc9f6ae47e13d340e0b8b18db8f",
        "title": "Uncovering Systemic and Environment Errors in Autonomous Systems Using Differential Testing"
      },
      {
        "paperId": "b3f17385dc095298b4e63d0ea5af23c4864b2a24",
        "title": "Verification-Guided Falsification for Safe RL via Explainable Abstraction and Risk-Aware Exploration"
      },
      {
        "paperId": "8db99422c3b0435c687ef7a0d282ee7efa8e986e",
        "title": "Extending AALpy with Passive Learning: A Generalized State-Merging Approach"
      },
      {
        "paperId": "ccd9649a0127268022998d37f470092f0c21eae5",
        "title": "Follow the STARs: Dynamic \u03c9-Regular Shielding of Learned Policies"
      },
      {
        "paperId": "f30aeecf41ae7440f38a15547326aea0b8dfe4ea",
        "title": "Rule-Guided Reinforcement Learning Policy Evaluation and Improvement"
      },
      {
        "paperId": "26eb26774a9c7c8b29524fb9ffc43e8afccff7f6",
        "title": "Probabilistic Automata-Based Method for Enhancing Performance of Deep Reinforcement Learning Systems"
      },
      {
        "paperId": "a64a2db9cf5bf27b5c7cc0da0c04bdc3f043c54f",
        "title": "Demo2Test: Transfer Testing of Agent in Competitive Environment with Failure Demonstrations"
      },
      {
        "paperId": "4bc29c3720b96da06b3137011d0c79fb88823afc",
        "title": "Learning Environment Models with Continuous Stochastic Dynamics - with an Application to Deep RL Testing"
      },
      {
        "paperId": "d042f2170c59e176f066f27eb6db2d68165f144c",
        "title": "Bridging the Gap Between Models in RL: Test Models vs. Neural Networks"
      }
    ],
    "score": 10.0
  },
  {
    "id": "4d3ffd8feca81d750aa30122b49cc1e874e70c1a",
    "title": "MAMBA: an Effective World Model Approach for Meta-Reinforcement Learning",
    "authors": [
      "Zohar Rimon",
      "Tom Jurgenson",
      "Orr Krupnik",
      "Gilad Adler",
      "Aviv Tamar"
    ],
    "year": 2024,
    "citationCount": 10,
    "abstract": "Meta-reinforcement learning (meta-RL) is a promising framework for tackling challenging domains requiring efficient exploration. Existing meta-RL algorithms are characterized by low sample efficiency, and mostly focus on low-dimensional task distributions. In parallel, model-based RL methods have been successful in solving partially observable MDPs, of which meta-RL is a special case. In this work, we leverage this success and propose a new model-based approach to meta-RL, based on elements from existing state-of-the-art model-based and meta-RL methods. We demonstrate the effectiveness of our approach on common meta-RL benchmark domains, attaining greater return with better sample efficiency (up to $15\\times$) while requiring very little hyperparameter tuning. In addition, we validate our approach on a slate of more challenging, higher-dimensional domains, taking a step towards real-world generalizing agents.",
    "url": "https://www.semanticscholar.org/paper/4d3ffd8feca81d750aa30122b49cc1e874e70c1a",
    "pdf_url": "https://arxiv.org/pdf/2403.09859.pdf",
    "venue": "International Conference on Learning Representations",
    "publicationDate": "2024-03-14",
    "externalIds": {
      "DBLP": "conf/iclr/RimonJKAT24",
      "ArXiv": "2403.09859",
      "DOI": "10.48550/arXiv.2403.09859",
      "CorpusId": 268509972
    },
    "references": [
      {
        "paperId": "2bd6dd744cf1b0799e1e47b3ffd1306bff49dacb",
        "title": "ContraBAR: Contrastive Bayes-Adaptive Deep RL"
      },
      {
        "paperId": "f2d952a183dfb0a1e031b8a3f535d9f8423d7a6e",
        "title": "Mastering Diverse Domains through World Models"
      },
      {
        "paperId": "5f164c00d9dcdc5da72b9f71a802eabc4d2e8e68",
        "title": "Evaluating Long-Term Memory in 3D Mazes"
      },
      {
        "paperId": "dbda90fbbcfc76f145fbf362e2117db5312ec2e7",
        "title": "A model-based approach to meta-Reinforcement Learning: Transformers and tree search"
      },
      {
        "paperId": "94f2fce92da8153d18ebd982896b7e3b2160c2aa",
        "title": "Meta Reinforcement Learning with Finite Training Tasks - a Density Estimation Approach"
      },
      {
        "paperId": "5a407a8312ce6237d30a4c9d3a28db9ab3f7709f",
        "title": "On the Effectiveness of Fine-tuning Versus Meta-reinforcement Learning"
      },
      {
        "paperId": "3b3d7adb9047d01af6dfa2975ad8addd69715e96",
        "title": "Mastering Atari Games with Limited Data"
      },
      {
        "paperId": "4545358c54c90dc5d5eb8f11a3c610b3eda88b55",
        "title": "Discovering and Achieving Goals via World Models"
      },
      {
        "paperId": "a7d58bd29778ef0d15b9e9e3eb2f37a8cf1ea70c",
        "title": "Recurrent Model-Free RL Can Be a Strong Baseline for Many POMDPs"
      },
      {
        "paperId": "a1a9a02deb172cfe0ec0c3c4fb70b32ae5d09ab4",
        "title": "Improving Generalization in Meta-RL with Imaginary Tasks from Latent Dynamics Mixture"
      },
      {
        "paperId": "aedd968f09752786785d2de91151d22a7dc36117",
        "title": "Model-based Meta Reinforcement Learning using Graph Structured Surrogate Models"
      },
      {
        "paperId": "b44bb1762640ed72091fd5f5fdc20719a6dc24af",
        "title": "Mastering Atari with Discrete World Models"
      },
      {
        "paperId": "0ca7c0d92a10359a2a7b8fd501a40c9ef768676d",
        "title": "Exploration in Approximate Hyper-State Space for Meta Reinforcement Learning"
      },
      {
        "paperId": "2a9336f92bcdc650c5257ec0cc1b4cd272f5ed1a",
        "title": "Decoupling Exploration and Exploitation for Meta-Reinforcement Learning without Sacrifices"
      },
      {
        "paperId": "8ba600c169f0d2422625822223976bce562eabe1",
        "title": "dm_control: Software and Tasks for Continuous Control"
      },
      {
        "paperId": "638538253332ebeba83f8de1d66f1eb4d2fe61b5",
        "title": "Model-based Adversarial Meta-Reinforcement Learning"
      },
      {
        "paperId": "14a44d4555bae15c01b1ed48fb3e1d75185573e9",
        "title": "Meta-Model-Based Meta-Policy Optimization"
      },
      {
        "paperId": "16ce156a802e43d34929f0b8d32b93db7e852690",
        "title": "Context-aware Dynamics Model for Generalization in Model-Based Reinforcement Learning"
      },
      {
        "paperId": "67a65737d17713ae8bee1b69b853ee658bc6626f",
        "title": "Generalized Hidden Parameter MDPs Transferable Model-based RL in a Handful of Trials"
      },
      {
        "paperId": "0cc956565c7d249d4197eeb1dbab6523c648b2c9",
        "title": "Dream to Control: Learning Behaviors by Latent Imagination"
      },
      {
        "paperId": "234302953fcf201e0848e6e88c60ea695395d6f1",
        "title": "Approximate information state for partially observed systems"
      },
      {
        "paperId": "c39fb7a46335c23f7529dd6f9f980462fd38653a",
        "title": "Mastering Atari, Go, chess and shogi by planning with a learned model"
      },
      {
        "paperId": "361e953f792a585496834ee14216b94d0ce9ae74",
        "title": "VariBAD: A Very Good Method for Bayes-Adaptive Deep RL via Meta-Learning"
      },
      {
        "paperId": "9001698e033524864d4d45f051a5ba362d4afd9e",
        "title": "When to Trust Your Model: Model-Based Policy Optimization"
      },
      {
        "paperId": "9a3c9a0ac460c7891d03d56146f2d566c7e0fb08",
        "title": "Meta reinforcement learning as task inference"
      },
      {
        "paperId": "4625628163a2ee0e6cd320cd7a14b4ccded2a631",
        "title": "Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables"
      },
      {
        "paperId": "fea3e63c97c7292dc6fbcb3ffe7131eb54053986",
        "title": "Learning Latent Dynamics for Planning from Pixels"
      },
      {
        "paperId": "a7e07e0ecd1727778ade42d2e1df856171ec0898",
        "title": "Model-Based Reinforcement Learning via Meta-Policy Optimization"
      },
      {
        "paperId": "944bd3b472c8a30163bbfc1b5cbab8545693c3e0",
        "title": "Learning to Adapt in Dynamic, Real-World Environments through Meta-Reinforcement Learning"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "9dbc218e182665300e4d2e8e9abae14c2cf9374e",
        "title": "Uniform Convergence Rates for Kernel Density Estimation"
      },
      {
        "paperId": "c889d6f98e6d79b89c3a6adf8a921f88fa6ba518",
        "title": "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks"
      },
      {
        "paperId": "282a380fb5ac26d99667224cef8c630f6882704f",
        "title": "Learning to reinforcement learn"
      },
      {
        "paperId": "954b01151ff13aef416d27adc60cd9a076753b1a",
        "title": "RL$^2$: Fast Reinforcement Learning via Slow Reinforcement Learning"
      },
      {
        "paperId": "a20f0ce0616def7cc9a87446c228906cd5da093b",
        "title": "Policy Gradient Methods for Reinforcement Learning with Function Approximation"
      },
      {
        "paperId": "2fa4be2647b4d088ee09320aaa008008728e4af8",
        "title": "Acting Optimally in Partially Observable Stochastic Domains"
      },
      {
        "paperId": "d51eb16dfed68bf6e16b8b4516d607370b91189a",
        "title": "The Complexity of Markov Decision Processes"
      },
      {
        "paperId": "2c455f0da2bd86a9b9ea432d1485049073d7c63d",
        "title": "Remarks on Some Nonparametric Estimates of a Density Function"
      },
      {
        "paperId": "d27edce419e1c731c0aeb9e1842d1e022b2cc6ab",
        "title": "Offline Meta Reinforcement Learning - Identifiability Challenges and Effective Data Collection Strategies"
      },
      {
        "paperId": "ef4f5a50837a7c1b3e87b9300ffc7ba00d461a0f",
        "title": "AUTO-ENCODING VARIATIONAL BAYES"
      },
      {
        "paperId": "6932937c3ac9e6e42c78f0e214445d017486542f",
        "title": "Optimal learning: computational procedures for bayes-adaptive markov decision processes"
      },
      {
        "paperId": "810b9ffea4c74db3923336a22dc9563679cfe564",
        "title": "Conference Paper"
      }
    ],
    "cited_by": [
      {
        "paperId": "aa0d9416c0892de8fb716dd65b1a1f68d04e1105",
        "title": "Embodied Intelligence: The Key to Unblocking Generalized Artificial Intelligence"
      },
      {
        "paperId": "50af671184c72fd38d8aae0b49ccdb66b392e690",
        "title": "Enhanced ECG Signal Classification Using Multi\u2010Branch Convolutions and Mamba Blocks With State\u2010Space Models"
      },
      {
        "paperId": "b670fb6b56bbb5c76849ac6e269626609c42d3a0",
        "title": "Fast Autolearning for Multimodal Walking in Humanoid Robots With Variability of Experience"
      },
      {
        "paperId": "8b3107bbd2d02585a9ab36312a70128c2cfc72f3",
        "title": "Distilling Reinforcement Learning Algorithms for In-Context Model-Based Planning"
      },
      {
        "paperId": "74d9b4d4e6be9f5aaf4a2b96f5216c401c4685c8",
        "title": "Task-Aware Virtual Training: Enhancing Generalization in Meta-Reinforcement Learning for Out-of-Distribution Tasks"
      },
      {
        "paperId": "a05a03089e1f8b7aa7dd0e2d140ca03dd04dcc56",
        "title": "AMAGO-2: Breaking the Multi-Task Barrier in Meta-Reinforcement Learning with Transformers"
      },
      {
        "paperId": "80eb4e5ab0e2d675afffd715be059e0bd3748970",
        "title": "Meta-DT: Offline Meta-RL as Conditional Sequence Modeling with World Model Disentanglement"
      },
      {
        "paperId": "fe1250c8af48711a38e3b998a952ff228df829ee",
        "title": "Craftium: Bridging Flexibility and Efficiency for Rich 3D Single- and Multi-Agent Environments"
      },
      {
        "paperId": "cbd497be42bb0e124c50adaa999a2b80f3a03347",
        "title": "CLIP-Mamba: CLIP Pretrained Mamba Models with OOD and Hessian Evaluation"
      },
      {
        "paperId": "ba4c5a116d07b37dea1046b6d16a60cb2d01cd47",
        "title": "Mamba-360: Survey of State Space Models as Transformer Alternative for Long Sequence Modelling: Methods, Applications, and Challenges"
      }
    ],
    "score": 10.0
  },
  {
    "id": "40d1e0a1e8a861305f9354be747620782fc203ce",
    "title": "Deep Reinforcement Learning: A Chronological Overview and Methods",
    "authors": [
      "Juan R. Terven"
    ],
    "year": 2025,
    "citationCount": 9,
    "abstract": "Introduction: Deep reinforcement learning (deep RL) integrates the principles of reinforcement learning with deep neural networks, enabling agents to excel in diverse tasks ranging from playing board games such as Go and Chess to controlling robotic systems and autonomous vehicles. By leveraging foundational concepts of value functions, policy optimization, and temporal difference methods, deep RL has rapidly evolved and found applications in areas such as gaming, robotics, finance, and healthcare. Objective: This paper seeks to provide a comprehensive yet accessible overview of the evolution of deep RL and its leading algorithms. It aims to serve both as an introduction for newcomers to the field and as a practical guide for those seeking to select the most appropriate methods for specific problem domains. Methods: We begin by outlining fundamental reinforcement learning principles, followed by an exploration of early tabular Q-learning methods. We then trace the historical development of deep RL, highlighting key milestones such as the advent of deep Q-networks (DQN). The survey extends to policy gradient methods, actor\u2013critic architectures, and state-of-the-art algorithms such as proximal policy optimization, soft actor\u2013critic, and emerging model-based approaches. Throughout, we discuss the current challenges facing deep RL, including issues of sample efficiency, interpretability, and safety, as well as open research questions involving large-scale training, hierarchical architectures, and multi-task learning. Results: Our analysis demonstrates how critical breakthroughs have driven deep RL into increasingly complex application domains. We highlight existing limitations and ongoing bottlenecks, such as high data requirements and the need for more transparent, ethically aligned systems. Finally, we survey potential future directions, highlighting the importance of reliability and ethical considerations for real-world deployments.",
    "url": "https://www.semanticscholar.org/paper/40d1e0a1e8a861305f9354be747620782fc203ce",
    "pdf_url": "https://doi.org/10.3390/ai6030046",
    "venue": "Applied Informatics",
    "publicationDate": "2025-02-24",
    "externalIds": {
      "DOI": "10.3390/ai6030046",
      "CorpusId": 276590554
    },
    "references": [],
    "cited_by": [
      {
        "paperId": "b35f7ba3ec29c55c1bcad547e8930ebad4ed61d0",
        "title": "InGrid: Towards a Simulation-Based Automated Decision-Making System for Transportation"
      },
      {
        "paperId": "1ee9c6f29e61249967512eb03beb3c18689a64da",
        "title": "Generative AI for drug discovery and protein design: the next frontier in AI-driven molecular science"
      },
      {
        "paperId": "0b2e60fb84fd98558df2abad6530f04da19c8ba5",
        "title": "A Deep Reinforcement Learning-Driven Seagull Optimization Algorithm for Solving Multi-UAV Task Allocation Problem in Plateau Ecological Restoration"
      },
      {
        "paperId": "e0d91162787d06441e3d7ac1e2a271877049e703",
        "title": "Combining Object Detection and Reinforcement Learning for Vision-Guided Robotic Mini-Golf"
      },
      {
        "paperId": "444fd87d1d95dfc1b8b1cb6bff3abe82ad51bbed",
        "title": "Materials discovery through reinforcement learning: a comprehensive review"
      },
      {
        "paperId": "eb7db9262a189fa8bb1da878736b030d2a564932",
        "title": "Deep Reinforcement Learning of Mobile Robot Navigation in Dynamic Environment: A Review"
      },
      {
        "paperId": "a278c27afeaf5f6eeda2a5a6b841441ae50521c4",
        "title": "Physics-Informed Neural Networks Identification and Reinforcement Learning Control for a Nonlinear Vibration System"
      },
      {
        "paperId": "d9eb826e7a068e267fa9b6a91f1f41df7dc47c65",
        "title": "Energy-efficient motion planning for robotic systems using polynomials in the Chebyshev basis"
      },
      {
        "paperId": "cb534be21cb0d2b6acebbc97d49056358a6002c2",
        "title": "Artificial General Intelligence: Advancements, Challenges, and Future Directions in AGI Research"
      }
    ],
    "score": 9.0
  },
  {
    "id": "736c35ed3e8e86ce99e02ce117dba7ee77e60dda",
    "title": "FastTuner: Transferable Physical Design Parameter Optimization using Fast Reinforcement Learning",
    "authors": [
      "Hao-Hsiang Hsiao",
      "Yi-Chen Lu",
      "Pruek Vanna-iampikul",
      "S. Lim"
    ],
    "year": 2024,
    "citationCount": 9,
    "abstract": "Current state-of-the-art Design Space Exploration (DSE) methods in Physical Design (PD), including Bayesian optimization (BO) and Ant Colony Optimization (ACO), mainly rely on black-boxed rather than parametric (e.g., neural networks) approaches to improve end-of-flow Power, Performance, and Area (PPA) metrics, which often fail to generalize across unseen designs as netlist features are not properly leveraged. To overcome this issue, in this paper, we develop a Reinforcement Learning (RL) agent that leverages Graph Neural Networks (GNNs) and Transformers to perform \"fast\" DSE on unseen designs by sequentially encoding netlist features across different PD stages. Particularly, an attention-based encoder-decoder framework is devised for \"conditional\" parameter tuning, and a PPA estimator is introduced to predict end-of-flow PPA metrics for RL reward estimation. Extensive studies across 7 industrial designs under the TSMC 28nm technology node demonstrate that the proposed framework FastTuner, significantly outperforms existing state-of-the-art DSE techniques in both optimization quality and runtime. where we observe improvements up to 79.38% in Total Negative Slack (TNS), 12.22% in total power, and 50x in runtime.",
    "url": "https://www.semanticscholar.org/paper/736c35ed3e8e86ce99e02ce117dba7ee77e60dda",
    "pdf_url": "https://doi.org/10.1145/3626184.3633328",
    "venue": "ACM International Symposium on Physical Design",
    "publicationDate": "2024-03-12",
    "externalIds": {
      "DBLP": "conf/ispd/HsiaoLVL24",
      "DOI": "10.1145/3626184.3633328",
      "CorpusId": 268389401
    },
    "references": [
      {
        "paperId": "be54015b8b931a37c1d1020cb19647640844cc7d",
        "title": "PTPT: Physical Design Tool Parameter Tuning via Multi-Objective Bayesian Optimization"
      },
      {
        "paperId": "c7eb08850f3bd344f72de8a63e8cb2a241c96641",
        "title": "A fast parameter tuning framework via transfer learning and multi-objective bayesian optimization"
      },
      {
        "paperId": "1cefbe38cc67a73390155f8c68083e79a71b159e",
        "title": "PPATuner: pareto-driven tool parameter auto-tuning in physical design via gaussian process transfer learning"
      },
      {
        "paperId": "657ca2a8fd7c4602e3007caa77a93b01024194b5",
        "title": "FlowTuner: A Multi-Stage EDA Flow Tuner Exploiting Parameter Knowledge Transfer"
      },
      {
        "paperId": "cf11b16039c405a4d59f3ad103eeac7ee7ba6f55",
        "title": "VLSI Placement Parameter Optimization using Deep Reinforcement Learning"
      },
      {
        "paperId": "cccac01e7caf262bc199df0617e4062fafd89367",
        "title": "TP-GNN: A Graph Neural Network Framework for Tier Partitioning in Monolithic 3D ICs"
      },
      {
        "paperId": "42f05f81535a1cc1d19b7737e76012e46dc1019c",
        "title": "GAN-CTS: A Generative Adversarial Framework for Clock Tree Prediction and Optimization"
      },
      {
        "paperId": "6cf890255dee9e12c52bdbb063e08bc7cec035a3",
        "title": "LAMDA: Learning-Assisted Multi-stage Autotuning for FPGA Design Closure"
      },
      {
        "paperId": null,
        "title": ": Graph Neural Network Inference for Transfer-able Power Estimation"
      },
      {
        "paperId": null,
        "title": "CADToolDesignSpaceExplorationviaBayesianOptimization"
      },
      {
        "paperId": null,
        "title": ": This component is responsible for encoding gate-level netlists"
      }
    ],
    "cited_by": [
      {
        "paperId": "4bf6215cb3040afb5bdf8fe76928384dc5f19534",
        "title": "AutoMarks: A GNN-based Automated Physical Design Watermarking Framework"
      },
      {
        "paperId": "d866bc4e08dba30a1336cd7a6c0a76128fa4de43",
        "title": "CROP: Circuit Retrieval and Optimization with Parameter Guidance using LLMs"
      },
      {
        "paperId": "439f06977cac678b9c65dc311fe8d55c5d5e07c7",
        "title": "InsightAlign: A Transferable Physical Design Recipe Recommender Based on Design Insights"
      },
      {
        "paperId": "4ee0d015517af2cdf85f57a49e7bde2f77fe1939",
        "title": "HyperPlace: Harnessing a Large Language Model for Efficient Hyperparameter Optimization in GPU-Accelerated VLSI Placement"
      },
      {
        "paperId": "0575a3a971952a022b94724cec56ab480ee1e1b7",
        "title": "Use Cases and Deployment of ML in IC Physical Design"
      },
      {
        "paperId": "5a62be0c6e98c92fb8d4e756568e6b62dbd085bc",
        "title": "Automated Physical Design Watermarking Leveraging Graph Neural Networks"
      },
      {
        "paperId": "addc97144dc17105687bf1273fcd495cd1c3ea10",
        "title": "BUFFALO: PPA-Configurable, LLM-based Buffer Tree Generation via Group Relative Policy Optimization"
      },
      {
        "paperId": "b40ef1a2e5565829a06e855bf977cd9b6b52847a",
        "title": "A Hybrid Reinforcement Learning Framework for Efficient Physical Design Parameter Tuning"
      },
      {
        "paperId": "9c5c53bd0832c1262fa5f9a07e156abd02173f68",
        "title": "Differentiable Tier Assignment for Timing and Congestion-Aware Routing in 3D ICs"
      }
    ],
    "score": 9.0
  },
  {
    "id": "a69adfee23dbc90b2292499ffb1bf9fbd7d656c5",
    "title": "Reimagining space layout design through deep reinforcement learning",
    "authors": [
      "R. Kakooee",
      "B. Dillenburger"
    ],
    "year": 2024,
    "citationCount": 9,
    "abstract": "\n Space layout design is a critical aspect of architectural design, influencing functionality and aesthetics. The inherent combinatorial nature of layout design poses challenges for traditional planning approaches; thus, it demands the exploration of novel methods. This paper presents a novel framework that leverages the potential of deep reinforcement learning (RL) algorithms to optimize space layouts. RL has demonstrated remarkable success in addressing complex decision-making problems, yet its application in the design process remains relatively unexplored. We argue that RL is particularly well-suited for the design process due to its ability to accommodate offline tasks and seamless integration with existing CAD software, effectively acting as a simulator for design exploration. Framing space layout design as an RL problem and employing RL methods allows for the automated exploration of the expansive design space, thereby enhancing the discovery of innovative solutions. This paper also elucidates the synergy between the design process and the RL problem, which opens new avenues for exploring the potential of RL algorithms in design. We aim to foster experimentation and collaboration within the RL and architecture communities. To facilitate our research, we have developed SpaceLayoutGym, an environment specifically designed for space layout design tasks. SpaceLayoutGym serves as a customizable environment that encapsulates the essential elements of the layout design process within an RL framework. To showcase the effectiveness of SpaceLayoutGym and the capabilities of RL as an artificial space layout designer, we employ the PPO algorithm to train the RL agent in selected design scenarios with both geometrical constraints and topological objectives. The study further extends to contrast the effectiveness of PPO agents with that of genetic algorithms, and also includes a comparative analysis with existing layouts. Our results demonstrate the potential of RL to optimize space layouts, offering a promising direction for the future of AI-aided design.",
    "url": "https://www.semanticscholar.org/paper/a69adfee23dbc90b2292499ffb1bf9fbd7d656c5",
    "pdf_url": "https://doi.org/10.1093/jcde/qwae025",
    "venue": "Journal of Computational Design and Engineering",
    "publicationDate": "2024-03-27",
    "externalIds": {
      "DBLP": "journals/jcde/KakooeeD24",
      "DOI": "10.1093/jcde/qwae025",
      "CorpusId": 268832786
    },
    "references": [
      {
        "paperId": "1889ae7c47137d7c8c39cc82022558c13a46f0b8",
        "title": "HouseDiffusion: Vector Floorplan Generation via a Diffusion Model with Discrete and Continuous Denoising"
      },
      {
        "paperId": "69b0012f366367a3bb2d182aa0a8ebae35b95255",
        "title": "FinRL: deep reinforcement learning framework to automate trading in quantitative finance"
      },
      {
        "paperId": "2481b43f0bdc54b65dbe4ebf7792ab881edac49d",
        "title": "Developing a Reinforcement Learning Algorithm to Model Pavlovian Approach Bias on Bidirectional Planning"
      },
      {
        "paperId": "936c6537fd5e7c2fef4720fd7e8128fda2c4c004",
        "title": "PyGAD: An Intuitive Genetic Algorithm Python Library"
      },
      {
        "paperId": "569ba73ad6162ffcf193a2c5a39eabcb8f75aabb",
        "title": "House-GAN++: Generative Adversarial Layout Refinement Networks"
      },
      {
        "paperId": "c57cec562ba556d201af881f373c555e6b6306c1",
        "title": "Addressing adjacency constraints in rectangular floor plans using Monte-Carlo Tree Search"
      },
      {
        "paperId": "a2e43270a9b1421e452c2975e5163e2a216abeac",
        "title": "A Survey of Deep Reinforcement Learning in Video Games"
      },
      {
        "paperId": "f815b4c44771b4ce1d0d3e7c7a0c34e554158e1c",
        "title": "Deep Generative Learning for the Generation and Analysis of Architectural Plans with Small Datasets"
      },
      {
        "paperId": "f04f828c1888ea968fad7af2259217b04d209fbe",
        "title": "Data-driven interior plan generation for residential buildings"
      },
      {
        "paperId": "417edb826b9dbf8e142e9358b78da70b0bbc7177",
        "title": "Structured agents for physical construction"
      },
      {
        "paperId": "aaf51f96ca1fe18852f586764bc3aa6e852d0cb6",
        "title": "A Tour of Reinforcement Learning: The View from Continuous Control"
      },
      {
        "paperId": "82a262a2034b349abaa720c7f8229a0ef19e87cd",
        "title": "RLlib: Abstractions for Distributed Reinforcement Learning"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "8acbe90d5b852dadea7810345451a99608ee54c7",
        "title": "Image-to-Image Translation with Conditional Adversarial Networks"
      },
      {
        "paperId": "2b10281297ee001a9f3f4ea1aa9bea6b638c27df",
        "title": "OpenAI Gym"
      },
      {
        "paperId": "51cdabcae3ff05d3c4b2a262f6582b283924da31",
        "title": "Building envelope shape design in early stages of the design process: Integrating architectural design systems and energy simulation"
      },
      {
        "paperId": "e8cd3ce20722b63d4c7441585ab149b244e34b5e",
        "title": "Design with shape grammars and reinforcement learning"
      },
      {
        "paperId": "c544d2fa76ef107ee289d0eec0b8f7d8b02b49df",
        "title": "Artist Agent: A Reinforcement Learning Approach to Automatic Stroke Generation in Oriental Ink Painting"
      },
      {
        "paperId": "4635770e832b7d8aa94c9023db8b9e96924f8e2b",
        "title": "Evolving design genes in space layout planning problems"
      },
      {
        "paperId": "8af4b5a82103c3d286789891738c9ee91c817dad",
        "title": "A Genetic Search Approach to Space Layout Planning"
      },
      {
        "paperId": "a9d52ec08dd2a6c302d802f9bdb1c4b96346fcdb",
        "title": "Introduction to Shape and Shape Grammars"
      },
      {
        "paperId": "8a0c7fd62cc364002c9135f6d828b4445c944767",
        "title": "Notes on the Synthesis of Form"
      },
      {
        "paperId": null,
        "title": "The sur prising effectiv eness of PPO in cooper ativ e m ulti-a gent games"
      },
      {
        "paperId": "a72b5da3adc7d79025dcdfd256264ef78307a4d2",
        "title": "Self-learning Agents for Spatial Synthesis"
      },
      {
        "paperId": "70adc568fa1c051c1c8b45e53f32af3118589611",
        "title": "Generative Modelling with Design Constraints - Reinforcement Learning for Object Generation"
      },
      {
        "paperId": null,
        "title": "Arc hitectur al planning with shape grammars and reinforcement learning: Habitability and energy efficiency"
      },
      {
        "paperId": null,
        "title": "T rul y pr oximal policy optimization"
      },
      {
        "paperId": null,
        "title": "Multi-agent space planning: A liter atur e r e vie w (2008\u20132017)"
      },
      {
        "paperId": null,
        "title": "ArchiGAN: A generative stack for apartment building design"
      },
      {
        "paperId": null,
        "title": "Synthesizing pr ogr ams for ima ges using r einforced"
      },
      {
        "paperId": "20cfcc722e3c08026b5b4dcab009e94482e72910",
        "title": "Raumindex. Ein datenbasiertes Entwurfsinstrument"
      },
      {
        "paperId": null,
        "title": "Computer-gener ated r es-idential building la youts"
      },
      {
        "paperId": "5cbcefc9593d5837281c99a681eec45248b67b45",
        "title": "Building design as individual compromise between qualities and costs: A general approach for automated building generation under permanent cost and quality control"
      },
      {
        "paperId": "97efafdb4a3942ab3efba53ded7413199f79c054",
        "title": "Reinforcement Learning: An Introduction"
      },
      {
        "paperId": null,
        "title": "Use of gr a phs to decide the optimum layout of buildings"
      },
      {
        "paperId": null,
        "title": "iii) The corridor is linked to the living room, facilitating convenient movement from the entrance to the living space"
      },
      {
        "paperId": null,
        "title": "ii) The entrance is connected to the corridor to ensure easy access"
      },
      {
        "paperId": null,
        "title": "iv) The living room is connected to all other r ooms, pr omoting smooth and comfortable flow of people"
      },
      {
        "paperId": null,
        "title": "as man y times as needed"
      }
    ],
    "cited_by": [
      {
        "paperId": "d3d1eec5b42c879958b7a490ac46369113199dd2",
        "title": "Generative design for early-stage optioneering in subsea layout design"
      },
      {
        "paperId": "2d471c3ae5b1662b280a5f63873891ba1bcf25dd",
        "title": "Controllable and flexible residential floor plan layout design based on multi-agent deep reinforcement learning with layout prior size and similar experience abandon"
      },
      {
        "paperId": "cb26c2f9488bc368eb0bd6f377a80dd1006b41de",
        "title": "Dynamic construction site layout optimization using deep reinforcement learning with PPO"
      },
      {
        "paperId": "5be4e1d573eb02c5501a36ecbd70d2e103e82258",
        "title": "Automated aggregation of dwelling units and traffic cores in high-rise residential floor plans using genetic algorithm and multi-agent cooperative deep Q-network"
      },
      {
        "paperId": "bcc80bf163d7cf6a8fff9bcd8945191fe636ebe3",
        "title": "Artificial Intelligence for Circuit Design and Optimization: Enhancing Efficiency and Performance"
      },
      {
        "paperId": "bbbe02679a06afe26190d9230dfc31ac23a8669f",
        "title": "Advanced Technologies in Creative Architectural Design: A Systematic Review"
      },
      {
        "paperId": "49ea1aa068182ab61bc080cd68faf326dae11350",
        "title": "Artificial Intelligence to Facilitate the Conceptual Stage of Interior Space Design: Conditional Generative Adversarial Network-Supported Long-Term Care Space Floor Plan Design of Retirement Home Buildings"
      },
      {
        "paperId": "503356d4dfb2e35ca4e015d820d386fc2827b944",
        "title": "A hypergraph model shows the carbon reduction potential of effective space use in housing"
      },
      {
        "paperId": "dbe336a618c258390c48f8921fa0ff7581df59dd",
        "title": "Automated design and optimization of distributed filter circuits using reinforcement learning"
      }
    ],
    "score": 9.0
  },
  {
    "id": "e5d6cca71ea0fb216a25f86e96d3480886fdba27",
    "title": "D5RL: Diverse Datasets for Data-Driven Deep Reinforcement Learning",
    "authors": [
      "Rafael Rafailov",
      "K. Hatch",
      "Anikait Singh",
      "Laura Smith",
      "Aviral Kumar",
      "Ilya Kostrikov",
      "Philippe Hansen-Estruch",
      "Victor Kolev",
      "Philip Ball",
      "Jiajun Wu",
      "Chelsea Finn",
      "Sergey Levine"
    ],
    "year": 2024,
    "citationCount": 9,
    "abstract": "Offline reinforcement learning algorithms hold the promise of enabling data-driven RL methods that do not require costly or dangerous real-world exploration and benefit from large pre-collected datasets. This in turn can facilitate real-world applications, as well as a more standardized approach to RL research. Furthermore, offline RL methods can provide effective initializations for online finetuning to overcome challenges with exploration. However, evaluating progress on offline RL algorithms requires effective and challenging benchmarks that capture properties of real-world tasks, provide a range of task difficulties, and cover a range of challenges both in terms of the parameters of the domain (e.g., length of the horizon, sparsity of rewards) and the parameters of the data (e.g., narrow demonstration data or broad exploratory data). While considerable progress in offline RL in recent years has been enabled by simpler benchmark tasks, the most widely used datasets are increasingly saturating in performance and may fail to reflect properties of realistic tasks. We propose a new benchmark for offline RL that focuses on realistic simulations of robotic manipulation and locomotion environments, based on models of real-world robotic systems, and comprising a variety of data sources, including scripted data, play-style data collected by human teleoperators, and other data sources. Our proposed benchmark covers state-based and image-based domains, and supports both offline RL and online fine-tuning evaluation, with some of the tasks specifically designed to require both pre-training and fine-tuning. We hope that our proposed benchmark will facilitate further progress on both offline RL and fine-tuning algorithms. Website with code, examples, tasks, and data is available at \\url{https://sites.google.com/view/d5rl/}",
    "url": "https://www.semanticscholar.org/paper/e5d6cca71ea0fb216a25f86e96d3480886fdba27",
    "pdf_url": "https://arxiv.org/pdf/2408.08441.pdf",
    "venue": "RLJ",
    "publicationDate": "2024-08-15",
    "externalIds": {
      "ArXiv": "2408.08441",
      "DBLP": "journals/corr/abs-2408-08441",
      "DOI": "10.48550/arXiv.2408.08441",
      "CorpusId": 271892214
    },
    "references": [
      {
        "paperId": "f5e1993f3f505e8fbb9cac9231285c8c9f1712a7",
        "title": "Cal-QL: Calibrated Offline RL Pre-Training for Efficient Online Fine-Tuning"
      },
      {
        "paperId": "bd38cbbb346a347cb5b60ac4a133b3d73cb44e07",
        "title": "Efficient Online Reinforcement Learning with Offline Data"
      },
      {
        "paperId": "cc3cb6b0ea04eb35c1907e3917a4db4b435c95b1",
        "title": "FinRL-Meta: Market Environments and Benchmarks for Data-Driven Financial Reinforcement Learning"
      },
      {
        "paperId": "2d33f309f7e92c75434a2bb16f70d6ec65ab7d2a",
        "title": "Hybrid RL: Using Both Offline and Online Data Can Make RL Efficient"
      },
      {
        "paperId": "87a00037444092e8ada8d3bb4c1f8c6baededdc0",
        "title": "Generalization with Lossy Affordances: Leveraging Broad Offline Data for Learning Visuomotor Tasks"
      },
      {
        "paperId": "bd3a0bbabae3260098e06bfb615147fb6d34e55a",
        "title": "Latent Plans for Task-Agnostic Offline Reinforcement Learning"
      },
      {
        "paperId": "b4888a20a9910e292448a51bf4248cc5b60e2af3",
        "title": "A Walk in the Park: Learning to Walk in 20 Minutes With Model-Free Reinforcement Learning"
      },
      {
        "paperId": "dac3a172b504f4e33c029655e9befb3386e5f63a",
        "title": "Emergent Abilities of Large Language Models"
      },
      {
        "paperId": "bbae3200de2d742b2bdcecab51f40a8dccb228cb",
        "title": "Challenges and Opportunities in Offline Reinforcement Learning from Visual Observations"
      },
      {
        "paperId": "3b51a29424b619ec5ce29125c4b88d8e24a09328",
        "title": "Planning to Practice: Efficient Online Fine-Tuning by Composing Goals in Latent Space"
      },
      {
        "paperId": "9f5120b815fddaaef25c7042035ffe5680507a65",
        "title": "Google Scanned Objects: A High-Quality Dataset of 3D Scanned Household Items"
      },
      {
        "paperId": "bc912d8c1a7165d89483d96d64b2ef20703ed2cb",
        "title": "Vision-Based Manipulators Need to Also See from Their Hands"
      },
      {
        "paperId": "990dc444f9aed0b69200a1b46d6ced846d139835",
        "title": "The Health Gym: synthetic health-related datasets for the development of reinforcement learning algorithms"
      },
      {
        "paperId": "277e63a452cb1fb6c5ad88d110d5f18401e840c0",
        "title": "Adversarially Trained Actor Critic for Offline Reinforcement Learning"
      },
      {
        "paperId": "4545358c54c90dc5d5eb8f11a3c610b3eda88b55",
        "title": "Discovering and Achieving Goals via World Models"
      },
      {
        "paperId": "348a855fe01f3f4273bf0ecf851ca688686dbfcc",
        "title": "Offline Reinforcement Learning with Implicit Q-Learning"
      },
      {
        "paperId": "0a8a9ceab1bb62445214084545bd3e389eb68324",
        "title": "Showing Your Offline Reinforcement Learning Work: Online Evaluation Budget Matters"
      },
      {
        "paperId": "558ca2e8c7eb56edd77a52b084e6cc24dffe5bcd",
        "title": "Deep Reinforcement Learning at the Edge of the Statistical Precipice"
      },
      {
        "paperId": "3032844d6ac6882ccb03e7a2c22a0026b210ac05",
        "title": "What Matters in Learning from Offline Human Demonstrations for Robot Manipulation"
      },
      {
        "paperId": "4aa88c1406414cda3ce9cf76c8af0abaa8391760",
        "title": "Habitat 2.0: Training Home Assistants to Rearrange their Habitat"
      },
      {
        "paperId": "c879b25308026d6538e52b27bcf4fd3cb60855f3",
        "title": "A Minimalist Approach to Offline Reinforcement Learning"
      },
      {
        "paperId": "5f1adc14a77fb61aa463fac728397bd32e00b617",
        "title": "Challenges of real-world reinforcement learning: definitions, benchmarks and analysis"
      },
      {
        "paperId": "de18baa4964804cf471d85a5a090498242d2e79f",
        "title": "Improved Denoising Diffusion Probabilistic Models"
      },
      {
        "paperId": "841f8e46c359b86ed1da7dafa3062ef9f351b5a4",
        "title": "NeoRL: A Near Real-World Benchmark for Offline Reinforcement Learning"
      },
      {
        "paperId": "f204041dd567025217adc8070ca292e89cc80488",
        "title": "COG: Connecting New Skills to Past Experience with Offline Reinforcement Learning"
      },
      {
        "paperId": "36a3901347beaa8cb160e81736f815412d630a96",
        "title": "OR-Gym: A Reinforcement Learning Library for Operations Research Problem"
      },
      {
        "paperId": "8ba600c169f0d2422625822223976bce562eabe1",
        "title": "dm_control: Software and Tasks for Continuous Control"
      },
      {
        "paperId": "0272b14dd471fe7b81df703af1b71d7600b77215",
        "title": "Accelerating Online Reinforcement Learning with Offline Datasets"
      },
      {
        "paperId": "28db20a81eec74a50204686c3cf796c42a020d2e",
        "title": "Conservative Q-Learning for Offline Reinforcement Learning"
      },
      {
        "paperId": "5e7bc93622416f14e6948a500278bfbe58cd3890",
        "title": "Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems"
      },
      {
        "paperId": "6568423cfaca7e24c88ea208cb0e67129e43aa9b",
        "title": "Image Augmentation Is All You Need: Regularizing Deep Reinforcement Learning from Pixels"
      },
      {
        "paperId": "a326d9f2d2d351001fece788165dbcbb524da2e4",
        "title": "D4RL: Datasets for Deep Data-Driven Reinforcement Learning"
      },
      {
        "paperId": "8d814620a1ca77e745bc8a33b96b86148f2804fe",
        "title": "Leveraging Procedural Generation to Benchmark Reinforcement Learning"
      },
      {
        "paperId": "8c54e8575e7c17a4097838305915e6e7b00fd4af",
        "title": "Relay Policy Learning: Solving Long-Horizon Tasks via Imitation and Reinforcement Learning"
      },
      {
        "paperId": "0bc855f84668b35cb65618d996d09f6e434d28c9",
        "title": "Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning"
      },
      {
        "paperId": "3606c313dccd21f784bedf5759152a3677f5719d",
        "title": "Behaviour Suite for Reinforcement Learning"
      },
      {
        "paperId": "4012d4ab621f3f5f04b0f91849a60c6eaabe64b4",
        "title": "An Optimistic Perspective on Offline Reinforcement Learning"
      },
      {
        "paperId": "4ee70fb32981f84f9dddc57bd59a69e677c91759",
        "title": "Benchmarking Model-Based Reinforcement Learning"
      },
      {
        "paperId": "82b4b03a4659d6e04bd7cbf51d6e08fde1348dbd",
        "title": "Stabilizing Off-Policy Q-Learning via Bootstrapping Error Reduction"
      },
      {
        "paperId": "99a7df93a2e16bd7ac3349d52cc34417cda7909d",
        "title": "Learning Latent Plans from Play"
      },
      {
        "paperId": "5285cb8faada5de8a92a47622950f6cfd476ac1d",
        "title": "Off-Policy Deep Reinforcement Learning without Exploration"
      },
      {
        "paperId": "ef2bc452812d6005ab0a66af6c3f97b6b0ba837e",
        "title": "Quantifying Generalization in Reinforcement Learning"
      },
      {
        "paperId": "eb37e7b76d26b75463df22b2a3aa32b6a765c672",
        "title": "QT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation"
      },
      {
        "paperId": "80196cdfcd0c6ce2953bf65a7f019971e2026386",
        "title": "IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures"
      },
      {
        "paperId": "ebf0615fc4d98cf1dbe527c79146ce1e50dce9af",
        "title": "CARLA: An Open Urban Driving Simulator"
      },
      {
        "paperId": "37d522c6b4b9c1d9244783658012681e8af0ef44",
        "title": "Flow: Architecture and Benchmarking for Reinforcement Learning in Traffic Control"
      },
      {
        "paperId": "33690ff21ef1efb576410e656f2e60c89d0307d6",
        "title": "Deep Reinforcement Learning that Matters"
      },
      {
        "paperId": "1bead9000a719cb258bac7320228055aee650d2c",
        "title": "Leveraging Demonstrations for Deep Reinforcement Learning on Robotics Problems with Sparse Rewards"
      },
      {
        "paperId": "e3b0ea7209731c47b582215c6c67f9c691ad9863",
        "title": "Deep Q-learning From Demonstrations"
      },
      {
        "paperId": "1464776f20e2bccb6182f183b5ff2e15b0ae5e56",
        "title": "Benchmarking Deep Reinforcement Learning for Continuous Control"
      },
      {
        "paperId": "2319a491378867c7049b3da055c5df60e1671158",
        "title": "Playing Atari with Deep Reinforcement Learning"
      },
      {
        "paperId": "b354ee518bfc1ac0d8ac447eece9edb69e92eae1",
        "title": "MuJoCo: A physics engine for model-based control"
      },
      {
        "paperId": "abd1c342495432171beb7ca8fd9551ef13cbd0ff",
        "title": "ImageNet classification with deep convolutional neural networks"
      },
      {
        "paperId": "9d3af1c5fd5f7fb51467b4308952bc4da8285396",
        "title": "Paper"
      },
      {
        "paperId": "4aa95dc3682d664b333a868ce350d1567abc47cd",
        "title": "Contributors"
      },
      {
        "paperId": "05ea5e3bbcc22ccf57f9b458ee7d72f9c51682b1",
        "title": "RL Unplugged: A Suite of Benchmarks for Of\ufb02ine Reinforcement Learning"
      },
      {
        "paperId": "75a01692eff00feed0dd21fa3b48ce0183866f20",
        "title": "Gibson Env V2: Embodied Simulation Environments for Interactive Navigation"
      },
      {
        "paperId": "4f8d648c52edf74e41b0996128aa536e13cc7e82",
        "title": "Deep Learning"
      },
      {
        "paperId": null,
        "title": "Implicit q-learning as an actor-critic method"
      },
      {
        "paperId": "62d4bab6e24f0a4b7867b0673325977234e73ed5",
        "title": "KitchenShift: Evaluating Zero-Shot Generalization of Imitation-Based Policy Learning Under Domain Shifts"
      }
    ],
    "cited_by": [
      {
        "paperId": "3553d5edea006af885a8534de9218f4be6499e57",
        "title": "floq: Training Critics via Flow-Matching for Scaling Compute in Value-Based RL"
      },
      {
        "paperId": "cbbcc17e6c6bd3dc06fe55a9f4d4f2ee10fe6852",
        "title": "Scaling Offline RL via Efficient and Expressive Shortcut Models"
      },
      {
        "paperId": "ddd7531271d7ae932b6b1051784e8f58b2443236",
        "title": "Diffusion Self-Weighted Guidance for Offline Reinforcement Learning"
      },
      {
        "paperId": "5d4922343adcdad8d242e53294f024007d733e1d",
        "title": "An Improved Social Force Model\u2010Driven Multi\u2010Agent Generative Adversarial Imitation Learning Framework for Pedestrian Trajectory Prediction"
      },
      {
        "paperId": "45439384b065b56f5b56140a1b40d1ca14d6ec31",
        "title": "A2Perf: Real-World Autonomous Agents Benchmark"
      },
      {
        "paperId": "8dcc0a142aefec2d7addb91775e5d58ab4a6771f",
        "title": "Flow Q-Learning"
      },
      {
        "paperId": "157ae9fe598f1f5c0ae60d286bae2712bf360d93",
        "title": "Trajectory World Models for Heterogeneous Environments"
      },
      {
        "paperId": "6f6e7a84bbc63dada1e9472482d0e94454a28223",
        "title": "Offline-to-online Reinforcement Learning for Image-based Grasping with Scarce Demonstrations"
      },
      {
        "paperId": "5971402c4cee5863b25c99405e886b673ade223c",
        "title": "Is Value Learning Really the Main Bottleneck in Offline RL?"
      }
    ],
    "score": 9.0
  },
  {
    "id": "5ecf53ab083f72f10421225a7dde25eb51cb6b22",
    "title": "Can Learned Optimization Make Reinforcement Learning Less Difficult?",
    "authors": [
      "A. D. Goldie",
      "Chris Lu",
      "Matthew Jackson",
      "S. Whiteson",
      "J. Foerster"
    ],
    "year": 2024,
    "citationCount": 9,
    "abstract": "While reinforcement learning (RL) holds great potential for decision making in the real world, it suffers from a number of unique difficulties which often need specific consideration. In particular: it is highly non-stationary; suffers from high degrees of plasticity loss; and requires exploration to prevent premature convergence to local optima and maximize return. In this paper, we consider whether learned optimization can help overcome these problems. Our method, Learned Optimization for Plasticity, Exploration and Non-stationarity (OPEN), meta-learns an update rule whose input features and output structure are informed by previously proposed solutions to these difficulties. We show that our parameterization is flexible enough to enable meta-learning in diverse learning contexts, including the ability to use stochasticity for exploration. Our experiments demonstrate that when meta-trained on single and small sets of environments, OPEN outperforms or equals traditionally used optimizers. Furthermore, OPEN shows strong generalization characteristics across a range of environments and agent architectures.",
    "url": "https://www.semanticscholar.org/paper/5ecf53ab083f72f10421225a7dde25eb51cb6b22",
    "pdf_url": "https://arxiv.org/pdf/2407.07082.pdf",
    "venue": "Neural Information Processing Systems",
    "publicationDate": "2024-07-09",
    "externalIds": {
      "ArXiv": "2407.07082",
      "DBLP": "journals/corr/abs-2407-07082",
      "DOI": "10.48550/arXiv.2407.07082",
      "CorpusId": 271064168
    },
    "references": [
      {
        "paperId": "ee5212ffa1c7aababb908c33cd146a3635e677a1",
        "title": "Simplifying Deep Temporal Difference Learning"
      },
      {
        "paperId": "5c0e88f9d0827ac6360e2c85e1fa55fc950fe67c",
        "title": "Disentangling the Causes of Plasticity Loss in Neural Networks"
      },
      {
        "paperId": "139a84c6fc4887ce2374489d79af0df9e1e7e4d6",
        "title": "Craftax: A Lightning-Fast Benchmark for Open-Ended Reinforcement Learning"
      },
      {
        "paperId": "912b861d8fe9d3237febb8b756a69ec42485fae9",
        "title": "Discovering Temporally-Aware Reinforcement Learning Algorithms"
      },
      {
        "paperId": "e082401d548031af4ebae59e019802f11d4b4b93",
        "title": "Revisiting Plasticity in Visual Reinforcement Learning: Data, Modules and Training Stages"
      },
      {
        "paperId": "339e4b4aaa5c43660bf48756066433e8a2045187",
        "title": "Small batch deep reinforcement learning"
      },
      {
        "paperId": "7afb8a00b808d6dc43cef350450f46e443329e67",
        "title": "Discovering General Reinforcement Learning Algorithms with Adversarial Environment Design"
      },
      {
        "paperId": "d0eac1da5d54638fd3dc41cd1e477d804dc4d806",
        "title": "Resetting the Optimizer in Deep RL: An Empirical Study"
      },
      {
        "paperId": "8bb7cecd1bc3fa470aa882158e7553705b1a6141",
        "title": "Minigrid & Miniworld: Modular & Customizable Reinforcement Learning Environments for Goal-Oriented Tasks"
      },
      {
        "paperId": "eb989c7ad4d9b0c5b8b32d0ae4a92b14b09b4fe8",
        "title": "PLASTIC: Improving Input and Label Plasticity for Sample Efficient Reinforcement Learning"
      },
      {
        "paperId": "13f775d1b7f9de1168b5d37a2622ee1938bf43cd",
        "title": "Faster sorting algorithms discovered using deep reinforcement learning"
      },
      {
        "paperId": "d364f8cb20daf958ebca53a807e79082eff98e78",
        "title": "Bigger, Better, Faster: Human-level Atari with human-level efficiency"
      },
      {
        "paperId": "b9c9338e247570bbcea0d947b5fbe18a19116ceb",
        "title": "Deep Reinforcement Learning with Plasticity Injection"
      },
      {
        "paperId": "3c93a1a97371665eb2ddaf2ddd9c0a3667e8262a",
        "title": "Massively Scalable Inverse Reinforcement Learning in Google Maps"
      },
      {
        "paperId": "4c4bc4e9f9d6ab6b476ae582574e9532529be443",
        "title": "Loss of Plasticity in Continual Deep Reinforcement Learning"
      },
      {
        "paperId": "542905f5fc96bce7572f6ded7f56aedfa62270c1",
        "title": "Understanding plasticity in neural networks"
      },
      {
        "paperId": "6c282567e9c452f81416214933b9b1e45ab3add4",
        "title": "The Dormant Neuron Phenomenon in Deep Reinforcement Learning"
      },
      {
        "paperId": "a7f59b2162ae0ea2520753b1b9b17277490a9458",
        "title": "Symbolic Discovery of Optimization Algorithms"
      },
      {
        "paperId": "d5131fe8b428e98928643742ff11266a85db73ef",
        "title": "Learning to Optimize for Reinforcement Learning"
      },
      {
        "paperId": "5af8c7c650e9ec50d91a16be287ce54b16075fe7",
        "title": "A Tutorial on Meta-Reinforcement Learning"
      },
      {
        "paperId": "0e43af94943b27c1357234ddb189ffe3a387f099",
        "title": "evosax: JAX-Based Evolution Strategies"
      },
      {
        "paperId": "fe60e2ad09a33bee70fb500f055b247a1bf35e0f",
        "title": "Transformer-Based Learned Optimization"
      },
      {
        "paperId": "c088b46519c036149a9f6da4ec36383b800a0d2a",
        "title": "VeLO: Training Versatile Learned Optimizers by Scaling Up"
      },
      {
        "paperId": "522ac9eb08bb0c5a422700bb254ea1c44e9157de",
        "title": "Discovered Policy Optimisation"
      },
      {
        "paperId": "69b80ce5ab6c2263b145b1eaa23664145938f351",
        "title": "The Primacy Bias in Deep Reinforcement Learning"
      },
      {
        "paperId": "0a818d426c813b7a4758c959d38ece5f2cc11476",
        "title": "Understanding and Preventing Capacity Loss in Reinforcement Learning"
      },
      {
        "paperId": "ef2dec2c570cbba3d49dd29ceee4990a83cf455f",
        "title": "Practical tradeoffs between memory, compute, and performance in learned optimizers"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "0b0cd862d6820b24fc4e3ca1c42cbe557ca49d9c",
        "title": "Unbiased Gradient Estimation in Unrolled Computation Graphs with Persistent Evolution Strategies"
      },
      {
        "paperId": "a8755cbdaca0ea9723fb81a06589a7837ea44619",
        "title": "A Survey of Zero-shot Generalisation in Deep Reinforcement Learning"
      },
      {
        "paperId": "8e128a1b2efb0ddf688902ade4405d22d5b61eec",
        "title": "Benchmarking the Spectrum of Agent Capabilities"
      },
      {
        "paperId": "558ca2e8c7eb56edd77a52b084e6cc24dffe5bcd",
        "title": "Deep Reinforcement Learning at the Edge of the Statistical Precipice"
      },
      {
        "paperId": "13263ec6b83e59442aafe1b45b7554bbba57b680",
        "title": "Brax - A Differentiable Physics Engine for Large Scale Rigid Body Simulation"
      },
      {
        "paperId": "7491cfff38e55e56fb86a41f58e5ddaffb9eee01",
        "title": "Correcting Momentum in Temporal Difference Learning"
      },
      {
        "paperId": "c0f77574d63a1d608009ad096e8f0fc791eb1297",
        "title": "A Generalizable Approach to Learning Optimizers"
      },
      {
        "paperId": "93b2788fb1f2aed0e545d9f9d7dca1c05a63208a",
        "title": "Emergent Complexity and Zero-shot Transfer via Unsupervised Environment Design"
      },
      {
        "paperId": "de0617beaf9341fa4a177b6a98d78788b2b80b39",
        "title": "Problem 3"
      },
      {
        "paperId": "8a858fb857abc06817d245bcb774a3901676f144",
        "title": "Tasks, stability, architecture, and compute: Training more effective learned optimizers, and using them to train themselves"
      },
      {
        "paperId": "7ea04c23675d82a051a91c9eefd20684e85d3444",
        "title": "Discovering Reinforcement Learning Algorithms"
      },
      {
        "paperId": "90974d9e0df8466a50338601e839fa0ea69c9872",
        "title": "Transient Non-stationarity and Generalisation in Deep Reinforcement Learning"
      },
      {
        "paperId": "2c819870111efb9fa70e359c15e2031e992c2b4a",
        "title": "Improving Generalization in Meta Reinforcement Learning using Learned Objectives"
      },
      {
        "paperId": "3606c313dccd21f784bedf5759152a3677f5719d",
        "title": "Behaviour Suite for Reinforcement Learning"
      },
      {
        "paperId": "81c4994d5ac809f44e3f5e1da807671cf6c115eb",
        "title": "Learning to learn with active adaptive perception"
      },
      {
        "paperId": "05d94cfe5768ebdcc1ecd8cb81de694e0d3e2f5d",
        "title": "MinAtar: An Atari-Inspired Testbed for Thorough and Reproducible Reinforcement Learning Experiments"
      },
      {
        "paperId": "f9717d29840f4d8f1cc19d1b1e80c5d12ec40608",
        "title": "A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play"
      },
      {
        "paperId": "cb4147fbd0704398c692667078efff935a36bb6d",
        "title": "Understanding and correcting pathologies in the training of learned optimizers"
      },
      {
        "paperId": "a02ed13d218b62ae2045ae822db94cde3c5f94a2",
        "title": "Where Did My Optimum Go?: An Empirical Analysis of Gradient Descent Optimization in Policy Gradient Methods"
      },
      {
        "paperId": "c6509a450bdda7ca5b8567103dfe9671dbf3b567",
        "title": "Meta-Learning Update Rules for Unsupervised Representation Learning"
      },
      {
        "paperId": "c983653841b6987d9959318f074a595783838576",
        "title": "On the Convergence of Adam and Beyond"
      },
      {
        "paperId": "811df72e210e20de99719539505da54762a11c6d",
        "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor"
      },
      {
        "paperId": "819bcae49054e00cef3c0972d48b4e40a525f4d9",
        "title": "Deep Neuroevolution: Genetic Algorithms Are a Competitive Alternative for Training Deep Neural Networks for Reinforcement Learning"
      },
      {
        "paperId": "d07284a6811f1b2745d91bdb06b040b57f226882",
        "title": "Decoupled Weight Decay Regularization"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "32954b976c86a8e35e9edf24b95a023bdbb89a76",
        "title": "Performance comparision of different momentum techniques on deep reinforcement learning"
      },
      {
        "paperId": "5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd",
        "title": "Deep Reinforcement Learning from Human Preferences"
      },
      {
        "paperId": "142497432fe179ddb6ffe600c64a837ec6179550",
        "title": "Parameter Space Noise for Exploration"
      },
      {
        "paperId": "1ecc2bd0bc6ffa0a2f466a058589c20593e3e57c",
        "title": "The Marginal Value of Adaptive Gradient Methods in Machine Learning"
      },
      {
        "paperId": "8499a250422a3c66357367c8d5fa504de5424c59",
        "title": "Intrinsic Motivation and Automatic Curricula via Asymmetric Self-Play"
      },
      {
        "paperId": "b8ff7e02ffa1577d125acd3e998e8ce76a9059dc",
        "title": "Learned Optimizers that Scale and Generalize"
      },
      {
        "paperId": "4ee802a58d32aa049d549d06be440ac947b53987",
        "title": "Evolution Strategies as a Scalable Alternative to Reinforcement Learning"
      },
      {
        "paperId": "12f67fb182bc934fc95ce97acff553d83e2ca72e",
        "title": "Count-Based Exploration with Neural Density Models"
      },
      {
        "paperId": "0fcb2034e31a2bc2f12a2b1363d0d77baf445fdf",
        "title": "#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning"
      },
      {
        "paperId": "b022f2a277a4bf5f42382e86e4380b96340b9e86",
        "title": "SGDR: Stochastic Gradient Descent with Warm Restarts"
      },
      {
        "paperId": "97fb4e3d45bb098e27e0071448b6152217bd35a5",
        "title": "Layer Normalization"
      },
      {
        "paperId": "71683e224ab91617950956b5005ed0439a733a71",
        "title": "Learning to learn by gradient descent by gradient descent"
      },
      {
        "paperId": "ead9a671428631e44f6fe49324efe69da628bc47",
        "title": "Learning to Optimize"
      },
      {
        "paperId": "317cd4522b1f4a6f889743578143bb8823623f8b",
        "title": "VIME: Variational Information Maximizing Exploration"
      },
      {
        "paperId": "846aedd869a00c09b40f1f1f35673cb22bc87490",
        "title": "Mastering the game of Go with deep neural networks and tree search"
      },
      {
        "paperId": "2470fcf0f89082de874ac9133ccb3a8667dd89a8",
        "title": "Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models"
      },
      {
        "paperId": "d316c82c12cf4c45f9e85211ef3d1fa62497bff8",
        "title": "High-Dimensional Continuous Control Using Generalized Advantage Estimation"
      },
      {
        "paperId": "37b5dfe87d82ba8f310155165d5bf841dc92dea2",
        "title": "Cyclical Learning Rates for Training Neural Networks"
      },
      {
        "paperId": "340f48901f72278f6bf78a04ee5b01df208cc508",
        "title": "Human-level control through deep reinforcement learning"
      },
      {
        "paperId": "1eb09fecd75eb27825dce4f964b97f4f5cc399d7",
        "title": "On the Properties of Neural Machine Translation: Encoder\u2013Decoder Approaches"
      },
      {
        "paperId": "b354ee518bfc1ac0d8ac447eece9edb69e92eae1",
        "title": "MuJoCo: A physics engine for model-based control"
      },
      {
        "paperId": "0cafe2903b097fc042782c359cb231ea34ef7ed3",
        "title": "Near-optimal Regret Bounds for Reinforcement Learning"
      },
      {
        "paperId": "dd5cf95a7af93d2733120d177c593989b19b98fe",
        "title": "Natural Evolution Strategies"
      },
      {
        "paperId": "dc649486b881e672eea6546da48c46e1f98daf32",
        "title": "Near-Optimal Reinforcement Learning in Polynomial Time"
      },
      {
        "paperId": "f1bdebedf07fd444628c955568f0d51e1a26835e",
        "title": "Completely Derandomized Self-Adaptation in Evolution Strategies"
      },
      {
        "paperId": "a91635f8d0e7fb804efd1c38d9c24ee952ba7076",
        "title": "Learning to predict by the methods of temporal differences"
      },
      {
        "paperId": "a155da36c401bdd7fee08fae2b0bf27e9c559466",
        "title": "Antithetic acceleration of Monte Carlo integration in Bayesian inference"
      },
      {
        "paperId": "33d84b1531f88d2bd2e516e1574f22e139133065",
        "title": "Sample-Efficient Reinforcement Learning by Breaking the Replay Ratio Barrier"
      },
      {
        "paperId": null,
        "title": "The DeepMind JAX Ecosystem"
      },
      {
        "paperId": null,
        "title": "JAX: Composable transformations of Python+NumPy programs"
      },
      {
        "paperId": null,
        "title": "A Method for Stochastic Optimization"
      },
      {
        "paperId": null,
        "title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude"
      },
      {
        "paperId": "c7dbf5ed7e9b63e104adb4e18bfbc98f5d6afdae",
        "title": "R-MAX - A General Polynomial Time Algorithm for Near-Optimal Reinforcement Learning"
      },
      {
        "paperId": "97efafdb4a3942ab3efba53ded7413199f79c054",
        "title": "Reinforcement Learning: An Introduction"
      },
      {
        "paperId": "d04942a086f9cafbb1c6453b64ba188beeb03823",
        "title": "Evolutionsstrategie : Optimierung technischer Systeme nach Prinzipien der biologischen Evolution"
      },
      {
        "paperId": null,
        "title": "A JAX-based reinforcement"
      },
      {
        "paperId": null,
        "title": "The 37 implementation details of proximal policy optimization"
      }
    ],
    "cited_by": [
      {
        "paperId": "ca8fb30e4425d6fde21695716f71d3f1433c9287",
        "title": "On Discovering Algorithms for Adversarial Imitation Learning"
      },
      {
        "paperId": "7a7b1d278b7ea8eb6b00418618df0a5afab9260b",
        "title": "How Should We Meta-Learn Reinforcement Learning Algorithms?"
      },
      {
        "paperId": "c28d1f25d8a778d09bb04620bead6fe8cfe05e75",
        "title": "Network Sparsity Unlocks the Scaling Potential of Deep Reinforcement Learning"
      },
      {
        "paperId": "815d397d10d8a6774ab35589ee8e30d60356899e",
        "title": "Unleashing the Reasoning Potential of Pre-trained LLMs by Critique Fine-Tuning on One Problem"
      },
      {
        "paperId": "13b1c6df671525f0135976b5a8bc04046aa41ebb",
        "title": "Vintix: Action Model via In-Context Reinforcement Learning"
      },
      {
        "paperId": "3fd5e6f90f34109f8737cb307d29d08a3638bd24",
        "title": "Celo: Training Versatile Learned Optimizers on a Compute Diet"
      },
      {
        "paperId": "812f20b087dd927512963fee56268419462b7efc",
        "title": "A Research Agenda for Usability and Generalisation in Reinforcement Learning"
      },
      {
        "paperId": "dcd9d46908854358192ca5b159359d5f20fbe07b",
        "title": "Meta-Learning Objectives for Preference Optimization"
      },
      {
        "paperId": "688c324664b8bf6147281f2771e9b17edeabf45d",
        "title": "Kinetix: Investigating the Training of General Agents through Open-Ended Physics-Based Control Tasks"
      }
    ],
    "score": 9.0
  },
  {
    "id": "4186f74da6d793c91c5ca4d8a10cf2f8638eade6",
    "title": "RLfOLD: Reinforcement Learning from Online Demonstrations in Urban Autonomous Driving",
    "authors": [
      "Daniel Coelho",
      "Miguel Oliveira",
      "Vitor Santos"
    ],
    "year": 2024,
    "citationCount": 9,
    "abstract": "Reinforcement Learning from Demonstrations (RLfD) has emerged as an effective method by fusing expert demonstrations into Reinforcement Learning (RL) training, harnessing the strengths of both Imitation Learning (IL) and RL. However, existing algorithms rely on offline demonstrations, which can introduce a distribution gap between the demonstrations and the actual training environment, limiting their performance. In this paper, we propose a novel approach, Reinforcement Learning from Online Demonstrations (RLfOLD), that leverages online demonstrations to address this limitation, ensuring the agent learns from relevant and up-to-date scenarios, thus effectively bridging the distribution gap. Unlike conventional policy networks used in typical actor-critic algorithms, RLfOLD introduces a policy network that outputs two standard deviations: one for exploration and the other for IL training. This novel design allows the agent to adapt to varying levels of uncertainty inherent in both RL and IL. Furthermore, we introduce an exploration process guided by an online expert, incorporating an uncertainty-based technique. Our experiments on the CARLA NoCrash benchmark demonstrate the effectiveness and efficiency of RLfOLD. Notably, even with a significantly smaller encoder and a single camera setup, RLfOLD surpasses state-of-the-art methods in this evaluation. These results, achieved with limited resources, highlight RLfOLD as a highly promising solution for real-world applications.",
    "url": "https://www.semanticscholar.org/paper/4186f74da6d793c91c5ca4d8a10cf2f8638eade6",
    "pdf_url": "https://doi.org/10.1609/aaai.v38i10.29049",
    "venue": "AAAI Conference on Artificial Intelligence",
    "publicationDate": "2024-03-24",
    "externalIds": {
      "DBLP": "conf/aaai/CoelhoOS24",
      "DOI": "10.1609/aaai.v38i10.29049",
      "CorpusId": 268692691
    },
    "references": [
      {
        "paperId": "4f327f6a8e8dbc730cc350f458ddf0753f57945e",
        "title": "RLAD: Reinforcement Learning From Pixels for Autonomous Driving in Urban Environments"
      },
      {
        "paperId": "74af06ac7fa260314064908a8be60d149c55a9ce",
        "title": "Imitation Is Not Enough: Robustifying Imitation with Reinforcement Learning for Challenging Driving Scenarios"
      },
      {
        "paperId": "b2004f4f19bc6ccae8e4afc554f39142870100f5",
        "title": "MoDem: Accelerating Visual Model-Based Reinforcement Learning with Demonstrations"
      },
      {
        "paperId": "a3335ad0d5e4061edda2b66567e517022642ea96",
        "title": "Safety-Enhanced Autonomous Driving Using Interpretable Sensor Fusion Transformer"
      },
      {
        "paperId": "1a8ac422f4e594155af1721837009264ce14fe32",
        "title": "TransFuser: Imitation With Transformer-Based Sensor Fusion for Autonomous Driving"
      },
      {
        "paperId": "651f256bf9f8a12672cfd29e8fd228a6ebe085d4",
        "title": "CADRE: A Cascade Deep Reinforcement Learning Framework for Vision-based Autonomous Urban Driving"
      },
      {
        "paperId": "f3e8c42b56bf8406726eaaccc68398df2eaccc61",
        "title": "Efficient Learning of Safe Driving Policy via Human-AI Copilot Optimization"
      },
      {
        "paperId": "25a1732e1166db3b1d74fc934318e9b4c2d16771",
        "title": "GRI: General Reinforced Imitation and its Application to Vision-Based Autonomous Driving"
      },
      {
        "paperId": "d49eb322bc426dde48ac5a7973fad0b6513ee395",
        "title": "Safe Driving via Expert Guided Policy Optimization"
      },
      {
        "paperId": "0c562afd1be443c98dc84347d4ab5156f2d4bbb2",
        "title": "A Joint Imitation-Reinforcement Learning Framework for Reduced Baseline Regret"
      },
      {
        "paperId": "010ac9acd45c0073e79130e855aad3b65c51a5e1",
        "title": "End-to-End Urban Driving by Imitating a Reinforcement Learning Coach"
      },
      {
        "paperId": "e06c005e98281af455c454ce2478285f6f3afeca",
        "title": "Mastering Visual Continuous Control: Improved Data-Augmented Reinforcement Learning"
      },
      {
        "paperId": "0614d0da5f49ff902ffd9209482f29b902a1a824",
        "title": "Learning to drive from a world on rails"
      },
      {
        "paperId": "c0559fc7e7d6ea0f783ba791ddd5deaa74cf58a9",
        "title": "Multi-Modal Fusion Transformer for End-to-End Autonomous Driving"
      },
      {
        "paperId": "68eebf01bfbfb7d37937d42c77a966df0cd2dd89",
        "title": "Improved Deep Reinforcement Learning with Expert Demonstrations for Urban Autonomous Driving"
      },
      {
        "paperId": "6568423cfaca7e24c88ea208cb0e67129e43aa9b",
        "title": "Image Augmentation Is All You Need: Regularizing Deep Reinforcement Learning from Pixels"
      },
      {
        "paperId": "ca5045c9d9e0bf2e95f6694dff657e28ffcd4f07",
        "title": "Learning by Cheating"
      },
      {
        "paperId": "3fa432268bfdf6aa75f563a3f0e7f90e270740c5",
        "title": "SAM: Squeeze-and-Mimic Networks for Conditional Visual Driving Policy Learning"
      },
      {
        "paperId": "3c8a456509e6c0805354bd40a35e3f2dbf8069b1",
        "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library"
      },
      {
        "paperId": "42d0b94c8cf71541e5b52f45ab845b6d47dc4efe",
        "title": "End-to-End Model-Free Reinforcement Learning for Urban Driving Using Implicit Affordances"
      },
      {
        "paperId": "fcfc344628c7604232804317eb90db268dfc85e8",
        "title": "Exploring the Limitations of Behavior Cloning for Autonomous Driving"
      },
      {
        "paperId": "12c0751b4f51ed833172a713b7e32390032ead93",
        "title": "Soft Actor-Critic Algorithms and Applications"
      },
      {
        "paperId": "c8482a7b09a13d08bed1da2348304125ccb1a30a",
        "title": "HG-DAgger: Interactive Imitation Learning with Human Experts"
      },
      {
        "paperId": "6736ef589a70cf91091a53bbdc7240fd87a3d540",
        "title": "EnsembleDAgger: A Bayesian Approach to Safe Imitation Learning"
      },
      {
        "paperId": "e3088dd1ab6993e3e9bc75e5b9b266139a2570b0",
        "title": "CIRL: Controllable Imitative Reinforcement Learning for Vision-based Self-driving"
      },
      {
        "paperId": "ad6309d1ea001098189425f54d069ef12abcb583",
        "title": "Dynamical Isometry and a Mean Field Theory of CNNs: How to Train 10, 000-Layer Vanilla Convolutional Neural Networks"
      },
      {
        "paperId": "880a018f1e0a587b39a4ca9a5c9f6ba4029e2ea1",
        "title": "End-to-End Driving Via Conditional Imitation Learning"
      },
      {
        "paperId": "c28ec2a40a2c77e20d64cf1c85dc931106df8e83",
        "title": "Overcoming Exploration in Reinforcement Learning with Demonstrations"
      },
      {
        "paperId": "1bead9000a719cb258bac7320228055aee650d2c",
        "title": "Leveraging Demonstrations for Deep Reinforcement Learning on Robotics Problems with Sparse Rewards"
      },
      {
        "paperId": "e3b0ea7209731c47b582215c6c67f9c691ad9863",
        "title": "Deep Q-learning From Demonstrations"
      },
      {
        "paperId": "3b9732bb07dc99bde5e1f9f75251c6ea5039373e",
        "title": "Deep Reinforcement Learning with Double Q-Learning"
      },
      {
        "paperId": "99c970348b8f70ce23d6641e201904ea49266b6e",
        "title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks"
      },
      {
        "paperId": "79ab3c49903ec8cb339437ccf5cf998607fc313e",
        "title": "A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning"
      },
      {
        "paperId": "c8221c054459e37edbf313668523d667fe5c1536",
        "title": "Maximum Entropy Inverse Reinforcement Learning"
      },
      {
        "paperId": "cfc3cf1210832f8e81e75a8d776407d0d2792440",
        "title": "International"
      },
      {
        "paperId": "1a56a76f297d067e2805780146069a3927222c32",
        "title": "A Review of End-to-End Autonomous Driving in Urban Environments"
      },
      {
        "paperId": null,
        "title": "2022. Learning From All Vehicles"
      }
    ],
    "cited_by": [
      {
        "paperId": "cc0074cc496ebb92fff1eb4ce91c56778658b10d",
        "title": "Advancing autonomous driving system testing: Demands, challenges, and future directions"
      },
      {
        "paperId": "b8c4a41b8fa6077a65d7e318da14900ef2f1d61a",
        "title": "Beyond the Map: Learning to Navigate Unseen Urban Dynamics Using Diffusion-Guided Deep Reinforcement Learning"
      },
      {
        "paperId": "be584c7f94fc7330581d7fe0b9845139f161b281",
        "title": "Optimizing Pretrained Autonomous Driving Models Using Deep Reinforcement Learning"
      },
      {
        "paperId": "13a48b74f801ad59f1f7509eec5d4f8720f2157b",
        "title": "The Current Development and Future Prospects of Autonomous Driving Driven by Artificial\u00a0Intelligence"
      },
      {
        "paperId": "6ac616b7f4ac7ddbe4f2cc24f0525132c065789f",
        "title": "Joint Perception and Prediction for Autonomous Driving: A Survey"
      },
      {
        "paperId": "0e3b0a91d5068b4a6aea144d1dde1ddff3845654",
        "title": "Diffusion Imitation from Observation"
      },
      {
        "paperId": "88f76398e17b4661cfb25a7379140e7b40e2a0b9",
        "title": "PRIBOOT: A New Data-Driven Expert for Improved Driving Simulations"
      },
      {
        "paperId": "08b6f22bab698a3a241de5fe94a8ff6638fadaae",
        "title": "End-to-End Autonomous Driving in CARLA: A Survey"
      },
      {
        "paperId": "70743db92db7dcc0870552edc02e6893a3dd9555",
        "title": "Human-Interactive Robot Learning: Definition, Challenges, and Recommendations"
      }
    ],
    "score": 9.0
  },
  {
    "id": "eb6b79b00d43fa03945ae2477c60e79edaf72d28",
    "title": "Human Knowledge Enhanced Reinforcement Learning for Mandatory Lane-Change of Autonomous Vehicles in Congested Traffic",
    "authors": [
      "Yanjun Huang",
      "Yuxiao Gu",
      "Kang Yuan",
      "Shuo Yang",
      "Tao Liu",
      "Hong Chen"
    ],
    "year": 2024,
    "citationCount": 9,
    "abstract": "Mandatory lane-change scenarios are often challenging for autonomous vehicles in complex environments. In this paper, a human-knowledge-enhanced reinforcement learning (RL) method for lane-change decision making is proposed, where the human intelligence is integrated with RL algorithm in a multiple manner. First, this paper constructs a complex ramp-off scenario with congested traffic flow to help agents master lane-change skills. On the basis of the Twin Delayed Deep Deterministic policy gradient (TD3) algorithm, the human prior experience is encoded into reward function and safety constraints offline, and the online guidance of experts is also introduced into the framework, which can limit the unsafe exploration during the training process and provide demonstration in complex scenarios. The experimental results indicate that our method can effectively improve the training efficiency and outperform typical RL method and expert drivers, without specific requirements on the expertise. The proposed method can enhance the learning ability of RL based driving strategies.",
    "url": "https://www.semanticscholar.org/paper/eb6b79b00d43fa03945ae2477c60e79edaf72d28",
    "pdf_url": "https://doi.org/10.1109/TIV.2023.3336768",
    "venue": "IEEE Transactions on Intelligent Vehicles",
    "publicationDate": "2024-02-01",
    "externalIds": {
      "DBLP": "journals/tiv/HuangGYYLC24",
      "DOI": "10.1109/TIV.2023.3336768",
      "CorpusId": 266064354
    },
    "references": [],
    "cited_by": [
      {
        "paperId": "9dc11aa0538a646aa2da254d53eb1617284ec452",
        "title": "PE-RLHF: Reinforcement Learning with Human Feedback and physics knowledge for safe and trustworthy autonomous driving"
      },
      {
        "paperId": "9aee4f61cca2f0c1a681cd7580610b55ebd3356a",
        "title": "Human Experience-Guided Reinforcement Learning for Carrier-Based Aircraft Support Operation Scheduling"
      },
      {
        "paperId": "1b48059a8e0433ce635cba48b38d6ac44cf0af20",
        "title": "Control Barrier Function-Guided Deep Reinforcement Learning for Decision-Making of Autonomous Vehicle at On-Ramp Merging"
      },
      {
        "paperId": "6b52fadf79289552c7ca8a7e053b36a49023d133",
        "title": "A Survey on Data-Driven Modeling of Human Drivers' Lane-Changing Decisions"
      },
      {
        "paperId": "840dce460e34704837ab14db0452fe8d5899e78c",
        "title": "Highway autonomous vehicle decision-making method based on prior knowledge and improved experience replay reinforcement learning algorithm"
      },
      {
        "paperId": "ea502917e2bad5211c191d8ee34b05252faf502f",
        "title": "HGRL: Human-Driving-Data Guided Reinforcement Learning for Autonomous Driving"
      },
      {
        "paperId": "80e37f09f7d368fc643237bff5e27a483412518a",
        "title": "Imitation-Based Personalized Driving Decision and Evaluation Method with Experimental Studies"
      },
      {
        "paperId": "607fbf81621cc806b03b46f20417a333e97e0912",
        "title": "Trustworthy Human-AI Collaboration: Reinforcement Learning with Human Feedback and Physics Knowledge for Safe Autonomous Driving"
      },
      {
        "paperId": "5ddb4b9d9c58dbe026ebf7c6c78875bbeb4c711a",
        "title": "Human-In-The-Loop Machine Learning for Safe and Ethical Autonomous Vehicles: Principles, Challenges, and Opportunities"
      }
    ],
    "score": 9.0
  },
  {
    "id": "b093a3fa79512c48524f81c754bddec7b16afb17",
    "title": "Outcome-directed Reinforcement Learning by Uncertainty & Temporal Distance-Aware Curriculum Goal Generation",
    "authors": [
      "Daesol Cho",
      "Seungjae Lee",
      "H. J. Kim"
    ],
    "year": 2023,
    "citationCount": 16,
    "abstract": "Current reinforcement learning (RL) often suffers when solving a challenging exploration problem where the desired outcomes or high rewards are rarely observed. Even though curriculum RL, a framework that solves complex tasks by proposing a sequence of surrogate tasks, shows reasonable results, most of the previous works still have difficulty in proposing curriculum due to the absence of a mechanism for obtaining calibrated guidance to the desired outcome state without any prior domain knowledge. To alleviate it, we propose an uncertainty&temporal distance-aware curriculum goal generation method for the outcome-directed RL via solving a bipartite matching problem. It could not only provide precisely calibrated guidance of the curriculum to the desired outcome states but also bring much better sample efficiency and geometry-agnostic curriculum goal proposal capability compared to previous curriculum RL methods. We demonstrate that our algorithm significantly outperforms these prior methods in a variety of challenging navigation tasks and robotic manipulation tasks in a quantitative and qualitative way.",
    "url": "https://www.semanticscholar.org/paper/b093a3fa79512c48524f81c754bddec7b16afb17",
    "pdf_url": "https://arxiv.org/pdf/2301.11741.pdf",
    "venue": "International Conference on Learning Representations",
    "publicationDate": "2023-01-27",
    "externalIds": {
      "DBLP": "conf/iclr/ChoLK23",
      "ArXiv": "2301.11741",
      "DOI": "10.48550/arXiv.2301.11741",
      "CorpusId": 256358497
    },
    "references": [
      {
        "paperId": "86785b861d24776bdc0f123e73a8a401f25d72e4",
        "title": "Curriculum Reinforcement Learning using Optimal Transport via Gradual Domain Adaptation"
      },
      {
        "paperId": "2d5edc038a177bed2f58e5374f38a411d8538f0b",
        "title": "Watch and Match: Supercharging Imitation with Regularized Optimal Transport"
      },
      {
        "paperId": "9bf925ecb1e6c6bfeecfc15aec1d0c6d7c28e135",
        "title": "CIC: Contrastive Intrinsic Control for Unsupervised Skill Discovery"
      },
      {
        "paperId": "6296aa7cab06eaf058f7291040b320b5a83c0091",
        "title": "Generative Adversarial Networks"
      },
      {
        "paperId": "1ff566afa26babdb6c93a77e761792e34f3ae934",
        "title": "Wasserstein Distance Maximizing Intrinsic Control"
      },
      {
        "paperId": "09f9af75d631ff9e6b71726c9344b9bedb921b42",
        "title": "Cross-Domain Imitation Learning via Optimal Transport"
      },
      {
        "paperId": "8a913111f23fbded7f2e9d2d6c9c4278e7c682c9",
        "title": "APS: Active Pretraining with Successor Features"
      },
      {
        "paperId": "6382bec2c2fd18d388483653409b1a18048521da",
        "title": "MURAL: Meta-Learning Uncertainty-Aware Rewards for Outcome-Driven Reinforcement Learning"
      },
      {
        "paperId": "acacd119213ff03453816f6cb51402109d443007",
        "title": "Adversarial Intrinsic Motivation for Reinforcement Learning"
      },
      {
        "paperId": "07283c33464eb4f56a13caeb52cb2f9b988557b3",
        "title": "Replacing Rewards with Examples: Example-Based Policy Search via Recursive Classification"
      },
      {
        "paperId": "4d12862b0c6daee864f6f5537270d95b88560ebc",
        "title": "Mutual Information State Intrinsic Control"
      },
      {
        "paperId": "0295df1b9d11e6e49b20119410fd755a4d7781af",
        "title": "Behavior From the Void: Unsupervised Active Pre-Training"
      },
      {
        "paperId": "a50f7bcf8a998f3de11bc085b0f4dea32be19783",
        "title": "Reinforcement Learning with Prototypical Representations"
      },
      {
        "paperId": "6999fd72868c4044e852c43a040a87a43d03ab3a",
        "title": "Prioritized Level Replay"
      },
      {
        "paperId": "9e9b55142f7c353fa262047dcea77685668213e6",
        "title": "Automatic Curriculum Learning through Value Disagreement"
      },
      {
        "paperId": "25287b593d2642b1627b96a79c5ff8d3c8ec1f5c",
        "title": "Primal Wasserstein Imitation Learning"
      },
      {
        "paperId": "3c8a456509e6c0805354bd40a35e3f2dbf8069b1",
        "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library"
      },
      {
        "paperId": "0bc855f84668b35cb65618d996d09f6e434d28c9",
        "title": "Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning"
      },
      {
        "paperId": "20b9c2ea1a49ed7789b99ae4c84b1b517b65bff5",
        "title": "Teacher algorithms for curriculum learning of Deep RL in continuously parameterized environments"
      },
      {
        "paperId": "294bba666eb44689f21839c3013819c3030eec25",
        "title": "Dynamical Distance Learning for Semi-Supervised and Unsupervised Skill Discovery"
      },
      {
        "paperId": "ffb3886a253ff927bcc46b78e00409893865a68e",
        "title": "Dynamics-Aware Unsupervised Discovery of Skills"
      },
      {
        "paperId": "20e7a22229dbfdc63dbd98cfe40b51134fbeccc7",
        "title": "Wasserstein Adversarial Imitation Learning"
      },
      {
        "paperId": "5a22ce57b02c8aa446c793435a2235bbe6afbc65",
        "title": "Exploration via Hindsight Goal Generation"
      },
      {
        "paperId": "87a40a2d506fec06d67dc8f14ce2c708a789a2b4",
        "title": "Self-Supervised Exploration via Disagreement"
      },
      {
        "paperId": "f6f9a4806e70d9cce33f28998032bcee57a7394e",
        "title": "Deep pNML: Predictive Normalized Maximum Likelihood for Deep Neural Networks"
      },
      {
        "paperId": "00a5cdb5768fdfabfa9decad28271afde9880579",
        "title": "End-to-End Robotic Reinforcement Learning without Reward Engineering"
      },
      {
        "paperId": "7388826b5ee00efe17cb7f19a623d9b5e955ae70",
        "title": "Skew-Fit: State-Covering Self-Supervised Reinforcement Learning"
      },
      {
        "paperId": "4f46688ecc1fd96c01096d4a0f20f5a5de8a0d38",
        "title": "Universal Supervised Learning for Individual Data"
      },
      {
        "paperId": "4cb3fd057949624aa4f0bbe7a6dcc8777ff04758",
        "title": "Exploration by Random Network Distillation"
      },
      {
        "paperId": "c46d80f83813fba0e8363a0ab36a19fba062540e",
        "title": "Learning Actionable Representations with Goal-Conditioned Policies"
      },
      {
        "paperId": "b65a6be07ce9c86797e6917258cf5ba45273ee73",
        "title": "Unsupervised Control Through Non-Parametric Discriminative Rewards"
      },
      {
        "paperId": "36f7f407bbad234929c69c0dd3bdcfcd80298c7c",
        "title": "Variational Inverse Control with Events: A General Framework for Data-Driven Reward Definition"
      },
      {
        "paperId": "5b01eaef54a653ba03ddd5a978690380fbc19bfc",
        "title": "Diversity is All You Need: Learning Skills without a Reward Function"
      },
      {
        "paperId": "811df72e210e20de99719539505da54762a11c6d",
        "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "acd87843a451d18b4dc6474ddce1ae946429eaf1",
        "title": "Wasserstein Generative Adversarial Networks"
      },
      {
        "paperId": "429ed4c9845d0abd1f8204e1d7705919559bc2a2",
        "title": "Hindsight Experience Replay"
      },
      {
        "paperId": "471f9742b4e32d8ee68f9ee493768ff0466a231d",
        "title": "Automatic Goal Generation for Reinforcement Learning Agents"
      },
      {
        "paperId": "225ab689f41cef1dc18237ef5dab059a49950abf",
        "title": "Curiosity-Driven Exploration by Self-Supervised Prediction"
      },
      {
        "paperId": "8499a250422a3c66357367c8d5fa504de5424c59",
        "title": "Intrinsic Motivation and Automatic Curricula via Asymmetric Self-Play"
      },
      {
        "paperId": "c889d6f98e6d79b89c3a6adf8a921f88fa6ba518",
        "title": "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks"
      },
      {
        "paperId": "12f67fb182bc934fc95ce97acff553d83e2ca72e",
        "title": "Count-Based Exploration with Neural Density Models"
      },
      {
        "paperId": "6e90fd78e8a3b98af3954aae5209703aa966603e",
        "title": "Unifying Count-Based Exploration and Intrinsic Motivation"
      },
      {
        "paperId": "449532187c94af3dd3aa55e16d2c50f7854d2199",
        "title": "Trust Region Policy Optimization"
      },
      {
        "paperId": "5f5dc5b9a2ba710937e2c413b37b053cd673df02",
        "title": "Auto-Encoding Variational Bayes"
      },
      {
        "paperId": "7f05b312f2de7e59c7869558507fa4a9fa0d0971",
        "title": "Optimal Transport: Old and New"
      },
      {
        "paperId": "1e8344fde962af321c047cc55369580d307ec49b",
        "title": "Conditional NML Universal Models"
      },
      {
        "paperId": "d87c392fe28c691ec4bc587b9dca1a921be16f76",
        "title": "Curriculum Reinforcement Learning via Constrained Optimal Transport"
      },
      {
        "paperId": null,
        "title": "CURROT (Klink et al., 2022): We follow the default setting in the original implementation"
      },
      {
        "paperId": null,
        "title": "Autonomous reinforcement learning: Benchmarking and formalism"
      },
      {
        "paperId": null,
        "title": "SkewFit (Pong et al., 2019): We follow the state-based version of SkewFit"
      },
      {
        "paperId": "5dbe244846bfedfd687be0cdaba19befcd96c8f6",
        "title": "Conference Paper"
      },
      {
        "paperId": "68d54f9dacbb5416c1aafb3399c072497c320021",
        "title": "Network Flows: Theory, Algorithms, and Applications"
      },
      {
        "paperId": null,
        "title": "Amortized conditional normalized maximum likelihood: Reliable out of distribution uncertainty estimation"
      }
    ],
    "cited_by": [
      {
        "paperId": "c30fb8d6688ee8c34d7ae9ffc6960903485b215c",
        "title": "Causal-Paced Deep Reinforcement Learning"
      },
      {
        "paperId": "760389d05f01f9acc7258b8ba00797d835f25044",
        "title": "DISCOVER: Automated Curricula for Sparse-Reward Reinforcement Learning"
      },
      {
        "paperId": "47fccd1b5e3faa0e9184c8b401c3b3486717e697",
        "title": "Imagine, Verify, Execute: Memory-Guided Agentic Exploration with Vision-Language Models"
      },
      {
        "paperId": "20ba3f353304917056f00e7bd95d9d0e08a1c20a",
        "title": "Causally Aligned Curriculum Learning"
      },
      {
        "paperId": "e853fd226fb5ed0bb32d1c19b71d8b4d3abad09b",
        "title": "Adversarial Environment Design via Regret-Guided Diffusion Models"
      },
      {
        "paperId": "5738c4d2ee9ac98309c6cd7869e9a1c1c0a913ef",
        "title": "Offline Policy Learning via Skill-step Abstraction for Long-horizon Goal-Conditioned Tasks"
      },
      {
        "paperId": "e07e4931a6df9fc3a54fc5452ab2ca18d60fb9f1",
        "title": "Domain Randomization via Entropy Maximization"
      },
      {
        "paperId": "746eea132521f94d23818a18e57bcadd31710e43",
        "title": "Diversify & Conquer: Outcome-directed Curriculum RL via Out-of-Distribution Disagreement"
      },
      {
        "paperId": "9d552f44fe2dc219038a4a3874b7fc67e917c0fd",
        "title": "CQM: Curriculum Reinforcement Learning with a Quantized World Model"
      },
      {
        "paperId": "5da75b9c90607843d502d128d0c197f77321a9d9",
        "title": "On the Benefit of Optimal Transport for Curriculum Reinforcement Learning"
      },
      {
        "paperId": "2e3ba918a407f5e5d7a4bae88e38e281578c9040",
        "title": "Can Pre-Trained Text-to-Image Models Generate Visual Goals for Reinforcement Learning?"
      },
      {
        "paperId": "dae6a9c393115ba36391d249918d743709e8aee8",
        "title": "What Went Wrong? Closing the Sim-to-Real Gap via Differentiable Causal Discovery"
      },
      {
        "paperId": "7fc69210214a55076b6109d6628120c96cdf825e",
        "title": "Demonstration-free Autonomous Reinforcement Learning via Implicit and Bidirectional Curriculum"
      },
      {
        "paperId": "d44b88ac4acf6964c3a1506ab3df38721125fb7c",
        "title": "CLUTR: Curriculum Learning via Unsupervised Task Representation Learning"
      },
      {
        "paperId": "2a5c1474526ca9d46164e4e64db99a2404ff94ad",
        "title": "Diffusion-based Curriculum Reinforcement Learning"
      },
      {
        "paperId": "322f0a5cda167cd34ae09a2a7c5894ace6cc869f",
        "title": "Generating curriculum via Decision Transformer in Maze and Robotics environments"
      }
    ],
    "score": 8.0
  },
  {
    "id": "9a67ff1d46d691f7741822d7a13587a517b1be14",
    "title": "MobileGUI-RL: Advancing Mobile GUI Agent through Reinforcement Learning in Online Environment",
    "authors": [
      "Yucheng Shi",
      "Wenhao Yu",
      "Zaitang Li",
      "Yonglin Wang",
      "Hongming Zhang",
      "Ninghao Liu",
      "Haitao Mi",
      "Dong Yu"
    ],
    "year": 2025,
    "citationCount": 8,
    "abstract": "Recently, there has been a surge of vision-based GUI agents designed to automate everyday mobile and web tasks. These agents interpret raw GUI screenshots and autonomously decide where to click, scroll, or type, which bypasses handcrafted rules and app-specific APIs. However, most existing methods trained GUI agent in the offline environment using pre-collected trajectories. This approach limits scalability, causes overfitting to specific UI templates, and leads to brittle policies when faced with unseen environment. We present MobileGUI-RL, a scalable framework that trains GUI agent in online environment. MobileGUI-RL contains two key components. It (i) synthesizes a curriculum of learnable tasks through self-exploration and filtering, and (ii) adapts GRPO to GUI navigation with trajectory-aware advantages and composite rewards that balance task success and execution efficiency. Experiments on three online mobile-agent benchmarks show consistent gains, validating the effectiveness of our approach.",
    "url": "https://www.semanticscholar.org/paper/9a67ff1d46d691f7741822d7a13587a517b1be14",
    "pdf_url": "https://arxiv.org/pdf/2507.05720.pdf",
    "venue": "arXiv.org",
    "publicationDate": "2025-07-08",
    "externalIds": {
      "DBLP": "journals/corr/abs-2507-05720",
      "ArXiv": "2507.05720",
      "DOI": "10.48550/arXiv.2507.05720",
      "CorpusId": 280136906
    },
    "references": [
      {
        "paperId": "ac7ac3c4c6a8fcbfb8180070a53392d96862c701",
        "title": "ZeroGUI: Automating Online GUI Learning at Zero Human Cost"
      },
      {
        "paperId": "99259d51eadfe2473fe8eb643625a6bd1c3c0a41",
        "title": "WebCoT: Enhancing Web Agent Reasoning by Reconstructing Chain-of-Thought in Reflection, Branching, and Rollback"
      },
      {
        "paperId": "a343a24a0d47e0acf4d71bfd0215924bc76d0256",
        "title": "WebAgent-R1: Training Web Agents via End-to-End Multi-Turn Reinforcement Learning"
      },
      {
        "paperId": "ea9591cf281409ffcbc284129d52050fea9cc115",
        "title": "ARPO:End-to-End Policy Optimization for GUI Agents with Experience Replay"
      },
      {
        "paperId": "e45dde9b1cd0aebe318282f70454d9dbe8501765",
        "title": "GUI-G1: Understanding R1-Zero-Like Training for Visual Grounding in GUI Agents"
      },
      {
        "paperId": "742d9c80b2ca4d01f8a8675cfe98487e0783d3d7",
        "title": "Group-in-Group Policy Optimization for LLM Agent Training"
      },
      {
        "paperId": "076f059aa724619ad346503623c5ee57d4625c9d",
        "title": "WebEvolver: Enhancing Web Agent Self-Improvement with Coevolving World Model"
      },
      {
        "paperId": "b7c5ad386f20954334a539c0558309a5a5bc311d",
        "title": "InfiGUI-R1: Advancing Multimodal GUI Agents from Reactive Actors to Deliberative Reasoners"
      },
      {
        "paperId": "4189a60effdf6f29c5e20cd0748a8a66411f150a",
        "title": "WebRollback: Enhancing Web Agents with Explicit Rollback Mechanisms"
      },
      {
        "paperId": "b12b7e05de83d3a3b345cf9392e79a03f8c2712e",
        "title": "GUI-R1 : A Generalist R1-Style Vision-Language Action Model For GUI Agents"
      },
      {
        "paperId": "dd4cfde3e135f799a9a71b4f57e13a29de89f7e3",
        "title": "DAPO: An Open-Source LLM Reinforcement Learning System at Scale"
      },
      {
        "paperId": "f61cc9b5583c6295d5cd756ec0f34e4c003aab29",
        "title": "Qwen2.5-VL Technical Report"
      },
      {
        "paperId": "34471a2fa18ea22efad5287cf4aeb18542c98a9b",
        "title": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "paperId": "23c5d8fb6de30553efd1e38f796b40f2bff33453",
        "title": "UI-TARS: Pioneering Automated GUI Interaction with Native Agents"
      },
      {
        "paperId": "285f38de28b79df0bb64a859ab0d79048e860f30",
        "title": "OS-Genesis: Automating GUI Agent Trajectory Construction via Reverse Task Synthesis"
      },
      {
        "paperId": "58ae4a85ae73095da668d6eac3a8c1f19270c89d",
        "title": "Aguvis: Unified Pure Vision Agents for Autonomous GUI Interaction"
      },
      {
        "paperId": "0663fea61d9fec219f3f84dd2e4aa716edc4223a",
        "title": "Large Language Model-Brained GUI Agents: A Survey"
      },
      {
        "paperId": "84fc28aba83502dafb6be68ca2e46fbbada03029",
        "title": "GUI Agents with Foundation Models: A Comprehensive Survey"
      },
      {
        "paperId": "9ceadd197cea2cd1b45e12983d528daf2b24efaa",
        "title": "OS-ATLAS: A Foundation Action Model for Generalist GUI Agents"
      },
      {
        "paperId": "7943ec4a67151a559b25cd34369e661c9a7924c8",
        "title": "GPT-4o System Card"
      },
      {
        "paperId": "cf925627d69833bedd0c63ec32932bd0e747fcf3",
        "title": "DistRL: An Asynchronous Distributed Reinforcement Learning Framework for On-Device Control Agents"
      },
      {
        "paperId": "a15a4cabe7653a87efc3302552fbc88b25d19dc1",
        "title": "Navigating the Digital World as Humans Do: Universal Visual Grounding for GUI Agents"
      },
      {
        "paperId": "c6e3e5f794233da2b27ea5dd4af19e62b95c863f",
        "title": "DigiRL: Training In-The-Wild Device-Control Agents with Autonomous Reinforcement Learning"
      },
      {
        "paperId": "d1b2eaf7aaebbd3d847272da04be180e35c7b68b",
        "title": "AndroidWorld: A Dynamic Benchmarking Environment for Autonomous Agents"
      },
      {
        "paperId": "2199a5ac145edbc46db9fcbf97b01461a2367cda",
        "title": "Android in the Zoo: Chain-of-Action-Thought for GUI Agents"
      },
      {
        "paperId": "35b142ea69598e6241f0011312128031df55895c",
        "title": "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models"
      },
      {
        "paperId": "7e281e8ab380affd3c5724feae038274df378511",
        "title": "Understanding the planning of LLM agents: A survey"
      },
      {
        "paperId": "c844694387a89a477e7a8bbf918171cdc3b85672",
        "title": "GPT-4V(ision) is a Generalist Web Agent, if Grounded"
      },
      {
        "paperId": "e50583008fe4ec049e42fdc01727ce98f6d86a35",
        "title": "CogAgent: A Visual Language Model for GUI Agents"
      },
      {
        "paperId": "a53c8ba374d430d6c3786d13c04edb200d547750",
        "title": "A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "5c3391bde2bb1b3d737913ee8caa01492a782732",
        "title": "WHO Technical Report"
      },
      {
        "paperId": null,
        "title": "Introducing computer use, a new claude 3.5 sonnet, and claude 3.5 haiku"
      },
      {
        "paperId": null,
        "title": "Android Developers. Run apps on the Android Emulator"
      },
      {
        "paperId": null,
        "title": "Introducing claude 3.5"
      }
    ],
    "cited_by": [
      {
        "paperId": "39088601f53158d14d8fca88fea4408ecc577f88",
        "title": "GUI-Shepherd: Reliable Process Reward and Verification for Long-Sequence GUI Tasks"
      },
      {
        "paperId": "191047ab6432d519c08712d3178db6c13be7d985",
        "title": "RISK: A Framework for GUI Agents in E-commerce Risk Management"
      },
      {
        "paperId": "0008694cd91187b7f4bd394831153bb94bebbdca",
        "title": "UI-S1: Advancing GUI Automation via Semi-online Reinforcement Learning"
      },
      {
        "paperId": "2034941745dea3fb9c35d17c89acf391ee57dfde",
        "title": "Dark Patterns Meet GUI Agents: LLM Agent Susceptibility to Manipulative Interfaces and the Role of Human Oversight"
      },
      {
        "paperId": "3a5548b9e9bb7b83e2179ca261f8f8aa0d5f1966",
        "title": "Parallel-R1: Towards Parallel Thinking via Reinforcement Learning"
      },
      {
        "paperId": "b9b5338f512c544faef4651364054570a0deaf36",
        "title": "UI-TARS-2 Technical Report: Advancing GUI Agent with Multi-Turn Reinforcement Learning"
      },
      {
        "paperId": "9a5071bacb2bcb98ddd95076dbd446240b4da2c6",
        "title": "Reinforcement Learning in Vision: A Survey"
      },
      {
        "paperId": "1385ac414416374b63acfd82ccd6d91ed62fe101",
        "title": "R-Zero: Self-Evolving Reasoning LLM from Zero Data"
      }
    ],
    "score": 8.0
  },
  {
    "id": "134acf45a016fced57cc8f8e171eeb6e0015b0b8",
    "title": "Open the Black Box: Step-based Policy Updates for Temporally-Correlated Episodic Reinforcement Learning",
    "authors": [
      "Ge Li",
      "Hongyi Zhou",
      "Dominik Roth",
      "Serge Thilges",
      "Fabian Otto",
      "Rudolf Lioutikov",
      "Gerhard Neumann"
    ],
    "year": 2024,
    "citationCount": 8,
    "abstract": "Current advancements in reinforcement learning (RL) have predominantly focused on learning step-based policies that generate actions for each perceived state. While these methods efficiently leverage step information from environmental interaction, they often ignore the temporal correlation between actions, resulting in inefficient exploration and unsmooth trajectories that are challenging to implement on real hardware. Episodic RL (ERL) seeks to overcome these challenges by exploring in parameters space that capture the correlation of actions. However, these approaches typically compromise data efficiency, as they treat trajectories as opaque \\emph{black boxes}. In this work, we introduce a novel ERL algorithm, Temporally-Correlated Episodic RL (TCE), which effectively utilizes step information in episodic policy updates, opening the 'black box' in existing ERL methods while retaining the smooth and consistent exploration in parameter space. TCE synergistically combines the advantages of step-based and episodic RL, achieving comparable performance to recent ERL methods while maintaining data efficiency akin to state-of-the-art (SoTA) step-based RL.",
    "url": "https://www.semanticscholar.org/paper/134acf45a016fced57cc8f8e171eeb6e0015b0b8",
    "pdf_url": "https://arxiv.org/pdf/2401.11437.pdf",
    "venue": "International Conference on Learning Representations",
    "publicationDate": "2024-01-21",
    "externalIds": {
      "DBLP": "journals/corr/abs-2401-11437",
      "ArXiv": "2401.11437",
      "DOI": "10.48550/arXiv.2401.11437",
      "CorpusId": 267069198
    },
    "references": [
      {
        "paperId": "d6c9fca2b4d589187548c0a809b73b6fa6b91e75",
        "title": "Deep Probabilistic Movement Primitives with a Bayesian Aggregator"
      },
      {
        "paperId": "b37ea6e8705d01c17eca2c037a7fb2b7b3a221a2",
        "title": "MP3: Movement Primitive-Based (Re-)Planning Policy"
      },
      {
        "paperId": "54952feedd88ca743dc6b01cfbc707d9254bbdb3",
        "title": "Latent Exploration for Reinforcement Learning"
      },
      {
        "paperId": "cf628a42ee56c8f1b858790822a2bc0a61a49110",
        "title": "Deep Black-Box Reinforcement Learning with Movement Primitives"
      },
      {
        "paperId": "8d1c713612618317907a3bebb6db768f42aa4f08",
        "title": "ProDMP: A Unified Perspective on Dynamic and Probabilistic Movement Primitives"
      },
      {
        "paperId": "04615a9955bce148aa7ba29e864389c26e10523a",
        "title": "DEP-RL: Embodied Exploration for Reinforcement Learning in Overactuated and Musculoskeletal Systems"
      },
      {
        "paperId": "a2e6cb2bcb7f89ded3b097005e5077e01004b33a",
        "title": "Specializing Versatile Skill Libraries using Local Mixture of Experts"
      },
      {
        "paperId": "6f11555ecb8d1421f66f06c07113062823f92fb3",
        "title": "Orientation Probabilistic Movement Primitives on Riemannian Manifolds"
      },
      {
        "paperId": "558ca2e8c7eb56edd77a52b084e6cc24dffe5bcd",
        "title": "Deep Reinforcement Learning at the Edge of the Statistical Precipice"
      },
      {
        "paperId": "9776324620082ab89edad134aa259b1ae7bf6f1f",
        "title": "TempoRL: Learning When to Act"
      },
      {
        "paperId": "57d5339c41981347b16bba3718c4cde04899842c",
        "title": "Jerk-limited Real-time Trajectory Generation with Arbitrary Target States"
      },
      {
        "paperId": "1508879dae81f73f56ba0cb0e25150d9c5f8f731",
        "title": "TAAC: Temporally Abstract Actor-Critic for Continuous Control"
      },
      {
        "paperId": "5fa8b76256a2125c7a72db372b6e0d6be90d3a54",
        "title": "Neural Dynamic Policies for End-to-End Sensorimotor Learning"
      },
      {
        "paperId": "3e0925355554e3aeb99de8165c268582a82de3bb",
        "title": "Smooth Exploration for Robotic Reinforcement Learning"
      },
      {
        "paperId": "6343d4b71fa91066fdccc0a9ccbd16b2fc099095",
        "title": "Training of deep neural networks for the generation of dynamic movement primitives"
      },
      {
        "paperId": "57b6ceaa623f3f881130b3974a9310390254e78d",
        "title": "ACNMP: Skill Transfer and Task Extrapolation through Learning from Demonstration and Reinforcement Learning via Representation Sharing"
      },
      {
        "paperId": "3659598fb5283c18c704f1452823cffa56380fd5",
        "title": "Improving Local Trajectory Optimisation using Probabilistic Movement Primitives"
      },
      {
        "paperId": "3bc1faf5a59eb124b53888c01f46cc62933e29e3",
        "title": "Learning Via-Point Movement Primitives with Inter- and Extrapolation Capabilities"
      },
      {
        "paperId": "88e83776313effc1564044d7bf19972981815e3c",
        "title": "Differentiable Convex Optimization Layers"
      },
      {
        "paperId": "0bc855f84668b35cb65618d996d09f6e434d28c9",
        "title": "Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning"
      },
      {
        "paperId": "123c4570902a5202a4fb49e9d0b3feeb3b4ef86e",
        "title": "Conditional Neural Movement Primitives"
      },
      {
        "paperId": "5d922b9bf05e99f5b26b349e40cc41e0dd9d8df5",
        "title": "Robot Learning System Based on Adaptive Neural Control and Dynamic Movement Primitives"
      },
      {
        "paperId": "12c0751b4f51ed833172a713b7e32390032ead93",
        "title": "Soft Actor-Critic Algorithms and Applications"
      },
      {
        "paperId": "811df72e210e20de99719539505da54762a11c6d",
        "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor"
      },
      {
        "paperId": "561c7fe68668787b084682665a8b36bb0629b613",
        "title": "Contextual Covariance Matrix Adaptation Evolutionary Strategies"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "4ee802a58d32aa049d549d06be440ac947b53987",
        "title": "Evolution Strategies as a Scalable Alternative to Reinforcement Learning"
      },
      {
        "paperId": "de19027b36ba0e9d669ce2f4194e5f3937aa0517",
        "title": "A Probabilistic Representation for Dynamic Movement Primitives"
      },
      {
        "paperId": "9328a4d5ad2d6a7bff5f2d94a8551df318b7f6bb",
        "title": "Policy Search with High-Dimensional Context Variables"
      },
      {
        "paperId": "73b1b19280594d9837dbeda477d7efe94269d7c6",
        "title": "Using probabilistic movement primitives for striking movements"
      },
      {
        "paperId": "d20b1b1ebe8f0368f9d7b81cc88ecdd471b01557",
        "title": "Model-Based Relative Entropy Stochastic Search"
      },
      {
        "paperId": "af2fab96e8cb67ebf9bdb47842dcaad581267d85",
        "title": "A tutorial on task-parameterized movement learning and retrieval"
      },
      {
        "paperId": "024006d4c2a89f7acacc6e4438d156525b60a98f",
        "title": "Continuous control with deep reinforcement learning"
      },
      {
        "paperId": "d316c82c12cf4c45f9e85211ef3d1fa62497bff8",
        "title": "High-Dimensional Continuous Control Using Generalized Advantage Estimation"
      },
      {
        "paperId": "31d24ff043c4656b6854701da0ac93549c86d31c",
        "title": "Trajectory generation for immediate path-accurate jerk-limited stopping of industrial robots"
      },
      {
        "paperId": "449532187c94af3dd3aa55e16d2c50f7854d2199",
        "title": "Trust Region Policy Optimization"
      },
      {
        "paperId": "609c895a37666e25aa503fd6e787ac937bd1110b",
        "title": "Learning interaction for collaborative tasks with probabilistic movement primitives"
      },
      {
        "paperId": "969bc45575676b1f11e7175b8317a7b7c24909cb",
        "title": "Interaction primitives for human-robot cooperation tasks"
      },
      {
        "paperId": "3a81cfb4a7a880b7cf8979f6067732e961aceb7c",
        "title": "Probabilistic Movement Primitives"
      },
      {
        "paperId": "5438f71c01dd713c6f4e05a48c4c8f9c5f82a805",
        "title": "Dynamical Movement Primitives: Learning Attractor Models for Motor Behaviors"
      },
      {
        "paperId": "4fbbbdd796b98743148d4a8b71ab97bd7e3fcbdc",
        "title": "Statistical dynamical systems for skills acquisition in humanoids"
      },
      {
        "paperId": "bd5fc28c7356915ec71abafbe86b7596c60720aa",
        "title": "The conference paper"
      },
      {
        "paperId": "8ac68370386cb90e72189dd4d0b487bb3da19665",
        "title": "Exploring parameter space in reinforcement learning"
      },
      {
        "paperId": "2a9566897343f07029baf3fda12eca67a84f81d8",
        "title": "Sensitivity of Smoothness Measures to Movement Duration, Amplitude, and Arrests"
      },
      {
        "paperId": "f9d7d776e0240b5714f730f4c11415facecb7ab3",
        "title": "Spatial resolution of spontaneous accelerations in reaching tasks."
      },
      {
        "paperId": "56109a43347a1a1d5dc84c0dff88016e58ea3938",
        "title": "State-Dependent Exploration for Policy Gradient Methods"
      },
      {
        "paperId": "966e41903b4aff42601a188bd7b26d71ef120d11",
        "title": "Accelerated Neural Evolution through Cooperatively Coevolved Synapses"
      },
      {
        "paperId": "ffced5b53ad956474a12d73b5cbfd38355dfb70a",
        "title": "Reinforcement learning of motor skills with policy gradients"
      },
      {
        "paperId": "04377cba52732168d3e55e229bae2ee1ad4d43a2",
        "title": "Neuroevolution for reinforcement learning using evolution strategies"
      },
      {
        "paperId": "a20f0ce0616def7cc9a87446c228906cd5da093b",
        "title": "Policy Gradient Methods for Reinforcement Learning with Function Approximation"
      },
      {
        "paperId": "caf4eba5187db2cdf9d0446679b6f80e6752c226",
        "title": "On generating power law noise."
      },
      {
        "paperId": "4a06ec48e4b413ab2563273981018df82faac32e",
        "title": "Pink Noise Is All You Need: Colored Noise Exploration in Deep Reinforcement Learning"
      },
      {
        "paperId": "e3fc5b5627af62ee6981a02090cf6bae368202d7",
        "title": "Stable-Baselines3: Reliable Reinforcement Learning Implementations"
      },
      {
        "paperId": "758ba118d7f161fe019d81c4f64a9069c342aa74",
        "title": "Simple random search of static linear policies is competitive for reinforcement learning"
      },
      {
        "paperId": "d893434d78a81f7c7a7c9aee44894b53c7b1359d",
        "title": "Frame Skip Is a Powerful Parameter for Learning to Play Atari"
      },
      {
        "paperId": "2065d9eb28be0700a235afb78e4a073845bfb67d",
        "title": "Dynamic Movement Primitives -A Framework for Motor Control in Humans and Humanoid Robotics"
      },
      {
        "paperId": "1fe7e65d0682e5a54b92c0ff61f461fd93381f37",
        "title": "Genetic Reinforcement Learning for Neurocontrol Problems"
      },
      {
        "paperId": "4c915c1eecb217c123a36dc6d3ce52d12c742614",
        "title": "Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning"
      },
      {
        "paperId": "97efafdb4a3942ab3efba53ded7413199f79c054",
        "title": "Reinforcement Learning: An Introduction"
      },
      {
        "paperId": "4d6a87d76ec0c0379f0afbf24f84bba848c6246e",
        "title": "Noname manuscript No. (will be inserted by the editor) Policy Search for Motor Primitives in Robotics"
      },
      {
        "paperId": null,
        "title": "The detailed hyper parameters used"
      }
    ],
    "cited_by": [
      {
        "paperId": "afce1a0a71c37b16b99a73ec3bcd382467543633",
        "title": "MoRe-ERL: Learning Motion Residuals Using Episodic Reinforcement Learning"
      },
      {
        "paperId": "574ec4679c9efdaac629b8fd7f01ee07d78b741d",
        "title": "Chunking the Critic: A Transformer-based Soft Actor-Critic with N-Step Returns"
      },
      {
        "paperId": "27c307a89d3a07563456eba69327371bff2c0c09",
        "title": "Geometry-aware RL for Manipulation of Varying Shapes and Deformable Objects"
      },
      {
        "paperId": "6c282fca36e15f2aeb978c6c6edf54bac43ea5cc",
        "title": "BMP: Bridging the Gap between B-Spline and Movement Primitives"
      },
      {
        "paperId": "a72661ca3c4de880152e69c7e20383645eb20056",
        "title": "TOP-ERL: Transformer-based Off-Policy Episodic Reinforcement Learning"
      },
      {
        "paperId": "6efb032415f6e62d8d5de7a1571f35901e1c6f97",
        "title": "Overcoming Slow Decision Frequencies in Continuous Control: Model-Based Sequence Reinforcement Learning for Model-Free Control"
      },
      {
        "paperId": "2ee493aa7f9939f7d52f7309fd88053b939b87e6",
        "title": "Bridging the gap between Learning-to-plan, Motion Primitives and Safe Reinforcement Learning"
      },
      {
        "paperId": "dd483e4956eafba4bc82af26a29e0075a696f788",
        "title": "Acquiring Diverse Skills using Curriculum Reinforcement Learning with Mixture of Experts"
      }
    ],
    "score": 8.0
  },
  {
    "id": "a9c896060fa85f01f289baaad346e98e94dbed4c",
    "title": "A Comprehensive Survey of Reinforcement Learning: From Algorithms to Practical Challenges",
    "authors": [
      "Majid Ghasemi",
      "Amir Hossein Moosavi",
      "Dariush Ebrahimi"
    ],
    "year": 2024,
    "citationCount": 8,
    "abstract": "Reinforcement Learning (RL) has emerged as a powerful paradigm in Artificial Intelligence (AI), enabling agents to learn optimal behaviors through interactions with their environments. Drawing from the foundations of trial and error, RL equips agents to make informed decisions through feedback in the form of rewards or penalties. This paper presents a comprehensive survey of RL, meticulously analyzing a wide range of algorithms, from foundational tabular methods to advanced Deep Reinforcement Learning (DRL) techniques. We categorize and evaluate these algorithms based on key criteria such as scalability, sample efficiency, and suitability. We compare the methods in the form of their strengths and weaknesses in diverse settings. Additionally, we offer practical insights into the selection and implementation of RL algorithms, addressing common challenges like convergence, stability, and the exploration-exploitation dilemma. This paper serves as a comprehensive reference for researchers and practitioners aiming to harness the full potential of RL in solving complex, real-world problems.",
    "url": "https://www.semanticscholar.org/paper/a9c896060fa85f01f289baaad346e98e94dbed4c",
    "pdf_url": "https://arxiv.org/pdf/2411.18892.pdf",
    "venue": "",
    "publicationDate": "2024-11-28",
    "externalIds": {
      "ArXiv": "2411.18892",
      "CorpusId": 274422589
    },
    "references": [],
    "cited_by": [
      {
        "paperId": "c1503b5f677a8e133db58edd4af0c2408dd73956",
        "title": "Train-Once Plan-Anywhere Kinodynamic Motion Planning via Diffusion Trees"
      },
      {
        "paperId": "c430897158e2bab32307dc91f60c256f4c0c80ae",
        "title": "ReCode: Updating Code API Knowledge with Reinforcement Learning"
      },
      {
        "paperId": "6f56e2f1bb6780b0d85b85425dade1a79d9f6820",
        "title": "Reliability Evidence for AI-Based Scores in Organizational Contexts: Applying Lessons Learned From Psychometrics"
      },
      {
        "paperId": "87918c80365e2ca82611dcd6313915aa4c13ded8",
        "title": "From Large AI Models to Agentic AI: A Tutorial on Future Intelligent Communications"
      },
      {
        "paperId": "5133dc6836def28e8fe3334736a6ffd3aea455e3",
        "title": "Frequency Resource Management in 6G User-Centric CFmMIMO: A Hybrid Reinforcement Learning and Metaheuristic Approach"
      },
      {
        "paperId": "b2a988ca779e548ba665d0db170ebe2800088fc8",
        "title": "Generative AI for Autonomous Driving: A Review"
      },
      {
        "paperId": "def32a1ed3dae5a7185b23720d5c7343daeea3fe",
        "title": "RLBSA-based Academic Information System Optimization for Student Performance Prediction"
      },
      {
        "paperId": "e54f2a86192ea5437e64107d6f4f3534b702ad8c",
        "title": "Next-Future: Sample-Efficient Policy Learning for Robotic-Arm Tasks"
      }
    ],
    "score": 8.0
  },
  {
    "id": "68de2fdc9b516eba96b9726dfa0e77ee66336605",
    "title": "A Motion Planning Method for Visual Servoing Using Deep Reinforcement Learning in Autonomous Robotic Assembly",
    "authors": [
      "Zhen-yu Liu",
      "Ke Wang",
      "Daxin Liu",
      "Qide Wang",
      "Jianrong Tan"
    ],
    "year": 2023,
    "citationCount": 15,
    "abstract": "Assembly positioning by visual servoing (VS) is a basis for autonomous robotic assembly. In practice, VS control suffers potential stability and convergence problems due to image and physical constraints, e.g., field of view constraints, image local minima, obstacle collisions, and occlusion. Therefore, this article proposes a novel deep reinforcement learning-based hybrid visual servoing (DRL-HVS) controller for motion planning of VS tasks. DRL-HVS controller takes current observed image features and camera pose as inputs, and the core parameters of hybrid VS are dynamically optimized using a deep deterministic policy gradient (DDPG) algorithm to obtain an optimal motion scheme, considering image/physical constraints and robot motion performance. In addition, an adaptive exploration strategy is proposed to further improve the training efficiency by adaptively tuning the exploration noise parameters. In this way, the offline pretrained DRL-HVS controller in the virtual environment, where the DDPG actor\u2013critic network is continuously optimized, can be quickly deployed to a real robot system for real-time control. Experiments based on an eye-in-hand VS system are conducted with a calibrated HIKVISION RGB camera mounted on the end-effector of a GSK-RB03A1 six degree-of-freedom (6-DoF) robot. Basic VS task experiments show that the proposed controller achieves better performance than the existing methods: the servoing time is 24% smaller than that of the five-dimensional VS method, a 100% success rate with the perturbed ranges of the initial position within 25 mm for translation and 20\u00b0 for rotation, and a 48% efficiency improvement. Moreover, a planetary gear component assembly process case study, where the robot aims to automatically put the gears on the gear shafts, is conducted to demonstrate the applicability of the proposed method in practice.",
    "url": "https://www.semanticscholar.org/paper/68de2fdc9b516eba96b9726dfa0e77ee66336605",
    "pdf_url": "https://doi.org/10.1109/TMECH.2023.3275854",
    "venue": "IEEE/ASME transactions on mechatronics",
    "publicationDate": "2023-12-01",
    "externalIds": {
      "DOI": "10.1109/TMECH.2023.3275854",
      "CorpusId": 258980672
    },
    "references": [],
    "cited_by": [
      {
        "paperId": "528f8f16829d10979c0c160c57767c22ecb618a0",
        "title": "A Transfer Reinforcement Learning-Based Control Method for Vertical n-Link Underactuated Manipulator With Two Passive Joints"
      },
      {
        "paperId": "74d1bb0ed17a825f2cb002c5e1d622922c46c2e6",
        "title": "Advance on Agricultural Robot Hand\u2013Eye Coordination for Agronomic Task: A Review"
      },
      {
        "paperId": "041b5e168be5ef5e1d5bbcff8591430717a1e3e2",
        "title": "Digital Triads as Next-Generation Mechatronic Systems for Sustainability\u2014A Case Study"
      },
      {
        "paperId": "97fa0d5ab4b10f835f9042f72cee047b40a6d5e4",
        "title": "Lightweight visual backbone network with enhanced comprehensive strength through context-aware dual attention mechanism"
      },
      {
        "paperId": "a97b237e291995555293cb7260f69966e5198161",
        "title": "An end-to-end controller with image-based visual servoing of industrial manipulators with soft-actor-critic algorithm"
      },
      {
        "paperId": "d1d46b3486818de77deef856ae0b6b3bed355998",
        "title": "Multimodal Variational DeepMDP: An Efficient Approach for Industrial Assembly in High-Mix, Low-Volume Production"
      },
      {
        "paperId": "2424d4c67f993f94db3c8e10b553e0410f41d1aa",
        "title": "Physically informed hierarchical learning based soft sensing for aero-engine health management unit"
      },
      {
        "paperId": "06bcc4735ecc821fb5b9d57e57dcdb325fb60b06",
        "title": "Detecting System of Contact Network Wrist Arm Bolts with 3D Template Matching and Visual Servoing Control"
      },
      {
        "paperId": "dbf9a8f654bbbac7aac57b9ade315570e2710e25",
        "title": "Robust Point Cloud Registration in Robotic Inspection With Locally Consistent Gaussian Mixture Model"
      },
      {
        "paperId": "570bd7ced3f21ac8e5b42f1fb4f8fd3b3fc1c50e",
        "title": "A residual reinforcement learning method for robotic assembly using visual and force information"
      },
      {
        "paperId": "3f4c937d1e6c768ee6073284eae0932e5d1609ad",
        "title": "Finite-time image-based visual servoing with field of view and performance constraints"
      },
      {
        "paperId": "7ab9f3e2747d03604439344803d746959ba5c41a",
        "title": "Multitime-Scale Model Predictive Control Method for Robot Grasping Based on Visual Servoing"
      },
      {
        "paperId": "c0ffce8d81f78bd4547ce8e0706bb49e95371be1",
        "title": "Adaptive Backstepping Optimal Tracking Control of Interconnected Robotic Manipulator System Based on Reinforcement Learning"
      },
      {
        "paperId": "a6078b68d2e527a0272d53b26d472160417bd5be",
        "title": "Visual Servoing Control for Robot Manipulators"
      },
      {
        "paperId": "ea9974a24e21dcbc47a3370efd3c4c860545921f",
        "title": "Enhancement of Control Performance for Degraded Robot Manipulators Using Digital Twin and Proximal Policy Optimization"
      }
    ],
    "score": 7.5
  },
  {
    "id": "01f35fa70fc881ab80206121738380c57f8d2074",
    "title": "Relative Entropy Regularized Sample-Efficient Reinforcement Learning With Continuous Actions",
    "authors": [
      "Zhiwei Shang",
      "Renxing Li",
      "Chunhuang Zheng",
      "Huiyun Li",
      "Yunduan Cui"
    ],
    "year": 2023,
    "citationCount": 15,
    "abstract": "In this article, a novel reinforcement learning (RL) approach, continuous dynamic policy programming (CDPP), is proposed to tackle the issues of both learning stability and sample efficiency in the current RL methods with continuous actions. The proposed method naturally extends the relative entropy regularization from the value function-based framework to the actor\u2013critic (AC) framework of deep deterministic policy gradient (DDPG) to stabilize the learning process in continuous action space. It tackles the intractable softmax operation over continuous actions in the critic by Monte Carlo estimation and explores the practical advantages of the Mellowmax operator. A Boltzmann sampling policy is proposed to guide the exploration of actor following the relative entropy regularized critic for superior learning capability, exploration efficiency, and robustness. Evaluated by several benchmark and real-robot-based simulation tasks, the proposed method illustrates the positive impact of the relative entropy regularization including efficient exploration behavior and stable policy update in RL with continuous action space and successfully outperforms the related baseline approaches in both sample efficiency and learning stability.",
    "url": "https://www.semanticscholar.org/paper/01f35fa70fc881ab80206121738380c57f8d2074",
    "pdf_url": "https://doi.org/10.1109/TNNLS.2023.3329513",
    "venue": "IEEE Transactions on Neural Networks and Learning Systems",
    "publicationDate": "2023-11-09",
    "externalIds": {
      "DBLP": "journals/tnn/ShangLZLC25",
      "DOI": "10.1109/TNNLS.2023.3329513",
      "CorpusId": 265103251,
      "PubMed": "37943648"
    },
    "references": [],
    "cited_by": [
      {
        "paperId": "767cb43db4c11b042a8501669f372ef776f1a98d",
        "title": "Efficient Model Based Reinforcement Learning Control Using Relative Entropy Regularization"
      },
      {
        "paperId": "8ac16de275714f25344d452ae266faf30e7116b3",
        "title": "Generalizable Causal Reinforcement Learning for Out-of-Distribution Environments"
      },
      {
        "paperId": "5f849d1e26da2663523e0bcc16e9538c96cb179f",
        "title": "Effective Reinforcement Learning Control using Conservative Soft Actor-Critic"
      },
      {
        "paperId": "62bd9a8883735935c00ad49a26972ea67c554ed5",
        "title": "Risk-aware deep reinforcement learning for mapless navigation of unmanned surface vehicles in uncertain and congested environments"
      },
      {
        "paperId": "e7e30405361d9a897d02a366d4ebf9a98a036e89",
        "title": "Visionary Policy Iteration for Continuous Control"
      },
      {
        "paperId": "bf628b15bf0b18ce928dc2f24bcf5aae09207cc0",
        "title": "Enhancing Maritime Remote Control via Edge-Cloud Computing: A Bifurcated Vehicle Control Approach"
      },
      {
        "paperId": "5c554f5380b0ba1a8ee8ce7df282cae87d7c40e5",
        "title": "Implementation of deep reinforcement learning models for emotion detection and personalization of learning in hybrid educational environments"
      },
      {
        "paperId": "f2dce2dc429595a137c3b7e28dcf0753e1365166",
        "title": "Recovering Permuted Sequential Features for effective Reinforcement Learning"
      },
      {
        "paperId": "bec3fffeab99d6058412605b6f6bd2fb8cf5eae8",
        "title": "Historical Decision-Making Regularized Maximum Entropy Reinforcement Learning"
      },
      {
        "paperId": "ba3bea4b15d5ef33579ce86da83b92ee1ae0b347",
        "title": "Using reinforcement learning to autonomously identify sources of error for agents in group missions"
      },
      {
        "paperId": "4b0283574bccae8993383e824d32f0561c9bec7f",
        "title": "Continuous Value Assignment: A Doubly Robust Data Augmentation for Off-Policy Learning"
      },
      {
        "paperId": "0cb21e40114469a27b4586258670a7a880a70c8e",
        "title": "Arm-Constrained Curriculum Learning for Loco-Manipulation of a Wheel-Legged Robot"
      },
      {
        "paperId": "f114e0d03a5fda080b1a04e31f8873464a2ed872",
        "title": "Risk-Sensitive Reinforcement Learning With Exponential Criteria"
      },
      {
        "paperId": "08867cbc02fd252c5b12d7de8ce147ecf4b5d288",
        "title": "Superior energy management for fuel cell vehicles guided by improved DDPG algorithm: Integrating driving intention speed prediction and health-aware control"
      },
      {
        "paperId": "e1e4d5ce95980560703695cc222c89fcf34f6cc0",
        "title": "Optimal dynamic thermal management for data center via soft actor-critic algorithm with dynamic control interval and combined-value state space"
      }
    ],
    "score": 7.5
  },
  {
    "id": "5dce3f2cfe018ada4bebeaa15aac6fe1d01b18ba",
    "title": "Imagine, Initialize, and Explore: An Effective Exploration Method in Multi-Agent Reinforcement Learning",
    "authors": [
      "Zeyang Liu",
      "Lipeng Wan",
      "Xinrui Yang",
      "Zhuoran Chen",
      "Xingyu Chen",
      "Xuguang Lan"
    ],
    "year": 2024,
    "citationCount": 7,
    "abstract": "Effective exploration is crucial to discovering optimal strategies for multi-agent reinforcement learning (MARL) in complex coordination tasks. Existing methods mainly utilize intrinsic rewards to enable committed exploration or use role-based learning for decomposing joint action spaces instead of directly conducting a collective search in the entire action-observation space. However, they often face challenges obtaining specific joint action sequences to reach successful states in long-horizon tasks. To address this limitation, we propose Imagine, Initialize, and Explore (IIE), a novel method that offers a promising solution for efficient multi-agent exploration in complex scenarios. IIE employs a transformer model to imagine how the agents reach a critical state that can influence each other's transition functions. Then, we initialize the environment at this state using a simulator before the exploration phase. We formulate the imagination as a sequence modeling problem, where the states, observations, prompts, actions, and rewards are predicted autoregressively. The prompt consists of timestep-to-go, return-to-go, influence value, and one-shot demonstration, specifying the desired state and trajectory as well as guiding the action generation. By initializing agents at the critical states, IIE significantly increases the likelihood of discovering potentially important under-explored regions. Despite its simplicity, empirical results demonstrate that our method outperforms multi-agent exploration baselines on the StarCraft Multi-Agent Challenge (SMAC) and SMACv2 environments. Particularly, IIE shows improved performance in the sparse-reward SMAC tasks and produces more effective curricula over the initialized states than other generative methods, such as CVAE-GAN and diffusion models.",
    "url": "https://www.semanticscholar.org/paper/5dce3f2cfe018ada4bebeaa15aac6fe1d01b18ba",
    "pdf_url": "https://arxiv.org/pdf/2402.17978.pdf",
    "venue": "AAAI Conference on Artificial Intelligence",
    "publicationDate": "2024-02-28",
    "externalIds": {
      "DBLP": "conf/aaai/LiuWYCCL24",
      "ArXiv": "2402.17978",
      "DOI": "10.48550/arXiv.2402.17978",
      "CorpusId": 268041208
    },
    "references": [
      {
        "paperId": "2c67ee597ed38f43ec0f123a3f1cce38cbd3b5b4",
        "title": "Sources of Hallucination by Large Language Models on Inference Tasks"
      },
      {
        "paperId": "83593fb7585711095e016f2b70368d132bacc3c7",
        "title": "MASER: Multi-Agent Reinforcement Learning with Subgoals Generated from Experience Replay Buffer"
      },
      {
        "paperId": "1386b8a11929cf02da291c56aca353e33bbc22ed",
        "title": "Diffusion-LM Improves Controllable Text Generation"
      },
      {
        "paperId": "3ebdd3db0dd91069fa0cd31cbf8308b60b1b565e",
        "title": "Planning with Diffusion for Flexible Behavior Synthesis"
      },
      {
        "paperId": "e227f71d7d84ed9a76278510a46f9b7db286ba92",
        "title": "Episodic Multi-agent Reinforcement Learning with Curiosity-Driven Exploration"
      },
      {
        "paperId": "c1ad5f9b32d80f1c65d67894e5b8c2fdf0ae4500",
        "title": "Decision Transformer: Reinforcement Learning via Sequence Modeling"
      },
      {
        "paperId": "cd37fee4da0d4483322d6fa3cc67af9ed8c07be6",
        "title": "Efficient Transformers in Reinforcement Learning using Actor-Learner Distillation"
      },
      {
        "paperId": "3a315c81a98851f0614c09fef6a14c30d6a1e63c",
        "title": "The Surprising Effectiveness of PPO in Cooperative Multi-Agent Games"
      },
      {
        "paperId": "5764095b0186a3fc3832c1052aa14996a5927edc",
        "title": "RODE: Learning Roles to Decompose Multi-Agent Tasks"
      },
      {
        "paperId": "052c100d45f949c06e8419b504e319b442cb3f0a",
        "title": "QPLEX: Duplex Dueling Multi-Agent Q-Learning"
      },
      {
        "paperId": "e423c07b36936ddce137bce009b318f2c2741be5",
        "title": "Adaptive Procedural Task Generation for Hard-Exploration Problems"
      },
      {
        "paperId": "1c4809748affb4d4f24d013ab4cd1b58ce4bd1f1",
        "title": "Weighted QMIX: Expanding Monotonic Value Function Factorisation"
      },
      {
        "paperId": "616ac6772508e72084360158078058dfa8bda7e7",
        "title": "First return, then explore"
      },
      {
        "paperId": "de93c8aed64229571b03e40b36499d4f07ce875d",
        "title": "PBCS : Efficient Exploration and Exploitation Using a Synergy between Reinforcement Learning and Motion Planning"
      },
      {
        "paperId": "c38e624c8d176153a23ada19978f334c9ec94bae",
        "title": "MAVEN: Multi-Agent Variational Exploration"
      },
      {
        "paperId": "59a916cdc943f0282908e6f3fa0360f4c5fb78d0",
        "title": "Stabilizing Transformers for Reinforcement Learning"
      },
      {
        "paperId": "abb0460b839a7cae30245c9690d54462f8275e47",
        "title": "Google Research Football: A Novel Reinforcement Learning Environment"
      },
      {
        "paperId": "56ca9b304e0476feaa6dd4fd1cccc8c0a1a9d8eb",
        "title": "QTRAN: Learning to Factorize with Transformation for Cooperative Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "82055eed2ba0d7156a54c586249742c848e5d565",
        "title": "The StarCraft Multi-Agent Challenge"
      },
      {
        "paperId": "c520bf47db3360ae3a52219771390a354ed8a91f",
        "title": "Go-Explore: a New Approach for Hard-Exploration Problems"
      },
      {
        "paperId": "f9717d29840f4d8f1cc19d1b1e80c5d12ec40608",
        "title": "A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play"
      },
      {
        "paperId": "e0121c1d2dc5c8cc77f4b1570e28f2443ece2a8f",
        "title": "Social Influence as Intrinsic Motivation for Multi-Agent Deep Reinforcement Learning"
      },
      {
        "paperId": "ffc211476f2e40e79466ffc198c919a97da3bb76",
        "title": "QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "7c3ece1ba41c415d7e81cfa5ca33a8de66efd434",
        "title": "Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments"
      },
      {
        "paperId": "471f9742b4e32d8ee68f9ee493768ff0466a231d",
        "title": "Automatic Goal Generation for Reinforcement Learning Agents"
      },
      {
        "paperId": "3d5e5d447a6627b517175ca7f3e7ba25702130ba",
        "title": "CVAE-GAN: Fine-Grained Image Generation through Asymmetric Training"
      },
      {
        "paperId": "ad7a5047a5c6871fd6aeaeaed52d78106c932243",
        "title": "QMDP-Net: Deep Learning for Planning under Partial Observability"
      },
      {
        "paperId": "836ad0c693bc5ef171ee2b07b3f4d1bd2a0ae24c",
        "title": "A Concise Introduction to Decentralized POMDPs"
      },
      {
        "paperId": "4e79dbb9068341f29d96199856b8c915766a59ef",
        "title": "Multi-agent reinforcement learning as a rehearsal for decentralized planning"
      },
      {
        "paperId": "f5f323e62acb75f785e00b4c90ace16f1690076f",
        "title": "Deep Recurrent Q-Learning for Partially Observable MDPs"
      },
      {
        "paperId": "449381674c67d6fd178776dfcfe1769da795b105",
        "title": "EAT-C: Environment-Adversarial sub-Task Curriculum for Efficient Reinforcement Learning"
      },
      {
        "paperId": "2ecac5bdd4dfe0b9c13e64e44e17caab1cfeb76b",
        "title": "SMARTS: An Open-Source Scalable Multi-Agent RL Training School for Autonomous Driving"
      },
      {
        "paperId": "a28899f255c1ca35b6254adc1a0cd64fc20c2ce9",
        "title": "Memory Based Trajectory-conditioned Policies for Learning from Sparse Rewards"
      },
      {
        "paperId": "3f963a700f3d89aebefde51a7e895a145964af53",
        "title": "LIIR: Learning Individual Intrinsic Reward in Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035",
        "title": "Improving Language Understanding by Generative Pre-Training"
      },
      {
        "paperId": null,
        "title": "2023. Self-checkgpt: Zero-resource black-box hallucination detection for generative large language models"
      },
      {
        "paperId": null,
        "title": "Define a log-probability log P ( I \u2217 | s 0 ) = log p ( I| s 0 ) + \u03ba ( I \u2212 I low ) / ( I high \u2212 I low ) # Sample a influence value"
      },
      {
        "paperId": null,
        "title": "2022. Multi-game decision trans-formers"
      },
      {
        "paperId": null,
        "title": "2020c. In\ufb02uence-Based Multi-Agent Exploration"
      }
    ],
    "cited_by": [
      {
        "paperId": "0aab03cbdd5f3971af9ade1b00dbb311ae8de202",
        "title": "State Revisit and Re-explore: Bridging Sim-to-Real Gaps in Offline-and-Online Reinforcement Learning with An Imperfect Simulator"
      },
      {
        "paperId": "bc2b9e42cc8cc660cce41cdf1fefc5c2bfb4030a",
        "title": "Communicating Plans, Not Percepts: Scalable Multi-Agent Coordination with Embodied World Models"
      },
      {
        "paperId": "1facb873da12e417fc515e6cf9c9fb306dcde81e",
        "title": "Center of Gravity-Guided Focusing Influence Mechanism for Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "052136a967f6dda0fcba463563c920fc81b1b1fa",
        "title": "Enhancing Cooperative Multi-Agent Reinforcement Learning with State Modelling and Adversarial Exploration"
      },
      {
        "paperId": "a929145017d69e588be95419980a991bd0044f02",
        "title": "Decentralized Navigation of a Cable-Towed Load using Quadrupedal Robot Team via MARL"
      },
      {
        "paperId": "67d19b3e36d430140c755ff8fc4bb4d240ac97be",
        "title": "Grounded Answers for Multi-agent Decision-making Problem through Generative World Model"
      },
      {
        "paperId": "f8564d05a3bf9a178849ed60ae523e3b07f50363",
        "title": "Efficient Exploration in Multi-Agent Reinforcement Learning via Farsighted Self-Direction"
      }
    ],
    "score": 7.0
  },
  {
    "id": "390ba2d438a33bf0add55e0e7b2831fc034db41f",
    "title": "Sample-Efficient Reinforcement Learning With Temporal Logic Objectives: Leveraging the Task Specification to Guide Exploration",
    "authors": [
      "Y. Kantaros",
      "Jun Wang"
    ],
    "year": 2024,
    "citationCount": 7,
    "abstract": "In this article, we address the problem of learning optimal control policies for systems with uncertain dynamics and high-level control objectives specified as linear temporal logic (LTL) formulas. Uncertainty is considered in the workspace structure and the outcomes of control decisions giving rise to an unknown Markov decision process (MDP). Existing reinforcement learning (RL) algorithms for LTL tasks typically rely on exploring a product MDP state-space uniformly (using e.g., an $\\epsilon$-greedy policy) compromising sample-efficiency. This issue becomes more pronounced as the rewards get sparser and the MDP size or the task complexity increase. In this article, we propose an accelerated RL algorithm that can learn control policies significantly faster than competitive approaches. Its sample-efficiency relies on a novel task-driven exploration strategy that biases exploration toward directions that may contribute to task satisfaction. We provide theoretical analysis and extensive comparative experiments demonstrating the sample-efficiency of the proposed method. The benefit of our method becomes more evident as the task complexity or the MDP size increases.",
    "url": "https://www.semanticscholar.org/paper/390ba2d438a33bf0add55e0e7b2831fc034db41f",
    "pdf_url": "https://arxiv.org/pdf/2410.12136.pdf",
    "venue": "IEEE Transactions on Automatic Control",
    "publicationDate": "2024-10-16",
    "externalIds": {
      "DBLP": "journals/corr/abs-2410-12136",
      "ArXiv": "2410.12136",
      "DOI": "10.1109/TAC.2024.3484290",
      "CorpusId": 273375678
    },
    "references": [
      {
        "paperId": "7e5ea69b20739831989801dbe40469214082f915",
        "title": "On the Uniqueness of Solution for the Bellman Equation of LTL Objectives"
      },
      {
        "paperId": "23514ced370ae993eb43453d13b00527536d220b",
        "title": "Learning Minimally-Violating Continuous Control for Infeasible Linear Temporal Logic Specifications"
      },
      {
        "paperId": "2c5d2a6966cda44886b50bb4174ea6ffd90325d9",
        "title": "LCRL: Certified Policy Synthesis via Logically-Constrained Reinforcement Learning"
      },
      {
        "paperId": "66a7f836749c18def4e715e08fbd34021fc8a85e",
        "title": "Decentralized route-planning for multi-vehicle teams to satisfy a subclass of linear temporal logic specifications"
      },
      {
        "paperId": "8d435266a42467e3dc4fbc8cbbd3b6eab6852fa3",
        "title": "Accelerated Reinforcement Learning for Temporal Logic Control Objectives"
      },
      {
        "paperId": "b3993d523559d297dbd592ab2bbd2635d32a92f8",
        "title": "Model-Free Reinforcement Learning for Spatiotemporal Tasks Using Symbolic Automata"
      },
      {
        "paperId": "28ddab3a41c6c7bf0b847695dddd0d65ef40a5ec",
        "title": "Overcoming Exploration: Deep Reinforcement Learning for Continuous Control in Cluttered Environments From Temporal Logic Specifications"
      },
      {
        "paperId": "3be84e24b144541a8cd9030526ef2b8ef2cbfe54",
        "title": "A Survey of Exploration Methods in Reinforcement Learning"
      },
      {
        "paperId": "c4ec534bd46033b862f3b6746da4b185e2222d6f",
        "title": "Computational Benefits of Intermediate Rewards for Goal-Reaching Policy Learning"
      },
      {
        "paperId": "ce7e0c1e5f22fa80fbeba7792c1323180a99359f",
        "title": "Compositional Reinforcement Learning from Logical Specifications"
      },
      {
        "paperId": "d7e8c8bef0c27bcfc62d7d88e30f511d2bad971a",
        "title": "DeepSynth: Automata Synthesis for Automatic Task Segmentation in Deep Reinforcement Learning"
      },
      {
        "paperId": "f41ce455e1019ebae9a7208a98951e279a4b41d4",
        "title": "Learning-Based Probabilistic LTL Motion Planning With Environment and Motion Uncertainties"
      },
      {
        "paperId": "453690a1a0b3c5d8faef9f8864d9c10a8e05f629",
        "title": "Learning Optimal Strategies for Temporal Tasks in Stochastic Games"
      },
      {
        "paperId": "5a2b60efb6d4fe10f6b896df053e1cffefc5d6cc",
        "title": "Perception-Based Temporal Logic Planning in Uncertain Semantic Maps"
      },
      {
        "paperId": "6778d6a0f959cdcc42718ee9fc279fd1f00f3d88",
        "title": "Reward Machines: Exploiting Reward Function Structure in Reinforcement Learning"
      },
      {
        "paperId": "23e2ba1830a10a3477a2752ddad0e40b46eee705",
        "title": "Optimal Probabilistic Motion Planning With Potential Infeasible LTL Constraints"
      },
      {
        "paperId": "0d02cb2c68d2ef3026b783fed0f016df29e7eb97",
        "title": "Reactive sampling-based path planning with temporal logic specifications"
      },
      {
        "paperId": "f9f0729e9156ea6c2b65bad5339434b958afe473",
        "title": "Formal Controller Synthesis for Continuous-Space MDPs via Model-Free Reinforcement Learning"
      },
      {
        "paperId": "129983331ca874142a3e8eb2d93d820bdf1f9aca",
        "title": "Deep Reinforcement Learning for Autonomous Driving: A Survey"
      },
      {
        "paperId": "7c7c36b83556b71221c898aa4e7ea2edabd80314",
        "title": "Control Synthesis from Linear Temporal Logic Specifications using Model-Free Reinforcement Learning"
      },
      {
        "paperId": "3ee06b4f0ed488bbffc84e441cd66372ee5de0e8",
        "title": "Reinforcement Learning for Temporal Logic Control Synthesis with Probabilistic Satisfaction Guarantees"
      },
      {
        "paperId": "c88d5b4f62832f2bc02dc696964b736e196915c9",
        "title": "Reduced variance deep reinforcement learning with temporal logic specifications"
      },
      {
        "paperId": "015e0e44f474969a4e00e6d304c997b2915c27a4",
        "title": "Reinforcement Learning with Probabilistic Guarantees for Autonomous Driving"
      },
      {
        "paperId": "9539673a4711775f7b8b30830293b6d19b01edcd",
        "title": "Certified Reinforcement Learning with Logic Guidance"
      },
      {
        "paperId": "7dbfb295615534834ab2d7706a7e481afe9be186",
        "title": "Omega-Regular Objectives in Model-Free Reinforcement Learning"
      },
      {
        "paperId": "910220d57571ac4a2a7f441fb98200de89391c1f",
        "title": "STyLuS*: A Temporal Logic Optimal Control Synthesis Algorithm for Large-Scale Multi-Robot Systems"
      },
      {
        "paperId": "00ec8123dd2ba03afab7c1fa02f774062f769181",
        "title": "Using Reward Machines for High-Level Task Specification and Decomposition in Reinforcement Learning"
      },
      {
        "paperId": "1485c5ac93b3f01a5f3ef7824eb428c8bff39ba3",
        "title": "UCB EXPLORATION VIA Q-ENSEMBLES"
      },
      {
        "paperId": "20ba0f36166d22224f926a0b8d28624f19c4c4c0",
        "title": "Distributed Intermittent Connectivity Control of Mobile Robot Networks"
      },
      {
        "paperId": "cf4f8d6530406271f0ca34f4332af80050b3254e",
        "title": "Probabilistic Motion Planning Under Temporal Tasks and Soft Constraints"
      },
      {
        "paperId": "11026fc42c4d30f9ea91ec8c32f8d75768b70f6d",
        "title": "Boltzmann Exploration Done Right"
      },
      {
        "paperId": "225ab689f41cef1dc18237ef5dab059a49950abf",
        "title": "Curiosity-Driven Exploration by Self-Supervised Prediction"
      },
      {
        "paperId": "fcd1c627e2f03eea711a42e230feff7c2ad774fc",
        "title": "Distributed data gathering with buffer constraints and intermittent communication"
      },
      {
        "paperId": "e6f47b224046bd639ae258c4da6f0b227f273522",
        "title": "Persistent surveillance for unmanned aerial vehicles subject to charging and temporal logic constraints"
      },
      {
        "paperId": "0f25aff62f5def45dea25d950177a8ff96f66be1",
        "title": "Optimal Policy Generation for Partially Satisfiable Co-Safe LTL Specifications"
      },
      {
        "paperId": "71b0f481e04c6da184a6c19919643187e0775493",
        "title": "Probably Approximately Correct MDP Learning and Control With Temporal Logic Constraints"
      },
      {
        "paperId": "5ffdc6b4129b6db44dfc28ed5fa0136e07996b51",
        "title": "Verification of Markov Decision Processes Using Learning Algorithms"
      },
      {
        "paperId": "afae05a4f1be733bce1c567c04fa9ae1e2798ebd",
        "title": "LTL receding horizon control for finite deterministic systems"
      },
      {
        "paperId": "33d66a9fd3faff7bf3c36859caa07113484b7a5b",
        "title": "Optimal Control of Markov Decision Processes With Linear Temporal Logic Constraints"
      },
      {
        "paperId": "f1e4773897b65a96f935aecc2d09cea8171395ec",
        "title": "Software"
      },
      {
        "paperId": "ed718e65042663167b98b08628e1024c5bdb9fa5",
        "title": "Temporal logic motion control using actor\u2013critic methods"
      },
      {
        "paperId": "44586746522d6fb341bc47293c1065f07f9c2f78",
        "title": "LTL Control in Uncertain Environments with Probabilistic Satisfaction Guarantees"
      },
      {
        "paperId": "e108bab11c7dffa0d6532c79dcfc04d3ff1e3155",
        "title": "Probabilistic Policy Reuse for inter-task transfer learning"
      },
      {
        "paperId": "881a5e20469d8d63e6a834ccfd713ebfb55702be",
        "title": "Temporal logic motion planning for dynamic robots"
      },
      {
        "paperId": "552a4887cdf1d8e2c24a6bc37b2ec8427a74ef43",
        "title": "Probabilistic reachability and safety for controlled discrete time stochastic hybrid systems"
      },
      {
        "paperId": "f1269591359fddc20f95da10c7bd4c054080b447",
        "title": "Principles of model checking"
      },
      {
        "paperId": "05498544d433b17d69acd4c05bbe444c243e3ff9",
        "title": "A Fully Automated Framework for Control of Linear Systems from Temporal Logic Specifications"
      },
      {
        "paperId": "12d1d070a53d4084d88a77b8b143bad51c40c38f",
        "title": "Reinforcement Learning: A Survey"
      },
      {
        "paperId": "a9cd8efe9184dddb1bedbbec3a356c4dfb22fe63",
        "title": "Markov Decision Processes: Discrete Stochastic Dynamic Programming"
      },
      {
        "paperId": "5cd52bcf8e7514d4cbbf1aaa0433dfa8ab44ee8d",
        "title": "On Efficiency in Hierarchical Reinforcement Learning"
      },
      {
        "paperId": null,
        "title": "\u201cLimit-deterministicB\u00fcchiautomataforlineartemporallogic,\u201din"
      },
      {
        "paperId": "fddab6f5edb57d3cbb7583c564b19586352705be",
        "title": "Reinforcement Learning and the Reward Engineering Principle"
      },
      {
        "paperId": "97efafdb4a3942ab3efba53ded7413199f79c054",
        "title": "Reinforcement Learning: An Introduction"
      },
      {
        "paperId": null,
        "title": "Systems Engineering, Washington St. Louis, St. Louis, MO, USA"
      },
      {
        "paperId": null,
        "title": "current research interests include machine learning, distributed and"
      }
    ],
    "cited_by": [
      {
        "paperId": "3b075710aecc4e8e8a035215d70004a0ea93be1f",
        "title": "ConformalNL2LTL: Translating Natural Language Instructions into Temporal Logic Formulas with Conformal Correctness Guarantees"
      },
      {
        "paperId": "85317048752487e06ba31239eb5b4233c4337242",
        "title": "Bridging Deep Reinforcement Learning and Motion Planning for Model-Free Navigation in Cluttered Environments"
      },
      {
        "paperId": "7795c6625bbec8e5b46998fdc2d4f6341b52f00a",
        "title": "LATMOS: Latent Automaton Task Model from Observation Sequences"
      },
      {
        "paperId": "b920e9ea80f388b5ccf41c3a1a7a2ddef555ac04",
        "title": "Optimal Control of Markov Decision Processes for Efficiency with Linear Temporal Logic Tasks"
      },
      {
        "paperId": "affb6fee2f8602d907378509b9ab85a050ca1e6f",
        "title": "Mission-driven Exploration for Accelerated Deep Reinforcement Learning with Temporal Logic Task Specifications"
      },
      {
        "paperId": "719dfe743343d170605649edb4bb34285e8bb3dc",
        "title": "Washington University Open Scholarship Washington University Open Scholarship"
      },
      {
        "paperId": "2418a6fd97ff213ed7483362e8987895ec94b0bb",
        "title": "WashU Scholarly Repository WashU Scholarly Repository"
      }
    ],
    "score": 7.0
  },
  {
    "id": "085dddfa3105967d4b9f09b1cd0fa7725779faf1",
    "title": "MCMC: Multi-Constrained Model Compression via One-Stage Envelope Reinforcement Learning",
    "authors": [
      "Siqi Li",
      "Jun Chen",
      "Shanqi Liu",
      "Chengrui Zhu",
      "Guanzhong Tian",
      "Yong Liu"
    ],
    "year": 2024,
    "citationCount": 7,
    "abstract": "Model compression methods are being developed to bridge the gap between the massive scale of neural networks and the limited hardware resources on edge devices. Since most real-world applications deployed on resource-limited hardware platforms typically have multiple hardware constraints simultaneously, most existing model compression approaches that only consider optimizing one single hardware objective are ineffective. In this article, we propose an automated pruning method called multi-constrained model compression (MCMC) that allows for the optimization of multiple hardware targets, such as latency, floating point operations (FLOPs), and memory usage, while minimizing the impact on accuracy. Specifically, we propose an improved multi-objective reinforcement learning (MORL) algorithm, the one-stage envelope deep deterministic policy gradient (DDPG) algorithm, to determine the pruning strategy for neural networks. Our improved one-stage envelope DDPG algorithm reduces exploration time and offers greater flexibility in adjusting target priorities, enhancing its suitability for pruning tasks. For instance, on the visual geometry group (VGG)-16 network, our method achieved an 80% reduction in FLOPs, a <inline-formula> <tex-math notation=\"LaTeX\">$2.31\\times $ </tex-math></inline-formula> reduction in memory usage, and a <inline-formula> <tex-math notation=\"LaTeX\">$1.92\\times $ </tex-math></inline-formula> acceleration, with an accuracy improvement of 0.09% compared with the baseline. For larger datasets, such as ImageNet, we reduced FLOPs by 50% for MobileNet-V1, resulting in a <inline-formula> <tex-math notation=\"LaTeX\">$4.7\\times $ </tex-math></inline-formula> faster speed and <inline-formula> <tex-math notation=\"LaTeX\">$1.48\\times $ </tex-math></inline-formula> memory compression, while maintaining the same accuracy. When applied to edge devices, such as JETSON XAVIER NX, our method resulted in a 71% reduction in FLOPs for MobileNet-V1, leading to a <inline-formula> <tex-math notation=\"LaTeX\">$1.63\\times $ </tex-math></inline-formula> faster speed, <inline-formula> <tex-math notation=\"LaTeX\">$1.64\\times $ </tex-math></inline-formula> memory compression, and an accuracy improvement.",
    "url": "https://www.semanticscholar.org/paper/085dddfa3105967d4b9f09b1cd0fa7725779faf1",
    "pdf_url": "https://doi.org/10.1109/TNNLS.2024.3353763",
    "venue": "IEEE Transactions on Neural Networks and Learning Systems",
    "publicationDate": "2024-01-30",
    "externalIds": {
      "DBLP": "journals/tnn/LiCLZTL25",
      "DOI": "10.1109/TNNLS.2024.3353763",
      "CorpusId": 267333309,
      "PubMed": "38289836"
    },
    "references": [],
    "cited_by": [
      {
        "paperId": "e077c1ee2b52888fa2fd8fb315be0297a608056f",
        "title": "CSRAP: Enhanced Canvas Attention Scheduling for Real-Time Mission Critical Perception"
      },
      {
        "paperId": "2185bbbf5b9b6e0ff930175e51112a8a832e6f7e",
        "title": "Singular values-driven automated filter pruning."
      },
      {
        "paperId": "65ac7f5646b374e299ff8db5218d6d0a7f16f3cb",
        "title": "Optimizing Deep Learning for Skin Cancer Classification: A Computationally Efficient CNN with Minimal Accuracy Trade-Off"
      },
      {
        "paperId": "0f324818656b10661c8c965fe6dbc5a01183b158",
        "title": "Resource-Constrained Specific Emitter Identification Based on Efficient Design and Network Compression"
      },
      {
        "paperId": "197ba6e19867b1df6c3bd20f6c59b85eac0aaf85",
        "title": "Physics-informed neural network compression mechanism for airfoil flow field prediction"
      },
      {
        "paperId": "a371bf2ca952d5dc4adf583890cbd5bf66ae5957",
        "title": "All-in-One Hardware-Oriented Model Compression for Efficient Multi-Hardware Deployment"
      },
      {
        "paperId": "bb8904af47896b6dd6a2c841f13bf8ef56c01770",
        "title": "Optimization of a Cluster-Based Energy Management System Using Deep Reinforcement Learning Without Affecting Prosumer Comfort: V2X Technologies and Peer-to-Peer Energy Trading"
      }
    ],
    "score": 7.0
  },
  {
    "id": "99d0760520eaa1dc81b5531cd45101e00474bbd1",
    "title": "De Novo Drug Design Using Transformer-Based Machine Translation and Reinforcement Learning of an Adaptive Monte Carlo Tree Search",
    "authors": [
      "Dony Ang",
      "Cyril Rakovski",
      "H. Atamian"
    ],
    "year": 2024,
    "citationCount": 7,
    "abstract": "The discovery of novel therapeutic compounds through de novo drug design represents a critical challenge in the field of pharmaceutical research. Traditional drug discovery approaches are often resource intensive and time consuming, leading researchers to explore innovative methods that harness the power of deep learning and reinforcement learning techniques. Here, we introduce a novel drug design approach called drugAI that leverages the Encoder\u2013Decoder Transformer architecture in tandem with Reinforcement Learning via a Monte Carlo Tree Search (RL-MCTS) to expedite the process of drug discovery while ensuring the production of valid small molecules with drug-like characteristics and strong binding affinities towards their targets. We successfully integrated the Encoder\u2013Decoder Transformer architecture, which generates molecular structures (drugs) from scratch with the RL-MCTS, serving as a reinforcement learning framework. The RL-MCTS combines the exploitation and exploration capabilities of a Monte Carlo Tree Search with the machine translation of a transformer-based Encoder\u2013Decoder model. This dynamic approach allows the model to iteratively refine its drug candidate generation process, ensuring that the generated molecules adhere to essential physicochemical and biological constraints and effectively bind to their targets. The results from drugAI showcase the effectiveness of the proposed approach across various benchmark datasets, demonstrating a significant improvement in both the validity and drug-likeness of the generated compounds, compared to two existing benchmark methods. Moreover, drugAI ensures that the generated molecules exhibit strong binding affinities to their respective targets. In summary, this research highlights the real-world applications of drugAI in drug discovery pipelines, potentially accelerating the identification of promising drug candidates for a wide range of diseases.",
    "url": "https://www.semanticscholar.org/paper/99d0760520eaa1dc81b5531cd45101e00474bbd1",
    "pdf_url": "https://doi.org/10.3390/ph17020161",
    "venue": "Pharmaceuticals",
    "publicationDate": "2024-01-27",
    "externalIds": {
      "PubMedCentral": "10892138",
      "DOI": "10.3390/ph17020161",
      "CorpusId": 267344774,
      "PubMed": "38399376"
    },
    "references": [
      {
        "paperId": "27df41b62e50bf5b7a6e835f916399a945c24daa",
        "title": "Virtual and In Vitro Screening of Natural Products Identifies Indole and Benzene Derivatives as Inhibitors of SARS-CoV-2 Main Protease (Mpro)"
      },
      {
        "paperId": "d83ee195225c14502fa6216d583d3cee1be7c3d3",
        "title": "Advances and Challenges in De Novo Drug Design Using Three-Dimensional Deep Generative Models"
      },
      {
        "paperId": "c363a4291f8af7b1a12cdcf1f92dcc05f4aa9a34",
        "title": "Generative machine learning for de novo drug discovery: A systematic review"
      },
      {
        "paperId": "6dc1db69749fcb6484a11cd9465e9945068027bf",
        "title": "PPL-MCTS: Constrained Textual Generation Through Discriminator-Guided MCTS Decoding"
      },
      {
        "paperId": "7f4c17cde1da6f7af2d8596f51a6e3e5041a3312",
        "title": "Molecular design in drug discovery: a comprehensive review of deep generative models"
      },
      {
        "paperId": "e8cc5b6204970a88cd1b2df491aa10c4333e083e",
        "title": "Machine Translation Decoding beyond Beam Search"
      },
      {
        "paperId": "8214d2934b7ab7cb682566c01344bb1e24320b49",
        "title": "Advances in De Novo Drug Design: From Conventional to Machine Learning Methods"
      },
      {
        "paperId": "80bb30e65c36545f7dcaae8fa9f1e66e550d4731",
        "title": "A survey on deep reinforcement learning for audio-based applications"
      },
      {
        "paperId": "65d59868889b11ed9ed8d5ec93b6472b6ead61f7",
        "title": "Molecular Dynamics Simulations in Drug Discovery and Pharmaceutical Development"
      },
      {
        "paperId": "59f29b423e313e8c3ab5dab7ef444190dc8cbe8d",
        "title": "Deep learning and generative methods in cheminformatics and chemical biology: navigating small molecule space intelligently"
      },
      {
        "paperId": "8509cc4e5421db6fdfd30528bcc15c651be0c0fa",
        "title": "A comprehensive survey and analysis of generative models in machine learning"
      },
      {
        "paperId": "52ff4fe2e8fb5837dbc86e764f76326a6c404c8d",
        "title": "Changing the HTS Paradigm: AI-Driven Iterative Screening for Hit Finding"
      },
      {
        "paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
        "title": "Language Models are Few-Shot Learners"
      },
      {
        "paperId": "b8b738721528ec7901e3e6cd530aecdb43822248",
        "title": "Protein promiscuity in drug discovery, drug-repurposing and antibiotic resistance."
      },
      {
        "paperId": "37102b58b92421dc3c517e8e4830620860efc433",
        "title": "Fundamentals of Artificial Intelligence"
      },
      {
        "paperId": "8a8492ffdea7ac2d4ad36b5670662927f0e9b57d",
        "title": "A Review on Applications of Computational Methods in Drug Screening and Design"
      },
      {
        "paperId": "43f2ad297941db230c089ba353efc3f281ab678c",
        "title": "5\u5206\u3067\u5206\u304b\u308b!? \u6709\u540d\u8ad6\u6587\u30ca\u30ca\u30e1\u8aad\u307f\uff1aJacob Devlin et al. : BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding"
      },
      {
        "paperId": "4925d50efc261cd9f251a1106f109f98ed2eb85a",
        "title": "Transformer neural network for protein-specific de novo drug generation as a machine translation problem"
      },
      {
        "paperId": "3c8b953cccb19a0a1679c28f73b33df20b2fd10e",
        "title": "Progress in molecular docking"
      },
      {
        "paperId": "5bfeb6901db481c08874cfe0ae807d8564513765",
        "title": "GuacaMol: Benchmarking Models for De Novo Molecular Design"
      },
      {
        "paperId": "a7b597e57b3ff7af51447ee3b170ef257bdedc4f",
        "title": "Virtual Chemical Libraries."
      },
      {
        "paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
        "title": "Attention is All you Need"
      },
      {
        "paperId": "c66786983236493651a4348176620a4f7375f743",
        "title": "Filtering promiscuous compounds in early drug discovery: is it a good idea?"
      },
      {
        "paperId": "8a544b19f567cec5788b4c83755ab78a636d8307",
        "title": "BDDCS, the Rule of 5 and drugability."
      },
      {
        "paperId": "5507dc32b368c8afd3b9507e9b5888da7bd7d7cd",
        "title": "Sequence-to-Sequence Learning as Beam-Search Optimization"
      },
      {
        "paperId": "846aedd869a00c09b40f1f1f35673cb22bc87490",
        "title": "Mastering the game of Go with deep neural networks and tree search"
      },
      {
        "paperId": "d4921d6c5dfd0741026028745b63b450e4e81cd1",
        "title": "BindingDB in 2015: A public database for medicinal chemistry, computational chemistry and systems pharmacology"
      },
      {
        "paperId": "cea967b59209c6be22829699f05b8b1ac4dc092d",
        "title": "Sequence to Sequence Learning with Neural Networks"
      },
      {
        "paperId": "b7e54a5d6149cde03fd5144d38a0647d25a795f6",
        "title": "The ChEMBL bioactivity database: an update"
      },
      {
        "paperId": "11bce438e2fdc7a7ae5f65c339c757f386f4f48a",
        "title": "Quantifying the chemical beauty of drugs."
      },
      {
        "paperId": "c1203ff7582cf15a160591f6d29e434e27db4e97",
        "title": "AutoDock Vina: Improving the speed and accuracy of docking with a new scoring function, efficient optimization, and multithreading"
      },
      {
        "paperId": "f3466604570f2da3b4d666c49fe89973f30cf49f",
        "title": "Drug discovery: Playing dirty"
      },
      {
        "paperId": "40412d274386872bf4a6f235505692864d864946",
        "title": "Lead- and drug-like compounds: the rule-of-five revolution."
      },
      {
        "paperId": "6dc9d05336cc3d3df78eef1ae7f974ae6e183eb8",
        "title": "A guide to drug discovery: Designing screens: how to make your hits a hit"
      },
      {
        "paperId": "d0c500408210b15b8c604da7c3481d68e574a009",
        "title": "Property distribution of drug-related chemical databases*"
      },
      {
        "paperId": "1a3f80af28be2a2c22fdd40379a9a2396de0b276",
        "title": "Natural-Language Processing"
      },
      {
        "paperId": null,
        "title": "RDKit: Open-Source Cheminformatics"
      },
      {
        "paperId": "55f4079602ac1e71939497072607fb037bae3c68",
        "title": "Structure- and ligand-based drug design"
      },
      {
        "paperId": "f7a093036ba75d772e3cd7cd31c25c3bb0baf440",
        "title": "Advances and applications of binding affinity prediction methods in drug discovery."
      },
      {
        "paperId": "07e6b38dc70fc45e50ee780f36e767bed15544fb",
        "title": "Optimal Structure Identification With Greedy Search"
      },
      {
        "paperId": null,
        "title": "b. Transformers are parallelizable, and this makes it efficient to parallelize the training and inference steps against Graphics Processing Units (GPUs) and Tensor Processing Units (TPUs)"
      }
    ],
    "cited_by": [
      {
        "paperId": "fe532399e33ed2ce410a686db2423c215b0e98a8",
        "title": "ABIET: An explainable transformer for identifying functional groups in biological active molecules"
      },
      {
        "paperId": "f79bd3c534639f098124ecce603327d9feb2399a",
        "title": "The future of pharmaceuticals: Artificial intelligence in drug discovery and development"
      },
      {
        "paperId": "9b24f856410791739d6ca883f6c4c351a5576e0d",
        "title": "Leveraging Reinforcement Learning and AlphaGo Algorithms for Enhanced Cancer Detection: A Novel Approach in Medical AI"
      },
      {
        "paperId": "a2c09283801b8e2e1fa838aaa745117d8df4ba0e",
        "title": "Harnessing AI and Quantum Computing for Revolutionizing Drug Discovery and Approval Processes: Case Example for Collagen Toxicity"
      },
      {
        "paperId": "b1db12852e3faff8e8255081f52944d9b65ce743",
        "title": "Biosimilars in the Era of Artificial Intelligence\u2014International Regulations and the Use in Oncological Treatments"
      },
      {
        "paperId": "0a73b79a9764f81058fc7045619621f1377a242e",
        "title": "Unlocking the Future of Drug Development: Generative AI, Digital Twins, and Beyond"
      },
      {
        "paperId": "78e93e93f43629d0fad7ec95fb52570f000a297e",
        "title": "Experiments with data-augmented modeling of ADME and Potency endpoints in the ASAP-Polaris-OpenADMET Antiviral Challenge"
      }
    ],
    "score": 7.0
  },
  {
    "id": "3da8d2e6a0f8d5d62cbaae18235235483144c6a0",
    "title": "Toward Optimized In\u2010Memory Reinforcement Learning: Leveraging 1/f Noise of Synaptic Ferroelectric Field\u2010Effect\u2010Transistors for Efficient Exploration",
    "authors": [
      "Jangsaeng Kim",
      "Wonjun Shin",
      "Jiyong Yim",
      "Dongseok Kwon",
      "Dae-Hun Kwon",
      "Jong\u2010Ho Lee"
    ],
    "year": 2024,
    "citationCount": 6,
    "abstract": "Reinforcement learning (RL), exhibiting outstanding performance in various fields, requires large amounts of data for high performance. While exploration techniques address this requirement, conventional exploration methods have limitations: complexity of hardware implementation and significant hardware burden. Herein, in\u2010memory RL systems leveraging intrinsic 1/f noise of synaptic ferroelectric field\u2010effect\u2010transistors (FeFETs) for efficient exploration are proposed. The electrical characteristics of fabricated FeFETs with low\u2010power operation capability verify their suitability for neuromorphic systems. The proposed system achieves comparable performance to the conventional exploration method without additional circuits. The intrinsic 1/f noise of the FeFETs facilitates efficient exploration and offers significant advantages: efficiency in hardware implementation and simplicity in adjusting the 1/f noise level for optimal performance. This approach effectively addresses the challenges of conventional exploration methods. The operation mechanism of the exploration method utilizing the 1/f noise is systematically analyzed. The proposed in\u2010memory RL system demonstrates robustness and reliability to the device\u2010to\u2010device variation and the initial conductance distribution. This work provides further insights into the exploration methods of RL, paving the way for advanced in\u2010memory RL systems.",
    "url": "https://www.semanticscholar.org/paper/3da8d2e6a0f8d5d62cbaae18235235483144c6a0",
    "pdf_url": "https://doi.org/10.1002/aisy.202300763",
    "venue": "Advanced Intelligent Systems",
    "publicationDate": "2024-03-19",
    "externalIds": {
      "DBLP": "journals/aisy/KimSYKKL24",
      "DOI": "10.1002/aisy.202300763",
      "CorpusId": 268594007
    },
    "references": [
      {
        "paperId": "945b96b48bb297519eaa3f098ad2507a49d96888",
        "title": "Self\u2010Curable Synaptic Ferroelectric FET Arrays for Neuromorphic Convolutional Neural Network"
      },
      {
        "paperId": "e7c7f954baaeae3ba9219bb153cc0c7774731491",
        "title": "Analog synaptic devices applied to spiking neural networks for reinforcement learning applications"
      },
      {
        "paperId": "da1e404ae194dcbcce198f5335f49f090403bbd7",
        "title": "Exploration in Deep Reinforcement Learning: A Survey"
      },
      {
        "paperId": "4bcade19ea72f280553b1b7d3614f556b5753ace",
        "title": "Device Variation Effects on Neural Network Inference Accuracy in Analog In\u2010Memory Computing Systems"
      },
      {
        "paperId": "e27e67d595322cfdb8cda9e423eae431c721b727",
        "title": "1/f noise in amorphous Sb2Te3 for energy-efficient stochastic synapses in neuromorphic computing"
      },
      {
        "paperId": "000127bd7a1d836c964bda3a2c52f067353146aa",
        "title": "On-chip trainable hardware-based deep Q-networks approximating a backpropagation algorithm"
      },
      {
        "paperId": "641b98b64a55cc4dd8afd4c26c97958f8344c60e",
        "title": "In situ learning using intrinsic memristor variability via Markov chain Monte Carlo sampling"
      },
      {
        "paperId": "34cd70e96a7b5553a3e461bfe8180d97c77e5476",
        "title": "Hardware-based spiking neural network architecture using simplified backpropagation algorithm and homeostasis functionality"
      },
      {
        "paperId": "0c39fbcc11e707901649c5bfc985a1989867283e",
        "title": "NROWAN-DQN: A Stable Noisy Network with Noise Reduction and Online Weight Adjustment for Exploration"
      },
      {
        "paperId": "5d7ab1295e555a0828ac6139816465308c6b4b75",
        "title": "Device and Circuit Architectures for In\u2010Memory Computing"
      },
      {
        "paperId": "1eedee160d228a9c70824876356aa57d9415eeae",
        "title": "Initial synaptic weight distribution for fast learning speed and high recognition rate in STDP-based spiking neural network"
      },
      {
        "paperId": "a2ad56c2f67689e1398bb5ad860dae4c29b1ccba",
        "title": "Impact of Synaptic Device Variations on Classification Accuracy in a Binarized Neural Network"
      },
      {
        "paperId": "6b850c5f80cabf2796b77a7ebb9823232b80d4f7",
        "title": "In-memory computing with resistive switching devices"
      },
      {
        "paperId": "7851d6f36a7863af4a302f2864a7de43b3dd3d5c",
        "title": "Signal and noise extraction from analog memory elements for neuromorphic computing"
      },
      {
        "paperId": "0d01d71892ea3d6cd838740fc38193ad89e64a86",
        "title": "Impact of Synaptic Device Variations on Pattern Recognition Accuracy in a Hardware Neural Network"
      },
      {
        "paperId": "915cc4b359863f256957485c8a60f2cceb78ab5f",
        "title": "Conversion of Continuous-Valued Deep Networks to Efficient Event-Driven Networks for Image Classification"
      },
      {
        "paperId": "cfad94900162ffcbc5a975e348a5cdccdc1e8b07",
        "title": "BP-STDP: Approximating Backpropagation using Spike Timing Dependent Plasticity"
      },
      {
        "paperId": "48e7f8e6d4514f4cca9eacd586f6485c31945b79",
        "title": "Softmax exploration strategies for multiobjective reinforcement learning"
      },
      {
        "paperId": "c27db32efa8137cbf654902f8f728f338e55cd1c",
        "title": "Mastering the game of Go without human knowledge"
      },
      {
        "paperId": "56c9abf82c1395196073d5fb0205cf7d6b99953c",
        "title": "Adaptive learning rule for hardware-based deep neural networks using electronic synapse devices"
      },
      {
        "paperId": "340f48901f72278f6bf78a04ee5b01df208cc508",
        "title": "Human-level control through deep reinforcement learning"
      },
      {
        "paperId": "915d7d845ff12aea9e1af99270cdd4a12715db7c",
        "title": "Synaptic electronics: materials, devices and applications"
      },
      {
        "paperId": "29f6f329d57169f93e354d58f8794fe2fef3a272",
        "title": "Low-Frequency Noise in Advanced MOS Devices"
      },
      {
        "paperId": null,
        "title": "Precup"
      },
      {
        "paperId": "eb7d83b726a746b1a9b2a33ac34263b3047b1a48",
        "title": "About the authors"
      }
    ],
    "cited_by": [
      {
        "paperId": "a4cd41e0e599df679b5cb98002f801106839e37d",
        "title": "Gaussian\u2010Sigmoid Reinforcement Transistors: Resolving Exploration\u2010Exploitation Trade\u2010Off Through Gate Voltage\u2010Controlled Activation Functions"
      },
      {
        "paperId": "0734d20dd26d9e3d9dd0e8eeadfc868cbcce2cf4",
        "title": "A New Back\u2010End\u2010Of\u2010Line Ferroelectric Field\u2010Effect Transistor Platform via Laser Processing"
      },
      {
        "paperId": "f552d2d4cf1a61761b8fa69227f0f58d39897a25",
        "title": "Analog reservoir computing via ferroelectric mixed phase boundary transistors"
      },
      {
        "paperId": "227c2361d8c4a38d29bfaf7c5aa5a22af900625a",
        "title": "All\u2010Ferroelectric Spiking Neural Networks via Morphotropic Phase Boundary Neurons"
      },
      {
        "paperId": "44e160908e5dad2c432b1507c225fdcae8d1aec4",
        "title": "Recent trends in neuromorphic systems for non-von Neumann in materia computing and cognitive functionalities"
      },
      {
        "paperId": "2d4a261f4b659936f8f7b7080c41e53823bfb1ba",
        "title": "Closed Loop Superparamagnetic Tunnel Junctions for Reliable True Randomness and Generative Artificial Intelligence."
      }
    ],
    "score": 6.0
  },
  {
    "id": "086a4d45a9deef5a10ee4febcd4c92c95a6305de",
    "title": "Improving Offline Reinforcement Learning With in-Sample Advantage Regularization for Robot Manipulation",
    "authors": [
      "Chengzhong Ma",
      "Deyu Yang",
      "Tianyu Wu",
      "Zeyang Liu",
      "Houxue Yang",
      "Xingyu Chen",
      "Xuguang Lan",
      "Nanning Zheng"
    ],
    "year": 2024,
    "citationCount": 6,
    "abstract": "Offline reinforcement learning (RL) aims to learn the possible policy from a fixed dataset without real-time interactions with the environment. By avoiding the risky exploration of the robot, this approach is expected to significantly improve the robot\u2019s learning efficiency and safety. However, due to errors in value estimation from out-of-distribution actions, most offline RL algorithms constrain or regularize the policy to the actions contained within the dataset. The cost of such methods is the introduction of new hyperparameters and additional complexity. In this article, we aim to adapt offline RL to robotic manipulation with minimal changes and to avoid evaluating out-of-distribution actions as much as possible. Therefore, we improve offline RL with in-sample advantage regularization (ISAR). To mitigate the impact of unseen actions, the ISAR learns the state-value function only with the dataset sample to regress the optimal action-value function. Our method calculates the advantage function of action-state pairs based on in-sample value estimation and adds a behavior cloning (BC) regularization term in the policy update. This improves sample efficiency with minimal changes, resulting in a simple and easy-to-implement method. The experiments of the D4RL robot benchmark and multigoal sparse rewards robotic tasks show that the ISAR achieves excellent performance comparable to current state-of-the-art algorithms without the need for complex parameter tuning and too much training time. In addition, we demonstrate the effectiveness of our method on a real-world robot platform.",
    "url": "https://www.semanticscholar.org/paper/086a4d45a9deef5a10ee4febcd4c92c95a6305de",
    "pdf_url": "https://doi.org/10.1109/TNNLS.2024.3443102",
    "venue": "IEEE Transactions on Neural Networks and Learning Systems",
    "publicationDate": "2024-09-20",
    "externalIds": {
      "DBLP": "journals/tnn/MaYWLYCLZ25",
      "DOI": "10.1109/TNNLS.2024.3443102",
      "CorpusId": 272763708,
      "PubMed": "39302799"
    },
    "references": [],
    "cited_by": [
      {
        "paperId": "7c5881743648a7dfa8ccbfb460ee8a27f358e473",
        "title": "Analytic Energy-Guided Policy Optimization for Offline Reinforcement Learning"
      },
      {
        "paperId": "c57a18f934b0a9f96b9adfd112fe7ca442198db5",
        "title": "MInCo: Mitigating Information Conflicts in Distracted Visual Model-based Reinforcement Learning"
      },
      {
        "paperId": "be7e9f75f2d4bfb9e80e9be965f1145843100325",
        "title": "Model-Based Offline Reinforcement Learning for AUV Path-Following Under Unknown Ocean Currents with Limited Data"
      },
      {
        "paperId": "ad593c01cf1961ca72aa1787557cfc22aad5695b",
        "title": "Learning on One Mode: Addressing Multi-Modality in Offline Reinforcement Learning"
      },
      {
        "paperId": "c89735e59d2c1a982a48b018b8191a20d93f8742",
        "title": "ISFORS-MIX: Multi-agent reinforcement learning with Importance-Sampling-Free Off-policy learning and Regularized-Softmax Mixing network"
      },
      {
        "paperId": "74dd55d1b9a83dbd555aadfe10dfd4cefa5a11fa",
        "title": "Reinforcement Learning With Sparse-Executing Action via Sparsity Regularization"
      }
    ],
    "score": 6.0
  },
  {
    "id": "3dee83a4b0fadde414e00ff350940303eb859be1",
    "title": "An Improved Distributed Maximum Power Point Tracking Technique in Photovoltaic Systems Based on Reinforcement Learning Algorithm",
    "authors": [
      "Zhihong Ge",
      "Xingshuo Li",
      "Fei Xu",
      "Haimeng Wu",
      "Ruichi Wang",
      "Shuye Ding"
    ],
    "year": 2024,
    "citationCount": 6,
    "abstract": "The mismatch problem is commonly happened in photovoltaic systems due to partial shading conditions. Distributed maximum power point tracking architectures can be used to solve such problem. Reinforcement learning (RL) method, which is one of the advanced artificial intelligence methods is proposed to improve the tracking speed. However, the drawbacks, such as the lack of limited adaptability and exploration\u2013exploitation tradeoff theory, make the RL method low in efficiency. Therefore, this article combines the Beta method and $\\varepsilon$\u2013greedy algorithm with the RL method to address this problem. The simulation and experimental tests have been carried out and the result shows the efficiency of the proposed RL method is up to 96.85%, which verifies the superiority of the proposed scheme.",
    "url": "https://www.semanticscholar.org/paper/3dee83a4b0fadde414e00ff350940303eb859be1",
    "pdf_url": "https://doi.org/10.1109/JESTIE.2023.3332572",
    "venue": "IEEE Journal of Emerging and Selected Topics in Industrial Electronics",
    "publicationDate": "2024-01-01",
    "externalIds": {
      "DOI": "10.1109/JESTIE.2023.3332572",
      "CorpusId": 265189137
    },
    "references": [],
    "cited_by": [
      {
        "paperId": "8f0e1b7fca863cbea38d1a30f486ffc46d64d757",
        "title": "Investigation on Disturbance Frequency in Photovoltaic Systems for the FPPT Strategy under Different Topologies"
      },
      {
        "paperId": "e710de7bf3d98ad77cda0d9ae29ef740cd3319b8",
        "title": "Alternatives for Connecting Photovoltaic Generators to Power Systems with Three-Port and Partial Power Converters"
      },
      {
        "paperId": "ce2be34680bb1be3b26f63d732e525663ee76cf6",
        "title": "A Jaya Algorithm Approach to DMPPT for Building- Integrated Photovoltaic System Application"
      },
      {
        "paperId": "aa8f7551a461750d10ee73e6699bf377f1286d28",
        "title": "Maximum Power Point Tracking (MPPT) Techniques Using Cubic Spline Interpolation \u2013 Perturb and Observe (P&O) for Partial Shading Conditions"
      },
      {
        "paperId": "50bdf830867dc9f146bd58345a2f9988aeaa4bbf",
        "title": "An investigation of Flexible Power Point Tracking with nonlinear controller in photovoltaic system"
      },
      {
        "paperId": "84b1f6722a5dd8efc08d9733fea84b844c0b427d",
        "title": "Artificial Intelligence in the Hierarchical Control of AC, DC, and Hybrid AC/DC Microgrids: A Review"
      }
    ],
    "score": 6.0
  },
  {
    "id": "22c1ec46a81e9db6194b8784f4fe431f71953757",
    "title": "Adaptive 3D UI Placement in Mixed Reality Using Deep Reinforcement Learning",
    "authors": [
      "Feiyu Lu",
      "Mengyu Chen",
      "Hsiang Hsu",
      "Pranav Deshpande",
      "Cheng Yao Wang",
      "Blair MacIntyre"
    ],
    "year": 2024,
    "citationCount": 6,
    "abstract": "Mixed Reality (MR) could assist users\u2019 tasks by continuously integrating virtual content with their view of the physical environment. However, where and how to place these content to best support the users has been a challenging problem due to the dynamic nature of MR experiences. In contrast to prior work that investigates optimization-based methods, we are exploring how reinforcement learning (RL) could assist with continuous 3D content placement that is aware of users\u2019 poses and their surrounding environments. Through an initial exploration and preliminary evaluation, our results demonstrate the potential of RL to position content that maximizes the reward for users on the go. We further identify future directions for research that could harness the power of RL for personalized and optimized UI and content placement in MR.",
    "url": "https://www.semanticscholar.org/paper/22c1ec46a81e9db6194b8784f4fe431f71953757",
    "pdf_url": "https://arxiv.org/pdf/2504.21731.pdf",
    "venue": "CHI Extended Abstracts",
    "publicationDate": "2024-05-11",
    "externalIds": {
      "DBLP": "journals/corr/abs-2504-21731",
      "ArXiv": "2504.21731",
      "DOI": "10.1145/3613905.3651059",
      "CorpusId": 269743225
    },
    "references": [
      {
        "paperId": "0d033ccfb83ffc3ff1f5bae0b8de4f2a8c281053",
        "title": "BlendMR: A Computational Method to Create Ambient Mixed Reality Interfaces"
      },
      {
        "paperId": "8ad630d5b13c9737656cb75cc02b1490de6d6079",
        "title": "Towards Flexible and Robust User Interface Adaptations With Multiple Objectives"
      },
      {
        "paperId": "069fc3f807f4905fa5baa1061e7296a0872f6b03",
        "title": "InteractionAdapt: Interaction-driven Workspace Adaptation for Situated Virtual Reality Environments"
      },
      {
        "paperId": "ec1ebd4e41d458c4d80afa38fbf18c31b50a3c42",
        "title": "GREIL-Crowds: Crowd Simulation with Deep Reinforcement Learning and Examples"
      },
      {
        "paperId": "c5efd10dd4d7f62b440a55bf0a6e5324ae44cd5f",
        "title": "Pareto Optimal Layouts for Adaptive Mixed Reality"
      },
      {
        "paperId": "502462527bd0443f7848f4f16d198a801bb49eec",
        "title": "WordGesture-GAN: Modeling Word-Gesture Movement with Generative Adversarial Network"
      },
      {
        "paperId": "18d750263f1dfe43374e8791cefa580a511c2098",
        "title": "Few-Shot Preference Learning for Human-in-the-Loop RL"
      },
      {
        "paperId": "a174869f2f2fa256df2974fc29f0468e1ed37efe",
        "title": "AUIT \u2013 the Adaptive User Interfaces Toolkit for Designing XR Applications"
      },
      {
        "paperId": "b9b810c6cac27ae27298a50351002bddf88d137d",
        "title": "Optimizing the Timing of Intelligent Suggestion in Virtual Reality"
      },
      {
        "paperId": "6f9d9b017705c375088337481ac26b867eed1f42",
        "title": "The future of mixed reality is adaptive"
      },
      {
        "paperId": "04cf21bf3f6ec114a2d0d6e8a905c5469c180c90",
        "title": "Exploring Spatial UI Transition Mechanisms with Head-Worn Augmented Reality"
      },
      {
        "paperId": "be1525d95869eb4c95ec2f421da20269c9740a81",
        "title": "On the Use and Misuse of Absorbing States in Multi-agent Reinforcement Learning"
      },
      {
        "paperId": "f0d63621a0ed4a5c2f87a7afc174b0cfbd2b3354",
        "title": "SemanticAdapt: Optimization-based Adaptation of Mixed Reality Layouts Leveraging Virtual-Physical Semantic Connections"
      },
      {
        "paperId": "b25ffb7188bb28f57d00a04a96ff342eae1dea99",
        "title": "A Predictive Performance Model for Immersive Interactions in Mixed Reality"
      },
      {
        "paperId": "e6bf7e809e51a5092105413db9c80d5e4911294a",
        "title": "XRgonomics: Facilitating the Creation of Ergonomic 3D Interfaces"
      },
      {
        "paperId": "223d97f0fe491c66508369b7f096b36344c2e698",
        "title": "Adapting User Interfaces with Model-based Reinforcement Learning"
      },
      {
        "paperId": "222e965c6c644d0fda1b176f4e03aa4c4681f69e",
        "title": "Context-Responsive Labeling in Augmented Reality"
      },
      {
        "paperId": "929bf1a2ff229d34f7907886989c621444c2b8fd",
        "title": "Chip Placement with Deep Reinforcement Learning"
      },
      {
        "paperId": "5d5567bff7b7277268c679a8593301346f223d81",
        "title": "Combinatorial Optimization of Graphical User Interface Designs"
      },
      {
        "paperId": "978c06878c5381caefc6e9dec2c49239ae7f8bd4",
        "title": "GRIDS: Interactive Layout Design with Integer Programming"
      },
      {
        "paperId": "b19729b27a1b4c24b52f87308c907653300afa7f",
        "title": "Dota 2 with Large Scale Deep Reinforcement Learning"
      },
      {
        "paperId": "f67f5c24ab539395edadf7e6fd509a8693e8698f",
        "title": "Context-Aware Online Adaptation of Mixed Reality Interfaces"
      },
      {
        "paperId": "1d4484c14344e5bf197b49e1f17fe696d2d4a79d",
        "title": "Learning Cooperative Personalized Policies from Gaze Data"
      },
      {
        "paperId": "60f511eef446120fe59563ee976f948d5e3f9064",
        "title": "The Replica Dataset: A Digital Replica of Indoor Spaces"
      },
      {
        "paperId": "fc2330cc2087a9f5111372443b1138826625b895",
        "title": "Automation Accuracy Is Good, but High Controllability May Be Better"
      },
      {
        "paperId": "1fd4694e7c2d9c872a427d50e81b5475056de6bc",
        "title": "Model-Based Reinforcement Learning for Atari"
      },
      {
        "paperId": "090e8ba000d6870d1ce365b4441a96eff3aa2479",
        "title": "External Labeling Techniques: A Taxonomy and Survey"
      },
      {
        "paperId": "8443865a2264d753ae0bbdc1a3137f8f7c605f08",
        "title": "Crowd-Robot Interaction: Crowd-Aware Robot Navigation With Attention-Based Deep Reinforcement Learning"
      },
      {
        "paperId": "252482d733d67230843fe99bf60427285f0ad8e8",
        "title": "Unity: A General Platform for Intelligent Agents"
      },
      {
        "paperId": "030c5f02c959fabc6abaf0f961c23704bab9cd40",
        "title": "Driverless Car: Autonomous Driving Using Deep Reinforcement Learning in Urban Environment"
      },
      {
        "paperId": "0287b37354293bb214b2674fa51b59b91b6e289b",
        "title": "MRTouch: Adding Touch Input to Head-Mounted Mixed Reality"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd",
        "title": "Deep Reinforcement Learning from Human Preferences"
      },
      {
        "paperId": "7c3ece1ba41c415d7e81cfa5ca33a8de66efd434",
        "title": "Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments"
      },
      {
        "paperId": "37c07e4d5b96a89e2e819b4d4ba4777fd79ed938",
        "title": "Towards Pervasive Augmented Reality: Context-Awareness in Augmented Reality"
      },
      {
        "paperId": "3a45aec642610e6eea49f719783579d0399d401b",
        "title": "Sketchplore: Sketch and Explore with a Layout Optimiser"
      },
      {
        "paperId": "d6a86a0150d2a6012e3bc44b177919525d852235",
        "title": "Spatial Constancy of Surface-Embedded Layouts across Multiple Environments"
      },
      {
        "paperId": "2b8fe530cac203f44959efd75ec1a9c7ecfaa519",
        "title": "Halo Content: Context-aware Viewspace Management for Non-invasive Augmented Reality"
      },
      {
        "paperId": "c02d1643d3510d879e9664348ffd1764850c540b",
        "title": "Hedgehog labeling: View management techniques for external labels in 3D space"
      },
      {
        "paperId": "65438e0ba226c1f97bd8a36333ebc3297b1a32fd",
        "title": "Reinforcement learning in robotics: A survey"
      },
      {
        "paperId": "93ae34af130488aa426ca5877e31aff5ee66fa54",
        "title": "Evaluating label placement for augmented reality view management"
      },
      {
        "paperId": "b7885f3055752ad702c24b87ac82a55c156b32d5",
        "title": "Performance Optimization of Virtual Keyboards"
      },
      {
        "paperId": "11035becfa4b6d71dac0d647abf78d201bf05417",
        "title": "View management for virtual and augmented reality"
      },
      {
        "paperId": "6e3a343ed03047922e19028e88126121a3cdcf37",
        "title": "The Effects of Stimulus Dimensionality on the Rate of Gain of Information"
      },
      {
        "paperId": "634c9fde5f1c411e4487658ac738dcf18d98ea8d",
        "title": "The information capacity of the human motor system in controlling the amplitude of movement."
      },
      {
        "paperId": null,
        "title": "2023. RL-LABEL: A Deep Reinforcement Learning Approach Intended for AR Label Placement in Dynamic Scenarios"
      },
      {
        "paperId": null,
        "title": "CHI EA \u201924, May 11\u201316, 2024, Honolulu, HI, USA"
      }
    ],
    "cited_by": [
      {
        "paperId": "f2c601122f0a2e7a934b90e97dafce17a06fbdfb",
        "title": "A Contextual Bandits Approach for Personalization of Hand Gesture Recognition"
      },
      {
        "paperId": "9648373248a27deb5cda66930c4d4a5300e8c1a3",
        "title": "Long-Term Experiences From Working with Extended Reality in the Wild"
      },
      {
        "paperId": "4667d161790fae292755c1b44cf174bf10dd8cec",
        "title": "BinoForce: A Force-Based 3D Dynamic Label Layout Method Under Binocular Viewpoints"
      },
      {
        "paperId": "6273f76ab56fa1d19344f206342f31b21e61c4cd",
        "title": "Immersive VR User Interaction Design in MaxWhere XR"
      },
      {
        "paperId": "84e2ec35050196fd67a4af293b6bda6a121da781",
        "title": "Crowd Data-driven Artwork Placement in Virtual Exhibitions for Visitor Density Distribution Planning"
      },
      {
        "paperId": "cddb0a87cc085252cfedba35dac5f890aba9c43f",
        "title": "Adaptive Content Placement in Mixed Reality Through Empirical User Behavioral Patterns"
      }
    ],
    "score": 6.0
  },
  {
    "id": "9c7209f3d6b9bf362ea4d34730f34cf8511967f4",
    "title": "EasySO: Exploration-enhanced Reinforcement Learning for Logic Synthesis Sequence Optimization and a Comprehensive RL Environment",
    "authors": [
      "Jianyong Yuan",
      "Peiyu Wang",
      "Junjie Ye",
      "Mingxuan Yuan",
      "Jianye Hao",
      "Junchi Yan"
    ],
    "year": 2023,
    "citationCount": 11,
    "abstract": "Optimizing the quality of results (QoR) of a circuit during the logic synthesis (LS) phase in chip design is critical yet challenging. While most existing methods often mitigate the computational hardness by restricting the action space to a small set of operators and fixing the operator's parameters, they are susceptible to local minima and may not meet the high demand from industrial cases. In this paper, we develop a more comprehensive optimization approach via sample-efficient reinforcement learning (RL). Specifically, we first build a complete logic synthesis-RL environment, where the action space consists of three types of operators: logic optimization, technology mapping, and post-mapping, along with their associated continuouslbinary parameters for optimization as well. Based on this environment, we devise a hybrid proximal policy optimization (PPO) model to handle both discrete operators and parameters and design a distributed architecture to improve sample collection efficiency. Furthermore, we devise a dynamic exploration module to improve the exploration efficiency under the constraint of limited samples. We term our method as Exploration-enhanced RL for Logic Synthesis Sequence Optimization(EasySO). Results on the EPFL benchmark show that our method significantly outperforms current state-of-the-art models based on Bayesian optimization (BO) and the previous RL-based methods. Compared to resyn2, our EasySO achieves an average of 25.4% LUT-6 count optimization without sacrificing level values. Moreover, as of the time for this submission, we rank 26 first places among 40 optimization targets in the EPFL competition.",
    "url": "https://www.semanticscholar.org/paper/9c7209f3d6b9bf362ea4d34730f34cf8511967f4",
    "pdf_url": "https://doi.org/10.1109/ICCAD57390.2023.10323973",
    "venue": "2023 IEEE/ACM International Conference on Computer Aided Design (ICCAD)",
    "publicationDate": "2023-10-28",
    "externalIds": {
      "DBLP": "conf/iccad/YuanWYYHY23",
      "DOI": "10.1109/ICCAD57390.2023.10323973",
      "CorpusId": 265526066
    },
    "references": [],
    "cited_by": [
      {
        "paperId": "53e57bc9032772cbd0a015f360100f61aa79845e",
        "title": "MILS: Modality Interaction Driven Learning for Logic Synthesis"
      },
      {
        "paperId": "7599f3a95daa630596e522b876a0847823f357fc",
        "title": "Enhancing Delay-Driven LUT Mapping With Boolean Decomposition"
      },
      {
        "paperId": "0d8e2fee453561ca89fb5512ddc86e52e03d2b93",
        "title": "DeLoSo: Detecting Logic Synthesis Optimization Faults Based on Configuration Diversity"
      },
      {
        "paperId": "6058dbf7a8dc456832a49fa9fba0cfd84ffe5b29",
        "title": "ReLS: Retrieval Is Efficient Knowledge Transfer For Logic Synthesis"
      },
      {
        "paperId": "ab407fd9e13fde2f9d1a5da50aa113585e0c87b1",
        "title": "On Accelerating Domain-Specific MC-TS with Knowledge Retention and Efficient Parallelization for Logic Optimization"
      },
      {
        "paperId": "3d42e5b18fa939fc8e4de0ec6377c3b5bfd41333",
        "title": "Large circuit models: opportunities and challenges"
      },
      {
        "paperId": "2a171e5057ddd3abda678d2d99eb3514da9ae5fa",
        "title": "PreRoutGNN for Timing Prediction with Order Preserving Partition: Global Circuit Pre-training, Local Delay Learning and Attentional Cell Modeling"
      },
      {
        "paperId": "beadfd41698c0382a5899077a8346c05ca6fe513",
        "title": "LLM4EDA: Emerging Progress in Large Language Models for Electronic Design Automation"
      },
      {
        "paperId": "d26f3de10f2505bb8a52a629fec7a0ddefb1ef0d",
        "title": "ShortCircuit: AlphaZero-Driven Circuit Design"
      },
      {
        "paperId": "54f78ec43ae05b3d40001736bbf3b00fb3ce4478",
        "title": "Towards Next-Generation Logic Synthesis: A Scalable Neural Circuit Generation Framework"
      },
      {
        "paperId": "076a897e3a5af1d7877b30ddc63b2a3ea4961a67",
        "title": "HubRouter: Learning Global Routing via Hub Generation and Pin-hub Connection"
      }
    ],
    "score": 5.5
  },
  {
    "id": "200726cba07dec06a56ff46aa38836e9730a23a2",
    "title": "Rethinking Population-assisted Off-policy Reinforcement Learning",
    "authors": [
      "Bowen Zheng",
      "Ran Cheng"
    ],
    "year": 2023,
    "citationCount": 11,
    "abstract": "While off-policy reinforcement learning (RL) algorithms are sample efficient due to gradient-based updates and data reuse in the replay buffer, they struggle with convergence to local optima due to limited exploration. On the other hand, population-based algorithms offer a natural exploration strategy, but their heuristic black-box operators are inefficient. Recent algorithms have integrated these two methods, connecting them through a shared replay buffer. However, the effect of using diverse data from population optimization iterations on off-policy RL algorithms has not been thoroughly investigated. In this paper, we first analyze the use of off-policy RL algorithms in combination with population-based algorithms, showing that the use of population data could introduce an overlooked error and harm performance. To test this, we propose a uniform and scalable training design and conduct experiments on our tailored framework in robot locomotion tasks from the OpenAI gym. Our results substantiate that using population data in off-policy RL can cause instability during training and even degrade performance. To remedy this issue, we further propose a double replay buffer design that provides more on-policy data and show its effectiveness through experiments. Our results offer practical insights for training these hybrid methods.",
    "url": "https://www.semanticscholar.org/paper/200726cba07dec06a56ff46aa38836e9730a23a2",
    "pdf_url": "https://arxiv.org/pdf/2305.02949.pdf",
    "venue": "Annual Conference on Genetic and Evolutionary Computation",
    "publicationDate": "2023-05-04",
    "externalIds": {
      "DBLP": "conf/gecco/ZhengC23",
      "ArXiv": "2305.02949",
      "DOI": "10.1145/3583131.3590512",
      "CorpusId": 258480212
    },
    "references": [
      {
        "paperId": "4bb9f478cb464049100abafc77b6491b6e5b731a",
        "title": "Evolutionary Reinforcement Learning: A Survey"
      },
      {
        "paperId": "401e43955af440bc2e8d1641f71aaf59249a524a",
        "title": "EvoX: A Distributed GPU-Accelerated Framework for Scalable Evolutionary Computation"
      },
      {
        "paperId": "1f1b01cb09e557e6a1e6b0292cb84a67839c2e1a",
        "title": "Lamarckian Platform: Pushing the Boundaries of Evolutionary Reinforcement Learning Toward Asynchronous Commercial Games"
      },
      {
        "paperId": "d028f2087cffc9ced5db649bfdf02878eb790fb1",
        "title": "Combining Soft-Actor Critic with Cross-Entropy Method for Policy Search in Continuous Control"
      },
      {
        "paperId": "b5955c31e6c748ce301a2902044c4f364f9d0c84",
        "title": "EvoJAX: hardware-accelerated neuroevolution"
      },
      {
        "paperId": "fc83e3bb81ef461f9848fae00885dbea66e76fbc",
        "title": "Policy gradient assisted MAP-Elites"
      },
      {
        "paperId": "b0c40766974df3eae8ff500379e66e5566cd16c9",
        "title": "An Efficient Asynchronous Method for Integrating Evolutionary and Gradient-based Policy Search"
      },
      {
        "paperId": "09d01a5ce6befad6e08c8b6185043ae0df3e78ae",
        "title": "Experience Replay with Likelihood-free Importance Weights"
      },
      {
        "paperId": "0dbf2d2c05e1955fbcf3461def336224cc68ce1f",
        "title": "An Evolution Strategy with Progressive Episode Lengths for Playing Games"
      },
      {
        "paperId": "6557f9e7832dd127ab3ea2bcd0d1a6b924d4efc2",
        "title": "Trust Region Evolution Strategies"
      },
      {
        "paperId": "50ba129bb69e4560d57c412e85d14bb43555abf4",
        "title": "Proximal Distilled Evolutionary Reinforcement Learning"
      },
      {
        "paperId": "4342598da04242cb0f8e2307d47ca50909ae0717",
        "title": "Exploiting the Sign of the Advantage Function to Learn Deterministic Policies in Continuous Domains"
      },
      {
        "paperId": "98c08d28a2319ce79b62b108dfed3acc0dd80983",
        "title": "Collaborative Evolutionary Reinforcement Learning"
      },
      {
        "paperId": "d9e08e872c69e5acfed2a93ec6f6d623cfd0c680",
        "title": "CEM-RL: Combining evolutionary and gradient-based methods for policy search"
      },
      {
        "paperId": "d5805a80b63ed0a605e5469e321a7e3c42eaf324",
        "title": "Evolution-Guided Policy Gradient in Reinforcement Learning"
      },
      {
        "paperId": "7491c03c9b3e798a5243c10f47e7da88570a0297",
        "title": "Smoothed Action Value Functions for Learning Gaussian Policies"
      },
      {
        "paperId": "4debb99c0c63bfaa97dd433bc2828e4dac81c48b",
        "title": "Addressing Function Approximation Error in Actor-Critic Methods"
      },
      {
        "paperId": "19af387c0ee32929b54ff45656129a721a84c192",
        "title": "Back to Basics: Benchmarking Canonical Evolution Strategies for Playing Atari"
      },
      {
        "paperId": "811df72e210e20de99719539505da54762a11c6d",
        "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor"
      },
      {
        "paperId": "38fb1902c6a2ab4f767d4532b28a92473ea737aa",
        "title": "Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm"
      },
      {
        "paperId": "1fdf0319b4a1db62c611980351e5f4c2f08958cd",
        "title": "GEP-PG: Decoupling Exploration and Exploitation in Deep Reinforcement Learning Algorithms"
      },
      {
        "paperId": "142497432fe179ddb6ffe600c64a837ec6179550",
        "title": "Parameter Space Noise for Exploration"
      },
      {
        "paperId": "4ee802a58d32aa049d549d06be440ac947b53987",
        "title": "Evolution Strategies as a Scalable Alternative to Reinforcement Learning"
      },
      {
        "paperId": "846aedd869a00c09b40f1f1f35673cb22bc87490",
        "title": "Mastering the game of Go with deep neural networks and tree search"
      },
      {
        "paperId": "024006d4c2a89f7acacc6e4438d156525b60a98f",
        "title": "Continuous control with deep reinforcement learning"
      },
      {
        "paperId": "340f48901f72278f6bf78a04ee5b01df208cc508",
        "title": "Human-level control through deep reinforcement learning"
      },
      {
        "paperId": "687d0e59d5c35f022ce4638b3e3a6142068efc94",
        "title": "Deterministic Policy Gradient Algorithms"
      },
      {
        "paperId": "2319a491378867c7049b3da055c5df60e1671158",
        "title": "Playing Atari with Deep Reinforcement Learning"
      },
      {
        "paperId": "b354ee518bfc1ac0d8ac447eece9edb69e92eae1",
        "title": "MuJoCo: A physics engine for model-based control"
      },
      {
        "paperId": "f01f81147e5e65b5596907b7b1764b0758e3b02e",
        "title": "Convergence Properties of (\u03bc + \u03bb) Evolutionary Algorithms"
      },
      {
        "paperId": "d6514a485cd08c5d9f4dc68d17fc3171d96e78d2",
        "title": "Efficient Reinforcement Learning Through Evolving Neural Network Topologies"
      },
      {
        "paperId": "a20f0ce0616def7cc9a87446c228906cd5da093b",
        "title": "Policy Gradient Methods for Reinforcement Learning with Function Approximation"
      },
      {
        "paperId": "03b7e51c52084ac1db5118342a00b5fbcfc587aa",
        "title": "Q-learning"
      },
      {
        "paperId": "da134ad1be3087a6a57ab4435705132378f73c9b",
        "title": "Guiding Evolutionary Strategies with Off-Policy Actor-Critic"
      },
      {
        "paperId": "f667c3a0c904db9d55c225c0411833abbc38a561",
        "title": "Genetic Soft Updates for Policy Evolution in Deep Reinforcement Learning"
      },
      {
        "paperId": null,
        "title": "United Kingdom) (AAMAS '21). International Foundation for Autonomous Agents and Multiagent Systems"
      },
      {
        "paperId": "4c915c1eecb217c123a36dc6d3ce52d12c742614",
        "title": "Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning"
      },
      {
        "paperId": null,
        "title": "GECCO \u201923, July 15\u201319, 2023, Lisbon, Portugal France) (GECCO \u201921)"
      }
    ],
    "cited_by": [
      {
        "paperId": "34b3cad03ec8d2ff83421dd0261a2ef2a24e2ea4",
        "title": "Learngene: Inheritable \u201cgenes\u201d in intelligent agents"
      },
      {
        "paperId": "5fb99fd13a583f56ea37d283845fb49573bbb4d3",
        "title": "Scaling DRL for Decision Making: A Survey on Data, Network, and Training Budget Strategies"
      },
      {
        "paperId": "348c2536c6fa2aab6d7788a7138e59ce4b9e285c",
        "title": "Evolutionary Policy Optimization"
      },
      {
        "paperId": "c6b2296dc9403790ba211ab639804cadf62ae38f",
        "title": "Evolutionary Reinforcement Learning: A Systematic Review and Future Directions"
      },
      {
        "paperId": "a4c719b829313784525c1a3f911fa12511956b2b",
        "title": "Wastewater treatment monitoring: Fault detection in sensors using transductive learning and improved reinforcement learning"
      },
      {
        "paperId": "bb47751c85d611de78c6b20742adacba80bc8723",
        "title": "Proximal evolutionary strategy: improving deep reinforcement learning through evolutionary policy optimization"
      },
      {
        "paperId": "316fd1658e4ae59cdbeaf5caa03f46d4d32d616d",
        "title": "Melanoma classification using generative adversarial network and proximal policy optimization"
      },
      {
        "paperId": "1c91310570d692f4cb912265dd7c8666436fad7f",
        "title": "Genetic Drift Regularization: On Preventing Actor Injection from Breaking Evolution Strategies"
      },
      {
        "paperId": "43d72582f68133ff7316f4fcbd6eeac41845f57a",
        "title": "Evolutionary Reinforcement Learning: A Systematic Review and Future Directions"
      },
      {
        "paperId": "25523c3e70daedcd0f1172844fcbafe30b8933f1",
        "title": "Bridging Evolutionary Algorithms and Reinforcement Learning: A Comprehensive Survey"
      },
      {
        "paperId": "37cd88994220ae72fc93de1b0ea0383487c624ed",
        "title": "Understanding the Role of Population Experiences in Proximal Distilled Evolutionary Reinforcement Learning"
      }
    ],
    "score": 5.5
  },
  {
    "id": "3c483c11f5fd9234576a93aea71a7ec1435b8514",
    "title": "FlagVNE: A Flexible and Generalizable Reinforcement Learning Framework for Network Resource Allocation",
    "authors": [
      "Tianfu Wang",
      "Qilin Fan",
      "Chao Wang",
      "Long Yang",
      "Leilei Ding",
      "Nicholas Jing Yuan",
      "Hui Xiong"
    ],
    "year": 2024,
    "citationCount": 5,
    "abstract": "Virtual network embedding (VNE) is an essential resource allocation task in network virtualization, aiming to map virtual network requests (VNRs) onto physical infrastructure. Reinforcement learning (RL) has recently emerged as a promising solution to this problem. However, existing RL-based VNE methods are limited by the unidirectional action design and one-size-fits-all training strategy, resulting in restricted searchability and generalizability. In this paper, we propose a flexible and generalizable RL framework for VNE, named FlagVNE. Specifically, we design a bidirectional action-based Markov decision process model that enables the joint selection of virtual and physical nodes, thus improving the exploration flexibility of solution space. To tackle the expansive and dynamic action space, we design a hierarchical decoder to generate adaptive action probability distributions and ensure high training efficiency. Furthermore, to overcome the generalization issue for varying VNR sizes, we propose a meta-RL-based training method with a curriculum scheduling strategy, facilitating specialized policy training for each VNR size. Finally, extensive experimental results show the effectiveness of FlagVNE across multiple key metrics. Our code is available at https://github.com/GeminiLight/flag-vne.",
    "url": "https://www.semanticscholar.org/paper/3c483c11f5fd9234576a93aea71a7ec1435b8514",
    "pdf_url": "https://arxiv.org/pdf/2404.12633.pdf",
    "venue": "International Joint Conference on Artificial Intelligence",
    "publicationDate": "2024-04-19",
    "externalIds": {
      "DBLP": "journals/corr/abs-2404-12633",
      "ArXiv": "2404.12633",
      "DOI": "10.48550/arXiv.2404.12633",
      "CorpusId": 269283075
    },
    "references": [
      {
        "paperId": "60c477570fa1b2c61a7326dbf173e6178f7e4fcc",
        "title": "SetRank: A Setwise Bayesian Approach for Collaborative Ranking in Recommender System"
      },
      {
        "paperId": "a3f296c1d41bf0c1e1978ccb13c7b1d8951713dc",
        "title": "GAL-VNE: Solving the VNE Problem with Global Reinforcement Learning and Local One-Shot Neural Prediction"
      },
      {
        "paperId": "d3d2c3671732300d2ea18583fd029250125defa0",
        "title": "Dapper: Deploying Service Function Chains in the Programmable Data Plane Via Deep Reinforcement Learning"
      },
      {
        "paperId": "240fd6b32c48dcca9d847cdf9f9930b3e717709e",
        "title": "Learning Profitable NFT Image Diffusions via Multiple Visual-Policy Guided Reinforcement Learning"
      },
      {
        "paperId": "fe148a4498dafb494257fc22867ab48e2cadf3db",
        "title": "Towards Omni-generalizable Neural Methods for Vehicle Routing Problems"
      },
      {
        "paperId": "f95356978b8f2975d6ce1726ae92677eba03907d",
        "title": "Node Essentiality Assessment and Distributed Collaborative Virtual Network Embedding in Datacenters"
      },
      {
        "paperId": "4aac8f0a423d5bb8a3cc0a8407c37ea7715e2343",
        "title": "Leveraging Deep Reinforcement Learning With Attention Mechanism for Virtual Network Function Placement and Routing"
      },
      {
        "paperId": "cf291f8752e9f29bd9cfef0b37edb6bbbc73d712",
        "title": "Impact-aware Maneuver Decision with Enhanced Perception for Autonomous Vehicle"
      },
      {
        "paperId": "9f477a2cf525c59c505014fdd97c43a5f72cad9d",
        "title": "Reinforcement Learning Assisted Bandwidth Aware Virtual Network Resource Allocation"
      },
      {
        "paperId": "3ffa359555ab15e7e2c379daec73fbcf5bafa458",
        "title": "Constrained Update Projection Approach to Safe Policy Optimization"
      },
      {
        "paperId": "1dcce96d5d0c32e9b7b8dbee831d6210170943a3",
        "title": "Efficient Join Order Selection Learning with Graph-based Representation"
      },
      {
        "paperId": "1c29ae78cbaf67f8c3a9132ba97ebc771a176cd1",
        "title": "Multi-modal Siamese Network for Entity Alignment"
      },
      {
        "paperId": "e7b905f73bb9508702fb93ec8620443d5b6c46e5",
        "title": "Entity Summarization via Exploiting Description Complementarity and Salience"
      },
      {
        "paperId": "422cfa0b5a2c760e03e57d71f5cace06d3768f0e",
        "title": "Personalized and Explainable Employee Training Course Recommendations: A Bayesian Variational Approach"
      },
      {
        "paperId": "b55d04dd2dcc3818419bfb837089db303d7507ab",
        "title": "DRL-SFCP: Adaptive Service Function Chains Placement with Deep Reinforcement Learning"
      },
      {
        "paperId": "65a7bba01b91626ab7753af3c302fe36c3da7570",
        "title": "Variable Interval Time Sequence Modeling for Career Trajectory Prediction: Deep Collaborative Perspective"
      },
      {
        "paperId": "55635aac4cd439a00356f83dad52bd8d7b0ea87e",
        "title": "A Survey on Curriculum Learning"
      },
      {
        "paperId": "6d9e9444ae6c089a268bfc61dece15b091d26db8",
        "title": "Recent Advances of Resource Allocation in Network Function Virtualization"
      },
      {
        "paperId": "ca3072dad2ee809c8fc3639e6fc1728b46f9ef66",
        "title": "MMEA: Entity Alignment for Multi-modal Knowledge Graph"
      },
      {
        "paperId": "c55cc603b74b8ba0cc58dbaa1d8df1af94ab934b",
        "title": "learn2learn: A Library for Meta-Learning Research"
      },
      {
        "paperId": "443e7600e63952f383de68d8de52edb5736a051d",
        "title": "Latency-aware VNF Chain Deployment with Efficient Resource Reuse at Network Edge"
      },
      {
        "paperId": "7d45afc1f12cba92a66da4e83a96380ac0a15997",
        "title": "Learning the travelling salesperson problem requires rethinking generalization"
      },
      {
        "paperId": "18b619339f0abfdae1edb19abb11f43ca4f90842",
        "title": "Automatic Virtual Network Embedding: A Deep Reinforcement Learning Approach With Graph Convolutional Networks"
      },
      {
        "paperId": "b808a9382aaf28560d770e9a6a5c4fa0fa7b7e34",
        "title": "On the Hardness and Inapproximability of Virtual Network Embeddings"
      },
      {
        "paperId": "701589b4e1b76c2df1c0e3c5aec4302c54a08196",
        "title": "A Continuous-Decision Virtual Network Embedding Scheme Relying on Reinforcement Learning"
      },
      {
        "paperId": "af6e89f7268583724e55031024e1f481e4865b4e",
        "title": "SDN/NFV-Empowered Future IoV With Enhanced Communication, Computing, and Caching"
      },
      {
        "paperId": "3c8a456509e6c0805354bd40a35e3f2dbf8069b1",
        "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library"
      },
      {
        "paperId": "bfe94756ba229c8718c2175821d08a60c6da5b02",
        "title": "Policy Optimization with Stochastic Mirror Descent"
      },
      {
        "paperId": "4fde54f694cb8203972404058d2eabb5931d598b",
        "title": "NFVdeep: Adaptive Online Service Function Chain Deployment with Deep Reinforcement Learning"
      },
      {
        "paperId": "0754f08c42d731206ec99434ffbd627ee1d0a0ac",
        "title": "DYVINE: Fitness-Based Dynamic Virtual Network Embedding in Cloud Computing"
      },
      {
        "paperId": "14ffe99a7d2888f69238e3e2ee7b85d6d76fe7a8",
        "title": "Virtual Network Embedding Based on Computing, Network, and Storage Resource Constraints"
      },
      {
        "paperId": "982adce0bc909f0ce130a6c69767d68f28a08d59",
        "title": "NeuroViNE: A Neural Preprocessor for Your Virtual Network Embedding Algorithm"
      },
      {
        "paperId": "314b4716c126a0301182a144713d074285d00193",
        "title": "Virtual Network Survivability Through Joint Spare Capacity Allocation and Embedding"
      },
      {
        "paperId": "712075c3cdcbbd550dca7e73ded994d200c43872",
        "title": "Virtual Network Embedding via Monte Carlo Tree Search"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "c889d6f98e6d79b89c3a6adf8a921f88fa6ba518",
        "title": "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks"
      },
      {
        "paperId": "36eff562f65125511b5dfab68ce7f7a943c27478",
        "title": "Semi-Supervised Classification with Graph Convolutional Networks"
      },
      {
        "paperId": "69e76e16740ed69f4dc55361a3d319ac2f1293dd",
        "title": "Asynchronous Methods for Deep Reinforcement Learning"
      },
      {
        "paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8",
        "title": "Adam: A Method for Stochastic Optimization"
      },
      {
        "paperId": "5e03df0eddbc92e7a0b194400ab10e31db20475d",
        "title": "Energy-Aware Virtual Network Embedding"
      },
      {
        "paperId": "f3a21c0ccdd53b9b9cca3205889b3d45a085888a",
        "title": "Virtual Network Embedding: A Survey"
      },
      {
        "paperId": "a20f0ce0616def7cc9a87446c228906cd5da093b",
        "title": "Policy Gradient Methods for Reinforcement Learning with Function Approximation"
      },
      {
        "paperId": "9b2b8b5dedb6c48853ee5199e528096f212afd44",
        "title": "Routing of multipoint connections"
      },
      {
        "paperId": null,
        "title": "The 37 implementation details of proximal policy optimization"
      },
      {
        "paperId": "bbf062bd97edf6e2d0114eb5136c13eaa9d4efa6",
        "title": "VNE-HPSO: Virtual Network Embedding Algorithm Based on Hybrid Particle Swarm Optimization"
      },
      {
        "paperId": "97efafdb4a3942ab3efba53ded7413199f79c054",
        "title": "Reinforcement Learning: An Introduction"
      }
    ],
    "cited_by": [
      {
        "paperId": "e9731d1b2f8afac7dd606ae607af3ce6d2fa42ec",
        "title": "Virne: A Comprehensive Benchmark for Deep RL-based Network Resource Allocation in NFV"
      },
      {
        "paperId": "0a63088d146fcd218b243acaacf9de5ef9c19106",
        "title": "Efficient Resource Allocation in Computing Power Networks Considering Similar Task Merging: A Lyapunov Optimization-Based DRL Approach"
      },
      {
        "paperId": "2caaa765c35f5cb29999278ffa195ff0a7e9537e",
        "title": "Towards Expert Models Deployment Cost Optimization in Edge Computing Networks"
      },
      {
        "paperId": "a684caa642d3950b68d996ca373f0a92e4f61809",
        "title": "Towards Constraint-aware Learning for Resource Allocation in NFV Networks"
      },
      {
        "paperId": "251293b6a960fc2a9cd9127216a0a6c088896ab3",
        "title": "COMET: NFT Price Prediction with Wallet Profiling"
      }
    ],
    "score": 5.0
  },
  {
    "id": "d06737f9395e592f35ef251e09bea1c18037b096",
    "title": "Random Latent Exploration for Deep Reinforcement Learning",
    "authors": [
      "Srinath Mahankali",
      "Zhang-Wei Hong",
      "Ayush Sekhari",
      "Alexander Rakhlin",
      "Pulkit Agrawal"
    ],
    "year": 2024,
    "citationCount": 5,
    "abstract": "We introduce Random Latent Exploration (RLE), a simple yet effective exploration strategy in reinforcement learning (RL). On average, RLE outperforms noise-based methods, which perturb the agent's actions, and bonus-based exploration, which rewards the agent for attempting novel behaviors. The core idea of RLE is to encourage the agent to explore different parts of the environment by pursuing randomly sampled goals in a latent space. RLE is as simple as noise-based methods, as it avoids complex bonus calculations but retains the deep exploration benefits of bonus-based methods. Our experiments show that RLE improves performance on average in both discrete (e.g., Atari) and continuous control tasks (e.g., Isaac Gym), enhancing exploration while remaining a simple and general plug-in for existing RL algorithms. Project website and code: https://srinathm1359.github.io/random-latent-exploration",
    "url": "https://www.semanticscholar.org/paper/d06737f9395e592f35ef251e09bea1c18037b096",
    "pdf_url": "https://arxiv.org/pdf/2407.13755.pdf",
    "venue": "International Conference on Machine Learning",
    "publicationDate": "2024-07-18",
    "externalIds": {
      "DBLP": "journals/corr/abs-2407-13755",
      "ArXiv": "2407.13755",
      "DOI": "10.48550/arXiv.2407.13755",
      "CorpusId": 271270226
    },
    "references": [
      {
        "paperId": "f2f9cc38d5a1a4fdef996ea03fdb71e01f65d574",
        "title": "Making RL with Preference-based Feedback Efficient via Randomization"
      },
      {
        "paperId": "c9808b9785e51e8ff84abdf2e4d5cdd2adfc3202",
        "title": "When is Agnostic Reinforcement Learning Statistically Tractable?"
      },
      {
        "paperId": "37c0a8503522a22d21bb851fd80c0dbf9ee5a567",
        "title": "Breadcrumbs to the Goal: Goal-Conditioned Exploration from Human-in-the-Loop Feedback"
      },
      {
        "paperId": "c90aa0f206c6fd41c490c142f63f7ba046cae6b7",
        "title": "Provable and Practical: Efficient Exploration in Reinforcement Learning via Langevin Monte Carlo"
      },
      {
        "paperId": "2c2180fbe7f38e88b1123e5fab43785b66814e5d",
        "title": "Extreme Q-Learning: MaxEnt RL without Entropy"
      },
      {
        "paperId": "0a9f6718ecdbbb8cf4b200a3d14695e1e350cf49",
        "title": "Exploring through Random Curiosity with General Value Functions"
      },
      {
        "paperId": "ea14465601f45bf50a148563c73c4d6e1971dcb1",
        "title": "Redeeming Intrinsic Rewards via Constrained Optimization"
      },
      {
        "paperId": "10b28bd8e9a7c54d063a1647dd8f38dae48cea22",
        "title": "Making Linear MDPs Practical via Contrastive Representation Learning"
      },
      {
        "paperId": "9cf615514ec531c1e573d13490383e21e44520ae",
        "title": "Guarantees for Epsilon-Greedy Reinforcement Learning with Function Approximation"
      },
      {
        "paperId": "33cd3831779bd67bd04d585d2b6f9a3f1aeea482",
        "title": "Follow-the-Perturbed-Leader for Adversarial Markov Decision Processes with Bandit Feedback"
      },
      {
        "paperId": "92c1d1f6488de9694cc881f06bddd9fa8bb55a35",
        "title": "The Statistical Complexity of Interactive Decision Making"
      },
      {
        "paperId": "dfee8970f6a4fa3e56b9dc7feb29ac721d04bb2a",
        "title": "Feel-Good Thompson Sampling for Contextual Bandits and Reinforcement Learning"
      },
      {
        "paperId": "3be84e24b144541a8cd9030526ef2b8ef2cbfe54",
        "title": "A Survey of Exploration Methods in Reinforcement Learning"
      },
      {
        "paperId": "558ca2e8c7eb56edd77a52b084e6cc24dffe5bcd",
        "title": "Deep Reinforcement Learning at the Edge of the Statistical Precipice"
      },
      {
        "paperId": "49142e3e381c0dc7fee0049ea41d2ef02c0340d7",
        "title": "Isaac Gym: High Performance GPU-Based Physics Simulation For Robot Learning"
      },
      {
        "paperId": "014e5f875578cbc6de620e47a0666056461f9aa5",
        "title": "Randomized Exploration for Reinforcement Learning with General Value Function Approximation"
      },
      {
        "paperId": "484ff56a646281c051840993adfedbb1d0af665d",
        "title": "Bilinear Classes: A Structural Framework for Provable Generalization in RL"
      },
      {
        "paperId": "b284afe9a7363b898661c9b3cfb7f015b158cc63",
        "title": "Maximum Entropy RL (Provably) Solves Some Robust RL Problems"
      },
      {
        "paperId": "9e34b78828b7cb58c2a69d6651ecf0f137fe7e0d",
        "title": "On The Effect of Auxiliary Tasks on Representation Dynamics"
      },
      {
        "paperId": "b3fd1299060356fcfde65ab88431ba11d70b8ad2",
        "title": "Meta-Thompson Sampling"
      },
      {
        "paperId": "46a3b966fb744b8992491575586f5a03ee5ce557",
        "title": "Bellman Eluder Dimension: New Rich Classes of RL Problems, and Sample-Efficient Algorithms"
      },
      {
        "paperId": "5da1b4e1ddc612970530e5bb29470fe43bfcf2d6",
        "title": "PC-PG: Policy Cover Directed Exploration for Provable Policy Gradient Learning"
      },
      {
        "paperId": "57ffba5ea5e602b5365752e19a6f85a716db1d8b",
        "title": "Bandit Algorithms"
      },
      {
        "paperId": "c035d3bd325fd374f4f4a2c3a9b6aa8483d64c66",
        "title": "The Value-Improvement Path: Towards Better Representations for Reinforcement Learning"
      },
      {
        "paperId": "5de6d8b4436f6598f5bcba00bc07d864e962f1fb",
        "title": "Temporally-Extended {\\epsilon}-Greedy Exploration"
      },
      {
        "paperId": "175a9a3f0bb4f31fa235386aff52ad18c67275d3",
        "title": "Model-Based Reinforcement Learning with Value-Targeted Regression"
      },
      {
        "paperId": "69efbf9ddee489b3bdc10a278d3cd375abbb0d54",
        "title": "Hypermodels for Exploration"
      },
      {
        "paperId": "616ac6772508e72084360158078058dfa8bda7e7",
        "title": "First return, then explore"
      },
      {
        "paperId": "9651e1987a39d100dc0f6696a2077199e5075ea2",
        "title": "Agent57: Outperforming the Atari Human Benchmark"
      },
      {
        "paperId": "86687ad06378954f57cc01922b0369d97e75fd19",
        "title": "Frequentist Regret Bounds for Randomized Least-Squares Value Iteration"
      },
      {
        "paperId": "9f437593868e04fcc2dc4b6c1b5e953588094d57",
        "title": "Old Dog Learns New Tricks: Randomized UCB for Bandit Problems"
      },
      {
        "paperId": "fa4277c83ae5c6c5b9fa5e58c9053e2d92cb8b0e",
        "title": "Benchmarking Bonus-Based Exploration Methods on the Arcade Learning Environment"
      },
      {
        "paperId": "d423fa6cd0f4d088941d9fe4bebd834d0137c9b9",
        "title": "Provably Efficient Reinforcement Learning with Linear Function Approximation"
      },
      {
        "paperId": "1d7a8b7ab7186360fc239678b3d07941cc0f553e",
        "title": "Randomized Exploration in Generalized Linear Bandits"
      },
      {
        "paperId": "87a40a2d506fec06d67dc8f14ce2c708a789a2b4",
        "title": "Self-Supervised Exploration via Disagreement"
      },
      {
        "paperId": "e7b621eefee67cc38def15430c3b4232ec910cc1",
        "title": "Perturbed-History Exploration in Stochastic Linear Bandits"
      },
      {
        "paperId": "c520bf47db3360ae3a52219771390a354ed8a91f",
        "title": "Go-Explore: a New Approach for Hard-Exploration Problems"
      },
      {
        "paperId": "adab275554eb745cdc21fd6c526ec55a5b2ef362",
        "title": "Provably Efficient Maximum Entropy Exploration"
      },
      {
        "paperId": "b19feda0f1a15e22a0a37c8de3aaeee06d841bb6",
        "title": "Model-based RL in Contextual Decision Processes: PAC bounds and Exponential Improvements over Model-free Approaches"
      },
      {
        "paperId": "5689eba5794194d42a6e285b0e9d03516e083d68",
        "title": "Garbage In, Reward Out: Bootstrapping Exploration in Multi-Armed Bandits"
      },
      {
        "paperId": "4cb3fd057949624aa4f0bbe7a6dcc8777ff04758",
        "title": "Exploration by Random Network Distillation"
      },
      {
        "paperId": "ca14dce53be20d3d23d4f0db844a8389ab619db3",
        "title": "Large-Scale Study of Curiosity-Driven Learning"
      },
      {
        "paperId": "3aadab924520c58be81781aafd51e6807e9c4576",
        "title": "Visual Reinforcement Learning with Imagined Goals"
      },
      {
        "paperId": "5b01eaef54a653ba03ddd5a978690380fbc19bfc",
        "title": "Diversity is All You Need: Learning Skills without a Reward Function"
      },
      {
        "paperId": "3c3093f5f8e9601637612fcfb8f160f116fa30e4",
        "title": "Diversity-Driven Exploration Strategy for Deep Reinforcement Learning"
      },
      {
        "paperId": "811df72e210e20de99719539505da54762a11c6d",
        "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor"
      },
      {
        "paperId": "1418c9da011db25fa95a32989d5a578bc3bc4601",
        "title": "Intrinsically Motivated Goal Exploration Processes with Automatic Curriculum Learning"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "c0cba3fbc696102ba38c9d754a1f22d9e9eb6366",
        "title": "A Tutorial on Thompson Sampling"
      },
      {
        "paperId": "3fab4d28be5ff73f42c8b307117aa8d680345ad9",
        "title": "Noisy Networks for Exploration"
      },
      {
        "paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
        "title": "Attention is All you Need"
      },
      {
        "paperId": "142497432fe179ddb6ffe600c64a837ec6179550",
        "title": "Parameter Space Noise for Exploration"
      },
      {
        "paperId": "11026fc42c4d30f9ea91ec8c32f8d75768b70f6d",
        "title": "Boltzmann Exploration Done Right"
      },
      {
        "paperId": "225ab689f41cef1dc18237ef5dab059a49950abf",
        "title": "Curiosity-Driven Exploration by Self-Supervised Prediction"
      },
      {
        "paperId": "a441728f9fd6af1946368240162a72c2028c8cb1",
        "title": "Deep Exploration via Randomized Value Functions"
      },
      {
        "paperId": "37088dec26231bc5a4937054ebc862bb83a3db4d",
        "title": "Neural Episodic Control"
      },
      {
        "paperId": "12f67fb182bc934fc95ce97acff553d83e2ca72e",
        "title": "Count-Based Exploration with Neural Density Models"
      },
      {
        "paperId": "4eb38b3460606a4042b04fc52d0044ab948b4a17",
        "title": "EX2: Exploration with Exemplar Models for Deep Reinforcement Learning"
      },
      {
        "paperId": "d7bd6e3addd8bc8e2e154048300eea15f030ed33",
        "title": "Reinforcement Learning with Unsupervised Auxiliary Tasks"
      },
      {
        "paperId": "6e90fd78e8a3b98af3954aae5209703aa966603e",
        "title": "Unifying Count-Based Exploration and Intrinsic Motivation"
      },
      {
        "paperId": "317cd4522b1f4a6f889743578143bb8823623f8b",
        "title": "VIME: Variational Information Maximizing Exploration"
      },
      {
        "paperId": "4b63e34276aa98d5345efa7fe09bb06d8a9d8f52",
        "title": "Deep Exploration via Bootstrapped DQN"
      },
      {
        "paperId": "1dd34f8879aeb03984ca507452d6b1938bcbbc64",
        "title": "BISTRO: An Efficient Relaxation-Based Method for Contextual Bandits"
      },
      {
        "paperId": "69e76e16740ed69f4dc55361a3d319ac2f1293dd",
        "title": "Asynchronous Methods for Deep Reinforcement Learning"
      },
      {
        "paperId": "5dc2a215bd7cd5bdd3a0baa8c967575632696fac",
        "title": "Universal Value Function Approximators"
      },
      {
        "paperId": "340f48901f72278f6bf78a04ee5b01df208cc508",
        "title": "Human-level control through deep reinforcement learning"
      },
      {
        "paperId": "3abbe0006c53644a5e6bea4d04b46c22c90bee52",
        "title": "Thompson Sampling for Learning Parameterized Markov Decision Processes"
      },
      {
        "paperId": "1389772b8a0f9c7fc43057f9da41a7d0ebf0308b",
        "title": "Generalization and Exploration via Randomized Value Functions"
      },
      {
        "paperId": "2319a491378867c7049b3da055c5df60e1671158",
        "title": "Playing Atari with Deep Reinforcement Learning"
      },
      {
        "paperId": "789783016fb708abbc061790612ebe91273c05d3",
        "title": "(More) Efficient Reinforcement Learning via Posterior Sampling"
      },
      {
        "paperId": "f82e4ff4f003581330338aaae71f60316e58dd26",
        "title": "The Arcade Learning Environment: An Evaluation Platform for General Agents"
      },
      {
        "paperId": "0cafe2903b097fc042782c359cb231ea34ef7ed3",
        "title": "Near-optimal Regret Bounds for Reinforcement Learning"
      },
      {
        "paperId": "e8077729ef723d71a0b7492f2f271ae413b5a4d5",
        "title": "What is Intrinsic Motivation? A Typology of Computational Approaches"
      },
      {
        "paperId": "0e7638dc16a5e5e9e46c91272bfb9c3dd242ef6d",
        "title": "Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning"
      },
      {
        "paperId": "116d7798c1123cf7fad4176e98f58fd49de4f8f1",
        "title": "Planning and Acting in Partially Observable Stochastic Domains"
      },
      {
        "paperId": "0d634ab4754232d6e17181241954fbf6d73e5803",
        "title": "Bootstrap confidence intervals"
      },
      {
        "paperId": "e02d1d482ea7a51ede5a0babd45ab3d4344a8e13",
        "title": "On the Theory of Apportionment"
      },
      {
        "paperId": "ee2cd1d17f833d3c157a1016a778c7c22af555a2",
        "title": "ON THE LIKELIHOOD THAT ONE UNKNOWN PROBABILITY EXCEEDS ANOTHER IN VIEW OF THE EVIDENCE OF TWO SAMPLES"
      },
      {
        "paperId": "bfa927f68594592faed4670a13ed8bf05d9cbb9a",
        "title": "Curiosity-Driven Learning of Joint Locomotion and Manipulation Tasks"
      },
      {
        "paperId": "cdd87786bcd0e6416be1bd1746d54f70526b5d44",
        "title": "Randomized Exploration in Reinforcement Learning with General Value Function Approximation"
      },
      {
        "paperId": null,
        "title": "Horde"
      },
      {
        "paperId": "25f8e9e35cafd7fb686d939f274111bcffeafd6b",
        "title": "The Development of Embodied Cognition: Six Lessons from Babies"
      },
      {
        "paperId": "ac4af1df88e178386d782705acc159eaa0c3904a",
        "title": "Actor-Critic Algorithms"
      },
      {
        "paperId": "97efafdb4a3942ab3efba53ded7413199f79c054",
        "title": "Reinforcement Learning: An Introduction"
      },
      {
        "paperId": "6bc8db0c7444d9c07aad440393b2fd300fb3595c",
        "title": "Function Optimization using Connectionist Reinforcement Learning Algorithms"
      },
      {
        "paperId": null,
        "title": "and probability of improvement over PPO in Figure 26. An ablation study of how using white noise for randomizing rewards affects performance"
      }
    ],
    "cited_by": [
      {
        "paperId": "22b711eb36f408251070c31ae5107fa79375ef82",
        "title": "Uncertainty-driven Adaptive Exploration"
      },
      {
        "paperId": "905cb1672c1d685c171acd8a1ed6db59e671409a",
        "title": "BILE: An Effective Behavior-based Latent Exploration Scheme for Deep Reinforcement Learning"
      },
      {
        "paperId": "47fccd1b5e3faa0e9184c8b401c3b3486717e697",
        "title": "Imagine, Verify, Execute: Memory-Guided Agentic Exploration with Vision-Language Models"
      },
      {
        "paperId": "91876fbf6d3da5ac4e16de5ac53a46e3a3f2cc50",
        "title": "Highly Efficient Self-Adaptive Reward Shaping for Reinforcement Learning"
      },
      {
        "paperId": "8929bb5e372fe36b85664fca0e6649a6f4dd2e49",
        "title": "A Survey Analyzing Generalization in Deep Reinforcement Learning"
      }
    ],
    "score": 5.0
  },
  {
    "id": "3ef01975a45bcf78120d76c1bc73e8ec12e213bf",
    "title": "Breadth-First Exploration on Adaptive Grid for Reinforcement Learning",
    "authors": [
      "Youngsik Yoon",
      "Gangbok Lee",
      "Sungsoo Ahn",
      "Jungseul Ok"
    ],
    "year": 2024,
    "citationCount": 5,
    "abstract": "",
    "url": "https://www.semanticscholar.org/paper/3ef01975a45bcf78120d76c1bc73e8ec12e213bf",
    "pdf_url": null,
    "venue": "International Conference on Machine Learning",
    "publicationDate": "",
    "externalIds": {
      "DBLP": "conf/icml/YoonLAO24",
      "CorpusId": 272330298
    },
    "references": [],
    "cited_by": [
      {
        "paperId": "33b58c2b0af63c261cb28264f8387e51dd104dde",
        "title": "Strict Subgoal Execution: Reliable Long-Horizon Planning in Hierarchical Reinforcement Learning"
      },
      {
        "paperId": "ff8712ee845d88ca99278dc28e45b55f63184880",
        "title": "Graph-Assisted Stitching for Offline Hierarchical Reinforcement Learning"
      },
      {
        "paperId": "38551600e48bd654e981d2dc41ef9afaa66b04e4",
        "title": "Don't Just Follow MLLM Plans: Robust and Efficient Planning for Open-world Agents"
      },
      {
        "paperId": "7430e343f677be0253b441d35f06d24ee4d8212e",
        "title": "Efficient Pathfinding in a Maze to overcome Challenges in Robotics and AI Using Breadth-First Search"
      },
      {
        "paperId": "890bf1ef5b457d8816c008c91eb6c925c9c1e094",
        "title": "Combinatorial Rising Bandit"
      }
    ],
    "score": 5.0
  },
  {
    "id": "39d2839aa4c3d8c0e64553891fe98ba261703154",
    "title": "More Efficient Randomized Exploration for Reinforcement Learning via Approximate Sampling",
    "authors": [
      "Haque Ishfaq",
      "Yixin Tan",
      "Yu Yang",
      "Qingfeng Lan",
      "Jianfeng Lu",
      "A. R. Mahmood",
      "D. Precup",
      "Pan Xu"
    ],
    "year": 2024,
    "citationCount": 5,
    "abstract": "Thompson sampling (TS) is one of the most popular exploration techniques in reinforcement learning (RL). However, most TS algorithms with theoretical guarantees are difficult to implement and not generalizable to Deep RL. While the emerging approximate sampling-based exploration schemes are promising, most existing algorithms are specific to linear Markov Decision Processes (MDP) with suboptimal regret bounds, or only use the most basic samplers such as Langevin Monte Carlo. In this work, we propose an algorithmic framework that incorporates different approximate sampling methods with the recently proposed Feel-Good Thompson Sampling (FGTS) approach (Zhang, 2022; Dann et al., 2021), which was previously known to be computationally intractable in general. When applied to linear MDPs, our regret analysis yields the best known dependency of regret on dimensionality, surpassing existing randomized algorithms. Additionally, we provide explicit sampling complexity for each employed sampler. Empirically, we show that in tasks where deep exploration is necessary, our proposed algorithms that combine FGTS and approximate sampling perform significantly better compared to other strong baselines. On several challenging games from the Atari 57 suite, our algorithms achieve performance that is either better than or on par with other strong baselines from the deep RL literature.",
    "url": "https://www.semanticscholar.org/paper/39d2839aa4c3d8c0e64553891fe98ba261703154",
    "pdf_url": "https://arxiv.org/pdf/2406.12241.pdf",
    "venue": "RLJ",
    "publicationDate": "2024-06-18",
    "externalIds": {
      "DBLP": "journals/corr/abs-2406-12241",
      "ArXiv": "2406.12241",
      "DOI": "10.48550/arXiv.2406.12241",
      "CorpusId": 270562137
    },
    "references": [
      {
        "paperId": "7da3eebf5c2f481bc38ae6115cec3fc71dc87f2b",
        "title": "Randomized Exploration in Cooperative Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "7c8ed72733835684c86420e168b2cb5515d42115",
        "title": "Q-Star Meets Scalable Posterior Sampling: Bridging Theory and Practice via HyperAgent"
      },
      {
        "paperId": "10fe645c0ffc41edaed4ab5bb425660d9a70002d",
        "title": "Accelerating Approximate Thompson Sampling with Underdamped Langevin Monte Carlo"
      },
      {
        "paperId": "c90aa0f206c6fd41c490c142f63f7ba046cae6b7",
        "title": "Provable and Practical: Efficient Exploration in Reinforcement Learning via Langevin Monte Carlo"
      },
      {
        "paperId": "0b8ddfd47a4431ed41807d9be6445d736f649032",
        "title": "Improved Discretization Analysis for Underdamped Langevin Monte Carlo"
      },
      {
        "paperId": "7b6b72bfe8ba1a3ab07dc641c163676a8ae5e59d",
        "title": "A Provably Efficient Model-Free Posterior Sampling Method for Episodic Reinforcement Learning"
      },
      {
        "paperId": "59c2a40761b12aef01dcb8b5e10065733551f331",
        "title": "Langevin Monte Carlo for Contextual Bandits"
      },
      {
        "paperId": "2dcc86a4a79fc3b53bc10462103225f9677ac132",
        "title": "Model-based RL with Optimistic Posterior Sampling: Structural Conditions and Sample Complexity"
      },
      {
        "paperId": "686ecf21ea2a8ce5313537625d5fa2c3f5797dca",
        "title": "Non-Linear Reinforcement Learning in Large Action Spaces: Structural Conditions and Sample-efficiency of Posterior Sampling"
      },
      {
        "paperId": "dfee8970f6a4fa3e56b9dc7feb29ac721d04bb2a",
        "title": "Feel-Good Thompson Sampling for Contextual Bandits and Reinforcement Learning"
      },
      {
        "paperId": "80438f2ef228776b22118915f1a27e2cf4a0f4e0",
        "title": "Tianshou: a Highly Modularized Deep Reinforcement Learning Library"
      },
      {
        "paperId": "014e5f875578cbc6de620e47a0666056461f9aa5",
        "title": "Randomized Exploration for Reinforcement Learning with General Value Function Approximation"
      },
      {
        "paperId": "e2381b66f552eacfb5f8ec3bdf6bb98c5e1e50f7",
        "title": "Near-Optimal Randomized Exploration for Tabular Markov Decision Processes"
      },
      {
        "paperId": "46a3b966fb744b8992491575586f5a03ee5ce557",
        "title": "Bellman Eluder Dimension: New Rich Classes of RL Problems, and Sample-Efficient Algorithms"
      },
      {
        "paperId": "14701a7dcb91c2afe139add048412ec26ac9c1ac",
        "title": "Faster Convergence of Stochastic Gradient Langevin Dynamics for Non-Log-Concave Sampling"
      },
      {
        "paperId": "828dabc885ca2a0683f6f904ded334450a818f4c",
        "title": "Structured Logconcave Sampling with a Restricted Gaussian Oracle"
      },
      {
        "paperId": "2b2735cffb0d2321a456363880ff5671e80df4cb",
        "title": "On Bonus Based Exploration Methods In The Arcade Learning Environment"
      },
      {
        "paperId": "5c5f76af16e9fc7f5b65df390b0d4cc6d64e6f20",
        "title": "Learning Near Optimal Policies with Low Inherent Bellman Error"
      },
      {
        "paperId": "86687ad06378954f57cc01922b0369d97e75fd19",
        "title": "Frequentist Regret Bounds for Randomized Least-Squares Value Iteration"
      },
      {
        "paperId": "d423fa6cd0f4d088941d9fe4bebd834d0137c9b9",
        "title": "Provably Efficient Reinforcement Learning with Linear Function Approximation"
      },
      {
        "paperId": "e5e440562aa293b43cd95a5b52dd705fbf88115b",
        "title": "Worst-Case Regret Bounds for Exploration via Randomized Value Functions"
      },
      {
        "paperId": "a4af0eb16016c5e1f36bafb7291a8a4932c0f17c",
        "title": "Reinforcement Learning in Feature Space: Matrix Bandit, Kernels, and Regret Bound"
      },
      {
        "paperId": "dbc024fb80661114e80e179cdfefb97fe5a58955",
        "title": "Rapid Convergence of the Unadjusted Langevin Algorithm: Isoperimetry Suffices"
      },
      {
        "paperId": "12cd30cab8281567f5a1e8e3145641336fbb819c",
        "title": "Sample-Optimal Parametric Q-Learning Using Linearly Additive Features"
      },
      {
        "paperId": "d85623ffae865f9ef386644dd02d0ea2d6a8c8de",
        "title": "Implicit Quantile Networks for Distributional Reinforcement Learning"
      },
      {
        "paperId": "f802802b3af5a22b79ac65d033ba3cbee33da91b",
        "title": "Randomized Prior Functions for Deep Reinforcement Learning"
      },
      {
        "paperId": "6a071644ee79857c26b6fd718663291d90421e92",
        "title": "Analysis of Langevin Monte Carlo via Convex Optimization"
      },
      {
        "paperId": "6f7cdcf98f559d93f33c8b6f0f376f0c8c8cfab2",
        "title": "Global Convergence of Langevin Dynamics Based Algorithms for Nonconvex Optimization"
      },
      {
        "paperId": "c1f4ef741242d629d1f56e442a09a7ba29595a0e",
        "title": "A Distributional Perspective on Reinforcement Learning"
      },
      {
        "paperId": "24e31e8cf65a099950ac3a5a3c18328f500887df",
        "title": "Underdamped Langevin MCMC: A non-asymptotic analysis"
      },
      {
        "paperId": "3fab4d28be5ff73f42c8b307117aa8d680345ad9",
        "title": "Noisy Networks for Exploration"
      },
      {
        "paperId": "142497432fe179ddb6ffe600c64a837ec6179550",
        "title": "Parameter Space Noise for Exploration"
      },
      {
        "paperId": "4b63e34276aa98d5345efa7fe09bb06d8a9d8f52",
        "title": "Deep Exploration via Bootstrapped DQN"
      },
      {
        "paperId": "c6170fa90d3b2efede5a2e1660cb23e1c824f2ca",
        "title": "Prioritized Experience Replay"
      },
      {
        "paperId": "3b9732bb07dc99bde5e1f9f75251c6ea5039373e",
        "title": "Deep Reinforcement Learning with Double Q-Learning"
      },
      {
        "paperId": "340f48901f72278f6bf78a04ee5b01df208cc508",
        "title": "Human-level control through deep reinforcement learning"
      },
      {
        "paperId": "d6f2f611da110b5b5061731be3fc4c7f45d8ee23",
        "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification"
      },
      {
        "paperId": "c6db2e81610c722d5b51fd7febb87bcaa387d86d",
        "title": "Theoretical guarantees for approximate sampling from smooth and log\u2010concave densities"
      },
      {
        "paperId": "9a471f962b3b2cbac5e84e876cd11f4921f339ec",
        "title": "Stochastic Gradient Hamiltonian Monte Carlo"
      },
      {
        "paperId": "1389772b8a0f9c7fc43057f9da41a7d0ebf0308b",
        "title": "Generalization and Exploration via Randomized Value Functions"
      },
      {
        "paperId": "789783016fb708abbc061790612ebe91273c05d3",
        "title": "(More) Efficient Reinforcement Learning via Posterior Sampling"
      },
      {
        "paperId": "f82e4ff4f003581330338aaae71f60316e58dd26",
        "title": "The Arcade Learning Environment: An Evaluation Platform for General Agents"
      },
      {
        "paperId": "a6c4839019e452834c3ab9a19a5c38615280111f",
        "title": "Handbook of Markov Chain Monte Carlo: Hardcover: 619 pages Publisher: Chapman and Hall/CRC Press (first edition, May 2011) Language: English ISBN-10: 1420079417"
      },
      {
        "paperId": "aeed631d6a84100b5e9a021ec1914095c66de415",
        "title": "Bayesian Learning via Stochastic Gradient Langevin Dynamics"
      },
      {
        "paperId": "644a079073969a92674f69483c4a85679d066545",
        "title": "Double Q-learning"
      },
      {
        "paperId": "48cce5ee49facf75eeb12832c387452424b645dd",
        "title": "A Bayesian Framework for Reinforcement Learning"
      },
      {
        "paperId": "41fb79ea7f0fcde0f2f55a2979446a28a733b6b9",
        "title": "Bayesian Computation and Stochastic Systems"
      },
      {
        "paperId": "9d3af1c5fd5f7fb51467b4308952bc4da8285396",
        "title": "Paper"
      },
      {
        "paperId": "ee2cd1d17f833d3c157a1016a778c7c22af555a2",
        "title": "ON THE LIKELIHOOD THAT ONE UNKNOWN PROBABILITY EXCEEDS ANOTHER IN VIEW OF THE EVIDENCE OF TWO SAMPLES"
      },
      {
        "paperId": null,
        "title": "DQN Zoo: Reference implementations of DQN-based agents"
      },
      {
        "paperId": null,
        "title": "A method for stochastic optimization"
      },
      {
        "paperId": "9443908296fa4f90975647e59534fc6fd2e71cdc",
        "title": "\u039b LANGEVIN"
      }
    ],
    "cited_by": [
      {
        "paperId": "2b99f1383a719b8ffc6c4a8b5b2b7cba12a6ec32",
        "title": "Universal Value-Function Uncertainties"
      },
      {
        "paperId": "fe01586174cdc2a5786c83248f9f460d4ea2ab43",
        "title": "CAE: Repurposing the Critic as an Explorer in Deep Reinforcement Learning"
      },
      {
        "paperId": "326f3b22fb91661048ea24da2e47a78f091430c8",
        "title": "IL-SOAR : Imitation Learning with Soft Optimistic Actor cRitic"
      },
      {
        "paperId": "e861a51688b6fc086854161c50eef9b04cf11e02",
        "title": "Langevin Soft Actor-Critic: Efficient Exploration through Uncertainty-Driven Critic Learning"
      },
      {
        "paperId": "023ac9657725b307f7460471b2f3caac20948cfd",
        "title": "Isoperimetry is All We Need: Langevin Posterior Sampling for RL with Sublinear Regret"
      }
    ],
    "score": 5.0
  },
  {
    "id": "cb7ea0176eec1f809f3bc3a34aeb279d44dda612",
    "title": "Global Reinforcement Learning: Beyond Linear and Convex Rewards via Submodular Semi-gradient Methods",
    "authors": [
      "Ric De Santi",
      "Manish Prajapat",
      "Andreas Krause"
    ],
    "year": 2024,
    "citationCount": 5,
    "abstract": "In classic Reinforcement Learning (RL), the agent maximizes an additive objective of the visited states, e.g., a value function. Unfortunately, objectives of this type cannot model many real-world applications such as experiment design, exploration, imitation learning, and risk-averse RL to name a few. This is due to the fact that additive objectives disregard interactions between states that are crucial for certain tasks. To tackle this problem, we introduce Global RL (GRL), where rewards are globally defined over trajectories instead of locally over states. Global rewards can capture negative interactions among states, e.g., in exploration, via submodularity, positive interactions, e.g., synergetic effects, via supermodularity, while mixed interactions via combinations of them. By exploiting ideas from submodular optimization, we propose a novel algorithmic scheme that converts any GRL problem to a sequence of classic RL problems and solves it efficiently with curvature-dependent approximation guarantees. We also provide hardness of approximation results and empirically demonstrate the effectiveness of our method on several GRL instances.",
    "url": "https://www.semanticscholar.org/paper/cb7ea0176eec1f809f3bc3a34aeb279d44dda612",
    "pdf_url": "https://arxiv.org/pdf/2407.09905.pdf",
    "venue": "International Conference on Machine Learning",
    "publicationDate": "2024-07-13",
    "externalIds": {
      "DBLP": "journals/corr/abs-2407-09905",
      "ArXiv": "2407.09905",
      "DOI": "10.48550/arXiv.2407.09905",
      "CorpusId": 271213356
    },
    "references": [
      {
        "paperId": "31fcbacd4874a772e77ac21e4ec7d0dff132ec4e",
        "title": "Submodular Reinforcement Learning"
      },
      {
        "paperId": "b2dde612d47a9d747f758579dc6eef3294c24ab7",
        "title": "Reinforcement Learning with General Utilities: Simpler Variance Reduction and Large State-Action Space"
      },
      {
        "paperId": "f2baa9e966667db627ec60ec5f558504e022431b",
        "title": "Near-Optimal Multi-Agent Learning for Safe Coverage Control"
      },
      {
        "paperId": "588be1711731acfc4f7dea1e6ae99bc8f9333449",
        "title": "Active Exploration via Experiment Design in Markov Chains"
      },
      {
        "paperId": "399bbcecf674a55fb1a91b178a5be4a8999bb0ac",
        "title": "The Importance of Non-Markovianity in Maximum State Entropy Exploration"
      },
      {
        "paperId": "fa950a27da0281095f7be7d8a2224391dcbd247b",
        "title": "Challenging Common Assumptions in Convex Reinforcement Learning"
      },
      {
        "paperId": "4f789ee52aaeac38123af0f957f841d40db34f0b",
        "title": "Submodularity In Machine Learning and Artificial Intelligence"
      },
      {
        "paperId": "65e36b8fc38819944528d232368d8669feb3b01a",
        "title": "Wasserstein Unsupervised Reinforcement Learning"
      },
      {
        "paperId": "9333138ad1d0ef3778bae9ae517aaf396de1b287",
        "title": "Density Constrained Reinforcement Learning"
      },
      {
        "paperId": "046bc091cbe8354d965cb157a44a9934621198ad",
        "title": "Concave Utility Reinforcement Learning: the Mean-field Game viewpoint"
      },
      {
        "paperId": "5c37023c35fc1c95565d56b4fc4821fcf768651a",
        "title": "Reward is enough for convex MDPs"
      },
      {
        "paperId": "0295df1b9d11e6e49b20119410fd755a4d7781af",
        "title": "Behavior From the Void: Unsupervised Active Pre-Training"
      },
      {
        "paperId": "18b1d9052cb82dc7c34827768c98aae09caeb649",
        "title": "Curiosity in exploring chemical spaces: intrinsic rewards for molecular reinforcement learning"
      },
      {
        "paperId": "c32a821e39593fb2dda872cd357ed0adf0000a97",
        "title": "Planning with Submodular Objective Functions"
      },
      {
        "paperId": "6bf9b58b8f418fb2922762550fb78bb22c83c0f8",
        "title": "Variational Policy Gradient Method for Reinforcement Learning with General Utilities"
      },
      {
        "paperId": "7ed50e8bd030093b78ab1a38567e2d426240b157",
        "title": "Constrained episodic reinforcement learning in concave-convex and knapsack settings"
      },
      {
        "paperId": "fed69ae21711272a06cbeb32a2eef50125e29800",
        "title": "Active Model Estimation in Markov Decision Processes"
      },
      {
        "paperId": "39b2551f109fd0495abfca1ab0bb81311c1b3996",
        "title": "Explore, Discover and Learn: Unsupervised Discovery of State-Covering Skills"
      },
      {
        "paperId": "157491897eb73fe9507b73ee1d7a84ad0c08f2a1",
        "title": "Reinforcement Learning with Non-Markovian Rewards"
      },
      {
        "paperId": "c0dabdc9036909e10c05628b395784078c0c8f6b",
        "title": "A Divergence Minimization Perspective on Imitation Learning Methods"
      },
      {
        "paperId": "a9beaafdc4b23d1becb55eaeef756e83d4e1045b",
        "title": "Stochastic Greedy Algorithm Is Still Good: Maximizing Submodular + Supermodular Functions"
      },
      {
        "paperId": "cfec54f9208dcbb2c558aa7ff45f57be2533d046",
        "title": "Efficient Exploration via State Marginal Matching"
      },
      {
        "paperId": "0701e981d04e2fc424fd3946f2140cd124719b38",
        "title": "Active Exploration in Markov Decision Processes"
      },
      {
        "paperId": "adab275554eb745cdc21fd6c526ec55a5b2ef362",
        "title": "Provably Efficient Maximum Entropy Exploration"
      },
      {
        "paperId": "914f279742a9f509934a0959c080b08e9fad771e",
        "title": "LTLf/LDLf Non-Markovian Rewards"
      },
      {
        "paperId": "5b01eaef54a653ba03ddd5a978690380fbc19bfc",
        "title": "Diversity is All You Need: Learning Skills without a Reward Function"
      },
      {
        "paperId": "3076aa03e4f5516e8fd50cc3e2b849d83f9ccc45",
        "title": "Greed is Still Good: Maximizing Monotone Submodular+Supermodular Functions"
      },
      {
        "paperId": "1202d804c6e9de227e8819b5fdd464d5ed2a7a97",
        "title": "Polyhedral aspects of Submodularity, Convexity and Concavity"
      },
      {
        "paperId": "61e904816fe00c024fdebb366664be8eb077bc0e",
        "title": "Adaptive Submodular Maximization in Bandit Setting"
      },
      {
        "paperId": "7df34cd9d000535f513e2f4c7dc4f37047125669",
        "title": "Fast Semidifferential-based Submodular Function Optimization"
      },
      {
        "paperId": "cb5ccf4aade0f0922d229887f470e1a165bb7386",
        "title": "Submodular-Bregman and the Lov\u00e1sz-Bregman Divergences with Applications"
      },
      {
        "paperId": "0625bb91c8fafb5d5eafd80d377a3a447573212a",
        "title": "Algorithms for Approximate Minimization of the Difference Between Submodular Functions, with Applications"
      },
      {
        "paperId": "9fce4aa236ae2f600281942644926d558c7e3db4",
        "title": "Linear Submodular Bandits and their Application to Diversified Retrieval"
      },
      {
        "paperId": "6971427e8f616e5e33d24674fa47b7a2ac69c0df",
        "title": "Submodularity and its applications in optimized information gathering"
      },
      {
        "paperId": "ec94a1d967333b3a5663f581554ad11d278d3cf6",
        "title": "Submodular meets Spectral: Greedy Algorithms for Subset Selection, Sparse Approximation and Dictionary Selection"
      },
      {
        "paperId": "73b6c9b4e398ef34c4da915d64713ac6f3646e88",
        "title": "Submodular function minimization"
      },
      {
        "paperId": "e85bb9fa4eeee8c8d6cd465eb897d7839d809e48",
        "title": "A recursive greedy algorithm for walks in directed graphs"
      },
      {
        "paperId": "69103f478bc99d9fd6bd3aa4cd13cacedaee82cd",
        "title": "A Submodular-supermodular Procedure with Applications to Discriminative Structure Learning"
      },
      {
        "paperId": "f65020fc3b1692d7989e099d6b6e698be5a50a93",
        "title": "Apprenticeship learning via inverse reinforcement learning"
      },
      {
        "paperId": "ca69b23f49a398489e78aedfce5e57a56560b6c9",
        "title": "Approximation algorithms for orienteering and discounted-reward TSP"
      },
      {
        "paperId": "a9cd8efe9184dddb1bedbbec3a356c4dfb22fe63",
        "title": "Markov Decision Processes: Discrete Stochastic Dynamic Programming"
      },
      {
        "paperId": "d82829c9fd9cfba8a44efe5ba048d3332a1671fc",
        "title": "Dynamic Programming"
      },
      {
        "paperId": "8a2f24bb8abbae2a6ed3fba1a981abf93d8ade7c",
        "title": "Maximizing a supermodular pseudoboolean function: A polynomial algorithm for supermodular cubic functions"
      },
      {
        "paperId": "d6f2a526207eb4808f59f2a6f15d6880c6491fbf",
        "title": "Submodular set functions, matroids and the greedy algorithm: Tight worst-case bounds and some generalizations of the Rado-Edmonds theorem"
      },
      {
        "paperId": "2418d6703faea16ec978649c281b89e0b70f15dd",
        "title": "Convex Reinforcement Learning in Finite Trials"
      },
      {
        "paperId": null,
        "title": "Global Reinforcement Learning: Beyond Linear and Convex Rewards via Submodular forcement learning: Theory and algorithms"
      },
      {
        "paperId": "3138448efce0a4548749ad551aba7d2d203fda4d",
        "title": "Interactive Submodular Bandit"
      },
      {
        "paperId": "c0f2c4104ef6e36bb67022001179887e6600d24d",
        "title": "A comprehensive survey on safe reinforcement learning"
      },
      {
        "paperId": "5d1321eff5f4e4dbf599a97f213383b27e28e8aa",
        "title": "Submodular Function Maximization"
      },
      {
        "paperId": null,
        "title": "Gaussian processes for machine learning , volume 1"
      },
      {
        "paperId": null,
        "title": "On the supermodular knap-sack problem"
      },
      {
        "paperId": "b4821d2dee04f27e55112090c9029da30cc8b291",
        "title": "Submodular functions and convexity"
      },
      {
        "paperId": "ab7466983afe852ab7b13e0e2eea2e741088b40d",
        "title": "Proceedings of the Twenty-Third International Joint Conference on Artificial Intelligence Linear Temporal Logic and Linear Dynamic Logic on Finite Traces"
      }
    ],
    "cited_by": [
      {
        "paperId": "cc696600b5fc6c918fa9a97a068f76ff8f7e8655",
        "title": "Provable Maximum Entropy Manifold Exploration via Diffusion Models"
      },
      {
        "paperId": "b87eb504a47b3df461e290ab0977749376252ad4",
        "title": "Convex Markov Games: A New Frontier for Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "2f6511c90bf4f50be8c9be77e754282089241bd1",
        "title": "Zero-Shot Offline Imitation Learning via Optimal Transport"
      },
      {
        "paperId": "84a857d340f0901ba38766f3b58486be50eefcbe",
        "title": "A Prospect-Theoretic Policy Gradient Algorithm for Behavioral Alignment in Reinforcement Learning"
      },
      {
        "paperId": "601f533fea1e221342500100d31905fb983ffe29",
        "title": "Efficient Reinforcement Learning in Probabilistic Reward Machines"
      }
    ],
    "score": 5.0
  },
  {
    "id": "b31c76815615c16cc8505dbb38d2921f921c029d",
    "title": "A Coverage-Guided Fuzzing Method for Automatic Software Vulnerability Detection Using Reinforcement Learning-Enabled Multi-Level Input Mutation",
    "authors": [
      "Van-Hau Pham",
      "Do Thi Thu Hien",
      "Nguyen Phuc Chuong",
      "Pham Thanh Thai",
      "Phan The Duy"
    ],
    "year": 2024,
    "citationCount": 5,
    "abstract": "Fuzzing is a popular and effective software testing technique that automatically generates or modifies inputs to test the stability and vulnerabilities of a software system, which has been widely applied and improved by security researchers and experts. The goal of fuzzing is to uncover potential weaknesses in software by providing unexpected and invalid inputs to the target program to monitor its behavior and identify errors or unintended outcomes. Recently, researchers have also integrated promising machine learning algorithms, such as reinforcement learning, to enhance the fuzzing process. Reinforcement learning (RL) has been proven to be able to improve the effectiveness of fuzzing by selecting and prioritizing transformation actions with higher coverage, which reduces the required effort to uncover vulnerabilities. However, RL-based fuzzing models also encounter certain limitations, including an imbalance between exploitation and exploration. In this study, we propose a coverage-guided RL-based fuzzing model that enhances grey-box fuzzing, in which we leverage deep Q-learning to predict and select input variations to maximize code coverage and use code coverage as a reward signal. This model is complemented by simple input selection and scheduling algorithms that promote a more balanced approach to exploiting and exploring software. Furthermore, we introduce a multi-level input mutation model combined with RL to create a sequence of actions for comprehensive input variation. The proposed model is compared to other fuzzing tools in testing various real-world programs, where the results indicate a notable enhancement in terms of code coverage, discovered paths, and execution speed of our solution.",
    "url": "https://www.semanticscholar.org/paper/b31c76815615c16cc8505dbb38d2921f921c029d",
    "pdf_url": "https://doi.org/10.1109/ACCESS.2024.3421989",
    "venue": "IEEE Access",
    "publicationDate": "",
    "externalIds": {
      "DBLP": "journals/access/PhamHCTD24",
      "DOI": "10.1109/ACCESS.2024.3421989",
      "CorpusId": 270940896
    },
    "references": [
      {
        "paperId": "ce41efb51f1d05385258f2ca4bcbf226fc1ffe38",
        "title": "KernelGPT: Enhanced Kernel Fuzzing via Large Language Models"
      },
      {
        "paperId": "c05686732c25eac3f7cd57019b7a64ad0170bec9",
        "title": "Demystify the Fuzzing Methods: A Comprehensive Survey"
      },
      {
        "paperId": "df6ab102aa4955949a6849c8030b493bfd2afcc8",
        "title": "CGFuzzer: A Fuzzing Approach Based on Coverage-Guided Generative Adversarial Networks for Industrial IoT Protocols"
      },
      {
        "paperId": "2534ff72c29fb0c87a65a6c60378bf9813057809",
        "title": "Fuzzing vulnerability discovery techniques: Survey, challenges and future directions"
      },
      {
        "paperId": "655a2654c28b5d81a9dedff2fa39640c8ed877c8",
        "title": "Fuzzing: A Survey for Roadmap"
      },
      {
        "paperId": "609b98450ac2d892fdb97e9f9fd684908ffc37a9",
        "title": "RapidFuzz: Accelerating fuzzing via Generative Adversarial Networks"
      },
      {
        "paperId": "12075ea34f5fbe32ec5582786761ab34d401209b",
        "title": "Exploration in Deep Reinforcement Learning: From Single-Agent to Multiagent Domain"
      },
      {
        "paperId": "c7f1a61118fd87e074bff8984b0c87fadb464c57",
        "title": "AFLPro: Direction sensitive fuzzing"
      },
      {
        "paperId": "18c5d7c3ff720aa4c331783c7c47f3832e0fac13",
        "title": "Reinforcement Learning-Based Fuzzing Technology"
      },
      {
        "paperId": "70f16181b41d9a3d125ccb143770f5e5faabd2dc",
        "title": "RDFuzz: Accelerating Directed Fuzzing with Intertwined Schedule and Optimized Mutation"
      },
      {
        "paperId": "75514e3a775852856a21092a5db584c00c931c6c",
        "title": "A systematic review of fuzzing based on machine learning techniques"
      },
      {
        "paperId": "bf43b9d1b66e1b274541e3bcbc940b2cddac731c",
        "title": "Automated Software Vulnerability Testing Using Deep Learning Methods"
      },
      {
        "paperId": "fbb27c0605b45ccfb541354eec01f06672b19d24",
        "title": "Matryoshka: Fuzzing Deeply Nested Branches"
      },
      {
        "paperId": "5479c0e59e71bc09b504206fdeaeb7dc9c7b5e9e",
        "title": "NeuFuzz: Efficient Fuzzing With Deep Neural Network"
      },
      {
        "paperId": "fcc555601887ac9f6466dde8ac8b6f2a19deff4c",
        "title": "The Art, Science, and Engineering of Fuzzing: A Survey"
      },
      {
        "paperId": "5f76631d8fbd4293a28aa4d0e4bbcabe2079e364",
        "title": "FuzzerGym: A Competitive Framework for Fuzzing and Learning"
      },
      {
        "paperId": "74e935a0320ccfa62cf38f6008676963892475ea",
        "title": "Deep Reinforcement Fuzzing"
      },
      {
        "paperId": "99f88a55e46c3228a7d624ea9a959b9216a032ba",
        "title": "Coverage-Based Greybox Fuzzing as Markov Chain"
      },
      {
        "paperId": "4fcd49afa8d0a960c6d07aa4e7a37956f5307a8a",
        "title": "Large Language Model guided Protocol Fuzzing"
      },
      {
        "paperId": null,
        "title": "\u2018\u2018Asystematicreview offuzzing,\u2019\u2019"
      },
      {
        "paperId": null,
        "title": "LibFuzzer\u2014A Library for Coverage-Guided Fuzz Testing"
      },
      {
        "paperId": "0053d54038537734f15df68a0c6b52aa557f43ee",
        "title": "Reinforcement Learning-based Hierarchical Seed Scheduling for Greybox Fuzzing"
      },
      {
        "paperId": "a9970f6392bd50c986aece522fa2ee095d8d055d",
        "title": "Exploratory Review of Hybrid Fuzzing for Automated Vulnerability Detection"
      },
      {
        "paperId": null,
        "title": "American Fuzzy Lop Plus Plus (AFL++)"
      },
      {
        "paperId": "de4653ccb1712e9cb9d7471e71d33a09e7b8d8f4",
        "title": "EcoFuzz: Adaptive Energy-Saving Greybox Fuzzing as a Variant of the Adversarial Multi-Armed Bandit"
      },
      {
        "paperId": null,
        "title": "\u2018\u2018Quicklygeneratingdiversevalidtestinputswithreinforcementlearning"
      },
      {
        "paperId": "1eb7d7c964a833b70f1f9808ca7f39c71d52b3e8",
        "title": "InsFuzz: Fuzzing Binaries With Location Sensitivity"
      },
      {
        "paperId": null,
        "title": "\u2018\u2018Fuzzing:Stateoftheart,\u2019\u2019"
      },
      {
        "paperId": null,
        "title": "\u2018\u2018OpenAIgym"
      },
      {
        "paperId": null,
        "title": "The model mutates the seed using the action chosen by the RL model and the number of attempts given by the scheduling algorithm, generating a series of test cases"
      },
      {
        "paperId": null,
        "title": "using the chosen input value"
      },
      {
        "paperId": null,
        "title": "the fuzzing tools test the generated inputs with the target program, collecting runtime information (program crashes, execution time, code coverage, etc.)"
      },
      {
        "paperId": null,
        "title": "Using the collected information"
      },
      {
        "paperId": null,
        "title": "The target application is executed with the generated test cases, while the code coverage information is collected"
      },
      {
        "paperId": null,
        "title": "received the degree in information security from the Honor Program, University of Information Technology, Vietnam NationalUniversityHoChiMinhCity(UIT-VNU-HCM),Vietnam"
      }
    ],
    "cited_by": [
      {
        "paperId": "282e61b8ed3381bcc56e2b98af1f07e7998e9752",
        "title": "Semantic-Aware Fuzzing: An Empirical Framework for LLM-Guided, Reasoning-Driven Input Mutation"
      },
      {
        "paperId": "e04d32df0b0ba19c0581be57e5b19858deed41db",
        "title": "Enhancing Software Vulnerability Detection Through Adaptive Test Input Generation Using Genetic Algorithm"
      },
      {
        "paperId": "8cb3e465ff7de2adbe35f1afe4b65854f012fe02",
        "title": "A Reinforcement Learning Approach to Multi-Parametric Input Mutation for Fuzzing"
      },
      {
        "paperId": "cb18a0102c4000052d966a7bb0e0d4c0e04fef40",
        "title": "TraceAwareness and dual-strategy fuzz testing: Enhancing path coverage and crash localization with stochastic science and large language models"
      },
      {
        "paperId": "873da7fb4f09875bdf1574ef49d213dac93fd016",
        "title": "WebFuzzAuto: An Automated Fuzz Testing Tool Integrating Reinforcement Learning and Large Language Models for Web Security"
      }
    ],
    "score": 5.0
  },
  {
    "id": "c1844cda42b3732a5576d05bb6e007eb1db00919",
    "title": "Incremental Reinforcement Learning with Dual-Adaptive \u03b5-Greedy Exploration",
    "authors": [
      "Wei Ding",
      "Siyang Jiang",
      "Hsi-Wen Chen",
      "Ming-Syan Chen"
    ],
    "year": 2023,
    "citationCount": 10,
    "abstract": "Reinforcement learning (RL) has achieved impressive performance in various domains. However, most RL frameworks oversimplify the problem by assuming a fixed-yet-known environment and often have difficulty being generalized to real-world scenarios. In this paper, we address a new challenge with a more realistic setting, Incremental Reinforcement Learning, where the search space of the Markov Decision Process continually expands. While previous methods usually suffer from the lack of efficiency in exploring the unseen transitions, especially with increasing search space, we present a new exploration framework named Dual-Adaptive \u03f5-greedy Exploration (DAE) to address the challenge of Incremental RL. Specifically, DAE employs a Meta Policy and an Explorer to avoid redundant computation on those sufficiently\nlearned samples. Furthermore, we release a testbed based on a synthetic environment and the Atari benchmark to validate the effectiveness of any exploration algorithms under Incremental RL. Experimental results demonstrate that the proposed framework can efficiently learn the unseen transitions in new environments, leading to notable performance improvement, i.e., an average of more than 80%, over eight baselines examined.",
    "url": "https://www.semanticscholar.org/paper/c1844cda42b3732a5576d05bb6e007eb1db00919",
    "pdf_url": "https://doi.org/10.1609/aaai.v37i6.25899",
    "venue": "AAAI Conference on Artificial Intelligence",
    "publicationDate": "2023-06-26",
    "externalIds": {
      "DBLP": "conf/aaai/DingJCC23",
      "DOI": "10.1609/aaai.v37i6.25899",
      "CorpusId": 259680461
    },
    "references": [
      {
        "paperId": "83bfcccc1b9e51591fa64043e2086d740260828c",
        "title": "Control strategies for physically simulated characters performing two-player competitive sports"
      },
      {
        "paperId": "15ad1cb53f907cf54cc4d44f4acdb18286315a6e",
        "title": "Rethinking the Implementation Tricks and Monotonicity Constraint in Cooperative Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "b33b735352c78743707f66e525f7cca65ff207b0",
        "title": "Latent World Models For Intrinsically Motivated Exploration"
      },
      {
        "paperId": "a365d7819697984d8e7e2ae65ec5a1f15f4e3e25",
        "title": "Lifelong Incremental Reinforcement Learning With Online Bayesian Inference"
      },
      {
        "paperId": "c6ba8e55f2fed2037378a94a3f8a2b1b09825636",
        "title": "Temporally-Extended \u03b5-Greedy Exploration"
      },
      {
        "paperId": "eda06f77f146d77c5984db41fcfb7170dc33fad4",
        "title": "Incremental Reinforcement Learning in Continuous Spaces via Policy Relaxation and Importance Weighting"
      },
      {
        "paperId": "00182904badd5a98d55ca9844797b2c0918d0c2f",
        "title": "IRDA: Incremental Reinforcement Learning for Dynamic Resource Allocation"
      },
      {
        "paperId": "9651e1987a39d100dc0f6696a2077199e5075ea2",
        "title": "Agent57: Outperforming the Atari Human Benchmark"
      },
      {
        "paperId": "129983331ca874142a3e8eb2d93d820bdf1f9aca",
        "title": "Deep Reinforcement Learning for Autonomous Driving: A Survey"
      },
      {
        "paperId": "a595767fa35bcc84362f629fbc4d2d9b05d7342a",
        "title": "A survey of deep learning techniques for autonomous driving"
      },
      {
        "paperId": "33a2c0f3b9a0adc452a13d53f950c0c3c4abb11b",
        "title": "Emergent Tool Use From Multi-Agent Autocurricula"
      },
      {
        "paperId": "22ade45a75c1ce8ae63feae8d5381316493cefe8",
        "title": "When to use parametric models in reinforcement learning?"
      },
      {
        "paperId": "a7859b059cfe01d01f1bd795e86eb3f0771fb53b",
        "title": "Model primitives for hierarchical lifelong reinforcement learning"
      },
      {
        "paperId": "1fd4694e7c2d9c872a427d50e81b5475056de6bc",
        "title": "Model-Based Reinforcement Learning for Atari"
      },
      {
        "paperId": "6a36441ea5ce1e00855d06fa637cf8cb72c91ecd",
        "title": "Incremental Reinforcement Learning With Prioritized Sweeping for Dynamic Environments"
      },
      {
        "paperId": "79c8f930bb66c82421b84617e4b6c0b2855cd063",
        "title": "Applications of Deep Reinforcement Learning in Communications and Networking: A Survey"
      },
      {
        "paperId": "4cb3fd057949624aa4f0bbe7a6dcc8777ff04758",
        "title": "Exploration by Random Network Distillation"
      },
      {
        "paperId": "12fcadd19ca4652934997ac537b071a4b6b221d2",
        "title": "Policy and Value Transfer in Lifelong Reinforcement Learning"
      },
      {
        "paperId": "8a5d0579590465494c9aba58a857af43b190b6a6",
        "title": "Deep Learning in Mobile and Wireless Networking: A Survey"
      },
      {
        "paperId": "0ab3f7ecbdc5a33565a234215604a6ca9d155a33",
        "title": "Rainbow: Combining Improvements in Deep Reinforcement Learning"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "3fab4d28be5ff73f42c8b307117aa8d680345ad9",
        "title": "Noisy Networks for Exploration"
      },
      {
        "paperId": "12f67fb182bc934fc95ce97acff553d83e2ca72e",
        "title": "Count-Based Exploration with Neural Density Models"
      },
      {
        "paperId": "0fcb2034e31a2bc2f12a2b1363d0d77baf445fdf",
        "title": "#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning"
      },
      {
        "paperId": "d6f2f611da110b5b5061731be3fc4c7f45d8ee23",
        "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification"
      },
      {
        "paperId": "d163ae2ae7ee2b7991ab017113f13f54fc5c8c5f",
        "title": "PAC-inspired Option Discovery in Lifelong Reinforcement Learning"
      },
      {
        "paperId": "2319a491378867c7049b3da055c5df60e1671158",
        "title": "Playing Atari with Deep Reinforcement Learning"
      },
      {
        "paperId": "f82e4ff4f003581330338aaae71f60316e58dd26",
        "title": "The Arcade Learning Environment: An Evaluation Platform for General Agents"
      },
      {
        "paperId": "237a1cf18ed83bb3ad852b34f443c6c1ff3336c1",
        "title": "An analysis of model-based Interval Estimation for Markov Decision Processes"
      },
      {
        "paperId": "c6e311a9640b47b9264e13de867ab6cc34d195db",
        "title": "Markov Decision Processes"
      },
      {
        "paperId": "7ca8ac34767d6e6cb389eeebcdabc4225b39edfe",
        "title": "Generalization in Reinforcement Learning: Successful Examples Using Sparse Coarse Coding"
      },
      {
        "paperId": "8d15f17ea8f807efe8801d236b7218b6659ac1d9",
        "title": "NovelD: A Simple yet Effective Exploration Criterion"
      },
      {
        "paperId": null,
        "title": "Minimalistic gridworld environment for openai gym"
      },
      {
        "paperId": "a64b5a923b8553897fac7253236ff3f601934af2",
        "title": "Adaptive \u03b5-greedy Exploration in Reinforcement Learning Based on Value Differences"
      },
      {
        "paperId": "97efafdb4a3942ab3efba53ded7413199f79c054",
        "title": "Reinforcement Learning: An Introduction"
      },
      {
        "paperId": "fc1c763627ced7a0670f512cb240721d4e885b71",
        "title": "An approach to Lifelong Reinforcement Learning through Multiple Environments"
      },
      {
        "paperId": "830ccb44084d9d6cdcb70d623df5012ae4835142",
        "title": "Training Stochastic Model Recognition Algorithms as Networks can Lead to Maximum Mutual Information Estimation of Parameters"
      }
    ],
    "cited_by": [
      {
        "paperId": "da5e3e299854419ea47e19a2b82800ee71e927bd",
        "title": "A Survey of Continual Reinforcement Learning"
      },
      {
        "paperId": "86af1f0bece8c264f0d202f4078ea9b8a4aae9e2",
        "title": "Reinforcement learning UAV navigation system based on improved ACO in 3D rescue scene"
      },
      {
        "paperId": "103a8b9743131259c99b8c12350a7d8e694ce5e1",
        "title": "Causality-aware Safety Testing for Autonomous Driving Systems"
      },
      {
        "paperId": "77b46649e90d854b32b1b8335b3207e6f51f7781",
        "title": "Action-Adaptive Continual Learning: Enabling Policy Generalization under Dynamic Action Spaces"
      },
      {
        "paperId": "3ce4defe3c3215623c3c88eb6248831ac5046fa1",
        "title": "Enhance Exploration in Safe Reinforcement Learning with Contrastive Representation Learning"
      },
      {
        "paperId": "c3260cd9717c4f112c00138a1bda68cf809fea24",
        "title": "Repetitive Backdoor Attacks and Countermeasures for Smart Grid Reinforcement Incremental Learning"
      },
      {
        "paperId": "0c87f3536442186b07ac4152896c5a40d9e83f31",
        "title": "Research on UAV Path Planning Algorithm Based on Improved DQN"
      },
      {
        "paperId": "16fa1e8cb87be29309fa3bad5d8a2a67f9cfe824",
        "title": "Rescue path planning for urban flood: A deep reinforcement learning\u2013based approach"
      },
      {
        "paperId": "4ae85a80beb1e174f2a9a64ab762cc3f49f14442",
        "title": "Optimizing Waste Reduction in Manufacturing Processes Utilizing IoT Data with Machine Learning Approach for Sustainable Production"
      },
      {
        "paperId": "b1345574fe4e3b2d8e5e7f572f7feba5e646a5a2",
        "title": "Efficiency Optimization Techniques in Privacy-Preserving Federated Learning With Homomorphic Encryption: A Brief Survey"
      }
    ],
    "score": 5.0
  },
  {
    "id": "8af5e79310ec1d8529eba38705e5f29dce789b00",
    "title": "Advancements in Reinforcement Learning Algorithms for Autonomous Systems",
    "authors": [
      "Jesu Narkarunai Arasu Malaiyappan",
      "Sai Mani Krishna Sistla",
      "Jawaharbabu Jeyaraman"
    ],
    "year": 2024,
    "citationCount": 5,
    "abstract": "Reinforcement learning, often known as RL, has developed as a strong paradigm to teach autonomous software agents to make choices in contexts that are both complicated and dynamic. This abstract investigates recent developments and uses of RL in a variety of fields, showing both its transformational potential and the constraints that it faces at present. Recent developments in reinforcement learning (RL) algorithms, in particular deep reinforcement learning (DRL), have made it possible to make major advancements in autonomous decision- making tasks. DRL algorithms can learn complicated representations of state-action spaces by using deep neural networks. This allows for more efficient exploration and exploitation methods to be implemented. Additionally, advancements in algorithmic enhancements, such as prioritized experience replay and distributional reinforcement learning, have improved the stability and sample efficiency of reinforcement learning algorithms, which has made it possible for these algorithms to be used in real-world applications. Robotics, autonomous cars, game playing, finance, and healthcare are just a few of the many fields that may benefit from the use of RL. In the field of robotics, reinforcement learning (RL) makes it possible for autonomous agents to learn how to navigate, manipulate, and move about in environments that are both complicated and unstructured. To improve both safety and efficiency on the road, autonomous cars make use of reinforcement learning (RL) to make decisions in dynamic traffic situations. In finance, RL algorithms are used for portfolio optimization, algorithmic trading, and risk management. These applications serve to improve investment techniques and decision-making procedures. Furthermore, in the field of healthcare, RL supports individualized treatment planning, clinical decision support, and medical image analysis, which enables physicians to provide patients with care that is specifically suited to their needs. Despite the promising improvements and applications, RL is still confronted with several difficulties that restrict its capacity to be widely adopted and scaled. Among these problems are the inefficiency of the sample, the trade-offs between exploration and exploitation, concerns about safety and dependability, and the need for explainability and interpretability in decision-making processes. To effectively address these difficulties, it is necessary to engage in collaborative efforts across several disciplines, conduct research on algorithmic developments, and establish extensive assessment frameworks (Anon, 2022).",
    "url": "https://www.semanticscholar.org/paper/8af5e79310ec1d8529eba38705e5f29dce789b00",
    "pdf_url": "https://doi.org/10.38124/ijisrt/ijisrt24mar983",
    "venue": "International Journal of Innovative Science and Research Technology",
    "publicationDate": "2024-04-04",
    "externalIds": {
      "DOI": "10.38124/ijisrt/ijisrt24mar983",
      "CorpusId": 268951478
    },
    "references": [
      {
        "paperId": "cf79c3a6249face9e78933981046378c80fde162",
        "title": "UAV Networks Surveillance Implementing an Effective Load-Aware Multipath Routing Protocol (ELAMRP)"
      },
      {
        "paperId": "2547aa2b4b9783de5307880f32053e758b080c57",
        "title": "Investigating the Cases of Measles among Children under 5 Years Old in Abu Ali Sina Balkhi Education Seminary Hospital"
      },
      {
        "paperId": "3998a199ef476377d6ca6b984b712d64a03e31f7",
        "title": "Using Convolutional Neural Network to Design and Predict the Forces and Kinematic Performance and External Rotation Moment of the Hip Joint in the Pelvis"
      },
      {
        "paperId": "594e2663ba038f02651f3de853f1064084f72839",
        "title": "Reinforcement Learning Algorithms: An Overview and Classification"
      },
      {
        "paperId": "bae901b2011089906a0d43446cf92e9891ecf594",
        "title": "Reinforcement learning for robot research: A comprehensive review and open issues"
      },
      {
        "paperId": "e0642272d01afd867c090c7beddd37218616fcfd",
        "title": "Reinforcement learning in robotic applications: a comprehensive survey"
      },
      {
        "paperId": "fac54bfee9d6efd82bef3f62c953f8eaef5fc426",
        "title": "A Survey of Reinforcement Learning Algorithms for Dynamically Varying Environments"
      },
      {
        "paperId": "399dbf5ba3d00f37b8476bef04ad0dee90f85102",
        "title": "Optimal and Autonomous Control Using Reinforcement Learning: A Survey"
      },
      {
        "paperId": null,
        "title": "The Role of Reinforcement Learning in Autonomous Systems |"
      }
    ],
    "cited_by": [
      {
        "paperId": "04a88af48baefde5b2676832553ee9cc064b2bc6",
        "title": "Identification of Required Stations for Autonomous Vehicles using AHP AND TOPSIS Method with GIS Approach"
      },
      {
        "paperId": "eb7db9262a189fa8bb1da878736b030d2a564932",
        "title": "Deep Reinforcement Learning of Mobile Robot Navigation in Dynamic Environment: A Review"
      },
      {
        "paperId": "40e703a2c2e3b324c6ecaf232c120eafc09f133a",
        "title": "Multi-Agent Systems for Autonomous IoT Network Management Using Distributed Reinforcement Learning"
      },
      {
        "paperId": "c2a0916f5c1aae7702746d0ab06f562d1213d4ce",
        "title": "A Comprehensive Survey of Deep Learning Approaches in Image Processing"
      },
      {
        "paperId": "21432c9cae4a62fd6b0293b483b1cd2b16b8dac2",
        "title": "Characteristic of Western and Kannada Absurd Dramas"
      }
    ],
    "score": 5.0
  },
  {
    "id": "5221ba291d5901f950220f50d289d5e01d81b0c4",
    "title": "Digital twin-enabled adaptive scheduling strategy based on deep reinforcement learning",
    "authors": [
      "Xuemei Gan",
      "Ying Zuo",
      "Ansi Zhang",
      "Shaobo Li",
      "Fei Tao"
    ],
    "year": 2023,
    "citationCount": 8,
    "abstract": "",
    "url": "https://www.semanticscholar.org/paper/5221ba291d5901f950220f50d289d5e01d81b0c4",
    "pdf_url": "https://doi.org/10.1007/s11431-022-2413-5",
    "venue": "Science China Technological Sciences",
    "publicationDate": "2023-06-14",
    "externalIds": {
      "DOI": "10.1007/s11431-022-2413-5",
      "CorpusId": 259377502
    },
    "references": [],
    "cited_by": [
      {
        "paperId": "c3ddf69cc6084af6fabeadc674160115ce8fbbcd",
        "title": "Sampling-based particle swarm optimization for dynamic effluent scheduling of wastewater treatment processes"
      },
      {
        "paperId": "e3ff337df8f9988ee4adaf409bbfb453b7428c41",
        "title": "Construction of an Efficient Digital Twin Surrogate Model for Robotic Arms Based on Random Forest"
      },
      {
        "paperId": "a03be8b5d79365f3507dd8532215ab202a3d17a5",
        "title": "From Simulation to Autonomy: Reviews of the Integration of Artificial Intelligence and Digital Twins"
      },
      {
        "paperId": "f3d30cf8d82232da02866d204f462aa654d2f572",
        "title": "Digital twin-based smart shop-floor management and control: A review"
      },
      {
        "paperId": "cbf2e0e3d7b6409dfa9a4ad4f04362bd01c42c7a",
        "title": "Reinforcement learning-assisted particle swarm algorithm for effluent scheduling problem with an influent estimation of WWTP"
      },
      {
        "paperId": "46e0bcc31be4069876dc2849b21214eba339d5c7",
        "title": "Review on ensemble meta-heuristics and reinforcement learning for manufacturing scheduling problems"
      },
      {
        "paperId": "db57e36cc379924ec825f812550e1519a4d3fdcf",
        "title": "Deep reinforcement learning-based dynamic scheduling for resilient and sustainable manufacturing: A systematic review"
      },
      {
        "paperId": "450f5fd2bd55029ac2cdff6a72c14b70f91d2d0e",
        "title": "A literature review of reinforcement learning methods applied to job-shop scheduling problems"
      }
    ],
    "score": 4.0
  },
  {
    "id": "8ae6c7cff8bb2d766f5f9d3585cd262032378b33",
    "title": "Ensemble-based Offline-to-Online Reinforcement Learning: From Pessimistic Learning to Optimistic Exploration",
    "authors": [
      "Kai-Wen Zhao",
      "Yi Ma",
      "Jinyi Liu",
      "Yan Zheng",
      "Zhaopeng Meng"
    ],
    "year": 2023,
    "citationCount": 8,
    "abstract": "",
    "url": "https://www.semanticscholar.org/paper/8ae6c7cff8bb2d766f5f9d3585cd262032378b33",
    "pdf_url": "https://doi.org/10.48550/arXiv.2306.06871",
    "venue": "arXiv.org",
    "publicationDate": "",
    "externalIds": {
      "DBLP": "journals/corr/abs-2306-06871",
      "DOI": "10.48550/arXiv.2306.06871",
      "CorpusId": 263887484
    },
    "references": [],
    "cited_by": [
      {
        "paperId": "c3f2c71e6aa8987b10f8d614ffbc1331b37b68bf",
        "title": "Learning to Trust Bellman Updates: Selective State-Adaptive Regularization for Offline RL"
      },
      {
        "paperId": "e4ba644f7a50bb6ea59be876ccf9c313fdd0539e",
        "title": "Your Offline Policy is Not Trustworthy: Bilevel Reinforcement Learning for Sequential Portfolio Optimization"
      },
      {
        "paperId": "cb52ed456c02472f7de5df58645fde7c2e8e8613",
        "title": "Optimistic Critic Reconstruction and Constrained Fine-Tuning for General Offline-to-Online RL"
      },
      {
        "paperId": "c12379e627f5bfc49e2694ed331a4e906377eaae",
        "title": "vMFER: von Mises-Fisher Experience Resampling Based on Uncertainty of Gradient Directions for Policy Improvement of Actor-Critic Algorithms"
      },
      {
        "paperId": "ee844f841fa9d93f63819c5dc63c314681534a5a",
        "title": "DCAC: Reducing Unnecessary Conservatism in Offline-to-online Reinforcement Learning"
      },
      {
        "paperId": "7c8c126558a986638e0548ad67252936ba496092",
        "title": "Uni-O4: Unifying Online and Offline Deep Reinforcement Learning with Multi-Step On-Policy Optimization"
      },
      {
        "paperId": "64170ce0f75998a1175a2f081238c80539391d29",
        "title": "Improving Offline-to-Online Reinforcement Learning with Q Conditioned State Entropy Exploration"
      },
      {
        "paperId": "65ab05a440e3a68bf1745c7bd680f16d7bcea43e",
        "title": "Towards Robust Offline-to-Online Reinforcement Learning via Uncertainty and Smoothness"
      }
    ],
    "score": 4.0
  },
  {
    "id": "dad0fcbc6f2c9b7c4d7b7b1d4513d94d57ea63c6",
    "title": "Federated Reinforcement Learning for Resource Allocation in V2X Networks",
    "authors": [
      "Kaidi Xu",
      "Shenglong Zhou",
      "Geoffrey Ye Li"
    ],
    "year": 2023,
    "citationCount": 8,
    "abstract": "Resource allocation significantly impacts the performance of vehicle-to-everything (V2X) networks. Most existing algorithms for resource allocation are based on optimization or machine learning (e.g., reinforcement learning). In this paper, we explore resource allocation in a V2X network under the framework of federated reinforcement learning (FRL). On one hand, the usage of RL overcomes many challenges from the model-based optimization schemes. On the other hand, federated learning (FL) enables agents to deal with a number of practical issues, such as privacy, communication overhead, and exploration efficiency. The framework of FRL is then implemented by the in-exact alternative direction method of multipliers (ADMM), where subproblems are solved approximately using policy gradients and their second moments. The developed algorithm, FRLPGiA, has a nice numerical performance compared with some baseline methods for solving the resource allocation problem in a V2X network.",
    "url": "https://www.semanticscholar.org/paper/dad0fcbc6f2c9b7c4d7b7b1d4513d94d57ea63c6",
    "pdf_url": "https://arxiv.org/pdf/2310.09858.pdf",
    "venue": "IEEE Vehicular Technology Conference",
    "publicationDate": "2023-10-15",
    "externalIds": {
      "ArXiv": "2310.09858",
      "DBLP": "conf/vtc/XuZL24",
      "DOI": "10.1109/VTC2024-Spring62846.2024.10683304",
      "CorpusId": 264146087
    },
    "references": [
      {
        "paperId": "976ca13338a670da259f5ceeea2244100607a0d6",
        "title": "Proximal Policy Optimization for RIS-Assisted Full Duplex 6G-V2X Communications"
      },
      {
        "paperId": "1150c99a362fec6d2559f979131319ae071ffc85",
        "title": "Federated Multi-Agent Deep Reinforcement Learning for Resource Allocation of Vehicle-to-Vehicle Communications"
      },
      {
        "paperId": "38f6e52838b725ff8db36b239fc20068417b9985",
        "title": "FedGiA: An Efficient Hybrid Algorithm for Federated Learning"
      },
      {
        "paperId": "c05d18f89932196b907813f3a107792632afda79",
        "title": "Federated Learning Via Inexact ADMM"
      },
      {
        "paperId": "eec0e2c6b3609c8fc4ca7b0288ab72d103d2b442",
        "title": "Dynamic Channel Access and Power Control in Wireless Interference Networks via Multi-Agent Deep Reinforcement Learning"
      },
      {
        "paperId": "b839c5eb7ad65bbde66cf9ad36c8daeced57b440",
        "title": "Communication-Efficient ADMM-based Federated Learning"
      },
      {
        "paperId": "027837300d895ab2f490482085109896b7d016f0",
        "title": "Federated Reinforcement Learning: Techniques, Applications, and Open Challenges"
      },
      {
        "paperId": "766f25e1e6adb4d76403e2a59733257b77d5db12",
        "title": "Global Convergence of Multi-Agent Policy Gradient in Markov Potential Games"
      },
      {
        "paperId": "04952b5658712152ccf1ca3c60e01279ea538a19",
        "title": "Resource Allocation based on Graph Neural Networks in Vehicular Communications"
      },
      {
        "paperId": "c9fbdf5453a7d638622585c3f06e830ee649419a",
        "title": "FedPD: A Federated Learning Framework With Adaptivity to Non-IID Data"
      },
      {
        "paperId": "ae1468354d87c754bfe964c8f07027e23413a9ef",
        "title": "A Survey on Resource Allocation in Vehicular Networks"
      },
      {
        "paperId": "f7f8f05eb2798272fc3a61443d45f2aa47e65135",
        "title": "On the Theory of Policy Gradient Methods: Optimality, Approximation, and Distribution Shift"
      },
      {
        "paperId": "0ac70f97142e121ac13df35f82865a5b817036a7",
        "title": "Spectrum Sharing in Vehicular Networks Based on Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "1284ed4bf6a043ecf8cebca09e4811f1e3b83b65",
        "title": "Federated Optimization in Heterogeneous Networks"
      },
      {
        "paperId": "2b429774b5d867ef60f8b464cfa0b4886ffd77f0",
        "title": "Deep Reinforcement Learning Based Resource Allocation for V2V Communications"
      },
      {
        "paperId": "2392c9c09d401e7318b2b3d3951d8d745eaad763",
        "title": "Federated Learning for Ultra-Reliable Low-Latency V2V Communications"
      },
      {
        "paperId": "cd1e6e795d499477697ed3d28a4b2451b8a553ec",
        "title": "Graph-Based Resource Sharing in Vehicular Communication"
      },
      {
        "paperId": "21ef7e94cf5d4f559f2d4c6c4624b77da9dc86fd",
        "title": "Joint Communication and Control for Wireless Autonomous Vehicular Platoon Systems"
      },
      {
        "paperId": "fa22515106603d01960569b87f2f1ab34c70f270",
        "title": "A Latency and Reliability Guaranteed Resource Allocation Scheme for LTE V2V Communication Systems"
      },
      {
        "paperId": "c4ef2204e341aa47ba9a54b2e5ed5b2bed4e9043",
        "title": "5G for Vehicular Communications"
      },
      {
        "paperId": "6aa61fa7e71bb1a1eed2a279c1ba2123a06cc8b1",
        "title": "Vehicular Communications: A Network Layer Perspective"
      },
      {
        "paperId": "d032321d287327ee737f8987e63810f0eee21a5c",
        "title": "Spectrum and Power Allocation for Vehicular Communications With Delayed CSI Feedback"
      },
      {
        "paperId": "8d58881c343f5698d56a64b3fb0e7211663b8190",
        "title": "Resource Allocation for D2D-Enabled Vehicular Communications"
      },
      {
        "paperId": "63ecd57f88a7cb90d85c8824a0218da4dda0d80a",
        "title": "Dynamic Proximity-Aware Resource Allocation in Vehicle-to-Vehicle (V2V) Communications"
      },
      {
        "paperId": "d1dbf643447405984eeef098b1b320dee0b3b8a7",
        "title": "Communication-Efficient Learning of Deep Networks from Decentralized Data"
      },
      {
        "paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8",
        "title": "Adam: A Method for Stochastic Optimization"
      },
      {
        "paperId": "687d0e59d5c35f022ce4638b3e3a6142068efc94",
        "title": "Deterministic Policy Gradient Algorithms"
      },
      {
        "paperId": "a20f0ce0616def7cc9a87446c228906cd5da093b",
        "title": "Policy Gradient Methods for Reinforcement Learning with Function Approximation"
      },
      {
        "paperId": "af97483e092b3ee09966fc19a07067118c70a51e",
        "title": "Distributed Deep Deterministic Policy Gradient for Power Allocation Control in D2D-Based V2V Communications"
      },
      {
        "paperId": null,
        "title": "Technical Specification Group Radio Access Network"
      },
      {
        "paperId": null,
        "title": "Study on LTE-based V2X services; (Release 14)"
      },
      {
        "paperId": null,
        "title": "agreement with IEEE. Restrictions apply"
      }
    ],
    "cited_by": [
      {
        "paperId": "b11855df7caaed1addd6aa2957a7854a359e7300",
        "title": "Communication resource allocation method in vehicular networks based on federated multi-agent deep reinforcement learning"
      },
      {
        "paperId": "95baed0dc5555adbdec232288ec6a6bfab0c71ce",
        "title": "Federated Multiagent Reinforcement Learning for Resource Allocation in NR-V2X Mode 2"
      },
      {
        "paperId": "27143ebb8f8721ff41af28a609c980f5f71c43ea",
        "title": "Neural Collapse based Deep Supervised Federated Learning for Signal Detection in OFDM Systems"
      },
      {
        "paperId": "5133dc6836def28e8fe3334736a6ffd3aea455e3",
        "title": "Frequency Resource Management in 6G User-Centric CFmMIMO: A Hybrid Reinforcement Learning and Metaheuristic Approach"
      },
      {
        "paperId": "ca6b75ec15d16e963f561b3ec595bd1b149b7382",
        "title": "Optimizing Resource Allocation for QoS and Stability in Dynamic VLC-NOMA Networks via MARL"
      },
      {
        "paperId": "297b7ce3e2f907b28b57dc8300350fa3f8331e52",
        "title": "Preconditioned Inexact Stochastic ADMM for Deep Model"
      },
      {
        "paperId": "d97d4cf22118ca21b707e3c072f6b63bb89a6d4f",
        "title": "A Federated Learning Approach for Self-Parking in V2X Environment by Enhancing Intelligence in Vehicle System"
      },
      {
        "paperId": "66d94ed529cc5a04b7c01d3a23f236a6bd2291f6",
        "title": "BADM: Batch ADMM for Deep Learning"
      }
    ],
    "score": 4.0
  },
  {
    "id": "ae47e4e2a4b749543c4de8624049c1d368cbc859",
    "title": "Reinforcement learning with modified exploration strategy for mobile robot path planning",
    "authors": [
      "Nesrine Khlif",
      "Khraief-Hadded Nahla",
      "Belghith Safya"
    ],
    "year": 2023,
    "citationCount": 7,
    "abstract": "Abstract Driven by the remarkable developments we have observed in recent years, path planning for mobile robots is a difficult part of robot navigation. Artificial intelligence applied to mobile robotics is also a distinct challenge; reinforcement learning (RL) is one of the most used algorithms in robotics. The exploration-exploitation dilemma is a motivating challenge for the performance of RL algorithms. The problem is balancing exploitation and exploration, as too much exploration leads to a decrease in cumulative reward, while too much exploitation locks the agent in a local optimum. This paper proposes a new path planning method for mobile robot based on Q-learning with an improved exploration strategy. In addition, a comparative study of Boltzmann distribution and \n$\\epsilon$\n -greedy politics is presented. Through simulations, the better performance of the proposed method in terms of execution time, path length, and cost function is confirmed.",
    "url": "https://www.semanticscholar.org/paper/ae47e4e2a4b749543c4de8624049c1d368cbc859",
    "pdf_url": "https://doi.org/10.1017/S0263574723000607",
    "venue": "Robotica (Cambridge. Print)",
    "publicationDate": "2023-05-11",
    "externalIds": {
      "DBLP": "journals/robotica/NesrineHB23",
      "DOI": "10.1017/S0263574723000607",
      "CorpusId": 259509978
    },
    "references": [],
    "cited_by": [
      {
        "paperId": "6a6d5444b17c9803c0d3078442f881e533466979",
        "title": "A Hybrid DWA and Multi-Sensors Local Path Planning Approach for Hexapod Robot"
      },
      {
        "paperId": "8503bf22c469bdabc2e59e77f3c6e6ba0195c592",
        "title": "Comparative Analysis of Modified Q-Learning and DQN for Autonomous Robot Navigation"
      },
      {
        "paperId": "0e810de4c9ff3a0aa8041b9607698e4b29b123e3",
        "title": "An End-to-End Path Planner Combining Potential Field Method With Deep Reinforcement Learning"
      },
      {
        "paperId": "5f4bfd34c87192a136a8bad3005e3b065d72bddb",
        "title": "An improved fuzzy inference strategy using reinforcement learning for trajectory-tracking of a mobile robot under a varying slip ratio"
      },
      {
        "paperId": "2102b7d818ca485a9f295a345a59daec58ceba5b",
        "title": "One-shot sim-to-real transfer policy for robotic assembly via reinforcement learning with visual demonstration"
      },
      {
        "paperId": "e8a1fe4dc1aeb48bf04f7b106c006ff451fa0bb9",
        "title": "Path Planning of Materials Distribution Robot for Sports Games Based on Improved Particle Swarm Optimization Algorithm"
      },
      {
        "paperId": "858dd67b19cc2407196d18a3e29ebc87ee354b06",
        "title": "Collaborative Design of Path Planning and Motion Control for Mobile Robots in Complex Environments"
      }
    ],
    "score": 3.5
  },
  {
    "id": "c6ed1f7f478f9d004d5f6b9783df79f82d0d1464",
    "title": "Optimistic Exploration in Reinforcement Learning Using Symbolic Model Estimates",
    "authors": [
      "S. Sreedharan",
      "Michael Katz"
    ],
    "year": 2023,
    "citationCount": 7,
    "abstract": "",
    "url": "https://www.semanticscholar.org/paper/c6ed1f7f478f9d004d5f6b9783df79f82d0d1464",
    "pdf_url": null,
    "venue": "Neural Information Processing Systems",
    "publicationDate": "",
    "externalIds": {
      "DBLP": "conf/nips/SreedharanK23",
      "CorpusId": 268042255
    },
    "references": [],
    "cited_by": [
      {
        "paperId": "987ebf8ea0bfbece991e157d5214a95cb5106120",
        "title": "Language Models For Generalised PDDL Planning: Synthesising Sound and Programmatic Policies"
      },
      {
        "paperId": "3b6a19c2117d1ec0bd15925c1d3f232e6791b1d0",
        "title": "What is behind the curtain? Increasing transparency in reinforcement learning with human preferences and explanations"
      },
      {
        "paperId": "f0e1f3555a1806afa84a7d6db6275d94db73f56b",
        "title": "Neural Fidelity Calibration for Informative Sim-to-Real Adaptation"
      },
      {
        "paperId": "23400ea8f828584be1bc38cfa17abe5e198527a7",
        "title": "Integrating Reinforcement Learning, Action Model Learning, and Numeric Planning for Tackling Complex Tasks"
      },
      {
        "paperId": "2f900a51a104ad026556fe382f673ea20b30f28a",
        "title": "Resiliency Graphs: Modelling the Interplay between Cyber Attacks and System Failures through AI Planning"
      },
      {
        "paperId": "27e085f8475792fc4af836ccf718851a15c767a6",
        "title": "A Survey on Model Repair in AI Planning"
      },
      {
        "paperId": "4eca1b2674b932e044644ac71348f926f7610f40",
        "title": "Epistemic Exploration for Generalizable Planning and Learning in Non-Stationary Settings"
      }
    ],
    "score": 3.5
  },
  {
    "id": "2807f9c666335946113fb11dccadf36f8d78b772",
    "title": "A Simple Unified Uncertainty-Guided Framework for Offline-to-Online Reinforcement Learning",
    "authors": [
      "Siyuan Guo",
      "Yanchao Sun",
      "Jifeng Hu",
      "Sili Huang",
      "Hechang Chen",
      "Haiyin Piao",
      "Lichao Sun",
      "Yi Chang"
    ],
    "year": 2023,
    "citationCount": 7,
    "abstract": "Offline reinforcement learning (RL) provides a promising solution to learning an agent fully relying on a data-driven paradigm. However, constrained by the limited quality of the offline dataset, its performance is often sub-optimal. Therefore, it is desired to further finetune the agent via extra online interactions before deployment. Unfortunately, offline-to-online RL can be challenging due to two main challenges: constrained exploratory behavior and state-action distribution shift. To this end, we propose a Simple Unified uNcertainty-Guided (SUNG) framework, which naturally unifies the solution to both challenges with the tool of uncertainty. Specifically, SUNG quantifies uncertainty via a VAE-based state-action visitation density estimator. To facilitate efficient exploration, SUNG presents a practical optimistic exploration strategy to select informative actions with both high value and high uncertainty. Moreover, SUNG develops an adaptive exploitation method by applying conservative offline RL objectives to high-uncertainty samples and standard online RL objectives to low-uncertainty samples to smoothly bridge offline and online stages. SUNG achieves state-of-the-art online finetuning performance when combined with different offline RL methods, across various environments and datasets in D4RL benchmark.",
    "url": "https://www.semanticscholar.org/paper/2807f9c666335946113fb11dccadf36f8d78b772",
    "pdf_url": "https://arxiv.org/pdf/2306.07541.pdf",
    "venue": "arXiv.org",
    "publicationDate": "2023-06-13",
    "externalIds": {
      "ArXiv": "2306.07541",
      "DBLP": "journals/corr/abs-2306-07541",
      "DOI": "10.48550/arXiv.2306.07541",
      "CorpusId": 259144812
    },
    "references": [
      {
        "paperId": "5cb2c8e049ab1fac04aca46ae3c467b9a4f775ab",
        "title": "Actor-Critic Alignment for Offline-to-Online Reinforcement Learning"
      },
      {
        "paperId": "57ea633772fd7987a1e7a305679759b606f9b4a8",
        "title": "Finetuning from Offline Reinforcement Learning: Challenges, Trade-offs and Practical Solutions"
      },
      {
        "paperId": "18d82f2a4aa1e2c1c4b447876c95b8f7e717e1a1",
        "title": "Offline RL with No OOD Actions: In-Sample Learning via Implicit Value Regularization"
      },
      {
        "paperId": "f6274e6ba614b46d6283c775cfe4565e8ce50bc8",
        "title": "Adaptive Policy Learning for Offline-to-Online Reinforcement Learning"
      },
      {
        "paperId": "a66bb19c36db1a2f400b4c8f90b5da464b6c9e80",
        "title": "User Retention-oriented Recommendation with Decision Transformer"
      },
      {
        "paperId": "f5e1993f3f505e8fbb9cac9231285c8c9f1712a7",
        "title": "Cal-QL: Calibrated Offline RL Pre-Training for Efficient Online Fine-Tuning"
      },
      {
        "paperId": "b6b1b3b9ee2ef36a195b43477ba8bd690c07dd0c",
        "title": "Behavior Proximal Policy Optimization"
      },
      {
        "paperId": "7f270c9b098727675c9d8b893e362b561d61f27e",
        "title": "Policy Expansion for Bridging Offline-to-Online Reinforcement Learning"
      },
      {
        "paperId": "cd7e947c7b9f85ed88f79ef8eb07ee38854f6694",
        "title": "Clinical Decision Transformer: Intended Treatment Recommendation through Goal Prompting"
      },
      {
        "paperId": "811eca8c2e6b25c99f488413dd6993ab3771292e",
        "title": "Adaptive Behavior Cloning Regularization for Stable Offline-to-Online Reinforcement Learning"
      },
      {
        "paperId": "60380ee913d20e722368245f23e0d4baf52e139a",
        "title": "A Policy-Guided Imitation Approach for Offline Reinforcement Learning"
      },
      {
        "paperId": "c97943178542a6191087f317b0d51448d666ee2b",
        "title": "Model-Based Offline Reinforcement Learning with Pessimism-Modulated Dynamics Belief"
      },
      {
        "paperId": "55174eacb397cadee19b3315a06e8b4df4c4cc0a",
        "title": "A Review of Uncertainty for Deep Reinforcement Learning"
      },
      {
        "paperId": "5b93eb7af42d546c9d2d7ac249fee7a2d238df32",
        "title": "Offline Reinforcement Learning at Multiple Frequencies"
      },
      {
        "paperId": "bbae3200de2d742b2bdcecab51f40a8dccb228cb",
        "title": "Challenges and Opportunities in Offline Reinforcement Learning from Visual Observations"
      },
      {
        "paperId": "7e045a7fe78a6c0de5511980f292c42d1055f396",
        "title": "Mildly Conservative Q-Learning for Offline Reinforcement Learning"
      },
      {
        "paperId": "5b353418cf914dccf85cfefcbdda892b600fdc6e",
        "title": "Why So Pessimistic? Estimating Uncertainties for Offline RL through Ensembles, and Why Their Independence Matters"
      },
      {
        "paperId": "23abe79046d7f7b430d5d21b6a93598d0aa1b9c2",
        "title": "Pessimistic Bootstrapping for Uncertainty-Driven Offline Reinforcement Learning"
      },
      {
        "paperId": "d373059e20daa0b4d91ef2a5fdd09d56692e7ca5",
        "title": "Supported Policy Optimization for Offline Reinforcement Learning"
      },
      {
        "paperId": "eb92a453cf982126fa2125d4c8915352a52af54d",
        "title": "Online Decision Transformer"
      },
      {
        "paperId": "15690113ac1a9aed37c53f0196d1c6629d4e7773",
        "title": "MOORe: Model-based Offline-to-Online Reinforcement Learning"
      },
      {
        "paperId": "63afc8d1a187d2f2faf603a51d3987db89574308",
        "title": "RvS: What is Essential for Offline RL via Supervised Learning?"
      },
      {
        "paperId": "348a855fe01f3f4273bf0ecf851ca688686dbfcc",
        "title": "Offline Reinforcement Learning with Implicit Q-Learning"
      },
      {
        "paperId": "9560080a2c32682bd1c1a9850a54ca6163f1956e",
        "title": "Uncertainty-Based Offline Reinforcement Learning with Diversified Q-Ensemble"
      },
      {
        "paperId": "33b456eb43e5391761540f17a29e598d7595565b",
        "title": "Offline-to-Online Reinforcement Learning via Balanced Replay and Pessimistic Q-Ensemble"
      },
      {
        "paperId": "c879b25308026d6538e52b27bcf4fd3cb60855f3",
        "title": "A Minimalist Approach to Offline Reinforcement Learning"
      },
      {
        "paperId": "399806e861a2ef960a81b37b593c2176a728c399",
        "title": "Offline Reinforcement Learning as Anti-Exploration"
      },
      {
        "paperId": "c1ad5f9b32d80f1c65d67894e5b8c2fdf0ae4500",
        "title": "Decision Transformer: Reinforcement Learning via Sequence Modeling"
      },
      {
        "paperId": "2c23085488337c4c1b5673b8d0f4ac95bda73529",
        "title": "Uncertainty Weighted Actor-Critic for Offline Reinforcement Learning"
      },
      {
        "paperId": "362cc80481b288874af0428107ab31e955dcf09f",
        "title": "Offline Reinforcement Learning with Fisher Divergence Critic Regularization"
      },
      {
        "paperId": "324effa5a7c737257b675f85bd93d777fc486878",
        "title": "SUNRISE: A Simple Unified Framework for Ensemble Learning in Deep Reinforcement Learning"
      },
      {
        "paperId": "0272b14dd471fe7b81df703af1b71d7600b77215",
        "title": "Accelerating Online Reinforcement Learning with Offline Datasets"
      },
      {
        "paperId": "28db20a81eec74a50204686c3cf796c42a020d2e",
        "title": "Conservative Q-Learning for Offline Reinforcement Learning"
      },
      {
        "paperId": "dea0f1c5949f8d898b9b6ff68226a781558e413c",
        "title": "MOPO: Model-based Offline Policy Optimization"
      },
      {
        "paperId": "309c2c5ee60e725244da09180f913cd8d4b8d4e9",
        "title": "MOReL : Model-Based Offline Reinforcement Learning"
      },
      {
        "paperId": "5e7bc93622416f14e6948a500278bfbe58cd3890",
        "title": "Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems"
      },
      {
        "paperId": "a326d9f2d2d351001fece788165dbcbb524da2e4",
        "title": "D4RL: Datasets for Deep Data-Driven Reinforcement Learning"
      },
      {
        "paperId": "72973e49f453f678eb0b79b5fa5311b158f3909d",
        "title": "Reinforcement Learning Upside Down: Don't Predict Rewards - Just Map Them to Actions"
      },
      {
        "paperId": "007fd689a806de99f053a0f5ef90aedb44f9ec7e",
        "title": "Better Exploration with Optimistic Actor-Critic"
      },
      {
        "paperId": "329b84a919bfd1771be5bd14fa81e7b3f74cc961",
        "title": "An Introduction to Variational Autoencoders"
      },
      {
        "paperId": "82b4b03a4659d6e04bd7cbf51d6e08fde1348dbd",
        "title": "Stabilizing Off-Policy Q-Learning via Bootstrapping Error Reduction"
      },
      {
        "paperId": "12c0751b4f51ed833172a713b7e32390032ead93",
        "title": "Soft Actor-Critic Algorithms and Applications"
      },
      {
        "paperId": "5285cb8faada5de8a92a47622950f6cfd476ac1d",
        "title": "Off-Policy Deep Reinforcement Learning without Exploration"
      },
      {
        "paperId": "4debb99c0c63bfaa97dd433bc2828e4dac81c48b",
        "title": "Addressing Function Approximation Error in Actor-Critic Methods"
      },
      {
        "paperId": "1485c5ac93b3f01a5f3ef7824eb428c8bff39ba3",
        "title": "UCB EXPLORATION VIA Q-ENSEMBLES"
      },
      {
        "paperId": "484ad17c926292fbe0d5211540832a8c8a8e958b",
        "title": "Stochastic Backpropagation and Approximate Inference in Deep Generative Models"
      },
      {
        "paperId": "5f5dc5b9a2ba710937e2c413b37b053cd673df02",
        "title": "Auto-Encoding Variational Bayes"
      },
      {
        "paperId": "8d2820ac17ff3cedf59f173b16b98872848bf3ad",
        "title": "Exploration-exploitation tradeoff using variance estimates in multi-armed bandits"
      },
      {
        "paperId": "c7dbf5ed7e9b63e104adb4e18bfbc98f5d6afdae",
        "title": "R-MAX - A General Polynomial Time Algorithm for Near-Optimal Reinforcement Learning"
      },
      {
        "paperId": null,
        "title": "Fine-tuning offline policies with optimistic action selection"
      },
      {
        "paperId": null,
        "title": "Exploitation: We replace the proposed adaptive exploitation method by the standard online RL objective of TD3 and SAC, respectively"
      },
      {
        "paperId": null,
        "title": "Analysis on OORB sampling probability p OORB . Furthermore, we give a hyper-parameter analysis on p OORB for the simple-yet-effective OORB. Specifically, we investigate"
      },
      {
        "paperId": null,
        "title": "Estimation w/ Q std: We replace VAE by the standard deviation of two Q value functions for uncertainty estimation, i.e., U(s, a) = \u03c3(Q \u03b81 (s, a), Q \u03b82 (s, a)) = |Q \u03b81 (s, a) \u2212 Q"
      },
      {
        "paperId": null,
        "title": "We do not utilize transitions from offline dataset for online finetuning by removing offline data from OORB"
      },
      {
        "paperId": null,
        "title": "Exploration w/o Unc.: We remove the uncertainty from the proposed exploration strategy to greedily select action that maximizes Q value"
      }
    ],
    "cited_by": [
      {
        "paperId": "046e9c3275b3614ccdb22f9cf1f358e1f09e1220",
        "title": "Offline-to-Online Reinforcement Learning with Classifier-Free Diffusion Generation"
      },
      {
        "paperId": "cb52ed456c02472f7de5df58645fde7c2e8e8613",
        "title": "Optimistic Critic Reconstruction and Constrained Fine-Tuning for General Offline-to-Online RL"
      },
      {
        "paperId": "f545f23405c753f83e3185b99b08111c193ce52a",
        "title": "A Non-Monolithic Policy Approach of Offline-to-Online Reinforcement Learning"
      },
      {
        "paperId": "b2203c382daacadf88c02c0a635c466e777e4ff7",
        "title": "SAMG: Offline-to-Online Reinforcement Learning via State-Action-Conditional Offline Model Guidance"
      },
      {
        "paperId": "3c79194ec98038f3af4c29d67b360bb610e1d996",
        "title": "Decision Mamba: Reinforcement Learning via Hybrid Selective Sequence Modeling"
      },
      {
        "paperId": "7c8c126558a986638e0548ad67252936ba496092",
        "title": "Uni-O4: Unifying Online and Offline Deep Reinforcement Learning with Multi-Step On-Policy Optimization"
      },
      {
        "paperId": "64170ce0f75998a1175a2f081238c80539391d29",
        "title": "Improving Offline-to-Online Reinforcement Learning with Q Conditioned State Entropy Exploration"
      }
    ],
    "score": 3.5
  },
  {
    "id": "9eed36fb9b9bbb97579953d5e303dc0cf6c70a58",
    "title": "Safe Reinforcement Learning With Dead-Ends Avoidance and Recovery",
    "authors": [
      "Xiao Zhang",
      "Hai Zhang",
      "Hongtu Zhou",
      "Chang Huang",
      "Di Zhang",
      "Chen Ye",
      "Junqiao Zhao"
    ],
    "year": 2023,
    "citationCount": 7,
    "abstract": "Safety is one of the main challenges in applying reinforcement learning to tasks in realistic environments. To ensure safety during and after the training process, existing methods tend to adopt overly conservative policies to avoid unsafe situations. However, an overly conservative policy severely hinders exploration. In this letter, we propose a method to construct a boundary that discriminates between safe and unsafe states. The boundary we construct is equivalent to distinguishing dead-end states, indicating the maximum extent to which safe exploration is guaranteed, and thus has a minimum limitation on exploration. Similar to Recovery Reinforcement Learning, we utilize a decoupled RL framework to learn two policies, (1) a task policy that only considers improving the task performance, and (2) a recovery policy that maximizes safety. The recovery policy and a corresponding safety critic are pre-trained on an offline dataset, in which the safety critic evaluates the upper bound of safety in each state as awareness of environmental safety for the agent. During online training, a behavior correction mechanism is adopted, ensuring the agent interacts with the environment using safe actions only. Finally, experiments of continuous control tasks demonstrate that our approach has better task performance with fewer safety violations than state-of-the-art algorithms.",
    "url": "https://www.semanticscholar.org/paper/9eed36fb9b9bbb97579953d5e303dc0cf6c70a58",
    "pdf_url": "https://arxiv.org/pdf/2306.13944.pdf",
    "venue": "IEEE Robotics and Automation Letters",
    "publicationDate": "2023-06-24",
    "externalIds": {
      "ArXiv": "2306.13944",
      "DBLP": "journals/corr/abs-2306-13944",
      "DOI": "10.1109/LRA.2023.3333248",
      "CorpusId": 259251801
    },
    "references": [
      {
        "paperId": "7be08125edabe044657313af94064a0381f7e13f",
        "title": "Dense reinforcement learning for safety validation of autonomous vehicles"
      },
      {
        "paperId": "23b1f60fbf01e4ec9f49b23d5f7d1cdc189d6ded",
        "title": "State-wise Safe Reinforcement Learning: A Survey"
      },
      {
        "paperId": "71446fc2294f5a70de6cf6fba9f8f4af0be4899e",
        "title": "Risk Sensitive Dead-end Identification in Safety-Critical Offline Reinforcement Learning"
      },
      {
        "paperId": "ac3fd058adc8109bcb2666caa6fff8eba425e5f6",
        "title": "Safety Correction from Baseline: Towards the Risk-aware Policy in Robotics via Dual-agent Reinforcement Learning"
      },
      {
        "paperId": "54f2331f75f8330203ef2402efa3102de6bcab99",
        "title": "Safety Guided Policy Optimization"
      },
      {
        "paperId": "d8da5d28807fe5f8af2e6942c67738bd9f12a1ec",
        "title": "Safe Reinforcement Learning by Imagining the Near Future"
      },
      {
        "paperId": "d2f4b49930d3976b29e11886c90a3b824c7d996b",
        "title": "MESA: Offline Meta-RL for Safe Adaptation and Fault Tolerance"
      },
      {
        "paperId": "348a855fe01f3f4273bf0ecf851ca688686dbfcc",
        "title": "Offline Reinforcement Learning with Implicit Q-Learning"
      },
      {
        "paperId": "5897389d1e85d0dd9a97ab24aaaf673775650923",
        "title": "Medical Dead-ends and Learning to Identify High-risk States and Treatments"
      },
      {
        "paperId": "5b2370ebd3439ff60ea64a0c8db88fea2dd86a9c",
        "title": "WCSAC: Worst-Case Soft Actor Critic for Safety-Constrained Reinforcement Learning"
      },
      {
        "paperId": "9e5fe2ba652774ba3b1127f626c192668a907132",
        "title": "Decoupled Exploration and Exploitation Policies for Sample-Efficient Reinforcement Learning"
      },
      {
        "paperId": "2c7d2ffc69e2f172956d63545d13abec4225d16b",
        "title": "Reachability-Based Trajectory Safeguard (RTS): A Safe and Fast Reinforcement Learning Safety Layer for Continuous Control"
      },
      {
        "paperId": "431dc05ac25510de6264084434254cca877f9ab3",
        "title": "Recovery RL: Safe Reinforcement Learning With Learned Recovery Zones"
      },
      {
        "paperId": "96055d058984b15a9b83024bb2e07292ee7559f5",
        "title": "Learning to be Safe: Deep RL with a Safety Critic"
      },
      {
        "paperId": "b3e48660560bde40194d019b18df0b952555b3bf",
        "title": "Safe Reinforcement Learning in Constrained Markov Decision Processes"
      },
      {
        "paperId": "9ec0bd60df62b997bb371e65d254434f8851ea9b",
        "title": "Projection-Based Constrained Policy Optimization"
      },
      {
        "paperId": "003987bfff295e76946bf430376af4fe3d466cb4",
        "title": "Learning to Walk in the Real World with Minimal Human Effort"
      },
      {
        "paperId": "5dce7362bdc0b5ba93f3aac8520c10740e8d0b42",
        "title": "Quantile QT-Opt for Risk-Aware Vision-Based Robotic Grasping"
      },
      {
        "paperId": "9001698e033524864d4d45f051a5ba362d4afd9e",
        "title": "When to Trust Your Model: Model-Based Policy Optimization"
      },
      {
        "paperId": "7f90db07c8ca85657cbc1a92e6e42c0389672e38",
        "title": "Dead-ends and Secure Exploration in Reinforcement Learning"
      },
      {
        "paperId": "babd88f02ff8d131f1907785fc6fd0ff2da4a1e0",
        "title": "2019 International Conference on Robotics and Automation (ICRA)"
      },
      {
        "paperId": "adcd4bfd88213da0c33d3cb7057411dd15b72c7a",
        "title": "End-to-End Safe Reinforcement Learning through Barrier Functions for Safety-Critical Continuous Control Tasks"
      },
      {
        "paperId": "c09a440abdc92d187c53812d8aa29ed7de29a37b",
        "title": "Learning to Drive in a Day"
      },
      {
        "paperId": "811df72e210e20de99719539505da54762a11c6d",
        "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor"
      },
      {
        "paperId": "498238a3bd5fd322fc3ce1572e33bbe3853a356f",
        "title": "Deep Reinforcement Learning: A Brief Survey"
      },
      {
        "paperId": "7a4193d0b042643a8bb9ec262ed7f9d509bdb12e",
        "title": "Constrained Policy Optimization"
      },
      {
        "paperId": "c889d6f98e6d79b89c3a6adf8a921f88fa6ba518",
        "title": "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks"
      },
      {
        "paperId": "340f48901f72278f6bf78a04ee5b01df208cc508",
        "title": "Human-level control through deep reinforcement learning"
      },
      {
        "paperId": "3cc2608fd77b9b65f5bd378e8797b2ab1b8acde7",
        "title": "Constrained Markov Decision Processes"
      },
      {
        "paperId": "3fc2f8f0c508d6bf7acc26ea51c81cb55aa71f96",
        "title": "Decoupling Exploration and Exploitation in Reinforcement Learning"
      },
      {
        "paperId": "4d0f6a6ffcd6ab04732ff76420fd9f8a7bb649c3",
        "title": "Benchmarking Safe Exploration in Deep Reinforcement Learning"
      },
      {
        "paperId": null,
        "title": "license agreement with IEEE. Restrictions apply"
      }
    ],
    "cited_by": [
      {
        "paperId": "50f0c65126cfc65b80bb04575ac64df54bbcafdb",
        "title": "Group Confident Policy Optimization"
      },
      {
        "paperId": "5f148c77abebd10d7a74f56088b378af4be6bd08",
        "title": "Safe Reinforcement Learning with Constraints: A Survey"
      },
      {
        "paperId": "52d7cdc3e8a2f50553d0c4d164800171a29491ed",
        "title": "Adaptive Episode Length Adjustment for Multi-agent Reinforcement Learning"
      },
      {
        "paperId": "e640036ae6eefd8b0ca963b428c6b2054e3f6f0c",
        "title": "DPMambaIR:All-in-One Image Restoration via Degradation-Aware Prompt State Space Model"
      },
      {
        "paperId": "f8cabb9734a7ce9733efbb4c2ac0276de4488901",
        "title": "Dependable Policy Improvement for Intelligent Agents in New Environments"
      },
      {
        "paperId": "1ea63847ffbade109b46ddfd797cc7cd57fb2406",
        "title": "Scrutinize What We Ignore: Reining In Task Representation Shift Of Context-Based Offline Meta Reinforcement Learning"
      },
      {
        "paperId": "c8a228b3bee8c131b06747da8685ef7bf234c7f8",
        "title": "Situation Awareness in AI-Based Technologies and Multimodal Systems: Architectures, Challenges and Applications"
      }
    ],
    "score": 3.5
  },
  {
    "id": "9c9a527ad885e664a31f0b70ec5f838d0c6ebfa6",
    "title": "TA-Explore: Teacher-Assisted Exploration for Facilitating Fast Reinforcement Learning",
    "authors": [
      "Ali Beikmohammadi",
      "S. Magn\u00fasson"
    ],
    "year": 2023,
    "citationCount": 6,
    "abstract": "",
    "url": "https://www.semanticscholar.org/paper/9c9a527ad885e664a31f0b70ec5f838d0c6ebfa6",
    "pdf_url": "https://doi.org/10.5555/3545946.3598951",
    "venue": "Adaptive Agents and Multi-Agent Systems",
    "publicationDate": "",
    "externalIds": {
      "DBLP": "conf/atal/BeikmohammadiM23",
      "DOI": "10.5555/3545946.3598951",
      "CorpusId": 258845327
    },
    "references": [],
    "cited_by": [
      {
        "paperId": "aa52afc6c070817dc11290f906e7476d0a89ec07",
        "title": "Large Language Model Integration with Reinforcement Learning to Augment Decision-Making in Autonomous Cyber Operations"
      },
      {
        "paperId": "dd7530fd9559e897dd34b702146fb32212742e9a",
        "title": "A Comparative Evaluation of Teacher-Guided Reinforcement Learning Techniques for Autonomous Cyber Operations"
      },
      {
        "paperId": "f3070bc3644cf6403f02e736d92aa806aaa6a55e",
        "title": "Collaborative Value Function Estimation Under Model Mismatch: A Federated Temporal Difference Analysis"
      },
      {
        "paperId": "c88b192ee0cde0d720edcd9f9ed944704294b0f7",
        "title": "Compressed Federated Reinforcement Learning with a Generative Model"
      },
      {
        "paperId": "0f05dfec0770b2aefa4b52635bdef23ec9c1083f",
        "title": "Accelerating actor-critic-based algorithms via pseudo-labels derived from prior knowledge"
      },
      {
        "paperId": "66fb5b32220041ab54c39afa38ea199b2cfb1c66",
        "title": "Human-Inspired Framework to Accelerate Reinforcement Learning"
      }
    ],
    "score": 3.0
  },
  {
    "id": "8ca9a74503c240b2746e351995ee0415657f1cd0",
    "title": "Reinforcement Learning by Guided Safe Exploration",
    "authors": [
      "Qisong Yang",
      "T. D. Sim\u00e3o",
      "N. Jansen",
      "Simon Tindemans",
      "M. Spaan"
    ],
    "year": 2023,
    "citationCount": 6,
    "abstract": "Safety is critical to broadening the application of reinforcement learning (RL). Often, we train RL agents in a controlled environment, such as a laboratory, before deploying them in the real world. However, the real-world target task might be unknown prior to deployment. Reward-free RL trains an agent without the reward to adapt quickly once the reward is revealed. We consider the constrained reward-free setting, where an agent (the guide) learns to explore safely without the reward signal. This agent is trained in a controlled environment, which allows unsafe interactions and still provides the safety signal. After the target task is revealed, safety violations are not allowed anymore. Thus, the guide is leveraged to compose a safe behaviour policy. Drawing from transfer learning, we also regularize a target policy (the student) towards the guide while the student is unreliable and gradually eliminate the influence of the guide as training progresses. The empirical analysis shows that this method can achieve safe transfer learning and helps the student solve the target task faster.",
    "url": "https://www.semanticscholar.org/paper/8ca9a74503c240b2746e351995ee0415657f1cd0",
    "pdf_url": "https://arxiv.org/pdf/2307.14316.pdf",
    "venue": "European Conference on Artificial Intelligence",
    "publicationDate": "2023-07-26",
    "externalIds": {
      "DBLP": "conf/ecai/YangSJTS23",
      "ArXiv": "2307.14316",
      "DOI": "10.3233/FAIA230598",
      "CorpusId": 260164670
    },
    "references": [
      {
        "paperId": "0a806876e419de58b650aacbf218e23f589327ad",
        "title": "CEM: Constrained Entropy Maximization for Task-Agnostic Safe Exploration"
      },
      {
        "paperId": "2eced32191850ddfbdbc4ef9e2a8059e99e7f84a",
        "title": "Safe Reinforcement Learning via Shielding under Partial Observability"
      },
      {
        "paperId": "013f90498dfed8ee6ed8a45d448664c84c6cdd56",
        "title": "Recommender systems under European AI regulations"
      },
      {
        "paperId": "2ecd943f7d2f147470577a2ae6fb00af2ea8325d",
        "title": "Curriculum learning for safe mapless navigation"
      },
      {
        "paperId": "d2f4b49930d3976b29e11886c90a3b824c7d996b",
        "title": "MESA: Offline Meta-RL for Safe Adaptation and Fault Tolerance"
      },
      {
        "paperId": "d2e3b134d0a9ef794bf02aab84cedd1f2e77950a",
        "title": "Maximum Entropy Model-based Reinforcement Learning"
      },
      {
        "paperId": "d49eb322bc426dde48ac5a7973fad0b6513ee395",
        "title": "Safe Driving via Expert Guided Policy Optimization"
      },
      {
        "paperId": "2957c024af67935ec18ff6a94e451115eeb8d98e",
        "title": "A Simple Reward-free Approach to Constrained Reinforcement Learning"
      },
      {
        "paperId": "9333138ad1d0ef3778bae9ae517aaf396de1b287",
        "title": "Density Constrained Reinforcement Learning"
      },
      {
        "paperId": "032731295fb9434a82b31fdb5e50309a2cbfef3d",
        "title": "Discovering Diverse Nearly Optimal Policies withSuccessor Features"
      },
      {
        "paperId": "daa02add19d5951a447a5d452c540c93b9d029af",
        "title": "Risk-Aware Transfer in Reinforcement Learning using Successor Features"
      },
      {
        "paperId": "5b2370ebd3439ff60ea64a0c8db88fea2dd86a9c",
        "title": "WCSAC: Worst-Case Soft Actor Critic for Safety-Constrained Reinforcement Learning"
      },
      {
        "paperId": "5f1adc14a77fb61aa463fac728397bd32e00b617",
        "title": "Challenges of real-world reinforcement learning: definitions, benchmarks and analysis"
      },
      {
        "paperId": "068660cdd795e20177a9ea7314e103e740f85e5c",
        "title": "Safe Continuous Control with Constrained Model-Based Policy Optimization"
      },
      {
        "paperId": "b284afe9a7363b898661c9b3cfb7f015b158cc63",
        "title": "Maximum Entropy RL (Provably) Solves Some Robust RL Problems"
      },
      {
        "paperId": "7fc04f8031598da6510bce4a7e5b4722305ca5b0",
        "title": "State Entropy Maximization with Random Encoders for Efficient Exploration"
      },
      {
        "paperId": "8214ce2a11fed3aa21efec9b5f329bb99fbde0ce",
        "title": "Multiple Plans are Better than One: Diverse Stochastic Planning"
      },
      {
        "paperId": "431dc05ac25510de6264084434254cca877f9ab3",
        "title": "Recovery RL: Safe Reinforcement Learning With Learned Recovery Zones"
      },
      {
        "paperId": "e4a46c64aafbef0406e9cfa90dd9c43e3e07598c",
        "title": "One Solution is Not All You Need: Few-Shot Extrapolation via Structured MaxEnt RL"
      },
      {
        "paperId": "96055d058984b15a9b83024bb2e07292ee7559f5",
        "title": "Learning to be Safe: Deep RL with a Safety Critic"
      },
      {
        "paperId": "f8492a321d66c381637b693a24af994af41b3cdf",
        "title": "Transfer Learning in Deep Reinforcement Learning: A Survey"
      },
      {
        "paperId": "a6d81aebe28939e3d371a29d1d95e7a29634ec6a",
        "title": "Safe Active Dynamics Learning and Control: A Sequential Exploration\u2013Exploitation Framework"
      },
      {
        "paperId": "3fd3f5e0780650610369d9d5bd096ad153b08228",
        "title": "Cautious Adaptation For Reinforcement Learning in Safety-Critical Settings"
      },
      {
        "paperId": "d75cf795676f10168853e443d1ca7a3c663e4e9e",
        "title": "Safe Reinforcement Learning via Curriculum Induction"
      },
      {
        "paperId": "0d1654fb6446b86f5b88b28aa01c0d5e1a37d0e7",
        "title": "Accelerating Safe Reinforcement Learning with Constraint-mismatched Baseline Policies"
      },
      {
        "paperId": "90974d9e0df8466a50338601e839fa0ea69c9872",
        "title": "Transient Non-stationarity and Generalisation in Deep Reinforcement Learning"
      },
      {
        "paperId": "c5fd78bec0b8230da9f2f01ab1abce787d62de80",
        "title": "Safe Reinforcement Learning through Meta-learned Instincts"
      },
      {
        "paperId": "9ec0bd60df62b997bb371e65d254434f8851ea9b",
        "title": "Projection-Based Constrained Policy Optimization"
      },
      {
        "paperId": "003987bfff295e76946bf430376af4fe3d466cb4",
        "title": "Learning to Walk in the Real World with Minimal Human Effort"
      },
      {
        "paperId": "8b559ea2c4abe9bbca4b5d2820dbb9cbeb37d510",
        "title": "Marginalized State Distribution Entropy Regularization in Policy Optimization"
      },
      {
        "paperId": "a15aafd000e1c350a198e06dc846eadea7413e40",
        "title": "Learning Transferable Domain Priors for Safe Exploration in Reinforcement Learning"
      },
      {
        "paperId": "cfec54f9208dcbb2c558aa7ff45f57be2533d046",
        "title": "Efficient Exploration via State Marginal Matching"
      },
      {
        "paperId": "801eb90352e603b1573a04a36804417d1c9496f0",
        "title": "Learning latent state representation for speeding up exploration"
      },
      {
        "paperId": "12c0751b4f51ed833172a713b7e32390032ead93",
        "title": "Soft Actor-Critic Algorithms and Applications"
      },
      {
        "paperId": "adab275554eb745cdc21fd6c526ec55a5b2ef362",
        "title": "Provably Efficient Maximum Entropy Exploration"
      },
      {
        "paperId": "3d789390b768ba6fb264216dacf0ea0e5e7c47c1",
        "title": "Entropy Maximization for Constrained Markov Decision Processes"
      },
      {
        "paperId": "cb7c479a36520da1caeeec67db10772351a390c6",
        "title": "Reward Constrained Policy Optimization"
      },
      {
        "paperId": "5b01eaef54a653ba03ddd5a978690380fbc19bfc",
        "title": "Diversity is All You Need: Learning Skills without a Reward Function"
      },
      {
        "paperId": "811df72e210e20de99719539505da54762a11c6d",
        "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor"
      },
      {
        "paperId": "e4b4066e46fd160cb84e246b0895a4cd674d8ce4",
        "title": "Safe Reinforcement Learning via Shielding"
      },
      {
        "paperId": "7a4193d0b042643a8bb9ec262ed7f9d509bdb12e",
        "title": "Constrained Policy Optimization"
      },
      {
        "paperId": "c889d6f98e6d79b89c3a6adf8a921f88fa6ba518",
        "title": "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks"
      },
      {
        "paperId": "9172cd6c253edf7c3a1568e03577db20648ad0c4",
        "title": "Reinforcement Learning with Deep Energy-Based Policies"
      },
      {
        "paperId": "3ab308a469439917204c5e9eab833e0f5f2f74da",
        "title": "Near Optimal Behavior via Approximate State Abstraction"
      },
      {
        "paperId": "759bbd8dd50cb4790cad7a3bccbdfcbfee5e3e89",
        "title": "Risk-Constrained Reinforcement Learning with Percentile Risk Criteria"
      },
      {
        "paperId": "da3e22c1f3cd7f7f68421f7295e56cfa0642baf4",
        "title": "Safe Exploration for Optimization with Gaussian Processes"
      },
      {
        "paperId": "d6cc19f33b7714de62e45295c8be1bf1b0642557",
        "title": "An Emphatic Approach to the Problem of Off-policy Temporal-Difference Learning"
      },
      {
        "paperId": "340f48901f72278f6bf78a04ee5b01df208cc508",
        "title": "Human-level control through deep reinforcement learning"
      },
      {
        "paperId": "16c248bec30bf7125b86527aded29a2fae7b9618",
        "title": "The design of LEO: A 2D bipedal walking robot for online autonomous Reinforcement Learning"
      },
      {
        "paperId": "467568f1777bc51a15a5100516cd4fe8de62b9ab",
        "title": "Transfer Learning for Reinforcement Learning Domains: A Survey"
      },
      {
        "paperId": "8de174ab5419b9d3127695405efd079808e956e8",
        "title": "Curriculum learning"
      },
      {
        "paperId": "3f08e2ce1e7440440aecab0d732433e40e5b28fd",
        "title": "An actor-critic algorithm for constrained Markov decision processes"
      },
      {
        "paperId": "3cc2608fd77b9b65f5bd378e8797b2ab1b8acde7",
        "title": "Constrained Markov Decision Processes"
      },
      {
        "paperId": "1cc88490a889a06c6468b6fc5603d26bbdc2836a",
        "title": "AlwaysSafe: Reinforcement Learning without Safety Constraint Violations during Training"
      },
      {
        "paperId": "b5583d080ef9f94cef1a2ba0997637833952b5dd",
        "title": "Safe Reinforcement Learning Using Probabilistic Shields (Invited Paper)"
      },
      {
        "paperId": "719068eb8b8c9ab8552ec3e82c1b1088a9eacdce",
        "title": "Learning Locomotion Skills for Cassie: Iterative Design and Sim-to-Real"
      },
      {
        "paperId": "4d0f6a6ffcd6ab04732ff76420fd9f8a7bb649c3",
        "title": "Benchmarking Safe Exploration in Deep Reinforcement Learning"
      },
      {
        "paperId": "c0f2c4104ef6e36bb67022001179887e6600d24d",
        "title": "A comprehensive survey on safe reinforcement learning"
      },
      {
        "paperId": "2a65434d43ffa6554eaf14b728780919ad4f33eb",
        "title": "Modeling Purposeful Adaptive Behavior with the Principle of Maximum Causal Entropy"
      },
      {
        "paperId": "ca9a2d326b9de48c095a6cb5912e1990d2c5ab46",
        "title": "Towards a Unified Theory of State Abstraction for MDPs"
      },
      {
        "paperId": "97efafdb4a3942ab3efba53ded7413199f79c054",
        "title": "Reinforcement Learning: An Introduction"
      },
      {
        "paperId": null,
        "title": "Constrained Optimization and Lagrange Multi-plier Methods , volume 1"
      }
    ],
    "cited_by": [
      {
        "paperId": "6ef14ea344cddb2a9576be09e5c6edbc8e49fb9b",
        "title": "Human-aligned Safe Reinforcement Learning for Highway On-Ramp Merging in Dense Traffic"
      },
      {
        "paperId": "ea2f030235313983c283c137a9911624a20bca42",
        "title": "Safe Multiagent Coordination via Entropic Exploration"
      },
      {
        "paperId": "7e742a7fb6affe5a67a443adfcd3d0dc3f90cd58",
        "title": "Model Checking for Reinforcement Learning in Autonomous Driving: One Can Do More Than You Think!"
      },
      {
        "paperId": "4998653d2c3ed0baa41c9fc87bd9a5c5662319cc",
        "title": "CommonUppRoad: A Framework of Formal Modelling, Verifying, Learning, and Visualisation of Autonomous Vehicles"
      },
      {
        "paperId": "e9a29ebd09aa2daf7b93bea0753330276c62945f",
        "title": "Making Reinforcement Learning Safer via Curriculum Learning"
      },
      {
        "paperId": "19b9484e309f368354eff42b97c6ccc6ebd448e3",
        "title": "Robustifying RL Agents for Safe Transfer through Action Disturbances"
      }
    ],
    "score": 3.0
  },
  {
    "id": "06ffbf40c29daa4f6b4c5025d96ab1c27c0ea117",
    "title": "Forest Fire Localization: From Reinforcement Learning Exploration to a Dynamic Drone Control",
    "authors": [
      "Jonatan Alvarez",
      "Assia Belbachir",
      "Faiza Belbachir",
      "Jamy Chahal",
      "Abdelhak Goudjil",
      "Johvany Gustave",
      "Ayb\u00fcke \u00d6zt\u00fcrk Suri"
    ],
    "year": 2023,
    "citationCount": 6,
    "abstract": "",
    "url": "https://www.semanticscholar.org/paper/06ffbf40c29daa4f6b4c5025d96ab1c27c0ea117",
    "pdf_url": "https://doi.org/10.1007/s10846-023-02004-z",
    "venue": "Journal of Intelligent and Robotic Systems",
    "publicationDate": "2023-11-30",
    "externalIds": {
      "DBLP": "journals/jirs/AlvarezBBCGGS23",
      "DOI": "10.1007/s10846-023-02004-z",
      "CorpusId": 265500969
    },
    "references": [],
    "cited_by": [
      {
        "paperId": "993e9195e1687f9622e6cd810440aec4fd1ca8e8",
        "title": "Deep reinforcement learning enhanced PID control for hydraulic servo systems in injection molding machines"
      },
      {
        "paperId": "38a7549d8326faaf5679d5ca57dcf58f250ce831",
        "title": "COGNIMAN Digital Twin Architecture for Flexible Manufacturing"
      },
      {
        "paperId": "f8d963a9defec94e3f840805d88f827e6b267a46",
        "title": "Study on the Combustion Characteristics of Forest Litter and the Impact of Combustion Slope in the Southwest China\u2019s Forest Regions"
      },
      {
        "paperId": "ec0ec54b7d2e30b61b7e4506c3b16088c067e13c",
        "title": "Curriculum-based Sample Efficient Reinforcement Learning for Robust Stabilization of a Quadrotor"
      },
      {
        "paperId": "0f604e86875c498d2d7aaad07b7d9f113badbc7c",
        "title": "FollowThePathNet: UAVs Use Neural Networks to Follow Paths in GPS-Denied Environments"
      },
      {
        "paperId": "60300acdffbfd184af97670e7f951b41932e9246",
        "title": "Deep Reinforcement Learning Unmanned Aerial Vehicle Autonomous Cruise System with Fusion of Visual Information"
      }
    ],
    "score": 3.0
  },
  {
    "id": "7f437f4af59ff994d97482ee1c12aaeb4b310e85",
    "title": "Implicit Posteriori Parameter Distribution Optimization in Reinforcement Learning",
    "authors": [
      "Tianyi Li",
      "Gen-ke Yang",
      "Jian Chu"
    ],
    "year": 2023,
    "citationCount": 6,
    "abstract": "Efficient and intelligent exploration remains a major challenge in the field of deep reinforcement learning (DRL). Bayesian inference with a distributional representation is usually an effective way to improve the exploration ability of the RL agent. However, when optimizing Bayesian neural networks (BNNs), most algorithms need to specify an explicit parameter distribution such as a multivariate Gaussian distribution. This may reduce the flexibility of model representation and affect the algorithm performance. Therefore, to improve sample efficiency and exploration based on Bayesian methods, we propose a novel implicit posteriori parameter distribution optimization (IPPDO) algorithm. First, we adopt a distributional perspective on the parameter and model it with an implicit distribution, which is approximated by generative models. Each model corresponds to a learned latent space, providing structured stochasticity for each layer in the network. Next, to make it possible to optimize an implicit posteriori parameter distribution, we build an energy-based model (EBM) with value function to represent the implicit distribution which is not constrained by any analytic density function. Then, we design a training algorithm based on amortized Stein variational gradient descent (SVGD) to improve the model learning efficiency. We compare IPPDO with other prevailing DRL algorithms on the OpenAI Gym, MuJoCo, and Box2D platforms. Experiments on various tasks demonstrate that the proposed algorithm can represent the parameter uncertainty implicitly for a learned policy and can consistently outperform competing approaches.",
    "url": "https://www.semanticscholar.org/paper/7f437f4af59ff994d97482ee1c12aaeb4b310e85",
    "pdf_url": "https://doi.org/10.1109/TCYB.2023.3254596",
    "venue": "IEEE Transactions on Cybernetics",
    "publicationDate": "2023-03-22",
    "externalIds": {
      "DBLP": "journals/tcyb/LiYC24",
      "DOI": "10.1109/TCYB.2023.3254596",
      "CorpusId": 257708080,
      "PubMed": "37030741"
    },
    "references": [],
    "cited_by": [
      {
        "paperId": "6f52b7230b06475ecfd8bd6c0d458adc8f624ce3",
        "title": "An effective exploration method based on N-step updated Dirichlet distribution and Dempster-Shafer theory for deep reinforcement learning"
      },
      {
        "paperId": "81a5ef40de4cb64267a3a408675d078efe4d4b63",
        "title": "Balancing State Exploration and Skill Diversity in Unsupervised Skill Discovery"
      },
      {
        "paperId": "013cccafad8e65ebbba56aac1479cc9d0792a79f",
        "title": "Characterization of aroma components and establishment of a year prediction model for Tuo teas stored for 2-10 years based on multispectral analysis and chemometrics"
      },
      {
        "paperId": "ce37384218421d884f0aff8d3ca0242fbbda25fd",
        "title": "ComSD: Balancing Behavioral Quality and Diversity in Unsupervised Skill Discovery"
      },
      {
        "paperId": "da2ff5e722cd0cf039735cc42256e2d2f7199cba",
        "title": "Sequential Action-Induced Invariant Representation for Reinforcement Learning"
      },
      {
        "paperId": "8bfa83f14d15918e855d35656cfd410bcf6d34ce",
        "title": "Implicit\u2013Explicit Collaborative Cross-Stitch Network With Multitask Disentanglement Learning for Fault Diagnosis and Size Estimation"
      }
    ],
    "score": 3.0
  },
  {
    "id": "839395c4823ac8fff990485e7ce54e53c94bae6b",
    "title": "Active Gamma-Ray Log Pattern Localization With Distributionally Robust Reinforcement Learning",
    "authors": [
      "Yuan Zi",
      "Lei Fan",
      "Xuqing Wu",
      "Jiefu Chen",
      "Shirui Wang",
      "Zhu Han"
    ],
    "year": 2023,
    "citationCount": 5,
    "abstract": "Accurately localizing 1-D signal patterns, such as Gamma-ray well-log depth matching, is crucial in the oilfield service industry as it directly affects the quality of oil and gas exploration. However, traditional methods such as well-log curve analysis and pattern hand-picking matching are labor-intensive and heavily rely on human expertise, leading to inconsistent results. Although attempts have been made to automate this process, challenges such as low computational performance, nonrobustness, and nongeneralization remain unsolved. To address these challenges, we have developed a data-driven AI system that learns an active signal pattern localization strategy inspired by human attention. Our artificial intelligence system uses an offline reinforcement learning (RL) framework as its central component, which solves a highly abstracted Markov decision process (MDP) problem via offline training on human-labeled historical data. The RL agent uses top-down reasoning to determine the location of target signal fragments by deforming a bounding window using simple transformation actions. To overcome distribution shifts between logged data and real and ensure generalization, we propose a discrete distributionally robust soft actor-critic (SAC) RL framework (DRSAC-Discrete) to solve the MDP problem under uncertainty. By exploring unfamiliar environments in a restrictive manner, the DRSAC-Discrete algorithm provides a safe solution that can be used when data is limited during the early stage of this industrial application. We evaluated the RL-based localization system on augmented field Gamma-ray well-log datasets, and the results showed promising localization capability. Furthermore, the DRSAC-Discrete algorithm demonstrated relatively robust performance guarantees when facing data shortage.",
    "url": "https://www.semanticscholar.org/paper/839395c4823ac8fff990485e7ce54e53c94bae6b",
    "pdf_url": "https://doi.org/10.1109/TGRS.2023.3278491",
    "venue": "IEEE Transactions on Geoscience and Remote Sensing",
    "publicationDate": "",
    "externalIds": {
      "DBLP": "journals/tgrs/ZiF0CW023",
      "DOI": "10.1109/TGRS.2023.3278491",
      "CorpusId": 258997612
    },
    "references": [],
    "cited_by": [
      {
        "paperId": "ab7ceb6f6cdcb61bc7170bd328d0834dd6ae9d62",
        "title": "CloudRuler: Rule-based transformer for cloud removal in Landsat images"
      },
      {
        "paperId": "69163d7e817f3983bccd905980db8a082ceaec9e",
        "title": "An intelligent log-seismic integrated stratigraphic correlation method based on wavelet frequency-division transform and dynamic time warping: A case study from the Lasaxing oilfield"
      },
      {
        "paperId": "2a3f862468cddc22acac12a238b79c3c9f667cb0",
        "title": "Automatic Well-Log Depth Shift with Multilevel Wavelet Decomposition Network and Dynamic Time Warping"
      },
      {
        "paperId": "da90a2636ec34d1cc99d357584d94e426368ce78",
        "title": "Automatic depth matching method of well log based on deep reinforcement learning"
      },
      {
        "paperId": "bc37bfca66f758c1d359d7b9494a1bc47aa30591",
        "title": "Inversion of Radial Shear Velocity Profile for Acoustic Logging Using CNN-LSTM Network"
      }
    ],
    "score": 2.5
  },
  {
    "id": "7825ea27ec1762f6ac41347603535500bcd121f7",
    "title": "Reducing the Learning Time of Reinforcement Learning for the Supervisory Control of Discrete Event Systems",
    "authors": [
      "Junjun Yang",
      "Kaige Tan",
      "Lei Feng",
      "Ahmed M. El-Sherbeeny",
      "Zhiwu Li"
    ],
    "year": 2023,
    "citationCount": 5,
    "abstract": "Reinforcement learning (RL) can obtain the supervisory controller for discrete-event systems modeled by finite automata and temporal logic. The published methods often have two limitations. First, a large number of training data are required to learn the RL controller. Second, the RL algorithms do not consider uncontrollable events, which are essential for supervisory control theory (SCT). To address the limitations, we first apply SCT to find the supervisors for the specifications modeled by automata. These supervisors remove illegal training data violating these specifications and hence reduce the exploration space of the RL algorithm. For the remaining specifications modeled by temporal logic, the RL algorithm is applied to search for the optimal control decision within the confined exploration space. Uncontrollable events are considered by the RL algorithm as uncertainties in the plant model. The proposed method can obtain a nonblocking supervisor for all specifications with less learning time than the published methods.",
    "url": "https://www.semanticscholar.org/paper/7825ea27ec1762f6ac41347603535500bcd121f7",
    "pdf_url": "https://doi.org/10.1109/ACCESS.2023.3285432",
    "venue": "IEEE Access",
    "publicationDate": "",
    "externalIds": {
      "DBLP": "journals/access/YangTFEL23",
      "DOI": "10.1109/ACCESS.2023.3285432",
      "CorpusId": 259235535
    },
    "references": [
      {
        "paperId": "4035e9cc3524cf9647aec19207e846c242cf8e61",
        "title": "Multi-Agent Optimal Control for Central Chiller Plants Using Reinforcement Learning and Game Theory"
      },
      {
        "paperId": "53d8d93b5527f38784ebb3bb854880bddc213afa",
        "title": "A model-based deep reinforcement learning approach to the nonblocking coordination of modular supervisors of discrete event systems"
      },
      {
        "paperId": "f269d1dfc442bfcaff6b43d92ca9f0bf14a7262b",
        "title": "Real-Time Scheduling of Pumps in Water Distribution Systems Based on Exploration-Enhanced Deep Reinforcement Learning"
      },
      {
        "paperId": "0a8890afdca4602b0695e393782e25eb43c967a0",
        "title": "Deliberative/Reactive Architecture of a Multirobot Patrol System Based on Supervisory Control Theory"
      },
      {
        "paperId": "c4e7357c8be208919e3f8a2523fba569089bb045",
        "title": "Active opacity of discrete-event systems"
      },
      {
        "paperId": "d6640d882918173aba5a9c210eb420c2a45a600f",
        "title": "Flexible control of Discrete Event Systems using environment simulation and Reinforcement Learning"
      },
      {
        "paperId": "501b05250a080b18ad7d0ccc85c993053240657a",
        "title": "Expandable-Partially Observable Markov Decision-Process Framework for Modeling and Analysis of Autonomous Vehicle Behavior"
      },
      {
        "paperId": "fcfb5c350263d6a818daf803ded50ee0ee00feb5",
        "title": "Model abstraction for discrete-event systems by binary linear programming with applications to manufacturing systems"
      },
      {
        "paperId": "fb6e8ec9b17300e2556f553d926f973b16ec37fb",
        "title": "Bellman's principle of optimality and deep reinforcement learning for time-varying tasks"
      },
      {
        "paperId": "c29ec620c26f82ff2a2224e2f9799518d670c68a",
        "title": "Coupled Multi-Robot Systems Under Linear Temporal Logic and Signal Temporal Logic Tasks"
      },
      {
        "paperId": "c7fb785b402b20072d0f7a09bab8014409fefbe7",
        "title": "Modular Deep Reinforcement Learning for Continuous Motion Planning With Temporal Logic"
      },
      {
        "paperId": "1a24a49e5eb3fbb3be716458519fa2d9aa605194",
        "title": "Robust corrective control against a class of actuator attacks in input/state asynchronous sequential machines"
      },
      {
        "paperId": "824980a101dee215c7c9259bc7cd821e8dd9b0bd",
        "title": "Response time evaluation of industrial-scale distributed control systems by discrete event systems formalisms"
      },
      {
        "paperId": "e74016393d02d14da19da2fb0bcb7183b83aa848",
        "title": "Resilient extended dissipative control for Markovian jump systems with partially known transition probabilities under actuator saturation"
      },
      {
        "paperId": "6b03c274ce4768b8a6eedaa1d65166d913e09e30",
        "title": "On-Line Permissive Supervisory Control of Discrete Event Systems for scLTL Specifications"
      },
      {
        "paperId": "edb3fcbc394d3444a51cd2165155c89faf585309",
        "title": "Approximate Automata for Omega-Regular Languages"
      },
      {
        "paperId": "3ee06b4f0ed488bbffc84e441cd66372ee5de0e8",
        "title": "Reinforcement Learning for Temporal Logic Control Synthesis with Probabilistic Satisfaction Guarantees"
      },
      {
        "paperId": "c88d5b4f62832f2bc02dc696964b736e196915c9",
        "title": "Reduced variance deep reinforcement learning with temporal logic specifications"
      },
      {
        "paperId": "fd0e3da7d5dbc60c282545be4c0b37363ea2b6c8",
        "title": "A novel multi-step reinforcement learning method for solving reward hacking"
      },
      {
        "paperId": "5e85352a721a8505f1ea054956e1996829c5ad1e",
        "title": "Discrete Event System Specification Framework for Self-Improving Healthcare Service Systems"
      },
      {
        "paperId": "c9296f6d20f18d8149fe52ec277155d011e1dfaf",
        "title": "Hierarchical Control of Concurrent Discrete Event Systems with Linear Temporal Logic Specifications"
      },
      {
        "paperId": "dc7c12045e062288297cc21a8db5a98f5497ccea",
        "title": "Formal Methods for Control of Traffic Flow: Automated Control Synthesis from Finite-State Transition Models"
      },
      {
        "paperId": "1a8ea33f2db68c8e68808732cf07ea7a142a9644",
        "title": "Optimal Control of Nonlinear Systems with Temporal Logic Specifications"
      },
      {
        "paperId": "f0ffa89f05cd7956e5a2bf8382aaa5cc44d00908",
        "title": "Decomposition of multi-agent planning under distributed motion and task LTL specifications"
      },
      {
        "paperId": "8838adafe2d1eef1b3cfe85fe3e21e43051d77c3",
        "title": "Learning an Optimal Control Policy for a Markov Decision Process Under Linear Temporal Logic Specifications"
      },
      {
        "paperId": "7c1202e6d1fec199a6ae379b2207a8e89e0289a6",
        "title": "Rabinizer 3: Safraless Translation of LTL to Small Deterministic Automata"
      },
      {
        "paperId": "4e5462d9f5789aba850d01ab9373718d974ca0cf",
        "title": "From LTL to deterministic automata"
      },
      {
        "paperId": "33d66a9fd3faff7bf3c36859caa07113484b7a5b",
        "title": "Optimal Control of Markov Decision Processes With Linear Temporal Logic Constraints"
      },
      {
        "paperId": "10dd581444c31c6f05b496db4ce72d864fefd7d1",
        "title": "Efficient reactive controller synthesis for a fragment of linear temporal logic"
      },
      {
        "paperId": "a66b0a309d9b27310442df2ac34e4f34a7f19a48",
        "title": "Reinforcement Learning and Feedback Control: Using Natural Decision Methods to Design Optimal Adaptive Controllers"
      },
      {
        "paperId": "88098ca95f9bf3e6572262cd5641f596b1c8c535",
        "title": "Optimal path planning for surveillance with temporal-logic constraints*"
      },
      {
        "paperId": "a83fba970f4c6ccfa86a17e996fe3ee692b3cd15",
        "title": "Effective Control Synthesis for Partially Observed Discrete-Event Systems"
      },
      {
        "paperId": "f1269591359fddc20f95da10c7bd4c054080b447",
        "title": "Principles of model checking"
      },
      {
        "paperId": "f4ef65bb24af05e0eb268f0c4d39730807e822dc",
        "title": "TCT: A Computation Tool for Supervisory Control Synthesis"
      },
      {
        "paperId": "972b7116e75a62730d449feafb017411bbb2fa1f",
        "title": "Introduction to Discrete Event Systems"
      },
      {
        "paperId": "dedb0678da59af204bf336fe8f731a90946a573e",
        "title": "A Verified and Compositional Translation of LTL to Deterministic Rabin Automata"
      },
      {
        "paperId": "863254ed3fbe5b111a344e4fcec479af675ab6dd",
        "title": "Supervisory control of discrete-event systems: A brief history"
      },
      {
        "paperId": "dfd34efdccd61c6de5d38461a7e46a61db10a0e7",
        "title": "Decentralized Supervision and Coordination of Concurrent Discrete Event Systems under LTL Constraints"
      },
      {
        "paperId": "b6e73a06707eb8b086e949b99320940081f3aee3",
        "title": "Decomposition of Finite LTL Specifications for Efficient Multi-agent Planning"
      },
      {
        "paperId": null,
        "title": "His current research interests include discrete event systems, systems control theory, reinforcement learning"
      },
      {
        "paperId": "6b23187c432fcf0660b98bd66554fbdec900dafc",
        "title": "SUPERVISORY CONTROL OF DISCRETE EVENT SYSTEMS"
      },
      {
        "paperId": "2484d8e9b31ddb0d4fec9ad58c3fa1f0bd8b88c2",
        "title": "Introduction to Discrete Event Systems, Second Edition"
      },
      {
        "paperId": "11b3cdc9b41d1ff539d5fc110a30214974521753",
        "title": "Automata, Logics, and Infinite Games"
      },
      {
        "paperId": "89ba50418d885a41715f112708ab34bf49231077",
        "title": "Automata Logics, and Infinite Games"
      },
      {
        "paperId": "97efafdb4a3942ab3efba53ded7413199f79c054",
        "title": "Reinforcement Learning: An Introduction"
      },
      {
        "paperId": "415f84e0d48ede982261bd1a011a82e80c5b04c5",
        "title": "Control of Discrete Event Systems"
      },
      {
        "paperId": null,
        "title": "On the complexity of \u03c9-automata"
      },
      {
        "paperId": null,
        "title": "Reducing the Learning Time of Reinforcement Learning for the Supervisory Control"
      }
    ],
    "cited_by": [
      {
        "paperId": "f33b7356782e23e4f26a4dba1aafe9c3cb17c6d5",
        "title": "Nonblocking Modular Supervisory Control of Discrete Event Systems via Reinforcement Learning and K-Means Clustering"
      },
      {
        "paperId": "d754dcf7319aeb4e611d930ba63017b1c4aedc23",
        "title": "Integrating reinforcement learning and supervisory control theory for optimal directed control of discrete-event systems"
      },
      {
        "paperId": "8623fe8e65755b76595c5ab2211e427b47b63f5d",
        "title": "On Efficient Sampling in Supervisory Reinforcement Learning"
      },
      {
        "paperId": "304eddf9f8eb34a6f7b53cb999765eca3a03cf2b",
        "title": "A Multifaceted Approach to Stock Market Trading Using Reinforcement Learning"
      },
      {
        "paperId": "170a3e21a34f925208c1480684451b050799ec21",
        "title": "DEScMaker: A tool for automated code generation for discrete event systems controllers"
      }
    ],
    "score": 2.5
  },
  {
    "id": "859172d8cb31ddebd9b18e3a7a7d982fab1d1341",
    "title": "Reward Space Noise for Exploration in Deep Reinforcement Learning",
    "authors": [
      "Chuxiong Sun",
      "Rui Wang",
      "Qian Li",
      "Xiaohui Hu"
    ],
    "year": 2021,
    "citationCount": 7,
    "abstract": "A fundamental challenge for reinforcement learning (RL) is how to achieve efficient exploration in initially unknown environments. Most state-of-the-art RL algorithms leverage action space noise to drive exploration. The classical strategies are computationally efficient and straightforward to implement. However, these methods may fail to perform effectively in complex environments. To address this issue, we propose a novel strategy named reward space noise (RSN) for farsighted and consistent exploration in RL. By introducing the stochasticity from reward space, we are able to change agent\u2019s understanding about environment and perturb its behaviors. We find that the simple RSN can achieve consistent exploration and scale to complex domains without intensive computational cost. To demonstrate the effectiveness and scalability of the proposed method, we implement a deep Q-learning agent with reward noise and evaluate its exploratory performance on a set of Atari games which are challenging for the naive [Formula: see text]-greedy strategy. The results show that reward noise outperforms action noise in most games and performs comparably in others. Concretely, we found that in the early training, the best exploratory performance of reward noise is obviously better than action noise, which demonstrates that the reward noise can quickly explore the valuable states and aid in finding the optimal policy. Moreover, the average scores and learning efficiency of reward noise are also higher than action noise through the whole training, which indicates that the reward noise can generate more stable and consistent performance.",
    "url": "https://www.semanticscholar.org/paper/859172d8cb31ddebd9b18e3a7a7d982fab1d1341",
    "pdf_url": "https://doi.org/10.1142/S0218001421520133",
    "venue": "International journal of pattern recognition and artificial intelligence",
    "publicationDate": "2021-04-12",
    "externalIds": {
      "MAG": "3153965298",
      "DBLP": "journals/ijprai/SunWLH21",
      "DOI": "10.1142/S0218001421520133",
      "CorpusId": 234889323
    },
    "references": [],
    "cited_by": [
      {
        "paperId": "5b576bebf7d18d8dc2ed90ef53cab0a8a283d1a6",
        "title": "Exploration by Random Reward Perturbation"
      },
      {
        "paperId": "10608535b12a94badcf2a80c5ca51488e015bc22",
        "title": "Group search optimization-assisted deep reinforcement learning intelligence decision for virtual network mapping"
      },
      {
        "paperId": "2f2e5f6feb8d65517afc10c0e93edfeab8f37f25",
        "title": "Evolving adaptive and interpretable decision trees for cooperative submarine search"
      },
      {
        "paperId": "2e3725f7126abdbf8c0d042ff5e78e6d26bef4d6",
        "title": "Adapting Image-based RL Policies via Predicted Rewards"
      },
      {
        "paperId": "7ba4916f6d28f0b11b5e3503ab9db70f75ec3de3",
        "title": "A Land-Based War-Gaming Simulation Method Based on Multi-Agent Proximal Policy Optimization"
      },
      {
        "paperId": "f050362f82a42c8d777119df8cf4977545c5b753",
        "title": "Intelligent Thresholding Method for Surface Mount Devices Based on Q-Learning"
      },
      {
        "paperId": "b0ce75b738dd5ae5244955755a5779d2bbcf1563",
        "title": "Simple Noisy Environment Augmentation for Reinforcement Learning"
      }
    ],
    "score": 1.75
  },
  {
    "id": "f58ada4680d374f5dba96b5e7fcb5b7be11a6acc",
    "title": "Survey of Reinforcement Learning based on Human Prior Knowledge",
    "authors": [
      "Zijing Guo",
      "Chendie Yao",
      "Yanghe Feng",
      "Yue Xu"
    ],
    "year": 2022,
    "citationCount": 5,
    "abstract": "At present, task planning is mainly solved by rules or operational research methods. The time complexity and space complexity of these methods increase exponentially with the growth of problem size. Thus, it is challenging to solve large-scale problems. Moreover, it is helpless to solve dynamic task planning problems. Reinforcement learning (RL) is often used to solve the dynamic planning problem of continuous decision making. RL agent constantly interacts with the environment to achieve the expected goal. However, the RL method meets challenges when handling the problem with ample state space. More sampling and exploration are needed to update the strategy gradient, and the convergence speed is slow. Humans ensure a quick start of learning by using prior knowledge, which reduces the exploration time of problems. Thus, we review the methods which combine human prior knowledge with RL through temporal node classification of human prior knowledge combined with RL. In this way, agents effectively reduce the sampling and exploration of the environment and eventually get the optimal strategy faster. Finally, we propose the development direction.",
    "url": "https://www.semanticscholar.org/paper/f58ada4680d374f5dba96b5e7fcb5b7be11a6acc",
    "pdf_url": "https://doi.org/10.1142/s1752890922300011",
    "venue": "Journal of Uncertain Systems",
    "publicationDate": "2022-04-06",
    "externalIds": {
      "DBLP": "journals/jus/GuoYFX22",
      "DOI": "10.1142/s1752890922300011",
      "CorpusId": 248031356
    },
    "references": [],
    "cited_by": [
      {
        "paperId": "6b2a136ad41b5a8d29feecde44a1b8ca63a87851",
        "title": "Selective Exploration and Information Gathering in Search and Rescue Using Hierarchical Learning Guided by Natural Language Input"
      },
      {
        "paperId": "8e26cd15335a352216161f18b76c54cf53135a51",
        "title": "A configuration of multi-agent reinforcement learning integrating prior knowledge"
      },
      {
        "paperId": "48e609d072cb1258e94d99e90754554f4567d09c",
        "title": "Bioinspired actor-critic algorithm for reinforcement learning interpretation with Levy-Brown hybrid exploration strategy"
      },
      {
        "paperId": "571288214b8ada4983866ee17a4bb4fd134e0b9d",
        "title": "Sample Efficient Reinforcement Learning by Automatically Learning to Compose Subtasks"
      },
      {
        "paperId": "79f9d10c28efb3b07f530c716917a70988130f1f",
        "title": "Learning Reward Structure with Subtasks in Reinforcement Learning"
      }
    ],
    "score": 1.6666666666666665
  },
  {
    "id": "aa65704a16138790678e2b9b59ae679b6c9353d7",
    "title": "Knowledge-Guided Exploration in Deep Reinforcement Learning",
    "authors": [
      "Sahisnu Mazumder",
      "Bing Liu",
      "Shuai Wang",
      "Yingxuan Zhu",
      "Xiaotian Yin",
      "Lifeng Liu",
      "Jian Li"
    ],
    "year": 2022,
    "citationCount": 5,
    "abstract": "This paper proposes a new method to drastically speed up deep reinforcement learning (deep RL) training for problems that have the property of state-action permissibility (SAP). Two types of permissibility are defined under SAP. The first type says that after an action $a_t$ is performed in a state $s_t$ and the agent has reached the new state $s_{t+1}$, the agent can decide whether $a_t$ is permissible or not permissible in $s_t$. The second type says that even without performing $a_t$ in $s_t$, the agent can already decide whether $a_t$ is permissible or not in $s_t$. An action is not permissible in a state if the action can never lead to an optimal solution and thus should not be tried (over and over again). We incorporate the proposed SAP property and encode action permissibility knowledge into two state-of-the-art deep RL algorithms to guide their state-action exploration together with a virtual stopping strategy. Results show that the SAP-based guidance can markedly speed up RL training.",
    "url": "https://www.semanticscholar.org/paper/aa65704a16138790678e2b9b59ae679b6c9353d7",
    "pdf_url": "https://arxiv.org/pdf/2210.15670.pdf",
    "venue": "arXiv.org",
    "publicationDate": "2022-10-26",
    "externalIds": {
      "ArXiv": "2210.15670",
      "DBLP": "journals/corr/abs-2210-15670",
      "DOI": "10.48550/arXiv.2210.15670",
      "CorpusId": 253224302
    },
    "references": [
      {
        "paperId": "a7e07e0ecd1727778ade42d2e1df856171ec0898",
        "title": "Model-Based Reinforcement Learning via Meta-Policy Optimization"
      },
      {
        "paperId": "3219527aa44d7789c2ed842c90bbc6da0eacd527",
        "title": "Learn What Not to Learn: Action Elimination with Deep Reinforcement Learning"
      },
      {
        "paperId": "6446167b04e0d9ae2693ddf5dfe874c42e46655d",
        "title": "Investigating Human Priors for Playing Video Games"
      },
      {
        "paperId": "338030edd55280031460366776965d95bdabd936",
        "title": "Belief Reward Shaping in Reinforcement Learning"
      },
      {
        "paperId": "7f567df97dc7e099d96e6c590ddf5aef8c5b11c4",
        "title": "Safe Exploration in Continuous Action Spaces"
      },
      {
        "paperId": "c28ec2a40a2c77e20d64cf1c85dc931106df8e83",
        "title": "Overcoming Exploration in Reinforcement Learning with Demonstrations"
      },
      {
        "paperId": "969e54e182a2c356c9d861756befcffe1fc9ef81",
        "title": "Adaptive Exploration-Exploitation Tradeoff for Opportunistic Bandits"
      },
      {
        "paperId": "cce22bf6405042a965a86557684c46a441f2a736",
        "title": "Neural Network Dynamics for Model-Based Deep Reinforcement Learning with Model-Free Fine-Tuning"
      },
      {
        "paperId": "9486d64580fcc72869d46407171285c82d3b005c",
        "title": "Efficient Reinforcement Learning with Hierarchies of Machines by Leveraging Internal Transitions"
      },
      {
        "paperId": "486dac5395c1c453d0fe6e1463d5cca06f1f60e7",
        "title": "Offline reinforcement learning with task hierarchies"
      },
      {
        "paperId": "88880d88073a99107bbc009c9f4a4197562e1e44",
        "title": "Safe Model-based Reinforcement Learning with Stability Guarantees"
      },
      {
        "paperId": "8833a5c0a374f54608615fa5fb2d59fd0546c33e",
        "title": "Symmetry Detection and Exploitation for Function Approximation in Deep RL"
      },
      {
        "paperId": "8db9df2eadea654f128c1887722c677c708e8a47",
        "title": "Deep Reinforcement Learning framework for Autonomous Driving"
      },
      {
        "paperId": "c889d6f98e6d79b89c3a6adf8a921f88fa6ba518",
        "title": "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks"
      },
      {
        "paperId": "cfc22be4649a8ec692ddf71688a8b52a416a3da6",
        "title": "What Can You Do with a Rock? Affordance Extraction via Word Embeddings"
      },
      {
        "paperId": "63a1cdaaef3a53470ceeb03c5133050e384cfc2d",
        "title": "Fast Reinforcement Learning using multiple models"
      },
      {
        "paperId": "282a380fb5ac26d99667224cef8c630f6882704f",
        "title": "Learning to reinforcement learn"
      },
      {
        "paperId": "954b01151ff13aef416d27adc60cd9a076753b1a",
        "title": "RL$^2$: Fast Reinforcement Learning via Slow Reinforcement Learning"
      },
      {
        "paperId": "5e0f2f82b4a28d59f1aa8b8ffe497790de1cdf9d",
        "title": "Combating Deep Reinforcement Learning's Sisyphean Curse with Intrinsic Fear"
      },
      {
        "paperId": "cf0930b6dddb437185c7d0c956cd64666b5f1d34",
        "title": "End-to-End Deep Reinforcement Learning for Lane Keeping Assist"
      },
      {
        "paperId": "15b26d8cb35d7e795c8832fe08794224ee1e9f84",
        "title": "The Option-Critic Architecture"
      },
      {
        "paperId": "885fe11ed7ab81c8609ccddb3e10f62577c04ab9",
        "title": "BBQ-Networks: Efficient Exploration in Deep Reinforcement Learning for Task-Oriented Dialogue Systems"
      },
      {
        "paperId": "d37620e6f8fe678a43e12930743281cd8cca6a66",
        "title": "Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation"
      },
      {
        "paperId": "3b9732bb07dc99bde5e1f9f75251c6ea5039373e",
        "title": "Deep Reinforcement Learning with Double Q-Learning"
      },
      {
        "paperId": "024006d4c2a89f7acacc6e4438d156525b60a98f",
        "title": "Continuous control with deep reinforcement learning"
      },
      {
        "paperId": "9249e04823904b5d8cd77a23f6835b63f284aa7c",
        "title": "Potential Based Reward Shaping for Hierarchical Reinforcement Learning"
      },
      {
        "paperId": "ab5d3c3a7f85f7b683d1f7dc1e3982c618910f7b",
        "title": "Goal-Based Action Priors"
      },
      {
        "paperId": "340f48901f72278f6bf78a04ee5b01df208cc508",
        "title": "Human-level control through deep reinforcement learning"
      },
      {
        "paperId": "2319a491378867c7049b3da055c5df60e1671158",
        "title": "Playing Atari with Deep Reinforcement Learning"
      },
      {
        "paperId": "789783016fb708abbc061790612ebe91273c05d3",
        "title": "(More) Efficient Reinforcement Learning via Posterior Sampling"
      },
      {
        "paperId": "18ce0b911234dd92e1815fc7b061ef6dc61a58fc",
        "title": "Concurrent learning-based approximate optimal regulation"
      },
      {
        "paperId": "9b1de5d93854d9dc364a4bc6a462193ccc3ea895",
        "title": "Simulated Car Racing Championship: Competition Software Manual"
      },
      {
        "paperId": "ddb1b8c205ba60308feef3e17850c0d2e02abb3f",
        "title": "What good are actions? Accelerating learning using learned action priors"
      },
      {
        "paperId": "60b7d47758a71978e74edff6dd8dea4d9c791d7a",
        "title": "PILCO: A Model-Based and Data-Efficient Approach to Policy Search"
      },
      {
        "paperId": "0ae2c69573673c16e96ab48aa9fc8b6ed807b451",
        "title": "Improving reinforcement learning by using sequence trees"
      },
      {
        "paperId": "8d6a5a3cf11650756c655efbc76e70e9fd8da581",
        "title": "Potential-based Shaping in Model-based Reinforcement Learning"
      },
      {
        "paperId": "a4fc537e82dfa6d0788bfdebf6b300ec1a4b2275",
        "title": "Learning Rates for Q-learning"
      },
      {
        "paperId": "413b32dc8855f9db4c95657a3de5ca6c1d793da0",
        "title": "Policy gradient reinforcement learning for fast quadrupedal locomotion"
      },
      {
        "paperId": "94066dc12fe31e96af7557838159bde598cb4f10",
        "title": "Policy Invariance Under Reward Transformations: Theory and Application to Reward Shaping"
      },
      {
        "paperId": "03b7e51c52084ac1db5118342a00b5fbcfc587aa",
        "title": "Q-learning"
      },
      {
        "paperId": "6dad90f530cb9aa0deec5fa232155dab539d1b49",
        "title": "Action Permissibility in Deep Reinforcement Learning and Application to Autonomous Driving"
      },
      {
        "paperId": "6d7a36eeb9b5dd4276de9753c997fc6f5ba99259",
        "title": "Human Learning in Atari"
      }
    ],
    "cited_by": [
      {
        "paperId": "fccab8461eafc257b3b08710ab5d03c35388f72b",
        "title": "Evolution, Future of AI, and Singularity"
      },
      {
        "paperId": "1f3adf7e6e3dafbdc0c5641310a0d258ac301fa1",
        "title": "An attention-based proximal policy optimization algorithm for imitation reinforcement learning"
      },
      {
        "paperId": "f1a5156491f16ae595fc579b82d47ae6e58d3a83",
        "title": "Improving Sample Efficiency of Reinforcement Learning with Background Knowledge from Large Language Models"
      },
      {
        "paperId": "d21a1fbeb53db40f9dc641e1b2c48cbaf36def08",
        "title": "AUTONODE: A Neuro-Graphic Self-Learnable Engine for Cognitive GUI Automation"
      },
      {
        "paperId": "9eb5e294b047e399a131f86df4e88274d796e7e5",
        "title": "Efficient Reinforcement Learning (ERL): Targeted Exploration Through Action Saturation"
      }
    ],
    "score": 1.6666666666666665
  },
  {
    "id": "bd8aaab29fa16f40ef016393ba7ca30127abab58",
    "title": "Risk Perspective Exploration in Distributional Reinforcement Learning",
    "authors": [
      "Ji-Yun Oh",
      "Joonkee Kim",
      "Se-Young Yun"
    ],
    "year": 2022,
    "citationCount": 5,
    "abstract": "Distributional reinforcement learning demonstrates state-of-the-art performance in continuous and discrete control settings with the features of variance and risk, which can be used to explore. However, the exploration method employing the risk property is hard to find, although numerous exploration methods in Distributional RL employ the variance of return distribution per action. In this paper, we present risk scheduling approaches that explore risk levels and optimistic behaviors from a risk perspective. We demonstrate the performance enhancement of the DMIX algorithm using risk scheduling in a multi-agent setting with comprehensive experiments.",
    "url": "https://www.semanticscholar.org/paper/bd8aaab29fa16f40ef016393ba7ca30127abab58",
    "pdf_url": "https://arxiv.org/pdf/2206.14170.pdf",
    "venue": "arXiv.org",
    "publicationDate": "2022-06-28",
    "externalIds": {
      "DBLP": "journals/corr/abs-2206-14170",
      "ArXiv": "2206.14170",
      "DOI": "10.48550/arXiv.2206.14170",
      "CorpusId": 250088884
    },
    "references": [
      {
        "paperId": "58b17c5a115c8440c530c6242a95076618107bf7",
        "title": "Non-decreasing Quantile Function Network with Efficient Exploration for Distributional Reinforcement Learning"
      },
      {
        "paperId": "3098e236a15908743cea2e83ca07d1992e7c0f6c",
        "title": "DFAC Framework: Factorizing the Value Function via Quantile Mixture for Multi-Agent Distributional Q-Learning"
      },
      {
        "paperId": "14c7017254940dd08c92e65426c9984554830a7b",
        "title": "RMIX: Learning Risk-Sensitive Policies for Cooperative Reinforcement Learning Agents"
      },
      {
        "paperId": "3f5f13c6f5c659754086a7faa51d6bc60f577cd1",
        "title": "Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "f6b218453170edcbb51e49dd44ba2f83af53ef92",
        "title": "Distributional Reinforcement Learning for Efficient Exploration"
      },
      {
        "paperId": "82055eed2ba0d7156a54c586249742c848e5d565",
        "title": "The StarCraft Multi-Agent Challenge"
      },
      {
        "paperId": "4cb3fd057949624aa4f0bbe7a6dcc8777ff04758",
        "title": "Exploration by Random Network Distillation"
      },
      {
        "paperId": "d85623ffae865f9ef386644dd02d0ea2d6a8c8de",
        "title": "Implicit Quantile Networks for Distributional Reinforcement Learning"
      },
      {
        "paperId": "ffc211476f2e40e79466ffc198c919a97da3bb76",
        "title": "QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "fe3e91e40a950c6b6601b8f0a641884774d949ae",
        "title": "Distributional Reinforcement Learning with Quantile Regression"
      },
      {
        "paperId": "c1f4ef741242d629d1f56e442a09a7ba29595a0e",
        "title": "A Distributional Perspective on Reinforcement Learning"
      },
      {
        "paperId": "3fab4d28be5ff73f42c8b307117aa8d680345ad9",
        "title": "Noisy Networks for Exploration"
      },
      {
        "paperId": "250bf5e1d40a619080dec553914c2905db6008c7",
        "title": "Value-Decomposition Networks For Cooperative Multi-Agent Learning"
      },
      {
        "paperId": "f82e4ff4f003581330338aaae71f60316e58dd26",
        "title": "The Arcade Learning Environment: An Evaluation Platform for General Agents"
      },
      {
        "paperId": "103f6fe35033f9327611ddafde74a2b544072980",
        "title": "Using Confidence Bounds for Exploitation-Exploration Trade-offs"
      },
      {
        "paperId": "59b50a775542e87f078db35b868ac10ab43d4c75",
        "title": "Learning from delayed rewards"
      },
      {
        "paperId": "c87d57da3b1f2b467ef4995d30df832ee2281107",
        "title": "On robust estimation of the location parameter"
      }
    ],
    "cited_by": [
      {
        "paperId": "48ea067b2c84680117ef5fc3bc01eb0e66907e66",
        "title": "Tackling Uncertainties in Multi-Agent Reinforcement Learning through Integration of Agent Termination Dynamics"
      },
      {
        "paperId": "87fbcfffdd5c1a3cca52deed98df0a8bb8d5fc94",
        "title": "RiskQ: Risk-sensitive Multi-Agent Reinforcement Learning Value Factorization"
      },
      {
        "paperId": "034b015f0a8e8f1921e2e4d8c854c939228c4e02",
        "title": "Pitfall of Optimism: Distributional Reinforcement Learning by Randomizing Risk Criterion"
      },
      {
        "paperId": "89455038e4e86388696a68a8b9558666268e44f3",
        "title": "DRL-ORA: Distributional Reinforcement Learning with Online Risk Adaption"
      },
      {
        "paperId": "49c8a03c38267939b6c02b2fc56a11e45d3675da",
        "title": "Distributional Reinforcement Learning with Online Risk-awareness Adaption"
      }
    ],
    "score": 1.6666666666666665
  },
  {
    "id": "a6c5e49425ac01c534d9663a4453b28f6dc76f7c",
    "title": "Accelerating Robot Trajectory Learning for Stochastic Tasks",
    "authors": [
      "J. Vidakovi\u0107",
      "B. Jerbi\u0107",
      "B. \u0160ekoranja",
      "M. \u0160vaco",
      "F. \u0160uligoj"
    ],
    "year": 2020,
    "citationCount": 6,
    "abstract": "Learning from demonstration provides ways to transfer knowledge and skills from humans to robots. Models based solely on learning from demonstration often have very good generalization capabilities but are not completely accurate when adapting to new scenarios. This happens especially when learning stochastic tasks because of the correspondence problem and unmodeled physical properties of tasks. On the other hand, reinforcement learning (RL) methods such as policy search have the capability to refine an initial skill through exploration, where the learning process is often very dependent on the initialization strategy and is efficient in finding only local solutions. These two approaches are, therefore, frequently combined. In this paper, we present how the iterative learning of tasks can be accelerated by a learning from demonstration (LfD) method based on the extraction of via-points. The paper provides an evaluation of the approach on two different primitive motion tasks.",
    "url": "https://www.semanticscholar.org/paper/a6c5e49425ac01c534d9663a4453b28f6dc76f7c",
    "pdf_url": "https://doi.org/10.1109/ACCESS.2020.2986999",
    "venue": "IEEE Access",
    "publicationDate": "2020-04-13",
    "externalIds": {
      "DBLP": "journals/access/VidakovicJSSS20",
      "MAG": "3016030121",
      "DOI": "10.1109/ACCESS.2020.2986999",
      "CorpusId": 216534408
    },
    "references": [
      {
        "paperId": "3b1c7687ce4644063b69718bdb18e26ea0d33278",
        "title": "Learning from Demonstration Based on a Classification of Task Parameters and Trajectory Optimization"
      },
      {
        "paperId": "a8614f060d9ee005c87a417bd749dc19f76795f4",
        "title": "Trajectory generation for robotic assembly operations using learning by demonstration"
      },
      {
        "paperId": "1d7a5adf3e85e285cb6c5848e3eda2ba0a608b9e",
        "title": "A Comparison of Policy Search in Joint Space and Cartesian Space for Refinement of Skills"
      },
      {
        "paperId": "ee9893ff2aa325ff3c9920f247436c514fd8b512",
        "title": "SOLAR: Deep Structured Representations for Model-Based Reinforcement Learning"
      },
      {
        "paperId": "a00e092ff523378e3f79f44891346827bc6673d4",
        "title": "A Reinforcement Learning Based Algorithm for Robot Action Planning"
      },
      {
        "paperId": "57acca4766755c35580fa36eebb1e15750f503c4",
        "title": "Smooth and Efficient Policy Exploration for Robot Trajectory Learning"
      },
      {
        "paperId": "dfb2b26f15466bf3ec34fbd72a22bb9d6ecd42f4",
        "title": "Policy Search in Continuous Action Domains: an Overview"
      },
      {
        "paperId": "333bbb7223272611da6e245e9abd7bb6b0b19ab6",
        "title": "Towards Robust Skill Generalization: Unifying Learning from Demonstration and Motion Planning"
      },
      {
        "paperId": "23ca54529dd6cf208e3484a3d02a35860a1470af",
        "title": "Learning task-parameterized dynamic movement primitives using mixture of GMMs"
      },
      {
        "paperId": "a7680e975d395891522d3c10e3bf892f9b618048",
        "title": "Vision-Based Multi-Task Manipulation for Inexpensive Robots Using End-to-End Learning from Demonstration"
      },
      {
        "paperId": "5c308a186127d3e61c74c6804ab5e4f794445042",
        "title": "Deriving and improving CMA-ES with information geometric trust regions"
      },
      {
        "paperId": "0e8364e61acae884edc8c1cee5df1f6ee2c6e8c4",
        "title": "Learning feedback terms for reactive planning and control"
      },
      {
        "paperId": "2d31f1b8a917637258757858e9d05a944cfb3140",
        "title": "Learning from demonstration with partially observable task parameters using dynamic movement primitives and Gaussian process regression"
      },
      {
        "paperId": "23da6f9eedf082ea8d45c444d6109bca25829fc5",
        "title": "Actor-critic versus direct policy search: a comparison based on sample complexity"
      },
      {
        "paperId": "a6d2660386341a5117b25a1fdfdfd870fa470168",
        "title": "Learning complex sequential tasks from demonstration: A pizza dough rolling case study"
      },
      {
        "paperId": "1d89f323a112f48481f3a0a24e69d539f237ed8e",
        "title": "Learning of parametric coupling terms for robot-environment interaction"
      },
      {
        "paperId": "85a82d1f39b95e2dde09d5c96363e500b33aa22d",
        "title": "Task Parameterization Using Continuous Constraints Extracted From Human Demonstrations"
      },
      {
        "paperId": "af2fab96e8cb67ebf9bdb47842dcaad581267d85",
        "title": "A tutorial on task-parameterized movement learning and retrieval"
      },
      {
        "paperId": "87dbd6ac8b0622e933da7166cf0f414040c12654",
        "title": "An incremental approach to learning generalizable robot tasks from human demonstration"
      },
      {
        "paperId": "2537ac352edc642f210082e39ad2026edcb37997",
        "title": "Kicking motion planning of Nao robots based on CMA-ES"
      },
      {
        "paperId": "b6b8a1b80891c96c28cc6340267b58186157e536",
        "title": "End-to-End Training of Deep Visuomotor Policies"
      },
      {
        "paperId": "58a7acac2a4ac240e293752be4ffd46f786e5293",
        "title": "Robot Skill Learning: From Reinforcement Learning to Evolution Strategies"
      },
      {
        "paperId": "65438e0ba226c1f97bd8a36333ebc3297b1a32fd",
        "title": "Reinforcement learning in robotics: A survey"
      },
      {
        "paperId": "b6bfae6efa1110a57a4d8362721d152d78aae358",
        "title": "A Survey on Policy Search for Robotics"
      },
      {
        "paperId": "5438f71c01dd713c6f4e05a48c4c8f9c5f82a805",
        "title": "Dynamical Movement Primitives: Learning Attractor Models for Motor Behaviors"
      },
      {
        "paperId": "1695dbabf8e905db0b391ff522c323db5fc8b958",
        "title": "Learning to select and generalize striking movements in robot table tennis"
      },
      {
        "paperId": "b7bc7f830f5f936cf9b1dae401dbb96ff968b2f3",
        "title": "Policy Improvement Methods: Between Black-Box Optimization and Episodic Reinforcement Learning"
      },
      {
        "paperId": "0136e02f24515b31809fcfee37c1ba55759b9b4e",
        "title": "Reinforcement learning to adjust parametrized motor primitives to new situations"
      },
      {
        "paperId": "60b7d47758a71978e74edff6dd8dea4d9c791d7a",
        "title": "PILCO: A Model-Based and Data-Efficient Approach to Policy Search"
      },
      {
        "paperId": "64034d5c2b82e81dd357fd32e3b84667b5c5472f",
        "title": "Imitation Learning of Positional and Force Skills Demonstrated via Kinesthetic Teaching and Haptic Input"
      },
      {
        "paperId": "5e45a5ef048f5769c97312ea5b1895e9fd8932a5",
        "title": "Learning-based control strategy for safe human-robot interaction exploiting task and robot redundancies"
      },
      {
        "paperId": "27f0504bb0ddb10249aeee4b0216cdb6f2a51dd8",
        "title": "Robot motor skill coordination with EM-based Reinforcement Learning"
      },
      {
        "paperId": "5ad05a69cdccc8cdf932419365c99ec295a04448",
        "title": "Learning Policy Improvements with Path Integrals"
      },
      {
        "paperId": "f1bdebedf07fd444628c955568f0d51e1a26835e",
        "title": "Completely Derandomized Self-Adaptation in Evolution Strategies"
      },
      {
        "paperId": "9f267bbb7002381dcdf62160c6f3aeda188c5104",
        "title": "Skill Generalization via Inference-based Planning"
      },
      {
        "paperId": null,
        "title": "JOSIP VIDAKOVI\u0106 received the Diploma degree in mechanical engineering from the Faculty of Mechanical Engineering and Naval Architecture, University of Zagreb (UNIZAG)"
      },
      {
        "paperId": "4c915c1eecb217c123a36dc6d3ce52d12c742614",
        "title": "Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning"
      },
      {
        "paperId": "4d6a87d76ec0c0379f0afbf24f84bba848c6246e",
        "title": "Noname manuscript No. (will be inserted by the editor) Policy Search for Motor Primitives in Robotics"
      },
      {
        "paperId": null,
        "title": "BOJAN"
      },
      {
        "paperId": null,
        "title": "Mechanical Engineering and Naval Architecture, UNIZAG"
      }
    ],
    "cited_by": [
      {
        "paperId": "ae191bec49a19d2c750cf18235da331a94b7d8ea",
        "title": "PI2-BDMPs in Combination With Contact Force Model: A Robotic Polishing Skill Learning and Generalization Approach"
      },
      {
        "paperId": "c91a7dd722c7c6988185c411bc04a42d129b59b9",
        "title": "Colonial bacterial memetic algorithm and its application on a darts playing robot"
      },
      {
        "paperId": "056040d964324bd73a467596944869a63b25477d",
        "title": "A Practical Roadmap to Learning from Demonstration for Robotic Manipulators in Manufacturing"
      },
      {
        "paperId": "acdde63fbc63e04872ac46de9f293b33744123f4",
        "title": "A Novel Concrete Placing Path Generation Method Based on Error Zone with Concrete Placing Robot"
      },
      {
        "paperId": "a699e220061948cfdf065d6322a79b4ab58db57c",
        "title": "Learning Deep Robotic Skills on Riemannian Manifolds"
      },
      {
        "paperId": "9f8ffd2bc04d520beee3d8f8fc454a285287e292",
        "title": "Students\u2019 Motivation to Learn Mathematics in the Robotics Environment"
      }
    ],
    "score": 1.2000000000000002
  },
  {
    "id": "d8bc75fbcfdecd5cd1952524d3e07db13722f5ff",
    "title": "Optimal Volt/Var Control for Unbalanced Distribution Networks With Human-in-the-Loop Deep Reinforcement Learning",
    "authors": [
      "Xianzhuo Sun",
      "Zhao Xu",
      "Jing Qiu",
      "Huichuan Liu",
      "Huayi Wu",
      "Yuechuan Tao"
    ],
    "year": 2024,
    "citationCount": 21,
    "abstract": "This paper proposes a human-in-the-loop deep reinforcement learning (HL-DRL)-based VVC strategy to simultaneously reduce power losses, mitigate voltage violations and compensate for voltage unbalance in three-phase unbalanced distribution networks. Instead of fully trusting DRL actions made by deep neural networks, a human intervention module is proposed to modify dangerous actions that violate operation constraints during offline training. This module refers to well-designed human guidance rules based on voltage-reactive power sensitivities, which regulate PV reactive power to sequentially address local voltage violation and unbalance issues to obtain safe transitions. To efficiently and safely learn the optimal control policy from these training samples, a human-in-the-loop soft actor-critic (HL-SAC) solution method is then developed. Different from the standard SAC algorithm, an online switch mechanism between action exploration and human intervention is designed. The actor network loss function is modified to incorporate human guidance terms, which alleviates the inconsistency of the updating direction of actor and critic networks. A hybrid experience replay buffer including both dangerous and safe transitions is also used to facilitate the learning process towards human actions. Comparative simulation results on a modified IEEE 123-bus unbalanced distribution system demonstrate the effectiveness and superiority of the proposed method in voltage control.",
    "url": "https://www.semanticscholar.org/paper/d8bc75fbcfdecd5cd1952524d3e07db13722f5ff",
    "pdf_url": "https://doi.org/10.1109/TSG.2023.3337843",
    "venue": "IEEE Transactions on Smart Grid",
    "publicationDate": "2024-05-01",
    "externalIds": {
      "DBLP": "journals/tsg/SunXQLWT24",
      "DOI": "10.1109/TSG.2023.3337843",
      "CorpusId": 265620145
    },
    "references": [],
    "cited_by": [
      {
        "paperId": "2ad53b06e412612d9d39937ba0421832f125fa6a",
        "title": "Reinforcement Learning Driven Volt-Var Control with PV Inverters and Solid-State Transformer-Enabled EV Charging Stations"
      },
      {
        "paperId": "d0b1f52701ad640f94ff5f83793905f117336331",
        "title": "Multi-Timescale Reward-Based DRL Energy Management for Regenerative Braking Energy Storage System"
      },
      {
        "paperId": "58f6acf3a3466f220a4ff1c822d25dc143f227bd",
        "title": "Local Distribution Voltage Control Using Large-Scale Coordinated PV Inverters: A Novel Multi-Agent Deep Reinforcement Learning-Based Approach"
      },
      {
        "paperId": "27a212e209c5aad32844b70117e4ce09deafdd7f",
        "title": "Quantifying Volt/Var Regulation Capability at TSO-DSO Interface Based on Deep Learning Methods"
      },
      {
        "paperId": "d40c03a21aea7dc513bdaf712b0d0bf592e18e81",
        "title": "Short-term Voltage Stability Assessment in Power Systems based on Deep Learning"
      },
      {
        "paperId": "f569c8756d2f9272c03ca717933bf80f23743768",
        "title": "A large language model for advanced power dispatch"
      },
      {
        "paperId": "34475cd6aa5f9696de8ff78870a4e43739904441",
        "title": "A Review of Machine Learning Approaches for Analysis, Modeling, and Decision-Making in Integrated Energy Systems"
      },
      {
        "paperId": "c9e69a24e963f0b1fa79de206459665d6808ef58",
        "title": "Coordinated frequency control strategy for modern power system considering engagement willingness"
      },
      {
        "paperId": "4b9f8735de6c38c5372e8d7d37aa08d5babc88f4",
        "title": "Adaptive Levels of Automation Adjustment with Reinforcement Learning and Fuzzy Logic"
      },
      {
        "paperId": "c13236ec3b4090f50c28f461be717006c16fc050",
        "title": "Advancements in data-driven voltage control in active distribution networks: A Comprehensive review"
      },
      {
        "paperId": "3c020d55e953fdcfaca5f55a829d6cbdf5c455b2",
        "title": "Robust Deep Reinforcement Learning for Inverter-based Volt-Var Control in Partially Observable Distribution Networks"
      },
      {
        "paperId": "f6e3c6e649013ee98641cb286386b61b57a8c083",
        "title": "Two-Critic Deep Reinforcement Learning for Inverter-Based Volt-Var Control in Active Distribution Networks"
      },
      {
        "paperId": "17682d6f13dcac703092e0f1500a4197e46bbf2c",
        "title": "Safe Reinforcement Learning for Power System Control: A Review"
      },
      {
        "paperId": "9a0d705852c8aa0b07277d9b1235d49262081865",
        "title": "A Review of Safe Reinforcement Learning Methods for Modern Power Systems"
      },
      {
        "paperId": "6b1299224398bc962933e2575dc541f828cef6d4",
        "title": "Review and Rethink of Power System Resilience Against Extreme Weather Events: Current Practices, Challenges, and Future Trends"
      },
      {
        "paperId": "6e6a9ddbd6b355d15929157a4ed595ccb262043b",
        "title": "Volt/VAR optimization for photovoltaic-storage-charging station high-permeability power distribution networks: A data-knowledge hybrid driven reinforcement learning method"
      },
      {
        "paperId": "ff45927a74418c6b993b2f0021df93eeefe9ac71",
        "title": "Coordinated central-local control strategy for voltage management in PV-integrated distribution networks considering energy storage degradation"
      },
      {
        "paperId": "b83f074c3b20d2d9d2c038feb8591f2a828f70e6",
        "title": "Two-Timescale Coordination of Discretely and Continuously Adjustable Devices in ADNs With DRL and Physical Convex Optimization"
      },
      {
        "paperId": "1fc98479b1f04026dacf814e118b0c1d4ef8e764",
        "title": "Safe reinforcement learning for power system control: A review"
      },
      {
        "paperId": "9cd70ad5f6db29c7ab9a80a881d19c4704e5c958",
        "title": "Leveraging AI for Enhanced Power Systems Control: An Introductory Study of Model-Free DRL Approaches"
      },
      {
        "paperId": "6bbac06157803d33a501d3f02c98efa5ea1ba91d",
        "title": "Voltage control of distribution grid with district cooling systems based on scenario-classified reinforcement learning"
      }
    ],
    "score": 21.0
  },
  {
    "id": "17682d6f13dcac703092e0f1500a4197e46bbf2c",
    "title": "Safe Reinforcement Learning for Power System Control: A Review",
    "authors": [
      "Peipei Yu",
      "Zhen-yu Wang",
      "Hongcai Zhang",
      "Yonghua Song"
    ],
    "year": 2024,
    "citationCount": 8,
    "abstract": "The large-scale integration of intermittent renewable energy resources introduces increased uncertainty and volatility to the supply side of power systems, thereby complicating system operation and control. Recently, data-driven approaches, particularly reinforcement learning (RL), have shown significant promise in addressing complex control challenges in power systems, because RL can learn from interactive feedback without needing prior knowledge of the system model. However, the training process of model-free RL methods relies heavily on random decisions for exploration, which may result in ``bad\"decisions that violate critical safety constraints and lead to catastrophic control outcomes. Due to the inability of RL methods to theoretically ensure decision safety in power systems, directly deploying traditional RL algorithms in the real world is deemed unacceptable. Consequently, the safety issue in RL applications, known as safe RL, has garnered considerable attention in recent years, leading to numerous important developments. This paper provides a comprehensive review of the state-of-the-art safe RL techniques and discusses how these techniques can be applied to power system control problems such as frequency regulation, voltage control, and energy management. We then present discussions on key challenges and future research directions, related to convergence and optimality, training efficiency, universality, and real-world deployment.",
    "url": "https://www.semanticscholar.org/paper/17682d6f13dcac703092e0f1500a4197e46bbf2c",
    "pdf_url": "https://arxiv.org/pdf/2407.00681.pdf",
    "venue": "arXiv.org",
    "publicationDate": "2024-06-30",
    "externalIds": {
      "DBLP": "journals/corr/abs-2407-00681",
      "ArXiv": "2407.00681",
      "DOI": "10.48550/arXiv.2407.00681",
      "CorpusId": 270875461
    },
    "references": [
      {
        "paperId": "d8bc75fbcfdecd5cd1952524d3e07db13722f5ff",
        "title": "Optimal Volt/Var Control for Unbalanced Distribution Networks With Human-in-the-Loop Deep Reinforcement Learning"
      },
      {
        "paperId": "dc02c53a6f613c0cea828aa510514654be8fe703",
        "title": "Dynamic Incentive Pricing on Charging Stations for Real-Time Congestion Management in Distribution Network: An Adaptive Model-Based Safe Deep Reinforcement Learning Method"
      },
      {
        "paperId": "a090ec199418e7cf05a3b137c495ded658759b2e",
        "title": "Online Operational Decision-Making for Integrated Electric-Gas Systems With Safe Reinforcement Learning"
      },
      {
        "paperId": "2fd0156d2381e2cb2febcbdea86592e4de13d32d",
        "title": "Techno\u2013Economic Modeling and Safe Operational Optimization of Multi-Network Constrained Integrated Community Energy Systems"
      },
      {
        "paperId": "576d558909058877063b3506482cb86a8de5b2e1",
        "title": "Real-Time Sequential Security-Constrained Optimal Power Flow: A Hybrid Knowledge-Data-Driven Reinforcement Learning Approach"
      },
      {
        "paperId": "8d67ffb9bf4603a65da4a37e3836a1fc0491e31c",
        "title": "Multi-Agent Safe Graph Reinforcement Learning for PV Inverters-Based Real-Time Decentralized Volt/Var Control in Zoned Distribution Networks"
      },
      {
        "paperId": "3181af59ac5e29d2b2180dd0d024efb49ebf5d2d",
        "title": "Online Preventive Control for Transmission Overload Relief Using Safe Reinforcement Learning With Enhanced Spatial-Temporal Awareness"
      },
      {
        "paperId": "428558f3bd43183ef38fca95480e160c1fec230a",
        "title": "Networked Multiagent-Based Safe Reinforcement Learning for Low-Carbon Demand Management in Distribution Networks"
      },
      {
        "paperId": "404b5aed9d8cb0de67efd5d8c4d9a354ed83805d",
        "title": "Efficient learning of power grid voltage control strategies via model-based deep reinforcement learning"
      },
      {
        "paperId": "5142c19ee2d2251d95256f86c8f7138d27e9e355",
        "title": "Safe multi-agent deep reinforcement learning for real-time decentralized control of inverter based renewable energy resources considering communication delay"
      },
      {
        "paperId": "32f90f1faa0cc36354942faa380875f6e0787bce",
        "title": "A Real-World Reinforcement Learning Framework for Safe and Human-Like Tactical Decision-Making"
      },
      {
        "paperId": "fe9ec80f1fdfd162102e33b63c905c45d89abfe8",
        "title": "Delay Safety-Aware Digital Twin Empowered Industrial Sensing-Actuation Systems Using Transferable and Reinforced Learning"
      },
      {
        "paperId": "ec103f35ef2c1d2b954c8d79237434f3c3e50717",
        "title": "A Barrier-Certificated Reinforcement Learning Approach for Enhancing Power System Transient Stability"
      },
      {
        "paperId": "d70b9ec2f4aca13e9fde964fe17f4e1b22ba1221",
        "title": "District cooling system control for providing regulation services based on safe reinforcement learning with barrier functions"
      },
      {
        "paperId": "9e7d0880404d8bfbeadfeb99aeeafd7882794eba",
        "title": "Safe Reinforcement Learning for Mitigation of Model Errors in FACTS Setpoint Control"
      },
      {
        "paperId": "bc982dae2e76c4be25ec588e058ac93e45a720ba",
        "title": "Deep Reinforcement Learning for Smart Grid Operations: Algorithms, Applications, and Prospects"
      },
      {
        "paperId": "41e1aba3c773ebe729c265b51877d09857832163",
        "title": "Safe Deep Reinforcement Learning for Microgrid Energy Management in Distribution Networks With Leveraged Spatial\u2013Temporal Perception"
      },
      {
        "paperId": "930dd6d2e224b40bc981c01e5841fcd3464a9afa",
        "title": "Robust Lagrangian and Adversarial Policy Gradient for Robust Constrained Markov Decision Processes"
      },
      {
        "paperId": "4a29dbaca4c9488d271fd882bde812e58c5b71db",
        "title": "Safe Deep Reinforcement Learning for Power System Operation under Scheduled Unavailability"
      },
      {
        "paperId": "3afbead850747d4a98c24cca0af1f5e78a128396",
        "title": "An effective energy management Layout-Based reinforcement learning for household demand response in digital twin simulation"
      },
      {
        "paperId": "33bef8ab31b2520301c798343da61d84e410585e",
        "title": "Multi-Agent attention-based deep reinforcement learning for demand response in grid-responsive buildings"
      },
      {
        "paperId": "11389a70e115c21169f365f3e944f517cebfe555",
        "title": "AdapSafe: Adaptive and Safe-Certified Deep Reinforcement Learning-Based Frequency Control for Carbon-Neutral Power Systems"
      },
      {
        "paperId": "74eb22eed396aa62b91f4e03b2bf95a1dcdcdfd5",
        "title": "An adaptive safety layer with hard constraints for safe reinforcement learning in multi-energy management systems"
      },
      {
        "paperId": "e8fb9b898b024b8c6fbef2cb8e39dd011e9259b9",
        "title": "Secure energy management of multi-energy microgrid: A physical-informed safe reinforcement learning approach"
      },
      {
        "paperId": "a4aa558ed77134f5458b8e92cdb128c2b0b9ce2b",
        "title": "Safe and Sample-Efficient Reinforcement Learning for Clustered Dynamic Environments"
      },
      {
        "paperId": "263622e8036274973677888724a53a500e8a91d8",
        "title": "Safe multi-agent reinforcement learning for multi-robot control"
      },
      {
        "paperId": "f173a8652eeda3fa3ede5cdf5a982ae9c9eff0b7",
        "title": "A human-centered safe robot reinforcement learning framework with interactive behaviors"
      },
      {
        "paperId": "144bca8c2c09629e4a6289ead32842c02c391821",
        "title": "Multi-agent hierarchical reinforcement learning for energy management"
      },
      {
        "paperId": "4e7f6283e029751d37624de2adf134f6c42b8d21",
        "title": "Event-Triggered Constrained Optimal Control for Organic Rankine Cycle Systems via Safe Reinforcement Learning"
      },
      {
        "paperId": "31243acb71735e6113a031a255b1e4f83c39bb54",
        "title": "Multi-Market Bidding Behavior Analysis of Energy Storage System Based on Inverse Reinforcement Learning"
      },
      {
        "paperId": "3510dc32710ec36bcfb8b735a028681ce6aba1ba",
        "title": "A Hybrid Data-Driven Method for Fast Solution of Security-Constrained Optimal Power Flow"
      },
      {
        "paperId": "a08b05f30188213b2efbe866a9607a66bfee98c9",
        "title": "Batch reinforcement learning for network-safe demand response in unknown electric grids"
      },
      {
        "paperId": "a3f9770c9d549a217ae2516e3db7f7c9af59b299",
        "title": "A Safe Policy Learning-Based Method for Decentralized and Economic Frequency Control in Isolated Networked-Microgrid Systems"
      },
      {
        "paperId": "52a220e9273dfa3c174563e6354d2a5284f6e710",
        "title": "Enforcing Hard Constraints with Soft Barriers: Safe Reinforcement Learning in Unknown Stochastic Environments"
      },
      {
        "paperId": "7b44e1687ffdb7d95defa92c9c4796c663cac0c7",
        "title": "Federated Reinforcement Learning for Decentralized Voltage Control in Distribution Networks"
      },
      {
        "paperId": "d6c35cb1905975fc740968e1d176146dcad5500b",
        "title": "Incorporating Constraints in Reinforcement Learning Assisted Energy System Decision Making: A Selected Review"
      },
      {
        "paperId": "c52eb616114c2a7716b7699f7734191641dd98fa",
        "title": "Reinforcement Learning for Distributed Transient Frequency Control with Stability and Safety Guarantees"
      },
      {
        "paperId": "5eca55f6d39f5a3dbb019ad598e77195549d5980",
        "title": "Penalized Proximal Policy Optimization for Safe Reinforcement Learning"
      },
      {
        "paperId": "945628ea784bbe98a8486f89e59c57cd6aa55d68",
        "title": "Provably Safe Deep Reinforcement Learning for Robotic Manipulation in Human Environments"
      },
      {
        "paperId": "73bb08e925a5098823eb7b47d7a0ec457f555140",
        "title": "Learning to Operate Distribution Networks With Safe Deep Reinforcement Learning"
      },
      {
        "paperId": "a0662e9eefc61b53ac61c18448c03293b9f7c0cf",
        "title": "Model-augmented safe reinforcement learning for Volt-VAR control in power distribution networks"
      },
      {
        "paperId": "2eced32191850ddfbdbc4ef9e2a8059e99e7f84a",
        "title": "Safe Reinforcement Learning via Shielding under Partial Observability"
      },
      {
        "paperId": "b2c08608c97fd4759c840d0ca2df89ea30af8307",
        "title": "Deep Reinforcement Learning From Demonstrations to Assist Service Restoration in Islanded Microgrids"
      },
      {
        "paperId": "61e7a3d5606043594a8ce377870479f77a6b58c2",
        "title": "A Survey on Offline Reinforcement Learning: Taxonomy, Review, and Open Problems"
      },
      {
        "paperId": "7b97ba3031e69bd5b9a817a94c95cac80773daa0",
        "title": "Safe reinforcement learning for real-time automatic control in a smart energy-hub"
      },
      {
        "paperId": "f3e8c42b56bf8406726eaaccc68398df2eaccc61",
        "title": "Efficient Learning of Safe Driving Policy via Human-AI Copilot Optimization"
      },
      {
        "paperId": "2606f51366e6ee1761315284180738a98d789f8a",
        "title": "Coordinated Frequency Control through Safe Reinforcement Learning"
      },
      {
        "paperId": "8f499601d0c10f621a90fe201793918b96399b20",
        "title": "District Cooling System Control for Providing Operating Reserve Based on Safe Deep Reinforcement Learning"
      },
      {
        "paperId": "ff3a2175da63ac311bf60e7fb8c3e61bc0dcadf3",
        "title": "Safe multi-agent deep reinforcement learning for joint bidding and maintenance scheduling of generation units"
      },
      {
        "paperId": "ac515f2576ffa0b93e9cf9d64860f6c1b44584fa",
        "title": "Online Microgrid Energy Management Based on Safe Deep Reinforcement Learning"
      },
      {
        "paperId": "d49eb322bc426dde48ac5a7973fad0b6513ee395",
        "title": "Safe Driving via Expert Guided Policy Optimization"
      },
      {
        "paperId": "ddd8798f1d4e48e6dc4b3100600d442d8f9240fc",
        "title": "Decentralized Safe Reinforcement Learning for Voltage Control"
      },
      {
        "paperId": "27302766f8d0eb6c052eb400e234c5be0e7a767e",
        "title": "Trust Region Policy Optimisation in Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "96e8aea638c842a32003aa689b237c40b3cfdc5d",
        "title": "Soft Actor-Critic With Integer Actions"
      },
      {
        "paperId": "6a0d4cc369093089b7c2384bb9045f28fe78a42f",
        "title": "Achieving Zero Constraint Violation for Constrained Reinforcement Learning via Primal-Dual Approach"
      },
      {
        "paperId": "ecc368ca0bd209466a23b86af17fbc187f3a0d29",
        "title": "Lyapunov-based uncertainty-aware safe reinforcement learning"
      },
      {
        "paperId": "63572d61745c4c09709463d9c2df00bf9bf8e054",
        "title": "Constraints Penalized Q-Learning for Safe Offline Reinforcement Learning"
      },
      {
        "paperId": "3405963cb5d8b3b0eb2b0b00a538b395a9d6993c",
        "title": "Safe Reinforcement Learning via Statistical Model Predictive Shielding"
      },
      {
        "paperId": "16ee2bd9519405f8a1e0a9fa21a8ae8778d0b957",
        "title": "Safe Exploration by Solving Early Terminated MDP"
      },
      {
        "paperId": "5b2370ebd3439ff60ea64a0c8db88fea2dd86a9c",
        "title": "WCSAC: Worst-Case Soft Actor Critic for Safety-Constrained Reinforcement Learning"
      },
      {
        "paperId": "a3624acaf21b2dc998746d44dac510af6a5e91d7",
        "title": "Bi-level Off-policy Reinforcement Learning for Volt/VAR Control Involving Continuous and Discrete Devices"
      },
      {
        "paperId": "55635aac4cd439a00356f83dad52bd8d7b0ea87e",
        "title": "A Survey on Curriculum Learning"
      },
      {
        "paperId": "bf79d3d4bb3654c1d8f07fbe4a35556791aecac3",
        "title": "Barrier Function-based Safe Reinforcement Learning for Emergency Control of Power Systems"
      },
      {
        "paperId": "bbd57eeb6e256dc954603f94e0314fb617879af7",
        "title": "Lyapunov-Regularized Reinforcement Learning for Power System Transient Stability"
      },
      {
        "paperId": "6792c9e7e0cee85937a82075dcfeb604cdd08dd2",
        "title": "Reinforcement Learning for Selective Key Applications in Power Systems: Recent Advances and Future Challenges"
      },
      {
        "paperId": "9a8b32c211666e4614a1d35e8e6cd7bf6025db89",
        "title": "Reinforcement Learning and Its Applications in Modern Power and Energy Systems: A Review"
      },
      {
        "paperId": "7dd12e72248387e207ad2be1a3c84e7d826a50c9",
        "title": "Safe Reinforcement Learning for Emergency Load Shedding of Power Systems"
      },
      {
        "paperId": "af256ef2770b6195b4a03cdf0153eec231111e9f",
        "title": "A Data-Driven Multi-Agent Autonomous Voltage Control Framework Using Deep Reinforcement Learning"
      },
      {
        "paperId": "96055d058984b15a9b83024bb2e07292ee7559f5",
        "title": "Learning to be Safe: Deep RL with a Safety Critic"
      },
      {
        "paperId": "19e4e04e9c48bc86ddd849abdda2ec305c060694",
        "title": "First Order Constrained Optimization in Policy Space"
      },
      {
        "paperId": "d49b5590db7b5ae451012a7e3b2129dcf8d50704",
        "title": "Reinforcement Learning for Optimal Primary Frequency Control: A Lyapunov Approach"
      },
      {
        "paperId": "2fb7c8103d370937e9c91bfdeb794d47d1af31a4",
        "title": "Deep Reinforcement Learning Based Multi-Objective Integrated Automatic Generation Control for Multiple Continuous Power Disturbances"
      },
      {
        "paperId": "dba5020c60ef005891d76d7e24d4e5496d0e3f08",
        "title": "Conservative Stochastic Optimization With Expectation Constraints"
      },
      {
        "paperId": "30a7cc5400e70ef83d1ce0d9deaa4e824a955001",
        "title": "Safe reinforcement learning: A control barrier function optimization approach"
      },
      {
        "paperId": "629d0ce250581471f07083bbab95f23623b00201",
        "title": "Responsive Safety in Reinforcement Learning by PID Lagrangian Methods"
      },
      {
        "paperId": "77e998110ddcf073329aaa3742501a8fc7e9d209",
        "title": "Consensus Multi-Agent Reinforcement Learning for Volt-VAR Control in Power Distribution Networks"
      },
      {
        "paperId": "4f69dcff09b3f6d3af574efa77500a0fd8a2cb3e",
        "title": "Safe Off-Policy Deep Reinforcement Learning Algorithm for Volt-VAR Control in Power Distribution Systems"
      },
      {
        "paperId": "49dbb376220a1decfe16f7056ddfce27bdc7cafc",
        "title": "Batch-Constrained Reinforcement Learning for Dynamic Distribution Network Reconfiguration"
      },
      {
        "paperId": "d75cf795676f10168853e443d1ca7a3c663e4e9e",
        "title": "Safe Reinforcement Learning via Curriculum Induction"
      },
      {
        "paperId": "da502e643bdc793f6a1fdff76255c1834ce7b3a4",
        "title": "A Multi-Agent Deep Reinforcement Learning Method for Cooperative Load Frequency Control of a Multi-Area Power System"
      },
      {
        "paperId": "5e7bc93622416f14e6948a500278bfbe58cd3890",
        "title": "Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems"
      },
      {
        "paperId": "932709a41eee54f3eddbd3b3a7baf043c3ed33ec",
        "title": "Learning-Based Model Predictive Control: Toward Safe Learning in Control"
      },
      {
        "paperId": "bed9daf903a3a2a1fb9607812f7dfde9456339ae",
        "title": "Constrained EV Charging Scheduling Based on Safe Deep Reinforcement Learning"
      },
      {
        "paperId": "9ec0bd60df62b997bb371e65d254434f8851ea9b",
        "title": "Projection-Based Constrained Policy Optimization"
      },
      {
        "paperId": "e416f77c2f69a098053065de2f19c7144adc2d5c",
        "title": "Safe deep reinforcement learning-based constrained optimal control scheme for active distribution networks"
      },
      {
        "paperId": "e102cd42c402026a3862d2e60a75eed7c78860a2",
        "title": "Curriculum Learning for Reinforcement Learning Domains: A Framework and Survey"
      },
      {
        "paperId": "f0cf757b8a47b440e6239eab6932c5306bc0886a",
        "title": "Real-Time Residential Demand Response"
      },
      {
        "paperId": "b852ed038421aa0f2ba29ccf52b580c082a51cef",
        "title": "Load Frequency Control: A Deep Multi-Agent Reinforcement Learning Approach"
      },
      {
        "paperId": "003987bfff295e76946bf430376af4fe3d466cb4",
        "title": "Learning to Walk in the Real World with Minimal Human Effort"
      },
      {
        "paperId": "70fbbd3d718367b2c806bead0e4f1d805e136c23",
        "title": "Safe Intermittent Reinforcement Learning With Static and Dynamic Event Generators"
      },
      {
        "paperId": "dfb8cb9d7af8c1a01f834695bbf8527bddcd8044",
        "title": "MAMPS: Safe Multi-Agent Reinforcement Learning via Model Predictive Shielding"
      },
      {
        "paperId": "b094fc7057f77a6bf66ad07f6c68be148ca2b442",
        "title": "Robust Model Predictive Shielding for Safe Reinforcement Learning with Stochastic Dynamics"
      },
      {
        "paperId": "0bc855f84668b35cb65618d996d09f6e434d28c9",
        "title": "Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning"
      },
      {
        "paperId": "acf26b3fbb18ce7a94ab10f5eb14d89aff4a36a7",
        "title": "Building HVAC Scheduling Using Reinforcement Learning via Neural Network Based Model Approximation"
      },
      {
        "paperId": "d41d380f0579ff73a1c5beea31f6cfa164d82013",
        "title": "Multi-Agent Safe Policy Learning for Power Management of Networked Microgrids"
      },
      {
        "paperId": "7b981858503322d3cdc8ac1a7a1b0ba50efbdeca",
        "title": "Safety-Aware Reinforcement Learning Framework with an Actor-Critic-Barrier Structure"
      },
      {
        "paperId": "536c35a136e327a96be2446292936cd9141b10bb",
        "title": "Probabilistic Model Predictive Safety Certification for Learning-Based Control"
      },
      {
        "paperId": "f1341dcb7c1a4fc3a8fa3a77e2da3c2b2c9dfab8",
        "title": "Improving Safety in Reinforcement Learning Using Model-Based Architectures and Human Intervention"
      },
      {
        "paperId": "adcd4bfd88213da0c33d3cb7057411dd15b72c7a",
        "title": "End-to-End Safe Reinforcement Learning through Barrier Functions for Safety-Critical Continuous Control Tasks"
      },
      {
        "paperId": "edb817560aad2d63272350208f80a0e979c4adc6",
        "title": "Safety-Guided Deep Reinforcement Learning via Online Gaussian Process Estimation"
      },
      {
        "paperId": "3fa50569925cfecc66fed5ec616682ecf3794ad7",
        "title": "Lyapunov-based Safe Policy Optimization for Continuous Control"
      },
      {
        "paperId": "40234f50b9e38fa68292807cfc603de7a729484f",
        "title": "A predictive safety filter for learning-based control of constrained nonlinear dynamical systems"
      },
      {
        "paperId": "d864b7436e39cef3182bed8839818a8d61e35bb6",
        "title": "A review of current challenges and trends in energy systems modeling"
      },
      {
        "paperId": "cb7c479a36520da1caeeec67db10772351a390c6",
        "title": "Reward Constrained Policy Optimization"
      },
      {
        "paperId": "8bf1aa5dcae7183c5bd634da7e39bef8909306d4",
        "title": "Safe Reinforcement Learning via Formal Methods: Toward Safe Control Through Proof and Learning"
      },
      {
        "paperId": "ede9f86e18d39993589963205d56de7a926c7196",
        "title": "Learning-Based Model Predictive Control for Safe Exploration"
      },
      {
        "paperId": "b3f063fef361d48d95e8ca8ba4eff0d9fb1bbc0a",
        "title": "Linear Model Predictive Safety Certification for Learning-Based Control"
      },
      {
        "paperId": "6ac9952638cd0e08877253b465f7b7da0edfa511",
        "title": "Permissive Barrier Certificates for Safe Stabilization Using Sum-of-squares"
      },
      {
        "paperId": "8e655458d7a9aa2a7a1e67fe5cdf2e22fd65a8df",
        "title": "Barrier-Certified Adaptive Reinforcement Learning With Applications to Brushbot Navigation"
      },
      {
        "paperId": "95f9eb3aa328c18c0b863c63a18044fac41a3c3a",
        "title": "Smart Grids : Advanced Technologies and Solutions, Second Edition"
      },
      {
        "paperId": "ec4df801c640169e18d8da1bdc65a0b6fc2d3d94",
        "title": "Leave no Trace: Learning to Reset for Safe and Autonomous Reinforcement Learning"
      },
      {
        "paperId": "a62e187d6fe0bce62139a9d6fe7897ebcbed6f56",
        "title": "Safe Learning of Quadrotor Dynamics Using Barrier Certificates"
      },
      {
        "paperId": "e4b4066e46fd160cb84e246b0895a4cd674d8ce4",
        "title": "Safe Reinforcement Learning via Shielding"
      },
      {
        "paperId": "30ff82cebce6fdc2957043c4085a426414474d78",
        "title": "Trial without Error: Towards Safe Reinforcement Learning via Human Intervention"
      },
      {
        "paperId": "7a4193d0b042643a8bb9ec262ed7f9d509bdb12e",
        "title": "Constrained Policy Optimization"
      },
      {
        "paperId": "759bbd8dd50cb4790cad7a3bccbdfcbfee5e3e89",
        "title": "Risk-Constrained Reinforcement Learning with Percentile Risk Criteria"
      },
      {
        "paperId": "449532187c94af3dd3aa55e16d2c50f7854d2199",
        "title": "Trust Region Policy Optimization"
      },
      {
        "paperId": "687d0e59d5c35f022ce4638b3e3a6142068efc94",
        "title": "Deterministic Policy Gradient Algorithms"
      },
      {
        "paperId": "f332ecd5d54adf0530a39dae189cf6b160ad5c0e",
        "title": "An Online Actor\u2013Critic Algorithm with Function Approximation for Constrained Markov Decision Processes"
      },
      {
        "paperId": "d59a614b0389257faffae499f6ccfb57ab073958",
        "title": "Intelligent Automatic Generation Control"
      },
      {
        "paperId": "3f08e2ce1e7440440aecab0d732433e40e5b28fd",
        "title": "An actor-critic algorithm for constrained Markov decision processes"
      },
      {
        "paperId": "1c0f7087367315e4e8cd1d8654ab33db12663c2b",
        "title": "Lyapunov Design for Safe Reinforcement Learning"
      },
      {
        "paperId": "3cc2608fd77b9b65f5bd378e8797b2ab1b8acde7",
        "title": "Constrained Markov Decision Processes"
      },
      {
        "paperId": "cb89a25141febad0c14d080e2791c506ae6e4a76",
        "title": "Constrained Optimization and Lagrange Multiplier Methods"
      },
      {
        "paperId": "9e1ec82cb1a1876a265ffef284111a0a8ee4292c",
        "title": "Safety-Integrated Online Deep Reinforcement Learning for Mobile Energy Storage System Scheduling and Volt/VAR Control in Power Distribution Networks"
      },
      {
        "paperId": "a87f30f52f8e2ac3f1ce06d8dbdcf374ae7aa873",
        "title": "Provably Safe Reinforcement Learning: A Theoretical and Experimental Comparison"
      },
      {
        "paperId": "7e16fed4579b2dc0ecf9a633d0fbd1b7beb18c93",
        "title": "Mixed Deep Reinforcement Learning Considering Discrete-continuous Hybrid Action Space for Smart Home Energy Management"
      },
      {
        "paperId": "69d52bfe01ed25a7749e538f32e3acfec1a72e84",
        "title": "Safe Reinforcement Learning-Based Resilient Proactive Scheduling for a Commercial Building Considering Correlated Demand Response"
      },
      {
        "paperId": "2e5dc4b028e670d1b9ac0b4e15580cdb7197cb68",
        "title": "Exploration in Deep Reinforcement Learning: A Comprehensive Survey"
      },
      {
        "paperId": "8b3fb7ec4d54dd1dd8e775bf98491bb3e1f71eab",
        "title": "Safe Model-Based Reinforcement Learning Using Robust Control Barrier Functions"
      },
      {
        "paperId": "4d0f6a6ffcd6ab04732ff76420fd9f8a7bb649c3",
        "title": "Benchmarking Safe Exploration in Deep Reinforcement Learning"
      },
      {
        "paperId": "c0f2c4104ef6e36bb67022001179887e6600d24d",
        "title": "A comprehensive survey on safe reinforcement learning"
      },
      {
        "paperId": "145a42e83ec142a125da3ad845ee95027ef702e5",
        "title": "Ieee Transactions on Systems, Man and Cybernetics\u2014part C: Applications and Reviews 1 a Survey of Actor-critic Reinforcement Learning: Standard and Natural Policy Gradients"
      },
      {
        "paperId": "477a3dc8c6cc9d7be19c52d9cb0d13aa649ee535",
        "title": "Lecture Notes in Computer Science: Safe Exploration Techniques for Reinforcement Learning \u2013 an Overview"
      }
    ],
    "cited_by": [
      {
        "paperId": "a7405a86e919cda95860dca23299dfc90382a700",
        "title": "Holistic mutual benefits aware P2P2G market among microgrids in a distribution network: A decentralized data-driven approach"
      },
      {
        "paperId": "21cb37c5ef9520e0cc9425897f456bd0e71ec4ca",
        "title": "A Unified Model for Smart Meter Data Applications"
      },
      {
        "paperId": "2ceac8e5972e01035557eb1dae6294e8785dba80",
        "title": "An improved algorithm of YOLOv8 for defect detection in high voltage transmission lines"
      },
      {
        "paperId": "fffedecd3bfc931d53995914fba0d918c8d789a1",
        "title": "Distributed Chance-Constrained Optimal Dispatch for Integrated Energy System With Electro-Thermal Couple and Wind-Storage Coordination"
      },
      {
        "paperId": "7cb3285c941b8860cbfe24c6acf6ed623592d639",
        "title": "A Review of the Reinforcement Learning for Wind Farm Operation and Control"
      },
      {
        "paperId": "43132992cd466f073c790620675c503e43b84b8a",
        "title": "Safe dynamic optimization of automatic generation control via imitation-based reinforcement learning"
      },
      {
        "paperId": "c13236ec3b4090f50c28f461be717006c16fc050",
        "title": "Advancements in data-driven voltage control in active distribution networks: A Comprehensive review"
      },
      {
        "paperId": "9a0d705852c8aa0b07277d9b1235d49262081865",
        "title": "A Review of Safe Reinforcement Learning Methods for Modern Power Systems"
      }
    ],
    "score": 8.0
  },
  {
    "id": "11c34b84c3ad6587529517c32923c446797c63e6",
    "title": "Cooperative multi-agent reinforcement learning for multi-area integrated scheduling in wafer fabs",
    "authors": [
      "Ming Wang",
      "Jie Zhang",
      "Peng Zhang",
      "Mengyu Jin"
    ],
    "year": 2024,
    "citationCount": 7,
    "abstract": "The existing scheduling methods of wafer fabs focus on single area, achieving local optimisation while failing to realise global optimisation due to neglecting the coordination of multi-area. Therefore, it is necessary to consider the complex opposing relationships between multi-area caused by constraints such as batch processing, re-entrance, and multiple residency times within and between areas to conduct integrated scheduling and shorten the production cycle time. For this issue, this paper proposes a cooperative multi-agent reinforcement learning for multi-area integrated scheduling. Aiming at the dynamic batching and scheduling considering the dynamic arrival lots in multi-area, a multi-agent reinforcement learning algorithm is presented to learn the optimal dynamic batching and scheduling policy firstly. Subsequently, a cooperative multi-agent framework is raised to achieve the global optimisation and coordination of multi-area. Furthermore, an adaptive exploration strategy is constructed to enhance the global exploration capability of the complex solution space caused by residency time constraints and re-entrant property. Moreover, a policy share enhanced Double DQN is employed to improve the generalisation and adaptability of the multi-agent. Finally, the experiments demonstrate that the proposed integrated scheduling method has better comprehensive performance compared to the previous area-separated scheduling methods.",
    "url": "https://www.semanticscholar.org/paper/11c34b84c3ad6587529517c32923c446797c63e6",
    "pdf_url": "https://doi.org/10.1080/00207543.2024.2411615",
    "venue": "International Journal of Production Research",
    "publicationDate": "2024-10-23",
    "externalIds": {
      "DBLP": "journals/ijpr/WangZZJ25",
      "DOI": "10.1080/00207543.2024.2411615",
      "CorpusId": 273555753
    },
    "references": [],
    "cited_by": [
      {
        "paperId": "14d9359e1accfb6d688ff795dfd77d34ea26d437",
        "title": "Optimisation method for semiconductor wafer manufacturing system scheduling: Reinforcement learning with decision graph guiding"
      },
      {
        "paperId": "2476f52fe76076e436992dbf01257bc15b896a78",
        "title": "Online sequential decision making of multi-stage assembly process parameters based on deep reinforcement learning and its application in diesel engine production"
      },
      {
        "paperId": "51bb74cb595a253269e0f92f2f665aed2318f503",
        "title": "Green and on-time scheduling in real-world textile production: a genetic algorithm approach validated by MIP and benchmarked with ACO"
      },
      {
        "paperId": "9862552004784da767a600734fc278243b0b270c",
        "title": "A hybrid surrogate-assisted dual-population co-evolutionary algorithm for multi-area integrated scheduling in wafer fabs"
      },
      {
        "paperId": "c626543f54732b5507011200776735f2f2800230",
        "title": "Scalability of Reinforcement Learning Methods for Dispatching in Semiconductor Frontend Fabs: A Comparison of Open-Source Models with Real Industry Datasets"
      },
      {
        "paperId": "50da411722f04aaee4c7c36ad97fb480cc02715b",
        "title": "Scheduling Reentrant FlowShops: Reinforcement Learning\u2010guided Meta\u2010Heuristics"
      },
      {
        "paperId": "24bed8577585cc8cfdf69eb0444055769dd25741",
        "title": "Real-time response to machine failures in self-organizing production execution using multi-agent reinforcement learning with effective samples"
      }
    ],
    "score": 7.0
  },
  {
    "id": "d929e0bc4e30910527a714f239b5e5b37a7ec6ad",
    "title": "Multi-UAV Cooperative Target Assignment Method Based on Reinforcement Learning",
    "authors": [
      "Yunlong Ding",
      "Minchi Kuang",
      "Heng Shi",
      "Jiazhan Gao"
    ],
    "year": 2024,
    "citationCount": 6,
    "abstract": "To overcome the problems of traditional distributed target allocation algorithms in terms of lack of target strategic priority, poor scalability, and robustness, this paper proposes a proximal strategy optimization algorithm that combines threat assessment and attention mechanism (TAPPO). Based on the distributed training framework, the algorithm integrates a threat assessment and dynamic attention strategy and designs a dynamic reward function based on the current hit rate of the drone and the missile benefit ratio to improve the algorithm\u2019s exploration ability and scalability. Through an 8vs8 multi-UAV confrontation experiment in a digital twin simulation environment, the results show that the agent using the TAPPO algorithm for target allocation defeats the state machine with an 85% winning rate and is significantly better than other current mainstream target allocation algorithms, verifying the effectiveness of the algorithm.",
    "url": "https://www.semanticscholar.org/paper/d929e0bc4e30910527a714f239b5e5b37a7ec6ad",
    "pdf_url": "https://doi.org/10.3390/drones8100562",
    "venue": "Drones",
    "publicationDate": "2024-10-09",
    "externalIds": {
      "DOI": "10.3390/drones8100562",
      "CorpusId": 273280620
    },
    "references": [
      {
        "paperId": "2bc13bf877f20adf07d27368140ac385f60ffa7b",
        "title": "Autonomous UAV Maneuvering Decisions by Refining Opponent Strategies"
      },
      {
        "paperId": "3605d3c8628bf94b77dc6f16c7c9c00ceb1b5f39",
        "title": "Multi-UAV Collaborative Dynamic Task Allocation Method Based on ISOM and Attention Mechanism"
      },
      {
        "paperId": "01dc620b22c4403932b23465dbf18d555749171d",
        "title": "Cooperative task allocation with simultaneous arrival and resource constraint for multi-UAV using a genetic algorithm"
      },
      {
        "paperId": "7a3c2fad9dac134a1bb71b20aa72e876f1fd704d",
        "title": "A Hierarchical Reinforcement Learning Algorithm Based on Attention Mechanism for UAV Autonomous Navigation"
      },
      {
        "paperId": "c363b3218ffaccb3b5589a091933105b10072d72",
        "title": "Self-attention based deep direct recurrent reinforcement learning with hybrid loss for trading signal generation"
      },
      {
        "paperId": "8accce10e6f2127255652a3835ed09bbb0ca2a55",
        "title": "A Multi-Target Consensus-Based Auction Algorithm for Distributed Target Assignment in Cooperative Beyond-Visual-Range Air Combat"
      },
      {
        "paperId": "573976cbd2be5d4a2f2cc02fbb2f1bbf7d5bb317",
        "title": "Modeling and Solving the Dynamic Task Allocation Problem of Heterogeneous UAV Swarm in Unknown Environment"
      },
      {
        "paperId": "f24b2638d8b38331bd85b94d62e93da1785cdeb5",
        "title": "Multi-UAV Optimal Mission Assignment and Path Planning for Disaster Rescue Using Adaptive Genetic Algorithm and Improved Artificial Bee Colony Method"
      },
      {
        "paperId": "c0b526cb8061e792f2c4b7d00cba3db87bcece3c",
        "title": "A distributed task reassignment method in dynamic environment for multi-UAV system"
      },
      {
        "paperId": "48d7ed2ec57b99fce5e6716e3c88874ef87e7aee",
        "title": "Attention-based Reinforcement Learning for Real-Time UAV Semantic Communication"
      },
      {
        "paperId": "0e5564ef144fd602d2f0611fc9b2b06ad87d46f5",
        "title": "UAV Swarm Mission Planning in Dynamic Environment Using Consensus-Based Bundle Algorithm"
      },
      {
        "paperId": "0f205e6ebf93571243e903b7cdebad8a4b4ce899",
        "title": "Target Allocation Method of Multi-Aircraft Cooperative Air Combat Based on Improved Artificial Immune Algorithm"
      },
      {
        "paperId": "b34aa0606626cf170118e81d11dec701598ee6d9",
        "title": "Consensus-Based Decentralized Auctions for Robust Task Allocation"
      },
      {
        "paperId": "040a0969a67bfcb95254ed8984510176dea44548",
        "title": "Air-Combat Strategy Using Approximate Dynamic Programming"
      },
      {
        "paperId": "b4bd8d27aa26ff0b410ee7b44ffe2551158dfb3a",
        "title": "Autonomous Air Combat Maneuver Decision-Making Based on PPO-BWDA"
      },
      {
        "paperId": null,
        "title": "Attention-based recurrent PPO algorithm and its application"
      },
      {
        "paperId": null,
        "title": "Curriculum Learning-based Simulation of UAV Air Combat Under Sparse Rewards"
      },
      {
        "paperId": null,
        "title": "Intelligent decision making and target assignment of multi-aircraft air combat based on the LSTM\u2013PPO algorithm"
      },
      {
        "paperId": "ced7e84bdbb92f040f67772693048e968cb2ce52",
        "title": "Scene Overlap Prediction for LiDAR-Based Place Recognition"
      },
      {
        "paperId": null,
        "title": "Dynamic Target Assignment of Multiple Unmanned Aerial Vehicles Based on Clustering of Network Nodes"
      },
      {
        "paperId": null,
        "title": "Genetic algorithm based multi-UAV mission planning method considering temporal constraints"
      },
      {
        "paperId": "988575dff052c7f6182f9eb335011d1bc860b70d",
        "title": "Collaborative Decision-Making Method for Multi-UAV Based on Multiagent Reinforcement Learning"
      },
      {
        "paperId": "1c1166ff6f7ada0ceb12df7a900502592210bcd0",
        "title": "Energy Minimization for Cellular-Connected UAV: From Optimization to Deep Reinforcement Learning"
      },
      {
        "paperId": "1555e5ab5ccd9db9f1fba5989654218dfafea944",
        "title": "Multi-UAV Objective Assignment Using Hungarian Fusion Genetic Algorithm"
      },
      {
        "paperId": null,
        "title": "Dynamic resource allocation of drone swarms based on cooperative competitive public goods game"
      }
    ],
    "cited_by": [
      {
        "paperId": "5e1f2fddf2fabeb3b696cd8ab403933e9aa168c8",
        "title": "Efficient Target Assignment via Binarized SHP Path Planning and Plasticity-Aware RL in Urban Adversarial Scenarios"
      },
      {
        "paperId": "dfd2579a8b87028e2e716314906ade3843ce968c",
        "title": "Command-agent: Reconstructing warfare simulation and command decision-making using large language models"
      },
      {
        "paperId": "e2ad89b765b01fbab4ace39497c7f5ef38b33ea6",
        "title": "An End-to-End Solution for Large-Scale Multi-UAV Mission Path Planning"
      },
      {
        "paperId": "255c655015fe0694ec2c858eaf4025e6f48af6f5",
        "title": "Distributed Relative Pose Estimation for Multi-UAV Systems Based on Inertial Navigation and Data Link Fusion"
      },
      {
        "paperId": "2b4431cc052a88a5964a441c73b1a4383fe28bf4",
        "title": "Demand-Driven Hierarchical Task Allocation in UAV Swarms: A Distributed ADMM Optimization Approach"
      },
      {
        "paperId": "7121f8725f5abfbaa587dd328e53f6757e3371a2",
        "title": "Enhanced UAV Swarm Networking: a Distributed Density Peaks Clustering Approach*"
      }
    ],
    "score": 6.0
  },
  {
    "id": "516c6ab3feab17bc158f12ef6768b26c603566b8",
    "title": "Variable Speed Limit Intelligent Decision-Making Control Strategy Based on Deep Reinforcement Learning under Emergencies",
    "authors": [
      "Jingwen Yang",
      "Ping Wang",
      "Yongfeng Ju"
    ],
    "year": 2024,
    "citationCount": 5,
    "abstract": "Uncertain emergency events are inevitable and occur unpredictably on the highway. Emergencies with lane capacity drops cause local congestion and can even cause a second accident if the response is not timely. To address this problem, a self-triggered variable speed limit (VSL) intelligent decision-making control strategy based on the improved deep deterministic policy gradient (DDPG) algorithm is proposed, which can eliminate or alleviate congestion in a timely manner. The action noise parameter is introduced to improve exploration efficiency and stability in the early stage of the algorithm training and then maximizes differential traffic flow as the control objective, taking the real-time traffic state as the input. The reward function is constructed to explore the values of the speed limit. The results show that in terms of safety, under different traffic flow levels, the proposed strategy has improved by over 28.30% compared to other methods. In terms of efficiency, except for being inferior to the no-control condition during low-traffic-flow conditions, our strategy has improved over 7.21% compared to the others. The proposed strategy greatly benefits traffic sustainability in Intelligent Transport Systems (ITSs).",
    "url": "https://www.semanticscholar.org/paper/516c6ab3feab17bc158f12ef6768b26c603566b8",
    "pdf_url": "https://doi.org/10.3390/su16030965",
    "venue": "Sustainability",
    "publicationDate": "2024-01-23",
    "externalIds": {
      "DOI": "10.3390/su16030965",
      "CorpusId": 267232919
    },
    "references": [
      {
        "paperId": "3489d943424271da03ddcd06052fb8019cc374b8",
        "title": "Audio Related Quality of Experience Evaluation in Urban Transportation Environments With Brain Inspired Graph Learning"
      },
      {
        "paperId": "4bd304d3958416db01a67e4876f68b03b2e483fd",
        "title": "Assessing the effectiveness of speed limit reduction in Edmonton: A case study analysis."
      },
      {
        "paperId": "ac336d3ec2a265dcaf4cdc0e68630c84366e1df0",
        "title": "Effects of lowering speed limits on crash severity in Seattle."
      },
      {
        "paperId": "919c1ad57a7cc68fe0b2e0b7043f34351bcfa961",
        "title": "A comprehensive operation and maintenance assessment for intelligent highways: A case study in Hong Kong-Zhuhai-Macao bridge"
      },
      {
        "paperId": "20cc176a888bc34189d4ae6c659dbe5022f15fd7",
        "title": "Variable Speed Limit Control for the Motorway\u2013Urban Merging Bottlenecks Using Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "fe3224d52f1ca23408ed9952a14ffa04b0877f8d",
        "title": "Coordinated Variable Speed Limit Control for Consecutive Bottlenecks on Freeways Using Multiagent Reinforcement Learning"
      },
      {
        "paperId": "9b6abd068150e2b68be0592d0d8e3d54e46dcfc8",
        "title": "Emergency Vehicle Driving Assistance System Using Recurrent Neural Network with Navigational Data Processing Method"
      },
      {
        "paperId": "61781e5d47d001f45e4917dabf598baa4c13dcdb",
        "title": "Cooperative Decision-Making of Connected and Autonomous Vehicles in an Emergency"
      },
      {
        "paperId": "745e4e9d6272309e672522d77689cfc670da3c0f",
        "title": "Statistical Analysis of Major and Extra Serious Traffic Accidents on Chinese Expressways from 2011 to 2021"
      },
      {
        "paperId": "dd60e481c6fe6424b8f406a4ddc0625d866b7767",
        "title": "A new reinforcement learning-based variable speed limit control approach to improve traffic efficiency against freeway jam waves"
      },
      {
        "paperId": "fb779571b44b43a90e6740807c85ed46bfd2481d",
        "title": "Analysis of the Impact of Variable Speed Limits on Environmental Sustainability and Traffic Performance in Urban Networks"
      },
      {
        "paperId": "00394683332b11c1f6b733af654fd4fcb589a437",
        "title": "A Variable Speed Limit Control Based on Variable Cell Transmission Model in the Connecting Traffic Environment"
      },
      {
        "paperId": "41ac3b835128da805253915e8afa664a94b5862d",
        "title": "A reliability-based weather-responsive variable speed limit system to improve the safety of rural highways."
      },
      {
        "paperId": "4fdfe491096d5dcbfab7a6215c0e52ca469a9690",
        "title": "Online Set-Point Estimation for Feedback-Based Traffic Control Applications"
      },
      {
        "paperId": "812630d86883202b560b886d42480ee27507c225",
        "title": "Selection of the Speed Command Distance for Improved Performance of a Rule-Based VSL and Lane Change Control"
      },
      {
        "paperId": "6034d7a81ce7e90fe8b123096e1eb523e0eb3bb0",
        "title": "A linear Lagrangian model predictive controller of macro- and micro- variable speed limits to eliminate freeway jam waves"
      },
      {
        "paperId": "bdd78ad06d90c254bbb678361d0afd623f428b3a",
        "title": "Automatic generation of optimal road trajectory for the rescue vehicle in case of emergency on mountain freeway using reinforcement learning approach"
      },
      {
        "paperId": "ba470052c337713d57d50ebcd772dba81a29dd07",
        "title": "Automated vehicle-involved traffic flow studies: A survey of assumptions, models, speculations, and perspectives"
      },
      {
        "paperId": "4ed73f71e9c2d976159a06f79b534e9add6257fe",
        "title": "Variable Speed Limit and Ramp Metering for Mixed Traffic Flows: A Review and Open Questions"
      },
      {
        "paperId": "ec73dfc574047073c963fa9e065d2209b1854b41",
        "title": "Economic, Social, and Ecological Impact Evaluation of Traffic Network in Beijing\u2013Tianjin\u2013Hebei Urban Agglomeration Based on the Entropy Weight TOPSIS Method"
      },
      {
        "paperId": "a6205925b84ef2d90c726a856e6640ace59cdc40",
        "title": "Evaluating the safety effectiveness of a weather-based variable speed limit for a rural mountainous freeway in Wyoming"
      },
      {
        "paperId": "5c6bea898f9dd83ad330c415524585e005d733b2",
        "title": "A Novel Entropy-Fuzzy PIPRECIA-DEA Model for Safety Evaluation of Railway Traffic"
      },
      {
        "paperId": "368c2226789dd3d4dce72cbfa83e8ca6a1e60f5a",
        "title": "Differential variable speed limits control for freeway recurrent bottlenecks via deep actor-critic algorithm"
      },
      {
        "paperId": "c48d352330ce4dce4ec0281d4e627af5f99b1a5e",
        "title": "Spatiotemporal trajectory characteristic analysis for traffic state transition prediction near expressway merge bottleneck"
      },
      {
        "paperId": "33be3ba3871e2fc502ac4b5020a22794255b5882",
        "title": "Integrated Variable Speed Limits and Lane-Changing Control for Freeway Lane-Drop Bottlenecks"
      },
      {
        "paperId": "87f5fae45cec4c786ba98b35d329013f118e2528",
        "title": "Effects of connected vehicle-based variable speed limit under different foggy conditions based on simulated driving."
      },
      {
        "paperId": "2120fe9df407c7b154a1837d6b1672631bc35be6",
        "title": "Modified Traffic Flow Model with Connected Vehicle Microscopic Data for Proactive Variable Speed Limit Control"
      },
      {
        "paperId": "648ea87fe7f99ca8ea5090cb1ba40242299ef4c4",
        "title": "Reinforcement learning for demand response: A review of algorithms and modeling techniques"
      },
      {
        "paperId": "59b9577d0b883a0ee90ec280832e74f76d6b262f",
        "title": "Variable Speed Limit Control at Fixed Freeway Bottlenecks Using Connected Vehicles"
      },
      {
        "paperId": "b81fa7307dc1279d8f59a6c81e12aae0b1636fb1",
        "title": "Genealogy of traffic flow models"
      },
      {
        "paperId": "337ad3ccb439a0eaebfe54b6d8b47e1f04ad569c",
        "title": "Trajectory planning and tracking control for autonomous lane change maneuver based on the cooperative vehicle infrastructure system"
      },
      {
        "paperId": "7e5868aa21ed49e627c2600a9ba82a9069affa6b",
        "title": "THE CELL TRANSMISSION MODEL.."
      },
      {
        "paperId": null,
        "title": "Influence of Variable Speed Limit Control on Fuel and Electric Energy Consumption, and Exhaust Gas Emissions in Mixed Traffic Flows"
      },
      {
        "paperId": "bff7e6ab2efb5269c4f76cbf1c9ffcd9173f192d",
        "title": "Evaluation of Integrated Variable Speed Limit and Lane Change Control for Highway Traffic Flow"
      },
      {
        "paperId": "bef1b86ee7fed0a1a09e45e191ee18e006ddb9eb",
        "title": "Resilience Analysis of Transport Networks by Combining Variable Message Signs With Agent-Based Day-to-Day Dynamic Learning"
      }
    ],
    "cited_by": [
      {
        "paperId": "93c88e1fab94c89151770ea9ecd28a017a03090c",
        "title": "State Compensation Model in Adaptive Event-Triggered Predictive Control: A Novel Approach to Mitigating Moving Bottlenecks"
      },
      {
        "paperId": "6899367c70b675ef05ba34f97807529b97564de4",
        "title": "Safety and Sustainability Indicators of Variable Speed Limit Control in Intelligent Driver Models"
      },
      {
        "paperId": "ba8bdc22f82d54744d85df8da698711f97b6fe24",
        "title": "Optimal Speed Ranges for Different Vehicle Types for Exhaust Emission Control"
      },
      {
        "paperId": "70f032a7dd2ca73f50fdd430efe487384c4ea515",
        "title": "Traffic Signal Control with State-Optimizing Deep Reinforcement Learning and Fuzzy Logic"
      },
      {
        "paperId": "d49540f6419e865f4ccf9c5af29140453544d4a2",
        "title": "Enhancing Traffic Efficiency and Sustainability through Strategic Placement of Roadside Units and Variable Speed Limits in a Connected Vehicle Environment"
      }
    ],
    "score": 5.0
  },
  {
    "id": "f5e09973834f852237a7d9db6583c7e6615a907d",
    "title": "Reinforcement learning layout\u2010based optimal energy management in smart home: AI\u2010based approach",
    "authors": [
      "Sajjad Afroosheh",
      "Khodakhast Esapour",
      "Reza Khorram\u2010Nia",
      "Mazaher Karimi"
    ],
    "year": 2024,
    "citationCount": 5,
    "abstract": "This research addresses the pressing need for enhanced energy management in smart homes, motivated by the inefficiencies of current methods in balancing power usage optimization with user comfort. By integrating reinforcement learning and a unique column\u2010and\u2010constraint generation strategy, the study aims to fill this gap and offer a comprehensive solution. Furthermore, the increasing adoption of renewable energy sources like solar panels underscores the importance of developing advanced energy management techniques, driving the exploration of innovative approaches such as the one proposed herein. The constraint coordination game (CCG) method is designed to efficiently manage the power usage of each appliance, including the charging and discharging of the energy storage system. Additionally, a deep learning model, specifically a deep neural network, is employed to forecast indoor temperatures, which significantly influence the energy demands of the air conditioning system. The synergistic combination of the CCG method with deep learning\u2010based indoor temperature forecasting promises significant reductions in homeowner energy expenses while maintaining optimal appliance performance and user satisfaction. Testing conducted in simulated environments demonstrates promising results, showcasing a 12% reduction in energy costs compared to conventional energy management strategies.",
    "url": "https://www.semanticscholar.org/paper/f5e09973834f852237a7d9db6583c7e6615a907d",
    "pdf_url": "https://doi.org/10.1049/gtd2.13203",
    "venue": "IET Generation, Transmission &amp; Distribution",
    "publicationDate": "2024-07-23",
    "externalIds": {
      "DOI": "10.1049/gtd2.13203",
      "CorpusId": 271422600
    },
    "references": [
      {
        "paperId": "c804bed8634dbaa2225518199f1eb8be97c4f96a",
        "title": "The role of smart communities integrated with renewable energy resources, smart homes and electric vehicles in providing ancillary services: A tri-stage optimization mechanism"
      },
      {
        "paperId": "778608dda540b51592624f5a80924b4920ef2a20",
        "title": "A fully robust home energy management model considering real time price and on-board vehicle batteries"
      },
      {
        "paperId": "31065825099fb86abd47969887e0e373693d9543",
        "title": "Real-time energy scheduling for home energy management systems with an energy storage system and electric vehicle based on a supervised-learning-based strategy"
      },
      {
        "paperId": "d423d58a12c2caa14086c69ef79bca8d6c0e36b7",
        "title": "Optimal energy management strategy for a renewable based microgrid with electric vehicles and demand response program"
      },
      {
        "paperId": "9e02e87de1a5e5891cd10b1c06dc003d142efa52",
        "title": "Energy management of smart homes over fog-based IoT architecture"
      },
      {
        "paperId": "3afbead850747d4a98c24cca0af1f5e78a128396",
        "title": "An effective energy management Layout-Based reinforcement learning for household demand response in digital twin simulation"
      },
      {
        "paperId": "83cd216428499773378ae125ea3eafed7d3faddd",
        "title": "Chance-constrained model predictive control-based operation management of more-electric aircraft using energy storage systems under uncertainty"
      },
      {
        "paperId": "15508abf5c5b8880c60a003154e4382bde602067",
        "title": "Data-driven distributionally robust joint chance-constrained energy management for multi-energy microgrid"
      },
      {
        "paperId": "eb967641e09ef2026d2655872f3da265604daa8a",
        "title": "Occupancy detection and localization strategies for demand modulated appliance control in Internet of Things enabled home energy management system"
      },
      {
        "paperId": "b24943b3e61908fc80e663344db13afac2acee47",
        "title": "Transactive charging management of electric vehicles in office buildings: A distributionally robust chance-constrained approach"
      },
      {
        "paperId": "b3e32d28a2b2528f0a1f73b6ad6689448272631f",
        "title": "Strategic retail pricing and demand bidding of retailers in electricity market: a data-driven chance-constrained programming"
      },
      {
        "paperId": "fb8d674add01b44e26feebaba5bb766a874d1530",
        "title": "A Novel Hybrid Lexicographic-IGDT Methodology for Robust Multi-Objective Solution of Home Energy Management Systems"
      },
      {
        "paperId": "8234a5c0eee656403436e65994539510eb425bca",
        "title": "Optimal energy management in smart sustainable buildings \u2013 A chance-constrained model predictive control approach"
      },
      {
        "paperId": "31e559d4cd8eb8cc68767f87fa1d85ea27c6d067",
        "title": "A stochastic optimal scheduling of multi-microgrid systems considering emissions: A chance constrained model"
      },
      {
        "paperId": "68bfd427763b51779d5a0e74f4e6ec0595bc3e66",
        "title": "A chance-constrained energy management in multi-microgrid systems considering degradation cost of energy storage elements"
      },
      {
        "paperId": "2d3695ff319cec33ad60d8cdac1b6114e4fca1a8",
        "title": "Robust chance-constrained programming approach for the planning of fast-charging stations in electrified transportation networks"
      },
      {
        "paperId": "be7099de09aceb7a8da058a1a5b5d09c9cce0703",
        "title": "Chance-constrained bi-level optimal dispatching model and benefit allocation strategy for off-grid microgrid considering bilateral uncertainty of supply and demand"
      },
      {
        "paperId": "d1ab6667fe2c4affb7ad0e5d0546fb715f316afb",
        "title": "Robust energy management in isolated microgrids with hydrogen storage and demand response"
      },
      {
        "paperId": "a8caed2ef494ad98b1eb005ef0895300049b2e19",
        "title": "Optimal home energy management strategy: A reinforcement learning method with actor-critic using Kronecker-factored trust region"
      },
      {
        "paperId": "1d4aeedc5e33eacacf0bf684c86bfb434cf068d7",
        "title": "Bi-level energy management and pricing for community energy retailer incorporating smart buildings based on chance-constrained programming"
      },
      {
        "paperId": "f7cb70981c7ef068a9bb1716ae47ecf0bf3ac926",
        "title": "Fair operating envelopes under uncertainty using chance constrained optimal power flow"
      },
      {
        "paperId": "571a9331fc6da8f0ce8c6153f2858f285c046aae",
        "title": "Maximum Demand Flexibility from the Demand Response of a Big Group of Residential Homes"
      }
    ],
    "cited_by": [
      {
        "paperId": "31968f3ded8a685977d7c87a86f2cd38bf6f0abb",
        "title": "Dynamic appliance scheduling and energy management in smart homes using adaptive reinforcement learning techniques"
      },
      {
        "paperId": "50e68a4cb9171afe2bf63496463c7fdf54f5931c",
        "title": "Home energy management system based on applied real-time load scheduling for self-consumption enhancement"
      },
      {
        "paperId": "5696566c5533f6db7e0e644ef0b63de8f898b81a",
        "title": "A GUI Application for Real-Time Home Energy Management Using Uncertainty-Aware Proximal Policy Optimization with Energy Storage, Electric Vehicle, and Renewables"
      },
      {
        "paperId": "a2d7be91bd31fc6d320608457fcd8a4a6bdb0d84",
        "title": "Design of Home Lighting Products Based on Artificial Intelligence and Deep Learning Techniques"
      },
      {
        "paperId": "c7c79c2282c8678e31644c9b121fcf346e77ea95",
        "title": "Energy Demand Response in a Food-Processing Plant: A Deep Reinforcement Learning Approach"
      }
    ],
    "score": 5.0
  },
  {
    "id": "b43f8bacd80f32734cdff4f9d8c79e397872aed6",
    "title": "RL-PLUS: Countering Capability Boundary Collapse of LLMs in Reinforcement Learning with Hybrid-policy Optimization",
    "authors": [
      "Yihong Dong",
      "Xue Jiang",
      "Yongding Tao",
      "Huanyu Liu",
      "Kechi Zhang",
      "Lili Mou",
      "Rongyu Cao",
      "Yingwei Ma",
      "Jue Chen",
      "Binhua Li",
      "Zhi Jin",
      "Fei Huang",
      "Yongbin Li",
      "Ge Li"
    ],
    "year": 2025,
    "citationCount": 4,
    "abstract": "Reinforcement Learning with Verifiable Reward (RLVR) has significantly advanced the complex reasoning abilities of Large Language Models (LLMs). However, it struggles to break through the inherent capability boundaries of the base LLM, due to its essentially on-policy strategy coupled with LLM's immense action space and sparse reward. Critically, RLVR can lead to the capability boundary collapse, narrowing the LLM's problem-solving scope. To address this problem, we propose RL-PLUS, a novel hybrid-policy optimization approach for LLMs that synergizes internal exploitation with external data to achieve stronger reasoning capabilities and surpass the boundaries of base models. RL-PLUS integrates two core components, i.e., Multiple Importance Sampling to address distributional mismatch from external data, and Exploration-Based Advantage Function to guide the model towards high-value, unexplored reasoning paths. We provide both theoretical analysis and extensive experiments to demonstrate the superiority and generalizability of our approach. Compared with existing RLVR methods, RL-PLUS achieves 1) state-of-the-art performance on six math reasoning benchmarks; 2) superior performance on six out-of-distribution reasoning tasks; 3) consistent and significant gains across diverse model families, with average relative improvements up to 69.2\\%. Moreover, the analysis of Pass@k curves indicates that RL-PLUS effectively resolves the capability boundary collapse problem.",
    "url": "https://www.semanticscholar.org/paper/b43f8bacd80f32734cdff4f9d8c79e397872aed6",
    "pdf_url": "https://arxiv.org/pdf/2508.00222.pdf",
    "venue": "arXiv.org",
    "publicationDate": "2025-07-31",
    "externalIds": {
      "ArXiv": "2508.00222",
      "DBLP": "journals/corr/abs-2508-00222",
      "DOI": "10.48550/arXiv.2508.00222",
      "CorpusId": 280417279
    },
    "references": [
      {
        "paperId": "82f59319e581cdfe16a97fe29bec6215ad818a81",
        "title": "Learning What Reinforcement Learning Can't: Interleaved Online Fine-Tuning for Hardest Questions"
      },
      {
        "paperId": "5c1996d7e61fa615f45dc31950bede770b4df0bc",
        "title": "SuperRL: Reinforcement Learning with Supervision to Boost Language Model Reasoning"
      },
      {
        "paperId": "def268940f3bc1f3541693fbc565ff32e0c0dbc9",
        "title": "How Much Backtracking is Enough? Exploring the Interplay of SFT and RL in Enhancing LLM Reasoning"
      },
      {
        "paperId": "532c7d150d7bfece96e3c51ad0c8e4e18271912c",
        "title": "The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models"
      },
      {
        "paperId": "571365687980777d3c73a72f57adbc4b71835921",
        "title": "Thought-Augmented Policy Optimization: Bridging External Guidance and Internal Capabilities"
      },
      {
        "paperId": "3e8b2bed3e3f78ca67be9064c3866d8fbd94bca3",
        "title": "Step-wise Adaptive Integration of Supervised Fine-tuning and Reinforcement Learning for Task-Specific LLMs"
      },
      {
        "paperId": "1122b654f8b47c1aa9c04ff6bbe7561c798e2ad0",
        "title": "Reinforcement Learning for Reasoning in Large Language Models with One Training Example"
      },
      {
        "paperId": "847d63914c51e3855ed7f13e74157daaa4a37d94",
        "title": "Learning to Reason under Off-Policy Guidance"
      },
      {
        "paperId": "143e18bfd7c356592e7c1439738a3525d3e16279",
        "title": "Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?"
      },
      {
        "paperId": "d85788857fd230169e17638631b96335368043ed",
        "title": "VAPO: Efficient and Reliable Reinforcement Learning for Advanced Reasoning Tasks"
      },
      {
        "paperId": "d981ce332586e6a29f595cbdfa9347cf425e5cd0",
        "title": "Open-Reasoner-Zero: An Open Source Approach to Scaling Up Reinforcement Learning on the Base Model"
      },
      {
        "paperId": "de23d38bc2604dcf334dcc46aff217eb6bcd1fe1",
        "title": "Understanding R1-Zero-Like Training: A Critical Perspective"
      },
      {
        "paperId": "16db56b5a57f2e675e5c4f60ca0fbd5764915a5e",
        "title": "SimpleRL-Zoo: Investigating and Taming Zero Reinforcement Learning for Open Base Models in the Wild"
      },
      {
        "paperId": "dd4cfde3e135f799a9a71b4f57e13a29de89f7e3",
        "title": "DAPO: An Open-Source LLM Reinforcement Learning System at Scale"
      },
      {
        "paperId": "e23379a9752f57732d311f7a97af2c69af6fae7b",
        "title": "Process Reinforcement through Implicit Rewards"
      },
      {
        "paperId": "668075792a7ab40457d92e09da28d35c879271c3",
        "title": "Kimi k1.5: Scaling Reinforcement Learning with LLMs"
      },
      {
        "paperId": "34471a2fa18ea22efad5287cf4aeb18542c98a9b",
        "title": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "paperId": "ca912676c26541da92f88cfcfb83b668e0cfee51",
        "title": "UFT: Unifying Fine-Tuning of SFT and RLHF/DPO/UNA through a Generalized Implicit Reward Function"
      },
      {
        "paperId": "f2d0f3d47ae850f49a58f4977393bd0025af4bec",
        "title": "HybridFlow: A Flexible and Efficient RLHF Framework"
      },
      {
        "paperId": "9fb201282f53a4ce89f28cbe5026af78912aa8c1",
        "title": "Qwen2.5-Math Technical Report: Toward Mathematical Expert Model via Self-Improvement"
      },
      {
        "paperId": "1406bb4cb6801bc4767b661308118c888a9b09da",
        "title": "MMLU-Pro: A More Robust and Challenging Multi-Task Language Understanding Benchmark"
      },
      {
        "paperId": "afe0998d191f3ea8490c7df100a3ffc5dcc62c5e",
        "title": "LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code"
      },
      {
        "paperId": "c78350e81298ca87bc1d59b466fa40081232caaa",
        "title": "Teaching Large Language Models to Reason with Reinforcement Learning"
      },
      {
        "paperId": "bcf2c7e3f4ed64c8294c35a59220a26dd4f40060",
        "title": "OlympiadBench: A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems"
      },
      {
        "paperId": "35b142ea69598e6241f0011312128031df55895c",
        "title": "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models"
      },
      {
        "paperId": "1f2a20a6efaf83214861dddae4a38a83ae18fe32",
        "title": "DeepSeek-Coder: When the Large Language Model Meets Programming - The Rise of Code Intelligence"
      },
      {
        "paperId": "210b0a3d76e93079cc51b03c4115fde545eea966",
        "title": "GPQA: A Graduate-Level Google-Proof Q&A Benchmark"
      },
      {
        "paperId": "ab0e3d3e4d42369de5933a3b4c237780b41c0d77",
        "title": "Solving Quantitative Reasoning Problems with Language Models"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
        "title": "Training Verifiers to Solve Math Word Problems"
      },
      {
        "paperId": "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269",
        "title": "Evaluating Large Language Models Trained on Code"
      },
      {
        "paperId": "57d1e7ac339e783898f2c3b1af55737cbeee9fc5",
        "title": "Measuring Mathematical Problem Solving With the MATH Dataset"
      },
      {
        "paperId": "88bb0a28bb58d847183ec505dda89b63771bb495",
        "title": "Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge"
      },
      {
        "paperId": "38fb1902c6a2ab4f767d4532b28a92473ea737aa",
        "title": "Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm"
      },
      {
        "paperId": "79cfb51a51fc093f66aac8e858afe2e14d4a1f20",
        "title": "Focal Loss for Dense Object Detection"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "846aedd869a00c09b40f1f1f35673cb22bc87490",
        "title": "Mastering the game of Go with deep neural networks and tree search"
      },
      {
        "paperId": "340f48901f72278f6bf78a04ee5b01df208cc508",
        "title": "Human-level control through deep reinforcement learning"
      },
      {
        "paperId": "a9cd8efe9184dddb1bedbbec3a356c4dfb22fe63",
        "title": "Markov Decision Processes: Discrete Stochastic Dynamic Programming"
      },
      {
        "paperId": null,
        "title": "Openai o1 system card and model report. 2024. Technical report"
      },
      {
        "paperId": null,
        "title": "Numinamath: The largest public dataset in ai4maths with 860k pairs of competition math problems and solutions. https: //huggingface.co/datasets/Numinamath , 2024"
      },
      {
        "paperId": null,
        "title": "Preprint, July 2025"
      },
      {
        "paperId": null,
        "title": "There may not be aha moment in r1-zero-like training \u2014 a pilot study"
      }
    ],
    "cited_by": [
      {
        "paperId": "4017b4a752a1693751855aed11b339c0815175d7",
        "title": "EPO: Entropy-regularized Policy Optimization for LLM Agents Reinforcement Learning"
      },
      {
        "paperId": "030dbcea8a5a5e2296afe8d19b9e6fa5e7e7084b",
        "title": "The Choice of Divergence: A Neglected Key to Mitigating Diversity Collapse in Reinforcement Learning with Verifiable Reward"
      },
      {
        "paperId": "d4fce730c7ceb40d92612f9abd7eb12a6d13a652",
        "title": "Breaking the Exploration Bottleneck: Rubric-Scaffolded Reinforcement Learning for General LLM Reasoning"
      },
      {
        "paperId": "ea7849c0fe8b73940e8e1d1b63f093cfa1f7cd48",
        "title": "Intern-S1: A Scientific Multimodal Foundation Model"
      }
    ],
    "score": 4.0
  },
  {
    "id": "8357670aac3c98a71b454ab5bca89558f265369d",
    "title": "Two-Stage Evolutionary Reinforcement Learning for Enhancing Exploration and Exploitation",
    "authors": [
      "Qingling Zhu",
      "Xiaoqiang Wu",
      "Qiuzhen Lin",
      "Weineng Chen"
    ],
    "year": 2024,
    "citationCount": 4,
    "abstract": "The integration of Evolutionary Algorithm (EA) and Reinforcement Learning (RL) has emerged as a promising approach for tackling some challenges in RL, such as sparse rewards, lack of exploration, and brittle convergence properties. However, existing methods often employ actor networks as individuals of EA, which may constrain their exploratory capabilities, as the entire actor population will stop evolution when the critic network in RL falls into local optimal. To alleviate this issue, this paper introduces a Two-stage Evolutionary Reinforcement Learning (TERL) framework that maintains a population containing both actor and critic networks. TERL divides the learning process into two stages. In the initial stage, individuals independently learn actor-critic networks, which are optimized alternatively by RL and Particle Swarm Optimization (PSO). This dual optimization fosters greater exploration, curbing susceptibility to local optima. Shared information from a common replay buffer and PSO algorithm substantially mitigates the computational load of training multiple agents. In the subsequent stage, TERL shifts to a refined exploitation phase. Here, only the best individual undergoes further refinement, while the rest individuals continue PSO-based optimization. This allocates more computational resources to the best individual for yielding superior performance. Empirical assessments, conducted across a range of continuous control problems, validate the efficacy of the proposed TERL paradigm.",
    "url": "https://www.semanticscholar.org/paper/8357670aac3c98a71b454ab5bca89558f265369d",
    "pdf_url": "https://doi.org/10.1609/aaai.v38i18.30079",
    "venue": "AAAI Conference on Artificial Intelligence",
    "publicationDate": "2024-03-24",
    "externalIds": {
      "DBLP": "conf/aaai/ZhuWLC24",
      "DOI": "10.1609/aaai.v38i18.30079",
      "CorpusId": 268710173
    },
    "references": [],
    "cited_by": [
      {
        "paperId": "a3793c9de664caa5f0a013bee6f72aeef7ff981c",
        "title": "Synergistic integration of metaheuristics and machine learning: latest advances and emerging trends"
      },
      {
        "paperId": "d5b4d9b5e2e7bb2160552343c427e8d1a4eedb94",
        "title": "Surrogate-Assisted Evolutionary Reinforcement Learning Based on Autoencoder and Hyperbolic Neural Network"
      },
      {
        "paperId": "d863b996014d71ead1b932f3b60ff3854ecb3690",
        "title": "Evolutionary Reinforcement Learning via Cooperative Coevolution"
      },
      {
        "paperId": "a05632dbd276b854335568a258dde8c633d4d260",
        "title": "A Bayesian Ensemble Framework for Financial Event Detection via Reward-Bayesian Stochastic Search and Combinatorial Thompson Sampling"
      }
    ],
    "score": 4.0
  },
  {
    "id": "58db2247187ac01acabc1c2fa02f9b189772729e",
    "title": "An Intelligent Parameter Identification Method of DFIG Systems Using Hybrid Particle Swarm Optimization and Reinforcement Learning",
    "authors": [
      "Xuanchen Xiang",
      "Ruisheng Diao",
      "S. Bernadin",
      "Simon Y. Foo",
      "Fangyuan Sun",
      "Ayodeji S. Ogundana"
    ],
    "year": 2024,
    "citationCount": 4,
    "abstract": "Precise modeling of power systems is vital to ensure stability, reliability, and secure operations. In power industrial settings, model parameters can become skewed over time due to prolonged device usage or modifications made to the control systems. Doubly-Fed Induction Generator (DFIG), one of the most prevalent generators in wind farms, is sensitive to transient occurrences. Consequently, parameter calibration of DFIG becomes a crucial focal point in power system planning and operational studies. In this paper, two baseline approaches are first developed to identify the potentially harmful parameters of the DFIG system, including the Particle Swarm Optimization (PSO) method and the state-of-the-art off-policy Reinforcement Learning (RL) method, Soft Actor-Critic (SAC). The outcomes demonstrated that the SAC method outperformed PSO, resulting in an impressive reduction of 74.67% Mean Squared Error (MSE) and a more efficient testing period. In further exploration, a novel hybrid approach called SAC-PSO is developed, with SAC being the teacher of PSO to tackle scenarios with multiple potential solutions. The results exhibited an even greater enhancement over using SAC alone, leading to a remarkable reduction of 87.84% MSE during the testing phase. The proposed method can also effectively apply to a power plant incorporating multiple wind generators.",
    "url": "https://www.semanticscholar.org/paper/58db2247187ac01acabc1c2fa02f9b189772729e",
    "pdf_url": "https://doi.org/10.1109/ACCESS.2024.3379146",
    "venue": "IEEE Access",
    "publicationDate": "",
    "externalIds": {
      "DBLP": "journals/access/XiangDBFSO24",
      "DOI": "10.1109/ACCESS.2024.3379146",
      "CorpusId": 268524910
    },
    "references": [
      {
        "paperId": "ee8d260541366791e27a98c9615cfb21d12840e2",
        "title": "Deep reinforcement learning approach to optimize the driving performance of shield tunnelling machines"
      },
      {
        "paperId": "d31ab2d6e608dc771422d4124c6b6d2b0bcd91f6",
        "title": "Deep Reinforcement Learning Empowered Particle Swarm Optimization for Aerial Base Station Deployment"
      },
      {
        "paperId": "ce1f13f57760450f7f2dd900910a0e6e71e38adb",
        "title": "A new path plan method based on hybrid algorithm of reinforcement learning and particle swarm optimization"
      },
      {
        "paperId": "f843ae4e33b392f79799180cb6103e1501f105bb",
        "title": "A Full Freedom Pose Measurement Method of Industrial Robot Based on Reinforcement Learning Algorithm"
      },
      {
        "paperId": "32d0d3778aa82e95fbdc4a7fc1e843e4756a8233",
        "title": "Employing reinforcement learning to enhance particle swarm optimization methods"
      },
      {
        "paperId": "45aa6bb81215ee4acf4276a5c8173440cc86e2aa",
        "title": "On Multi-Event Co-Calibration of Dynamic Model Parameters Using Soft Actor-Critic"
      },
      {
        "paperId": "f378297ce6179b27eecb66a15ebbd9e336509088",
        "title": "Population size in Particle Swarm Optimization"
      },
      {
        "paperId": "fd86ed306c6e1ec6718f133298e51d5b338cdac3",
        "title": "Deep Learning for Model Parameter Calibration in Power Systems"
      },
      {
        "paperId": "552fb41e41cd3e05c64bf7cf0496a8fc6ae74aa2",
        "title": "Real-Time Model Calibration with Deep Reinforcement Learning"
      },
      {
        "paperId": "7eaede68d954eac6bc15b4b7bf8c9387ac81c83d",
        "title": "Industrial Power Load Forecasting Method Based on Reinforcement Learning and PSO-LSSVM"
      },
      {
        "paperId": "d734082ff76752100b2b87fd5ba9353d412c7b13",
        "title": "A novel policy gradient algorithm with PSO-based parameter exploration for continuous control"
      },
      {
        "paperId": "506daf04bcba6ac8ebbca0852e97a4355d34b51c",
        "title": "A new asynchronous reinforcement learning algorithm based on improved parallel PSO"
      },
      {
        "paperId": "bdf9b5be55e145b3085685b544386516e71ed8a9",
        "title": "An Improved Control Strategy for DFIG Low Voltage Ride-Through Using Optimal Demagnetizing method"
      },
      {
        "paperId": "12c0751b4f51ed833172a713b7e32390032ead93",
        "title": "Soft Actor-Critic Algorithms and Applications"
      },
      {
        "paperId": "857e5a8b4ac4b925f02b8cc98737cb7b41952afd",
        "title": "Calibrating Parameters of Power System Stability Models Using Advanced Ensemble Kalman Filter"
      },
      {
        "paperId": "b43d360993718350bbc5295e794dc792bc4998d0",
        "title": "Effects of Random Values for Particle Swarm Optimization Algorithm"
      },
      {
        "paperId": "811df72e210e20de99719539505da54762a11c6d",
        "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor"
      },
      {
        "paperId": "de80ccde5d18519af13dea6bbb6d185c86cf3007",
        "title": "Transient Responses of the Doubly-Fed Induction Generator Wind Turbine under Grid Fault Conditions"
      },
      {
        "paperId": "d0c49a9ed109cb8573217a9a0affbad7881b77a4",
        "title": "Reinforcement Learning with Particle Swarm Optimization Policy (PSO-P) in Continuous State and Action Spaces"
      },
      {
        "paperId": "e8b39996cc9f05f95625d97c2675faad7dd8677d",
        "title": "Dynamic state estimation and parameter calibration of a DFIG using the ensemble Kalman filter"
      },
      {
        "paperId": "9853bf43bae04db5abfd861344fbe63f1b11cbc9",
        "title": "Calibrating multi-machine power system parameters with the extended Kalman filter"
      },
      {
        "paperId": "5ec6bbada5707fc0c9fc81f59bc8517d560da66d",
        "title": "Parameter Tuning for Wind Turbine with Doubly Fed Induction Generator Using PSO"
      },
      {
        "paperId": "0efa18cc9b5a1911a0b1cc411ad55ac14a125295",
        "title": "Application of extended Kalman filter techniques for dynamic model parameter calibration"
      },
      {
        "paperId": "6b55ac0a9be252d59f42616c3fc0836104b118d4",
        "title": "Using a support vector machine (SVM) to improve generalization ability of load model parameters"
      },
      {
        "paperId": "6c1d7719940f6a142f09d8d3043cd9a3aed6b7af",
        "title": "Load modeling by finding support vectors of load data from field measurements"
      },
      {
        "paperId": "a9bfdc3fc2685abc304a8db4dab81124bbc5b133",
        "title": "Power system dynamic load identification and stability"
      },
      {
        "paperId": "275db979887e40978f4185d3f59e3f10a808ce4d",
        "title": "Intelligent Decision-Making of Load Balancing Using Deep Reinforcement Learning and Parallel PSO in Cloud Environment"
      },
      {
        "paperId": null,
        "title": "received the B.S. degree in electrical engineering from Huazhong University of Science and Technology, China, in 2017, and the M.S. degree from Florida State University"
      },
      {
        "paperId": "abd88de568a5b97d43b923c160a0de84921aeebd",
        "title": "Study for Performance Comparison of SFIG and DFIG Based Wind Turbines"
      },
      {
        "paperId": null,
        "title": "Specification of the Second Generation Generic Models for Wind Turbine Generators"
      },
      {
        "paperId": null,
        "title": "\u2018\u2018Load modeling andcalibrationtechniquesforpowersystemstudies"
      },
      {
        "paperId": null,
        "title": "\u2018\u2018Optimisationtechniquesforelectricalpowersystems.Part2:Heuristicoptimisationmethods"
      },
      {
        "paperId": null,
        "title": "Intelligent Parameter Identification Method of DFIG Systems"
      },
      {
        "paperId": null,
        "title": "Member, IEEE) received the Ph.D. degree from the Department of Electrical Engineering, Arizona State University"
      }
    ],
    "cited_by": [
      {
        "paperId": "92ee8929fa1e875b9d6158594751446e552b05d2",
        "title": "Research on the Identification of Simulation Error Regions Based on Graph Information in Power Systems"
      },
      {
        "paperId": "985820e79e2a2ceef783e6f5642876f589bc7b40",
        "title": "Parameter Identification of DFIG Drive System Based on Whale Optimization Algorithm"
      },
      {
        "paperId": "74a1c84af34c70e5e597139598e180e8ea909dff",
        "title": "Parameter Identification of Phase-Locked Loop in DFIG-Based Wind Turbine"
      },
      {
        "paperId": "3e9ed19a5e6e09e1a004c3a0ffa8b8319623b100",
        "title": "Digital Twin-Based Blockchain for Power Support in Networked Microgrids"
      }
    ],
    "score": 4.0
  },
  {
    "id": "e401ba782c2da93959582295089d3f04a051d6c1",
    "title": "Reinforcement Learning and Sim-to-Real Transfer of Reorientation and Landing Control for Quadruped Robots on Asteroids",
    "authors": [
      "Ji Qi",
      "Haibo Gao",
      "Huanli Su",
      "M. Huo",
      "Haitao Yu",
      "Zongquan Deng"
    ],
    "year": 2024,
    "citationCount": 4,
    "abstract": "In surface exploration missions, wheeled planetary vehicles have difficulty traveling on asteroids due to their weak gravitational fields. With the rapid development of hardware performance and control methods, quadruped robots have great potential to serve in asteroid exploration. When deploying or controlling the jumping motions of a quadruped robot on an asteroid, landing stability must be guaranteed. Under the weak and irregular gravitational fields of asteroids, the robot should be reoriented to an appropriate attitude for a smooth and stable landing. To achieve this objective, a model-free control method based on reinforcement learning (RL) was proposed. Sim-to-real transfer methods including domain randomization and transfer learning were proposed to address the sim-to-real problem. The original control model trained by RL was transferred to a new version that could be applied and tested on a real quadruped robot. An equivalent asteroid's weak gravitational environment experimental platform was for the first time designed and employed to test the performance of the proposed control scheme. Both the simulation and experimental results validated the effectiveness of the proposed controller training and sim-to-real transfer methods.",
    "url": "https://www.semanticscholar.org/paper/e401ba782c2da93959582295089d3f04a051d6c1",
    "pdf_url": "https://doi.org/10.1109/TIE.2024.3360604",
    "venue": "IEEE transactions on industrial electronics (1982. Print)",
    "publicationDate": "2024-11-01",
    "externalIds": {
      "DBLP": "journals/tie/QiGSHYD24",
      "DOI": "10.1109/TIE.2024.3360604",
      "CorpusId": 268191520
    },
    "references": [],
    "cited_by": [
      {
        "paperId": "1d367788ebcc2bb05c49ef4147f847093b4e8b2b",
        "title": "Goal-Oriented Quadruped Navigation in Dynamic Environments Using Reinforcement Learning"
      },
      {
        "paperId": "32b90237c92aa5ba8a4f48c857c33e2a044cfba0",
        "title": "PTRL: Prior Transfer Deep Reinforcement Learning for Legged Robots Locomotion"
      },
      {
        "paperId": "814ca36dae45689f9959524df0cc8ed069bb5dda",
        "title": "A Survey on Transfer Reinforcement Learning"
      },
      {
        "paperId": "0582734f80f907a5e298acc5241ef33a46bec347",
        "title": "Dynamic model updating of an asteroid probe considering experimental condition uncertainty"
      }
    ],
    "score": 4.0
  },
  {
    "id": "afa538f59cf2996837863be60a34eef5271a5ee9",
    "title": "Offline Reinforcement Learning for Asynchronous Task Offloading in Mobile Edge Computing",
    "authors": [
      "Bolei Zhang",
      "Fu Xiao",
      "Lifa Wu"
    ],
    "year": 2024,
    "citationCount": 4,
    "abstract": "Edge servers, which are located in close proximity to mobile users, have become key components for providing augmented computation and bandwidth. As the resources of edge servers are limited and shared, it is critical for the decentralized mobile users to determine the amount of offloaded workload, to avoid competition or waste of the public resources at the edge servers. Reinforcement learning (RL) methods, which are sequential and model-free, have been widely considered as a promising approach. However, directly deploying RL in edge computing remains elusive, since arbitrary exploration in real online environments often leads to poor user experience. To avoid the costly interactions, in this paper, we propose an offline RL framework which can be optimized by using a static offline dataset only. In essence, our method first trains a supervised offline model to simulate the edge computing environment dynamics, and then optimize the offloading policy in the offline environment with cost-free interactions. As the offloading requests are mostly asynchronous, we adopt a mean-field approach that treats all neighboring users as a single agent. The problem can then be simplified and reduced to a game between only two players. Moreover, we limit the length of the offline model rollout to ensure the simulated trajectories are accurate, so that the trained offloading policies can be generalized to unseen online environments. Theoretical analyses are conducted to validate the accuracy and convergence of our algorithm. In the experiments, we first train the offline simulation environment with a real historical data set, and then optimize the offloading policy in this environment model. The results show that our algorithm can converge very fast during training. In the execution, the algorithm still achieves high performance in the online environment.",
    "url": "https://www.semanticscholar.org/paper/afa538f59cf2996837863be60a34eef5271a5ee9",
    "pdf_url": "https://doi.org/10.1109/TNSM.2023.3316626",
    "venue": "IEEE Transactions on Network and Service Management",
    "publicationDate": "2024-02-01",
    "externalIds": {
      "DBLP": "journals/tnsm/ZhangXW24",
      "DOI": "10.1109/TNSM.2023.3316626",
      "CorpusId": 262072757
    },
    "references": [],
    "cited_by": [
      {
        "paperId": "0367eca86afb63f5240ecfe2026fe8dacc57d290",
        "title": "On-Dyn-CDA: A Real-Time Cost-Driven Task Offloading Algorithm for Vehicular Networks With Reduced Latency and Task Loss"
      },
      {
        "paperId": "fbb51728b3513536fa6dc06b053ddbfe708c3e9d",
        "title": "A hybrid approach to task offloading optimization: integrating hybrid whale genetic algorithm and reinforcement learning"
      },
      {
        "paperId": "838b64995020feb6a28ccc07ae3871651946f385",
        "title": "Task Offloading in Edge Computing System with Deep Reinforcement Learning and Reward Shaping"
      },
      {
        "paperId": "0265e5d2eb961b7228a8f6c459c7139137856794",
        "title": "QECO: A QoE-Oriented Computation Offloading Algorithm Based on Deep Reinforcement Learning for Mobile Edge Computing"
      }
    ],
    "score": 4.0
  },
  {
    "id": "88f98e3629a7aef2312f9e214ed2a75672113e4f",
    "title": "A modified evolutionary reinforcement learning for multi-agent region protection with fewer defenders",
    "authors": [
      "Siqing Sun",
      "Huachao Dong",
      "Tianbo Li"
    ],
    "year": 2024,
    "citationCount": 4,
    "abstract": "Autonomous region protection is a significant research area in multi-agent systems, aiming to empower defenders in preventing intruders from accessing specific regions. This paper presents a Multi-agent Region Protection Environment (MRPE) featuring fewer defenders, defender damages, and intruder evasion strategies targeting defenders. MRPE poses challenges for traditional protection methods due to its high nonstationarity and limited interception time window. To surmount these hurdles, we modify evolutionary reinforcement learning, giving rise to the corresponding multi-agent region protection method (MRPM). MRPM amalgamates the merits of evolutionary algorithms and deep reinforcement learning, specifically leveraging Differential Evolution (DE) and Multi-Agent Deep Deterministic Policy Gradient (MADDPG). DE facilitates diverse sample exploration and overcomes sparse rewards, while MADDPG trains defenders and expedites the DE convergence process. Additionally, an elite selection strategy tailored for multi-agent systems is devised to enhance defender collaboration. The paper also presents ingenious designs for the fitness and reward functions to effectively drive policy optimizations. Finally, extensive numerical simulations are conducted to validate the effectiveness of MRPM.",
    "url": "https://www.semanticscholar.org/paper/88f98e3629a7aef2312f9e214ed2a75672113e4f",
    "pdf_url": "https://doi.org/10.1007/s40747-024-01385-4",
    "venue": "Complex &amp; Intelligent Systems",
    "publicationDate": "2024-02-22",
    "externalIds": {
      "DOI": "10.1007/s40747-024-01385-4",
      "CorpusId": 267996740
    },
    "references": [
      {
        "paperId": "e6a1c36368afd1dffdfcd88ffcd7a6f4373d08a4",
        "title": "Hysteresis quantified control for switched reaction\u2013diffusion systems and its application"
      },
      {
        "paperId": "ed86f984b879485a21d5569a5322ae0d7c6f1730",
        "title": "Optimization of reward shaping function based on genetic algorithm applied to a cross validated deep deterministic policy gradient in a powered landing guidance problem"
      },
      {
        "paperId": "2b7660227f3cbd6b7fec994c493908088bcf55f6",
        "title": "Research on Dynamic Scheduling Model of Plant Protection UAV Based on Levy Simulated Annealing Algorithm"
      },
      {
        "paperId": "efd4c14d9007765d7b2d8ecb4b96676a937a8214",
        "title": "Proximal policy optimization guidance algorithm for intercepting near-space maneuvering targets"
      },
      {
        "paperId": "b1e3a55440160b56ae5dd9c452d9f87837a0a95d",
        "title": "Robust point\u2010to\u2010point iterative learning control for\u00a0constrained systems: A minimum energy approach"
      },
      {
        "paperId": "ab19fc37a9065b0a464a4cfdf34b7bea26d79989",
        "title": "Surrogate-assisted hierarchical learning water cycle algorithm for high-dimensional expensive optimization"
      },
      {
        "paperId": "30c9d9cc7bbb905a2e3d1b88e273eeb3e529f717",
        "title": "Dissipativity-based finite-time asynchronous output feedback control for wind turbine system via a hidden Markov model"
      },
      {
        "paperId": "f675bf9ff260f2555719329ede5b7321b5517b6f",
        "title": "A survey for deep reinforcement learning in markovian cyber-physical systems: Common problems and solutions"
      },
      {
        "paperId": "586ae997fb9ec8a31e5db75d567cf6ef90403aa1",
        "title": "Reinforcement learning technology for air combat confrontation of unmanned aerial vehicle"
      },
      {
        "paperId": "33286daf5e3347f11f2c7052ecc2e3279a487885",
        "title": "Hawk and pigeon\u2019s intelligence for UAV swarm dynamic combat game via competitive learning pigeon-inspired optimization"
      },
      {
        "paperId": "67bf57595409a6a3f2aa50f69490ad5d431d5903",
        "title": "UAV Swarm Confrontation Using Hierarchical Multiagent Reinforcement Learning"
      },
      {
        "paperId": "f39bc068858519569162022a63b74df6d74b3613",
        "title": "A meta-heuristic assisted underwater glider path planning method"
      },
      {
        "paperId": "ed70c152134b6404e141c4c89b8a35ff33844a4c",
        "title": "Sliding mode control of multi-agent system with application to UAV air combat"
      },
      {
        "paperId": "a4f6d084903c09738ae92a3b33d29b8020caa1fa",
        "title": "An adaptive bi-level task planning strategy for multi-USVs target visitation"
      },
      {
        "paperId": "e0642272d01afd867c090c7beddd37218616fcfd",
        "title": "Reinforcement learning in robotic applications: a comprehensive survey"
      },
      {
        "paperId": "54fb6d19b34b468f87d95f02d798840383ab9d8a",
        "title": "A deep reinforcement learning-based method applied for solving multi-agent defense and attack problems"
      },
      {
        "paperId": "1209536e099bd4205281303309671ee72d1363bc",
        "title": "Harbour protection: moving invasion target interception for multi-AUV based on prediction planning interception method"
      },
      {
        "paperId": "361c00b22e29d0816ca896513d2c165e26399821",
        "title": "Grandmaster level in StarCraft II using multi-agent reinforcement learning"
      },
      {
        "paperId": "f7c15e9ac6653330b7dd18a89301a3b333927db3",
        "title": "A review of cooperative multi-agent deep reinforcement learning"
      },
      {
        "paperId": "40f54e88a391214afc7a939b7b40e2375a64b5d6",
        "title": "Reinforcement learning versus evolutionary computation: A survey on hybrid algorithms"
      },
      {
        "paperId": "c27db32efa8137cbf654902f8f728f338e55cd1c",
        "title": "Mastering the game of Go without human knowledge"
      },
      {
        "paperId": "d22192c9c551fa9e56eabd514aaa16fa15fc1919",
        "title": "A nature inspired optimal control of pneumatic-driven parallel robot platform"
      },
      {
        "paperId": "368b90210770af86654edf889c7b6ba2b1bf3e7e",
        "title": "Distributed cooperative guidance of multiple anti-ship missiles with arbitrary impact angle constraint"
      },
      {
        "paperId": "4b7588b32f32680e2fa0a7738dc9dd74ad466d63",
        "title": "Model-predictive asset guarding by team of autonomous surface vehicles in environment with civilian boats"
      },
      {
        "paperId": "340f48901f72278f6bf78a04ee5b01df208cc508",
        "title": "Human-level control through deep reinforcement learning"
      },
      {
        "paperId": "2463c15970e77160e41a753f119307030ab552e4",
        "title": "Optimal cascade hydraulic control for a parallel robot platform by PSO"
      },
      {
        "paperId": "8cc39cde9fce311412eaefe3b6e16965dafa42ce",
        "title": "A Nature Inspired Parameter Tuning Approach to Cascade Control for Hydraulically Driven Parallel Robot Platform"
      }
    ],
    "cited_by": [
      {
        "paperId": "65c3646777885606f7bdab9d97ca2c22c737e832",
        "title": "A time advantage function-based solution to two-player circular target defense games"
      },
      {
        "paperId": "6686bc08d57cbe8d8ed270274aebbcbec322c6ba",
        "title": "ERLNEIL-MDP: Evolutionary reinforcement learning with novelty-driven exploration for medical data processing"
      },
      {
        "paperId": "eb0fb41ffbad8616e6e96a144d06a1c5da5baf3a",
        "title": "Multi-missile coordinated penetration strategy based on hierarchical reinforcement learning in reduced space"
      },
      {
        "paperId": "2bdd77f7cf9acd4be8d95620bfb2b65717bc6549",
        "title": "Identifying Community-Bridge Network Structures via Bayesian Learning With Mixed Sparsity Mode"
      }
    ],
    "score": 4.0
  },
  {
    "id": "c4aafb184f285d004d8c8072b5d6408e876428e1",
    "title": "Reinforcement Learning for an Efficient and Effective Malware Investigation during Cyber Incident Response",
    "authors": [
      "Dipo Dunsin",
      "Mohamed Chahine Ghanem",
      "Karim Ouazzane",
      "Vassil T. Vassilev"
    ],
    "year": 2024,
    "citationCount": 4,
    "abstract": "This research focused on enhancing post-incident malware forensic investigation using reinforcement learning RL. We proposed an advanced MDP post incident malware forensics investigation model and framework to expedite post incident forensics. We then implement our RL Malware Investigation Model based on structured MDP within the proposed framework. To identify malware artefacts, the RL agent acquires and examines forensics evidence files, iteratively improving its capabilities using Q Table and temporal difference learning. The Q learning algorithm significantly improved the agent ability to identify malware. An epsilon greedy exploration strategy and Q learning updates enabled efficient learning and decision making. Our experimental testing revealed that optimal learning rates depend on the MDP environment complexity, with simpler environments benefiting from higher rates for quicker convergence and complex ones requiring lower rates for stability. Our model performance in identifying and classifying malware reduced malware analysis time compared to human experts, demonstrating robustness and adaptability. The study highlighted the significance of hyper parameter tuning and suggested adaptive strategies for complex environments. Our RL based approach produced promising results and is validated as an alternative to traditional methods notably by offering continuous learning and adaptation to new and evolving malware threats which ultimately enhance the post incident forensics investigations.",
    "url": "https://www.semanticscholar.org/paper/c4aafb184f285d004d8c8072b5d6408e876428e1",
    "pdf_url": "https://arxiv.org/pdf/2408.01999.pdf",
    "venue": "arXiv.org",
    "publicationDate": "2024-08-04",
    "externalIds": {
      "ArXiv": "2408.01999",
      "DBLP": "journals/corr/abs-2408-01999",
      "DOI": "10.48550/arXiv.2408.01999",
      "CorpusId": 271709673
    },
    "references": [
      {
        "paperId": "971f604f53a15a38a2c50513321b516d55a29ca6",
        "title": "AI-Enabled System for Efficient and Effective Cyber Incident Detection and Response in Cloud Environments"
      },
      {
        "paperId": "13c0364cc7efe33f6ea84e8d5f9905fa65dbfc54",
        "title": "A Comprehensive Analysis of the Role of Artificial Intelligence and Machine Learning in Modern Digital Forensics and Incident Response"
      },
      {
        "paperId": "6a5a05818c28a803ef8b4b2192b8d5135fc5a6df",
        "title": "D2WFP: A Novel Protocol for Forensically Identifying, Extracting, and Analysing Deep and Dark Web Browsing Activities"
      },
      {
        "paperId": "c3c533e0c85509b6d2f5b7d400c4203ac6deb35f",
        "title": "H4rm0ny: A Competitive Zero-Sum Two-Player Markov Game for Multi-Agent Learning on Evasive Malware Generation and Detection"
      },
      {
        "paperId": "a18a0c4bc98eb44e75cb40ae2f5156fa4171c534",
        "title": "MERLIN - Malware Evasion with Reinforcement LearnINg"
      },
      {
        "paperId": "2d9fcef2455d8a4a4e953b026d915a1fa84dc253",
        "title": "Binary Black-Box Attacks Against Static Malware Detectors with Reinforcement Learning in Discrete Action Spaces"
      },
      {
        "paperId": "ac006d13d98d80c5bde2a6f9a3d41af56bf20153",
        "title": "Deep Learning for Android Malware Defenses: A Systematic Literature Review"
      },
      {
        "paperId": "1ec63b25e4075007d3fb38f12276a731cfbf366f",
        "title": "Using Knowledge Graphs and Reinforcement Learning for Malware Analysis"
      },
      {
        "paperId": "3cbf5c0e2e0c14607128c6512803d0624ab0a359",
        "title": "Policy Teaching in Reinforcement Learning via Environment Poisoning Attacks"
      },
      {
        "paperId": "c1a0ecd81c830074986a5a6044b034986afc455b",
        "title": "Reinforcement Learning for Anti-Ransomware Testing"
      },
      {
        "paperId": "eee7f7ee91b024b1c85e232e6e2032e4e9aad439",
        "title": "Cost-Effective Malware Detection as a Service Over Serverless Cloud Using Deep Reinforcement Learning"
      },
      {
        "paperId": "86b94e67976a57480289f0c8e2bb66002dda14d8",
        "title": "A Comprehensive Review on Malware Detection Approaches"
      },
      {
        "paperId": "3e02b3099a4ff64d14973704879637552129862e",
        "title": "A Deep Reinforcement Learning Malware Detection Method Based on PE Feature Distribution"
      },
      {
        "paperId": "a568d21c879933796e918b16c7b7f4bed074d5de",
        "title": "A survey of game theoretic approach for adversarial machine learning"
      },
      {
        "paperId": "4742fcc4c15acb6fd59df85ea0de9af51274e7f3",
        "title": "Learning-Based Resource Allocation in Cloud Data Center using Advantage Actor-Critic"
      },
      {
        "paperId": "1ef51dd2dd6a08dac525dcfb5e3d9bc0fa817426",
        "title": "Evading Anti-Malware Engines With Deep Reinforcement Learning"
      },
      {
        "paperId": "94358f789c1d7fa01127c4b2f06b20cd7a453ff8",
        "title": "Learning to Evade Static PE Machine Learning Malware Models via Reinforcement Learning"
      },
      {
        "paperId": "03a507a0876c7e1a26608358b1a9dd39f1eb08e0",
        "title": "Adversarial Examples: Attacks and Defenses for Deep Learning"
      },
      {
        "paperId": "4417dfcfc722b8b31278a0ebcc1595963dab5a1c",
        "title": "Malware Detection by Eating a Whole EXE"
      },
      {
        "paperId": "5a4247676b9fc546410b731e72c38c950f54ce65",
        "title": "DeepDGA: Adversarially-Tuned Domain Generation and Detection"
      },
      {
        "paperId": "846aedd869a00c09b40f1f1f35673cb22bc87490",
        "title": "Mastering the game of Go with deep neural networks and tree search"
      },
      {
        "paperId": "d891dc72cbd40ffaeefdc79f2e7afe1e530a23ad",
        "title": "Intriguing properties of neural networks"
      },
      {
        "paperId": "9334213d5a98bcb7e58a500b8a60aa25b209be01",
        "title": "Review: Rethinking Miscarriages of Justice: Beyond the Tip of the Iceberg Michael Naughton Palgrave Macmillan, Houndsmills, 2008, 233pp, ISBN 978\u20140\u2014230\u201401906\u20145, \u00a345.00 (hbk)"
      },
      {
        "paperId": "76cb5a926a789e1f28035fcc7fa01d5900df6d0e",
        "title": "Perceptron-based learning algorithms"
      },
      {
        "paperId": null,
        "title": "Mab-malware: A reinforcement learning framework for attacking static malware classifiers"
      },
      {
        "paperId": "22b1b5989e08ddf7c73155cd0cf70013c3882697",
        "title": "Feature Selection for Malware Detection Based on Reinforcement Learning"
      },
      {
        "paperId": "1b570ed4b58908444465823880cb88fbb812b4fc",
        "title": "Evading Machine Learning Malware Detection"
      },
      {
        "paperId": null,
        "title": "Append the (current_q, new_q, episode, action) to the storage list"
      },
      {
        "paperId": null,
        "title": "Store Q-value updates if specific conditions (e.g., state, action) are met"
      },
      {
        "paperId": null,
        "title": "Update the Q-table with the new Q-value"
      },
      {
        "paperId": null,
        "title": "2022. Malware Analysis and Detection Using Machine Learning Algorithms"
      },
      {
        "paperId": null,
        "title": "Update state: Set the current state to the next state"
      }
    ],
    "cited_by": [
      {
        "paperId": "cb4960c3a896aa8064a1151351f4628acd2cd09a",
        "title": "Optimizing Cybersecurity Incident Response via Adaptive Reinforcement Learning"
      },
      {
        "paperId": "c7f8d6ef91752ca72ba171cf6e69ce79b97a1aec",
        "title": "A Novel Reinforcement Learning Model for Post-Incident Malware Investigations"
      },
      {
        "paperId": "971f604f53a15a38a2c50513321b516d55a29ca6",
        "title": "AI-Enabled System for Efficient and Effective Cyber Incident Detection and Response in Cloud Environments"
      },
      {
        "paperId": "4b81338b7a641fd20a58b80c64319673eca90e8b",
        "title": "Advancing Cyber Incident Timeline Analysis Through Rule Based AI and Large Language Models"
      }
    ],
    "score": 4.0
  },
  {
    "id": "e825e6325dfb5b93066817e7cc6b226ba7d3b799",
    "title": "Bayesian Design Principles for Offline-to-Online Reinforcement Learning",
    "authors": [
      "Haotian Hu",
      "Yiqin Yang",
      "Jianing Ye",
      "Chengjie Wu",
      "Ziqing Mai",
      "Yujing Hu",
      "Tangjie Lv",
      "Changjie Fan",
      "Qianchuan Zhao",
      "Chongjie Zhang"
    ],
    "year": 2024,
    "citationCount": 4,
    "abstract": "Offline reinforcement learning (RL) is crucial for real-world applications where exploration can be costly or unsafe. However, offline learned policies are often suboptimal, and further online fine-tuning is required. In this paper, we tackle the fundamental dilemma of offline-to-online fine-tuning: if the agent remains pessimistic, it may fail to learn a better policy, while if it becomes optimistic directly, performance may suffer from a sudden drop. We show that Bayesian design principles are crucial in solving such a dilemma. Instead of adopting optimistic or pessimistic policies, the agent should act in a way that matches its belief in optimal policies. Such a probability-matching agent can avoid a sudden performance drop while still being guaranteed to find the optimal policy. Based on our theoretical findings, we introduce a novel algorithm that outperforms existing methods on various benchmarks, demonstrating the efficacy of our approach. Overall, the proposed approach provides a new perspective on offline-to-online RL that has the potential to enable more effective learning from offline data.",
    "url": "https://www.semanticscholar.org/paper/e825e6325dfb5b93066817e7cc6b226ba7d3b799",
    "pdf_url": "https://arxiv.org/pdf/2405.20984.pdf",
    "venue": "International Conference on Machine Learning",
    "publicationDate": "2024-05-31",
    "externalIds": {
      "DBLP": "conf/icml/0006YYWMHLFZZ24",
      "ArXiv": "2405.20984",
      "DOI": "10.48550/arXiv.2405.20984",
      "CorpusId": 270199759
    },
    "references": [
      {
        "paperId": "10d0c7cdb266e8c9a6f7211d468328a37c8e4c16",
        "title": "Bayesian Design Principles for Frequentist Sequential Learning"
      },
      {
        "paperId": "5cb2c8e049ab1fac04aca46ae3c467b9a4f775ab",
        "title": "Actor-Critic Alignment for Offline-to-Online Reinforcement Learning"
      },
      {
        "paperId": "f5e1993f3f505e8fbb9cac9231285c8c9f1712a7",
        "title": "Cal-QL: Calibrated Offline RL Pre-Training for Efficient Online Fine-Tuning"
      },
      {
        "paperId": "bd38cbbb346a347cb5b60ac4a133b3d73cb44e07",
        "title": "Efficient Online Reinforcement Learning with Offline Data"
      },
      {
        "paperId": "7f270c9b098727675c9d8b893e362b561d61f27e",
        "title": "Policy Expansion for Bridging Offline-to-Online Reinforcement Learning"
      },
      {
        "paperId": "9d1445f1845a2880ff9c752845660e9c294aa7b5",
        "title": "Flow to Control: Offline Reinforcement Learning with Lossless Primitive Discovery"
      },
      {
        "paperId": "9bdad7f737c1303f199384c5b65dc67da7e5c4d8",
        "title": "Offline Q-Learning on Diverse Multi-Task Data Both Scales And Generalizes"
      },
      {
        "paperId": "6d7e11f7cb280df884e017002d2e4eb30985629e",
        "title": "Leveraging Offline Data in Online Reinforcement Learning"
      },
      {
        "paperId": "49bbd1064f1a17f314f8c61528bcce959a7c249b",
        "title": "GEC: A Unified Framework for Interactive Decision Making in MDP, POMDP, and Beyond"
      },
      {
        "paperId": "fb3dc5e20e0a71134ca916f0d6d8d41f01225b4b",
        "title": "Scaling Laws for Reward Model Overoptimization"
      },
      {
        "paperId": "2d33f309f7e92c75434a2bb16f70d6ec65ab7d2a",
        "title": "Hybrid RL: Using Both Offline and Online Data Can Make RL Efficient"
      },
      {
        "paperId": "3d444eafdd2cd17af73cc88cdbe35e6e21795ff2",
        "title": "The Role of Coverage in Online Reinforcement Learning"
      },
      {
        "paperId": "7b6b72bfe8ba1a3ab07dc641c163676a8ae5e59d",
        "title": "A Provably Efficient Model-Free Posterior Sampling Method for Episodic Reinforcement Learning"
      },
      {
        "paperId": "c24b813f60519029115fdef1142148b57a8a3b9c",
        "title": "Offline RL Policies Should be Trained to be Adaptive"
      },
      {
        "paperId": "514a1e7af5884a8e1c6dae488fa618787b7a69f7",
        "title": "On the Role of Discount Factor in Offline Reinforcement Learning"
      },
      {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "paperId": "eb92a453cf982126fa2125d4c8915352a52af54d",
        "title": "Online Decision Transformer"
      },
      {
        "paperId": "88e488a08dcd629c8ce90099bd8ab0a87f10cafb",
        "title": "Magnetic control of tokamak plasmas through deep reinforcement learning"
      },
      {
        "paperId": "b7ba291af983b5e31f3adfa9a0ceb7a7b3114c7f",
        "title": "Offline Reinforcement Learning with Value-based Episodic Memory"
      },
      {
        "paperId": "4ff8454c524163bbc5d25f6c8984b1c31ad057e4",
        "title": "Pessimistic Model-based Offline Reinforcement Learning under Partial Coverage"
      },
      {
        "paperId": "33b456eb43e5391761540f17a29e598d7595565b",
        "title": "Offline-to-Online Reinforcement Learning via Balanced Replay and Pessimistic Q-Ensemble"
      },
      {
        "paperId": "c879b25308026d6538e52b27bcf4fd3cb60855f3",
        "title": "A Minimalist Approach to Offline Reinforcement Learning"
      },
      {
        "paperId": "d769ca62d90adc7e7869849a421426bdc54a32fb",
        "title": "Policy Finetuning: Bridging Sample-Efficient Offline and Online Reinforcement Learning"
      },
      {
        "paperId": "9b4cd7b9865a34c8dffb8bfb516a8a66717d0c58",
        "title": "Believe What You See: Implicit Constraint Approach for Offline Multi-Agent Reinforcement Learning"
      },
      {
        "paperId": "0bcc734246586966596b1aa22efac2565224ebee",
        "title": "Bridging Offline Reinforcement Learning and Imitation Learning: A Tale of Pessimism"
      },
      {
        "paperId": "245682e8b3fa76f4a3e2991b5497577af95cbb3f",
        "title": "COMBO: Conservative Offline Model-Based Policy Optimization"
      },
      {
        "paperId": "9f8a70e9188a913317e97ba1874c8f763fd13c04",
        "title": "Is Pessimism Provably Efficient for Offline RL?"
      },
      {
        "paperId": "28db20a81eec74a50204686c3cf796c42a020d2e",
        "title": "Conservative Q-Learning for Offline Reinforcement Learning"
      },
      {
        "paperId": "5e7bc93622416f14e6948a500278bfbe58cd3890",
        "title": "Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems"
      },
      {
        "paperId": "a326d9f2d2d351001fece788165dbcbb524da2e4",
        "title": "D4RL: Datasets for Deep Data-Driven Reinforcement Learning"
      },
      {
        "paperId": "d0cf6bc0d44e2af3135d238a54c58dcd5c2d4e84",
        "title": "Provably Efficient Exploration in Policy Optimization"
      },
      {
        "paperId": "3231ac937b2620cd3ea7c39fdacaf416a558d31c",
        "title": "Information-Theoretic Confidence Bounds for Reinforcement Learning"
      },
      {
        "paperId": "d423fa6cd0f4d088941d9fe4bebd834d0137c9b9",
        "title": "Provably Efficient Reinforcement Learning with Linear Function Approximation"
      },
      {
        "paperId": "a4af0eb16016c5e1f36bafb7291a8a4932c0f17c",
        "title": "Reinforcement Learning in Feature Space: Matrix Bandit, Kernels, and Regret Bound"
      },
      {
        "paperId": "56136aa0b2c347cbcf3d50821f310c4253155026",
        "title": "Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models"
      },
      {
        "paperId": "4debb99c0c63bfaa97dd433bc2828e4dac81c48b",
        "title": "Addressing Function Approximation Error in Actor-Critic Methods"
      },
      {
        "paperId": "daea16e16370c9d141ce6bd1b42ee3e5287bf275",
        "title": "Learning Unknown Markov Decision Processes: A Thompson Sampling Approach"
      },
      {
        "paperId": "88909a57da9a43ceb52aae8424b1f348dba99cab",
        "title": "Why is Posterior Sampling Better than Optimism for Reinforcement Learning?"
      },
      {
        "paperId": "4b63e34276aa98d5345efa7fe09bb06d8a9d8f52",
        "title": "Deep Exploration via Bootstrapped DQN"
      },
      {
        "paperId": "846aedd869a00c09b40f1f1f35673cb22bc87490",
        "title": "Mastering the game of Go with deep neural networks and tree search"
      },
      {
        "paperId": "de6c988f7a6962a09a1c11f41ded0b63a5418559",
        "title": "An Information-Theoretic Analysis of Thompson Sampling"
      },
      {
        "paperId": "2319a491378867c7049b3da055c5df60e1671158",
        "title": "Playing Atari with Deep Reinforcement Learning"
      },
      {
        "paperId": "28cf1bd6110e734e20fc63f727d0d5bba612b921",
        "title": "Learning to Optimize via Posterior Sampling"
      },
      {
        "paperId": "43be33ef48e66d1f293c73af73f2f6753c6c392c",
        "title": "Agnostic System Identification for Model-Based Reinforcement Learning"
      },
      {
        "paperId": "c13dc0da5352fb844b55793903028da5a63ace46",
        "title": "TAIL BOUNDS FOR ALL EIGENVALUES OF A SUM OF RANDOM MATRICES"
      },
      {
        "paperId": "103f6fe35033f9327611ddafde74a2b544072980",
        "title": "Using Confidence Bounds for Exploitation-Exploration Trade-offs"
      }
    ],
    "cited_by": [
      {
        "paperId": "f1a79e891506b6f286122ecdc987a711db010dd7",
        "title": "Multi-Fidelity Hybrid Reinforcement Learning via Information Gain Maximization"
      },
      {
        "paperId": "c5c5a0d4cfe702e2c432789bb0034efdec7af678",
        "title": "Learning from Suboptimal Data in Continuous Control via Auto-Regressive Soft Q-Network"
      },
      {
        "paperId": "38646e980a9665b44bd14f0559d6ecfdfc4ce22c",
        "title": "Offline-to-Online Multi-Agent Reinforcement Learning with Offline Value Function Memory and Sequential Exploration"
      },
      {
        "paperId": "c7fe09b6c8f067e0ed97401fbac56242d77371ef",
        "title": "Leveraging Unlabeled Data Sharing through Kernel Function Approximation in Offline Reinforcement Learning"
      }
    ],
    "score": 4.0
  },
  {
    "id": "3416214ca1d4f790a048ece4229829333e836b4d",
    "title": "Sparse Graph Representation Learning Based on Reinforcement Learning for Personalized Mild Cognitive Impairment (MCI) Diagnosis",
    "authors": [
      "Chang-Hoon Ji",
      "Dong-Hee Shin",
      "Young-Han Son",
      "Tae-Eui Kam"
    ],
    "year": 2024,
    "citationCount": 4,
    "abstract": "Resting-state functional magnetic resonance imaging (rs-fMRI) has gained attention as a reliable technique for investigating the intrinsic function patterns of the brain. It facilitates the extraction of functional connectivity networks (FCNs) that capture synchronized activity patterns among regions of interest (ROIs). Analyzing FCNs enables the identification of distinctive connectivity patterns associated with mild cognitive impairment (MCI). For MCI diagnosis, various sparse representation techniques have been introduced, including statistical- and deep learning-based methods. However, these methods face limitations due to their reliance on supervised learning schemes, which restrict the exploration necessary for probing novel solutions. To overcome such limitation, prior work has incorporated reinforcement learning (RL) to dynamically select ROIs, but effective exploration remains challenging due to the vast search space during training. To tackle this issue, in this study, we propose an advanced RL-based framework that utilizes a divide-and-conquer approach to decompose the FCN construction task into smaller sub-problems in a subject-specific manner, enabling efficient exploration under each sub-problem condition. Additionally, we leverage the learned value function to determine the sparsity level of FCNs, considering individual characteristics of FCNs. We validate the effectiveness of our proposed framework by demonstrating its superior performance in MCI diagnosis on publicly available cohort datasets.",
    "url": "https://www.semanticscholar.org/paper/3416214ca1d4f790a048ece4229829333e836b4d",
    "pdf_url": "https://doi.org/10.1109/JBHI.2024.3393625",
    "venue": "IEEE journal of biomedical and health informatics",
    "publicationDate": "2024-04-29",
    "externalIds": {
      "DBLP": "journals/titb/JiSSK24",
      "DOI": "10.1109/JBHI.2024.3393625",
      "CorpusId": 269463077,
      "PubMed": "38683720"
    },
    "references": [],
    "cited_by": [
      {
        "paperId": "e0ad44bcd3cc34817dfd1acea80c58eee9357cec",
        "title": "Sharpness-aware minimization with physics-informed regularizations for predicting semiconductor material properties in molecular dynamics"
      },
      {
        "paperId": "a1519b3c4abb878eff19b84d102cdf69bffbb517",
        "title": "Designing Selective Drugs: Multi-Objective Optimization to Mitigate Off-Target Effects"
      },
      {
        "paperId": "fcf35f3dbd8f75d7640584923ffa55ec1c8be8f9",
        "title": "Population-based evolutionary search for joint hyperparameter and architecture optimization in brain-computer interface"
      },
      {
        "paperId": "1780fc3f4ff30125dd5ab45904702d4ae26c4d1f",
        "title": "Dynamic Graph Transformer for Brain Disorder Diagnosis"
      }
    ],
    "score": 4.0
  },
  {
    "id": "6bf550ce7f10a4347d448eb810a92e1a5cfffa4b",
    "title": "Beyond Optimism: Exploration With Partially Observable Rewards",
    "authors": [
      "Simone Parisi",
      "Alireza Kazemipour",
      "Michael Bowling"
    ],
    "year": 2024,
    "citationCount": 3,
    "abstract": "Exploration in reinforcement learning (RL) remains an open challenge. RL algorithms rely on observing rewards to train the agent, and if informative rewards are sparse the agent learns slowly or may not learn at all. To improve exploration and reward discovery, popular algorithms rely on optimism. But what if sometimes rewards are unobservable, e.g., situations of partial monitoring in bandits and the recent formalism of monitored Markov decision process? In this case, optimism can lead to suboptimal behavior that does not explore further to collapse uncertainty. With this paper, we present a novel exploration strategy that overcomes the limitations of existing methods and guarantees convergence to an optimal policy even when rewards are not always observable. We further propose a collection of tabular environments for benchmarking exploration in RL (with and without unobservable rewards) and show that our method outperforms existing ones.",
    "url": "https://www.semanticscholar.org/paper/6bf550ce7f10a4347d448eb810a92e1a5cfffa4b",
    "pdf_url": "https://arxiv.org/pdf/2406.13909.pdf",
    "venue": "Neural Information Processing Systems",
    "publicationDate": "2024-06-20",
    "externalIds": {
      "DBLP": "journals/corr/abs-2406-13909",
      "ArXiv": "2406.13909",
      "DOI": "10.48550/arXiv.2406.13909",
      "CorpusId": 270619736
    },
    "references": [
      {
        "paperId": "839b6e963039b1e3382e861f6bd9b95417707562",
        "title": "Monitored Markov Decision Processes"
      },
      {
        "paperId": "e66d55565f6cd6336f92be01b4efc7e6a1eb2381",
        "title": "Optimistic Active Exploration of Dynamical Systems"
      },
      {
        "paperId": "ec24cba023e59a11253c60a5516df0562c62d62c",
        "title": "Flipping Coins to Estimate Pseudocounts for Exploration in Reinforcement Learning"
      },
      {
        "paperId": "5045c2a64a4ea41d8da9ee8eb279498db1ff3d78",
        "title": "Planning Goals for Exploration"
      },
      {
        "paperId": "296f99cbf0425b770cd3f32119e2a9355a937b19",
        "title": "Linear Partial Monitoring for Sequential Decision-Making: Algorithms, Regret Bounds and Applications"
      },
      {
        "paperId": "e6e6dc98b1747ac37665930c27d59691515810aa",
        "title": "Deep Laplacian-based Options for Temporally-Extended Exploration"
      },
      {
        "paperId": "c64a99ec780d5e1c6ba242a82e72e96eb0c3ef8a",
        "title": "An Information-Theoretic Perspective on Intrinsic Motivation in Reinforcement Learning: A Survey"
      },
      {
        "paperId": "466d4c34833efafec1944c8ffdf0a8748943af07",
        "title": "On the Statistical Efficiency of Reward-Free Exploration in Non-Linear RL"
      },
      {
        "paperId": "da1e404ae194dcbcce198f5335f49f090403bbd7",
        "title": "Exploration in Deep Reinforcement Learning: A Survey"
      },
      {
        "paperId": "39d2e380967c91b447d47377840fa541e276b479",
        "title": "Interesting Object, Curious Agent: Learning Task-Agnostic Exploration"
      },
      {
        "paperId": "f8d4cb5d710fd6fc93bb55a8e1ccf3a1af05443d",
        "title": "Successor Feature Representations"
      },
      {
        "paperId": "ccb01a90b16b119db0201ed012f989ed48c68d9f",
        "title": "Temporal Abstraction in Reinforcement Learning with the Successor Representation"
      },
      {
        "paperId": "0707738c08009bc84e0836dcccb608a639a70f87",
        "title": "Information Directed Reward Learning for Reinforcement Learning"
      },
      {
        "paperId": "31eaf8fe40e07458911ebf4340e0c83f3e7d4c9e",
        "title": "Tactical Optimism and Pessimism for Deep Reinforcement Learning"
      },
      {
        "paperId": "a47f52b25ce1e56a03876d9c0fd7c45e63270eb4",
        "title": "Active Reinforcement Learning: Observing Rewards at a Cost"
      },
      {
        "paperId": "712e3a8b0291413ee44f27058853cfd1e5dad7b6",
        "title": "Active Learning for Nonlinear System Identification with Guarantees"
      },
      {
        "paperId": "bebe8ffb0c357ac0c7eea2556f817b03ee22b570",
        "title": "RIDE: Rewarding Impact-Driven Exploration for Procedurally-Generated Environments"
      },
      {
        "paperId": "9cdc3e333e583df3c6059ba41efcffa7fcf2a28e",
        "title": "Information Directed Sampling for Linear Partial Monitoring"
      },
      {
        "paperId": "90f9ef793c1e4000ee523e1f8cea501fb8806fa0",
        "title": "Reward-Free Exploration for Reinforcement Learning"
      },
      {
        "paperId": "140788dc3c2e3d716fc6d8b3ec9ab04bccf7001d",
        "title": "Long-Term Visitation Value for Deep Exploration in Sparse Reward Reinforcement Learning"
      },
      {
        "paperId": "c39fb7a46335c23f7529dd6f9f980462fd38653a",
        "title": "Mastering Atari, Go, chess and shogi by planning with a learned model"
      },
      {
        "paperId": "648d2cd200584f790bd6d07ba875eabc21710d6d",
        "title": "Explicit Explore-Exploit Algorithms in Continuous State Spaces"
      },
      {
        "paperId": "222baa4e9e7ce691fdfddbc826a70e027daed70d",
        "title": "Reinforcement Learning in Healthcare: A Survey"
      },
      {
        "paperId": "7b22ed1e17afecb1d45e8862a71a6114da989ba2",
        "title": "Exploiting Action-Value Uncertainty to Drive Exploration in Reinforcement Learning"
      },
      {
        "paperId": "55203cd25f9c03d7c6b691ba84e95bb82df0bc6f",
        "title": "Fast Task Inference with Variational Intrinsic Successor Features"
      },
      {
        "paperId": "129762bfc35c85362fc65dad10e2e3c6dc30deba",
        "title": "An Information-Theoretic Approach to Minimax Regret in Partial Monitoring"
      },
      {
        "paperId": "b35484a629152d8349046e81ac4671fc98d4d0b3",
        "title": "Successor Features Combine Elements of Model-Free and Model-based Reinforcement Learning"
      },
      {
        "paperId": "225cc727daeca281f4f932a70765e0a32d849d6b",
        "title": "Q-learning with UCB Exploration is Sample Efficient for Infinite-Horizon MDP"
      },
      {
        "paperId": "4cb3fd057949624aa4f0bbe7a6dcc8777ff04758",
        "title": "Exploration by Random Network Distillation"
      },
      {
        "paperId": "9f67b3edc67a35c884bd532a5e73fa3a7f3660d8",
        "title": "Count-Based Exploration with the Successor Representation"
      },
      {
        "paperId": "03cc81e98942bdafd994af7a1d1e62a68ff8b682",
        "title": "Is Q-learning Provably Efficient?"
      },
      {
        "paperId": "2386895779a1450613e6801ee63ff7f79d66cc6b",
        "title": "Near Optimal Exploration-Exploitation in Non-Communicating Markov Decision Processes"
      },
      {
        "paperId": "073c33d5023653ea34fdd2713dbbd8f512b6548a",
        "title": "Active Reinforcement Learning with Monte-Carlo Tree Search"
      },
      {
        "paperId": "0f810eb4777fd05317951ebaa7a3f5835ee84cf4",
        "title": "Count-Based Exploration in Feature Space for Reinforcement Learning"
      },
      {
        "paperId": "225ab689f41cef1dc18237ef5dab059a49950abf",
        "title": "Curiosity-Driven Exploration by Self-Supervised Prediction"
      },
      {
        "paperId": "a441728f9fd6af1946368240162a72c2028c8cb1",
        "title": "Deep Exploration via Randomized Value Functions"
      },
      {
        "paperId": "d6757aedcb53142bc439ec64bfd0b056d99b1881",
        "title": "Surprise-Based Intrinsic Motivation for Deep Reinforcement Learning"
      },
      {
        "paperId": "0fcb2034e31a2bc2f12a2b1363d0d77baf445fdf",
        "title": "#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning"
      },
      {
        "paperId": "72e529ee310ab7fe52cd193217397e2430e5d9e6",
        "title": "The End of Optimism? An Asymptotic Analysis of Finite-Armed Linear Bandits"
      },
      {
        "paperId": "88909a57da9a43ceb52aae8424b1f348dba99cab",
        "title": "Why is Posterior Sampling Better than Optimism for Reinforcement Learning?"
      },
      {
        "paperId": "d8686b657b61a37da351af2952aabd8b281de408",
        "title": "Successor Features for Transfer in Reinforcement Learning"
      },
      {
        "paperId": "6e90fd78e8a3b98af3954aae5209703aa966603e",
        "title": "Unifying Count-Based Exploration and Intrinsic Motivation"
      },
      {
        "paperId": "317cd4522b1f4a6f889743578143bb8823623f8b",
        "title": "VIME: Variational Information Maximizing Exploration"
      },
      {
        "paperId": "c6170fa90d3b2efede5a2e1660cb23e1c824f2ca",
        "title": "Prioritized Experience Replay"
      },
      {
        "paperId": "024006d4c2a89f7acacc6e4438d156525b60a98f",
        "title": "Continuous control with deep reinforcement learning"
      },
      {
        "paperId": "5dc2a215bd7cd5bdd3a0baa8c967575632696fac",
        "title": "Universal Value Function Approximators"
      },
      {
        "paperId": "2470fcf0f89082de874ac9133ccb3a8667dd89a8",
        "title": "Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models"
      },
      {
        "paperId": "340f48901f72278f6bf78a04ee5b01df208cc508",
        "title": "Human-level control through deep reinforcement learning"
      },
      {
        "paperId": "2e21b210c3cd3ac621b4fac372a48aa8364c7b9a",
        "title": "Partial Monitoring - Classification, Regret Bounds, and Algorithms"
      },
      {
        "paperId": "f770614e497f456cfbe310bf7fd5223a4c28edd7",
        "title": "Learning to Optimize via Information-Directed Sampling"
      },
      {
        "paperId": "2319a491378867c7049b3da055c5df60e1671158",
        "title": "Playing Atari with Deep Reinforcement Learning"
      },
      {
        "paperId": "65438e0ba226c1f97bd8a36333ebc3297b1a32fd",
        "title": "Reinforcement learning in robotics: A survey"
      },
      {
        "paperId": "789783016fb708abbc061790612ebe91273c05d3",
        "title": "(More) Efficient Reinforcement Learning via Posterior Sampling"
      },
      {
        "paperId": "50e9a441f56124b7b969e6537b66469a0e1aa707",
        "title": "Horde: a scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction"
      },
      {
        "paperId": "33224ad0cdf6e2dc4893194dd587309c7887f0ba",
        "title": "Formal Theory of Creativity, Fun, and Intrinsic Motivation (1990\u20132010)"
      },
      {
        "paperId": "4282669947ca340de9f93a9a2a38007fe542195d",
        "title": "Near-Bayesian exploration in polynomial time"
      },
      {
        "paperId": "0cafe2903b097fc042782c359cb231ea34ef7ed3",
        "title": "Near-optimal Regret Bounds for Reinforcement Learning"
      },
      {
        "paperId": "237a1cf18ed83bb3ad852b34f443c6c1ff3336c1",
        "title": "An analysis of model-based Interval Estimation for Markov Decision Processes"
      },
      {
        "paperId": "1ecc0111559116772662532af635fe628c69ca3b",
        "title": "The many faces of optimism: a unifying approach"
      },
      {
        "paperId": "94e9d4c63268abdebcff529c452a30b272221b5c",
        "title": "Logarithmic Online Regret Bounds for Undiscounted Reinforcement Learning"
      },
      {
        "paperId": "40e4d0eaff2ce7538ecff773bda326388a87b515",
        "title": "An analytic solution to discrete Bayesian reinforcement learning"
      },
      {
        "paperId": "841c5f535174bedab6b8b4c608977d946f65a67b",
        "title": "Bayesian sparse sampling for on-line reward optimization"
      },
      {
        "paperId": "dc649486b881e672eea6546da48c46e1f98daf32",
        "title": "Near-Optimal Reinforcement Learning in Polynomial Time"
      },
      {
        "paperId": "1e1d35136b1bf3b13ef6b53f6039f39d9ee820e3",
        "title": "Finite-time Analysis of the Multiarmed Bandit Problem"
      },
      {
        "paperId": "4bf4495b9b643b3714a45df08f6942811d28d405",
        "title": "Convergence of Optimistic and Incremental Q-Learning"
      },
      {
        "paperId": "48cce5ee49facf75eeb12832c387452424b645dd",
        "title": "A Bayesian Framework for Reinforcement Learning"
      },
      {
        "paperId": "712ec1bd9287ac210a7630ce03ca2b0930ebd351",
        "title": "Convergence Results for Single-Step On-Policy Reinforcement-Learning Algorithms"
      },
      {
        "paperId": "a9cd8efe9184dddb1bedbbec3a356c4dfb22fe63",
        "title": "Markov Decision Processes: Discrete Stochastic Dynamic Programming"
      },
      {
        "paperId": "c2e8806f0bd1d504bcb395ef1f6fe509a023a048",
        "title": "Improving Generalization for Temporal Difference Learning: The Successor Representation"
      },
      {
        "paperId": "28be6c2ed074a7fa63818a1730b04219d8a01c02",
        "title": "The convergence of TD(\u03bb) for general \u03bb"
      },
      {
        "paperId": "03b7e51c52084ac1db5118342a00b5fbcfc587aa",
        "title": "Q-learning"
      },
      {
        "paperId": "22cd8ee12f34e75781dd34be2f9cd9f5a1ffe2fc",
        "title": "The weighted majority algorithm"
      },
      {
        "paperId": "ee2cd1d17f833d3c157a1016a778c7c22af555a2",
        "title": "ON THE LIKELIHOOD THAT ONE UNKNOWN PROBABILITY EXCEEDS ANOTHER IN VIEW OF THE EVIDENCE OF TWO SAMPLES"
      },
      {
        "paperId": "6222c8b38521e38ebbe6ea4868d4aaafa619a335",
        "title": "Exploration Bonus for Regret Minimization in Discrete and Continuous Average Reward MDPs"
      },
      {
        "paperId": "fae8bbf868681b83d91b2fec6c840d4d2b32005b",
        "title": "Intrinsic Motivation and Reinforcement Learning"
      },
      {
        "paperId": null,
        "title": "Gaussian process optimization in the bandit setting: No regret and experimental design"
      },
      {
        "paperId": "26e17f6b62a7caec660b3356d49e879e6e0eeabc",
        "title": "The Nonstochastic Multiarmed Bandit Problem"
      },
      {
        "paperId": "c7dbf5ed7e9b63e104adb4e18bfbc98f5d6afdae",
        "title": "R-MAX - A General Polynomial Time Algorithm for Near-Optimal Reinforcement Learning"
      },
      {
        "paperId": "b55987b4cfff292dd121ee03c46b41f4f696136e",
        "title": "Intrinsic and Extrinsic Motivations: Classic Definitions and New Directions."
      },
      {
        "paperId": "97efafdb4a3942ab3efba53ded7413199f79c054",
        "title": "Reinforcement Learning: An Introduction"
      },
      {
        "paperId": "75e154b5dd3be0fa65e97b3d19d80d85b056579f",
        "title": "Noname manuscript No. (will be inserted by the editor) TEXPLORE: Real-Time Sample-Efficient Reinforcement Learning for Robots"
      },
      {
        "paperId": "572379c1d56e7e19422ae38218ee228c61aefb2f",
        "title": "Asymptotically Efficient Adaptive Allocation Rules"
      },
      {
        "paperId": null,
        "title": "If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully"
      }
    ],
    "cited_by": [
      {
        "paperId": "5b12af22a9fc9ff6549612c8ab58585c70e76edc",
        "title": "Which Rewards Matter? Reward Selection for Reinforcement Learning under Limited Feedback"
      },
      {
        "paperId": "c3e679068ed53b1a6efc3281448f78cb0628743a",
        "title": "Generalization in Monitored Markov Decision Processes (Mon-MDPs)"
      },
      {
        "paperId": "1f327387a82b973518408d5f0c09843688661f2e",
        "title": "Model-Based Exploration in Monitored Markov Decision Processes"
      }
    ],
    "score": 3.0
  },
  {
    "id": "54cb726595cd8c03f59f49c9a97b76b4d3f932f2",
    "title": "Rethinking Exploration in Reinforcement Learning with Effective Metric-Based Exploration Bonus",
    "authors": [
      "Yiming Wang",
      "Kaiyan Zhao",
      "Furui Liu",
      "Leong Hou"
    ],
    "year": 2024,
    "citationCount": 3,
    "abstract": "",
    "url": "https://www.semanticscholar.org/paper/54cb726595cd8c03f59f49c9a97b76b4d3f932f2",
    "pdf_url": null,
    "venue": "Neural Information Processing Systems",
    "publicationDate": "",
    "externalIds": {
      "DBLP": "conf/nips/WangZLU24",
      "CorpusId": 276236277
    },
    "references": [],
    "cited_by": [
      {
        "paperId": "6f9e52261322524747841b2447a703317535f612",
        "title": "Beyond Noisy-TVs: Noise-Robust Exploration Via Learning Progress Monitoring"
      },
      {
        "paperId": "905cb1672c1d685c171acd8a1ed6db59e671409a",
        "title": "BILE: An Effective Behavior-based Latent Exploration Scheme for Deep Reinforcement Learning"
      },
      {
        "paperId": "b7af43849da34b4eb84df6b6167f6cc6adb8f4ed",
        "title": "Efficient Diversity-based Experience Replay for Deep Reinforcement Learning"
      }
    ],
    "score": 3.0
  },
  {
    "id": "b44b0e8222021b51783c62b0bce071ef6fb3f12c",
    "title": "OCEAN-MBRL: Offline Conservative Exploration for Model-Based Offline Reinforcement Learning",
    "authors": [
      "Fan Wu",
      "Rui Zhang",
      "Qi Yi",
      "Yunkai Gao",
      "Jiaming Guo",
      "Shaohui Peng",
      "Siming Lan",
      "Husheng Han",
      "Yansong Pan",
      "Kaizhao Yuan",
      "Pengwei Jin",
      "Rui Chen",
      "Yunji Chen",
      "Ling Li"
    ],
    "year": 2024,
    "citationCount": 3,
    "abstract": "Model-based offline reinforcement learning (RL) algorithms have emerged as a promising paradigm for offline RL.\nThese algorithms usually learn a dynamics model from a static dataset of transitions, use the model to generate synthetic trajectories, and perform conservative policy optimization within these trajectories. \nHowever, our observations indicate that policy optimization methods used in these model-based offline RL algorithms are not effective at exploring the learned model and induce biased exploration, which ultimately impairs the performance of the algorithm.\nTo address this issue, we propose Offline Conservative ExplorAtioN (OCEAN), a novel rollout approach to model-based offline RL.\nIn our method, we incorporate additional exploration techniques and introduce three conservative constraints based on uncertainty estimation to mitigate the potential impact of significant dynamic errors resulting from exploratory transitions. \nOur work is a plug-in method and can be combined with classical model-based RL algorithms, such as MOPO, COMBO, and RAMBO.\nExperiment results of our method on the D4RL MuJoCo benchmark show that OCEAN significantly improves the performance of existing algorithms.",
    "url": "https://www.semanticscholar.org/paper/b44b0e8222021b51783c62b0bce071ef6fb3f12c",
    "pdf_url": "https://doi.org/10.1609/aaai.v38i14.29520",
    "venue": "AAAI Conference on Artificial Intelligence",
    "publicationDate": "2024-03-24",
    "externalIds": {
      "DBLP": "conf/aaai/WuZYGGPLHPYJCCL24",
      "DOI": "10.1609/aaai.v38i14.29520",
      "CorpusId": 268696816
    },
    "references": [
      {
        "paperId": "348a855fe01f3f4273bf0ecf851ca688686dbfcc",
        "title": "Offline Reinforcement Learning with Implicit Q-Learning"
      },
      {
        "paperId": "877e8140cb7f26042f6c5f1eefcf68a2748721f0",
        "title": "Revisiting Design Choices in Offline Model Based Reinforcement Learning"
      },
      {
        "paperId": "9e8df3b1fad0e219bd96d254784dee40736057ff",
        "title": "Offline Reinforcement Learning with Reverse Model-based Imagination"
      },
      {
        "paperId": "a3b82f4fc10caf6243afbd77c9c990ce03ae36d1",
        "title": "Offline RL Without Off-Policy Evaluation"
      },
      {
        "paperId": "f71da178cd63958fe659ad613d474b67c5615bd3",
        "title": "Behavioral Priors and Dynamics Models: Improving Performance and Domain Transfer in Offline RL"
      },
      {
        "paperId": "c879b25308026d6538e52b27bcf4fd3cb60855f3",
        "title": "A Minimalist Approach to Offline Reinforcement Learning"
      },
      {
        "paperId": "f864d4d2267abba15eb43db54f58286aef78292b",
        "title": "Offline Reinforcement Learning as One Big Sequence Modeling Problem"
      },
      {
        "paperId": "c1ad5f9b32d80f1c65d67894e5b8c2fdf0ae4500",
        "title": "Decision Transformer: Reinforcement Learning via Sequence Modeling"
      },
      {
        "paperId": "2c23085488337c4c1b5673b8d0f4ac95bda73529",
        "title": "Uncertainty Weighted Actor-Critic for Offline Reinforcement Learning"
      },
      {
        "paperId": "245682e8b3fa76f4a3e2991b5497577af95cbb3f",
        "title": "COMBO: Conservative Offline Model-Based Policy Optimization"
      },
      {
        "paperId": "124b683a6f0f8ac8e13e204dfcd30a5497ab581d",
        "title": "Overcoming Model Bias for Robust Offline Deep Reinforcement Learning"
      },
      {
        "paperId": "28db20a81eec74a50204686c3cf796c42a020d2e",
        "title": "Conservative Q-Learning for Offline Reinforcement Learning"
      },
      {
        "paperId": "79ebde314ab90d066cee3b82193ef05666323394",
        "title": "Deployment-Efficient Reinforcement Learning via Model-Based Offline Optimization"
      },
      {
        "paperId": "dea0f1c5949f8d898b9b6ff68226a781558e413c",
        "title": "MOPO: Model-based Offline Policy Optimization"
      },
      {
        "paperId": "309c2c5ee60e725244da09180f913cd8d4b8d4e9",
        "title": "MOReL : Model-Based Offline Reinforcement Learning"
      },
      {
        "paperId": "5e7bc93622416f14e6948a500278bfbe58cd3890",
        "title": "Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems"
      },
      {
        "paperId": "a326d9f2d2d351001fece788165dbcbb524da2e4",
        "title": "D4RL: Datasets for Deep Data-Driven Reinforcement Learning"
      },
      {
        "paperId": "129983331ca874142a3e8eb2d93d820bdf1f9aca",
        "title": "Deep Reinforcement Learning for Autonomous Driving: A Survey"
      },
      {
        "paperId": "7ece9a25ce276c563470a92c90c53edfa6302217",
        "title": "Integrating Deep Reinforcement Learning with Model-based Path Planners for Automated Driving"
      },
      {
        "paperId": "27d78a26ddb9698b9cefcf6cdeafa4f834466103",
        "title": "AlgaeDICE: Policy Gradient from Arbitrary Experience"
      },
      {
        "paperId": "9be492858863c8c7c24be1ecb75724de5086bd8e",
        "title": "Behavior Regularized Offline Reinforcement Learning"
      },
      {
        "paperId": "9001698e033524864d4d45f051a5ba362d4afd9e",
        "title": "When to Trust Your Model: Model-Based Policy Optimization"
      },
      {
        "paperId": "82b4b03a4659d6e04bd7cbf51d6e08fde1348dbd",
        "title": "Stabilizing Off-Policy Q-Learning via Bootstrapping Error Reduction"
      },
      {
        "paperId": "12c0751b4f51ed833172a713b7e32390032ead93",
        "title": "Soft Actor-Critic Algorithms and Applications"
      },
      {
        "paperId": "5285cb8faada5de8a92a47622950f6cfd476ac1d",
        "title": "Off-Policy Deep Reinforcement Learning without Exploration"
      },
      {
        "paperId": "55e5368377e07038c062e088e074e59453cd43d9",
        "title": "Deep Reinforcement Learning for Vision-Based Robotic Grasping: A Simulated Comparative Evaluation of Off-Policy Methods"
      },
      {
        "paperId": "802168a81571dde28f5ddb94d84677bc007afa7b",
        "title": "Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles"
      },
      {
        "paperId": "2af9eaae9191ee522cb97dc57615a071e5403d26",
        "title": "Learning and Policy Search in Stochastic Dynamical Systems with Bayesian Neural Networks"
      },
      {
        "paperId": "60b7d47758a71978e74edff6dd8dea4d9c791d7a",
        "title": "PILCO: A Model-Based and Data-Efficient Approach to Policy Search"
      },
      {
        "paperId": "1e650cb12aaad371a49b0c8c4514e1b988a5178c",
        "title": "Pareto Policy Pool for Model-based Offline Reinforcement Learning"
      },
      {
        "paperId": "97efafdb4a3942ab3efba53ded7413199f79c054",
        "title": "Reinforcement Learning: An Introduction"
      }
    ],
    "cited_by": [
      {
        "paperId": "a552bef347f7992870a96682ba7534a60468c018",
        "title": "Hybrid Cross-domain Robust Reinforcement Learning"
      },
      {
        "paperId": "9da58a29f476f560dda373976e820a369fba9568",
        "title": "Temporal Distance-aware Transition Augmentation for Offline Model-based Reinforcement Learning"
      },
      {
        "paperId": "aa37bbbdd95343c8f7bd61d6e13164d767eaa330",
        "title": "Ensemble RL through Classifier Models: Enhancing Risk-Return Trade-offs in Trading Strategies"
      }
    ],
    "score": 3.0
  },
  {
    "id": "5ac5f3827ee4c32cebadbc9de8a892853575efb9",
    "title": "Active Exploration Deep Reinforcement Learning for Continuous Action Space with Forward Prediction",
    "authors": [
      "Dongfang Zhao",
      "Huanshi Xu",
      "Zhang Xun"
    ],
    "year": 2024,
    "citationCount": 3,
    "abstract": "The application of reinforcement learning (RL) to the field of autonomous robotics has high requirements about sample efficiency, since the agent expends for interaction with the environment. One method for sample efficiency is to extract knowledge from existing samples and used to exploration. Typical RL algorithms achieve exploration using task-specific knowledge or adding exploration noise. These methods are limited to current policy improvement level and lack of long-term planning. We propose a novel active exploration deep RL algorithm for the continuous action space problem named active exploration deep reinforcement learning (AEDRL). Our method uses the Gaussian process to model dynamic model, enabling the probability description of prediction sample. Action selection is formulated as the solution of the optimization problem. Thus, the optimization objective is specifically designed for selecting samples that can minimize the uncertainty of the dynamic model. Active exploration is achieved through long-term optimized action selection. This long-term considered action exploration method is more guidance for learning. Enable intelligent agents to explore more interesting action spaces. The proposed AEDRL algorithm is evaluated on several robotic control task including classic pendulum problem and five complex articulated robots. The AEDRL can learn a controller using fewer episodes and demonstrates performance and sample efficiency.",
    "url": "https://www.semanticscholar.org/paper/5ac5f3827ee4c32cebadbc9de8a892853575efb9",
    "pdf_url": "https://doi.org/10.1007/s44196-023-00389-1",
    "venue": "International Journal of Computational Intelligence Systems",
    "publicationDate": "2024-01-08",
    "externalIds": {
      "DBLP": "journals/ijcisys/ZhaoHX24",
      "DOI": "10.1007/s44196-023-00389-1",
      "CorpusId": 266884966
    },
    "references": [
      {
        "paperId": "3a1501829ce7205f25939dd26e1089df920c9988",
        "title": "Reward is enough"
      },
      {
        "paperId": "d6e783bce3b8e3ad082c2757235c34cb86c4e653",
        "title": "Safe Learning in Robotics: From Learning-Based Control to Safe Reinforcement Learning"
      },
      {
        "paperId": "3b9732bb07dc99bde5e1f9f75251c6ea5039373e",
        "title": "Deep Reinforcement Learning with Double Q-Learning"
      },
      {
        "paperId": "5fc69f93422b11c944b2d53e9d2f93295eca3d19",
        "title": "Autonomous Helicopter Aerobatics through Apprenticeship Learning"
      },
      {
        "paperId": "6c96ca73b381c4d8191ad73ea2fd9272ff0799c4",
        "title": "Apprenticeship learning for helicopter control"
      },
      {
        "paperId": "04afcad47365fb45403b7e21508b01b488978030",
        "title": "Entropy expressions and their estimators for multivariate distributions"
      }
    ],
    "cited_by": [
      {
        "paperId": "fccab8461eafc257b3b08710ab5d03c35388f72b",
        "title": "Evolution, Future of AI, and Singularity"
      },
      {
        "paperId": "31141c0ea89e91a78f3c2dd63cd08d239e8e76f9",
        "title": "On the Parallels Between Evolutionary Theory and the State of AI"
      },
      {
        "paperId": "9b9615790ae08a6f2992ba9fde2f0f37378b7ff1",
        "title": "A survey on autonomous environmental monitoring approaches: towards unifying active sensing and reinforcement learning"
      }
    ],
    "score": 3.0
  },
  {
    "id": "cce1245ba1ec154120b3b256faf7bf28f769b505",
    "title": "A Novel Guided Deep Reinforcement Learning Tracking Control Strategy for Multirotors",
    "authors": [
      "Hean Hua",
      "Yaonan Wang",
      "Hang Zhong",
      "Hui Zhang",
      "Yongchun Fang"
    ],
    "year": 2025,
    "citationCount": 3,
    "abstract": "This paper presents an intelligent control scheme for multirotors, where accurate trajectory tracking, strong robustness and reliable generalization are guaranteed by the dual-feedback sliding-mode (DFSM) guided deep reinforcement learning (RL). Different from current solutions, the proposed method explores optimal learning strategy on the sliding surface according to the DFSM demonstrations, where the elegantly designed parallel evaluation takes full advantage of model knowledge and learning exploration. Specifically, the intelligent tracking control is achieved in a two-step design. First, the DFSM algorithm is designed for multirotors, where the linear and nonlinear feedback terms work cooperatively. Second, the DFSM-guided deep RL is put forward to achieve intelligent switching on the sliding surface, where position and velocity errors are both considered to generate accurate switching decisions. In the framework, explorations and the DFSM demonstrations are evaluated in parallel, where only the explorations that are better than the DFSM baseline, are kept for policy improvement. In this way, the DFSM algorithm keeps pushing the RL policy to explore better strategy, where the unavoidable bad experiences arisen from exploration are identified accurately. Practical comparative experimental results are included to verify the effectiveness of the proposed strategy. Note to Practitioners\u2014This paper is motivated by the practical problem of controlling multirotor system in uncertain environments. Up until now, most existing approaches are proposed without taking full advantage of model knowledge and deep learning techniques simultaneously, which lacks of reliability in practical application. To deal with the problem, a new dual-feedback sliding-mode (DFSM) guided deep reinforcement learning (RL) strategy is proposed, where the dual feedback and guided RL are designed to achieve satisfactory tracking control and simultaneously handle uncertainties. Specifically, by introducing double-check framework, the RL strategy explores optimal switching policy on the sliding surface according to the DFSM demonstrations, guaranteeing strong robustness and reliable generalization of the obtained RL policy in uncertain environments. The key feature of the framework is that the DFSM-driven training guarantees practice-oriented tracking control in a DFSM-RL cooperative manner. Comparative experiments are implemented to verify the tracking performance of the proposed intelligent control strategy.",
    "url": "https://www.semanticscholar.org/paper/cce1245ba1ec154120b3b256faf7bf28f769b505",
    "pdf_url": "https://doi.org/10.1109/TASE.2024.3374752",
    "venue": "IEEE Transactions on Automation Science and Engineering",
    "publicationDate": "",
    "externalIds": {
      "DBLP": "journals/tase/HuaWZZF25",
      "DOI": "10.1109/TASE.2024.3374752",
      "CorpusId": 268527061
    },
    "references": [],
    "cited_by": [
      {
        "paperId": "90e0fb4308cc8525ae880708f5236517b1cf67e6",
        "title": "Model-Based Reinforcement Learning for Containing Malware Propagation in Wireless Radar Sensor Networks"
      },
      {
        "paperId": "6970a51e04d1cd87344048cdd620009b2a14b3b6",
        "title": "Integrated Guidance and Control of Morphing Flight Vehicle via Sliding-Mode-Based Robust Reinforcement Learning"
      },
      {
        "paperId": "69f2762b24dcc98f449552639da7aa6866223e4b",
        "title": "High-Speed Trajectory Tracking Control for Quadrotors via Deep Reinforcement Learning"
      }
    ],
    "score": 3.0
  },
  {
    "id": "37fe2a997bf07a972473abd079d175335940e6bd",
    "title": "CDE: Curiosity-Driven Exploration for Efficient Reinforcement Learning in Large Language Models",
    "authors": [
      "Runpeng Dai",
      "Linfeng Song",
      "Haolin Liu",
      "Zhenwen Liang",
      "Dian Yu",
      "Haitao Mi",
      "Zhaopeng Tu",
      "Rui Liu",
      "Tong Zheng",
      "Hongtu Zhu",
      "Dong Yu"
    ],
    "year": 2025,
    "citationCount": 3,
    "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) is a powerful paradigm for enhancing the reasoning ability of Large Language Models (LLMs). Yet current RLVR methods often explore poorly, leading to premature convergence and entropy collapse. To address this challenge, we introduce Curiosity-Driven Exploration (CDE), a framework that leverages the model's own intrinsic sense of curiosity to guide exploration. We formalize curiosity with signals from both the actor and the critic: for the actor, we use perplexity over its generated response, and for the critic, we use the variance of value estimates from a multi-head architecture. Both signals serve as an exploration bonus within the RLVR framework to guide the model. Our theoretical analysis shows that the actor-wise bonus inherently penalizes overconfident errors and promotes diversity among correct responses; moreover, we connect the critic-wise bonus to the well-established count-based exploration bonus in RL. Empirically, our method achieves an approximate +3 point improvement over standard RLVR using GRPO/PPO on AIME benchmarks. Further analysis identifies a calibration collapse mechanism within RLVR, shedding light on common LLM failure modes.",
    "url": "https://www.semanticscholar.org/paper/37fe2a997bf07a972473abd079d175335940e6bd",
    "pdf_url": "https://arxiv.org/pdf/2509.09675.pdf",
    "venue": "",
    "publicationDate": "2025-09-11",
    "externalIds": {
      "ArXiv": "2509.09675",
      "CorpusId": 281252441
    },
    "references": [
      {
        "paperId": "3a5548b9e9bb7b83e2179ca261f8f8aa0d5f1966",
        "title": "Parallel-R1: Towards Parallel Thinking via Reinforcement Learning"
      },
      {
        "paperId": "47ffcbf9a852ab0e36c4fe342e07e486d87c5276",
        "title": "On Entropy Control in LLM-RL Algorithms"
      },
      {
        "paperId": "71bae1ffe7691f9164d1d38ef0090782051f4d71",
        "title": "Self-Rewarding Vision-Language Model via Reasoning Decomposition"
      },
      {
        "paperId": "09d55928aeff4f9967809a333313ede2870a4b93",
        "title": "Deep Think with Confidence"
      },
      {
        "paperId": "1385ac414416374b63acfd82ccd6d91ed62fe101",
        "title": "R-Zero: Self-Evolving Reasoning LLM from Zero Data"
      },
      {
        "paperId": "6478c185582f5e48c54677c683a9297ff2916118",
        "title": "One Token to Fool LLM-as-a-Judge"
      },
      {
        "paperId": "91b693098aa12c0fb33e456d9e7fd91005814c74",
        "title": "R1-RE: Cross-Domain Relation Extraction with RLVR"
      },
      {
        "paperId": "9efe66da43abf4779c4c63f27c2ac544dec3159a",
        "title": "Dissecting Logical Reasoning in LLMs: A Fine-Grained Evaluation and Supervision Study"
      },
      {
        "paperId": "532c7d150d7bfece96e3c51ad0c8e4e18271912c",
        "title": "The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models"
      },
      {
        "paperId": "e5bf3534ecc3a496660a974787a102ed0e1958ec",
        "title": "Navigate the Unknown: Enhancing LLM Reasoning with Intrinsic Motivation Guided Exploration"
      },
      {
        "paperId": "c00a3b8e7bca375bb79a67017115694cdccd51dc",
        "title": "Learning to Reason via Mixture-of-Thought for Logical Reasoning"
      },
      {
        "paperId": "d2d84d56f730f81d276a02b48d5d44db5bde0b4a",
        "title": "Qwen3 Technical Report"
      },
      {
        "paperId": "6f0f0d9f29586344ae6403fe906c24e4f16eaed8",
        "title": "VL-Rethinker: Incentivizing Self-Reflection of Vision-Language Models with Reinforcement Learning"
      },
      {
        "paperId": "d85788857fd230169e17638631b96335368043ed",
        "title": "VAPO: Efficient and Reliable Reinforcement Learning for Advanced Reasoning Tasks"
      },
      {
        "paperId": "cfaec38388e89fc5ffe911f449e7f15771680143",
        "title": "Breach in the Shield: Unveiling the Vulnerabilities of Large Language Models"
      },
      {
        "paperId": "dd4cfde3e135f799a9a71b4f57e13a29de89f7e3",
        "title": "DAPO: An Open-Source LLM Reinforcement Learning System at Scale"
      },
      {
        "paperId": "1fcaafeb72142fe3d1a5d698a072d69778d244b0",
        "title": "Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning"
      },
      {
        "paperId": "34471a2fa18ea22efad5287cf4aeb18542c98a9b",
        "title": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      },
      {
        "paperId": "c927d4c668339147fdee3353f110b6d254babe0f",
        "title": "Online Preference Alignment for Language Models via Count-based Exploration"
      },
      {
        "paperId": "6a7c29829227bfd65ae0ffec294a874bb9ea0871",
        "title": "T\u00dcLU 3: Pushing Frontiers in Open Language Model Post-Training"
      },
      {
        "paperId": "4f98157c298b87408646672b812801f3d6618c76",
        "title": "Towards Self-Improvement of LLMs via MCTS: Leveraging Stepwise Knowledge with Curriculum Preference Learning"
      },
      {
        "paperId": "29d090aeb509663ed4a240b3f4d445508a5ec52b",
        "title": "Warm-up Free Policy Optimization: Improved Regret in Linear Markov Decision Processes"
      },
      {
        "paperId": "f02c466bdef9bb273f92b41f4c1d31e5d332ab25",
        "title": "LiteSearch: Efficacious Tree Search for LLM"
      },
      {
        "paperId": "d269ad2a38bcbfc533303ce0f9be2537ba7b71c2",
        "title": "Q*: Improving Multi-step Reasoning for LLMs with Deliberative Planning"
      },
      {
        "paperId": "dcebd9de6fb8ffd5e5fff7734e277abd6f3858ce",
        "title": "AlphaMath Almost Zero: process Supervision without process"
      },
      {
        "paperId": "6c17de3e719c0f4d1df6a16f770c2a9a5f18206f",
        "title": "Toward Self-Improvement of LLMs via Imagination, Searching, and Criticizing"
      },
      {
        "paperId": "3c45d3c19a17a3eada0a1ff20b75fdee7520117b",
        "title": "Calibrating Large Language Models Using Their Generations Only"
      },
      {
        "paperId": "0614dd89988419d379f618f46bbad72b6f5b686f",
        "title": "Thermometer: Towards Universal Calibration for Large Language Models"
      },
      {
        "paperId": "35b142ea69598e6241f0011312128031df55895c",
        "title": "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models"
      },
      {
        "paperId": "1f2a20a6efaf83214861dddae4a38a83ae18fe32",
        "title": "DeepSeek-Coder: When the Large Language Model Meets Programming - The Rise of Code Intelligence"
      },
      {
        "paperId": "210b0a3d76e93079cc51b03c4115fde545eea966",
        "title": "GPQA: A Graduate-Level Google-Proof Q&A Benchmark"
      },
      {
        "paperId": "e8df1cf6742b50a15500b8dd3dde3942e9c91418",
        "title": "Alphazero-like Tree-Search can Guide Large Language Model Decoding and Training"
      },
      {
        "paperId": "6d7b8a478801bd9d21df82d5f33ae6eced90da5e",
        "title": "Solving math word problems with process- and outcome-based feedback"
      },
      {
        "paperId": "9cf615514ec531c1e573d13490383e21e44520ae",
        "title": "Guarantees for Epsilon-Greedy Reinforcement Learning with Function Approximation"
      },
      {
        "paperId": "5f19ae1135a9500940978104ec15a5b8751bc7d2",
        "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models"
      },
      {
        "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
        "title": "Training Verifiers to Solve Math Word Problems"
      },
      {
        "paperId": "3be84e24b144541a8cd9030526ef2b8ef2cbfe54",
        "title": "A Survey of Exploration Methods in Reinforcement Learning"
      },
      {
        "paperId": "6d2f554fbd24715c2268d64f90b53a5f19044774",
        "title": "Principled Exploration via Optimistic Bootstrapping and Backward Induction"
      },
      {
        "paperId": "57d1e7ac339e783898f2c3b1af55737cbeee9fc5",
        "title": "Measuring Mathematical Problem Solving With the MATH Dataset"
      },
      {
        "paperId": "f8a36ef98e4780019fb8bba315dca0da43e6ee57",
        "title": "Play, Curiosity, and Cognition"
      },
      {
        "paperId": "007fd689a806de99f053a0f5ef90aedb44f9ec7e",
        "title": "Better Exploration with Optimistic Actor-Critic"
      },
      {
        "paperId": "d423fa6cd0f4d088941d9fe4bebd834d0137c9b9",
        "title": "Provably Efficient Reinforcement Learning with Linear Function Approximation"
      },
      {
        "paperId": "4cb3fd057949624aa4f0bbe7a6dcc8777ff04758",
        "title": "Exploration by Random Network Distillation"
      },
      {
        "paperId": "811df72e210e20de99719539505da54762a11c6d",
        "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "225ab689f41cef1dc18237ef5dab059a49950abf",
        "title": "Curiosity-Driven Exploration by Self-Supervised Prediction"
      },
      {
        "paperId": "802168a81571dde28f5ddb94d84677bc007afa7b",
        "title": "Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles"
      },
      {
        "paperId": "0fcb2034e31a2bc2f12a2b1363d0d77baf445fdf",
        "title": "#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning"
      },
      {
        "paperId": "4b63e34276aa98d5345efa7fe09bb06d8a9d8f52",
        "title": "Deep Exploration via Bootstrapped DQN"
      },
      {
        "paperId": "024006d4c2a89f7acacc6e4438d156525b60a98f",
        "title": "Continuous control with deep reinforcement learning"
      },
      {
        "paperId": "f35de4f9b1a7c4d3fa96a0d2ab1bf8937671f6b6",
        "title": "Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning"
      },
      {
        "paperId": "ec0072bc37f83f1a81459df43289613e04cc61e1",
        "title": "A contextual-bandit approach to personalized news article recommendation"
      },
      {
        "paperId": "52f72a347c92ad10079efc89f6f537affe9d2341",
        "title": "Adaptive treatment allocation and the multi-armed bandit problem"
      },
      {
        "paperId": "5c3391bde2bb1b3d737913ee8caa01492a782732",
        "title": "WHO Technical Report"
      },
      {
        "paperId": "f6264b11ec0dd9133f1d88a5288a3267a93182f8",
        "title": "What Matters for On-Policy Deep Actor-Critic Methods? A Large-Scale Study"
      },
      {
        "paperId": "97efafdb4a3942ab3efba53ded7413199f79c054",
        "title": "Reinforcement Learning: An Introduction"
      },
      {
        "paperId": null,
        "title": "Bootstrap methods and their application . Number 1"
      },
      {
        "paperId": null,
        "title": "American invitational mathematics examination (AIME)"
      },
      {
        "paperId": null,
        "title": "American mathematics competitions (AMC 10/12)"
      },
      {
        "paperId": null,
        "title": "Rag-gym: Optimizing reasoning and search agents with process supervision"
      }
    ],
    "cited_by": [
      {
        "paperId": "0133cb4289ea5e4e4a5b4937b7c0e42ccd4b52e2",
        "title": "Evolving Language Models without Labels: Majority Drives Selection, Novelty Promotes Variation"
      },
      {
        "paperId": "6478c185582f5e48c54677c683a9297ff2916118",
        "title": "One Token to Fool LLM-as-a-Judge"
      },
      {
        "paperId": "e2f05b7885ec5295e80a51d7264adc08681d9f14",
        "title": "Thinking Outside the (Gray) Box: A Context-Based Score for Assessing Value and Originality in Neural Text Generation"
      }
    ],
    "score": 3.0
  },
  {
    "id": "e32e28a8a06739997957113b7fa1bd033f6801ba",
    "title": "An Improved Reinforcement Learning Method Based on Unsupervised Learning",
    "authors": [
      "Xin Chang",
      "Yanbin Li",
      "Guanjie Zhang",
      "Donghui Liu",
      "Changjun Fu"
    ],
    "year": 2024,
    "citationCount": 3,
    "abstract": "The approach of directly combining clustering method and reinforcement learning (RL) will lead to encounter the issue where states may have different state transition processes under the same action, resulting in poor policy performance. To address this challenge with multi-dimensional continuous observation data, an improved reinforcement learning method based on unsupervised learning is proposed with a novel framework. Instead of dimensionality reduction methods, unsupervised clustering is employed to indirectly capture the underlying structure of the data. First, the proposed framework incorporates multi-dimensional information, including the current observation data, the next observation data and reward information, during the clustering process, leading to a more accurate and comprehensive low-dimensional discrete representation of the observation data while retaining preserving transition of Markov decision process. Second, by compressing the observation data into a well-defined state space, the resulting cluster labels serve as the low-dimensional discrete label-states for reinforcement learning to generate more effective and robust policies. Comparative analysis with state-of-the-art RL methods demonstrates that the improved RL methods base on framework achieves higher rewards, indicating its superior performance. Furthermore, the framework exhibits computational efficiency, as evidenced by its reasonable time complexity. This structural innovation allows for better exploration and exploitation of the transition, leading to improved policy performance in engineering applications.",
    "url": "https://www.semanticscholar.org/paper/e32e28a8a06739997957113b7fa1bd033f6801ba",
    "pdf_url": "https://doi.org/10.1109/ACCESS.2024.3351696",
    "venue": "IEEE Access",
    "publicationDate": "",
    "externalIds": {
      "DBLP": "journals/access/ChangLZLF24",
      "DOI": "10.1109/ACCESS.2024.3351696",
      "CorpusId": 266907199
    },
    "references": [
      {
        "paperId": "7be08125edabe044657313af94064a0381f7e13f",
        "title": "Dense reinforcement learning for safety validation of autonomous vehicles"
      },
      {
        "paperId": "0256a2f617b84d6d4444dcffaacf3691aa56ee7c",
        "title": "Automating DBSCAN via Deep Reinforcement Learning"
      },
      {
        "paperId": "0119762c13de7736cfc2bbae706ec316708ee054",
        "title": "A Scattered Wave Deceptive Jamming Method Based on Genetic Algorithm against Three Channel SAR GMTI"
      },
      {
        "paperId": "361c00b22e29d0816ca896513d2c165e26399821",
        "title": "Grandmaster level in StarCraft II using multi-agent reinforcement learning"
      },
      {
        "paperId": "12c0751b4f51ed833172a713b7e32390032ead93",
        "title": "Soft Actor-Critic Algorithms and Applications"
      },
      {
        "paperId": "6bdb53396c6be8f53b3c1a2c9cdd7a4521e60405",
        "title": "A Novel Scattered Wave Deception Jamming Against Three Channel SAR GMTI"
      },
      {
        "paperId": "a50ecb9ed4f376eac69c456cf0d31089b435ec28",
        "title": "K-means properties on six clustering benchmark datasets"
      },
      {
        "paperId": "c27078d60737ea10e8ca4f05acd114fef29c8276",
        "title": "A Tutorial on Bayesian Optimization"
      },
      {
        "paperId": "1ffcb25bb4562ce51929d30eb7c4e5e6481b4c75",
        "title": "Stackelberg Game Approaches for Anti-Jamming Defence in Wireless Networks"
      },
      {
        "paperId": "cc59b4b1eb7d4629f753bc24f029c5cced301381",
        "title": "Large scale distributed neural network training through online distillation"
      },
      {
        "paperId": "811df72e210e20de99719539505da54762a11c6d",
        "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor"
      },
      {
        "paperId": "af10f3c1c0859aa620623f760c8a29e78f177f7f",
        "title": "Population Based Training of Neural Networks"
      },
      {
        "paperId": "453921513df609aa59ce0e2536ba4d838cf3ee72",
        "title": "Anti-Jamming Communications Using Spectrum Waterfall: A Deep Reinforcement Learning Approach"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "bd75ef44d2a706d02fa99b1c1e5d293d30a411ec",
        "title": "Deep Multi-User Reinforcement Learning for Distributed Dynamic Spectrum Access"
      },
      {
        "paperId": "9172cd6c253edf7c3a1568e03577db20648ad0c4",
        "title": "Reinforcement Learning with Deep Energy-Based Policies"
      },
      {
        "paperId": "10a6abdded2bec4d82a2bd6144fcc2441cce89b8",
        "title": "You Can Jam But You Cannot Hide: Defending Against Jamming Attacks for Geo-Location Database Driven Spectrum Sharing"
      },
      {
        "paperId": "cf4de496ff4e9c7ce3d65380349a2c7b2a3669af",
        "title": "Three-dimensional deceptive scene generation against single-pass InSAR based on coherent transponders"
      },
      {
        "paperId": "25fb5a6abcd88ee52bdb3165b844c941e90eb9bf",
        "title": "Revisiting Distributed Synchronous SGD"
      },
      {
        "paperId": "4c05d7caa357148f0bbd61720bdd35f0bc05eb81",
        "title": "Dueling Network Architectures for Deep Reinforcement Learning"
      },
      {
        "paperId": "340f48901f72278f6bf78a04ee5b01df208cc508",
        "title": "Human-level control through deep reinforcement learning"
      },
      {
        "paperId": "f50ac6159deddbf93efd4180e53296f977ec8da3",
        "title": "Jamming and anti-jamming techniques in wireless networks: a survey"
      },
      {
        "paperId": "03911c85305d42aa2eeb02be82ef6fb7da644dd0",
        "title": "Algorithms for Hyper-Parameter Optimization"
      },
      {
        "paperId": "0ef24f3ba8304ab5b397660c83d1e8b8d1d82250",
        "title": "Optimized Approximation Algorithm in Neural Networks Without Overfitting"
      },
      {
        "paperId": "906e1af8589d3b8918b0af239cadb246c8a8837e",
        "title": "Wireless Communications"
      },
      {
        "paperId": "7adb5b7379a60af6e37262af9a4673a15003bee0",
        "title": "Computational Intelligence: An Introduction"
      },
      {
        "paperId": "48b86fa13f84f1c741c184f9951f6a6f0da5ff76",
        "title": "Detection of Android Malware Based on Deep Forest and Feature Enhancement"
      },
      {
        "paperId": "e827aa16dd1a1587993e51582db0194cf6901a57",
        "title": "Performance Improvement on Traditional Chinese Task-Oriented Dialogue Systems With Reinforcement Learning and Regularized Dropout Technique"
      },
      {
        "paperId": "df7fff6de467411df249c55a557985da00c3e476",
        "title": "An Improved Anti-jamming Method Based on Deep Reinforcement Learning and Feature Engineering"
      },
      {
        "paperId": "94d56b2b1a9bb641a6e1cd35733bdd298828f967",
        "title": "A Multiple-Jammer Deceptive Jamming Method Based on Particle Swarm Optimization against Three-channel SAR GMTI"
      },
      {
        "paperId": null,
        "title": "Deep Hands-On Unsupervised Learning Using Python"
      },
      {
        "paperId": "8ca331d1fa033c3914d1d7c8dda4bc5ac678ec15",
        "title": "Pattern-Aware Intelligent Anti-Jamming Communication: A Sequential Deep Reinforcement Learning Approach"
      },
      {
        "paperId": null,
        "title": "Hands-on Machine Learning With Scikit-Learn, Keras, and TensorFlow"
      },
      {
        "paperId": null,
        "title": "Deep Reinforcement Learning Hands-On: Apply Modern RL methods,WithDeepQ-Networks,ValueIteration,PolicyGradients,TRPO,AlphaGoZeroandMore . Birmingham, U.K."
      },
      {
        "paperId": "04edb335135d3fe16240ba5b4e941cefff8cadce",
        "title": "Detection of Genes Patterns with an Enhanced Partitioning-Based DBSCAN Algorithm"
      },
      {
        "paperId": null,
        "title": "Hands-On Reinforcement Learning With Python: Master Reinforcement and Deep Reinforcement Learning Using OpenAI Gym and TensorFlow"
      },
      {
        "paperId": null,
        "title": "\u2018\u2018Study on distributed cooperative jamming techniques against multichannel SAR,\u2019\u2019"
      },
      {
        "paperId": null,
        "title": "Anti-JammingTransmissionsinCognitiveRadioNetworks"
      },
      {
        "paperId": "97efafdb4a3942ab3efba53ded7413199f79c054",
        "title": "Reinforcement Learning: An Introduction"
      },
      {
        "paperId": null,
        "title": "Interpretable Machine Learning. A Guide for Making Black Box Models Explainable . Victoria, BC, Canada"
      },
      {
        "paperId": null,
        "title": "Improved Reinforcement Learning Method Based on Unsupervised Learning"
      }
    ],
    "cited_by": [
      {
        "paperId": "53d24297aa5f71fe43f7b81cdd6d630d67433fc3",
        "title": "Data clustering: a fundamental method in data science and management"
      },
      {
        "paperId": "4b243457fbcbb1a3b046837c87309a7dfac82431",
        "title": "AI-driven fusion with cybersecurity: Exploring current trends, advanced techniques, future directions, and policy implications for evolving paradigms- A comprehensive review"
      },
      {
        "paperId": "bbf8b25506da2876d8a98edf0c55ed55aff4d3aa",
        "title": "Advances in Non-Contact Human Vital Sign Detection: A Detailed Survey of Radar and Wireless Solutions"
      }
    ],
    "score": 3.0
  },
  {
    "id": "c81a06446fa1df12cc043d9d9c8cfd5775090c59",
    "title": "Enhancing Scalability in Reinforcement Learning for Open Spaces",
    "authors": [
      "J. Janjua",
      "Shagufta Kousar",
      "Areeba Khan",
      "Anaum Ihsan",
      "Tahir Abbas",
      "Ali Q Saeed"
    ],
    "year": 2024,
    "citationCount": 3,
    "abstract": "Reinforcement Learning (RL) has been successful when the environment has specific objectives and boundaries. But with the emerging focus on open-world application which makes all or some of the rules or purpose go to naught, it makes traditional methods of RL a bit more difficult. This paper goes over various advancements and changes in Reinforcement Learning which can be employed for open-ended environments. Among the other strategies, hierarchical reinforcement learning, intrinsic motivation-based exploration, meta-learning and unsupervised skill acquisition are also among the ones that are examined. As a result, such a position based on the technology argues the promising future of open-ended methods for the management of complex problems and high level of uncertainty associated with the preset target or purpose. Also, we study cases in video games, robotics and autonomous systems, where RL is implemented in an open-ended and dynamic environment. We also outline existing limitations and perspectives, highlighting the need for more flexible methods and inter-scientific collaboration to fully realize RL's potential in open-ended contexts.",
    "url": "https://www.semanticscholar.org/paper/c81a06446fa1df12cc043d9d9c8cfd5775090c59",
    "pdf_url": "https://doi.org/10.1109/DASA63652.2024.10836237",
    "venue": "2024 International Conference on Decision Aid Sciences and Applications (DASA)",
    "publicationDate": "2024-12-11",
    "externalIds": {
      "DOI": "10.1109/DASA63652.2024.10836237",
      "CorpusId": 275583732
    },
    "references": [],
    "cited_by": [
      {
        "paperId": "36c02d8ceed9cd123760249e2024d75555cb5d6a",
        "title": "Predictive and Reinforcement Learning-Based Framework for Cloud Resource Optimization"
      },
      {
        "paperId": "2ad559f81f4ee0dd362b382f9f99c5fc6b1b6053",
        "title": "An Advanced Deep Learning Framework for Proactive Potato Leaf Disease Detection to Revolutionize Agriculture"
      },
      {
        "paperId": "3ac16c032573c8e7d64eef5658f5df80c5db2784",
        "title": "Human Activity-Based Machine Learning and Deep Learning Techniques"
      }
    ],
    "score": 3.0
  },
  {
    "id": "66c5bed508f9dce55b2beeaaf36a29929c0bc2ac",
    "title": "Water Age Control for Water Distribution Networks via Safe Reinforcement Learning",
    "authors": [
      "Jorge Val Ledesma",
      "Rafa\u0142 Wi\u015bniewski",
      "C. Kalles\u00f8e",
      "Agisilaos Tsouvalas"
    ],
    "year": 2024,
    "citationCount": 3,
    "abstract": "Reinforcement learning (RL) is a widely used control technique that finds an optimal policy using the feedback of its actions. The search for the optimal policy requires that the system explores a broad region of the state space. This search puts at risk the safe operation, since some of the explored regions might be near the physical system limits. Implementing learning methods in industrial applications is limited because of its uncertain behavior when finding an optimal policy. This work proposes an RL control algorithm with a filter that supervises the safety of the exploration based on a nominal model. The performance of this safety filter is increased by modeling the uncertainty with a Gaussian process (GP) regression. This method is applied to the optimization of the management of a water distribution network (WDN) with an elevated reservoir; the management objectives are to regulate the tank filling while maintaining an adequate water turnover. The proposed method is validated in a laboratory setup that emulates the hydraulic features of a WDN.",
    "url": "https://www.semanticscholar.org/paper/66c5bed508f9dce55b2beeaaf36a29929c0bc2ac",
    "pdf_url": "https://doi.org/10.1109/TCST.2024.3426300",
    "venue": "IEEE Transactions on Control Systems Technology",
    "publicationDate": "2024-11-01",
    "externalIds": {
      "DBLP": "journals/tcst/LedesmaWKT24",
      "DOI": "10.1109/TCST.2024.3426300",
      "CorpusId": 271657652
    },
    "references": [],
    "cited_by": [
      {
        "paperId": "0b3701085d610398f13f9bff7f5dcad3487178f9",
        "title": "A zone predictive safety filter for drill string torsional vibrations control considering measurement delay and data loss"
      },
      {
        "paperId": "ffc4b80c481d2f4fad2f4f7c1fd5b55f9fb4861e",
        "title": "Welfare and Cost Aggregation for Multi-Agent Control: When to Choose Which Social Cost Function, and Why?"
      },
      {
        "paperId": "221f56256e29d098cf0816fe533b05db8735a698",
        "title": "Water Distribution Network for Irrigation using Artificial Neural Network (ANN) Algorithm"
      }
    ],
    "score": 3.0
  },
  {
    "id": "2389fafc2a97e13fa810c4014babe73bd886c06f",
    "title": "Offline Reinforcement Learning with Failure Under Sparse Reward Environments",
    "authors": [
      "Mingkang Wu",
      "Umer Siddique",
      "Abhinav Sinha",
      "Yongcan Cao"
    ],
    "year": 2024,
    "citationCount": 3,
    "abstract": "This paper presents a new reinforcement learning approach that leverages failed experiences in sparse reward environments. Unlike traditional reinforcement learning methods that rely on successful experiences or expert demonstrations, the proposed approach utilizes failed experiences to guide the policy update during the learning process. The primary objective behind this work is to develop a method that can efficiently utilize failed experiences to guide the search direction as directional cues from successful experiences may be limited in sparse environments. To achieve this objective, we introduce a new objective function that aims to maximize the dissimilarity between the RL agent's actions and actions from failed experiences. This discrepancy serves as a valuable indicator to guide the agent's exploration. In other words, our method focuses on achieving the desired objectives by leveraging failed experiences to provide a significant opportunity for the agent to refine its policy. We further employ hindsight experience replay (HER) to enhance the directional search by creating and achieving potential subgoals that align with the primary objectives. To assess the effectiveness of our method, we conduct experiments on three sparse reward environments. Our findings demonstrate that the proposed approach significantly enhances the agent's learning efficiency and improves robustness to variations in demonstration quality compared to conventional reinforcement learning techniques.",
    "url": "https://www.semanticscholar.org/paper/2389fafc2a97e13fa810c4014babe73bd886c06f",
    "pdf_url": "https://doi.org/10.1109/ICMI60790.2024.10585936",
    "venue": "International Conference on Multimodal Interaction",
    "publicationDate": "2024-04-13",
    "externalIds": {
      "DOI": "10.1109/ICMI60790.2024.10585936",
      "CorpusId": 271115033
    },
    "references": [],
    "cited_by": [
      {
        "paperId": "3e3f5f2a0ee3536277711047f04d2317db75867b",
        "title": "Multi-Task Reward Learning from Human Ratings"
      },
      {
        "paperId": "a24a85b1e45770385ee668ad02f8794498e5c5eb",
        "title": "TW-CRL: Time-Weighted Contrastive Reward Learning for Efficient Inverse Reinforcement Learning"
      },
      {
        "paperId": "c3cd8042268a35436a1e1cbbe745fd2da138d873",
        "title": "RbRL2.0: Integrated Reward and Policy Learning for Rating-based Reinforcement Learning"
      }
    ],
    "score": 3.0
  },
  {
    "id": "b2d827c286e32dbf0739e8c796b119b1074809b4",
    "title": "Meta SAC-Lag: Towards Deployable Safe Reinforcement Learning via MetaGradient-based Hyperparameter Tuning",
    "authors": [
      "Homayoun Honari",
      "Amir M. Soufi Enayati",
      "Mehran Ghafarian Tamizi",
      "Homayoun Najjaran"
    ],
    "year": 2024,
    "citationCount": 3,
    "abstract": "Safe Reinforcement Learning (Safe RL) is one of the prevalently studied subcategories of trial-and-error-based methods with the intention to be deployed on real-world systems. In safe RL, the goal is to maximize reward performance while minimizing constraints, often achieved by setting bounds on constraint functions and utilizing the Lagrangian method. However, deploying Lagrangian-based safe RL in real-world scenarios is challenging due to the necessity of threshold fine-tuning, as imprecise adjustments may lead to suboptimal policy convergence. To mitigate this challenge, we propose a unified Lagrangian-based model-free architecture called Meta Soft Actor-Critic Lagrangian (Meta SAC-Lag). Meta SAC-Lag uses meta-gradient optimization to automatically update the safety-related hyperparameters. The proposed method is designed to address safe exploration and threshold adjustment with minimal hyperparameter tuning requirement. In our pipeline, the inner parameters are updated through the conventional formulation and the hyperparameters are adjusted using the meta-objectives which are defined based on the updated parameters. Our results show that the agent can reliably adjust the safety performance due to the relatively fast convergence rate of the safety threshold. We evaluate the performance of Meta SAC-Lag in five simulated environments against Lagrangian baselines, and the results demonstrate its capability to create synergy between parameters, yielding better or competitive results. Furthermore, we conduct a real-world experiment involving a robotic arm tasked with pouring coffee into a cup without spillage. Meta SAC-Lag is successfully trained to execute the task, while minimizing effort constraints. The success of Meta SAC-Lag in performing the experiment is intended to be a step toward practical deployment of safe RL algorithms to learn the control process of safety-critical real-world systems without explicit engineering.",
    "url": "https://www.semanticscholar.org/paper/b2d827c286e32dbf0739e8c796b119b1074809b4",
    "pdf_url": "https://arxiv.org/pdf/2408.07962.pdf",
    "venue": "IEEE/RJS International Conference on Intelligent RObots and Systems",
    "publicationDate": "2024-08-15",
    "externalIds": {
      "DBLP": "conf/iros/HonariETN24",
      "ArXiv": "2408.07962",
      "DOI": "10.1109/IROS58592.2024.10802547",
      "CorpusId": 271874554
    },
    "references": [
      {
        "paperId": "7739a5b98e205c8ea6591406989eff4952f88075",
        "title": "A Review of Safe Reinforcement Learning: Methods, Theories, and Applications"
      },
      {
        "paperId": "e63f0dc1963db342f8e87be1b565f4273404db5b",
        "title": "Safety Optimized Reinforcement Learning via Multi-Objective Policy Optimization"
      },
      {
        "paperId": "e56769f6f43c1922e037afef75a7d6c3177516b1",
        "title": "Safety-Gymnasium: A Unified Safe Reinforcement Learning Benchmark"
      },
      {
        "paperId": "f080c1a4ff16a79bf5022b9d41fed5d90c453736",
        "title": "Facilitating Sim-to-Real by Intrinsic Stochasticity of Real-Time Simulation in Reinforcement Learning for Robot Manipulation"
      },
      {
        "paperId": "9c5f056c4e7986064722bb522e46e3546be8da51",
        "title": "A Review of Safe Reinforcement Learning: Methods, Theory and Applications"
      },
      {
        "paperId": "d8da5d28807fe5f8af2e6942c67738bd9f12a1ec",
        "title": "Safe Reinforcement Learning by Imagining the Near Future"
      },
      {
        "paperId": "c77f42bb6c3f12a6110a56f08ee9b14259a5b66e",
        "title": "Improving Safety in Deep Reinforcement Learning using Unsupervised Action Planning"
      },
      {
        "paperId": "d6e783bce3b8e3ad082c2757235c34cb86c4e653",
        "title": "Safe Learning in Robotics: From Learning-Based Control to Safe Reinforcement Learning"
      },
      {
        "paperId": "5f1adc14a77fb61aa463fac728397bd32e00b617",
        "title": "Challenges of real-world reinforcement learning: definitions, benchmarks and analysis"
      },
      {
        "paperId": "127d6cd7da419cf02a5ae6e84e3035323509a61b",
        "title": "Applications of reinforcement learning in energy systems"
      },
      {
        "paperId": "8343b6f3c8424ac1a8069d31b7a0de1e8f3c40b8",
        "title": "Discovery of Options via Meta-Learned Subgoals"
      },
      {
        "paperId": "431dc05ac25510de6264084434254cca877f9ab3",
        "title": "Recovery RL: Safe Reinforcement Learning With Learned Recovery Zones"
      },
      {
        "paperId": "a334f9897a330abddf99cfec0b5a70f751e9497b",
        "title": "Conservative Safety Critics for Exploration"
      },
      {
        "paperId": "eecc04e4751ef623ecd9f9e69e9601c9431152d2",
        "title": "Balancing Constraints and Rewards with Meta-Gradient D4PG"
      },
      {
        "paperId": "cf34efc663284da131e747407ac3f389f898e471",
        "title": "robosuite: A Modular Simulation Framework and Benchmark for Robot Learning"
      },
      {
        "paperId": "629d0ce250581471f07083bbab95f23623b00201",
        "title": "Responsive Safety in Reinforcement Learning by PID Lagrangian Methods"
      },
      {
        "paperId": "9e72a468fb287b31ddbe65b167a5d15c0055d3a5",
        "title": "Meta-SAC: Auto-tune the Entropy Temperature of Soft Actor-Critic via Metagradient"
      },
      {
        "paperId": "0b8c259e7fabb1c49c907af9a8ff63d8d38ebcff",
        "title": "A Self-Tuning Actor-Critic Algorithm"
      },
      {
        "paperId": "a2e43270a9b1421e452c2975e5163e2a216abeac",
        "title": "A Survey of Deep Reinforcement Learning in Video Games"
      },
      {
        "paperId": "896e5529de1da1e4494033404721b70339bb9557",
        "title": "Challenges of Real-World Reinforcement Learning"
      },
      {
        "paperId": "12c0751b4f51ed833172a713b7e32390032ead93",
        "title": "Soft Actor-Critic Algorithms and Applications"
      },
      {
        "paperId": "2ed619fbc7902155d54f6f21da16ad6c120eac63",
        "title": "Learning to Walk via Deep Reinforcement Learning"
      },
      {
        "paperId": "7b35984b9b0bde78290cf92d8176682b65e59f54",
        "title": "GPU-Accelerated Robotic Simulation for Distributed Reinforcement Learning"
      },
      {
        "paperId": "d37a34c204a8beefcaef4dddddb7a90c16e973d4",
        "title": "Learning dexterous in-hand manipulation"
      },
      {
        "paperId": "15561ab20c298e113b0008b7a029486a422e7ca3",
        "title": "Bilevel Programming for Hyperparameter Optimization and Meta-Learning"
      },
      {
        "paperId": "cb7c479a36520da1caeeec67db10772351a390c6",
        "title": "Reward Constrained Policy Optimization"
      },
      {
        "paperId": "2a49a71c9d40051a03c4445fe49025bc75d9eeb6",
        "title": "Meta-Gradient Reinforcement Learning"
      },
      {
        "paperId": "65fb1b37c41902793ac65db3532a6e51631a9aff",
        "title": "A Lyapunov-based Approach to Safe Reinforcement Learning"
      },
      {
        "paperId": "811df72e210e20de99719539505da54762a11c6d",
        "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor"
      },
      {
        "paperId": "c7ec956cb69ee85269067bdaa3c06a8aa63f04ac",
        "title": "On Optimal Generalizability in Parametric Learning"
      },
      {
        "paperId": "88880d88073a99107bbc009c9f4a4197562e1e44",
        "title": "Safe Model-based Reinforcement Learning with Stability Guarantees"
      },
      {
        "paperId": "c889d6f98e6d79b89c3a6adf8a921f88fa6ba518",
        "title": "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks"
      },
      {
        "paperId": "769ef3d5021cd71c37d2c403f231a53d1accf786",
        "title": "An overview of gradient descent optimization algorithms"
      },
      {
        "paperId": "024006d4c2a89f7acacc6e4438d156525b60a98f",
        "title": "Continuous control with deep reinforcement learning"
      },
      {
        "paperId": "f0c4b7568f378e652645232e66a1dab4c5b5293f",
        "title": "Risk-Sensitive Reinforcement Learning"
      },
      {
        "paperId": "b354ee518bfc1ac0d8ac447eece9edb69e92eae1",
        "title": "MuJoCo: A physics engine for model-based control"
      },
      {
        "paperId": "d66ab178e941e3a79954c858d3c1bcdee8e1d17d",
        "title": "Safe Exploration in Markov Decision Processes"
      },
      {
        "paperId": "9c1a20ae539c4e4a7b991d46f7d9b3f8262e0894",
        "title": "Neuroevolutionary reinforcement learning for generalized control of simulated helicopters"
      },
      {
        "paperId": "3cc2608fd77b9b65f5bd378e8797b2ab1b8acde7",
        "title": "Constrained Markov Decision Processes"
      },
      {
        "paperId": null,
        "title": "Gymnasium robotics"
      },
      {
        "paperId": "49f67c0ee24f28cf2fe5670e37d195330d833524",
        "title": "Dynamic"
      },
      {
        "paperId": "97efafdb4a3942ab3efba53ded7413199f79c054",
        "title": "Reinforcement Learning: An Introduction"
      },
      {
        "paperId": null,
        "title": "Pybullet, a python module for physics simulation for games, robotics and machine learning"
      }
    ],
    "cited_by": [
      {
        "paperId": "b8560e6ba7b58590e68f087f8fb09be4558f7bd1",
        "title": "Proactive Constrained Policy Optimization with Preemptive Penalty"
      },
      {
        "paperId": "73c73f5ceaa7241f9aa54eefda579858f859b913",
        "title": "Uncertainty-Aware Safety-Critical Decision and Control for Autonomous Vehicles at Unsignalized Intersections"
      },
      {
        "paperId": "f3480a830b2aa0d64e099b226c5184ff199cc059",
        "title": "Machine learning-based optimization of cytotoxicity testing for assessing Zn-based biodegradable metals"
      }
    ],
    "score": 3.0
  },
  {
    "id": "fae722ae17483aeef3485f0177346ba3ce332ea9",
    "title": "Task-Driven Autonomous Driving: Balanced Strategies Integrating Curriculum Reinforcement Learning and Residual Policy",
    "authors": [
      "Jiamin Shi",
      "Tangyike Zhang",
      "Ziqi Zong",
      "Shitao Chen",
      "Jingmin Xin",
      "Nanning Zheng"
    ],
    "year": 2024,
    "citationCount": 3,
    "abstract": "Achieving fully autonomous driving in urban traffic scenarios is a significant challenge that necessitates balancing safety, efficiency, and compliance with traffic regulations. In this letter, we introduce a novel Curriculum Residual Hierarchical Reinforcement Learning (CR-HRL) framework. It integrates a rule-based planning model as a guiding mechanism, while a deep reinforcement learning algorithm generates supplementary residual strategies. This combination enables the RL agent to perform safe and efficient overtaking in complex traffic scenarios. Furthermore, we implement a detailed three-stage curriculum learning strategy that enhances the training process. By progressively increasing task complexity, the curriculum strategy effectively guides the exploration of autonomous vehicles and improves the reusability of sub-strategies. The effectiveness of the CR-HRL framework is confirmed through ablation experiments. Comparative experiments further highlight the superior efficiency and decision-making capabilities of our framework over traditional rule-based and RL baseline methods. Tests conducted with actual vehicles also demonstrate its practical applicability in real-world settings.",
    "url": "https://www.semanticscholar.org/paper/fae722ae17483aeef3485f0177346ba3ce332ea9",
    "pdf_url": "https://doi.org/10.1109/LRA.2024.3448237",
    "venue": "IEEE Robotics and Automation Letters",
    "publicationDate": "2024-11-01",
    "externalIds": {
      "DBLP": "journals/ral/ShiZZCXZ24",
      "DOI": "10.1109/LRA.2024.3448237",
      "CorpusId": 272079062
    },
    "references": [],
    "cited_by": [
      {
        "paperId": "6f740b20ff86f443c0514458390fdc6c0fe3854d",
        "title": "Multi-residual Mixture of Experts Learning for Cooperative Control in Multi-vehicle Systems"
      },
      {
        "paperId": "c8d8513bcde9c9225baa0eef14ecdd399cb96d0a",
        "title": "A Survey of Reinforcement Learning-Based Motion Planning for Autonomous Driving: Lessons Learned from a Driving Task Perspective"
      },
      {
        "paperId": "1f78a2f06ac12c3470e55f0ad0a9f3c80122babf",
        "title": "FSDP: Fast and Safe Data-Driven Overtaking Trajectory Planning for Head-to-Head Autonomous Racing Competitions"
      }
    ],
    "score": 3.0
  },
  {
    "id": "dca6c5f8f6c64b4188212e16c2faf461ffd4e49a",
    "title": "Reinforcement learning-based dynamic field exploration and reconstruction using multi-robot systems for environmental monitoring",
    "authors": [
      "Thinh Lu",
      "Divyam Sobti",
      "Deepak Talwar",
      "Wencen Wu"
    ],
    "year": 2025,
    "citationCount": 3,
    "abstract": "In the realm of real-time environmental monitoring and hazard detection, multi-robot systems present a promising solution for exploring and mapping dynamic fields, particularly in scenarios where human intervention poses safety risks. This research introduces a strategy for path planning and control of a group of mobile sensing robots to efficiently explore and reconstruct a dynamic field consisting of multiple non-overlapping diffusion sources. Our approach integrates a reinforcement learning-based path planning algorithm to guide the multi-robot formation in identifying diffusion sources, with a clustering-based method for destination selection once a new source is detected, to enhance coverage and accelerate exploration in unknown environments. Simulation results and real-world laboratory experiments demonstrate the effectiveness of our approach in exploring and reconstructing dynamic fields. This study advances the field of multi-robot systems in environmental monitoring and has practical implications for rescue missions and field explorations.",
    "url": "https://www.semanticscholar.org/paper/dca6c5f8f6c64b4188212e16c2faf461ffd4e49a",
    "pdf_url": "https://doi.org/10.3389/frobt.2025.1492526",
    "venue": "Frontiers Robotics AI",
    "publicationDate": "2025-03-25",
    "externalIds": {
      "DBLP": "journals/firai/LuSTW25",
      "PubMedCentral": "11975907",
      "DOI": "10.3389/frobt.2025.1492526",
      "CorpusId": 277372699,
      "PubMed": "40201564"
    },
    "references": [
      {
        "paperId": "cfb54ac023dffd7fd2647540fc8be3565d992f80",
        "title": "Multi-robots Path Planning and Mapping for Exploring Unknown Environment"
      },
      {
        "paperId": "2c6a4036c0961b80c0b8da986b1d23eb783da157",
        "title": "CURE: A Hierarchical Framework for Multi-Robot Autonomous Exploration Inspired by Centroids of Unknown Regions"
      },
      {
        "paperId": "22acd6529ad0b849c1b3b5124cf0cfca829aeafa",
        "title": "HMA-SAR: Multi-Agent Search and Rescue for Unknown Located Dynamic Targets in Completely Unknown Environments"
      },
      {
        "paperId": "393797551f0499860ba72c56346383374853c3b0",
        "title": "Multi-Robot Autonomous Exploration in Unknown Environment: A Review"
      },
      {
        "paperId": "8faa8354e7f1ca15ec330a6e1b4df554b5dfa2ae",
        "title": "Graph Soft Actor\u2013Critic Reinforcement Learning for Large-Scale Distributed Multirobot Coordination"
      },
      {
        "paperId": "db01720987e8070f011f59bbfe6912b58bf87ded",
        "title": "A Novel Collaborative Knowledge Sharing and Self-Learning Framework for Robotic Systems in Search and Rescue Operations"
      },
      {
        "paperId": "bbdb2c71c041dad5a5fa62c2e49c612b1a919691",
        "title": "Analytical and numerical insights into wildfire dynamics: Exploring the advection-diffusion-reaction model"
      },
      {
        "paperId": "da2ff4f1de763c7e60a6e984e841bc9364e43f9d",
        "title": "Distributed cooperative Kalman filter constrained by advection\u2013diffusion equation for mobile sensor networks"
      },
      {
        "paperId": "45369b0ef2f65ba65179a36d79c9ab939dd2a5cc",
        "title": "Curriculum Reinforcement Learning From Avoiding Collisions to Navigating Among Movable Obstacles in Diverse Environments"
      },
      {
        "paperId": "969de41a7fc99dda9402efc3a6e62c0f0b2bcd0f",
        "title": "Cooperative Filtering and Parameter Identification for Advection\u2013Diffusion Processes Using a Mobile Sensor Network"
      },
      {
        "paperId": "1f8ccbe2b3a520229554c40c5c382d5fc089645e",
        "title": "Adaptive Optimal Tracking Control of an Underactuated Surface Vessel Using Actor\u2013Critic Reinforcement Learning"
      },
      {
        "paperId": "4c85ee1699f31087fd12b370add95856b0931f16",
        "title": "Robotic Swarm for Marine and Submarine Missions: Challenges and Perspectives"
      },
      {
        "paperId": "55635aac4cd439a00356f83dad52bd8d7b0ea87e",
        "title": "A Survey on Curriculum Learning"
      },
      {
        "paperId": "1951c098107c15ddfc7481458ec4aaf831123043",
        "title": "Parameter Identification of Spatial\u2013Temporal Varying Processes by a Multi-Robot System in Realistic Diffusion Fields"
      },
      {
        "paperId": "3170a03ca43d074af1b7b0174cc31cd4931afa46",
        "title": "A Graph Partitioning Approach for Fast Exploration with Multi-Robot Coordination"
      },
      {
        "paperId": "e0107d9dec4bd61f99537e4df0013d66cdbbe0a8",
        "title": "Early Forest Fire Detection Using Drones and Artificial Intelligence"
      },
      {
        "paperId": "cc13c12259d70d6ef09e38ce6cdfcf3d23db6d86",
        "title": "Deep Reinforcement Learning Robot for Search and Rescue Applications: Exploration in Unknown Cluttered Environments"
      },
      {
        "paperId": "938b3be9f2b846c1c6e76a9724b424cb9542e14e",
        "title": "Geometric Reinforcement Learning Based Path Planning for Mobile Sensor Networks in Advection-Diffusion Field Reconstruction"
      },
      {
        "paperId": "e70394dff36c2e2980f89a1ee75a510698670a0a",
        "title": "UX 1 system design - A robotic system for underwater mining exploration"
      },
      {
        "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
        "title": "Proximal Policy Optimization Algorithms"
      },
      {
        "paperId": "f69eaca07e9cd080dee9a955e83e12ce2540ae52",
        "title": "Cooperative filtering for parameter identification of diffusion processes"
      },
      {
        "paperId": "539a197d4cf08d56cbdd6151914359f9d1ca6e0c",
        "title": "Autonomous Gas Detection and Mapping With Unmanned Aerial Vehicles"
      },
      {
        "paperId": "449532187c94af3dd3aa55e16d2c50f7854d2199",
        "title": "Trust Region Policy Optimization"
      },
      {
        "paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8",
        "title": "Adam: A Method for Stochastic Optimization"
      },
      {
        "paperId": "367482fd67ae68865fc526b12c8ecd199b80b923",
        "title": "Coupled Controls-Computational Fluids Approach for the Estimation of the Concentration From a Moving Gaseous Source in a 2-D Domain With a Lyapunov-Guided Sensing Aerial Vehicle"
      },
      {
        "paperId": "be3def30206899053f0e34b8ca4b2518d0be0011",
        "title": "Robust Cooperative Exploration With a Switching Strategy"
      },
      {
        "paperId": "9364c99f98a9f9380468f8496b118c90ae7c3424",
        "title": "Robots for Environmental Monitoring: Significant Advancements and Applications"
      },
      {
        "paperId": "1ab5a36d75dce8d81dc4a9bb37cb6e7da372f35b",
        "title": "Optimal trajectories of mobile remote sensors for parameter estimation in distributed Cyber-Physical Systems"
      },
      {
        "paperId": "4a30ee2e1cd0bcb90934ef056732151ec87868cc",
        "title": "Distributed Consensus in Multi-vehicle Cooperative Control: Theory and Applications (Ren, W. and Beard, R.W.; 2008) [Book Shelf]"
      },
      {
        "paperId": "024abd649e2393e57951e9eaadee8372cc054658",
        "title": "Research on k-means Clustering Algorithm: An Improved k-means Clustering Algorithm"
      },
      {
        "paperId": "0f8176ae43d28ee8d60ff82cb7f2514e160c6a21",
        "title": "Cooperative Filters and Control for Cooperative Exploration"
      },
      {
        "paperId": "69e74abc6c2b2953c6da9bfcd33c9426434e8c80",
        "title": "Distributed Consensus in Multi-vehicle Cooperative Control - Theory and Applications"
      },
      {
        "paperId": "5b6783cda05aa2c5e2f4684ced771001cb9a7648",
        "title": "Performance analysis of multirobot Cooperative localization"
      },
      {
        "paperId": "0becaea40a9e998165f6e423ec2a94fbfe2f0acc",
        "title": "Time&#8211;Optimal Path Planning of Moving Sensors for Parameter Estimation of Distributed Systems"
      },
      {
        "paperId": "ce81a0b905dcfc206d5122dd93785be3d1f40bc7",
        "title": "Coordinated multi-robot exploration"
      },
      {
        "paperId": "e82eb1a61d657e25f127fad800b6b10c27ee2e91",
        "title": "ON THE RATE OF SPREAD FOR SOME REACTION-DIFFUSION MODELS OF FOREST FIRE PROPAGATION"
      },
      {
        "paperId": "0bfd174e2823c396cc627faee2d286ee2e5870ff",
        "title": "Optimal measurement methods for distributed parameter system identification"
      },
      {
        "paperId": null,
        "title": "ICML\u201915 Front"
      },
      {
        "paperId": "e3fc5b5627af62ee6981a02090cf6bae368202d7",
        "title": "Stable-Baselines3: Reliable Reinforcement Learning Implementations"
      },
      {
        "paperId": "f515d0aee8249b79279962df3c7561d17f383c67",
        "title": "Deep Reinforcement Learning based Path-Planning for Multi-Agent Systems in Advection-Diffusion Field Reconstruction Tasks"
      },
      {
        "paperId": "7ff09f598f3fa3c171e791a741de2802c1be0253",
        "title": "Collaborative Multi-Robot Search and Rescue: Planning, Coordination, Perception, and Active Vision"
      },
      {
        "paperId": null,
        "title": "controls-computational a moving vehicle"
      },
      {
        "paperId": "d45eaee8b2e047306329e5dbfc954e6dd318ca1e",
        "title": "ROS: an open-source Robot Operating System"
      },
      {
        "paperId": null,
        "title": "Kalman and extended kalman filters: concept, derivation and properties"
      },
      {
        "paperId": null,
        "title": "parameterestimationofdistributedsystems"
      },
      {
        "paperId": null,
        "title": "YDLIDAR-G4-Datasheet (2024"
      }
    ],
    "cited_by": [
      {
        "paperId": "acd134a857271de79c8619e9f1785d6d14a7a7eb",
        "title": "MAST: Multi-Agent Spatial Transformer for Learning to Collaborate"
      },
      {
        "paperId": "97cb43ef32f7d49623268427b8b00d1fdcf07abc",
        "title": "VAE-Informed Patrolling for Online Environmental Monitoring with Heterogeneous ASV-UAV Teams"
      },
      {
        "paperId": "9872536a8ba348c1b8dec7e7ed0e2189f19b89ea",
        "title": "Improving Fault Detection in Sparse Underwater Sensor Networks with K-MeansBased Distributed Algorithms"
      }
    ],
    "score": 3.0
  },
  {
    "id": "9ac3f4e74e9837cb47f43211cf143d4c7115e7f1",
    "title": "``Give Me an Example Like This'': Episodic Active Reinforcement Learning from Demonstrations",
    "authors": [
      "Muhan Hou",
      "Koen V. Hindriks",
      "Guszti Eiben",
      "Kim Baraka"
    ],
    "year": 2024,
    "citationCount": 3,
    "abstract": "Reinforcement Learning (RL) has achieved great success in sequential decision-making problems but often requires extensive agent-environment interactions. To improve sample efficiency, methods like Reinforcement Learning from Expert Demonstrations (RLED) incorporate external expert demonstrations to aid agent exploration during the learning process. However, these demonstrations, typically collected from human users, are costly and thus often limited in quantity. Therefore, how to select the optimal set of human demonstrations that most effectively aids learning becomes a critical concern. This paper introduces EARLY (Episodic Active Learning from demonstration querY), an algorithm designed to enable a learning agent to generate optimized queries for expert demonstrations in a trajectory-based feature space. EARLY employs a trajectory-level estimate of uncertainty in the agent\u2019s current policy to determine the optimal timing and content for feature-based queries. By querying episodic demonstrations instead of isolated state-action pairs, EARLY enhances the human teaching experience and achieves better learning performance. We validate the effectiveness of our method across three simulated navigation tasks of increasing difficulty. Results indicate that our method achieves expert-level performance in all three tasks, converging over 50% faster than other four baseline methods when demonstrations are generated by simulated oracle policies. A follow-up pilot user study (N = 18) further supports that our method maintains significantly better convergence with human expert demonstrators, while also providing a better user experience in terms of perceived task load and requiring significantly less human time.",
    "url": "https://www.semanticscholar.org/paper/9ac3f4e74e9837cb47f43211cf143d4c7115e7f1",
    "pdf_url": "https://arxiv.org/pdf/2406.03069.pdf",
    "venue": "International Conference on Human-Agent Interaction",
    "publicationDate": "2024-06-05",
    "externalIds": {
      "DBLP": "journals/corr/abs-2406-03069",
      "ArXiv": "2406.03069",
      "DOI": "10.1145/3687272.3688298",
      "CorpusId": 270258129
    },
    "references": [
      {
        "paperId": "d0b220b4f027894900fa5be09eb65a1ff3cb3cb0",
        "title": "Shaping Imbalance into Balance: Active Robot Guidance of Human Teachers for Better Learning from Demonstrations"
      },
      {
        "paperId": "c51f3fdf341dbbe7ca3af0327b2bf1e6672c0e87",
        "title": "imitation: Clean Imitation Learning Implementations"
      },
      {
        "paperId": "101f42c32c9a52a87c57c93d16d4e72252c148eb",
        "title": "Back to Reality for Imitation Learning"
      },
      {
        "paperId": "274e32e992bb5ce70feb1a474ea9a4cc7886dad8",
        "title": "Model-free reinforcement learning from expert demonstrations: a survey"
      },
      {
        "paperId": "68eebf01bfbfb7d37937d42c77a966df0cd2dd89",
        "title": "Improved Deep Reinforcement Learning with Expert Demonstrations for Urban Autonomous Driving"
      },
      {
        "paperId": "f5275f5eb6569ddb5ba9a959ede09875d56e3bac",
        "title": "Parrot: Data-Driven Behavioral Priors for Reinforcement Learning"
      },
      {
        "paperId": "0272b14dd471fe7b81df703af1b71d7600b77215",
        "title": "Accelerating Online Reinforcement Learning with Offline Datasets"
      },
      {
        "paperId": "44323ec1ea9c1432966ea0d5c2ef78f4b8e2a619",
        "title": "A Framework for Learning From Demonstration With Minimal Human Effort"
      },
      {
        "paperId": "2818d77e398341f5ef8a8aa55f85220b392f7ec2",
        "title": "Urban Driving with Conditional Imitation Learning"
      },
      {
        "paperId": "73b5c8b2e4d291a556c63c29b2033b63c0eeb10a",
        "title": "Active deep Q-learning with demonstration"
      },
      {
        "paperId": "c8482a7b09a13d08bed1da2348304125ccb1a30a",
        "title": "HG-DAgger: Interactive Imitation Learning with Human Experts"
      },
      {
        "paperId": "c979efe1f0a8b0b343ea332368e5b51dc153c522",
        "title": "Policy Optimization with Demonstrations"
      },
      {
        "paperId": "811df72e210e20de99719539505da54762a11c6d",
        "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor"
      },
      {
        "paperId": "c28ec2a40a2c77e20d64cf1c85dc931106df8e83",
        "title": "Overcoming Exploration in Reinforcement Learning with Demonstrations"
      },
      {
        "paperId": "585182e93bc33daacfc7cf52744eae569e180c52",
        "title": "Improving Reinforcement Learning with Confidence-Based Demonstrations"
      },
      {
        "paperId": "1bead9000a719cb258bac7320228055aee650d2c",
        "title": "Leveraging Demonstrations for Deep Reinforcement Learning on Robotics Problems with Sparse Rewards"
      },
      {
        "paperId": "e3b0ea7209731c47b582215c6c67f9c691ad9863",
        "title": "Deep Q-learning From Demonstrations"
      },
      {
        "paperId": "4ab53de69372ec2cd2d90c126b6a100165dc8ed1",
        "title": "Generative Adversarial Imitation Learning"
      },
      {
        "paperId": "dc0adbb27267f1ae88dc5c9982ec7acb5124b95f",
        "title": "Exploration from Demonstration for Interactive Reinforcement Learning"
      },
      {
        "paperId": "c6170fa90d3b2efede5a2e1660cb23e1c824f2ca",
        "title": "Prioritized Experience Replay"
      },
      {
        "paperId": "e40a0969aac73d50485c05de7f1c0ab081d77028",
        "title": "Interactive Policy Learning through Confidence-Based Autonomy"
      },
      {
        "paperId": "2319a491378867c7049b3da055c5df60e1671158",
        "title": "Playing Atari with Deep Reinforcement Learning"
      },
      {
        "paperId": "e4e589a03517aef45106b042a5fd594d9ef54c68",
        "title": "Smart exploration in reinforcement learning using absolute temporal difference errors"
      },
      {
        "paperId": "3d1335ff5e8c2ce9dba143de155e2eccf5dcda5a",
        "title": "Active learning from demonstration for robust autonomous navigation"
      },
      {
        "paperId": "aaab75af076bbdbd4d9c5cd7a2c332b03f9b58d4",
        "title": "Integrating reinforcement learning with human demonstrations of varying ability"
      },
      {
        "paperId": "9d7065858ffed116277a15b2e3d23718e3d22754",
        "title": "Incremental Sampling-based Algorithms for Optimal Motion Planning"
      },
      {
        "paperId": "2dd9d0a2047320f44972e77c612aa491635b19db",
        "title": "Active Reinforcement Learning from Demonstration in Continuous Action Spaces"
      },
      {
        "paperId": "f0ea414d9ac87233c421b48cdf9c356c772b844b",
        "title": "Policies for Active Learning from Demonstration"
      },
      {
        "paperId": "7786bc6c25ba38ff0135f1bdad192f6b3c4ad0b3",
        "title": "ALVINN, an autonomous land vehicle in a neural network"
      },
      {
        "paperId": null,
        "title": "Active lmitation learning: formal and practical reductions to IID learning"
      },
      {
        "paperId": "56acd58c15e00e0dabf2c03fb180ae98d2d60880",
        "title": "Active Learning with Mixed Query Types in Learning from Demonstration"
      }
    ],
    "cited_by": [
      {
        "paperId": "8453d2687f06f41a99d310845a9b47c917436aec",
        "title": "Robot Policy Transfer with Online Demonstrations: An Active Reinforcement Learning Approach"
      },
      {
        "paperId": "462396a1cbef6a3b71fed11fd0e2da7c6d877984",
        "title": "Active Robot Curriculum Learning from Online Human Demonstrations"
      },
      {
        "paperId": "70743db92db7dcc0870552edc02e6893a3dd9555",
        "title": "Human-Interactive Robot Learning: Definition, Challenges, and Recommendations"
      }
    ],
    "score": 3.0
  }
]