{
  "nodes": [
    {
      "id": "c28ec2a40a2c77e20d64cf1c85dc931106df8e83",
      "title": "Overcoming Exploration in Reinforcement Learning with Demonstrations",
      "abstract": "Exploration in environments with sparse rewards has been a persistent problem in reinforcement learning (RL). Many tasks are natural to specify with a sparse reward, and manually shaping a reward function can result in suboptimal performance. However, finding a non-zero reward is exponentially more difficult with increasing task horizon or action dimensionality. This puts many real-world tasks out of practical reach of RL methods. In this work, we use demonstrations to overcome the exploration problem and successfully learn to perform long-horizon, multi-step robotics tasks with continuous control such as stacking blocks with a robot arm. Our method, which builds on top of Deep Deterministic Policy Gradients and Hindsight Experience Replay, provides an order of magnitude of speedup over RL on simulated robotics tasks. It is simple to implement and makes only the additional assumption that we can collect a small set of demonstrations. Furthermore, our method is able to solve tasks not solvable by either RL or behavior cloning alone, and often ends up outperforming the demonstrator policy.",
      "authors": [
        "Ashvin Nair",
        "Bob McGrew",
        "Marcin Andrychowicz",
        "Wojciech Zaremba",
        "P. Abbeel"
      ],
      "year": 2017,
      "citation_count": 814,
      "layer": 1,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/c28ec2a40a2c77e20d64cf1c85dc931106df8e83",
      "pdf_link": "",
      "venue": "IEEE International Conference on Robotics and Automation",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "0fcb2034e31a2bc2f12a2b1363d0d77baf445fdf",
      "title": "#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning",
      "abstract": "Count-based exploration algorithms are known to perform near-optimally when used in conjunction with tabular reinforcement learning (RL) methods for solving small discrete Markov decision processes (MDPs). It is generally thought that count-based methods cannot be applied in high-dimensional state spaces, since most states will only occur once. Recent deep RL exploration strategies are able to deal with high-dimensional continuous state spaces through complex heuristics, often relying on optimism in the face of uncertainty or intrinsic motivation. In this work, we describe a surprising finding: a simple generalization of the classic count-based approach can reach near state-of-the-art performance on various high-dimensional and/or continuous deep RL benchmarks. States are mapped to hash codes, which allows to count their occurrences with a hash table. These counts are then used to compute a reward bonus according to the classic count-based exploration theory. We find that simple hash functions can achieve surprisingly good results on many challenging tasks. Furthermore, we show that a domain-dependent learned hash code may further improve these results. Detailed analysis reveals important aspects of a good hash function: 1) having appropriate granularity and 2) encoding information relevant to solving the MDP. This exploration strategy achieves near state-of-the-art performance on both continuous control tasks and Atari 2600 games, hence providing a simple yet powerful baseline for solving MDPs that require considerable exploration.",
      "authors": [
        "Haoran Tang",
        "Rein Houthooft",
        "Davis Foote",
        "Adam Stooke",
        "Xi Chen",
        "Yan Duan",
        "John Schulman",
        "F. Turck",
        "P. Abbeel"
      ],
      "year": 2016,
      "citation_count": 798,
      "layer": 1,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/0fcb2034e31a2bc2f12a2b1363d0d77baf445fdf",
      "pdf_link": "",
      "venue": "Neural Information Processing Systems",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "45f573f302dc7e77cbc5d1a74ccbac3564bbebc8",
      "title": "PEBBLE: Feedback-Efficient Interactive Reinforcement Learning via Relabeling Experience and Unsupervised Pre-training",
      "abstract": "Conveying complex objectives to reinforcement learning (RL) agents can often be difficult, involving meticulous design of reward functions that are sufficiently informative yet easy enough to provide. Human-in-the-loop RL methods allow practitioners to instead interactively teach agents through tailored feedback; however, such approaches have been challenging to scale since human feedback is very expensive. In this work, we aim to make this process more sample- and feedback-efficient. We present an off-policy, interactive RL algorithm that capitalizes on the strengths of both feedback and off-policy learning. Specifically, we learn a reward model by actively querying a teacher's preferences between two clips of behavior and use it to train an agent. To enable off-policy learning, we relabel all the agent's past experience when its reward model changes. We additionally show that pre-training our agents with unsupervised exploration substantially increases the mileage of its queries. We demonstrate that our approach is capable of learning tasks of higher complexity than previously considered by human-in-the-loop methods, including a variety of locomotion and robotic manipulation skills. We also show that our method is able to utilize real-time human feedback to effectively prevent reward exploitation and learn new behaviors that are difficult to specify with standard reward functions.",
      "authors": [
        "Kimin Lee",
        "Laura M. Smith",
        "P. Abbeel"
      ],
      "year": 2021,
      "citation_count": 316,
      "layer": 1,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/45f573f302dc7e77cbc5d1a74ccbac3564bbebc8",
      "pdf_link": "",
      "venue": "International Conference on Machine Learning",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "f8d8e192979ab5d8d593e76a0c4e2c7581778732",
      "title": "Voronoi-Based Multi-Robot Autonomous Exploration in Unknown Environments via Deep Reinforcement Learning",
      "abstract": "Autonomous exploration is an important application of multi-vehicle systems, where a team of networked robots are coordinated to explore an unknown environment collaboratively. This technique has earned significant research interest due to its usefulness in search and rescue, fault detection and monitoring, localization and mapping, etc. In this paper, a novel cooperative exploration strategy is proposed for multiple mobile robots, which reduces the overall task completion time and energy costs compared to conventional methods. To efficiently navigate the networked robots during the collaborative tasks, a hierarchical control architecture is designed which contains a high-level decision making layer and a low-level target tracking layer. The proposed cooperative exploration approach is developed using dynamic Voronoi partitions, which minimizes duplicated exploration areas by assigning different target locations to individual robots. To deal with sudden obstacles in the unknown environment, an integrated deep reinforcement learning based collision avoidance algorithm is then proposed, which enables the control policy to learn from human demonstration data and thus improve the learning speed and performance. Finally, simulation and experimental results are provided to demonstrate the effectiveness of the proposed scheme.",
      "authors": [
        "Junyan Hu",
        "Hanlin Niu",
        "J. Carrasco",
        "B. Lennox",
        "F. Arvin"
      ],
      "year": 2020,
      "citation_count": 291,
      "layer": 1,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/f8d8e192979ab5d8d593e76a0c4e2c7581778732",
      "pdf_link": "",
      "venue": "IEEE Transactions on Vehicular Technology",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "2470fcf0f89082de874ac9133ccb3a8667dd89a8",
      "title": "Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models",
      "abstract": "Achieving efficient and scalable exploration in complex domains poses a major challenge in reinforcement learning. While Bayesian and PAC-MDP approaches to the exploration problem offer strong formal guarantees, they are often impractical in higher dimensions due to their reliance on enumerating the state-action space. Hence, exploration in complex domains is often performed with simple epsilon-greedy methods. In this paper, we consider the challenging Atari games domain, which requires processing raw pixel inputs and delayed rewards. We evaluate several more sophisticated exploration strategies, including Thompson sampling and Boltzman exploration, and propose a new exploration method based on assigning exploration bonuses from a concurrently learned model of the system dynamics. By parameterizing our learned model with a neural network, we are able to develop a scalable and efficient approach to exploration bonuses that can be applied to tasks with complex, high-dimensional state spaces. In the Atari domain, our method provides the most consistent improvement across a range of games that pose a major challenge for prior methods. In addition to raw game-scores, we also develop an AUC-100 metric for the Atari Learning domain to evaluate the impact of exploration on this benchmark.",
      "authors": [
        "Bradly C. Stadie",
        "S. Levine",
        "P. Abbeel"
      ],
      "year": 2015,
      "citation_count": 512,
      "layer": 1,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/2470fcf0f89082de874ac9133ccb3a8667dd89a8",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "68c108795deef06fa929d1f6e96b75dbf7ce8531",
      "title": "Meta-Reinforcement Learning of Structured Exploration Strategies",
      "abstract": "Exploration is a fundamental challenge in reinforcement learning (RL). Many of the current exploration methods for deep RL use task-agnostic objectives, such as information gain or bonuses based on state visitation. However, many practical applications of RL involve learning more than a single task, and prior tasks can be used to inform how exploration should be performed in new tasks. In this work, we explore how prior tasks can inform an agent about how to explore effectively in new situations. We introduce a novel gradient-based fast adaptation algorithm -- model agnostic exploration with structured noise (MAESN) -- to learn exploration strategies from prior experience. The prior experience is used both to initialize a policy and to acquire a latent exploration space that can inject structured stochasticity into a policy, producing exploration strategies that are informed by prior knowledge and are more effective than random action-space noise. We show that MAESN is more effective at learning exploration strategies when compared to prior meta-RL methods, RL without learned exploration strategies, and task-agnostic exploration methods. We evaluate our method on a variety of simulated tasks: locomotion with a wheeled robot, locomotion with a quadrupedal walker, and object manipulation.",
      "authors": [
        "Abhishek Gupta",
        "Russell Mendonca",
        "Yuxuan Liu",
        "P. Abbeel",
        "S. Levine"
      ],
      "year": 2018,
      "citation_count": 357,
      "layer": 1,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/68c108795deef06fa929d1f6e96b75dbf7ce8531",
      "pdf_link": "",
      "venue": "Neural Information Processing Systems",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "431dc05ac25510de6264084434254cca877f9ab3",
      "title": "Recovery RL: Safe Reinforcement Learning With Learned Recovery Zones",
      "abstract": "Safety remains a central obstacle preventing widespread use of RL in the real world: learning new tasks in uncertain environments requires extensive exploration, but safety requires limiting exploration. We propose Recovery RL, an algorithm which navigates this tradeoff by (1) leveraging offline data to learn about constraint violating zones before policy learning and (2) separating the goals of improving task performance and constraint satisfaction across two policies: a task policy that only optimizes the task reward and a recovery policy that guides the agent to safety when constraint violation is likely. We evaluate Recovery RL on 6 simulation domains, including two contact-rich manipulation tasks and an image-based navigation task, and an image-based obstacle avoidance task on a physical robot. We compare Recovery RL to 5 prior safe RL methods which jointly optimize for task performance and safety via constrained optimization or reward shaping and find that Recovery RL outperforms the next best prior method across all domains. Results suggest that Recovery RL trades off constraint violations and task successes 2–20 times more efficiently in simulation domains and 3 times more efficiently in physical experiments. See https://tinyurl.com/rl-recovery for videos and supplementary material.",
      "authors": [
        "Brijen Thananjeyan",
        "A. Balakrishna",
        "Suraj Nair",
        "Michael Luo",
        "K. Srinivasan",
        "M. Hwang",
        "Joseph E. Gonzalez",
        "Julian Ibarz",
        "Chelsea Finn",
        "Ken Goldberg"
      ],
      "year": 2020,
      "citation_count": 247,
      "layer": 1,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/431dc05ac25510de6264084434254cca877f9ab3",
      "pdf_link": "",
      "venue": "IEEE Robotics and Automation Letters",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "2c23085488337c4c1b5673b8d0f4ac95bda73529",
      "title": "Uncertainty Weighted Actor-Critic for Offline Reinforcement Learning",
      "abstract": "Offline Reinforcement Learning promises to learn effective policies from previously-collected, static datasets without the need for exploration. However, existing Q-learning and actor-critic based off-policy RL algorithms fail when bootstrapping from out-of-distribution (OOD) actions or states. We hypothesize that a key missing ingredient from the existing methods is a proper treatment of uncertainty in the offline setting. We propose Uncertainty Weighted Actor-Critic (UWAC), an algorithm that detects OOD state-action pairs and down-weights their contribution in the training objectives accordingly. Implementation-wise, we adopt a practical and effective dropout-based uncertainty estimation method that introduces very little overhead over existing RL algorithms. Empirically, we observe that UWAC substantially improves model stability during training. In addition, UWAC out-performs existing offline RL methods on a variety of competitive tasks, and achieves significant performance gains over the state-of-the-art baseline on datasets with sparse demonstrations collected from human experts.",
      "authors": [
        "Yue Wu",
        "Shuangfei Zhai",
        "Nitish Srivastava",
        "J. Susskind",
        "Jian Zhang",
        "R. Salakhutdinov",
        "Hanlin Goh"
      ],
      "year": 2021,
      "citation_count": 196,
      "layer": 1,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/2c23085488337c4c1b5673b8d0f4ac95bda73529",
      "pdf_link": "",
      "venue": "International Conference on Machine Learning",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "2064020586d5832b55f80a7dffea1fd90a5d94dd",
      "title": "Improving Exploration in Evolution Strategies for Deep Reinforcement Learning via a Population of Novelty-Seeking Agents",
      "abstract": "Evolution strategies (ES) are a family of black-box optimization algorithms able to train deep neural networks roughly as well as Q-learning and policy gradient methods on challenging deep reinforcement learning (RL) problems, but are much faster (e.g. hours vs. days) because they parallelize better. However, many RL problems require directed exploration because they have reward functions that are sparse or deceptive (i.e. contain local optima), and it is unknown how to encourage such exploration with ES. Here we show that algorithms that have been invented to promote directed exploration in small-scale evolved neural networks via populations of exploring agents, specifically novelty search (NS) and quality diversity (QD) algorithms, can be hybridized with ES to improve its performance on sparse or deceptive deep RL tasks, while retaining scalability. Our experiments confirm that the resultant new algorithms, NS-ES and two QD algorithms, NSR-ES and NSRA-ES, avoid local optima encountered by ES to achieve higher performance on Atari and simulated robots learning to walk around a deceptive trap. This paper thus introduces a family of fast, scalable algorithms for reinforcement learning that are capable of directed exploration. It also adds this new family of exploration algorithms to the RL toolbox and raises the interesting possibility that analogous algorithms with multiple simultaneous paths of exploration might also combine well with existing RL algorithms outside ES.",
      "authors": [
        "Edoardo Conti",
        "Vashisht Madhavan",
        "F. Such",
        "J. Lehman",
        "Kenneth O. Stanley",
        "J. Clune"
      ],
      "year": 2017,
      "citation_count": 351,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/2064020586d5832b55f80a7dffea1fd90a5d94dd",
      "pdf_link": "",
      "venue": "Neural Information Processing Systems",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "1c35807e1a4c24e2013fa0a090cee9cc4716a5f5",
      "title": "Reinforcement Learning with Action-Free Pre-Training from Videos",
      "abstract": "Recent unsupervised pre-training methods have shown to be effective on language and vision domains by learning useful representations for multiple downstream tasks. In this paper, we investigate if such unsupervised pre-training methods can also be effective for vision-based reinforcement learning (RL). To this end, we introduce a framework that learns representations useful for understanding the dynamics via generative pre-training on videos. Our framework consists of two phases: we pre-train an action-free latent video prediction model, and then utilize the pre-trained representations for efficiently learning action-conditional world models on unseen environments. To incorporate additional action inputs during fine-tuning, we introduce a new architecture that stacks an action-conditional latent prediction model on top of the pre-trained action-free prediction model. Moreover, for better exploration, we propose a video-based intrinsic bonus that leverages pre-trained representations. We demonstrate that our framework significantly improves both final performances and sample-efficiency of vision-based RL in a variety of manipulation and locomotion tasks. Code is available at https://github.com/younggyoseo/apv.",
      "authors": [
        "Younggyo Seo",
        "Kimin Lee",
        "Stephen James",
        "P. Abbeel"
      ],
      "year": 2022,
      "citation_count": 129,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/1c35807e1a4c24e2013fa0a090cee9cc4716a5f5",
      "pdf_link": "",
      "venue": "International Conference on Machine Learning",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "f3a63a840185fa8c5e0db4bbe12afa7d3de7d029",
      "title": "Jump-Start Reinforcement Learning",
      "abstract": "Reinforcement learning (RL) provides a theoretical framework for continuously improving an agent's behavior via trial and error. However, efficiently learning policies from scratch can be very difficult, particularly for tasks with exploration challenges. In such settings, it might be desirable to initialize RL with an existing policy, offline data, or demonstrations. However, naively performing such initialization in RL often works poorly, especially for value-based methods. In this paper, we present a meta algorithm that can use offline data, demonstrations, or a pre-existing policy to initialize an RL policy, and is compatible with any RL approach. In particular, we propose Jump-Start Reinforcement Learning (JSRL), an algorithm that employs two policies to solve tasks: a guide-policy, and an exploration-policy. By using the guide-policy to form a curriculum of starting states for the exploration-policy, we are able to efficiently improve performance on a set of simulated robotic tasks. We show via experiments that JSRL is able to significantly outperform existing imitation and reinforcement learning algorithms, particularly in the small-data regime. In addition, we provide an upper bound on the sample complexity of JSRL and show that with the help of a guide-policy, one can improve the sample complexity for non-optimism exploration methods from exponential in horizon to polynomial.",
      "authors": [
        "Ikechukwu Uchendu",
        "Ted Xiao",
        "Yao Lu",
        "Banghua Zhu",
        "Mengyuan Yan",
        "J. Simón",
        "Matthew Bennice",
        "Chuyuan Fu",
        "Cong Ma",
        "Jiantao Jiao",
        "S. Levine",
        "Karol Hausman"
      ],
      "year": 2022,
      "citation_count": 127,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/f3a63a840185fa8c5e0db4bbe12afa7d3de7d029",
      "pdf_link": "",
      "venue": "International Conference on Machine Learning",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "52a6657cf1cb3cc847695a386bd65b5eea34bc13",
      "title": "Deep Reinforcement Learning-Based Automatic Exploration for Navigation in Unknown Environment",
      "abstract": "This paper investigates the automatic exploration problem under the unknown environment, which is the key point of applying the robotic system to some social tasks. The solution to this problem via stacking decision rules is impossible to cover various environments and sensor properties. Learning-based control methods are adaptive for these scenarios. However, these methods are damaged by low learning efficiency and awkward transferability from simulation to reality. In this paper, we construct a general exploration framework via decomposing the exploration process into the decision, planning, and mapping modules, which increases the modularity of the robotic system. Based on this framework, we propose a deep reinforcement learning-based decision algorithm that uses a deep neural network to learning exploration strategy from the partial map. The results show that this proposed algorithm has better learning efficiency and adaptability for unknown environments. In addition, we conduct the experiments on the physical robot, and the results suggest that the learned policy can be well transferred from simulation to the real robot.",
      "authors": [
        "Haoran Li",
        "Qichao Zhang",
        "Dongbin Zhao"
      ],
      "year": 2020,
      "citation_count": 206,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/52a6657cf1cb3cc847695a386bd65b5eea34bc13",
      "pdf_link": "",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "12075ea34f5fbe32ec5582786761ab34d401209b",
      "title": "Exploration in Deep Reinforcement Learning: From Single-Agent to Multiagent Domain",
      "abstract": "Deep reinforcement learning (DRL) and deep multiagent reinforcement learning (MARL) have achieved significant success across a wide range of domains, including game artificial intelligence (AI), autonomous vehicles, and robotics. However, DRL and deep MARL agents are widely known to be sample inefficient that millions of interactions are usually needed even for relatively simple problem settings, thus preventing the wide application and deployment in real-industry scenarios. One bottleneck challenge behind is the well-known exploration problem, i.e., how efficiently exploring the environment and collecting informative experiences that could benefit policy learning toward the optimal ones. This problem becomes more challenging in complex environments with sparse rewards, noisy distractions, long horizons, and nonstationary co-learners. In this article, we conduct a comprehensive survey on existing exploration methods for both single-agent RL and multiagent RL. We start the survey by identifying several key challenges to efficient exploration. Then, we provide a systematic survey of existing approaches by classifying them into two major categories: uncertainty-oriented exploration and intrinsic motivation-oriented exploration. Beyond the above two main branches, we also include other notable exploration methods with different ideas and techniques. In addition to algorithmic analysis, we provide a comprehensive and unified empirical comparison of different exploration methods for DRL on a set of commonly used benchmarks. According to our algorithmic and empirical investigation, we finally summarize the open problems of exploration in DRL and deep MARL and point out a few future directions.",
      "authors": [
        "Tianpei Yang",
        "Hongyao Tang",
        "Chenjia Bai",
        "Jinyi Liu",
        "Jianye Hao",
        "Zhaopeng Meng",
        "Peng Liu",
        "Zhen Wang"
      ],
      "year": 2021,
      "citation_count": 120,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/12075ea34f5fbe32ec5582786761ab34d401209b",
      "pdf_link": "",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "dc05886db1e6f17f4489d867477b38fe13e31783",
      "title": "Network Randomization: A Simple Technique for Generalization in Deep Reinforcement Learning",
      "abstract": "Deep reinforcement learning (RL) agents often fail to generalize to unseen environments (yet semantically similar to trained agents), particularly when they are trained on high-dimensional state spaces, such as images. In this paper, we propose a simple technique to improve a generalization ability of deep RL agents by introducing a randomized (convolutional) neural network that randomly perturbs input observations. It enables trained agents to adapt to new domains by learning robust features invariant across varied and randomized environments. Furthermore, we consider an inference method based on the Monte Carlo approximation to reduce the variance induced by this randomization. We demonstrate the superiority of our method across 2D CoinRun, 3D DeepMind Lab exploration and 3D robotics control tasks: it significantly outperforms various regularization and data augmentation methods for the same purpose.",
      "authors": [
        "Kimin Lee",
        "Kibok Lee",
        "Jinwoo Shin",
        "Honglak Lee"
      ],
      "year": 2019,
      "citation_count": 178,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/dc05886db1e6f17f4489d867477b38fe13e31783",
      "pdf_link": "",
      "venue": "International Conference on Learning Representations",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "6bf9b58b8f418fb2922762550fb78bb22c83c0f8",
      "title": "Variational Policy Gradient Method for Reinforcement Learning with General Utilities",
      "abstract": "In recent years, reinforcement learning (RL) systems with general goals beyond a cumulative sum of rewards have gained traction, such as in constrained problems, exploration, and acting upon prior experiences. In this paper, we consider policy optimization in Markov Decision Problems, where the objective is a general concave utility function of the state-action occupancy measure, which subsumes several of the aforementioned examples as special cases. Such generality invalidates the Bellman equation. As this means that dynamic programming no longer works, we focus on direct policy search. Analogously to the Policy Gradient Theorem \\cite{sutton2000policy} available for RL with cumulative rewards, we derive a new Variational Policy Gradient Theorem for RL with general utilities, which establishes that the parametrized policy gradient may be obtained as the solution of a stochastic saddle point problem involving the Fenchel dual of the utility function. We develop a variational Monte Carlo gradient estimation algorithm to compute the policy gradient based on sample paths. We prove that the variational policy gradient scheme converges globally to the optimal policy for the general objective, though the optimization problem is nonconvex. We also establish its rate of convergence of the order $O(1/t)$ by exploiting the hidden convexity of the problem, and proves that it converges exponentially when the problem admits hidden strong convexity. Our analysis applies to the standard RL problem with cumulative rewards as a special case, in which case our result improves the available convergence rate.",
      "authors": [
        "Junyu Zhang",
        "Alec Koppel",
        "A. S. Bedi",
        "Csaba Szepesvari",
        "Mengdi Wang"
      ],
      "year": 2020,
      "citation_count": 146,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/6bf9b58b8f418fb2922762550fb78bb22c83c0f8",
      "pdf_link": "",
      "venue": "Neural Information Processing Systems",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "cc9f2fd320a279741403c4bfbeb91179803c428c",
      "title": "Reward Uncertainty for Exploration in Preference-based Reinforcement Learning",
      "abstract": "Conveying complex objectives to reinforcement learning (RL) agents often requires meticulous reward engineering. Preference-based RL methods are able to learn a more flexible reward model based on human preferences by actively incorporating human feedback, i.e. teacher's preferences between two clips of behaviors. However, poor feedback-efficiency still remains a problem in current preference-based RL algorithms, as tailored human feedback is very expensive. To handle this issue, previous methods have mainly focused on improving query selection and policy initialization. At the same time, recent exploration methods have proven to be a recipe for improving sample-efficiency in RL. We present an exploration method specifically for preference-based RL algorithms. Our main idea is to design an intrinsic reward by measuring the novelty based on learned reward. Specifically, we utilize disagreement across ensemble of learned reward models. Our intuition is that disagreement in learned reward model reflects uncertainty in tailored human feedback and could be useful for exploration. Our experiments show that exploration bonus from uncertainty in learned reward improves both feedback- and sample-efficiency of preference-based RL algorithms on complex robot manipulation tasks from MetaWorld benchmarks, compared with other existing exploration methods that measure the novelty of state visitation.",
      "authors": [
        "Xi-Xi Liang",
        "Katherine Shu",
        "Kimin Lee",
        "P. Abbeel"
      ],
      "year": 2022,
      "citation_count": 59,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/cc9f2fd320a279741403c4bfbeb91179803c428c",
      "pdf_link": "",
      "venue": "International Conference on Learning Representations",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "3c3093f5f8e9601637612fcfb8f160f116fa30e4",
      "title": "Diversity-Driven Exploration Strategy for Deep Reinforcement Learning",
      "abstract": "Efficient exploration remains a challenging research problem in reinforcement learning, especially when an environment contains large state spaces, deceptive local optima, or sparse rewards. To tackle this problem, we present a diversity-driven approach for exploration, which can be easily combined with both off- and on-policy reinforcement learning algorithms. We show that by simply adding a distance measure to the loss function, the proposed methodology significantly enhances an agent's exploratory behaviors, and thus preventing the policy from being trapped in local optima. We further propose an adaptive scaling method for stabilizing the learning process. Our experimental results in Atari 2600 show that our method outperforms baseline approaches in several tasks in terms of mean scores and exploration efficiency.",
      "authors": [
        "Zhang-Wei Hong",
        "Tzu-Yun Shann",
        "Shih-Yang Su",
        "Yi-Hsiang Chang",
        "Chun-Yi Lee"
      ],
      "year": 2018,
      "citation_count": 128,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/3c3093f5f8e9601637612fcfb8f160f116fa30e4",
      "pdf_link": "",
      "venue": "Neural Information Processing Systems",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "b2004f4f19bc6ccae8e4afc554f39142870100f5",
      "title": "MoDem: Accelerating Visual Model-Based Reinforcement Learning with Demonstrations",
      "abstract": "Poor sample efficiency continues to be the primary challenge for deployment of deep Reinforcement Learning (RL) algorithms for real-world applications, and in particular for visuo-motor control. Model-based RL has the potential to be highly sample efficient by concurrently learning a world model and using synthetic rollouts for planning and policy improvement. However, in practice, sample-efficient learning with model-based RL is bottlenecked by the exploration challenge. In this work, we find that leveraging just a handful of demonstrations can dramatically improve the sample-efficiency of model-based RL. Simply appending demonstrations to the interaction dataset, however, does not suffice. We identify key ingredients for leveraging demonstrations in model learning -- policy pretraining, targeted exploration, and oversampling of demonstration data -- which forms the three phases of our model-based RL framework. We empirically study three complex visuo-motor control domains and find that our method is 150%-250% more successful in completing sparse reward tasks compared to prior approaches in the low data regime (100K interaction steps, 5 demonstrations). Code and videos are available at: https://nicklashansen.github.io/modemrl",
      "authors": [
        "Nicklas Hansen",
        "Yixin Lin",
        "H. Su",
        "Xiaolong Wang",
        "Vikash Kumar",
        "A. Rajeswaran"
      ],
      "year": 2022,
      "citation_count": 54,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/b2004f4f19bc6ccae8e4afc554f39142870100f5",
      "pdf_link": "",
      "venue": "International Conference on Learning Representations",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "61f371768cdc093828f432660e22f7a17f22e2af",
      "title": "Offline Meta-Reinforcement Learning with Online Self-Supervision",
      "abstract": "Meta-reinforcement learning (RL) methods can meta-train policies that adapt to new tasks with orders of magnitude less data than standard RL, but meta-training itself is costly and time-consuming. If we can meta-train on offline data, then we can reuse the same static dataset, labeled once with rewards for different tasks, to meta-train policies that adapt to a variety of new tasks at meta-test time. Although this capability would make meta-RL a practical tool for real-world use, offline meta-RL presents additional challenges beyond online meta-RL or standard offline RL settings. Meta-RL learns an exploration strategy that collects data for adapting, and also meta-trains a policy that quickly adapts to data from a new task. Since this policy was meta-trained on a fixed, offline dataset, it might behave unpredictably when adapting to data collected by the learned exploration strategy, which differs systematically from the offline data and thus induces distributional shift. We propose a hybrid offline meta-RL algorithm, which uses offline data with rewards to meta-train an adaptive policy, and then collects additional unsupervised online data, without any reward labels to bridge this distribution shift. By not requiring reward labels for online collection, this data can be much cheaper to collect. We compare our method to prior work on offline meta-RL on simulated robot locomotion and manipulation tasks and find that using additional unsupervised online data collection leads to a dramatic improvement in the adaptive capabilities of the meta-trained policies, matching the performance of fully online meta-RL on a range of challenging domains that require generalization to new tasks.",
      "authors": [
        "Vitchyr H. Pong",
        "Ashvin Nair",
        "Laura M. Smith",
        "Catherine Huang",
        "S. Levine"
      ],
      "year": 2021,
      "citation_count": 70,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/61f371768cdc093828f432660e22f7a17f22e2af",
      "pdf_link": "",
      "venue": "International Conference on Machine Learning",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "116fc10b798e3294b00e92f2b8053d0c89ad9182",
      "title": "A robot exploration strategy based on Q-learning network",
      "abstract": "",
      "authors": [
        "L. Tai",
        "Ming Liu"
      ],
      "year": 2016,
      "citation_count": 143,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/116fc10b798e3294b00e92f2b8053d0c89ad9182",
      "pdf_link": "",
      "venue": "International Conference on Real-time Computing and Robotics",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "0f810eb4777fd05317951ebaa7a3f5835ee84cf4",
      "title": "Count-Based Exploration in Feature Space for Reinforcement Learning",
      "abstract": "We introduce a new count-based optimistic exploration algorithm for Reinforcement Learning (RL) that is feasible in environments with high-dimensional state-action spaces. The success of RL algorithms in these domains depends crucially on generalisation from limited training experience. Function approximation techniques enable RL agents to generalise in order to estimate the value of unvisited states, but at present few methods enable generalisation regarding uncertainty. This has prevented the combination of scalable RL algorithms with efficient exploration strategies that drive the agent to reduce its uncertainty. We present a new method for computing a generalised state visit-count, which allows the agent to estimate the uncertainty associated with any state. Our \\phi-pseudocount achieves generalisation by exploiting same feature representation of the state space that is used for value function approximation. States that have less frequently observed features are deemed more uncertain. The \\phi-Exploration-Bonus algorithm rewards the agent for exploring in feature space rather than in the untransformed state space. The method is simpler and less computationally expensive than some previous proposals, and achieves near state-of-the-art results on high-dimensional RL benchmarks.",
      "authors": [
        "Jarryd Martin",
        "S. N. Sasikumar",
        "Tom Everitt",
        "Marcus Hutter"
      ],
      "year": 2017,
      "citation_count": 126,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/0f810eb4777fd05317951ebaa7a3f5835ee84cf4",
      "pdf_link": "",
      "venue": "International Joint Conference on Artificial Intelligence",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "535d184eadf47fa17ce4073b6e2f180783e85300",
      "title": "Curiosity-driven Exploration for Mapless Navigation with Deep Reinforcement Learning",
      "abstract": "This paper investigates exploration strategies of Deep Reinforcement Learning (DRL) methods to learn navigation policies for mobile robots. In particular, we augment the normal external reward for training DRL algorithms with intrinsic reward signals measured by curiosity. We test our approach in a mapless navigation setting, where the autonomous agent is required to navigate without the occupancy map of the environment, to targets whose relative locations can be easily acquired through low-cost solutions (e.g., visible light localization, Wi-Fi signal localization). We validate that the intrinsic motivation is crucial for improving DRL performance in tasks with challenging exploration requirements. Our experimental results show that our proposed method is able to more effectively learn navigation policies, and has better generalization capabilities in previously unseen environments. A video of our experimental results can be found at this https URL",
      "authors": [
        "Oleksii Zhelo",
        "Jingwei Zhang",
        "L. Tai",
        "Ming Liu",
        "Wolfram Burgard"
      ],
      "year": 2018,
      "citation_count": 108,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/535d184eadf47fa17ce4073b6e2f180783e85300",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "0d82360a4da311a277607db355dda3f196e8eb3d",
      "title": "Deep Interactive Reinforcement Learning for Path Following of Autonomous Underwater Vehicle",
      "abstract": "Autonomous underwater vehicle (AUV) plays an increasingly important role in ocean exploration. Existing AUVs are usually not fully autonomous and generally limited to pre-planning or pre-programming tasks. Reinforcement learning (RL) and deep reinforcement learning have been introduced into the AUV design and research to improve its autonomy. However, these methods are still difficult to apply directly to the actual AUV system because of the sparse rewards and low learning efficiency. In this paper, we proposed a deep interactive reinforcement learning method for path following of AUV by combining the advantages of deep reinforcement learning and interactive RL. In addition, since the human trainer cannot provide human rewards for AUV when it is running in the ocean and AUV needs to adapt to a changing environment, we further propose a deep reinforcement learning method that learns from both human rewards and environmental rewards at the same time. We test our methods in two path following tasks—straight line and sinusoids curve following of AUV by simulating in the Gazebo platform. Our experimental results show that with our proposed deep interactive RL method, AUV can converge faster than a DQN learner from only environmental reward. Moreover, AUV learning with our deep RL from both human and environmental rewards can also achieve a similar or even better performance than that with deep interactive RL and can adapt to the actual environment by further learning from environmental rewards.",
      "authors": [
        "Qilei Zhang",
        "Jinying Lin",
        "Q. Sha",
        "Bo He",
        "Guangliang Li"
      ],
      "year": 2020,
      "citation_count": 77,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/0d82360a4da311a277607db355dda3f196e8eb3d",
      "pdf_link": "",
      "venue": "IEEE Access",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "f6b218453170edcbb51e49dd44ba2f83af53ef92",
      "title": "Distributional Reinforcement Learning for Efficient Exploration",
      "abstract": "In distributional reinforcement learning (RL), the estimated distribution of value function models both the parametric and intrinsic uncertainties. We propose a novel and efficient exploration method for deep RL that has two components. The first is a decaying schedule to suppress the intrinsic uncertainty. The second is an exploration bonus calculated from the upper quantiles of the learned distribution. In Atari 2600 games, our method outperforms QR-DQN in 12 out of 14 hard games (achieving 483 \\% average gain across 49 games in cumulative rewards over QR-DQN with a big win in Venture). We also compared our algorithm with QR-DQN in a challenging 3D driving simulator (CARLA). Results show that our algorithm achieves near-optimal safety rewards twice faster than QRDQN.",
      "authors": [
        "B. Mavrin",
        "Shangtong Zhang",
        "Hengshuai Yao",
        "Linglong Kong",
        "Kaiwen Wu",
        "Yaoliang Yu"
      ],
      "year": 2019,
      "citation_count": 92,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/f6b218453170edcbb51e49dd44ba2f83af53ef92",
      "pdf_link": "",
      "venue": "International Conference on Machine Learning",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "04615a9955bce148aa7ba29e864389c26e10523a",
      "title": "DEP-RL: Embodied Exploration for Reinforcement Learning in Overactuated and Musculoskeletal Systems",
      "abstract": "Muscle-actuated organisms are capable of learning an unparalleled diversity of dexterous movements despite their vast amount of muscles. Reinforcement learning (RL) on large musculoskeletal models, however, has not been able to show similar performance. We conjecture that ineffective exploration in large overactuated action spaces is a key problem. This is supported by the finding that common exploration noise strategies are inadequate in synthetic examples of overactuated systems. We identify differential extrinsic plasticity (DEP), a method from the domain of self-organization, as being able to induce state-space covering exploration within seconds of interaction. By integrating DEP into RL, we achieve fast learning of reaching and locomotion in musculoskeletal systems, outperforming current approaches in all considered tasks in sample efficiency and robustness.",
      "authors": [
        "Pierre Schumacher",
        "D. Haeufle",
        "Dieter Büchler",
        "S. Schmitt",
        "G. Martius"
      ],
      "year": 2022,
      "citation_count": 44,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/04615a9955bce148aa7ba29e864389c26e10523a",
      "pdf_link": "",
      "venue": "International Conference on Learning Representations",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "c64a99ec780d5e1c6ba242a82e72e96eb0c3ef8a",
      "title": "An Information-Theoretic Perspective on Intrinsic Motivation in Reinforcement Learning: A Survey",
      "abstract": "The reinforcement learning (RL) research area is very active, with an important number of new contributions, especially considering the emergent field of deep RL (DRL). However, a number of scientific and technical challenges still need to be resolved, among which we acknowledge the ability to abstract actions or the difficulty to explore the environment in sparse-reward settings which can be addressed by intrinsic motivation (IM). We propose to survey these research works through a new taxonomy based on information theory: we computationally revisit the notions of surprise, novelty, and skill-learning. This allows us to identify advantages and disadvantages of methods and exhibit current outlooks of research. Our analysis suggests that novelty and surprise can assist the building of a hierarchy of transferable skills which abstracts dynamics and makes the exploration process more robust.",
      "authors": [
        "A. Aubret",
        "L. Matignon",
        "S. Hassas"
      ],
      "year": 2022,
      "citation_count": 44,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/c64a99ec780d5e1c6ba242a82e72e96eb0c3ef8a",
      "pdf_link": "",
      "venue": "Entropy",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "399806e861a2ef960a81b37b593c2176a728c399",
      "title": "Offline Reinforcement Learning as Anti-Exploration",
      "abstract": "Offline Reinforcement Learning (RL) aims at learning an optimal control from a fixed dataset, without interactions with the system. An agent in this setting should avoid selecting actions whose consequences cannot be predicted from the data. This is the converse of exploration in RL, which favors such actions. We thus take inspiration from the literature on bonus-based exploration to design a new offline RL agent. The core idea is to subtract a prediction-based exploration bonus from the reward, instead of adding it for exploration. This allows the policy to stay close to the support of the dataset and practically extends some previous pessimism-based offline RL methods to a deep learning setting with arbitrary bonuses. We also connect this approach to a more common regularization of the learned policy towards the data. Instantiated with a bonus based on the prediction error of a variational autoencoder, we show that our simple agent is competitive with the state of the art on a set of continuous control locomotion and manipulation tasks.",
      "authors": [
        "Shideh Rezaeifar",
        "Robert Dadashi",
        "Nino Vieillard",
        "L'eonard Hussenot",
        "Olivier Bachem",
        "O. Pietquin",
        "M. Geist"
      ],
      "year": 2021,
      "citation_count": 57,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/399806e861a2ef960a81b37b593c2176a728c399",
      "pdf_link": "",
      "venue": "AAAI Conference on Artificial Intelligence",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "174be0bacee04d9eb13a698d484ab5ae441c1100",
      "title": "Effective deep Q-networks (EDQN) strategy for resource allocation based on optimized reinforcement learning algorithm",
      "abstract": "The healthcare industry has always been an early adopter of new technology and a big benefactor of it. The use of reinforcement learning in the healthcare system has repeatedly resulted in improved outcomes.. Many challenges exist concerning the architecture of the RL method, measurement metrics, and model choice. More significantly, the validation of RL in authentic clinical settings needs further work. This paper presents a new Effective Resource Allocation Strategy (ERAS) for the Fog environment, which is suitable for Healthcare applications. ERAS tries to achieve effective resource management in the Fog environment via real-time resource allocating as well as prediction algorithms. Comparing the ERAS with the state-of-the-art algorithms, ERAS achieved the minimum Makespan as compared to previous resource allocation algorithms, while maximizing the Average Resource Utilization (ARU) and the Load Balancing Level (LBL). For each application, we further compared and contrasted the architecture of the RL models and the assessment metrics. In critical care, RL has tremendous potential to enhance decision-making. This paper presents two main contributions, (i) Optimization of the RL hyperparameters using PSO, and (ii) Using the optimized RL for the resource allocation and load balancing in the fog environment. Because of its exploitation, exploration, and capacity to get rid of local minima, the PSO has a significant significance when compared to other optimization methodologies.",
      "authors": [
        "Fatma M. Talaat"
      ],
      "year": 2022,
      "citation_count": 42,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/174be0bacee04d9eb13a698d484ab5ae441c1100",
      "pdf_link": "",
      "venue": "Multimedia tools and applications",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "3e0925355554e3aeb99de8165c268582a82de3bb",
      "title": "Smooth Exploration for Robotic Reinforcement Learning",
      "abstract": "Reinforcement learning (RL) enables robots to learn skills from interactions with the real world. In practice, the unstructured step-based exploration used in Deep RL -- often very successful in simulation -- leads to jerky motion patterns on real robots. Consequences of the resulting shaky behavior are poor exploration, or even damage to the robot. We address these issues by adapting state-dependent exploration (SDE) to current Deep RL algorithms. To enable this adaptation, we propose two extensions to the original SDE, using more general features and re-sampling the noise periodically, which leads to a new exploration method generalized state-dependent exploration (gSDE). We evaluate gSDE both in simulation, on PyBullet continuous control tasks, and directly on three different real robots: a tendon-driven elastic robot, a quadruped and an RC car. The noise sampling interval of gSDE permits to have a compromise between performance and smoothness, which allows training directly on the real robots without loss of performance. The code is available at https://github.com/DLR-RM/stable-baselines3.",
      "authors": [
        "A. Raffin",
        "Jens Kober",
        "F. Stulp"
      ],
      "year": 2020,
      "citation_count": 64,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/3e0925355554e3aeb99de8165c268582a82de3bb",
      "pdf_link": "",
      "venue": "Conference on Robot Learning",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "442e9f1e8f6218e68f944fd3028c5385691d4112",
      "title": "Aggressive Quadrotor Flight Using Curiosity-Driven Reinforcement Learning",
      "abstract": "The ability to perform aggressive movements,which are called aggressive flights, is important for quadrotors during navigation. However, aggressive quadrotor flights are still a great challenge to practical applications. The existing solutions to aggressive flights heavily rely on a predefined trajectory, which is a time-consuming preprocessing step. To avoid such path planning, we propose a curiosity-driven reinforcement learning method for aggressive flight missions and a similarity-based curiosity module is introduced to speed up the training procedure. A branch structure exploration strategy is also applied to guarantee the robustness of the policy and to ensure the policy trained in simulations can be performed in real-world experiments directly. The experimental results in simulations demonstrate that our reinforcement learning algorithm performs well in aggressive flight tasks, speeds up the convergence process and improves the robustness of the policy. Besides, our algorithm shows a satisfactory simulated to real transferability and performs well in real-world experiments.",
      "authors": [
        "Qiyu Sun",
        "Jinbao Fang",
        "Weixing Zheng",
        "Yang Tang"
      ],
      "year": 2022,
      "citation_count": 32,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/442e9f1e8f6218e68f944fd3028c5385691d4112",
      "pdf_link": "",
      "venue": "IEEE transactions on industrial electronics (1982. Print)",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "a25b645c3d24f91164230a0ac5bb2d4ec88c1538",
      "title": "Information-Directed Exploration for Deep Reinforcement Learning",
      "abstract": "Efficient exploration remains a major challenge for reinforcement learning. One reason is that the variability of the returns often depends on the current state and action, and is therefore heteroscedastic. Classical exploration strategies such as upper confidence bound algorithms and Thompson sampling fail to appropriately account for heteroscedasticity, even in the bandit setting. Motivated by recent findings that address this issue in bandits, we propose to use Information-Directed Sampling (IDS) for exploration in reinforcement learning. As our main contribution, we build on recent advances in distributional reinforcement learning and propose a novel, tractable approximation of IDS for deep Q-learning. The resulting exploration strategy explicitly accounts for both parametric uncertainty and heteroscedastic observation noise. We evaluate our method on Atari games and demonstrate a significant improvement over alternative approaches.",
      "authors": [
        "Nikolay Nikolov",
        "Johannes Kirschner",
        "Felix Berkenkamp",
        "Andreas Krause"
      ],
      "year": 2018,
      "citation_count": 74,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/a25b645c3d24f91164230a0ac5bb2d4ec88c1538",
      "pdf_link": "",
      "venue": "International Conference on Learning Representations",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "0ba7f2e592dda172bc3d07f88cdcf2a57830deec",
      "title": "Towards Safe Reinforcement Learning with a Safety Editor Policy",
      "abstract": "We consider the safe reinforcement learning (RL) problem of maximizing utility with extremely low constraint violation rates. Assuming no prior knowledge or pre-training of the environment safety model given a task, an agent has to learn, via exploration, which states and actions are safe. A popular approach in this line of research is to combine a model-free RL algorithm with the Lagrangian method to adjust the weight of the constraint reward relative to the utility reward dynamically. It relies on a single policy to handle the conflict between utility and constraint rewards, which is often challenging. We present SEditor, a two-policy approach that learns a safety editor policy transforming potentially unsafe actions proposed by a utility maximizer policy into safe ones. The safety editor is trained to maximize the constraint reward while minimizing a hinge loss of the utility state-action values before and after an action is edited. SEditor extends existing safety layer designs that assume simplified safety models, to general safe RL scenarios where the safety model can in theory be arbitrarily complex. As a first-order method, it is easy to implement and efficient for both inference and training. On 12 Safety Gym tasks and 2 safe racing tasks, SEditor obtains much a higher overall safety-weighted-utility (SWU) score than the baselines, and demonstrates outstanding utility performance with constraint violation rates as low as once per 2k time steps, even in obstacle-dense environments. On some tasks, this low violation rate is up to 200 times lower than that of an unconstrained RL method with similar utility performance. Code is available at https://github.com/hnyu/seditor.",
      "authors": [
        "Haonan Yu",
        "Wei Xu",
        "Haichao Zhang"
      ],
      "year": 2022,
      "citation_count": 31,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/0ba7f2e592dda172bc3d07f88cdcf2a57830deec",
      "pdf_link": "",
      "venue": "Neural Information Processing Systems",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "09da56cd3bf72b632c43969be97874fa14a3765c",
      "title": "The Challenges of Exploration for Offline Reinforcement Learning",
      "abstract": "Offline Reinforcement Learning (ORL) enablesus to separately study the two interlinked processes of reinforcement learning: collecting informative experience and inferring optimal behaviour. The second step has been widely studied in the offline setting, but just as critical to data-efficient RL is the collection of informative data. The task-agnostic setting for data collection, where the task is not known a priori, is of particular interest due to the possibility of collecting a single dataset and using it to solve several downstream tasks as they arise. We investigate this setting via curiosity-based intrinsic motivation, a family of exploration methods which encourage the agent to explore those states or transitions it has not yet learned to model. With Explore2Offline, we propose to evaluate the quality of collected data by transferring the collected data and inferring policies with reward relabelling and standard offline RL algorithms. We evaluate a wide variety of data collection strategies, including a new exploration agent, Intrinsic Model Predictive Control (IMPC), using this scheme and demonstrate their performance on various tasks. We use this decoupled framework to strengthen intuitions about exploration and the data prerequisites for effective offline RL.",
      "authors": [
        "Nathan Lambert",
        "Markus Wulfmeier",
        "William F. Whitney",
        "Arunkumar Byravan",
        "Michael Bloesch",
        "Vibhavari Dasagi",
        "Tim Hertweck",
        "Martin A. Riedmiller"
      ],
      "year": 2022,
      "citation_count": 30,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/09da56cd3bf72b632c43969be97874fa14a3765c",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "46eb68c585bdb8a1051dfda98b4b35610301264f",
      "title": "Spacecraft Proximity Maneuvering and Rendezvous With Collision Avoidance Based on Reinforcement Learning",
      "abstract": "The rapid development of the aerospace industry puts forward the urgent need for the evolution of autonomous spacecraft rendezvous technology, which has gained significant attention recently due to increased applications in various space missions. This article studies the relative position tracking problem of the autonomous spacecraft rendezvous under the requirement of collision avoidance. An exploration-adaptive deep deterministic policy gradient (DDPG) algorithm is proposed to train a definite control strategy for this mission. Similar to the DDPG algorithm, four neural networks are used in this method, where two of them are used to generate the deterministic policy, whereas the other two are used to score the obtained policy. Differently, adaptive noise is introduced to reduce the possibility of oscillations and divergences and to cut down the unnecessary computation by weakening the exploration of stabilization problems. In addition, in order to effectively and quickly adapt to some other similar scenarios, a metalearning-based idea is introduced by fine-tuning the prior strategy. Finally, two numerical simulations show that the trained control strategy can effectively avoid the oscillation phenomenon caused by the artificial potential function. Benefiting from this, the trained control strategy based on deep reinforcement learning technology can decrease the energy consumption by 16.44% during the close proximity phase, compared with the traditional artificial potential function method. Besides, after introducing the metalearning-based idea, a strategy available for some other perturbed scenarios can be trained in a relatively short period of time, which illustrates its adaptability.",
      "authors": [
        "Qingyu Qu",
        "Kexin Liu",
        "Wei Wang",
        "Jinhu Lu"
      ],
      "year": 2022,
      "citation_count": 29,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/46eb68c585bdb8a1051dfda98b4b35610301264f",
      "pdf_link": "",
      "venue": "IEEE Transactions on Aerospace and Electronic Systems",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "04efc9768a8e0c5f23b8c8504fb6db8803ffc071",
      "title": "Generative Design by Using Exploration Approaches of Reinforcement Learning in Density-Based Structural Topology Optimization",
      "abstract": "A central challenge in generative design is the exploration of vast number of solutions. In this work, we extend two major density-based structural topology optimization (STO) methods based on four classes of exploration algorithms of reinforcement learning (RL) to STO problems, which approaches generative design in a new way. The four methods are: first, using ε -greedy policy to disturb the search direction; second, using upper confidence bound (UCB) to add a bonus on sensitivity; last, using Thompson sampling (TS) as well as information-directed sampling (IDS) to direct the search, where the posterior function of reward is fitted by Beta distribution or Gaussian distribution. Those combined methods are evaluated on some structure compliance minimization tasks from 2D to 3D, including the variable thickness design problem of an atmospheric diving suit (ADS). We show that all methods can generate various acceptable design options by varying one or two parameters simply, except that IDS fails to reach the convergence for complex structures due to the limitation of computation ability. We also show that both Beta distribution and Gaussian distribution work well to describe the posterior probability.",
      "authors": [
        "H. Sun",
        "Ling Ma"
      ],
      "year": 2020,
      "citation_count": 48,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/04efc9768a8e0c5f23b8c8504fb6db8803ffc071",
      "pdf_link": "",
      "venue": "Designs",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "fb3c6456708b0e143f545d77dc8ec804eb947395",
      "title": "Curiosity-driven Exploration in Deep Reinforcement Learning via Bayesian Neural Networks",
      "abstract": "Scalable and effective exploration remains a key challenge in reinforcement learning (RL). While there are methods with optimality guarantees in the setting of discrete state and action spaces, these methods cannot be applied in high-dimensional deep RL scenarios. As such, most contemporary RL relies on simple heuristics such as epsilon-greedy exploration or adding Gaussian noise to the controls. This paper introduces Variational Information Maximizing Exploration (VIME), an exploration strategy based on maximization of information gain about the agent's belief of environment dynamics. We propose a practical implementation, using variational inference in Bayesian neural networks which efficiently handles continuous state and action spaces. VIME modifies the MDP reward function, and can be applied with several different underlying RL algorithms. We demonstrate that VIME achieves significantly better performance compared to heuristic exploration methods across a variety of continuous control tasks and algorithms, including tasks with very sparse rewards.",
      "authors": [
        "Rein Houthooft",
        "Xi Chen",
        "Yan Duan",
        "John Schulman",
        "F. Turck",
        "P. Abbeel"
      ],
      "year": 2016,
      "citation_count": 79,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/fb3c6456708b0e143f545d77dc8ec804eb947395",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "813f6e34feb3dc0346b6392d061af12ff186ba7e",
      "title": "Learning Efficient Multi-Agent Cooperative Visual Exploration",
      "abstract": "We tackle the problem of cooperative visual exploration where multiple agents need to jointly explore unseen regions as fast as possible based on visual signals. Classical planning-based methods often suffer from expensive computation overhead at each step and a limited expressiveness of complex cooperation strategy. By contrast, reinforcement learning (RL) has recently become a popular paradigm for tackling this challenge due to its modeling capability of arbitrarily complex strategies and minimal inference overhead. In this paper, we extend the state-of-the-art single-agent visual navigation method, Active Neural SLAM (ANS), to the multi-agent setting by introducing a novel RL-based planning module, Multi-agent Spatial Planner (MSP).MSP leverages a transformer-based architecture, Spatial-TeamFormer, which effectively captures spatial relations and intra-agent interactions via hierarchical spatial self-attentions. In addition, we also implement a few multi-agent enhancements to process local information from each agent for an aligned spatial representation and more precise planning. Finally, we perform policy distillation to extract a meta policy to significantly improve the generalization capability of final policy. We call this overall solution, Multi-Agent Active Neural SLAM (MAANS). MAANS substantially outperforms classical planning-based baselines for the first time in a photo-realistic 3D simulator, Habitat. Code and videos can be found at https://sites.google.com/view/maans.",
      "authors": [
        "Chao Yu",
        "Xinyi Yang",
        "Jiaxuan Gao",
        "Huazhong Yang",
        "Yu Wang",
        "Yi Wu"
      ],
      "year": 2021,
      "citation_count": 32,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/813f6e34feb3dc0346b6392d061af12ff186ba7e",
      "pdf_link": "",
      "venue": "European Conference on Computer Vision",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "714b7c61d81687d093fd8b7d6d6737d582a4c9b7",
      "title": "Towards Applicable Reinforcement Learning: Improving the Generalization and Sample Efficiency with Policy Ensemble",
      "abstract": "It is challenging for reinforcement learning (RL) algorithms to succeed in real-world applications. Take financial trading as an example, the market information is noisy yet imperfect and the macroeconomic regulation or other factors may shift between training and evaluation, thus it requires both generalization and high sample efficiency for resolving the task. However, directly applying typical RL algorithms can lead to poor performance in such scenarios. To derive a robust and applicable RL algorithm, in this work, we design a simple but effective method named Ensemble Proximal Policy Optimization (EPPO), which learns ensemble policies in an end-to-end manner. Notably, EPPO combines each policy and the policy ensemble organically and optimizes both simultaneously. In addition, EPPO adopts a diversity enhancement regularization over the policy space which helps to generalize to unseen states and promotes exploration. We theoretically prove that EPPO can increase exploration efficacy, and through comprehensive experimental evaluations on various tasks, we demonstrate that EPPO achieves higher efficiency and is robust for real-world applications compared with vanilla policy optimization algorithms and other ensemble methods. Code and supplemental materials are available at https://seqml.github.io/eppo.",
      "authors": [
        "Zhengyu Yang",
        "Kan Ren",
        "Xufang Luo",
        "Minghuan Liu",
        "Weiqing Liu",
        "J. Bian",
        "Weinan Zhang",
        "Dongsheng Li"
      ],
      "year": 2022,
      "citation_count": 23,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/714b7c61d81687d093fd8b7d6d6737d582a4c9b7",
      "pdf_link": "",
      "venue": "International Joint Conference on Artificial Intelligence",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "fe7382db243694c67c667cf2ec80072577d2372b",
      "title": "Rule-Aware Reinforcement Learning for Knowledge Graph Reasoning",
      "abstract": "Multi-hop reasoning is an effective and ex-plainable approach to predicting missing facts in Knowledge Graphs (KGs). It usually adopts the Reinforcement Learning (RL) framework and searches over the KG to ﬁnd an evidential path. However, due to the large exploration space, the RL-based model struggles with the serious sparse reward problem and needs to make a lot of trials. Moreover, its exploration can be biased towards spurious paths that coin-cidentally lead to correct answers. To solve both problems, we propose a simple but effective RL-based method called RARL (Rule-Aware RL). It injects high quality symbolic rules into the model’s reasoning process and employs partially random beam search, which can not only increase the probability of paths getting rewards, but also alleviate the impact of spurious paths. Experimental results show that it outperforms existing multi-hop methods in terms of Hit@1 and MRR.",
      "authors": [
        "Zhongni Hou",
        "Xiaolong Jin",
        "Zixuan Li",
        "Long Bai"
      ],
      "year": 2021,
      "citation_count": 29,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/fe7382db243694c67c667cf2ec80072577d2372b",
      "pdf_link": "",
      "venue": "Findings",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "cf628a42ee56c8f1b858790822a2bc0a61a49110",
      "title": "Deep Black-Box Reinforcement Learning with Movement Primitives",
      "abstract": "\\Episode-based reinforcement learning (ERL) algorithms treat reinforcement learning (RL) as a black-box optimization problem where we learn to select a parameter vector of a controller, often represented as a movement primitive, for a given task descriptor called a context. ERL offers several distinct benefits in comparison to step-based RL. It generates smooth control trajectories, can handle non-Markovian reward definitions, and the resulting exploration in parameter space is well suited for solving sparse reward settings. Yet, the high dimensionality of the movement primitive parameters has so far hampered the effective use of deep RL methods. In this paper, we present a new algorithm for deep ERL. It is based on differentiable trust region layers, a successful on-policy deep RL algorithm. These layers allow us to specify trust regions for the policy update that are solved exactly for each state using convex optimization, which enables policies learning with the high precision required for the ERL. We compare our ERL algorithm to state-of-the-art step-based algorithms in many complex simulated robotic control tasks. In doing so, we investigate different reward formulations - dense, sparse, and non-Markovian. While step-based algorithms perform well only on dense rewards, ERL performs favorably on sparse and non-Markovian rewards. Moreover, our results show that the sparse and the non-Markovian rewards are also often better suited to define the desired behavior, allowing us to obtain considerably higher quality policies compared to step-based RL.",
      "authors": [
        "Fabian Otto",
        "Onur Çelik",
        "Hongyi Zhou",
        "Hanna Ziesche",
        "Ngo Anh Vien",
        "G. Neumann"
      ],
      "year": 2022,
      "citation_count": 21,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/cf628a42ee56c8f1b858790822a2bc0a61a49110",
      "pdf_link": "",
      "venue": "Conference on Robot Learning",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "cae05340421a18c64ac0897d57bcdcc9a496a3b8",
      "title": "Research on UCAV Maneuvering Decision Method Based on Heuristic Reinforcement Learning",
      "abstract": "With the rapid development of unmanned combat aerial vehicle (UCAV)-related technologies, UCAVs are playing an increasingly important role in military operations. It has become an inevitable trend in the development of future air combat battlefields that UCAVs complete air combat tasks independently to acquire air superiority. In this paper, the UCAV maneuver decision problem in continuous action space is studied based on the deep reinforcement learning strategy optimization method. The UCAV platform model of continuous action space was established. Focusing on the problem of insufficient exploration ability of Ornstein–Uhlenbeck (OU) exploration strategy in the deep deterministic policy gradient (DDPG) algorithm, a heuristic DDPG algorithm was proposed by introducing heuristic exploration strategy, and then a UCAV air combat maneuver decision method based on a heuristic DDPG algorithm is proposed. The superior performance of the algorithm is verified by comparison with different algorithms in the test environment, and the effectiveness of the decision method is verified by simulation of air combat tasks with different difficulty and attack modes.",
      "authors": [
        "Wang Yuan",
        "Z. Xiwen",
        "Zhou Rong",
        "Shangqin Tang",
        "Zhou Huan",
        "Dingkai Wei"
      ],
      "year": 2022,
      "citation_count": 20,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/cae05340421a18c64ac0897d57bcdcc9a496a3b8",
      "pdf_link": "",
      "venue": "Computational Intelligence and Neuroscience",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "5fd3ce235f5fcebd3d2807f710b060add527183b",
      "title": "Deep Curiosity Search: Intra-Life Exploration Improves Performance on Challenging Deep Reinforcement Learning Problems",
      "abstract": "Traditional exploration methods in RL require agents to perform random actions to find rewards. But these approaches struggle on sparse-reward domains like Montezuma's Revenge where the probability that any random action sequence leads to reward is extremely low. Recent algorithms have performed well on such tasks by encouraging agents to visit new states or perform new actions in relation to all prior training episodes (which we call across-training novelty). But such algorithms do not consider whether an agent exhibits intra-life novelty: doing something new within the current episode, regardless of whether those behaviors have been performed in previous episodes. We hypothesize that across-training novelty might discourage agents from revisiting initially non-rewarding states that could become important stepping stones later in training. We introduce Deep Curiosity Search (DeepCS), which encourages intra-life exploration by rewarding agents for visiting as many different states as possible within each episode, and show that DeepCS matches the performance of current state-of-the-art methods on Montezuma's Revenge. We further show that DeepCS improves exploration on Amidar, Freeway, Gravitar, and Tutankham (many of which are hard exploration games). Surprisingly, DeepCS doubles A2C performance on Seaquest, a game we would not have expected to benefit from intra-life exploration because the arena is small and already easily navigated by naive exploration techniques. In one run, DeepCS achieves a maximum training score of 80,000 points on Seaquest, higher than any methods other than Ape-X. The strong performance of DeepCS on these sparse- and dense-reward tasks suggests that encouraging intra-life novelty is an interesting, new approach for improving performance in Deep RL and motivates further research into hybridizing across-training and intra-life exploration methods.",
      "authors": [
        "C. Stanton",
        "J. Clune"
      ],
      "year": 2018,
      "citation_count": 42,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/5fd3ce235f5fcebd3d2807f710b060add527183b",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "1d4288c47a7802575d2a0d231d1283a4f225a85b",
      "title": "MetaCURE: Meta Reinforcement Learning with Empowerment-Driven Exploration",
      "abstract": "Meta reinforcement learning (meta-RL) extracts knowledge from previous tasks and achieves fast adaptation to new tasks. Despite recent progress, efficient exploration in meta-RL remains a key challenge in sparse-reward tasks, as it requires quickly finding informative task-relevant experiences in both meta-training and adaptation. To address this challenge, we explicitly model an exploration policy learning problem for meta-RL, which is separated from exploitation policy learning, and introduce a novel empowerment-driven exploration objective, which aims to maximize information gain for task identification. We derive a corresponding intrinsic reward and develop a new off-policy meta-RL framework, which efficiently learns separate context-aware exploration and exploitation policies by sharing the knowledge of task inference. Experimental evaluation shows that our meta-RL method significantly outperforms state-of-the-art baselines on various sparse-reward MuJoCo locomotion tasks and more complex sparse-reward Meta-World tasks.",
      "authors": [
        "Jin Zhang",
        "Jianhao Wang",
        "Hao Hu",
        "Tong Chen",
        "Yingfeng Chen",
        "Changjie Fan",
        "Chongjie Zhang"
      ],
      "year": 2020,
      "citation_count": 28,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/1d4288c47a7802575d2a0d231d1283a4f225a85b",
      "pdf_link": "",
      "venue": "International Conference on Machine Learning",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "1a39ad2d1553d07084b5e44a28878c8bb5018cef",
      "title": "PC-MLP: Model-based Reinforcement Learning with Policy Cover Guided Exploration",
      "abstract": "Model-based Reinforcement Learning (RL) is a popular learning paradigm due to its potential sample efficiency compared to model-free RL. However, existing empirical model-based RL approaches lack the ability to explore. This work studies a computationally and statistically efficient model-based algorithm for both Kernelized Nonlinear Regulators (KNR) and linear Markov Decision Processes (MDPs). For both models, our algorithm guarantees polynomial sample complexity and only uses access to a planning oracle. Experimentally, we first demonstrate the flexibility and efficacy of our algorithm on a set of exploration challenging control tasks where existing empirical model-based RL approaches completely fail. We then show that our approach retains excellent performance even in common dense reward control benchmarks that do not require heavy exploration. Finally, we demonstrate that our method can also perform reward-free exploration efficiently. Our code can be found at https://github.com/yudasong/PCMLP.",
      "authors": [
        "Yuda Song",
        "Wen Sun"
      ],
      "year": 2021,
      "citation_count": 22,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/1a39ad2d1553d07084b5e44a28878c8bb5018cef",
      "pdf_link": "",
      "venue": "International Conference on Machine Learning",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "ecf5dc817fd6326e943b759c889d1285e673b24a",
      "title": "Digital Twin-Assisted Efficient Reinforcement Learning for Edge Task Scheduling",
      "abstract": "Task scheduling is a critical problem when one user offloads multiple different tasks to the edge server. When a user has multiple tasks to offload and only one task can be transmitted to server at a time, while server processes tasks according to the transmission order, the problem is NP-hard. However, it is difficult for traditional optimization methods to quickly obtain the optimal solution, while approaches based on reinforcement learning face with the challenge of excessively large action space and slow convergence. In this paper, we propose a Digital Twin (DT)-assisted RL-based task scheduling method in order to improve the performance and convergence of the RL. We use DT to simulate the results of different decisions made by the agent, so that one agent can try multiple actions at a time, or, similarly, multiple agents can interact with environment in parallel in DT. In this way, the exploration efficiency of RL can be significantly improved via DT, and thus RL can converges faster and local optimality is less likely to happen. Particularly, two algorithms are designed to made task scheduling decisions, i.e., DT-assisted asynchronous Q-learning (DTAQL) and DT-assisted exploring Q-learning (DTEQL). Simulation results show that both algorithms significantly improve the convergence speed of Q-learning by increasing the exploration efficiency.",
      "authors": [
        "Xiucheng Wang",
        "Longfei Ma",
        "Hao Li",
        "Zhisheng Yin",
        "T. Luan",
        "Nan Cheng"
      ],
      "year": 2022,
      "citation_count": 16,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/ecf5dc817fd6326e943b759c889d1285e673b24a",
      "pdf_link": "",
      "venue": "IEEE Vehicular Technology Conference",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "02ad21eea9ec32783ba529487e74a76e85499a53",
      "title": "Model-Based Offline Meta-Reinforcement Learning with Regularization",
      "abstract": "Existing offline reinforcement learning (RL) methods face a few major challenges, particularly the distributional shift between the learned policy and the behavior policy. Offline Meta-RL is emerging as a promising approach to address these challenges, aiming to learn an informative meta-policy from a collection of tasks. Nevertheless, as shown in our empirical studies, offline Meta-RL could be outperformed by offline single-task RL methods on tasks with good quality of datasets, indicating that a right balance has to be delicately calibrated between\"exploring\"the out-of-distribution state-actions by following the meta-policy and\"exploiting\"the offline dataset by staying close to the behavior policy. Motivated by such empirical analysis, we explore model-based offline Meta-RL with regularized Policy Optimization (MerPO), which learns a meta-model for efficient task structure inference and an informative meta-policy for safe exploration of out-of-distribution state-actions. In particular, we devise a new meta-Regularized model-based Actor-Critic (RAC) method for within-task policy optimization, as a key building block of MerPO, using conservative policy evaluation and regularized policy improvement; and the intrinsic tradeoff therein is achieved via striking the right balance between two regularizers, one based on the behavior policy and the other on the meta-policy. We theoretically show that the learnt policy offers guaranteed improvement over both the behavior policy and the meta-policy, thus ensuring the performance improvement on new tasks via offline Meta-RL. Experiments corroborate the superior performance of MerPO over existing offline Meta-RL methods.",
      "authors": [
        "Sen Lin",
        "Jialin Wan",
        "Tengyu Xu",
        "Yingbin Liang",
        "Junshan Zhang"
      ],
      "year": 2022,
      "citation_count": 16,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/02ad21eea9ec32783ba529487e74a76e85499a53",
      "pdf_link": "",
      "venue": "International Conference on Learning Representations",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "a17a7256c04afee68f9aa0b7bfdc67fbca998b9c",
      "title": "Accelerating Reinforcement Learning for Autonomous Driving Using Task-Agnostic and Ego-Centric Motion Skills",
      "abstract": "Efficient and effective exploration in continuous space is a central problem in applying reinforcement learning (RL) to autonomous driving. Skills learned from expert demonstrations or designed for specific tasks can benefit the exploration, but they are usually costly-collected, unbalanced/suboptimal, or failing to transfer to diverse tasks. However, human drivers can adapt to varied driving tasks without demonstrations by taking efficient and structural explorations in the entire skill space rather than a limited space with task-specific skills. Inspired by the above fact, we propose an RL algorithm exploring all feasible motion skills instead of a limited set of task-specific and object-centric skills. Without demonstrations, our method can still perform well in diverse tasks. First, we build a task-agnostic and ego-centric (TaEc) motion skill library in a pure motion perspective, which is diverse enough to be reusable in different complex tasks. The motion skills are then encoded into a low-dimension latent skill space, in which RL can do exploration efficiently. Validations in various challenging driving scenarios demonstrate that our proposed method, TaEc-RL, outperforms its counterparts significantly in learning efficiency and task performance.",
      "authors": [
        "Tong Zhou",
        "Letian Wang",
        "Ruobing Chen",
        "Wenshuo Wang",
        "Y. Liu"
      ],
      "year": 2022,
      "citation_count": 16,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/a17a7256c04afee68f9aa0b7bfdc67fbca998b9c",
      "pdf_link": "",
      "venue": "IEEE/RJS International Conference on Intelligent RObots and Systems",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "69bdc99655204190697067c3da5296e544e6865d",
      "title": "Safe Model-Based Reinforcement Learning With an Uncertainty-Aware Reachability Certificate",
      "abstract": "Safe reinforcement learning (RL) that solves constraint-satisfactory policies provides a promising way to the broader safety-critical applications of RL in real-world problems such as robotics. Among all safe RL approaches, model-based methods reduce training time violations further due to their high sample efficiency. However, lacking safety robustness against the model uncertainties remains an issue in safe model-based RL, especially in training time safety. In this paper, we propose a distributional reachability certificate (DRC) and its Bellman equation to address model uncertainties and characterize robust persistently safe states. Furthermore, we build a safe RL framework to resolve constraints required by the DRC and its corresponding shield policy. We also devise a line search method to maintain safety and reach higher returns simultaneously while leveraging the shield policy. Comprehensive experiments on classical benchmarks such as constrained tracking and navigation indicate that the proposed algorithm achieves comparable returns with much fewer constraint violations during training. Our code is available at https://github.com/ManUtdMoon/Distributional-Reachability-Policy-Optimization. Note to Practitioners—Although it has been proven that RL can be applied in complex robotics control tasks, the training process of an RL control policy induces frequent failures because the agent needs to learn safety through constraint violations. This issue hinders the promotion of RL because a large amount of failure of robots is too expensive to afford. This paper aims to reduce the training-time violations of RL-based control methods, enabling RL to be leveraged in a broader application area. To achieve the goal, we first introduce a safety quantity describing the distribution of potential constraint violations in the long term. By imposing constraints on the quantile of the safety distribution, we can realize safety robust to the model uncertainty, which is necessary for real-world robot learning with environment uncertainty. Second, we further devise a shield policy aiming to minimize the constraint violation. The policy will intervene when the agent is about to violate state constraints, further enhancing exploration safety. Third, we implement a line search method to find an action pursuing near-optimal performance when fulfilling safety requirements strictly. Our experimental results indicate that the proposed algorithm reduces training-time violations significantly while maintaining competitive task performance. We make a step towards applying RL safely in real-world tasks. Our future work includes conducting physical verification on real robots to evaluate the algorithm and improving safety further by starting from an initially safe control policy that comes from domain knowledge.",
      "authors": [
        "Dongjie Yu",
        "Wenjun Zou",
        "Yujie Yang",
        "Haitong Ma",
        "Sheng Li",
        "Yuming Yin",
        "Jianyu Chen",
        "Jingliang Duan"
      ],
      "year": 2022,
      "citation_count": 16,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/69bdc99655204190697067c3da5296e544e6865d",
      "pdf_link": "",
      "venue": "IEEE Transactions on Automation Science and Engineering",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "70e1d6b227fdd605fe61239a953e803df97e521d",
      "title": "Model-based Lifelong Reinforcement Learning with Bayesian Exploration",
      "abstract": "We propose a model-based lifelong reinforcement-learning approach that estimates a hierarchical Bayesian posterior distilling the common structure shared across different tasks. The learned posterior combined with a sample-based Bayesian exploration procedure increases the sample efficiency of learning across a family of related tasks. We first derive an analysis of the relationship between the sample complexity and the initialization quality of the posterior in the finite MDP setting. We next scale the approach to continuous-state domains by introducing a Variational Bayesian Lifelong Reinforcement Learning algorithm that can be combined with recent model-based deep RL methods, and that exhibits backward transfer. Experimental results on several challenging domains show that our algorithms achieve both better forward and backward transfer performance than state-of-the-art lifelong RL methods.",
      "authors": [
        "Haotian Fu",
        "Shangqun Yu",
        "Michael S. Littman",
        "G. Konidaris"
      ],
      "year": 2022,
      "citation_count": 14,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/70e1d6b227fdd605fe61239a953e803df97e521d",
      "pdf_link": "",
      "venue": "Neural Information Processing Systems",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "5c0b56d9440ea70c79d1d0032d46c14e06f541f5",
      "title": "Reactive Exploration to Cope with Non-Stationarity in Lifelong Reinforcement Learning",
      "abstract": "In lifelong learning, an agent learns throughout its entire life without resets, in a constantly changing environment, as we humans do. Consequently, lifelong learning comes with a plethora of research problems such as continual domain shifts, which result in non-stationary rewards and environment dynamics. These non-stationarities are difficult to detect and cope with due to their continuous nature. Therefore, exploration strategies and learning methods are required that are capable of tracking the steady domain shifts, and adapting to them. We propose Reactive Exploration to track and react to continual domain shifts in lifelong reinforcement learning, and to update the policy correspondingly. To this end, we conduct experiments in order to investigate different exploration strategies. We empirically show that representatives of the policy-gradient family are better suited for lifelong learning, as they adapt more quickly to distribution shifts than Q-learning. Thereby, policy-gradient methods profit the most from Reactive Exploration and show good results in lifelong learning with continual domain shifts. Our code is available at: https://github.com/ml-jku/reactive-exploration.",
      "authors": [
        "C. Steinparz",
        "Thomas Schmied",
        "Fabian Paischer",
        "Marius-Constantin Dinu",
        "Vihang Patil",
        "Angela Bitto-Nemling",
        "Hamid Eghbalzadeh",
        "Sepp Hochreiter"
      ],
      "year": 2022,
      "citation_count": 14,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/5c0b56d9440ea70c79d1d0032d46c14e06f541f5",
      "pdf_link": "",
      "venue": "CoLLAs",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "0cdbd90989e5f60b6a42dae76b23bd489fcf65cc",
      "title": "Robust Policy Optimization in Deep Reinforcement Learning",
      "abstract": "The policy gradient method enjoys the simplicity of the objective where the agent optimizes the cumulative reward directly. Moreover, in the continuous action domain, parameterized distribution of action distribution allows easy control of exploration, resulting from the variance of the representing distribution. Entropy can play an essential role in policy optimization by selecting the stochastic policy, which eventually helps better explore the environment in reinforcement learning (RL). However, the stochasticity often reduces as the training progresses; thus, the policy becomes less exploratory. Additionally, certain parametric distributions might only work for some environments and require extensive hyperparameter tuning. This paper aims to mitigate these issues. In particular, we propose an algorithm called Robust Policy Optimization (RPO), which leverages a perturbed distribution. We hypothesize that our method encourages high-entropy actions and provides a way to represent the action space better. We further provide empirical evidence to verify our hypothesis. We evaluated our methods on various continuous control tasks from DeepMind Control, OpenAI Gym, Pybullet, and IsaacGym. We observed that in many settings, RPO increases the policy entropy early in training and then maintains a certain level of entropy throughout the training period. Eventually, our agent RPO shows consistently improved performance compared to PPO and other techniques: entropy regularization, different distributions, and data augmentation. Furthermore, in several settings, our method stays robust in performance, while other baseline mechanisms fail to improve and even worsen the performance.",
      "authors": [
        "Md Masudur Rahman",
        "Yexiang Xue"
      ],
      "year": 2022,
      "citation_count": 14,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/0cdbd90989e5f60b6a42dae76b23bd489fcf65cc",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "b0c40766974df3eae8ff500379e66e5566cd16c9",
      "title": "An Efficient Asynchronous Method for Integrating Evolutionary and Gradient-based Policy Search",
      "abstract": "Deep reinforcement learning (DRL) algorithms and evolution strategies (ES) have been applied to various tasks, showing excellent performances. These have the opposite properties, with DRL having good sample efficiency and poor stability, while ES being vice versa. Recently, there have been attempts to combine these algorithms, but these methods fully rely on synchronous update scheme, making it not ideal to maximize the benefits of the parallelism in ES. To solve this challenge, asynchronous update scheme was introduced, which is capable of good time-efficiency and diverse policy exploration. In this paper, we introduce an Asynchronous Evolution Strategy-Reinforcement Learning (AES-RL) that maximizes the parallel efficiency of ES and integrates it with policy gradient methods. Specifically, we propose 1) a novel framework to merge ES and DRL asynchronously and 2) various asynchronous update methods that can take all advantages of asynchronism, ES, and DRL, which are exploration and time efficiency, stability, and sample efficiency, respectively. The proposed framework and update methods are evaluated in continuous control benchmark work, showing superior performance as well as time efficiency compared to the previous methods.",
      "authors": [
        "Kyunghyun Lee",
        "Byeong-uk Lee",
        "Ukcheol Shin",
        "In-So Kweon"
      ],
      "year": 2020,
      "citation_count": 23,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/b0c40766974df3eae8ff500379e66e5566cd16c9",
      "pdf_link": "",
      "venue": "Neural Information Processing Systems",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "3d3b0704c61d47c7bafb70ae2670b2786b8e4d81",
      "title": "Efficient Exploration in Resource-Restricted Reinforcement Learning",
      "abstract": "In many real-world applications of reinforcement learning (RL), performing actions requires consuming certain types of resources that are non-replenishable in each episode. Typical applications include robotic control with limited energy and video games with consumable items. In tasks with non-replenishable resources, we observe that popular RL methods such as soft actor critic suffer from poor sample efficiency. The major reason is that, they tend to exhaust resources fast and thus the subsequent exploration is severely restricted due to the absence of resources. To address this challenge, we first formalize the aforementioned problem as a resource-restricted reinforcement learning, and then propose a novel resource-aware exploration bonus (RAEB) to make reasonable usage of resources. An appealing feature of RAEB is that, it can significantly reduce unnecessary resource-consuming trials while effectively encouraging the agent to explore unvisited states. Experiments demonstrate that the proposed RAEB significantly outperforms state-of-the-art exploration strategies in resource-restricted reinforcement learning environments, improving the sample efficiency by up to an order of magnitude.",
      "authors": [
        "Zhihai Wang",
        "Taoxing Pan",
        "Qi Zhou",
        "Jie Wang"
      ],
      "year": 2022,
      "citation_count": 13,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/3d3b0704c61d47c7bafb70ae2670b2786b8e4d81",
      "pdf_link": "",
      "venue": "AAAI Conference on Artificial Intelligence",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "9e5fe2ba652774ba3b1127f626c192668a907132",
      "title": "Decoupled Exploration and Exploitation Policies for Sample-Efficient Reinforcement Learning",
      "abstract": "Despite the close connection between exploration and sample efficiency, most state of the art reinforcement learning algorithms include no considerations for exploration beyond maximizing the entropy of the policy. In this work we address this seeming missed opportunity. We observe that the most common formulation of directed exploration in deep RL, known as bonus-based exploration (BBE), suffers from bias and slow coverage in the few-sample regime. This causes BBE to be actively detrimental to policy learning in many control tasks. We show that by decoupling the task policy from the exploration policy, directed exploration can be highly effective for sample-efficient continuous control. Our method, Decoupled Exploration and Exploitation Policies (DEEP), can be combined with any off-policy RL algorithm without modification. When used in conjunction with soft actor-critic, DEEP incurs no performance penalty in densely-rewarding environments. On sparse environments, DEEP gives a several-fold improvement in data efficiency due to better exploration.",
      "authors": [
        "William F. Whitney",
        "Michael Bloesch",
        "Jost Tobias Springenberg",
        "A. Abdolmaleki",
        "Kyunghyun Cho",
        "Martin A. Riedmiller"
      ],
      "year": 2021,
      "citation_count": 17,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/9e5fe2ba652774ba3b1127f626c192668a907132",
      "pdf_link": "",
      "venue": "",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "103f1674121780097f896ffe525bab2c6ae0bcdc",
      "title": "Exploration Entropy for Reinforcement Learning",
      "abstract": "The training process analysis and termination condition of the training process of a Reinforcement Learning (RL) system have always been the key issues to train an RL agent. In this paper, a new approach based on State Entropy and Exploration Entropy is proposed to analyse the training process. The concept of State Entropy is used to denote the uncertainty for an RL agent to select the action at every state that the agent will traverse, while the Exploration Entropy denotes the action selection uncertainty of the whole system. Actually, the action selection uncertainty of a certain state or the whole system reflects the degree of exploration and the stage of the learning process for an agent. The Exploration Entropy is a new criterion to analyse and manage the training process of RL. The theoretical analysis and experiment results illustrate that the curve of Exploration Entropy contains more information than the existing analytical methods.",
      "authors": [
        "Bo Xin",
        "Haixu Yu",
        "You Qin",
        "Qing Tang",
        "Zhangqing Zhu"
      ],
      "year": 2020,
      "citation_count": 19,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/103f1674121780097f896ffe525bab2c6ae0bcdc",
      "pdf_link": "",
      "venue": "",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "de93c8aed64229571b03e40b36499d4f07ce875d",
      "title": "PBCS : Efficient Exploration and Exploitation Using a Synergy between Reinforcement Learning and Motion Planning",
      "abstract": "The exploration-exploitation trade-off is at the heart of reinforcement learning (RL). However, most continuous control benchmarks used in recent RL research only require local exploration. This led to the development of algorithms that have basic exploration capabilities, and behave poorly in benchmarks that require more versatile exploration. For instance, as demonstrated in our empirical study, state-of-the-art RL algorithms such as DDPG and TD3 are unable to steer a point mass in even small 2D mazes. In this paper, we propose a new algorithm called \"Plan, Backplay, Chain Skills\" (PBCS) that combines motion planning and reinforcement learning to solve hard exploration environments. In a first phase, a motion planning algorithm is used to find a single good trajectory, then an RL algorithm is trained using a curriculum derived from the trajectory, by combining a variant of the Backplay algorithm and skill chaining. We show that this method outperforms state-of-the-art RL algorithms in 2D maze environments of various sizes, and is able to improve on the trajectory obtained by the motion planning phase.",
      "authors": [
        "Guillaume Matheron",
        "Nicolas Perrin",
        "Olivier Sigaud"
      ],
      "year": 2020,
      "citation_count": 19,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/de93c8aed64229571b03e40b36499d4f07ce875d",
      "pdf_link": "",
      "venue": "International Conference on Artificial Neural Networks",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "9d1445f1845a2880ff9c752845660e9c294aa7b5",
      "title": "Flow to Control: Offline Reinforcement Learning with Lossless Primitive Discovery",
      "abstract": "Offline reinforcement learning (RL) enables the agent to effectively learn from logged data, which significantly extends the applicability of RL algorithms in real-world scenarios where exploration can be expensive or unsafe. Previous works have shown that extracting primitive skills from the recurring and temporally extended structures in the logged data yields better learning. However, these methods suffer greatly when the primitives have limited representation ability to recover the original policy space, especially in offline settings. In this paper, we give a quantitative characterization of the performance of offline hierarchical learning and highlight the importance of learning lossless primitives. To this end, we propose to use a flow-based structure as the representation for low-level policies. This allows us to represent the behaviors in the dataset faithfully while keeping the expression ability to recover the whole policy space. We show that such lossless primitives can drastically improve the performance of hierarchical policies. The experimental results and extensive ablation studies on the standard D4RL benchmark show that our method has a good representation ability for policies and achieves superior performance in most tasks.",
      "authors": [
        "Yiqin Yang",
        "Haotian Hu",
        "Wenzhe Li",
        "Siyuan Li",
        "Jun Yang",
        "Qianchuan Zhao",
        "Chongjie Zhang"
      ],
      "year": 2022,
      "citation_count": 11,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/9d1445f1845a2880ff9c752845660e9c294aa7b5",
      "pdf_link": "",
      "venue": "AAAI Conference on Artificial Intelligence",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "83f1343500c9f0df62da0d61736738d8d7a9bba0",
      "title": "Zero-Shot Policy Transfer with Disentangled Task Representation of Meta-Reinforcement Learning",
      "abstract": "Humans are capable of abstracting various tasks as different combinations of multiple attributes. This perspective of compositionality is vital for human rapid learning and adaption since previous experiences from related tasks can be combined to generalize across novel compositional settings. In this work, we aim to achieve zero-shot policy generalization of Reinforcement Learning (RL) agents by leveraging the task compositionality. Our proposed method is a meta-RL algorithm with disentangled task representation, explicitly encoding different aspects of the tasks. Policy generalization is then performed by inferring unseen compositional task representations via the obtained disentanglement without extra exploration. The evaluation is conducted on three simulated tasks and a challenging real-world robotic insertion task. Experimental results demonstrate that our proposed method achieves policy generalization to unseen compositional tasks in a zero-shot manner.",
      "authors": [
        "Zheng Wu",
        "Yichen Xie",
        "Wenzhao Lian",
        "Changhao Wang",
        "Yanjiang Guo",
        "Jianyu Chen",
        "S. Schaal",
        "M. Tomizuka"
      ],
      "year": 2022,
      "citation_count": 11,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/83f1343500c9f0df62da0d61736738d8d7a9bba0",
      "pdf_link": "",
      "venue": "IEEE International Conference on Robotics and Automation",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "06318722ebbac1d9b59e2408ecd3994eadb0ec6b",
      "title": "Deep Reinforcement Learning Task Assignment Based on Domain Knowledge",
      "abstract": "Deep Reinforcement Learning (DRL) methods are inefficient in the initial strategy exploration process due to the huge state space and action space in large-scale complex scenarios. This is becoming one of the bottlenecks in their application to large-scale game adversarial scenarios. This paper proposes a Safe reinforcement learning combined with Imitation learning for Task Assignment (SITA) method for a representative red-blue game confrontation scenario. Aiming at the problem of difficult sampling of Imitation Learning (IL), this paper combines human knowledge with adversarial rules to build a knowledge rule base; We propose the Imitation Learning with the Decoupled Network (ILDN) pre-training method to solve the problem of excessive initial invalid exploration; In order to reduce invalid exploration and improve the stability in the later stages of training, we incorporate Safe Reinforcement Learning (Safe RL) method after pre-training. Finally, we verified in the digital battlefield that the SITA method has higher training efficiency and strong generalization ability in large-scale complex scenarios.",
      "authors": [
        "Jiayi Liu",
        "Gang Wang",
        "Xiangke Guo",
        "Siyuan Wang",
        "Qiang Fu"
      ],
      "year": 2022,
      "citation_count": 10,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/06318722ebbac1d9b59e2408ecd3994eadb0ec6b",
      "pdf_link": "",
      "venue": "IEEE Access",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "1f577f6f0785c7967bef4ac6d0ab0370c2815b4d",
      "title": "Occupancy Reward-Driven Exploration with Deep Reinforcement Learning for Mobile Robot System",
      "abstract": "This paper investigates the solution to a mobile-robot exploration problem following autonomous driving principles. The exploration task is formulated in this study as a process of building a map while a robot moves in an indoor environment beginning from full uncertainties. The sequence of robot decisions of how to move defines the strategy of the exploration that this paper aims to investigate, applying one of the Deep Reinforcement Learning methods, known as the Deep Deterministic Policy Gradient (DDPG) algorithm. A custom environment is created representing the mapping process with a map visualization, a robot model, and a reward function. The actor-critic network receives and sends input and output data, respectively, to the custom environment. The input is the data from the laser sensor, which is equipped on the robot. The output is the continuous actions of the robot in terms of linear and angular velocities. The training results of this study show the strengths and weaknesses of the DDPG algorithm for the robotic mapping problem. The implementation was developed in MATLAB platform using its corresponding toolboxes. A comparison with another exploration algorithm is also provided.",
      "authors": [
        "A. Kamalova",
        "Suk-Gyu Lee",
        "Soon-H. Kwon"
      ],
      "year": 2022,
      "citation_count": 10,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/1f577f6f0785c7967bef4ac6d0ab0370c2815b4d",
      "pdf_link": "",
      "venue": "Applied Sciences",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "807f377de905eda62e4cd2f0797153a59296adbb",
      "title": "Partial Off-policy Learning: Balance Accuracy and Diversity for Human-Oriented Image Captioning",
      "abstract": "Human-oriented image captioning with both high diversity and accuracy is a challenging task in vision+language modeling. The reinforcement learning (RL) based frameworks promote the accuracy of image captioning, yet seriously hurt the diversity. In contrast, other methods based on variational auto-encoder (VAE) or generative adversarial network (GAN) can produce diverse yet less accurate captions. In this work, we devote our attention to promote the diversity of RL-based image captioning. To be specific, we devise a partial off-policy learning scheme to balance accuracy and diversity. First, we keep the model exposed to varied candidate captions by sampling from the initial state before RL launched. Second, a novel criterion named max-CIDEr is proposed to serve as the reward for promoting diversity. We combine the above-mentioned offpolicy strategy with the on-policy one to moderate the exploration effect, further balancing the diversity and accuracy for human-like image captioning. Experiments show that our method locates the closest to human performance in the diversity-accuracy space, and achieves the highest Pearson correlation as 0.337 with human performance.",
      "authors": [
        "Jiahe Shi",
        "Yali Li",
        "Shengjin Wang"
      ],
      "year": 2021,
      "citation_count": 12,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/807f377de905eda62e4cd2f0797153a59296adbb",
      "pdf_link": "",
      "venue": "IEEE International Conference on Computer Vision",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "f715558b65fd4f3c6966505c237d9a622947010b",
      "title": "Sample Efficient Reinforcement Learning Method via High Efficient Episodic Memory",
      "abstract": "Reinforcement Learning (RL), especially Deep Reinforcement Learning (DRL), has made great progress in many areas, such as robots, video games and driving. However, sample inefficiency is a big obstacle to the widespread practical application of DRL. Inspired by the decision making in human brain, this problem can be solved by incorporating instance based learning, i.e. episodic memory. Many episodic memory based RL algorithms have emerged recently. However, these algorithms either only replace parametric DRL algorithm with episodic control or incorporate episodic memory in a single component of DRL. In contrast to preview works, this paper proposes a new sample-efficient reinforcement learning architecture which introduces a new episodic memory module and incorporates episodic thought into some key components of DRL: exploration, experience replay and loss function. Taking Deep Q-Network (DQN) algorithm for example, when combined with DQN, our algorithm is called High Efficient Episodic Memory DQN (HE-EMDQN). In HE-EMDQN, a new non-parametric episodic memory module is introduced to help calculate the loss and modify the predicted value for exploration. For the sake of accelerating the sample learning in experience replay, an auxiliary small buffer called percentile best episode replay memory is designed to compose a mixed mini-batch. We show across the testing environments that our algorithm is significantly more powerful and sample-efficient than DQN and the recent episodic memory deep q-network (EMDQN). This work provides a new perspective for other RL algorithms to improve sample efficiency by utilising episodic memory efficiently.",
      "authors": [
        "Dujia Yang",
        "Xiaowei Qin",
        "Xiaodong Xu",
        "Chensheng Li",
        "Guo Wei"
      ],
      "year": 2020,
      "citation_count": 15,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/f715558b65fd4f3c6966505c237d9a622947010b",
      "pdf_link": "",
      "venue": "IEEE Access",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "117cd9e1dafc577e53e2d46897a784ed1e65996f",
      "title": "Adaptive Exploration Strategy With Multi-Attribute Decision-Making for Reinforcement Learning",
      "abstract": "Reinforcement Learning (RL) agents often encounter the bottleneck of the performance when the dilemma of exploration and exploitation arises. In this study, an adaptive exploration strategy with multi-attribute decision-making is proposed to address the trade-off problem between exploration and exploitation. Firstly, the proposed method decomposes a complex task into several sub-tasks and trains each sub-task using the same training method individually. Then, the proposed method uses a multi-attribute decision-making method to develop an action policy integrating the training results of these trained sub-tasks. There are practical advantages to improve learning performance by allowing multiple learners to learn in parallel. An adaptive exploration strategy determines the probability of exploration depending on the information entropy instead of the suffocating work of empirical tuning. Finally, transfer learning extends the applicability of the proposed method. The experiment of the robot migration, the robot confrontation, and the real wheeled mobile robot are used to demonstrate the availability and practicability of the proposed method.",
      "authors": [
        "Chunyang Hu",
        "Meng Xu"
      ],
      "year": 2020,
      "citation_count": 14,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/117cd9e1dafc577e53e2d46897a784ed1e65996f",
      "pdf_link": "",
      "venue": "IEEE Access",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "071de005741bd666c7a9ccf40d5ed1d502f5282b",
      "title": "Curiosity-Driven Exploration for Off-Policy Reinforcement Learning Methods*",
      "abstract": "Deep reinforcement learning (DRL) has achieved remarkable results in many high-dimensional continuous control tasks. However, the RL agent still explores the environment randomly, resulting in low exploration efficiency and learning performance, especially in robotic manipulation tasks with sparse rewards. To address this problem, in this paper, we intro-duce a simplified Intrinsic Curiosity Module (S-ICM) into the off-policy RL methods to encourage the agent to pursue novel and surprising states for improving the exploration competence. This method can be combined with an arbitrary off-policy RL algorithm. We evaluate our approach on three challenging robotic manipulation tasks provided by OpenAI Gym. In our experiments, we combined our method with Deep Deterministic Policy Gradient (DDPG) with and without Hindsight Experience Replay (HER). The empirical results show that our proposed method significantly outperforms vanilla RL algorithms both in sample-efficiency and learning performance.",
      "authors": [
        "Boyao Li",
        "Tao Lu",
        "Jiayi Li",
        "N. Lu",
        "Yinghao Cai",
        "Shuo Wang"
      ],
      "year": 2019,
      "citation_count": 14,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/071de005741bd666c7a9ccf40d5ed1d502f5282b",
      "pdf_link": "",
      "venue": "IEEE International Conference on Robotics and Biomimetics",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "3f673101c2cac3b47639056e2988e018546c3c90",
      "title": "Zeroth-Order Supervised Policy Improvement",
      "abstract": "Despite the remarkable progress made by the policy gradient algorithms in reinforcement learning (RL), sub-optimal policies usually result from the local exploration property of the policy gradient update. In this work, we propose a method referred to as Zeroth-Order Supervised Policy Improvement (ZOSPI) that exploits the estimated value function Q globally while preserves the local exploitation of the policy gradient methods. We prove that with a good function structure, the zeroth-order optimization strategy combining both local and global samplings can find the global minima within a polynomial number of samples. To improve the exploration efficiency in unknown environments, ZOSPI is further combined with bootstrapped Q networks. Different from the standard policy gradient methods, the policy learning of ZOSPI is conducted in a self-supervision manner so that the policy can be implemented with gradient-free non-parametric models besides the neural network approximator. Experiments show that ZOSPI achieves competitive results on MuJoCo locomotion tasks with a remarkable sample efficiency.",
      "authors": [
        "Hao Sun",
        "Ziping Xu",
        "Yuhang Song",
        "Meng Fang",
        "Jiechao Xiong",
        "Bo Dai",
        "Zhengyou Zhang",
        "Bolei Zhou"
      ],
      "year": 2020,
      "citation_count": 10,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/3f673101c2cac3b47639056e2988e018546c3c90",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "bd38cbbb346a347cb5b60ac4a133b3d73cb44e07",
      "title": "Efficient Online Reinforcement Learning with Offline Data",
      "abstract": "Sample efficiency and exploration remain major challenges in online reinforcement learning (RL). A powerful approach that can be applied to address these issues is the inclusion of offline data, such as prior trajectories from a human expert or a sub-optimal exploration policy. Previous methods have relied on extensive modifications and additional complexity to ensure the effective use of this data. Instead, we ask: can we simply apply existing off-policy methods to leverage offline data when learning online? In this work, we demonstrate that the answer is yes; however, a set of minimal but important changes to existing off-policy RL algorithms are required to achieve reliable performance. We extensively ablate these design choices, demonstrating the key factors that most affect performance, and arrive at a set of recommendations that practitioners can readily apply, whether their data comprise a small number of expert demonstrations or large volumes of sub-optimal trajectories. We see that correct application of these simple recommendations can provide a $\\mathbf{2.5\\times}$ improvement over existing approaches across a diverse set of competitive benchmarks, with no additional computational overhead. We have released our code at https://github.com/ikostrikov/rlpd.",
      "authors": [
        "Philip J. Ball",
        "Laura M. Smith",
        "Ilya Kostrikov",
        "S. Levine"
      ],
      "year": 2023,
      "citation_count": 220,
      "layer": 1,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/bd38cbbb346a347cb5b60ac4a133b3d73cb44e07",
      "pdf_link": "",
      "venue": "International Conference on Machine Learning",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "1b1efa2f9731ab3801c46bfc877695d41e437406",
      "title": "An Online Reinforcement Learning-Based Energy Management Strategy for Microgrids With Centralized Control",
      "abstract": "To address the issue of significant unpredictability and intermittent nature of renewable energy sources, particularly wind and solar power, this paper introduces a novel optimization model based on online reinforcement learning. Initially, an energy management optimization model is designed to achieve plan adherence and minimize energy storage (ES) operation costs, taking into account the inherent challenges of wind power-photovoltaic energy storage systems (WPESS). An online reinforcement learning framework is employed, which defines various state variables, action variables, and reward functions for the energy management optimization model. The state-action-reward-state-action (SARSA) algorithm is applied to learn the joint scheduling strategy for the microgrid system, utilizing its iterative exploration mechanisms and interaction with the environment. This strategy aims to accomplish the goals of effective power tracking and reduction of storage charging and discharging. The proposed method's effectiveness is validated using a residential community with electric vehicle (EV) charging loads as a test case. Numerical analyses demonstrate that the approach is not reliant on traditional mathematical models and adapts well to the uncertainties and complex constraints of the WPESS, maintaining a low-cost requirement while achieving computational efficiency significantly higher than that of the model predictive control (MPC) and deep Q-network (DQN) algorithm.",
      "authors": [
        "Qing-ran Meng",
        "Sheharyar Hussain",
        "Fengzhang Luo",
        "Zhongguan Wang",
        "Xiaolong Jin"
      ],
      "year": 2025,
      "citation_count": 62,
      "layer": 1,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/1b1efa2f9731ab3801c46bfc877695d41e437406",
      "pdf_link": "",
      "venue": "IEEE transactions on industry applications",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "08e84c939b88fc50aaa74ef76e202e61a1ad940b",
      "title": "StepCoder: Improve Code Generation with Reinforcement Learning from Compiler Feedback",
      "abstract": "The advancement of large language models (LLMs) has significantly propelled the field of code generation. Previous work integrated reinforcement learning (RL) with compiler feedback for exploring the output space of LLMs to enhance code generation quality. However, the lengthy code generated by LLMs in response to complex human requirements makes RL exploration a challenge. Also, since the unit tests may not cover the complicated code, optimizing LLMs by using these unexecuted code snippets is ineffective. To tackle these challenges, we introduce StepCoder, a novel RL framework for code generation, consisting of two main components: CCCS addresses the exploration challenge by breaking the long sequences code generation task into a Curriculum of Code Completion Subtasks, while FGO only optimizes the model by masking the unexecuted code segments to provide Fine-Grained Optimization. In addition, we furthermore construct the APPS+ dataset for RL training, which is manually verified to ensure the correctness of unit tests. Experimental results show that our method improves the ability to explore the output space and outperforms state-of-the-art approaches in corresponding benchmarks. Our dataset APPS+ and StepCoder are available online.",
      "authors": [
        "Shihan Dou",
        "Yan Liu",
        "Haoxiang Jia",
        "Limao Xiong",
        "Enyu Zhou",
        "Junjie Shan",
        "Caishuang Huang",
        "Wei Shen",
        "Xiaoran Fan",
        "Zhiheng Xi",
        "Yuhao Zhou",
        "Tao Ji",
        "Rui Zheng",
        "Qi Zhang",
        "Xuanjing Huang",
        "Tao Gui"
      ],
      "year": 2024,
      "citation_count": 61,
      "layer": 1,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/08e84c939b88fc50aaa74ef76e202e61a1ad940b",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "5bac7d00035bc1e246a34f9ee3152b290f97bb92",
      "title": "Supervised Pretraining Can Learn In-Context Reinforcement Learning",
      "abstract": "Large transformer models trained on diverse datasets have shown a remarkable ability to learn in-context, achieving high few-shot performance on tasks they were not explicitly trained to solve. In this paper, we study the in-context learning capabilities of transformers in decision-making problems, i.e., reinforcement learning (RL) for bandits and Markov decision processes. To do so, we introduce and study Decision-Pretrained Transformer (DPT), a supervised pretraining method where the transformer predicts an optimal action given a query state and an in-context dataset of interactions, across a diverse set of tasks. This procedure, while simple, produces a model with several surprising capabilities. We find that the pretrained transformer can be used to solve a range of RL problems in-context, exhibiting both exploration online and conservatism offline, despite not being explicitly trained to do so. The model also generalizes beyond the pretraining distribution to new tasks and automatically adapts its decision-making strategies to unknown structure. Theoretically, we show DPT can be viewed as an efficient implementation of Bayesian posterior sampling, a provably sample-efficient RL algorithm. We further leverage this connection to provide guarantees on the regret of the in-context algorithm yielded by DPT, and prove that it can learn faster than algorithms used to generate the pretraining data. These results suggest a promising yet simple path towards instilling strong in-context decision-making abilities in transformers.",
      "authors": [
        "Jonathan Lee",
        "Annie Xie",
        "Aldo Pacchiano",
        "Yash Chandak",
        "Chelsea Finn",
        "Ofir Nachum",
        "E. Brunskill"
      ],
      "year": 2023,
      "citation_count": 110,
      "layer": 1,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/5bac7d00035bc1e246a34f9ee3152b290f97bb92",
      "pdf_link": "",
      "venue": "Neural Information Processing Systems",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "043582a1ed2d2b7d08a804bafe9db188e9a65d96",
      "title": "Neural Network Model-Based Reinforcement Learning Control for AUV 3-D Path Following",
      "abstract": "Autonomous underwater vehicles (AUVs) have become important tools in the ocean exploration and have drawn considerable attention. Precise control for AUVs is the prerequisite to effectively execute underwater tasks. However, the classical control methods such as model predictive control (MPC) rely heavily on the dynamics model of the controlled system which is difficult to obtain for AUVs. To address this issue, a new reinforcement learning (RL) framework for AUV path-following control is proposed in this article. Specifically, we propose a novel actor-model-critic (AMC) architecture integrating a neural network model with the traditional actor-critic architecture. The neural network model is designed to learn the state transition function to explore the spatio-temporal change patterns of the AUV as well as the surrounding environment. Based on the AMC architecture, a RL-based controller agent named ModelPPO is constructed to control the AUV. With the required sailing speed achieved by a traditional proportional-integral (PI) controller, ModelPPO can control the rudder and elevator fins so that the AUV follows the desired path. Finally, a simulation platform is built to evaluate the performance of the proposed method that is compared with MPC and other RL-based methods. The obtained results demonstrate that the proposed method can achieve better performance than other methods, which demonstrate the great potential of the advanced artificial intelligence methods in solving the traditional motion control problems for intelligent vehicles.",
      "authors": [
        "D. Ma",
        "Xi Chen",
        "Weihao Ma",
        "Huarong Zheng",
        "Fengzhong Qu"
      ],
      "year": 2024,
      "citation_count": 51,
      "layer": 1,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/043582a1ed2d2b7d08a804bafe9db188e9a65d96",
      "pdf_link": "",
      "venue": "IEEE Transactions on Intelligent Vehicles",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "139a84c6fc4887ce2374489d79af0df9e1e7e4d6",
      "title": "Craftax: A Lightning-Fast Benchmark for Open-Ended Reinforcement Learning",
      "abstract": "Benchmarks play a crucial role in the development and analysis of reinforcement learning (RL) algorithms. We identify that existing benchmarks used for research into open-ended learning fall into one of two categories. Either they are too slow for meaningful research to be performed without enormous computational resources, like Crafter, NetHack and Minecraft, or they are not complex enough to pose a significant challenge, like Minigrid and Procgen. To remedy this, we first present Craftax-Classic: a ground-up rewrite of Crafter in JAX that runs up to 250x faster than the Python-native original. A run of PPO using 1 billion environment interactions finishes in under an hour using only a single GPU and averages 90% of the optimal reward. To provide a more compelling challenge we present the main Craftax benchmark, a significant extension of the Crafter mechanics with elements inspired from NetHack. Solving Craftax requires deep exploration, long term planning and memory, as well as continual adaptation to novel situations as more of the world is discovered. We show that existing methods including global and episodic exploration, as well as unsupervised environment design fail to make material progress on the benchmark. We believe that Craftax can for the first time allow researchers to experiment in a complex, open-ended environment with limited computational resources.",
      "authors": [
        "Michael Matthews",
        "Michael Beukman",
        "Benjamin Ellis",
        "Mikayel Samvelyan",
        "Matthew Jackson",
        "Samuel Coward",
        "Jakob Foerster"
      ],
      "year": 2024,
      "citation_count": 47,
      "layer": 1,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/139a84c6fc4887ce2374489d79af0df9e1e7e4d6",
      "pdf_link": "",
      "venue": "International Conference on Machine Learning",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "26662adf92cacf0810a14faa514360f270e97b53",
      "title": "A Lightweight Reinforcement-Learning-Based Real-Time Path-Planning Method for Unmanned Aerial Vehicles",
      "abstract": "The unmanned aerial vehicles (UAVs) are competent to perform a variety of applications, possessing great potential and promise. The deep neural networks (DNN) technology has enabled the UAV-assisted paradigm, accelerated the construction of smart cities, and propelled the development of the Internet of Things (IoT). UAVs play an increasingly important role in various applications, such as surveillance, environmental monitoring, emergency rescue, and supplies delivery, for which a robust path-planning technique is the foundation and prerequisite. However, existing methods lack comprehensive consideration of the complicated urban environment and do not provide an overall assessment of the robustness and generalization. Meanwhile, due to the resource constraints and hardware limitations of UAVs, the complexity of deploying the network needs to be reduced. This article proposes a lightweight, reinforcement-learning-based (RL-based) real-time path-planning method for UAVs, adaptive SAC (ASAC) algorithm, which optimizing the training process, network architecture, and algorithmic models. First of all, we establish a framework of global training and local adaptation, where the structured environment model is constructed for interaction, and local dynamically varying information aids in improving generalization. Second, ASAC introduces a cross-layer connection approach that passes the original state information into the higher layers to avoid feature loss and improve learning efficiency. Finally, we propose an adaptive temperature coefficient, which flexibly adjusts the exploration probability of UAVs with the training phase and experience data accumulation. In addition, a series of comparison experiments have been conducted in conjunction with practical application requirements, and the results have fully proved the favorable superiority of ASAC.",
      "authors": [
        "Meng Xi",
        "Huiao Dai",
        "Jingyi He",
        "Wenjie Li",
        "Jiabao Wen",
        "Shuai Xiao",
        "Jiachen Yang"
      ],
      "year": 2024,
      "citation_count": 44,
      "layer": 1,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/26662adf92cacf0810a14faa514360f270e97b53",
      "pdf_link": "",
      "venue": "IEEE Internet of Things Journal",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "7d6168fbd3ed72f9098573007f4b8c2ec9e576b9",
      "title": "Training Large Language Models for Reasoning through Reverse Curriculum Reinforcement Learning",
      "abstract": "In this paper, we propose R$^3$: Learning Reasoning through Reverse Curriculum Reinforcement Learning (RL), a novel method that employs only outcome supervision to achieve the benefits of process supervision for large language models. The core challenge in applying RL to complex reasoning is to identify a sequence of actions that result in positive rewards and provide appropriate supervision for optimization. Outcome supervision provides sparse rewards for final results without identifying error locations, whereas process supervision offers step-wise rewards but requires extensive manual annotation. R$^3$ overcomes these limitations by learning from correct demonstrations. Specifically, R$^3$ progressively slides the start state of reasoning from a demonstration's end to its beginning, facilitating easier model exploration at all stages. Thus, R$^3$ establishes a step-wise curriculum, allowing outcome supervision to offer step-level signals and precisely pinpoint errors. Using Llama2-7B, our method surpasses RL baseline on eight reasoning tasks by $4.1$ points on average. Notebaly, in program-based reasoning on GSM8K, it exceeds the baseline by $4.2$ points across three backbone models, and without any extra data, Codellama-7B + R$^3$ performs comparable to larger models or closed-source models.",
      "authors": [
        "Zhiheng Xi",
        "Wenxiang Chen",
        "Boyang Hong",
        "Senjie Jin",
        "Rui Zheng",
        "Wei He",
        "Yiwen Ding",
        "Shichun Liu",
        "Xin Guo",
        "Junzhe Wang",
        "Honglin Guo",
        "Wei Shen",
        "Xiaoran Fan",
        "Yuhao Zhou",
        "Shihan Dou",
        "Xiao Wang",
        "Xinbo Zhang",
        "Peng Sun",
        "Tao Gui",
        "Qi Zhang",
        "Xuanjing Huang"
      ],
      "year": 2024,
      "citation_count": 39,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/7d6168fbd3ed72f9098573007f4b8c2ec9e576b9",
      "pdf_link": "",
      "venue": "International Conference on Machine Learning",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "a694a3fb49f3c6e63e2fa5ddb72fe3aa490592f1",
      "title": "Toward Enhanced Reinforcement Learning-Based Resource Management via Digital Twin: Opportunities, Applications, and Challenges",
      "abstract": "This article presents a digital twin (DT)-enhanced reinforcement learning (RL) framework aimed at optimizing performance and reliability in network resource management, since the traditional RL methods face several unified challenges when applied to physical networks, including limited exploration efficiency, slow convergence, poor long-term performance, and safety concerns during the exploration phase. To deal with the above challenges, a comprehensive DT-based framework is proposed to enhance the convergence speed and performance for unified RL-based resource management. The proposed framework provides safe action exploration, more accurate estimates of long-term returns, faster training convergence, higher convergence performance, and real-time adaptation to varying network conditions. Then, two case studies on ultra-reliable and low-latency communication (URLLC) services and multiple unmanned aerial vehicles (UAV) network are presented, demonstrating improvements of the proposed framework in performance, convergence speed, and training cost reduction both on traditional RL and neural network based Deep RL (DRL). Finally, the article identifies and explores some of the research challenges and open issues in this rapidly evolving field.",
      "authors": [
        "Nan Cheng",
        "Xiucheng Wang",
        "Zan Li",
        "Zhisheng Yin",
        "Tom H. Luan",
        "Xuemin Shen"
      ],
      "year": 2024,
      "citation_count": 21,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/a694a3fb49f3c6e63e2fa5ddb72fe3aa490592f1",
      "pdf_link": "",
      "venue": "IEEE Network",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "4e98282f5f3f1a388b8d95380473d4ef4878266e",
      "title": "DrM: Mastering Visual Reinforcement Learning through Dormant Ratio Minimization",
      "abstract": "Visual reinforcement learning (RL) has shown promise in continuous control tasks. Despite its progress, current algorithms are still unsatisfactory in virtually every aspect of the performance such as sample efficiency, asymptotic performance, and their robustness to the choice of random seeds. In this paper, we identify a major shortcoming in existing visual RL methods that is the agents often exhibit sustained inactivity during early training, thereby limiting their ability to explore effectively. Expanding upon this crucial observation, we additionally unveil a significant correlation between the agents' inclination towards motorically inactive exploration and the absence of neuronal activity within their policy networks. To quantify this inactivity, we adopt dormant ratio as a metric to measure inactivity in the RL agent's network. Empirically, we also recognize that the dormant ratio can act as a standalone indicator of an agent's activity level, regardless of the received reward signals. Leveraging the aforementioned insights, we introduce DrM, a method that uses three core mechanisms to guide agents' exploration-exploitation trade-offs by actively minimizing the dormant ratio. Experiments demonstrate that DrM achieves significant improvements in sample efficiency and asymptotic performance with no broken seeds (76 seeds in total) across three continuous control benchmark environments, including DeepMind Control Suite, MetaWorld, and Adroit. Most importantly, DrM is the first model-free algorithm that consistently solves tasks in both the Dog and Manipulator domains from the DeepMind Control Suite as well as three dexterous hand manipulation tasks without demonstrations in Adroit, all based on pixel observations.",
      "authors": [
        "Guowei Xu",
        "Ruijie Zheng",
        "Yongyuan Liang",
        "Xiyao Wang",
        "Zhecheng Yuan",
        "Tianying Ji",
        "Yu Luo",
        "Xiaoyu Liu",
        "Jiaxin Yuan",
        "Pu Hua",
        "Shuzhen Li",
        "Yanjie Ze",
        "Hal Daum'e",
        "Furong Huang",
        "Huazhe Xu"
      ],
      "year": 2023,
      "citation_count": 39,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/4e98282f5f3f1a388b8d95380473d4ef4878266e",
      "pdf_link": "",
      "venue": "International Conference on Learning Representations",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "4a3e88d203564e547f5fb3f3d816a0b381492eae",
      "title": "VLA-RL: Towards Masterful and General Robotic Manipulation with Scalable Reinforcement Learning",
      "abstract": "Recent high-capacity vision-language-action (VLA) models have demonstrated impressive performance on a range of robotic manipulation tasks by imitating human demonstrations. However, exploiting offline data with limited visited states will cause execution failure in out-of-distribution scenarios. Intuitively, an exploration-based method that improves on online collected data at test time could address this limitation. We present VLA-RL, an algorithmic and systematic framework that leverages online reinforcement learning (RL) to improve pretrained auto-regressive VLAs in downstream tasks. Within a unified perspective, we first introduce a trajectory-level RL formulation for auto-regressive VLA training, which models general robotic manipulation trajectory as multi-modal multi-turn conversation. To address the challenge of sparse rewards, we fine-tune a pretrained vision-language model as a robotic process reward model, which is trained on pseudo reward labels annotated on automatically extracted task segments. To scale up, we identify several implementation findings that improve the stability and efficiency including curriculum selection strategy, GPU-balanced vectorized environments, batch decoding, and critic warmup. VLA-RL enables OpenVLA-7B to surpass the strongest finetuned baseline by 4.5% on 40 challenging robotic manipulation tasks in LIBERO, and even matches the performance of advanced commercial models such as $\\pi_0$-FAST. Notably, we observe that VLA-RL benefits from increased test-time optimization, indicating an early spark of inference scaling laws in robotics.",
      "authors": [
        "Guanxing Lu",
        "Wenkai Guo",
        "Chubin Zhang",
        "Yuheng Zhou",
        "Hao Jiang",
        "Zifeng Gao",
        "Yansong Tang",
        "Ziwei Wang"
      ],
      "year": 2025,
      "citation_count": 18,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/4a3e88d203564e547f5fb3f3d816a0b381492eae",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "1a73038804052a40c12aae696848ece2168f6da7",
      "title": "On the Importance of Exploration for Generalization in Reinforcement Learning",
      "abstract": "Existing approaches for improving generalization in deep reinforcement learning (RL) have mostly focused on representation learning, neglecting RL-specific aspects such as exploration. We hypothesize that the agent's exploration strategy plays a key role in its ability to generalize to new environments. Through a series of experiments in a tabular contextual MDP, we show that exploration is helpful not only for efficiently finding the optimal policy for the training environments but also for acquiring knowledge that helps decision making in unseen environments. Based on these observations, we propose EDE: Exploration via Distributional Ensemble, a method that encourages exploration of states with high epistemic uncertainty through an ensemble of Q-value distributions. Our algorithm is the first value-based approach to achieve state-of-the-art on both Procgen and Crafter, two benchmarks for generalization in RL with high-dimensional observations. The open-sourced implementation can be found at https://github.com/facebookresearch/ede .",
      "authors": [
        "Yiding Jiang",
        "J. Z. Kolter",
        "R. Raileanu"
      ],
      "year": 2023,
      "citation_count": 32,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/1a73038804052a40c12aae696848ece2168f6da7",
      "pdf_link": "",
      "venue": "Neural Information Processing Systems",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "03c7f9a6b743eb1c9127b8d4552147b2b1a0c1e1",
      "title": "Demonstration-Guided Reinforcement Learning with Efficient Exploration for Task Automation of Surgical Robot",
      "abstract": "Task automation of surgical robot has the potentials to improve surgical efficiency. Recent reinforcement learning (RL) based approaches provide scalable solutions to surgical automation, but typically require extensive data collection to solve a task if no prior knowledge is given. This issue is known as the exploration challenge, which can be alleviated by providing expert demonstrations to an RL agent. Yet, how to make effective use of demonstration data to improve exploration efficiency still remains an open challenge. In this work, we introduce Demonstration-guided EXploration (DEX), an efficient reinforcement learning algorithm that aims to overcome the exploration problem with expert demonstrations for surgical automation. To effectively exploit demonstrations, our method estimates expert-like behaviors with higher values to facilitate productive interactions, and adopts non-parametric regression to enable such guidance at states unobserved in demonstration data. Extensive experiments on 10 surgical manipulation tasks from SurRoL, a comprehensive surgical simulation platform, demonstrate significant improvements in the exploration efficiency and task success rates of our method. Moreover, we also deploy the learned policies to the da Vinci Research Kit (dVRK) platform to show the effectiveness on the real robot. Code is available at https://github.com/med-air/DEX.",
      "authors": [
        "Tao Huang",
        "Kai Chen",
        "Bin Li",
        "Yunhui Liu",
        "Qingxu Dou"
      ],
      "year": 2023,
      "citation_count": 26,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/03c7f9a6b743eb1c9127b8d4552147b2b1a0c1e1",
      "pdf_link": "",
      "venue": "IEEE International Conference on Robotics and Automation",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "c90aa0f206c6fd41c490c142f63f7ba046cae6b7",
      "title": "Provable and Practical: Efficient Exploration in Reinforcement Learning via Langevin Monte Carlo",
      "abstract": "We present a scalable and effective exploration strategy based on Thompson sampling for reinforcement learning (RL). One of the key shortcomings of existing Thompson sampling algorithms is the need to perform a Gaussian approximation of the posterior distribution, which is not a good surrogate in most practical settings. We instead directly sample the Q function from its posterior distribution, by using Langevin Monte Carlo, an efficient type of Markov Chain Monte Carlo (MCMC) method. Our method only needs to perform noisy gradient descent updates to learn the exact posterior distribution of the Q function, which makes our approach easy to deploy in deep RL. We provide a rigorous theoretical analysis for the proposed method and demonstrate that, in the linear Markov decision process (linear MDP) setting, it has a regret bound of $\\tilde{O}(d^{3/2}H^{3/2}\\sqrt{T})$, where $d$ is the dimension of the feature mapping, $H$ is the planning horizon, and $T$ is the total number of steps. We apply this approach to deep RL, by using Adam optimizer to perform gradient updates. Our approach achieves better or similar results compared with state-of-the-art deep RL algorithms on several challenging exploration tasks from the Atari57 suite.",
      "authors": [
        "Haque Ishfaq",
        "Qingfeng Lan",
        "Pan Xu",
        "A. Mahmood",
        "Doina Precup",
        "Anima Anandkumar",
        "K. Azizzadenesheli"
      ],
      "year": 2023,
      "citation_count": 24,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/c90aa0f206c6fd41c490c142f63f7ba046cae6b7",
      "pdf_link": "",
      "venue": "International Conference on Learning Representations",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "53db22a9d4ae77dd8218ba867184898adc84d1d1",
      "title": "Sample Efficient Offline-to-Online Reinforcement Learning",
      "abstract": "Offline reinforcement learning (RL) makes it possible to train the agents entirely from a previously collected dataset. However, constrained by the quality of the offline dataset, offline RL agents typically have limited performance and cannot be directly deployed. Thus, it is desirable to further finetune the pretrained offline RL agents via online interactions with the environment. Existing offline-to-online RL algorithms suffer from the low sample efficiency issue, due to two inherent challenges, i.e., exploration limitation and distribution shift. To this end, we propose a sample-efficient offline-to-online RL algorithm via Optimistic Exploration and Meta Adaptation (OEMA). Specifically, we first propose an optimistic exploration strategy according to the principle of optimism in the face of uncertainty. This allows agents to sufficiently explore the environment in a stable manner. Moreover, we propose a meta learning based adaptation method, which can reduce the distribution shift and accelerate the offline-to-online adaptation process. We empirically demonstrate that OEMA improves the sample efficiency on D4RL benchmark. Besides, we provide in-depth analyses to verify the effectiveness of both optimistic exploration and meta adaptation.",
      "authors": [
        "Siyuan Guo",
        "Lixin Zou",
        "Hechang Chen",
        "B. Qu",
        "Haotian Chi",
        "Philip S. Yu",
        "Yi Chang"
      ],
      "year": 2024,
      "citation_count": 12,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/53db22a9d4ae77dd8218ba867184898adc84d1d1",
      "pdf_link": "",
      "venue": "IEEE Transactions on Knowledge and Data Engineering",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "81fdf2b3db2b3e7be90b51866a31b73587eecd30",
      "title": "A Novel Reinforcement Learning-Based Robust Control Strategy for a Quadrotor",
      "abstract": "In this article, a novel reinforcement learning (RL)-based robust control approach is proposed for quadrotors, which guarantees efficient learning and satisfactory tracking performance by simultaneously evaluating the RL and the baseline method in training. Different from existing works, the key novelty is to design a practice-reliable RL control framework for quadrotors in a two-part cooperative manner. In the first part, based on the hierarchical property, a new robust integral of the signum of the error (RISE) design is proposed to ensure asymptotic convergence, which includes the nonlinear and the disturbance rejection terms. In the second part, a one-actor-dual-critic (OADC) learning framework is proposed, where the designed switching logic in the first part works as a benchmark to guide the learning. Specifically, the two critics independently evaluate the RL policy and the switching logic simultaneously, which are utilized for policy update, only when both are positive, corresponding to the remarkable actor-better exploration actions. The asymptotic RISE controller, together with the two critics in OADC learning framework, guarantees accurate judgment on every exploration. On this basis, the satisfactory performance of the RL policy is guaranteed by the actor-better exploration based learning while the chattering problem arisen from the switching logic is addressed completely. Plenty of comparative experimental tests are presented to illustrate the superior performance of the proposed RL controller in terms of tracking accuracy and robustness.",
      "authors": [
        "Hean Hua",
        "Yongchun Fang"
      ],
      "year": 2023,
      "citation_count": 23,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/81fdf2b3db2b3e7be90b51866a31b73587eecd30",
      "pdf_link": "",
      "venue": "IEEE transactions on industrial electronics (1982. Print)",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "e4fef8d5864c5468100ca167639ef3fa374c0442",
      "title": "MaxInfoRL: Boosting exploration in reinforcement learning through information gain maximization",
      "abstract": "Reinforcement learning (RL) algorithms aim to balance exploiting the current best strategy with exploring new options that could lead to higher rewards. Most common RL algorithms use undirected exploration, i.e., select random sequences of actions. Exploration can also be directed using intrinsic rewards, such as curiosity or model epistemic uncertainty. However, effectively balancing task and intrinsic rewards is challenging and often task-dependent. In this work, we introduce a framework, MaxInfoRL, for balancing intrinsic and extrinsic exploration. MaxInfoRL steers exploration towards informative transitions, by maximizing intrinsic rewards such as the information gain about the underlying task. When combined with Boltzmann exploration, this approach naturally trades off maximization of the value function with that of the entropy over states, rewards, and actions. We show that our approach achieves sublinear regret in the simplified setting of multi-armed bandits. We then apply this general formulation to a variety of off-policy model-free RL methods for continuous state-action spaces, yielding novel algorithms that achieve superior performance across hard exploration problems and complex scenarios such as visual control tasks.",
      "authors": [
        "Bhavya Sukhija",
        "Stelian Coros",
        "Andreas Krause",
        "Pieter Abbeel",
        "Carmelo Sferrazza"
      ],
      "year": 2024,
      "citation_count": 11,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/e4fef8d5864c5468100ca167639ef3fa374c0442",
      "pdf_link": "",
      "venue": "International Conference on Learning Representations",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "7da3eebf5c2f481bc38ae6115cec3fc71dc87f2b",
      "title": "Randomized Exploration in Cooperative Multi-Agent Reinforcement Learning",
      "abstract": "We present the first study on provably efficient randomized exploration in cooperative multi-agent reinforcement learning (MARL). We propose a unified algorithm framework for randomized exploration in parallel Markov Decision Processes (MDPs), and two Thompson Sampling (TS)-type algorithms, CoopTS-PHE and CoopTS-LMC, incorporating the perturbed-history exploration (PHE) strategy and the Langevin Monte Carlo exploration (LMC) strategy, respectively, which are flexible in design and easy to implement in practice. For a special class of parallel MDPs where the transition is (approximately) linear, we theoretically prove that both CoopTS-PHE and CoopTS-LMC achieve a $\\widetilde{\\mathcal{O}}(d^{3/2}H^2\\sqrt{MK})$ regret bound with communication complexity $\\widetilde{\\mathcal{O}}(dHM^2)$, where $d$ is the feature dimension, $H$ is the horizon length, $M$ is the number of agents, and $K$ is the number of episodes. This is the first theoretical result for randomized exploration in cooperative MARL. We evaluate our proposed method on multiple parallel RL environments, including a deep exploration problem (i.e., $N$-chain), a video game, and a real-world problem in energy systems. Our experimental results support that our framework can achieve better performance, even under conditions of misspecified transition models. Additionally, we establish a connection between our unified framework and the practical application of federated learning.",
      "authors": [
        "Hao-Lun Hsu",
        "Weixin Wang",
        "Miroslav Pajic",
        "Pan Xu"
      ],
      "year": 2024,
      "citation_count": 11,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/7da3eebf5c2f481bc38ae6115cec3fc71dc87f2b",
      "pdf_link": "",
      "venue": "Neural Information Processing Systems",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "01936f6df3c760d23df237d8d15cb7faadce9520",
      "title": "Design of an Adaptive Robust PI Controller for DC/DC Boost Converter Using Reinforcement-Learning Technique and Snake Optimization Algorithm",
      "abstract": "The DC/DC Boost converter exhibits a non-minimum phase system with a right half-plane zero structure, posing significant challenges for the design of effective control approaches. This article presents the design of a robust Proportional-Integral (PI) controller for this converter with an online adaptive mechanism based on the Reinforcement-Learning (RL) strategy. Classical PI controllers are simple and easy to build, but they need to be more robust against a wide range of disturbances and more adaptable to operational parameters. To address these issues, the RL adaptive strategy is used to optimize the performance of the PI controller. Some of the main advantages of the RL are lower sensitivity to error, more reliable results through the collection of data from the environment, an ideal model behavior within a specific context, and better frequency matching in real-time applications. Random exploration, nevertheless, can result in disastrous outcomes and surprising performance in real-world settings. Therefore, we opt for the Deterministic Policy Gradient (DPG) technique, which employs a deterministic action function as opposed to a stochastic one. DPG combines the benefits of actor-critics, deep Q-networks, and the deterministic policy gradient method. In addition, this method adopts the Snake Optimization (SO) algorithm to optimize the initial condition of gains, yielding more reliable results with faster dynamics. The SO method is known for its disciplined and nature-inspired approach, which results in faster decision-making and greater accuracy compared to other optimization algorithms. A structure using a hardware setup with CONTROLLINO MAXI Automation is built, which offers a more cost-effective and precise measurement method. Finally, the results achieved by simulations and experiments demonstrate the robustness of this approach.",
      "authors": [
        "S. Ghamari",
        "Mojtaba Hajihosseini",
        "D. Habibi",
        "Asma Aziz"
      ],
      "year": 2024,
      "citation_count": 11,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/01936f6df3c760d23df237d8d15cb7faadce9520",
      "pdf_link": "",
      "venue": "IEEE Access",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "c734971c6000e3f2769ab5165d00816af80dd76f",
      "title": "In-context Exploration-Exploitation for Reinforcement Learning",
      "abstract": "In-context learning is a promising approach for online policy learning of offline reinforcement learning (RL) methods, which can be achieved at inference time without gradient optimization. However, this method is hindered by significant computational costs resulting from the gathering of large training trajectory sets and the need to train large Transformer models. We address this challenge by introducing an In-context Exploration-Exploitation (ICEE) algorithm, designed to optimize the efficiency of in-context policy learning. Unlike existing models, ICEE performs an exploration-exploitation trade-off at inference time within a Transformer model, without the need for explicit Bayesian inference. Consequently, ICEE can solve Bayesian optimization problems as efficiently as Gaussian process biased methods do, but in significantly less time. Through experiments in grid world environments, we demonstrate that ICEE can learn to solve new RL tasks using only tens of episodes, marking a substantial improvement over the hundreds of episodes needed by the previous in-context learning method.",
      "authors": [
        "Zhenwen Dai",
        "Federico Tomasi",
        "Sina Ghiassian"
      ],
      "year": 2024,
      "citation_count": 10,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/c734971c6000e3f2769ab5165d00816af80dd76f",
      "pdf_link": "",
      "venue": "International Conference on Learning Representations",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "ed8ea3d06c173849f02ee8afcf8db07df0f31261",
      "title": "Optimal Energy Management of Plug-In Hybrid Electric Vehicles Through Ensemble Reinforcement Learning With Exploration-to-Exploitation Ratio Control",
      "abstract": "Reinforcement learning (RL) has demonstrated its advantages in the intelligent control of many vehicle systems. However, controlling the exploration-to-exploitation (E2E) ratio of RL for the best performance in real-world operations is a great challenge. To obtain the optimal E2E ratio for managing the energy flow of a plug-in hybrid electric vehicle (PHEV) in real-world driving, this paper proposes an ensemble learning scheme based on two independent Q-learning agents working back-to-back competitively. At each sampling time, these agents generate two candidate control actions based on state observation and their control policies. Three decay functions, including the widely-used exponential decay and two new decay methods i.e., reciprocal function-base decay and step-based decay, are introduced to formulate one-dimensional look-up tables for E2E control. Then the PHEV control action will be selected from the candidate actions by a learning automata module(LAM) developed in this paper. The combinations of the three decay methods and three ensemble strategies with maximum-based, random-based, and weighted-based methods are investigated with the considerations of their energy efficiency, real-time performance, and robustness. Experiments are carried out on software-in-loop and hardware-in-the-loop platforms to demonstrate the energy-saving potentials. By implementing the ensemble learning scheme based on the weighted-based ensemble method in the control of the studied PHEV, up to 1.04% of energy can be saved under the predefined real-world driving cycles compared to the conventional Q-learning scheme.",
      "authors": [
        "Bin Shuai",
        "Min Hua",
        "Yanfei Li",
        "Shijin Shuai",
        "Hongming Xu",
        "Quan Zhou"
      ],
      "year": 2025,
      "citation_count": 10,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/ed8ea3d06c173849f02ee8afcf8db07df0f31261",
      "pdf_link": "",
      "venue": "IEEE Transactions on Intelligent Vehicles",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "e5d6cca71ea0fb216a25f86e96d3480886fdba27",
      "title": "D5RL: Diverse Datasets for Data-Driven Deep Reinforcement Learning",
      "abstract": "Offline reinforcement learning algorithms hold the promise of enabling data-driven RL methods that do not require costly or dangerous real-world exploration and benefit from large pre-collected datasets. This in turn can facilitate real-world applications, as well as a more standardized approach to RL research. Furthermore, offline RL methods can provide effective initializations for online finetuning to overcome challenges with exploration. However, evaluating progress on offline RL algorithms requires effective and challenging benchmarks that capture properties of real-world tasks, provide a range of task difficulties, and cover a range of challenges both in terms of the parameters of the domain (e.g., length of the horizon, sparsity of rewards) and the parameters of the data (e.g., narrow demonstration data or broad exploratory data). While considerable progress in offline RL in recent years has been enabled by simpler benchmark tasks, the most widely used datasets are increasingly saturating in performance and may fail to reflect properties of realistic tasks. We propose a new benchmark for offline RL that focuses on realistic simulations of robotic manipulation and locomotion environments, based on models of real-world robotic systems, and comprising a variety of data sources, including scripted data, play-style data collected by human teleoperators, and other data sources. Our proposed benchmark covers state-based and image-based domains, and supports both offline RL and online fine-tuning evaluation, with some of the tasks specifically designed to require both pre-training and fine-tuning. We hope that our proposed benchmark will facilitate further progress on both offline RL and fine-tuning algorithms. Website with code, examples, tasks, and data is available at \\url{https://sites.google.com/view/d5rl/}",
      "authors": [
        "Rafael Rafailov",
        "K. Hatch",
        "Anikait Singh",
        "Laura Smith",
        "Aviral Kumar",
        "Ilya Kostrikov",
        "Philippe Hansen-Estruch",
        "Victor Kolev",
        "Philip Ball",
        "Jiajun Wu",
        "Chelsea Finn",
        "Sergey Levine"
      ],
      "year": 2024,
      "citation_count": 9,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/e5d6cca71ea0fb216a25f86e96d3480886fdba27",
      "pdf_link": "",
      "venue": "RLJ",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "5ecf53ab083f72f10421225a7dde25eb51cb6b22",
      "title": "Can Learned Optimization Make Reinforcement Learning Less Difficult?",
      "abstract": "While reinforcement learning (RL) holds great potential for decision making in the real world, it suffers from a number of unique difficulties which often need specific consideration. In particular: it is highly non-stationary; suffers from high degrees of plasticity loss; and requires exploration to prevent premature convergence to local optima and maximize return. In this paper, we consider whether learned optimization can help overcome these problems. Our method, Learned Optimization for Plasticity, Exploration and Non-stationarity (OPEN), meta-learns an update rule whose input features and output structure are informed by previously proposed solutions to these difficulties. We show that our parameterization is flexible enough to enable meta-learning in diverse learning contexts, including the ability to use stochasticity for exploration. Our experiments demonstrate that when meta-trained on single and small sets of environments, OPEN outperforms or equals traditionally used optimizers. Furthermore, OPEN shows strong generalization characteristics across a range of environments and agent architectures.",
      "authors": [
        "A. D. Goldie",
        "Chris Lu",
        "Matthew Jackson",
        "S. Whiteson",
        "J. Foerster"
      ],
      "year": 2024,
      "citation_count": 9,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/5ecf53ab083f72f10421225a7dde25eb51cb6b22",
      "pdf_link": "",
      "venue": "Neural Information Processing Systems",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "4186f74da6d793c91c5ca4d8a10cf2f8638eade6",
      "title": "RLfOLD: Reinforcement Learning from Online Demonstrations in Urban Autonomous Driving",
      "abstract": "Reinforcement Learning from Demonstrations (RLfD) has emerged as an effective method by fusing expert demonstrations into Reinforcement Learning (RL) training, harnessing the strengths of both Imitation Learning (IL) and RL. However, existing algorithms rely on offline demonstrations, which can introduce a distribution gap between the demonstrations and the actual training environment, limiting their performance. In this paper, we propose a novel approach, Reinforcement Learning from Online Demonstrations (RLfOLD), that leverages online demonstrations to address this limitation, ensuring the agent learns from relevant and up-to-date scenarios, thus effectively bridging the distribution gap. Unlike conventional policy networks used in typical actor-critic algorithms, RLfOLD introduces a policy network that outputs two standard deviations: one for exploration and the other for IL training. This novel design allows the agent to adapt to varying levels of uncertainty inherent in both RL and IL. Furthermore, we introduce an exploration process guided by an online expert, incorporating an uncertainty-based technique. Our experiments on the CARLA NoCrash benchmark demonstrate the effectiveness and efficiency of RLfOLD. Notably, even with a significantly smaller encoder and a single camera setup, RLfOLD surpasses state-of-the-art methods in this evaluation. These results, achieved with limited resources, highlight RLfOLD as a highly promising solution for real-world applications.",
      "authors": [
        "Daniel Coelho",
        "Miguel Oliveira",
        "Vitor Santos"
      ],
      "year": 2024,
      "citation_count": 9,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/4186f74da6d793c91c5ca4d8a10cf2f8638eade6",
      "pdf_link": "",
      "venue": "AAAI Conference on Artificial Intelligence",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "134acf45a016fced57cc8f8e171eeb6e0015b0b8",
      "title": "Open the Black Box: Step-based Policy Updates for Temporally-Correlated Episodic Reinforcement Learning",
      "abstract": "Current advancements in reinforcement learning (RL) have predominantly focused on learning step-based policies that generate actions for each perceived state. While these methods efficiently leverage step information from environmental interaction, they often ignore the temporal correlation between actions, resulting in inefficient exploration and unsmooth trajectories that are challenging to implement on real hardware. Episodic RL (ERL) seeks to overcome these challenges by exploring in parameters space that capture the correlation of actions. However, these approaches typically compromise data efficiency, as they treat trajectories as opaque \\emph{black boxes}. In this work, we introduce a novel ERL algorithm, Temporally-Correlated Episodic RL (TCE), which effectively utilizes step information in episodic policy updates, opening the 'black box' in existing ERL methods while retaining the smooth and consistent exploration in parameter space. TCE synergistically combines the advantages of step-based and episodic RL, achieving comparable performance to recent ERL methods while maintaining data efficiency akin to state-of-the-art (SoTA) step-based RL.",
      "authors": [
        "Ge Li",
        "Hongyi Zhou",
        "Dominik Roth",
        "Serge Thilges",
        "Fabian Otto",
        "Rudolf Lioutikov",
        "Gerhard Neumann"
      ],
      "year": 2024,
      "citation_count": 8,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/134acf45a016fced57cc8f8e171eeb6e0015b0b8",
      "pdf_link": "",
      "venue": "International Conference on Learning Representations",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "01f35fa70fc881ab80206121738380c57f8d2074",
      "title": "Relative Entropy Regularized Sample-Efficient Reinforcement Learning With Continuous Actions",
      "abstract": "In this article, a novel reinforcement learning (RL) approach, continuous dynamic policy programming (CDPP), is proposed to tackle the issues of both learning stability and sample efficiency in the current RL methods with continuous actions. The proposed method naturally extends the relative entropy regularization from the value function-based framework to the actor–critic (AC) framework of deep deterministic policy gradient (DDPG) to stabilize the learning process in continuous action space. It tackles the intractable softmax operation over continuous actions in the critic by Monte Carlo estimation and explores the practical advantages of the Mellowmax operator. A Boltzmann sampling policy is proposed to guide the exploration of actor following the relative entropy regularized critic for superior learning capability, exploration efficiency, and robustness. Evaluated by several benchmark and real-robot-based simulation tasks, the proposed method illustrates the positive impact of the relative entropy regularization including efficient exploration behavior and stable policy update in RL with continuous action space and successfully outperforms the related baseline approaches in both sample efficiency and learning stability.",
      "authors": [
        "Zhiwei Shang",
        "Renxing Li",
        "Chunhuang Zheng",
        "Huiyun Li",
        "Yunduan Cui"
      ],
      "year": 2023,
      "citation_count": 15,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/01f35fa70fc881ab80206121738380c57f8d2074",
      "pdf_link": "",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "5dce3f2cfe018ada4bebeaa15aac6fe1d01b18ba",
      "title": "Imagine, Initialize, and Explore: An Effective Exploration Method in Multi-Agent Reinforcement Learning",
      "abstract": "Effective exploration is crucial to discovering optimal strategies for multi-agent reinforcement learning (MARL) in complex coordination tasks. Existing methods mainly utilize intrinsic rewards to enable committed exploration or use role-based learning for decomposing joint action spaces instead of directly conducting a collective search in the entire action-observation space. However, they often face challenges obtaining specific joint action sequences to reach successful states in long-horizon tasks. To address this limitation, we propose Imagine, Initialize, and Explore (IIE), a novel method that offers a promising solution for efficient multi-agent exploration in complex scenarios. IIE employs a transformer model to imagine how the agents reach a critical state that can influence each other's transition functions. Then, we initialize the environment at this state using a simulator before the exploration phase. We formulate the imagination as a sequence modeling problem, where the states, observations, prompts, actions, and rewards are predicted autoregressively. The prompt consists of timestep-to-go, return-to-go, influence value, and one-shot demonstration, specifying the desired state and trajectory as well as guiding the action generation. By initializing agents at the critical states, IIE significantly increases the likelihood of discovering potentially important under-explored regions. Despite its simplicity, empirical results demonstrate that our method outperforms multi-agent exploration baselines on the StarCraft Multi-Agent Challenge (SMAC) and SMACv2 environments. Particularly, IIE shows improved performance in the sparse-reward SMAC tasks and produces more effective curricula over the initialized states than other generative methods, such as CVAE-GAN and diffusion models.",
      "authors": [
        "Zeyang Liu",
        "Lipeng Wan",
        "Xinrui Yang",
        "Zhuoran Chen",
        "Xingyu Chen",
        "Xuguang Lan"
      ],
      "year": 2024,
      "citation_count": 7,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/5dce3f2cfe018ada4bebeaa15aac6fe1d01b18ba",
      "pdf_link": "",
      "venue": "AAAI Conference on Artificial Intelligence",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "085dddfa3105967d4b9f09b1cd0fa7725779faf1",
      "title": "MCMC: Multi-Constrained Model Compression via One-Stage Envelope Reinforcement Learning",
      "abstract": "Model compression methods are being developed to bridge the gap between the massive scale of neural networks and the limited hardware resources on edge devices. Since most real-world applications deployed on resource-limited hardware platforms typically have multiple hardware constraints simultaneously, most existing model compression approaches that only consider optimizing one single hardware objective are ineffective. In this article, we propose an automated pruning method called multi-constrained model compression (MCMC) that allows for the optimization of multiple hardware targets, such as latency, floating point operations (FLOPs), and memory usage, while minimizing the impact on accuracy. Specifically, we propose an improved multi-objective reinforcement learning (MORL) algorithm, the one-stage envelope deep deterministic policy gradient (DDPG) algorithm, to determine the pruning strategy for neural networks. Our improved one-stage envelope DDPG algorithm reduces exploration time and offers greater flexibility in adjusting target priorities, enhancing its suitability for pruning tasks. For instance, on the visual geometry group (VGG)-16 network, our method achieved an 80% reduction in FLOPs, a <inline-formula> <tex-math notation=\"LaTeX\">$2.31\\times $ </tex-math></inline-formula> reduction in memory usage, and a <inline-formula> <tex-math notation=\"LaTeX\">$1.92\\times $ </tex-math></inline-formula> acceleration, with an accuracy improvement of 0.09% compared with the baseline. For larger datasets, such as ImageNet, we reduced FLOPs by 50% for MobileNet-V1, resulting in a <inline-formula> <tex-math notation=\"LaTeX\">$4.7\\times $ </tex-math></inline-formula> faster speed and <inline-formula> <tex-math notation=\"LaTeX\">$1.48\\times $ </tex-math></inline-formula> memory compression, while maintaining the same accuracy. When applied to edge devices, such as JETSON XAVIER NX, our method resulted in a 71% reduction in FLOPs for MobileNet-V1, leading to a <inline-formula> <tex-math notation=\"LaTeX\">$1.63\\times $ </tex-math></inline-formula> faster speed, <inline-formula> <tex-math notation=\"LaTeX\">$1.64\\times $ </tex-math></inline-formula> memory compression, and an accuracy improvement.",
      "authors": [
        "Siqi Li",
        "Jun Chen",
        "Shanqi Liu",
        "Chengrui Zhu",
        "Guanzhong Tian",
        "Yong Liu"
      ],
      "year": 2024,
      "citation_count": 7,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/085dddfa3105967d4b9f09b1cd0fa7725779faf1",
      "pdf_link": "",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "086a4d45a9deef5a10ee4febcd4c92c95a6305de",
      "title": "Improving Offline Reinforcement Learning With in-Sample Advantage Regularization for Robot Manipulation",
      "abstract": "Offline reinforcement learning (RL) aims to learn the possible policy from a fixed dataset without real-time interactions with the environment. By avoiding the risky exploration of the robot, this approach is expected to significantly improve the robot’s learning efficiency and safety. However, due to errors in value estimation from out-of-distribution actions, most offline RL algorithms constrain or regularize the policy to the actions contained within the dataset. The cost of such methods is the introduction of new hyperparameters and additional complexity. In this article, we aim to adapt offline RL to robotic manipulation with minimal changes and to avoid evaluating out-of-distribution actions as much as possible. Therefore, we improve offline RL with in-sample advantage regularization (ISAR). To mitigate the impact of unseen actions, the ISAR learns the state-value function only with the dataset sample to regress the optimal action-value function. Our method calculates the advantage function of action-state pairs based on in-sample value estimation and adds a behavior cloning (BC) regularization term in the policy update. This improves sample efficiency with minimal changes, resulting in a simple and easy-to-implement method. The experiments of the D4RL robot benchmark and multigoal sparse rewards robotic tasks show that the ISAR achieves excellent performance comparable to current state-of-the-art algorithms without the need for complex parameter tuning and too much training time. In addition, we demonstrate the effectiveness of our method on a real-world robot platform.",
      "authors": [
        "Chengzhong Ma",
        "Deyu Yang",
        "Tianyu Wu",
        "Zeyang Liu",
        "Houxue Yang",
        "Xingyu Chen",
        "Xuguang Lan",
        "Nanning Zheng"
      ],
      "year": 2024,
      "citation_count": 6,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/086a4d45a9deef5a10ee4febcd4c92c95a6305de",
      "pdf_link": "",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "200726cba07dec06a56ff46aa38836e9730a23a2",
      "title": "Rethinking Population-assisted Off-policy Reinforcement Learning",
      "abstract": "While off-policy reinforcement learning (RL) algorithms are sample efficient due to gradient-based updates and data reuse in the replay buffer, they struggle with convergence to local optima due to limited exploration. On the other hand, population-based algorithms offer a natural exploration strategy, but their heuristic black-box operators are inefficient. Recent algorithms have integrated these two methods, connecting them through a shared replay buffer. However, the effect of using diverse data from population optimization iterations on off-policy RL algorithms has not been thoroughly investigated. In this paper, we first analyze the use of off-policy RL algorithms in combination with population-based algorithms, showing that the use of population data could introduce an overlooked error and harm performance. To test this, we propose a uniform and scalable training design and conduct experiments on our tailored framework in robot locomotion tasks from the OpenAI gym. Our results substantiate that using population data in off-policy RL can cause instability during training and even degrade performance. To remedy this issue, we further propose a double replay buffer design that provides more on-policy data and show its effectiveness through experiments. Our results offer practical insights for training these hybrid methods.",
      "authors": [
        "Bowen Zheng",
        "Ran Cheng"
      ],
      "year": 2023,
      "citation_count": 11,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/200726cba07dec06a56ff46aa38836e9730a23a2",
      "pdf_link": "",
      "venue": "Annual Conference on Genetic and Evolutionary Computation",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "d06737f9395e592f35ef251e09bea1c18037b096",
      "title": "Random Latent Exploration for Deep Reinforcement Learning",
      "abstract": "We introduce Random Latent Exploration (RLE), a simple yet effective exploration strategy in reinforcement learning (RL). On average, RLE outperforms noise-based methods, which perturb the agent's actions, and bonus-based exploration, which rewards the agent for attempting novel behaviors. The core idea of RLE is to encourage the agent to explore different parts of the environment by pursuing randomly sampled goals in a latent space. RLE is as simple as noise-based methods, as it avoids complex bonus calculations but retains the deep exploration benefits of bonus-based methods. Our experiments show that RLE improves performance on average in both discrete (e.g., Atari) and continuous control tasks (e.g., Isaac Gym), enhancing exploration while remaining a simple and general plug-in for existing RL algorithms. Project website and code: https://srinathm1359.github.io/random-latent-exploration",
      "authors": [
        "Srinath Mahankali",
        "Zhang-Wei Hong",
        "Ayush Sekhari",
        "Alexander Rakhlin",
        "Pulkit Agrawal"
      ],
      "year": 2024,
      "citation_count": 5,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/d06737f9395e592f35ef251e09bea1c18037b096",
      "pdf_link": "",
      "venue": "International Conference on Machine Learning",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "39d2839aa4c3d8c0e64553891fe98ba261703154",
      "title": "More Efficient Randomized Exploration for Reinforcement Learning via Approximate Sampling",
      "abstract": "Thompson sampling (TS) is one of the most popular exploration techniques in reinforcement learning (RL). However, most TS algorithms with theoretical guarantees are difficult to implement and not generalizable to Deep RL. While the emerging approximate sampling-based exploration schemes are promising, most existing algorithms are specific to linear Markov Decision Processes (MDP) with suboptimal regret bounds, or only use the most basic samplers such as Langevin Monte Carlo. In this work, we propose an algorithmic framework that incorporates different approximate sampling methods with the recently proposed Feel-Good Thompson Sampling (FGTS) approach (Zhang, 2022; Dann et al., 2021), which was previously known to be computationally intractable in general. When applied to linear MDPs, our regret analysis yields the best known dependency of regret on dimensionality, surpassing existing randomized algorithms. Additionally, we provide explicit sampling complexity for each employed sampler. Empirically, we show that in tasks where deep exploration is necessary, our proposed algorithms that combine FGTS and approximate sampling perform significantly better compared to other strong baselines. On several challenging games from the Atari 57 suite, our algorithms achieve performance that is either better than or on par with other strong baselines from the deep RL literature.",
      "authors": [
        "Haque Ishfaq",
        "Yixin Tan",
        "Yu Yang",
        "Qingfeng Lan",
        "Jianfeng Lu",
        "A. R. Mahmood",
        "D. Precup",
        "Pan Xu"
      ],
      "year": 2024,
      "citation_count": 5,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/39d2839aa4c3d8c0e64553891fe98ba261703154",
      "pdf_link": "",
      "venue": "RLJ",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "cb7ea0176eec1f809f3bc3a34aeb279d44dda612",
      "title": "Global Reinforcement Learning: Beyond Linear and Convex Rewards via Submodular Semi-gradient Methods",
      "abstract": "In classic Reinforcement Learning (RL), the agent maximizes an additive objective of the visited states, e.g., a value function. Unfortunately, objectives of this type cannot model many real-world applications such as experiment design, exploration, imitation learning, and risk-averse RL to name a few. This is due to the fact that additive objectives disregard interactions between states that are crucial for certain tasks. To tackle this problem, we introduce Global RL (GRL), where rewards are globally defined over trajectories instead of locally over states. Global rewards can capture negative interactions among states, e.g., in exploration, via submodularity, positive interactions, e.g., synergetic effects, via supermodularity, while mixed interactions via combinations of them. By exploiting ideas from submodular optimization, we propose a novel algorithmic scheme that converts any GRL problem to a sequence of classic RL problems and solves it efficiently with curvature-dependent approximation guarantees. We also provide hardness of approximation results and empirically demonstrate the effectiveness of our method on several GRL instances.",
      "authors": [
        "Ric De Santi",
        "Manish Prajapat",
        "Andreas Krause"
      ],
      "year": 2024,
      "citation_count": 5,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/cb7ea0176eec1f809f3bc3a34aeb279d44dda612",
      "pdf_link": "",
      "venue": "International Conference on Machine Learning",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "b31c76815615c16cc8505dbb38d2921f921c029d",
      "title": "A Coverage-Guided Fuzzing Method for Automatic Software Vulnerability Detection Using Reinforcement Learning-Enabled Multi-Level Input Mutation",
      "abstract": "Fuzzing is a popular and effective software testing technique that automatically generates or modifies inputs to test the stability and vulnerabilities of a software system, which has been widely applied and improved by security researchers and experts. The goal of fuzzing is to uncover potential weaknesses in software by providing unexpected and invalid inputs to the target program to monitor its behavior and identify errors or unintended outcomes. Recently, researchers have also integrated promising machine learning algorithms, such as reinforcement learning, to enhance the fuzzing process. Reinforcement learning (RL) has been proven to be able to improve the effectiveness of fuzzing by selecting and prioritizing transformation actions with higher coverage, which reduces the required effort to uncover vulnerabilities. However, RL-based fuzzing models also encounter certain limitations, including an imbalance between exploitation and exploration. In this study, we propose a coverage-guided RL-based fuzzing model that enhances grey-box fuzzing, in which we leverage deep Q-learning to predict and select input variations to maximize code coverage and use code coverage as a reward signal. This model is complemented by simple input selection and scheduling algorithms that promote a more balanced approach to exploiting and exploring software. Furthermore, we introduce a multi-level input mutation model combined with RL to create a sequence of actions for comprehensive input variation. The proposed model is compared to other fuzzing tools in testing various real-world programs, where the results indicate a notable enhancement in terms of code coverage, discovered paths, and execution speed of our solution.",
      "authors": [
        "Van-Hau Pham",
        "Do Thi Thu Hien",
        "Nguyen Phuc Chuong",
        "Pham Thanh Thai",
        "Phan The Duy"
      ],
      "year": 2024,
      "citation_count": 5,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/b31c76815615c16cc8505dbb38d2921f921c029d",
      "pdf_link": "",
      "venue": "IEEE Access",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "c1844cda42b3732a5576d05bb6e007eb1db00919",
      "title": "Incremental Reinforcement Learning with Dual-Adaptive ε-Greedy Exploration",
      "abstract": "Reinforcement learning (RL) has achieved impressive performance in various domains. However, most RL frameworks oversimplify the problem by assuming a fixed-yet-known environment and often have difficulty being generalized to real-world scenarios. In this paper, we address a new challenge with a more realistic setting, Incremental Reinforcement Learning, where the search space of the Markov Decision Process continually expands. While previous methods usually suffer from the lack of efficiency in exploring the unseen transitions, especially with increasing search space, we present a new exploration framework named Dual-Adaptive ϵ-greedy Exploration (DAE) to address the challenge of Incremental RL. Specifically, DAE employs a Meta Policy and an Explorer to avoid redundant computation on those sufficiently\nlearned samples. Furthermore, we release a testbed based on a synthetic environment and the Atari benchmark to validate the effectiveness of any exploration algorithms under Incremental RL. Experimental results demonstrate that the proposed framework can efficiently learn the unseen transitions in new environments, leading to notable performance improvement, i.e., an average of more than 80%, over eight baselines examined.",
      "authors": [
        "Wei Ding",
        "Siyang Jiang",
        "Hsi-Wen Chen",
        "Ming-Syan Chen"
      ],
      "year": 2023,
      "citation_count": 10,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/c1844cda42b3732a5576d05bb6e007eb1db00919",
      "pdf_link": "",
      "venue": "AAAI Conference on Artificial Intelligence",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "2807f9c666335946113fb11dccadf36f8d78b772",
      "title": "A Simple Unified Uncertainty-Guided Framework for Offline-to-Online Reinforcement Learning",
      "abstract": "Offline reinforcement learning (RL) provides a promising solution to learning an agent fully relying on a data-driven paradigm. However, constrained by the limited quality of the offline dataset, its performance is often sub-optimal. Therefore, it is desired to further finetune the agent via extra online interactions before deployment. Unfortunately, offline-to-online RL can be challenging due to two main challenges: constrained exploratory behavior and state-action distribution shift. To this end, we propose a Simple Unified uNcertainty-Guided (SUNG) framework, which naturally unifies the solution to both challenges with the tool of uncertainty. Specifically, SUNG quantifies uncertainty via a VAE-based state-action visitation density estimator. To facilitate efficient exploration, SUNG presents a practical optimistic exploration strategy to select informative actions with both high value and high uncertainty. Moreover, SUNG develops an adaptive exploitation method by applying conservative offline RL objectives to high-uncertainty samples and standard online RL objectives to low-uncertainty samples to smoothly bridge offline and online stages. SUNG achieves state-of-the-art online finetuning performance when combined with different offline RL methods, across various environments and datasets in D4RL benchmark.",
      "authors": [
        "Siyuan Guo",
        "Yanchao Sun",
        "Jifeng Hu",
        "Sili Huang",
        "Hechang Chen",
        "Haiyin Piao",
        "Lichao Sun",
        "Yi Chang"
      ],
      "year": 2023,
      "citation_count": 7,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/2807f9c666335946113fb11dccadf36f8d78b772",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "9eed36fb9b9bbb97579953d5e303dc0cf6c70a58",
      "title": "Safe Reinforcement Learning With Dead-Ends Avoidance and Recovery",
      "abstract": "Safety is one of the main challenges in applying reinforcement learning to tasks in realistic environments. To ensure safety during and after the training process, existing methods tend to adopt overly conservative policies to avoid unsafe situations. However, an overly conservative policy severely hinders exploration. In this letter, we propose a method to construct a boundary that discriminates between safe and unsafe states. The boundary we construct is equivalent to distinguishing dead-end states, indicating the maximum extent to which safe exploration is guaranteed, and thus has a minimum limitation on exploration. Similar to Recovery Reinforcement Learning, we utilize a decoupled RL framework to learn two policies, (1) a task policy that only considers improving the task performance, and (2) a recovery policy that maximizes safety. The recovery policy and a corresponding safety critic are pre-trained on an offline dataset, in which the safety critic evaluates the upper bound of safety in each state as awareness of environmental safety for the agent. During online training, a behavior correction mechanism is adopted, ensuring the agent interacts with the environment using safe actions only. Finally, experiments of continuous control tasks demonstrate that our approach has better task performance with fewer safety violations than state-of-the-art algorithms.",
      "authors": [
        "Xiao Zhang",
        "Hai Zhang",
        "Hongtu Zhou",
        "Chang Huang",
        "Di Zhang",
        "Chen Ye",
        "Junqiao Zhao"
      ],
      "year": 2023,
      "citation_count": 7,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/9eed36fb9b9bbb97579953d5e303dc0cf6c70a58",
      "pdf_link": "",
      "venue": "IEEE Robotics and Automation Letters",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "8ca9a74503c240b2746e351995ee0415657f1cd0",
      "title": "Reinforcement Learning by Guided Safe Exploration",
      "abstract": "Safety is critical to broadening the application of reinforcement learning (RL). Often, we train RL agents in a controlled environment, such as a laboratory, before deploying them in the real world. However, the real-world target task might be unknown prior to deployment. Reward-free RL trains an agent without the reward to adapt quickly once the reward is revealed. We consider the constrained reward-free setting, where an agent (the guide) learns to explore safely without the reward signal. This agent is trained in a controlled environment, which allows unsafe interactions and still provides the safety signal. After the target task is revealed, safety violations are not allowed anymore. Thus, the guide is leveraged to compose a safe behaviour policy. Drawing from transfer learning, we also regularize a target policy (the student) towards the guide while the student is unreliable and gradually eliminate the influence of the guide as training progresses. The empirical analysis shows that this method can achieve safe transfer learning and helps the student solve the target task faster.",
      "authors": [
        "Qisong Yang",
        "T. D. Simão",
        "N. Jansen",
        "Simon Tindemans",
        "M. Spaan"
      ],
      "year": 2023,
      "citation_count": 6,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/8ca9a74503c240b2746e351995ee0415657f1cd0",
      "pdf_link": "",
      "venue": "European Conference on Artificial Intelligence",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "06ffbf40c29daa4f6b4c5025d96ab1c27c0ea117",
      "title": "Forest Fire Localization: From Reinforcement Learning Exploration to a Dynamic Drone Control",
      "abstract": "",
      "authors": [
        "Jonatan Alvarez",
        "Assia Belbachir",
        "Faiza Belbachir",
        "Jamy Chahal",
        "Abdelhak Goudjil",
        "Johvany Gustave",
        "Aybüke Öztürk Suri"
      ],
      "year": 2023,
      "citation_count": 6,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/06ffbf40c29daa4f6b4c5025d96ab1c27c0ea117",
      "pdf_link": "",
      "venue": "Journal of Intelligent and Robotic Systems",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "7f437f4af59ff994d97482ee1c12aaeb4b310e85",
      "title": "Implicit Posteriori Parameter Distribution Optimization in Reinforcement Learning",
      "abstract": "Efficient and intelligent exploration remains a major challenge in the field of deep reinforcement learning (DRL). Bayesian inference with a distributional representation is usually an effective way to improve the exploration ability of the RL agent. However, when optimizing Bayesian neural networks (BNNs), most algorithms need to specify an explicit parameter distribution such as a multivariate Gaussian distribution. This may reduce the flexibility of model representation and affect the algorithm performance. Therefore, to improve sample efficiency and exploration based on Bayesian methods, we propose a novel implicit posteriori parameter distribution optimization (IPPDO) algorithm. First, we adopt a distributional perspective on the parameter and model it with an implicit distribution, which is approximated by generative models. Each model corresponds to a learned latent space, providing structured stochasticity for each layer in the network. Next, to make it possible to optimize an implicit posteriori parameter distribution, we build an energy-based model (EBM) with value function to represent the implicit distribution which is not constrained by any analytic density function. Then, we design a training algorithm based on amortized Stein variational gradient descent (SVGD) to improve the model learning efficiency. We compare IPPDO with other prevailing DRL algorithms on the OpenAI Gym, MuJoCo, and Box2D platforms. Experiments on various tasks demonstrate that the proposed algorithm can represent the parameter uncertainty implicitly for a learned policy and can consistently outperform competing approaches.",
      "authors": [
        "Tianyi Li",
        "Gen-ke Yang",
        "Jian Chu"
      ],
      "year": 2023,
      "citation_count": 6,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/7f437f4af59ff994d97482ee1c12aaeb4b310e85",
      "pdf_link": "",
      "venue": "IEEE Transactions on Cybernetics",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "839395c4823ac8fff990485e7ce54e53c94bae6b",
      "title": "Active Gamma-Ray Log Pattern Localization With Distributionally Robust Reinforcement Learning",
      "abstract": "Accurately localizing 1-D signal patterns, such as Gamma-ray well-log depth matching, is crucial in the oilfield service industry as it directly affects the quality of oil and gas exploration. However, traditional methods such as well-log curve analysis and pattern hand-picking matching are labor-intensive and heavily rely on human expertise, leading to inconsistent results. Although attempts have been made to automate this process, challenges such as low computational performance, nonrobustness, and nongeneralization remain unsolved. To address these challenges, we have developed a data-driven AI system that learns an active signal pattern localization strategy inspired by human attention. Our artificial intelligence system uses an offline reinforcement learning (RL) framework as its central component, which solves a highly abstracted Markov decision process (MDP) problem via offline training on human-labeled historical data. The RL agent uses top-down reasoning to determine the location of target signal fragments by deforming a bounding window using simple transformation actions. To overcome distribution shifts between logged data and real and ensure generalization, we propose a discrete distributionally robust soft actor-critic (SAC) RL framework (DRSAC-Discrete) to solve the MDP problem under uncertainty. By exploring unfamiliar environments in a restrictive manner, the DRSAC-Discrete algorithm provides a safe solution that can be used when data is limited during the early stage of this industrial application. We evaluated the RL-based localization system on augmented field Gamma-ray well-log datasets, and the results showed promising localization capability. Furthermore, the DRSAC-Discrete algorithm demonstrated relatively robust performance guarantees when facing data shortage.",
      "authors": [
        "Yuan Zi",
        "Lei Fan",
        "Xuqing Wu",
        "Jiefu Chen",
        "Shirui Wang",
        "Zhu Han"
      ],
      "year": 2023,
      "citation_count": 5,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/839395c4823ac8fff990485e7ce54e53c94bae6b",
      "pdf_link": "",
      "venue": "IEEE Transactions on Geoscience and Remote Sensing",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "aa65704a16138790678e2b9b59ae679b6c9353d7",
      "title": "Knowledge-Guided Exploration in Deep Reinforcement Learning",
      "abstract": "This paper proposes a new method to drastically speed up deep reinforcement learning (deep RL) training for problems that have the property of state-action permissibility (SAP). Two types of permissibility are defined under SAP. The first type says that after an action $a_t$ is performed in a state $s_t$ and the agent has reached the new state $s_{t+1}$, the agent can decide whether $a_t$ is permissible or not permissible in $s_t$. The second type says that even without performing $a_t$ in $s_t$, the agent can already decide whether $a_t$ is permissible or not in $s_t$. An action is not permissible in a state if the action can never lead to an optimal solution and thus should not be tried (over and over again). We incorporate the proposed SAP property and encode action permissibility knowledge into two state-of-the-art deep RL algorithms to guide their state-action exploration together with a virtual stopping strategy. Results show that the SAP-based guidance can markedly speed up RL training.",
      "authors": [
        "Sahisnu Mazumder",
        "Bing Liu",
        "Shuai Wang",
        "Yingxuan Zhu",
        "Xiaotian Yin",
        "Lifeng Liu",
        "Jian Li"
      ],
      "year": 2022,
      "citation_count": 5,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/aa65704a16138790678e2b9b59ae679b6c9353d7",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "bd8aaab29fa16f40ef016393ba7ca30127abab58",
      "title": "Risk Perspective Exploration in Distributional Reinforcement Learning",
      "abstract": "Distributional reinforcement learning demonstrates state-of-the-art performance in continuous and discrete control settings with the features of variance and risk, which can be used to explore. However, the exploration method employing the risk property is hard to find, although numerous exploration methods in Distributional RL employ the variance of return distribution per action. In this paper, we present risk scheduling approaches that explore risk levels and optimistic behaviors from a risk perspective. We demonstrate the performance enhancement of the DMIX algorithm using risk scheduling in a multi-agent setting with comprehensive experiments.",
      "authors": [
        "Ji-Yun Oh",
        "Joonkee Kim",
        "Se-Young Yun"
      ],
      "year": 2022,
      "citation_count": 5,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/bd8aaab29fa16f40ef016393ba7ca30127abab58",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "d8bc75fbcfdecd5cd1952524d3e07db13722f5ff",
      "title": "Optimal Volt/Var Control for Unbalanced Distribution Networks With Human-in-the-Loop Deep Reinforcement Learning",
      "abstract": "This paper proposes a human-in-the-loop deep reinforcement learning (HL-DRL)-based VVC strategy to simultaneously reduce power losses, mitigate voltage violations and compensate for voltage unbalance in three-phase unbalanced distribution networks. Instead of fully trusting DRL actions made by deep neural networks, a human intervention module is proposed to modify dangerous actions that violate operation constraints during offline training. This module refers to well-designed human guidance rules based on voltage-reactive power sensitivities, which regulate PV reactive power to sequentially address local voltage violation and unbalance issues to obtain safe transitions. To efficiently and safely learn the optimal control policy from these training samples, a human-in-the-loop soft actor-critic (HL-SAC) solution method is then developed. Different from the standard SAC algorithm, an online switch mechanism between action exploration and human intervention is designed. The actor network loss function is modified to incorporate human guidance terms, which alleviates the inconsistency of the updating direction of actor and critic networks. A hybrid experience replay buffer including both dangerous and safe transitions is also used to facilitate the learning process towards human actions. Comparative simulation results on a modified IEEE 123-bus unbalanced distribution system demonstrate the effectiveness and superiority of the proposed method in voltage control.",
      "authors": [
        "Xianzhuo Sun",
        "Zhao Xu",
        "Jing Qiu",
        "Huichuan Liu",
        "Huayi Wu",
        "Yuechuan Tao"
      ],
      "year": 2024,
      "citation_count": 21,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/d8bc75fbcfdecd5cd1952524d3e07db13722f5ff",
      "pdf_link": "",
      "venue": "IEEE Transactions on Smart Grid",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "17682d6f13dcac703092e0f1500a4197e46bbf2c",
      "title": "Safe Reinforcement Learning for Power System Control: A Review",
      "abstract": "The large-scale integration of intermittent renewable energy resources introduces increased uncertainty and volatility to the supply side of power systems, thereby complicating system operation and control. Recently, data-driven approaches, particularly reinforcement learning (RL), have shown significant promise in addressing complex control challenges in power systems, because RL can learn from interactive feedback without needing prior knowledge of the system model. However, the training process of model-free RL methods relies heavily on random decisions for exploration, which may result in ``bad\"decisions that violate critical safety constraints and lead to catastrophic control outcomes. Due to the inability of RL methods to theoretically ensure decision safety in power systems, directly deploying traditional RL algorithms in the real world is deemed unacceptable. Consequently, the safety issue in RL applications, known as safe RL, has garnered considerable attention in recent years, leading to numerous important developments. This paper provides a comprehensive review of the state-of-the-art safe RL techniques and discusses how these techniques can be applied to power system control problems such as frequency regulation, voltage control, and energy management. We then present discussions on key challenges and future research directions, related to convergence and optimality, training efficiency, universality, and real-world deployment.",
      "authors": [
        "Peipei Yu",
        "Zhen-yu Wang",
        "Hongcai Zhang",
        "Yonghua Song"
      ],
      "year": 2024,
      "citation_count": 8,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/17682d6f13dcac703092e0f1500a4197e46bbf2c",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "11c34b84c3ad6587529517c32923c446797c63e6",
      "title": "Cooperative multi-agent reinforcement learning for multi-area integrated scheduling in wafer fabs",
      "abstract": "The existing scheduling methods of wafer fabs focus on single area, achieving local optimisation while failing to realise global optimisation due to neglecting the coordination of multi-area. Therefore, it is necessary to consider the complex opposing relationships between multi-area caused by constraints such as batch processing, re-entrance, and multiple residency times within and between areas to conduct integrated scheduling and shorten the production cycle time. For this issue, this paper proposes a cooperative multi-agent reinforcement learning for multi-area integrated scheduling. Aiming at the dynamic batching and scheduling considering the dynamic arrival lots in multi-area, a multi-agent reinforcement learning algorithm is presented to learn the optimal dynamic batching and scheduling policy firstly. Subsequently, a cooperative multi-agent framework is raised to achieve the global optimisation and coordination of multi-area. Furthermore, an adaptive exploration strategy is constructed to enhance the global exploration capability of the complex solution space caused by residency time constraints and re-entrant property. Moreover, a policy share enhanced Double DQN is employed to improve the generalisation and adaptability of the multi-agent. Finally, the experiments demonstrate that the proposed integrated scheduling method has better comprehensive performance compared to the previous area-separated scheduling methods.",
      "authors": [
        "Ming Wang",
        "Jie Zhang",
        "Peng Zhang",
        "Mengyu Jin"
      ],
      "year": 2024,
      "citation_count": 7,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/11c34b84c3ad6587529517c32923c446797c63e6",
      "pdf_link": "",
      "venue": "International Journal of Production Research",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "8357670aac3c98a71b454ab5bca89558f265369d",
      "title": "Two-Stage Evolutionary Reinforcement Learning for Enhancing Exploration and Exploitation",
      "abstract": "The integration of Evolutionary Algorithm (EA) and Reinforcement Learning (RL) has emerged as a promising approach for tackling some challenges in RL, such as sparse rewards, lack of exploration, and brittle convergence properties. However, existing methods often employ actor networks as individuals of EA, which may constrain their exploratory capabilities, as the entire actor population will stop evolution when the critic network in RL falls into local optimal. To alleviate this issue, this paper introduces a Two-stage Evolutionary Reinforcement Learning (TERL) framework that maintains a population containing both actor and critic networks. TERL divides the learning process into two stages. In the initial stage, individuals independently learn actor-critic networks, which are optimized alternatively by RL and Particle Swarm Optimization (PSO). This dual optimization fosters greater exploration, curbing susceptibility to local optima. Shared information from a common replay buffer and PSO algorithm substantially mitigates the computational load of training multiple agents. In the subsequent stage, TERL shifts to a refined exploitation phase. Here, only the best individual undergoes further refinement, while the rest individuals continue PSO-based optimization. This allocates more computational resources to the best individual for yielding superior performance. Empirical assessments, conducted across a range of continuous control problems, validate the efficacy of the proposed TERL paradigm.",
      "authors": [
        "Qingling Zhu",
        "Xiaoqiang Wu",
        "Qiuzhen Lin",
        "Weineng Chen"
      ],
      "year": 2024,
      "citation_count": 4,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/8357670aac3c98a71b454ab5bca89558f265369d",
      "pdf_link": "",
      "venue": "AAAI Conference on Artificial Intelligence",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "e401ba782c2da93959582295089d3f04a051d6c1",
      "title": "Reinforcement Learning and Sim-to-Real Transfer of Reorientation and Landing Control for Quadruped Robots on Asteroids",
      "abstract": "In surface exploration missions, wheeled planetary vehicles have difficulty traveling on asteroids due to their weak gravitational fields. With the rapid development of hardware performance and control methods, quadruped robots have great potential to serve in asteroid exploration. When deploying or controlling the jumping motions of a quadruped robot on an asteroid, landing stability must be guaranteed. Under the weak and irregular gravitational fields of asteroids, the robot should be reoriented to an appropriate attitude for a smooth and stable landing. To achieve this objective, a model-free control method based on reinforcement learning (RL) was proposed. Sim-to-real transfer methods including domain randomization and transfer learning were proposed to address the sim-to-real problem. The original control model trained by RL was transferred to a new version that could be applied and tested on a real quadruped robot. An equivalent asteroid's weak gravitational environment experimental platform was for the first time designed and employed to test the performance of the proposed control scheme. Both the simulation and experimental results validated the effectiveness of the proposed controller training and sim-to-real transfer methods.",
      "authors": [
        "Ji Qi",
        "Haibo Gao",
        "Huanli Su",
        "M. Huo",
        "Haitao Yu",
        "Zongquan Deng"
      ],
      "year": 2024,
      "citation_count": 4,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/e401ba782c2da93959582295089d3f04a051d6c1",
      "pdf_link": "",
      "venue": "IEEE transactions on industrial electronics (1982. Print)",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "e825e6325dfb5b93066817e7cc6b226ba7d3b799",
      "title": "Bayesian Design Principles for Offline-to-Online Reinforcement Learning",
      "abstract": "Offline reinforcement learning (RL) is crucial for real-world applications where exploration can be costly or unsafe. However, offline learned policies are often suboptimal, and further online fine-tuning is required. In this paper, we tackle the fundamental dilemma of offline-to-online fine-tuning: if the agent remains pessimistic, it may fail to learn a better policy, while if it becomes optimistic directly, performance may suffer from a sudden drop. We show that Bayesian design principles are crucial in solving such a dilemma. Instead of adopting optimistic or pessimistic policies, the agent should act in a way that matches its belief in optimal policies. Such a probability-matching agent can avoid a sudden performance drop while still being guaranteed to find the optimal policy. Based on our theoretical findings, we introduce a novel algorithm that outperforms existing methods on various benchmarks, demonstrating the efficacy of our approach. Overall, the proposed approach provides a new perspective on offline-to-online RL that has the potential to enable more effective learning from offline data.",
      "authors": [
        "Haotian Hu",
        "Yiqin Yang",
        "Jianing Ye",
        "Chengjie Wu",
        "Ziqing Mai",
        "Yujing Hu",
        "Tangjie Lv",
        "Changjie Fan",
        "Qianchuan Zhao",
        "Chongjie Zhang"
      ],
      "year": 2024,
      "citation_count": 4,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/e825e6325dfb5b93066817e7cc6b226ba7d3b799",
      "pdf_link": "",
      "venue": "International Conference on Machine Learning",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "6bf550ce7f10a4347d448eb810a92e1a5cfffa4b",
      "title": "Beyond Optimism: Exploration With Partially Observable Rewards",
      "abstract": "Exploration in reinforcement learning (RL) remains an open challenge. RL algorithms rely on observing rewards to train the agent, and if informative rewards are sparse the agent learns slowly or may not learn at all. To improve exploration and reward discovery, popular algorithms rely on optimism. But what if sometimes rewards are unobservable, e.g., situations of partial monitoring in bandits and the recent formalism of monitored Markov decision process? In this case, optimism can lead to suboptimal behavior that does not explore further to collapse uncertainty. With this paper, we present a novel exploration strategy that overcomes the limitations of existing methods and guarantees convergence to an optimal policy even when rewards are not always observable. We further propose a collection of tabular environments for benchmarking exploration in RL (with and without unobservable rewards) and show that our method outperforms existing ones.",
      "authors": [
        "Simone Parisi",
        "Alireza Kazemipour",
        "Michael Bowling"
      ],
      "year": 2024,
      "citation_count": 3,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/6bf550ce7f10a4347d448eb810a92e1a5cfffa4b",
      "pdf_link": "",
      "venue": "Neural Information Processing Systems",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "b44b0e8222021b51783c62b0bce071ef6fb3f12c",
      "title": "OCEAN-MBRL: Offline Conservative Exploration for Model-Based Offline Reinforcement Learning",
      "abstract": "Model-based offline reinforcement learning (RL) algorithms have emerged as a promising paradigm for offline RL.\nThese algorithms usually learn a dynamics model from a static dataset of transitions, use the model to generate synthetic trajectories, and perform conservative policy optimization within these trajectories. \nHowever, our observations indicate that policy optimization methods used in these model-based offline RL algorithms are not effective at exploring the learned model and induce biased exploration, which ultimately impairs the performance of the algorithm.\nTo address this issue, we propose Offline Conservative ExplorAtioN (OCEAN), a novel rollout approach to model-based offline RL.\nIn our method, we incorporate additional exploration techniques and introduce three conservative constraints based on uncertainty estimation to mitigate the potential impact of significant dynamic errors resulting from exploratory transitions. \nOur work is a plug-in method and can be combined with classical model-based RL algorithms, such as MOPO, COMBO, and RAMBO.\nExperiment results of our method on the D4RL MuJoCo benchmark show that OCEAN significantly improves the performance of existing algorithms.",
      "authors": [
        "Fan Wu",
        "Rui Zhang",
        "Qi Yi",
        "Yunkai Gao",
        "Jiaming Guo",
        "Shaohui Peng",
        "Siming Lan",
        "Husheng Han",
        "Yansong Pan",
        "Kaizhao Yuan",
        "Pengwei Jin",
        "Rui Chen",
        "Yunji Chen",
        "Ling Li"
      ],
      "year": 2024,
      "citation_count": 3,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/b44b0e8222021b51783c62b0bce071ef6fb3f12c",
      "pdf_link": "",
      "venue": "AAAI Conference on Artificial Intelligence",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "cce1245ba1ec154120b3b256faf7bf28f769b505",
      "title": "A Novel Guided Deep Reinforcement Learning Tracking Control Strategy for Multirotors",
      "abstract": "This paper presents an intelligent control scheme for multirotors, where accurate trajectory tracking, strong robustness and reliable generalization are guaranteed by the dual-feedback sliding-mode (DFSM) guided deep reinforcement learning (RL). Different from current solutions, the proposed method explores optimal learning strategy on the sliding surface according to the DFSM demonstrations, where the elegantly designed parallel evaluation takes full advantage of model knowledge and learning exploration. Specifically, the intelligent tracking control is achieved in a two-step design. First, the DFSM algorithm is designed for multirotors, where the linear and nonlinear feedback terms work cooperatively. Second, the DFSM-guided deep RL is put forward to achieve intelligent switching on the sliding surface, where position and velocity errors are both considered to generate accurate switching decisions. In the framework, explorations and the DFSM demonstrations are evaluated in parallel, where only the explorations that are better than the DFSM baseline, are kept for policy improvement. In this way, the DFSM algorithm keeps pushing the RL policy to explore better strategy, where the unavoidable bad experiences arisen from exploration are identified accurately. Practical comparative experimental results are included to verify the effectiveness of the proposed strategy. Note to Practitioners—This paper is motivated by the practical problem of controlling multirotor system in uncertain environments. Up until now, most existing approaches are proposed without taking full advantage of model knowledge and deep learning techniques simultaneously, which lacks of reliability in practical application. To deal with the problem, a new dual-feedback sliding-mode (DFSM) guided deep reinforcement learning (RL) strategy is proposed, where the dual feedback and guided RL are designed to achieve satisfactory tracking control and simultaneously handle uncertainties. Specifically, by introducing double-check framework, the RL strategy explores optimal switching policy on the sliding surface according to the DFSM demonstrations, guaranteeing strong robustness and reliable generalization of the obtained RL policy in uncertain environments. The key feature of the framework is that the DFSM-driven training guarantees practice-oriented tracking control in a DFSM-RL cooperative manner. Comparative experiments are implemented to verify the tracking performance of the proposed intelligent control strategy.",
      "authors": [
        "Hean Hua",
        "Yaonan Wang",
        "Hang Zhong",
        "Hui Zhang",
        "Yongchun Fang"
      ],
      "year": 2025,
      "citation_count": 3,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/cce1245ba1ec154120b3b256faf7bf28f769b505",
      "pdf_link": "",
      "venue": "IEEE Transactions on Automation Science and Engineering",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "37fe2a997bf07a972473abd079d175335940e6bd",
      "title": "CDE: Curiosity-Driven Exploration for Efficient Reinforcement Learning in Large Language Models",
      "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) is a powerful paradigm for enhancing the reasoning ability of Large Language Models (LLMs). Yet current RLVR methods often explore poorly, leading to premature convergence and entropy collapse. To address this challenge, we introduce Curiosity-Driven Exploration (CDE), a framework that leverages the model's own intrinsic sense of curiosity to guide exploration. We formalize curiosity with signals from both the actor and the critic: for the actor, we use perplexity over its generated response, and for the critic, we use the variance of value estimates from a multi-head architecture. Both signals serve as an exploration bonus within the RLVR framework to guide the model. Our theoretical analysis shows that the actor-wise bonus inherently penalizes overconfident errors and promotes diversity among correct responses; moreover, we connect the critic-wise bonus to the well-established count-based exploration bonus in RL. Empirically, our method achieves an approximate +3 point improvement over standard RLVR using GRPO/PPO on AIME benchmarks. Further analysis identifies a calibration collapse mechanism within RLVR, shedding light on common LLM failure modes.",
      "authors": [
        "Runpeng Dai",
        "Linfeng Song",
        "Haolin Liu",
        "Zhenwen Liang",
        "Dian Yu",
        "Haitao Mi",
        "Zhaopeng Tu",
        "Rui Liu",
        "Tong Zheng",
        "Hongtu Zhu",
        "Dong Yu"
      ],
      "year": 2025,
      "citation_count": 3,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/37fe2a997bf07a972473abd079d175335940e6bd",
      "pdf_link": "",
      "venue": "",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "2389fafc2a97e13fa810c4014babe73bd886c06f",
      "title": "Offline Reinforcement Learning with Failure Under Sparse Reward Environments",
      "abstract": "This paper presents a new reinforcement learning approach that leverages failed experiences in sparse reward environments. Unlike traditional reinforcement learning methods that rely on successful experiences or expert demonstrations, the proposed approach utilizes failed experiences to guide the policy update during the learning process. The primary objective behind this work is to develop a method that can efficiently utilize failed experiences to guide the search direction as directional cues from successful experiences may be limited in sparse environments. To achieve this objective, we introduce a new objective function that aims to maximize the dissimilarity between the RL agent's actions and actions from failed experiences. This discrepancy serves as a valuable indicator to guide the agent's exploration. In other words, our method focuses on achieving the desired objectives by leveraging failed experiences to provide a significant opportunity for the agent to refine its policy. We further employ hindsight experience replay (HER) to enhance the directional search by creating and achieving potential subgoals that align with the primary objectives. To assess the effectiveness of our method, we conduct experiments on three sparse reward environments. Our findings demonstrate that the proposed approach significantly enhances the agent's learning efficiency and improves robustness to variations in demonstration quality compared to conventional reinforcement learning techniques.",
      "authors": [
        "Mingkang Wu",
        "Umer Siddique",
        "Abhinav Sinha",
        "Yongcan Cao"
      ],
      "year": 2024,
      "citation_count": 3,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/2389fafc2a97e13fa810c4014babe73bd886c06f",
      "pdf_link": "",
      "venue": "International Conference on Multimodal Interaction",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "b2d827c286e32dbf0739e8c796b119b1074809b4",
      "title": "Meta SAC-Lag: Towards Deployable Safe Reinforcement Learning via MetaGradient-based Hyperparameter Tuning",
      "abstract": "Safe Reinforcement Learning (Safe RL) is one of the prevalently studied subcategories of trial-and-error-based methods with the intention to be deployed on real-world systems. In safe RL, the goal is to maximize reward performance while minimizing constraints, often achieved by setting bounds on constraint functions and utilizing the Lagrangian method. However, deploying Lagrangian-based safe RL in real-world scenarios is challenging due to the necessity of threshold fine-tuning, as imprecise adjustments may lead to suboptimal policy convergence. To mitigate this challenge, we propose a unified Lagrangian-based model-free architecture called Meta Soft Actor-Critic Lagrangian (Meta SAC-Lag). Meta SAC-Lag uses meta-gradient optimization to automatically update the safety-related hyperparameters. The proposed method is designed to address safe exploration and threshold adjustment with minimal hyperparameter tuning requirement. In our pipeline, the inner parameters are updated through the conventional formulation and the hyperparameters are adjusted using the meta-objectives which are defined based on the updated parameters. Our results show that the agent can reliably adjust the safety performance due to the relatively fast convergence rate of the safety threshold. We evaluate the performance of Meta SAC-Lag in five simulated environments against Lagrangian baselines, and the results demonstrate its capability to create synergy between parameters, yielding better or competitive results. Furthermore, we conduct a real-world experiment involving a robotic arm tasked with pouring coffee into a cup without spillage. Meta SAC-Lag is successfully trained to execute the task, while minimizing effort constraints. The success of Meta SAC-Lag in performing the experiment is intended to be a step toward practical deployment of safe RL algorithms to learn the control process of safety-critical real-world systems without explicit engineering.",
      "authors": [
        "Homayoun Honari",
        "Amir M. Soufi Enayati",
        "Mehran Ghafarian Tamizi",
        "Homayoun Najjaran"
      ],
      "year": 2024,
      "citation_count": 3,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/b2d827c286e32dbf0739e8c796b119b1074809b4",
      "pdf_link": "",
      "venue": "IEEE/RJS International Conference on Intelligent RObots and Systems",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "9ac3f4e74e9837cb47f43211cf143d4c7115e7f1",
      "title": "``Give Me an Example Like This'': Episodic Active Reinforcement Learning from Demonstrations",
      "abstract": "Reinforcement Learning (RL) has achieved great success in sequential decision-making problems but often requires extensive agent-environment interactions. To improve sample efficiency, methods like Reinforcement Learning from Expert Demonstrations (RLED) incorporate external expert demonstrations to aid agent exploration during the learning process. However, these demonstrations, typically collected from human users, are costly and thus often limited in quantity. Therefore, how to select the optimal set of human demonstrations that most effectively aids learning becomes a critical concern. This paper introduces EARLY (Episodic Active Learning from demonstration querY), an algorithm designed to enable a learning agent to generate optimized queries for expert demonstrations in a trajectory-based feature space. EARLY employs a trajectory-level estimate of uncertainty in the agent’s current policy to determine the optimal timing and content for feature-based queries. By querying episodic demonstrations instead of isolated state-action pairs, EARLY enhances the human teaching experience and achieves better learning performance. We validate the effectiveness of our method across three simulated navigation tasks of increasing difficulty. Results indicate that our method achieves expert-level performance in all three tasks, converging over 50% faster than other four baseline methods when demonstrations are generated by simulated oracle policies. A follow-up pilot user study (N = 18) further supports that our method maintains significantly better convergence with human expert demonstrators, while also providing a better user experience in terms of perceived task load and requiring significantly less human time.",
      "authors": [
        "Muhan Hou",
        "Koen V. Hindriks",
        "Guszti Eiben",
        "Kim Baraka"
      ],
      "year": 2024,
      "citation_count": 3,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/9ac3f4e74e9837cb47f43211cf143d4c7115e7f1",
      "pdf_link": "",
      "venue": "International Conference on Human-Agent Interaction",
      "paper_type": "",
      "keywords": []
    }
  ],
  "edges": [
    {
      "source": "c28ec2a40a2c77e20d64cf1c85dc931106df8e83",
      "target": "9ac3f4e74e9837cb47f43211cf143d4c7115e7f1",
      "weight": 0.3450238430136142
    },
    {
      "source": "c28ec2a40a2c77e20d64cf1c85dc931106df8e83",
      "target": "2389fafc2a97e13fa810c4014babe73bd886c06f",
      "weight": 0.29807209088423126
    },
    {
      "source": "c28ec2a40a2c77e20d64cf1c85dc931106df8e83",
      "target": "4186f74da6d793c91c5ca4d8a10cf2f8638eade6",
      "weight": 0.3002768789775585
    },
    {
      "source": "c28ec2a40a2c77e20d64cf1c85dc931106df8e83",
      "target": "7d6168fbd3ed72f9098573007f4b8c2ec9e576b9",
      "weight": 0.27210612289618985
    },
    {
      "source": "c28ec2a40a2c77e20d64cf1c85dc931106df8e83",
      "target": "03c7f9a6b743eb1c9127b8d4552147b2b1a0c1e1",
      "weight": 0.2985113581378972
    },
    {
      "source": "c28ec2a40a2c77e20d64cf1c85dc931106df8e83",
      "target": "bd38cbbb346a347cb5b60ac4a133b3d73cb44e07",
      "weight": 0.2985703840614813
    },
    {
      "source": "c28ec2a40a2c77e20d64cf1c85dc931106df8e83",
      "target": "b2004f4f19bc6ccae8e4afc554f39142870100f5",
      "weight": 0.3306452293396378
    },
    {
      "source": "c28ec2a40a2c77e20d64cf1c85dc931106df8e83",
      "target": "aa65704a16138790678e2b9b59ae679b6c9353d7",
      "weight": 0.22837435597645392
    },
    {
      "source": "c28ec2a40a2c77e20d64cf1c85dc931106df8e83",
      "target": "a17a7256c04afee68f9aa0b7bfdc67fbca998b9c",
      "weight": 0.33549984661572396
    },
    {
      "source": "c28ec2a40a2c77e20d64cf1c85dc931106df8e83",
      "target": "f3a63a840185fa8c5e0db4bbe12afa7d3de7d029",
      "weight": 0.3963319031959407
    },
    {
      "source": "c28ec2a40a2c77e20d64cf1c85dc931106df8e83",
      "target": "fe7382db243694c67c667cf2ec80072577d2372b",
      "weight": 0.2730367898080424
    },
    {
      "source": "c28ec2a40a2c77e20d64cf1c85dc931106df8e83",
      "target": "de93c8aed64229571b03e40b36499d4f07ce875d",
      "weight": 0.2625562773290348
    },
    {
      "source": "c28ec2a40a2c77e20d64cf1c85dc931106df8e83",
      "target": "06318722ebbac1d9b59e2408ecd3994eadb0ec6b",
      "weight": 0.058442985695957
    },
    {
      "source": "0fcb2034e31a2bc2f12a2b1363d0d77baf445fdf",
      "target": "37fe2a997bf07a972473abd079d175335940e6bd",
      "weight": 0.08084265790404171
    },
    {
      "source": "0fcb2034e31a2bc2f12a2b1363d0d77baf445fdf",
      "target": "5ecf53ab083f72f10421225a7dde25eb51cb6b22",
      "weight": 0.24465778124637935
    },
    {
      "source": "0fcb2034e31a2bc2f12a2b1363d0d77baf445fdf",
      "target": "6bf550ce7f10a4347d448eb810a92e1a5cfffa4b",
      "weight": 0.07200227846886081
    },
    {
      "source": "0fcb2034e31a2bc2f12a2b1363d0d77baf445fdf",
      "target": "53db22a9d4ae77dd8218ba867184898adc84d1d1",
      "weight": 0.24552802741591637
    },
    {
      "source": "0fcb2034e31a2bc2f12a2b1363d0d77baf445fdf",
      "target": "4e98282f5f3f1a388b8d95380473d4ef4878266e",
      "weight": 0.23999707881688337
    },
    {
      "source": "0fcb2034e31a2bc2f12a2b1363d0d77baf445fdf",
      "target": "c1844cda42b3732a5576d05bb6e007eb1db00919",
      "weight": 0.25360691969537225
    },
    {
      "source": "0fcb2034e31a2bc2f12a2b1363d0d77baf445fdf",
      "target": "c90aa0f206c6fd41c490c142f63f7ba046cae6b7",
      "weight": 0.2974056418096127
    },
    {
      "source": "0fcb2034e31a2bc2f12a2b1363d0d77baf445fdf",
      "target": "3d3b0704c61d47c7bafb70ae2670b2786b8e4d81",
      "weight": 0.2406349436708245
    },
    {
      "source": "0fcb2034e31a2bc2f12a2b1363d0d77baf445fdf",
      "target": "c64a99ec780d5e1c6ba242a82e72e96eb0c3ef8a",
      "weight": 0.05811515760149735
    },
    {
      "source": "0fcb2034e31a2bc2f12a2b1363d0d77baf445fdf",
      "target": "5c0b56d9440ea70c79d1d0032d46c14e06f541f5",
      "weight": 0.2524471030648425
    },
    {
      "source": "0fcb2034e31a2bc2f12a2b1363d0d77baf445fdf",
      "target": "cc9f2fd320a279741403c4bfbeb91179803c428c",
      "weight": 0.2895144578991103
    },
    {
      "source": "0fcb2034e31a2bc2f12a2b1363d0d77baf445fdf",
      "target": "1c35807e1a4c24e2013fa0a090cee9cc4716a5f5",
      "weight": 0.23177186086026622
    },
    {
      "source": "0fcb2034e31a2bc2f12a2b1363d0d77baf445fdf",
      "target": "807f377de905eda62e4cd2f0797153a59296adbb",
      "weight": 0.23332392641053679
    },
    {
      "source": "0fcb2034e31a2bc2f12a2b1363d0d77baf445fdf",
      "target": "12075ea34f5fbe32ec5582786761ab34d401209b",
      "weight": 0.07763117382275636
    },
    {
      "source": "0fcb2034e31a2bc2f12a2b1363d0d77baf445fdf",
      "target": "399806e861a2ef960a81b37b593c2176a728c399",
      "weight": 0.11065258775232581
    },
    {
      "source": "0fcb2034e31a2bc2f12a2b1363d0d77baf445fdf",
      "target": "45f573f302dc7e77cbc5d1a74ccbac3564bbebc8",
      "weight": 0.24339107022680662
    },
    {
      "source": "0fcb2034e31a2bc2f12a2b1363d0d77baf445fdf",
      "target": "9e5fe2ba652774ba3b1127f626c192668a907132",
      "weight": 0.3020505996498895
    },
    {
      "source": "0fcb2034e31a2bc2f12a2b1363d0d77baf445fdf",
      "target": "de93c8aed64229571b03e40b36499d4f07ce875d",
      "weight": 0.30991620195746716
    },
    {
      "source": "0fcb2034e31a2bc2f12a2b1363d0d77baf445fdf",
      "target": "f6b218453170edcbb51e49dd44ba2f83af53ef92",
      "weight": 0.3008797614412257
    },
    {
      "source": "0fcb2034e31a2bc2f12a2b1363d0d77baf445fdf",
      "target": "a25b645c3d24f91164230a0ac5bb2d4ec88c1538",
      "weight": 0.2601275140745367
    },
    {
      "source": "0fcb2034e31a2bc2f12a2b1363d0d77baf445fdf",
      "target": "5fd3ce235f5fcebd3d2807f710b060add527183b",
      "weight": 0.27211081060770165
    },
    {
      "source": "0fcb2034e31a2bc2f12a2b1363d0d77baf445fdf",
      "target": "3c3093f5f8e9601637612fcfb8f160f116fa30e4",
      "weight": 0.27925438395319596
    },
    {
      "source": "0fcb2034e31a2bc2f12a2b1363d0d77baf445fdf",
      "target": "2064020586d5832b55f80a7dffea1fd90a5d94dd",
      "weight": 0.25824879776128573
    },
    {
      "source": "0fcb2034e31a2bc2f12a2b1363d0d77baf445fdf",
      "target": "0f810eb4777fd05317951ebaa7a3f5835ee84cf4",
      "weight": 0.4477583365140573
    },
    {
      "source": "0fcb2034e31a2bc2f12a2b1363d0d77baf445fdf",
      "target": "f715558b65fd4f3c6966505c237d9a622947010b",
      "weight": 0.24319885124888982
    },
    {
      "source": "45f573f302dc7e77cbc5d1a74ccbac3564bbebc8",
      "target": "cc9f2fd320a279741403c4bfbeb91179803c428c",
      "weight": 0.6600562469354594
    },
    {
      "source": "f8d8e192979ab5d8d593e76a0c4e2c7581778732",
      "target": "813f6e34feb3dc0346b6392d061af12ff186ba7e",
      "weight": 0.06867467329307975
    },
    {
      "source": "2470fcf0f89082de874ac9133ccb3a8667dd89a8",
      "target": "e4fef8d5864c5468100ca167639ef3fa374c0442",
      "weight": 0.29748570335619634
    },
    {
      "source": "2470fcf0f89082de874ac9133ccb3a8667dd89a8",
      "target": "5ecf53ab083f72f10421225a7dde25eb51cb6b22",
      "weight": 0.275897745505663
    },
    {
      "source": "2470fcf0f89082de874ac9133ccb3a8667dd89a8",
      "target": "6bf550ce7f10a4347d448eb810a92e1a5cfffa4b",
      "weight": 0.07193652849129065
    },
    {
      "source": "2470fcf0f89082de874ac9133ccb3a8667dd89a8",
      "target": "c64a99ec780d5e1c6ba242a82e72e96eb0c3ef8a",
      "weight": 0.026697425357424955
    },
    {
      "source": "2470fcf0f89082de874ac9133ccb3a8667dd89a8",
      "target": "5c0b56d9440ea70c79d1d0032d46c14e06f541f5",
      "weight": 0.30575982075325814
    },
    {
      "source": "2470fcf0f89082de874ac9133ccb3a8667dd89a8",
      "target": "cc9f2fd320a279741403c4bfbeb91179803c428c",
      "weight": 0.29503724134164316
    },
    {
      "source": "2470fcf0f89082de874ac9133ccb3a8667dd89a8",
      "target": "12075ea34f5fbe32ec5582786761ab34d401209b",
      "weight": 0.11596450753429821
    },
    {
      "source": "2470fcf0f89082de874ac9133ccb3a8667dd89a8",
      "target": "9e5fe2ba652774ba3b1127f626c192668a907132",
      "weight": 0.26633308381327664
    },
    {
      "source": "2470fcf0f89082de874ac9133ccb3a8667dd89a8",
      "target": "1d4288c47a7802575d2a0d231d1283a4f225a85b",
      "weight": 0.29021862521794234
    },
    {
      "source": "2470fcf0f89082de874ac9133ccb3a8667dd89a8",
      "target": "de93c8aed64229571b03e40b36499d4f07ce875d",
      "weight": 0.2470380748547566
    },
    {
      "source": "2470fcf0f89082de874ac9133ccb3a8667dd89a8",
      "target": "a25b645c3d24f91164230a0ac5bb2d4ec88c1538",
      "weight": 0.37538263193843735
    },
    {
      "source": "2470fcf0f89082de874ac9133ccb3a8667dd89a8",
      "target": "68c108795deef06fa929d1f6e96b75dbf7ce8531",
      "weight": 0.17383803325410346
    },
    {
      "source": "2470fcf0f89082de874ac9133ccb3a8667dd89a8",
      "target": "3c3093f5f8e9601637612fcfb8f160f116fa30e4",
      "weight": 0.3331641555409265
    },
    {
      "source": "2470fcf0f89082de874ac9133ccb3a8667dd89a8",
      "target": "2064020586d5832b55f80a7dffea1fd90a5d94dd",
      "weight": 0.2729270662796385
    },
    {
      "source": "2470fcf0f89082de874ac9133ccb3a8667dd89a8",
      "target": "0f810eb4777fd05317951ebaa7a3f5835ee84cf4",
      "weight": 0.31500815677970595
    },
    {
      "source": "2470fcf0f89082de874ac9133ccb3a8667dd89a8",
      "target": "0fcb2034e31a2bc2f12a2b1363d0d77baf445fdf",
      "weight": 0.34882617578850444
    },
    {
      "source": "2470fcf0f89082de874ac9133ccb3a8667dd89a8",
      "target": "fb3c6456708b0e143f545d77dc8ec804eb947395",
      "weight": 0.392408063822497
    },
    {
      "source": "68c108795deef06fa929d1f6e96b75dbf7ce8531",
      "target": "53db22a9d4ae77dd8218ba867184898adc84d1d1",
      "weight": 0.2910344609691362
    },
    {
      "source": "68c108795deef06fa929d1f6e96b75dbf7ce8531",
      "target": "5bac7d00035bc1e246a34f9ee3152b290f97bb92",
      "weight": 0.26819174796953427
    },
    {
      "source": "68c108795deef06fa929d1f6e96b75dbf7ce8531",
      "target": "7f437f4af59ff994d97482ee1c12aaeb4b310e85",
      "weight": 0.2956176405354931
    },
    {
      "source": "68c108795deef06fa929d1f6e96b75dbf7ce8531",
      "target": "83f1343500c9f0df62da0d61736738d8d7a9bba0",
      "weight": 0.31882812582266945
    },
    {
      "source": "68c108795deef06fa929d1f6e96b75dbf7ce8531",
      "target": "61f371768cdc093828f432660e22f7a17f22e2af",
      "weight": 0.351632327046528
    },
    {
      "source": "68c108795deef06fa929d1f6e96b75dbf7ce8531",
      "target": "1d4288c47a7802575d2a0d231d1283a4f225a85b",
      "weight": 0.4254564825845717
    },
    {
      "source": "431dc05ac25510de6264084434254cca877f9ab3",
      "target": "b2d827c286e32dbf0739e8c796b119b1074809b4",
      "weight": 0.3046649406120742
    },
    {
      "source": "431dc05ac25510de6264084434254cca877f9ab3",
      "target": "8ca9a74503c240b2746e351995ee0415657f1cd0",
      "weight": 0.1467760676997516
    },
    {
      "source": "431dc05ac25510de6264084434254cca877f9ab3",
      "target": "9eed36fb9b9bbb97579953d5e303dc0cf6c70a58",
      "weight": 0.566018815149089
    },
    {
      "source": "431dc05ac25510de6264084434254cca877f9ab3",
      "target": "69bdc99655204190697067c3da5296e544e6865d",
      "weight": 0.46761746375011437
    },
    {
      "source": "431dc05ac25510de6264084434254cca877f9ab3",
      "target": "0ba7f2e592dda172bc3d07f88cdcf2a57830deec",
      "weight": 0.4248648554216391
    },
    {
      "source": "2c23085488337c4c1b5673b8d0f4ac95bda73529",
      "target": "086a4d45a9deef5a10ee4febcd4c92c95a6305de",
      "weight": 0.35688844527267627
    },
    {
      "source": "2c23085488337c4c1b5673b8d0f4ac95bda73529",
      "target": "b44b0e8222021b51783c62b0bce071ef6fb3f12c",
      "weight": 0.4042540987188824
    },
    {
      "source": "2c23085488337c4c1b5673b8d0f4ac95bda73529",
      "target": "2807f9c666335946113fb11dccadf36f8d78b772",
      "weight": 0.39282960981700565
    },
    {
      "source": "2c23085488337c4c1b5673b8d0f4ac95bda73529",
      "target": "714b7c61d81687d093fd8b7d6d6737d582a4c9b7",
      "weight": 0.2532968334642266
    },
    {
      "source": "2064020586d5832b55f80a7dffea1fd90a5d94dd",
      "target": "8357670aac3c98a71b454ab5bca89558f265369d",
      "weight": 0.2783559912807223
    },
    {
      "source": "2064020586d5832b55f80a7dffea1fd90a5d94dd",
      "target": "c64a99ec780d5e1c6ba242a82e72e96eb0c3ef8a",
      "weight": 0.05179321340767725
    },
    {
      "source": "2064020586d5832b55f80a7dffea1fd90a5d94dd",
      "target": "3f673101c2cac3b47639056e2988e018546c3c90",
      "weight": 0.2834774785061544
    },
    {
      "source": "2064020586d5832b55f80a7dffea1fd90a5d94dd",
      "target": "5fd3ce235f5fcebd3d2807f710b060add527183b",
      "weight": 0.2895074554908033
    },
    {
      "source": "2064020586d5832b55f80a7dffea1fd90a5d94dd",
      "target": "3c3093f5f8e9601637612fcfb8f160f116fa30e4",
      "weight": 0.36912400044274307
    },
    {
      "source": "f3a63a840185fa8c5e0db4bbe12afa7d3de7d029",
      "target": "4a3e88d203564e547f5fb3f3d816a0b381492eae",
      "weight": 0.2921137317889563
    },
    {
      "source": "12075ea34f5fbe32ec5582786761ab34d401209b",
      "target": "08e84c939b88fc50aaa74ef76e202e61a1ad940b",
      "weight": 0.24661492191317294
    },
    {
      "source": "12075ea34f5fbe32ec5582786761ab34d401209b",
      "target": "b31c76815615c16cc8505dbb38d2921f921c029d",
      "weight": 0.2376057306395017
    },
    {
      "source": "dc05886db1e6f17f4489d867477b38fe13e31783",
      "target": "1a73038804052a40c12aae696848ece2168f6da7",
      "weight": 0.3575903134369849
    },
    {
      "source": "6bf9b58b8f418fb2922762550fb78bb22c83c0f8",
      "target": "cb7ea0176eec1f809f3bc3a34aeb279d44dda612",
      "weight": 0.2842772079540884
    },
    {
      "source": "6bf9b58b8f418fb2922762550fb78bb22c83c0f8",
      "target": "f3a63a840185fa8c5e0db4bbe12afa7d3de7d029",
      "weight": 0.2878656442145666
    },
    {
      "source": "3c3093f5f8e9601637612fcfb8f160f116fa30e4",
      "target": "d06737f9395e592f35ef251e09bea1c18037b096",
      "weight": 0.26399727397169925
    },
    {
      "source": "3c3093f5f8e9601637612fcfb8f160f116fa30e4",
      "target": "714b7c61d81687d093fd8b7d6d6737d582a4c9b7",
      "weight": 0.2568155744412553
    },
    {
      "source": "b2004f4f19bc6ccae8e4afc554f39142870100f5",
      "target": "4186f74da6d793c91c5ca4d8a10cf2f8638eade6",
      "weight": 0.36241259289736394
    },
    {
      "source": "b2004f4f19bc6ccae8e4afc554f39142870100f5",
      "target": "bd38cbbb346a347cb5b60ac4a133b3d73cb44e07",
      "weight": 0.32857158283958493
    },
    {
      "source": "61f371768cdc093828f432660e22f7a17f22e2af",
      "target": "c734971c6000e3f2769ab5165d00816af80dd76f",
      "weight": 0.2780396577063506
    },
    {
      "source": "61f371768cdc093828f432660e22f7a17f22e2af",
      "target": "5bac7d00035bc1e246a34f9ee3152b290f97bb92",
      "weight": 0.28539310047572913
    },
    {
      "source": "0f810eb4777fd05317951ebaa7a3f5835ee84cf4",
      "target": "6bf550ce7f10a4347d448eb810a92e1a5cfffa4b",
      "weight": 0.09663701827677516
    },
    {
      "source": "0f810eb4777fd05317951ebaa7a3f5835ee84cf4",
      "target": "c64a99ec780d5e1c6ba242a82e72e96eb0c3ef8a",
      "weight": 0.04236511563511247
    },
    {
      "source": "0f810eb4777fd05317951ebaa7a3f5835ee84cf4",
      "target": "5c0b56d9440ea70c79d1d0032d46c14e06f541f5",
      "weight": 0.22867279073978883
    },
    {
      "source": "0f810eb4777fd05317951ebaa7a3f5835ee84cf4",
      "target": "12075ea34f5fbe32ec5582786761ab34d401209b",
      "weight": 0.06905667959923992
    },
    {
      "source": "535d184eadf47fa17ce4073b6e2f180783e85300",
      "target": "1f577f6f0785c7967bef4ac6d0ab0370c2815b4d",
      "weight": 0.2849805402817055
    },
    {
      "source": "535d184eadf47fa17ce4073b6e2f180783e85300",
      "target": "52a6657cf1cb3cc847695a386bd65b5eea34bc13",
      "weight": 0.2927932151624245
    },
    {
      "source": "f6b218453170edcbb51e49dd44ba2f83af53ef92",
      "target": "1a73038804052a40c12aae696848ece2168f6da7",
      "weight": 0.25939356395470653
    },
    {
      "source": "f6b218453170edcbb51e49dd44ba2f83af53ef92",
      "target": "bd8aaab29fa16f40ef016393ba7ca30127abab58",
      "weight": 0.2568359056844505
    },
    {
      "source": "f6b218453170edcbb51e49dd44ba2f83af53ef92",
      "target": "12075ea34f5fbe32ec5582786761ab34d401209b",
      "weight": 0.0503411824939956
    },
    {
      "source": "04615a9955bce148aa7ba29e864389c26e10523a",
      "target": "134acf45a016fced57cc8f8e171eeb6e0015b0b8",
      "weight": 0.24354709367892838
    },
    {
      "source": "c64a99ec780d5e1c6ba242a82e72e96eb0c3ef8a",
      "target": "e4fef8d5864c5468100ca167639ef3fa374c0442",
      "weight": 0.26799299462273735
    },
    {
      "source": "c64a99ec780d5e1c6ba242a82e72e96eb0c3ef8a",
      "target": "6bf550ce7f10a4347d448eb810a92e1a5cfffa4b",
      "weight": 0.03644227957073561
    },
    {
      "source": "399806e861a2ef960a81b37b593c2176a728c399",
      "target": "2807f9c666335946113fb11dccadf36f8d78b772",
      "weight": 0.3482292512058498
    },
    {
      "source": "399806e861a2ef960a81b37b593c2176a728c399",
      "target": "bd38cbbb346a347cb5b60ac4a133b3d73cb44e07",
      "weight": 0.3439951229566306
    },
    {
      "source": "3e0925355554e3aeb99de8165c268582a82de3bb",
      "target": "134acf45a016fced57cc8f8e171eeb6e0015b0b8",
      "weight": 0.27825124972882276
    },
    {
      "source": "442e9f1e8f6218e68f944fd3028c5385691d4112",
      "target": "cce1245ba1ec154120b3b256faf7bf28f769b505",
      "weight": 0.2522283256427173
    },
    {
      "source": "a25b645c3d24f91164230a0ac5bb2d4ec88c1538",
      "target": "e4fef8d5864c5468100ca167639ef3fa374c0442",
      "weight": 0.2983653313923197
    },
    {
      "source": "a25b645c3d24f91164230a0ac5bb2d4ec88c1538",
      "target": "12075ea34f5fbe32ec5582786761ab34d401209b",
      "weight": 0.07177358913573015
    },
    {
      "source": "0ba7f2e592dda172bc3d07f88cdcf2a57830deec",
      "target": "12075ea34f5fbe32ec5582786761ab34d401209b",
      "weight": 0.035482429200085125
    },
    {
      "source": "46eb68c585bdb8a1051dfda98b4b35610301264f",
      "target": "e401ba782c2da93959582295089d3f04a051d6c1",
      "weight": 0.2659287858915112
    },
    {
      "source": "cf628a42ee56c8f1b858790822a2bc0a61a49110",
      "target": "134acf45a016fced57cc8f8e171eeb6e0015b0b8",
      "weight": 0.5602556393516129
    },
    {
      "source": "5fd3ce235f5fcebd3d2807f710b060add527183b",
      "target": "c64a99ec780d5e1c6ba242a82e72e96eb0c3ef8a",
      "weight": 0.12661321685756277
    },
    {
      "source": "5fd3ce235f5fcebd3d2807f710b060add527183b",
      "target": "12075ea34f5fbe32ec5582786761ab34d401209b",
      "weight": 0.06211404560412354
    },
    {
      "source": "1d4288c47a7802575d2a0d231d1283a4f225a85b",
      "target": "70e1d6b227fdd605fe61239a953e803df97e521d",
      "weight": 0.26379506947213516
    },
    {
      "source": "ecf5dc817fd6326e943b759c889d1285e673b24a",
      "target": "a694a3fb49f3c6e63e2fa5ddb72fe3aa490592f1",
      "weight": 0.45674158042863044
    },
    {
      "source": "b0c40766974df3eae8ff500379e66e5566cd16c9",
      "target": "200726cba07dec06a56ff46aa38836e9730a23a2",
      "weight": 0.2770563368867347
    },
    {
      "source": "9e5fe2ba652774ba3b1127f626c192668a907132",
      "target": "9eed36fb9b9bbb97579953d5e303dc0cf6c70a58",
      "weight": 0.31421456603813364
    },
    {
      "source": "9e5fe2ba652774ba3b1127f626c192668a907132",
      "target": "09da56cd3bf72b632c43969be97874fa14a3765c",
      "weight": 0.29409023872287465
    },
    {
      "source": "de93c8aed64229571b03e40b36499d4f07ce875d",
      "target": "5dce3f2cfe018ada4bebeaa15aac6fe1d01b18ba",
      "weight": 0.2655400650789921
    },
    {
      "source": "9d1445f1845a2880ff9c752845660e9c294aa7b5",
      "target": "e825e6325dfb5b93066817e7cc6b226ba7d3b799",
      "weight": 0.36207433640752096
    },
    {
      "source": "117cd9e1dafc577e53e2d46897a784ed1e65996f",
      "target": "ed8ea3d06c173849f02ee8afcf8db07df0f31261",
      "weight": 0.24727236954637355
    },
    {
      "source": "bd38cbbb346a347cb5b60ac4a133b3d73cb44e07",
      "target": "4a3e88d203564e547f5fb3f3d816a0b381492eae",
      "weight": 0.2603281635739698
    },
    {
      "source": "bd38cbbb346a347cb5b60ac4a133b3d73cb44e07",
      "target": "e5d6cca71ea0fb216a25f86e96d3480886fdba27",
      "weight": 0.3783481552934519
    },
    {
      "source": "bd38cbbb346a347cb5b60ac4a133b3d73cb44e07",
      "target": "e825e6325dfb5b93066817e7cc6b226ba7d3b799",
      "weight": 0.36787861546894846
    },
    {
      "source": "139a84c6fc4887ce2374489d79af0df9e1e7e4d6",
      "target": "5ecf53ab083f72f10421225a7dde25eb51cb6b22",
      "weight": 0.2655117297223303
    },
    {
      "source": "4e98282f5f3f1a388b8d95380473d4ef4878266e",
      "target": "e4fef8d5864c5468100ca167639ef3fa374c0442",
      "weight": 0.27503215631375166
    },
    {
      "source": "c90aa0f206c6fd41c490c142f63f7ba046cae6b7",
      "target": "d06737f9395e592f35ef251e09bea1c18037b096",
      "weight": 0.2337015566750475
    },
    {
      "source": "c90aa0f206c6fd41c490c142f63f7ba046cae6b7",
      "target": "39d2839aa4c3d8c0e64553891fe98ba261703154",
      "weight": 0.5050202891299516
    },
    {
      "source": "81fdf2b3db2b3e7be90b51866a31b73587eecd30",
      "target": "cce1245ba1ec154120b3b256faf7bf28f769b505",
      "weight": 0.4426171209974608
    },
    {
      "source": "7da3eebf5c2f481bc38ae6115cec3fc71dc87f2b",
      "target": "39d2839aa4c3d8c0e64553891fe98ba261703154",
      "weight": 0.3956147222111286
    },
    {
      "source": "d8bc75fbcfdecd5cd1952524d3e07db13722f5ff",
      "target": "17682d6f13dcac703092e0f1500a4197e46bbf2c",
      "weight": 0.3556681837132621
    }
  ]
}