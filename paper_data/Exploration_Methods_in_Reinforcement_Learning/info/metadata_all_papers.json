{
    "c28ec2a40a2c77e20d64cf1c85dc931106df8e83.pdf": {
        "title": "Overcoming Exploration in Reinforcement Learning with Demonstrations",
        "authors": [
            "Ashvin Nair",
            "Bob McGrew",
            "Marcin Andrychowicz",
            "Wojciech Zaremba",
            "P. Abbeel"
        ],
        "published_date": "2017",
        "abstract": "Exploration in environments with sparse rewards has been a persistent problem in reinforcement learning (RL). Many tasks are natural to specify with a sparse reward, and manually shaping a reward function can result in suboptimal performance. However, finding a non-zero reward is exponentially more difficult with increasing task horizon or action dimensionality. This puts many real-world tasks out of practical reach of RL methods. In this work, we use demonstrations to overcome the exploration problem and successfully learn to perform long-horizon, multi-step robotics tasks with continuous control such as stacking blocks with a robot arm. Our method, which builds on top of Deep Deterministic Policy Gradients and Hindsight Experience Replay, provides an order of magnitude of speedup over RL on simulated robotics tasks. It is simple to implement and makes only the additional assumption that we can collect a small set of demonstrations. Furthermore, our method is able to solve tasks not solvable by either RL or behavior cloning alone, and often ends up outperforming the demonstrator policy.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/c28ec2a40a2c77e20d64cf1c85dc931106df8e83.pdf",
        "venue": "IEEE International Conference on Robotics and Automation",
        "citationCount": 814,
        "score": 101.75
    },
    "0fcb2034e31a2bc2f12a2b1363d0d77baf445fdf.pdf": {
        "title": "#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning",
        "authors": [
            "Haoran Tang",
            "Rein Houthooft",
            "Davis Foote",
            "Adam Stooke",
            "Xi Chen",
            "Yan Duan",
            "John Schulman",
            "F. Turck",
            "P. Abbeel"
        ],
        "published_date": "2016",
        "abstract": "Count-based exploration algorithms are known to perform near-optimally when used in conjunction with tabular reinforcement learning (RL) methods for solving small discrete Markov decision processes (MDPs). It is generally thought that count-based methods cannot be applied in high-dimensional state spaces, since most states will only occur once. Recent deep RL exploration strategies are able to deal with high-dimensional continuous state spaces through complex heuristics, often relying on optimism in the face of uncertainty or intrinsic motivation. In this work, we describe a surprising finding: a simple generalization of the classic count-based approach can reach near state-of-the-art performance on various high-dimensional and/or continuous deep RL benchmarks. States are mapped to hash codes, which allows to count their occurrences with a hash table. These counts are then used to compute a reward bonus according to the classic count-based exploration theory. We find that simple hash functions can achieve surprisingly good results on many challenging tasks. Furthermore, we show that a domain-dependent learned hash code may further improve these results. Detailed analysis reveals important aspects of a good hash function: 1) having appropriate granularity and 2) encoding information relevant to solving the MDP. This exploration strategy achieves near state-of-the-art performance on both continuous control tasks and Atari 2600 games, hence providing a simple yet powerful baseline for solving MDPs that require considerable exploration.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/0fcb2034e31a2bc2f12a2b1363d0d77baf445fdf.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 798,
        "score": 88.66666666666666
    },
    "45f573f302dc7e77cbc5d1a74ccbac3564bbebc8.pdf": {
        "title": "PEBBLE: Feedback-Efficient Interactive Reinforcement Learning via Relabeling Experience and Unsupervised Pre-training",
        "authors": [
            "Kimin Lee",
            "Laura M. Smith",
            "P. Abbeel"
        ],
        "published_date": "2021",
        "abstract": "Conveying complex objectives to reinforcement learning (RL) agents can often be difficult, involving meticulous design of reward functions that are sufficiently informative yet easy enough to provide. Human-in-the-loop RL methods allow practitioners to instead interactively teach agents through tailored feedback; however, such approaches have been challenging to scale since human feedback is very expensive. In this work, we aim to make this process more sample- and feedback-efficient. We present an off-policy, interactive RL algorithm that capitalizes on the strengths of both feedback and off-policy learning. Specifically, we learn a reward model by actively querying a teacher's preferences between two clips of behavior and use it to train an agent. To enable off-policy learning, we relabel all the agent's past experience when its reward model changes. We additionally show that pre-training our agents with unsupervised exploration substantially increases the mileage of its queries. We demonstrate that our approach is capable of learning tasks of higher complexity than previously considered by human-in-the-loop methods, including a variety of locomotion and robotic manipulation skills. We also show that our method is able to utilize real-time human feedback to effectively prevent reward exploitation and learn new behaviors that are difficult to specify with standard reward functions.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/45f573f302dc7e77cbc5d1a74ccbac3564bbebc8.pdf",
        "venue": "International Conference on Machine Learning",
        "citationCount": 316,
        "score": 79.0
    },
    "f8d8e192979ab5d8d593e76a0c4e2c7581778732.pdf": {
        "title": "Voronoi-Based Multi-Robot Autonomous Exploration in Unknown Environments via Deep Reinforcement Learning",
        "authors": [
            "Junyan Hu",
            "Hanlin Niu",
            "J. Carrasco",
            "B. Lennox",
            "F. Arvin"
        ],
        "published_date": "2020",
        "abstract": "Autonomous exploration is an important application of multi-vehicle systems, where a team of networked robots are coordinated to explore an unknown environment collaboratively. This technique has earned significant research interest due to its usefulness in search and rescue, fault detection and monitoring, localization and mapping, etc. In this paper, a novel cooperative exploration strategy is proposed for multiple mobile robots, which reduces the overall task completion time and energy costs compared to conventional methods. To efficiently navigate the networked robots during the collaborative tasks, a hierarchical control architecture is designed which contains a high-level decision making layer and a low-level target tracking layer. The proposed cooperative exploration approach is developed using dynamic Voronoi partitions, which minimizes duplicated exploration areas by assigning different target locations to individual robots. To deal with sudden obstacles in the unknown environment, an integrated deep reinforcement learning based collision avoidance algorithm is then proposed, which enables the control policy to learn from human demonstration data and thus improve the learning speed and performance. Finally, simulation and experimental results are provided to demonstrate the effectiveness of the proposed scheme.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/f8d8e192979ab5d8d593e76a0c4e2c7581778732.pdf",
        "venue": "IEEE Transactions on Vehicular Technology",
        "citationCount": 291,
        "score": 58.2
    },
    "2470fcf0f89082de874ac9133ccb3a8667dd89a8.pdf": {
        "title": "Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models",
        "authors": [
            "Bradly C. Stadie",
            "S. Levine",
            "P. Abbeel"
        ],
        "published_date": "2015",
        "abstract": "Achieving efficient and scalable exploration in complex domains poses a major challenge in reinforcement learning. While Bayesian and PAC-MDP approaches to the exploration problem offer strong formal guarantees, they are often impractical in higher dimensions due to their reliance on enumerating the state-action space. Hence, exploration in complex domains is often performed with simple epsilon-greedy methods. In this paper, we consider the challenging Atari games domain, which requires processing raw pixel inputs and delayed rewards. We evaluate several more sophisticated exploration strategies, including Thompson sampling and Boltzman exploration, and propose a new exploration method based on assigning exploration bonuses from a concurrently learned model of the system dynamics. By parameterizing our learned model with a neural network, we are able to develop a scalable and efficient approach to exploration bonuses that can be applied to tasks with complex, high-dimensional state spaces. In the Atari domain, our method provides the most consistent improvement across a range of games that pose a major challenge for prior methods. In addition to raw game-scores, we also develop an AUC-100 metric for the Atari Learning domain to evaluate the impact of exploration on this benchmark.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/2470fcf0f89082de874ac9133ccb3a8667dd89a8.pdf",
        "venue": "arXiv.org",
        "citationCount": 512,
        "score": 51.2
    },
    "68c108795deef06fa929d1f6e96b75dbf7ce8531.pdf": {
        "title": "Meta-Reinforcement Learning of Structured Exploration Strategies",
        "authors": [
            "Abhishek Gupta",
            "Russell Mendonca",
            "Yuxuan Liu",
            "P. Abbeel",
            "S. Levine"
        ],
        "published_date": "2018",
        "abstract": "Exploration is a fundamental challenge in reinforcement learning (RL). Many of the current exploration methods for deep RL use task-agnostic objectives, such as information gain or bonuses based on state visitation. However, many practical applications of RL involve learning more than a single task, and prior tasks can be used to inform how exploration should be performed in new tasks. In this work, we explore how prior tasks can inform an agent about how to explore effectively in new situations. We introduce a novel gradient-based fast adaptation algorithm -- model agnostic exploration with structured noise (MAESN) -- to learn exploration strategies from prior experience. The prior experience is used both to initialize a policy and to acquire a latent exploration space that can inject structured stochasticity into a policy, producing exploration strategies that are informed by prior knowledge and are more effective than random action-space noise. We show that MAESN is more effective at learning exploration strategies when compared to prior meta-RL methods, RL without learned exploration strategies, and task-agnostic exploration methods. We evaluate our method on a variety of simulated tasks: locomotion with a wheeled robot, locomotion with a quadrupedal walker, and object manipulation.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/68c108795deef06fa929d1f6e96b75dbf7ce8531.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 357,
        "score": 51.0
    },
    "431dc05ac25510de6264084434254cca877f9ab3.pdf": {
        "title": "Recovery RL: Safe Reinforcement Learning With Learned Recovery Zones",
        "authors": [
            "Brijen Thananjeyan",
            "A. Balakrishna",
            "Suraj Nair",
            "Michael Luo",
            "K. Srinivasan",
            "M. Hwang",
            "Joseph E. Gonzalez",
            "Julian Ibarz",
            "Chelsea Finn",
            "Ken Goldberg"
        ],
        "published_date": "2020",
        "abstract": "Safety remains a central obstacle preventing widespread use of RL in the real world: learning new tasks in uncertain environments requires extensive exploration, but safety requires limiting exploration. We propose Recovery RL, an algorithm which navigates this tradeoff by (1) leveraging offline data to learn about constraint violating zones before policy learning and (2) separating the goals of improving task performance and constraint satisfaction across two policies: a task policy that only optimizes the task reward and a recovery policy that guides the agent to safety when constraint violation is likely. We evaluate Recovery RL on 6 simulation domains, including two contact-rich manipulation tasks and an image-based navigation task, and an image-based obstacle avoidance task on a physical robot. We compare Recovery RL to 5 prior safe RL methods which jointly optimize for task performance and safety via constrained optimization or reward shaping and find that Recovery RL outperforms the next best prior method across all domains. Results suggest that Recovery RL trades off constraint violations and task successes 2\u201320 times more efficiently in simulation domains and 3 times more efficiently in physical experiments. See https://tinyurl.com/rl-recovery for videos and supplementary material.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/431dc05ac25510de6264084434254cca877f9ab3.pdf",
        "venue": "IEEE Robotics and Automation Letters",
        "citationCount": 247,
        "score": 49.400000000000006
    },
    "2c23085488337c4c1b5673b8d0f4ac95bda73529.pdf": {
        "title": "Uncertainty Weighted Actor-Critic for Offline Reinforcement Learning",
        "authors": [
            "Yue Wu",
            "Shuangfei Zhai",
            "Nitish Srivastava",
            "J. Susskind",
            "Jian Zhang",
            "R. Salakhutdinov",
            "Hanlin Goh"
        ],
        "published_date": "2021",
        "abstract": "Offline Reinforcement Learning promises to learn effective policies from previously-collected, static datasets without the need for exploration. However, existing Q-learning and actor-critic based off-policy RL algorithms fail when bootstrapping from out-of-distribution (OOD) actions or states. We hypothesize that a key missing ingredient from the existing methods is a proper treatment of uncertainty in the offline setting. We propose Uncertainty Weighted Actor-Critic (UWAC), an algorithm that detects OOD state-action pairs and down-weights their contribution in the training objectives accordingly. Implementation-wise, we adopt a practical and effective dropout-based uncertainty estimation method that introduces very little overhead over existing RL algorithms. Empirically, we observe that UWAC substantially improves model stability during training. In addition, UWAC out-performs existing offline RL methods on a variety of competitive tasks, and achieves significant performance gains over the state-of-the-art baseline on datasets with sparse demonstrations collected from human experts.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/2c23085488337c4c1b5673b8d0f4ac95bda73529.pdf",
        "venue": "International Conference on Machine Learning",
        "citationCount": 196,
        "score": 49.0
    },
    "2064020586d5832b55f80a7dffea1fd90a5d94dd.pdf": {
        "title": "Improving Exploration in Evolution Strategies for Deep Reinforcement Learning via a Population of Novelty-Seeking Agents",
        "authors": [
            "Edoardo Conti",
            "Vashisht Madhavan",
            "F. Such",
            "J. Lehman",
            "Kenneth O. Stanley",
            "J. Clune"
        ],
        "published_date": "2017",
        "abstract": "Evolution strategies (ES) are a family of black-box optimization algorithms able to train deep neural networks roughly as well as Q-learning and policy gradient methods on challenging deep reinforcement learning (RL) problems, but are much faster (e.g. hours vs. days) because they parallelize better. However, many RL problems require directed exploration because they have reward functions that are sparse or deceptive (i.e. contain local optima), and it is unknown how to encourage such exploration with ES. Here we show that algorithms that have been invented to promote directed exploration in small-scale evolved neural networks via populations of exploring agents, specifically novelty search (NS) and quality diversity (QD) algorithms, can be hybridized with ES to improve its performance on sparse or deceptive deep RL tasks, while retaining scalability. Our experiments confirm that the resultant new algorithms, NS-ES and two QD algorithms, NSR-ES and NSRA-ES, avoid local optima encountered by ES to achieve higher performance on Atari and simulated robots learning to walk around a deceptive trap. This paper thus introduces a family of fast, scalable algorithms for reinforcement learning that are capable of directed exploration. It also adds this new family of exploration algorithms to the RL toolbox and raises the interesting possibility that analogous algorithms with multiple simultaneous paths of exploration might also combine well with existing RL algorithms outside ES.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/2064020586d5832b55f80a7dffea1fd90a5d94dd.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 351,
        "score": 43.875
    },
    "1c35807e1a4c24e2013fa0a090cee9cc4716a5f5.pdf": {
        "title": "Reinforcement Learning with Action-Free Pre-Training from Videos",
        "authors": [
            "Younggyo Seo",
            "Kimin Lee",
            "Stephen James",
            "P. Abbeel"
        ],
        "published_date": "2022",
        "abstract": "Recent unsupervised pre-training methods have shown to be effective on language and vision domains by learning useful representations for multiple downstream tasks. In this paper, we investigate if such unsupervised pre-training methods can also be effective for vision-based reinforcement learning (RL). To this end, we introduce a framework that learns representations useful for understanding the dynamics via generative pre-training on videos. Our framework consists of two phases: we pre-train an action-free latent video prediction model, and then utilize the pre-trained representations for efficiently learning action-conditional world models on unseen environments. To incorporate additional action inputs during fine-tuning, we introduce a new architecture that stacks an action-conditional latent prediction model on top of the pre-trained action-free prediction model. Moreover, for better exploration, we propose a video-based intrinsic bonus that leverages pre-trained representations. We demonstrate that our framework significantly improves both final performances and sample-efficiency of vision-based RL in a variety of manipulation and locomotion tasks. Code is available at https://github.com/younggyoseo/apv.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/1c35807e1a4c24e2013fa0a090cee9cc4716a5f5.pdf",
        "venue": "International Conference on Machine Learning",
        "citationCount": 129,
        "score": 43.0
    },
    "f3a63a840185fa8c5e0db4bbe12afa7d3de7d029.pdf": {
        "title": "Jump-Start Reinforcement Learning",
        "authors": [
            "Ikechukwu Uchendu",
            "Ted Xiao",
            "Yao Lu",
            "Banghua Zhu",
            "Mengyuan Yan",
            "J. Sim\u00f3n",
            "Matthew Bennice",
            "Chuyuan Fu",
            "Cong Ma",
            "Jiantao Jiao",
            "S. Levine",
            "Karol Hausman"
        ],
        "published_date": "2022",
        "abstract": "Reinforcement learning (RL) provides a theoretical framework for continuously improving an agent's behavior via trial and error. However, efficiently learning policies from scratch can be very difficult, particularly for tasks with exploration challenges. In such settings, it might be desirable to initialize RL with an existing policy, offline data, or demonstrations. However, naively performing such initialization in RL often works poorly, especially for value-based methods. In this paper, we present a meta algorithm that can use offline data, demonstrations, or a pre-existing policy to initialize an RL policy, and is compatible with any RL approach. In particular, we propose Jump-Start Reinforcement Learning (JSRL), an algorithm that employs two policies to solve tasks: a guide-policy, and an exploration-policy. By using the guide-policy to form a curriculum of starting states for the exploration-policy, we are able to efficiently improve performance on a set of simulated robotic tasks. We show via experiments that JSRL is able to significantly outperform existing imitation and reinforcement learning algorithms, particularly in the small-data regime. In addition, we provide an upper bound on the sample complexity of JSRL and show that with the help of a guide-policy, one can improve the sample complexity for non-optimism exploration methods from exponential in horizon to polynomial.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/f3a63a840185fa8c5e0db4bbe12afa7d3de7d029.pdf",
        "venue": "International Conference on Machine Learning",
        "citationCount": 127,
        "score": 42.33333333333333
    },
    "52a6657cf1cb3cc847695a386bd65b5eea34bc13.pdf": {
        "title": "Deep Reinforcement Learning-Based Automatic Exploration for Navigation in Unknown Environment",
        "authors": [
            "Haoran Li",
            "Qichao Zhang",
            "Dongbin Zhao"
        ],
        "published_date": "2020",
        "abstract": "This paper investigates the automatic exploration problem under the unknown environment, which is the key point of applying the robotic system to some social tasks. The solution to this problem via stacking decision rules is impossible to cover various environments and sensor properties. Learning-based control methods are adaptive for these scenarios. However, these methods are damaged by low learning efficiency and awkward transferability from simulation to reality. In this paper, we construct a general exploration framework via decomposing the exploration process into the decision, planning, and mapping modules, which increases the modularity of the robotic system. Based on this framework, we propose a deep reinforcement learning-based decision algorithm that uses a deep neural network to learning exploration strategy from the partial map. The results show that this proposed algorithm has better learning efficiency and adaptability for unknown environments. In addition, we conduct the experiments on the physical robot, and the results suggest that the learned policy can be well transferred from simulation to the real robot.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/52a6657cf1cb3cc847695a386bd65b5eea34bc13.pdf",
        "venue": "IEEE Transactions on Neural Networks and Learning Systems",
        "citationCount": 206,
        "score": 41.2
    },
    "12075ea34f5fbe32ec5582786761ab34d401209b.pdf": {
        "title": "Exploration in Deep Reinforcement Learning: From Single-Agent to Multiagent Domain",
        "authors": [
            "Tianpei Yang",
            "Hongyao Tang",
            "Chenjia Bai",
            "Jinyi Liu",
            "Jianye Hao",
            "Zhaopeng Meng",
            "Peng Liu",
            "Zhen Wang"
        ],
        "published_date": "2021",
        "abstract": "Deep reinforcement learning (DRL) and deep multiagent reinforcement learning (MARL) have achieved significant success across a wide range of domains, including game artificial intelligence (AI), autonomous vehicles, and robotics. However, DRL and deep MARL agents are widely known to be sample inefficient that millions of interactions are usually needed even for relatively simple problem settings, thus preventing the wide application and deployment in real-industry scenarios. One bottleneck challenge behind is the well-known exploration problem, i.e., how efficiently exploring the environment and collecting informative experiences that could benefit policy learning toward the optimal ones. This problem becomes more challenging in complex environments with sparse rewards, noisy distractions, long horizons, and nonstationary co-learners. In this article, we conduct a comprehensive survey on existing exploration methods for both single-agent RL and multiagent RL. We start the survey by identifying several key challenges to efficient exploration. Then, we provide a systematic survey of existing approaches by classifying them into two major categories: uncertainty-oriented exploration and intrinsic motivation-oriented exploration. Beyond the above two main branches, we also include other notable exploration methods with different ideas and techniques. In addition to algorithmic analysis, we provide a comprehensive and unified empirical comparison of different exploration methods for DRL on a set of commonly used benchmarks. According to our algorithmic and empirical investigation, we finally summarize the open problems of exploration in DRL and deep MARL and point out a few future directions.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/12075ea34f5fbe32ec5582786761ab34d401209b.pdf",
        "venue": "IEEE Transactions on Neural Networks and Learning Systems",
        "citationCount": 120,
        "score": 30.0
    },
    "dc05886db1e6f17f4489d867477b38fe13e31783.pdf": {
        "title": "Network Randomization: A Simple Technique for Generalization in Deep Reinforcement Learning",
        "authors": [
            "Kimin Lee",
            "Kibok Lee",
            "Jinwoo Shin",
            "Honglak Lee"
        ],
        "published_date": "2019",
        "abstract": "Deep reinforcement learning (RL) agents often fail to generalize to unseen environments (yet semantically similar to trained agents), particularly when they are trained on high-dimensional state spaces, such as images. In this paper, we propose a simple technique to improve a generalization ability of deep RL agents by introducing a randomized (convolutional) neural network that randomly perturbs input observations. It enables trained agents to adapt to new domains by learning robust features invariant across varied and randomized environments. Furthermore, we consider an inference method based on the Monte Carlo approximation to reduce the variance induced by this randomization. We demonstrate the superiority of our method across 2D CoinRun, 3D DeepMind Lab exploration and 3D robotics control tasks: it significantly outperforms various regularization and data augmentation methods for the same purpose.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/dc05886db1e6f17f4489d867477b38fe13e31783.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 178,
        "score": 29.666666666666664
    },
    "6bf9b58b8f418fb2922762550fb78bb22c83c0f8.pdf": {
        "title": "Variational Policy Gradient Method for Reinforcement Learning with General Utilities",
        "authors": [
            "Junyu Zhang",
            "Alec Koppel",
            "A. S. Bedi",
            "Csaba Szepesvari",
            "Mengdi Wang"
        ],
        "published_date": "2020",
        "abstract": "In recent years, reinforcement learning (RL) systems with general goals beyond a cumulative sum of rewards have gained traction, such as in constrained problems, exploration, and acting upon prior experiences. In this paper, we consider policy optimization in Markov Decision Problems, where the objective is a general concave utility function of the state-action occupancy measure, which subsumes several of the aforementioned examples as special cases. Such generality invalidates the Bellman equation. As this means that dynamic programming no longer works, we focus on direct policy search. Analogously to the Policy Gradient Theorem \\cite{sutton2000policy} available for RL with cumulative rewards, we derive a new Variational Policy Gradient Theorem for RL with general utilities, which establishes that the parametrized policy gradient may be obtained as the solution of a stochastic saddle point problem involving the Fenchel dual of the utility function. We develop a variational Monte Carlo gradient estimation algorithm to compute the policy gradient based on sample paths. We prove that the variational policy gradient scheme converges globally to the optimal policy for the general objective, though the optimization problem is nonconvex. We also establish its rate of convergence of the order $O(1/t)$ by exploiting the hidden convexity of the problem, and proves that it converges exponentially when the problem admits hidden strong convexity. Our analysis applies to the standard RL problem with cumulative rewards as a special case, in which case our result improves the available convergence rate.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/6bf9b58b8f418fb2922762550fb78bb22c83c0f8.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 146,
        "score": 29.200000000000003
    },
    "6ce21379ffac786207632d16ea7d6e3eb150f910.pdf": {
        "title": "Deep Reinforcement Learning for Dynamic Flexible Job Shop Scheduling with Random Job Arrival",
        "authors": [
            "Jingru Chang",
            "Dong Yu",
            "Y. Hu",
            "Wuwei He",
            "Haoyu Yu"
        ],
        "published_date": "2022",
        "abstract": "The production process of a smart factory is complex and dynamic. As the core of manufacturing management, the research into the flexible job shop scheduling problem (FJSP) focuses on optimizing scheduling decisions in real time, according to the changes in the production environment. In this paper, deep reinforcement learning (DRL) is proposed to solve the dynamic FJSP (DFJSP) with random job arrival, with the goal of minimizing penalties for earliness and tardiness. A double deep Q-networks (DDQN) architecture is proposed and state features, actions and rewards are designed. A soft \u03b5-greedy behavior policy is designed according to the scale of the problem. The experimental results show that the proposed DRL is better than other reinforcement learning (RL) algorithms, heuristics and metaheuristics in terms of solution quality and generalization. In addition, the soft \u03b5-greedy strategy reasonably balances exploration and exploitation, thereby improving the learning efficiency of the scheduling agent. The DRL method is adaptive to the dynamic changes of the production environment in a flexible job shop, which contributes to the establishment of a flexible scheduling system with self-learning, real-time optimization and intelligent decision-making.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/6ce21379ffac786207632d16ea7d6e3eb150f910.pdf",
        "venue": "Processes",
        "citationCount": 82,
        "score": 27.333333333333332
    },
    "2e5dc4b028e670d1b9ac0b4e15580cdb7197cb68.pdf": {
        "title": "Exploration in Deep Reinforcement Learning: A Comprehensive Survey",
        "authors": [
            "Tianpei Yang",
            "Hongyao Tang",
            "Chenjia Bai",
            "Jinyi Liu",
            "Jianye Hao",
            "Zhaopeng Meng",
            "Peng Liu"
        ],
        "published_date": "2021",
        "abstract": "",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/2e5dc4b028e670d1b9ac0b4e15580cdb7197cb68.pdf",
        "venue": "arXiv.org",
        "citationCount": 103,
        "score": 25.75
    },
    "f593dc96b20ce8427182e773e3b2192d707706a8.pdf": {
        "title": "Hierarchical Planning Through Goal-Conditioned Offline Reinforcement Learning",
        "authors": [
            "Jinning Li",
            "Chen Tang",
            "M. Tomizuka",
            "Wei Zhan"
        ],
        "published_date": "2022",
        "abstract": "Offline Reinforcement learning (RL) has shown potent in many safe-critical tasks in robotics where exploration is risky and expensive. However, it still struggles to acquire skills in temporally extended tasks. In this paper, we study the problem of offline RL for temporally extended tasks. We propose a hierarchical planning framework, consisting of a low-level goal-conditioned RL policy and a high-level goal planner. The low-level policy is trained via offline RL. We improve the offline training to deal with out-of-distribution goals by a perturbed goal sampling process. The high-level planner selects intermediate sub-goals by taking advantages of model-based planning methods. It plans over future sub-goal sequences based on the learned value function of the low-level policy. We adopt a Conditional Variational Autoencoder to sample meaningful high-dimensional sub-goal candidates and to solve the high-level long-term strategy optimization problem. We evaluate our proposed method in long-horizon driving and robot navigation tasks. Experiments show that our method outperforms baselines with different hierarchical designs and other regular planners without hierarchy in these complex tasks.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/f593dc96b20ce8427182e773e3b2192d707706a8.pdf",
        "venue": "IEEE Robotics and Automation Letters",
        "citationCount": 65,
        "score": 21.666666666666664
    },
    "cc9f2fd320a279741403c4bfbeb91179803c428c.pdf": {
        "title": "Reward Uncertainty for Exploration in Preference-based Reinforcement Learning",
        "authors": [
            "Xi-Xi Liang",
            "Katherine Shu",
            "Kimin Lee",
            "P. Abbeel"
        ],
        "published_date": "2022",
        "abstract": "Conveying complex objectives to reinforcement learning (RL) agents often requires meticulous reward engineering. Preference-based RL methods are able to learn a more flexible reward model based on human preferences by actively incorporating human feedback, i.e. teacher's preferences between two clips of behaviors. However, poor feedback-efficiency still remains a problem in current preference-based RL algorithms, as tailored human feedback is very expensive. To handle this issue, previous methods have mainly focused on improving query selection and policy initialization. At the same time, recent exploration methods have proven to be a recipe for improving sample-efficiency in RL. We present an exploration method specifically for preference-based RL algorithms. Our main idea is to design an intrinsic reward by measuring the novelty based on learned reward. Specifically, we utilize disagreement across ensemble of learned reward models. Our intuition is that disagreement in learned reward model reflects uncertainty in tailored human feedback and could be useful for exploration. Our experiments show that exploration bonus from uncertainty in learned reward improves both feedback- and sample-efficiency of preference-based RL algorithms on complex robot manipulation tasks from MetaWorld benchmarks, compared with other existing exploration methods that measure the novelty of state visitation.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/cc9f2fd320a279741403c4bfbeb91179803c428c.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 59,
        "score": 19.666666666666664
    },
    "3c3093f5f8e9601637612fcfb8f160f116fa30e4.pdf": {
        "title": "Diversity-Driven Exploration Strategy for Deep Reinforcement Learning",
        "authors": [
            "Zhang-Wei Hong",
            "Tzu-Yun Shann",
            "Shih-Yang Su",
            "Yi-Hsiang Chang",
            "Chun-Yi Lee"
        ],
        "published_date": "2018",
        "abstract": "Efficient exploration remains a challenging research problem in reinforcement learning, especially when an environment contains large state spaces, deceptive local optima, or sparse rewards. To tackle this problem, we present a diversity-driven approach for exploration, which can be easily combined with both off- and on-policy reinforcement learning algorithms. We show that by simply adding a distance measure to the loss function, the proposed methodology significantly enhances an agent's exploratory behaviors, and thus preventing the policy from being trapped in local optima. We further propose an adaptive scaling method for stabilizing the learning process. Our experimental results in Atari 2600 show that our method outperforms baseline approaches in several tasks in terms of mean scores and exploration efficiency.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/3c3093f5f8e9601637612fcfb8f160f116fa30e4.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 128,
        "score": 18.285714285714285
    },
    "b2004f4f19bc6ccae8e4afc554f39142870100f5.pdf": {
        "title": "MoDem: Accelerating Visual Model-Based Reinforcement Learning with Demonstrations",
        "authors": [
            "Nicklas Hansen",
            "Yixin Lin",
            "H. Su",
            "Xiaolong Wang",
            "Vikash Kumar",
            "A. Rajeswaran"
        ],
        "published_date": "2022",
        "abstract": "Poor sample efficiency continues to be the primary challenge for deployment of deep Reinforcement Learning (RL) algorithms for real-world applications, and in particular for visuo-motor control. Model-based RL has the potential to be highly sample efficient by concurrently learning a world model and using synthetic rollouts for planning and policy improvement. However, in practice, sample-efficient learning with model-based RL is bottlenecked by the exploration challenge. In this work, we find that leveraging just a handful of demonstrations can dramatically improve the sample-efficiency of model-based RL. Simply appending demonstrations to the interaction dataset, however, does not suffice. We identify key ingredients for leveraging demonstrations in model learning -- policy pretraining, targeted exploration, and oversampling of demonstration data -- which forms the three phases of our model-based RL framework. We empirically study three complex visuo-motor control domains and find that our method is 150%-250% more successful in completing sparse reward tasks compared to prior approaches in the low data regime (100K interaction steps, 5 demonstrations). Code and videos are available at: https://nicklashansen.github.io/modemrl",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/b2004f4f19bc6ccae8e4afc554f39142870100f5.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 54,
        "score": 18.0
    },
    "61f371768cdc093828f432660e22f7a17f22e2af.pdf": {
        "title": "Offline Meta-Reinforcement Learning with Online Self-Supervision",
        "authors": [
            "Vitchyr H. Pong",
            "Ashvin Nair",
            "Laura M. Smith",
            "Catherine Huang",
            "S. Levine"
        ],
        "published_date": "2021",
        "abstract": "Meta-reinforcement learning (RL) methods can meta-train policies that adapt to new tasks with orders of magnitude less data than standard RL, but meta-training itself is costly and time-consuming. If we can meta-train on offline data, then we can reuse the same static dataset, labeled once with rewards for different tasks, to meta-train policies that adapt to a variety of new tasks at meta-test time. Although this capability would make meta-RL a practical tool for real-world use, offline meta-RL presents additional challenges beyond online meta-RL or standard offline RL settings. Meta-RL learns an exploration strategy that collects data for adapting, and also meta-trains a policy that quickly adapts to data from a new task. Since this policy was meta-trained on a fixed, offline dataset, it might behave unpredictably when adapting to data collected by the learned exploration strategy, which differs systematically from the offline data and thus induces distributional shift. We propose a hybrid offline meta-RL algorithm, which uses offline data with rewards to meta-train an adaptive policy, and then collects additional unsupervised online data, without any reward labels to bridge this distribution shift. By not requiring reward labels for online collection, this data can be much cheaper to collect. We compare our method to prior work on offline meta-RL on simulated robot locomotion and manipulation tasks and find that using additional unsupervised online data collection leads to a dramatic improvement in the adaptive capabilities of the meta-trained policies, matching the performance of fully online meta-RL on a range of challenging domains that require generalization to new tasks.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/61f371768cdc093828f432660e22f7a17f22e2af.pdf",
        "venue": "International Conference on Machine Learning",
        "citationCount": 70,
        "score": 17.5
    },
    "1d6beec78e720beab5adb0a5fce6fa6b846aed1a.pdf": {
        "title": "Local and global stimuli in reinforcement learning",
        "authors": [
            "Danyang Jia",
            "Hao Guo",
            "Z. Song",
            "Lei Shi",
            "Xinyang Deng",
            "M. Perc",
            "Zhen Wang"
        ],
        "published_date": "2021",
        "abstract": "In efforts to resolve social dilemmas, reinforcement learning is an alternative to imitation and exploration in evolutionary game theory. While imitation and exploration rely on the performance of neighbors, in reinforcement learning individuals alter their strategies based on their own performance in the past. For example, according to the Bush\u2013Mosteller model of reinforcement learning, an individual\u2019s strategy choice is driven by whether the received payoff satisfies a preset aspiration or not. Stimuli also play a key role in reinforcement learning in that they can determine whether a strategy should be kept or not. Here we use the Monte Carlo method to study pattern formation and phase transitions towards cooperation in social dilemmas that are driven by reinforcement learning. We distinguish local and global players according to the source of the stimulus they experience. While global players receive their stimuli from the whole neighborhood, local players focus solely on individual performance. We show that global players play a decisive role in ensuring cooperation, while local players fail in this regard, although both types of players show properties of \u2018moody cooperators\u2019. In particular, global players evoke stronger conditional cooperation in their neighborhoods based on direct reciprocity, which is rooted in the emerging spatial patterns and stronger interfaces around cooperative clusters.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/1d6beec78e720beab5adb0a5fce6fa6b846aed1a.pdf",
        "venue": "New Journal of Physics",
        "citationCount": 70,
        "score": 17.5
    },
    "ba44a95f1a8bc5765438d03c01137799e930c88d.pdf": {
        "title": "Barrier Lyapunov Function-Based Safe Reinforcement Learning for Autonomous Vehicles With Optimized Backstepping",
        "authors": [
            "Yuxiang Zhang",
            "Xiaoling Liang",
            "Dongyu Li",
            "S. Ge",
            "B. Gao",
            "Hong Chen",
            "Tong-heng Lee"
        ],
        "published_date": "2022",
        "abstract": "Guaranteed safety and performance under various circumstances remain technically critical and practically challenging for the wide deployment of autonomous vehicles. Safety-critical systems in general, require safe performance even during the reinforcement learning (RL) period. To address this issue, a Barrier Lyapunov Function-based safe RL (BLF-SRL) algorithm is proposed here for the formulated nonlinear system in strict-feedback form. This approach appropriately arranges and incorporates the BLF items into the optimized backstepping control method to constrain the state-variables in the designed safety region during learning. Wherein, thus, the optimal virtual/actual control in every backstepping subsystem is decomposed with BLF items and also with an adaptive uncertain item to be learned, which achieves safe exploration during the learning process. Then, the principle of Bellman optimality of continuous-time Hamilton\u2013Jacobi\u2013Bellman equation in every backstepping subsystem is satisfied with independently approximated actor and critic under the framework of actor-critic through the designed iterative updating. Eventually, the overall system control is optimized with the proposed BLF-SRL method. It is furthermore noteworthy that the variance of the attained control performance under uncertainty is also reduced with the proposed method. The effectiveness of the proposed method is verified with two motion control problems for autonomous vehicles through appropriate comparison simulations.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/ba44a95f1a8bc5765438d03c01137799e930c88d.pdf",
        "venue": "IEEE Transactions on Neural Networks and Learning Systems",
        "citationCount": 51,
        "score": 17.0
    },
    "d27edce419e1c731c0aeb9e1842d1e022b2cc6ab.pdf": {
        "title": "Offline Meta Reinforcement Learning - Identifiability Challenges and Effective Data Collection Strategies",
        "authors": [
            "Ron Dorfman",
            "Idan Shenfeld",
            "Aviv Tamar"
        ],
        "published_date": "2021",
        "abstract": "",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/d27edce419e1c731c0aeb9e1842d1e022b2cc6ab.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 64,
        "score": 16.0
    },
    "116fc10b798e3294b00e92f2b8053d0c89ad9182.pdf": {
        "title": "A robot exploration strategy based on Q-learning network",
        "authors": [
            "L. Tai",
            "Ming Liu"
        ],
        "published_date": "2016",
        "abstract": "",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/116fc10b798e3294b00e92f2b8053d0c89ad9182.pdf",
        "venue": "International Conference on Real-time Computing and Robotics",
        "citationCount": 143,
        "score": 15.888888888888888
    },
    "0f810eb4777fd05317951ebaa7a3f5835ee84cf4.pdf": {
        "title": "Count-Based Exploration in Feature Space for Reinforcement Learning",
        "authors": [
            "Jarryd Martin",
            "S. N. Sasikumar",
            "Tom Everitt",
            "Marcus Hutter"
        ],
        "published_date": "2017",
        "abstract": "We introduce a new count-based optimistic exploration algorithm for Reinforcement Learning (RL) that is feasible in environments with high-dimensional state-action spaces. The success of RL algorithms in these domains depends crucially on generalisation from limited training experience. Function approximation techniques enable RL agents to generalise in order to estimate the value of unvisited states, but at present few methods enable generalisation regarding uncertainty. This has prevented the combination of scalable RL algorithms with efficient exploration strategies that drive the agent to reduce its uncertainty. We present a new method for computing a generalised state visit-count, which allows the agent to estimate the uncertainty associated with any state. Our \\phi-pseudocount achieves generalisation by exploiting same feature representation of the state space that is used for value function approximation. States that have less frequently observed features are deemed more uncertain. The \\phi-Exploration-Bonus algorithm rewards the agent for exploring in feature space rather than in the untransformed state space. The method is simpler and less computationally expensive than some previous proposals, and achieves near state-of-the-art results on high-dimensional RL benchmarks.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/0f810eb4777fd05317951ebaa7a3f5835ee84cf4.pdf",
        "venue": "International Joint Conference on Artificial Intelligence",
        "citationCount": 126,
        "score": 15.75
    },
    "468a3e85a2da0908c6f371ad83f6bcf6b6fdf193.pdf": {
        "title": "Adaptive Informative Path Planning Using Deep Reinforcement Learning for UAV-based Active Sensing",
        "authors": [
            "Julius R\u00fcckin",
            "Liren Jin",
            "Marija Popovic"
        ],
        "published_date": "2021",
        "abstract": "Aerial robots are increasingly being utilized for environmental monitoring and exploration. However, a key challenge is efficiently planning paths to maximize the information value of acquired data as an initially unknown environment is explored. To address this, we propose a new approach for informative path planning based on deep reinforcement learning (RL). Combining recent advances in RL and robotic applications, our method combines tree search with an offline-learned neural network predicting informative sensing actions. We introduce several components making our approach applicable for robotic tasks with high-dimensional state and large action spaces. By deploying the trained network during a mission, our method enables sample-efficient online replanning on platforms with limited computational resources. Simulations show that our approach performs on par with existing methods while reducing runtime by 8-10\u00d7. We validate its performance using real-world surface temperature data.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/468a3e85a2da0908c6f371ad83f6bcf6b6fdf193.pdf",
        "venue": "IEEE International Conference on Robotics and Automation",
        "citationCount": 63,
        "score": 15.75
    },
    "535d184eadf47fa17ce4073b6e2f180783e85300.pdf": {
        "title": "Curiosity-driven Exploration for Mapless Navigation with Deep Reinforcement Learning",
        "authors": [
            "Oleksii Zhelo",
            "Jingwei Zhang",
            "L. Tai",
            "Ming Liu",
            "Wolfram Burgard"
        ],
        "published_date": "2018",
        "abstract": "This paper investigates exploration strategies of Deep Reinforcement Learning (DRL) methods to learn navigation policies for mobile robots. In particular, we augment the normal external reward for training DRL algorithms with intrinsic reward signals measured by curiosity. We test our approach in a mapless navigation setting, where the autonomous agent is required to navigate without the occupancy map of the environment, to targets whose relative locations can be easily acquired through low-cost solutions (e.g., visible light localization, Wi-Fi signal localization). We validate that the intrinsic motivation is crucial for improving DRL performance in tasks with challenging exploration requirements. Our experimental results show that our proposed method is able to more effectively learn navigation policies, and has better generalization capabilities in previously unseen environments. A video of our experimental results can be found at this https URL",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/535d184eadf47fa17ce4073b6e2f180783e85300.pdf",
        "venue": "arXiv.org",
        "citationCount": 108,
        "score": 15.428571428571427
    },
    "0d82360a4da311a277607db355dda3f196e8eb3d.pdf": {
        "title": "Deep Interactive Reinforcement Learning for Path Following of Autonomous Underwater Vehicle",
        "authors": [
            "Qilei Zhang",
            "Jinying Lin",
            "Q. Sha",
            "Bo He",
            "Guangliang Li"
        ],
        "published_date": "2020",
        "abstract": "Autonomous underwater vehicle (AUV) plays an increasingly important role in ocean exploration. Existing AUVs are usually not fully autonomous and generally limited to pre-planning or pre-programming tasks. Reinforcement learning (RL) and deep reinforcement learning have been introduced into the AUV design and research to improve its autonomy. However, these methods are still difficult to apply directly to the actual AUV system because of the sparse rewards and low learning efficiency. In this paper, we proposed a deep interactive reinforcement learning method for path following of AUV by combining the advantages of deep reinforcement learning and interactive RL. In addition, since the human trainer cannot provide human rewards for AUV when it is running in the ocean and AUV needs to adapt to a changing environment, we further propose a deep reinforcement learning method that learns from both human rewards and environmental rewards at the same time. We test our methods in two path following tasks\u2014straight line and sinusoids curve following of AUV by simulating in the Gazebo platform. Our experimental results show that with our proposed deep interactive RL method, AUV can converge faster than a DQN learner from only environmental reward. Moreover, AUV learning with our deep RL from both human and environmental rewards can also achieve a similar or even better performance than that with deep interactive RL and can adapt to the actual environment by further learning from environmental rewards.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/0d82360a4da311a277607db355dda3f196e8eb3d.pdf",
        "venue": "IEEE Access",
        "citationCount": 77,
        "score": 15.4
    },
    "f6b218453170edcbb51e49dd44ba2f83af53ef92.pdf": {
        "title": "Distributional Reinforcement Learning for Efficient Exploration",
        "authors": [
            "B. Mavrin",
            "Shangtong Zhang",
            "Hengshuai Yao",
            "Linglong Kong",
            "Kaiwen Wu",
            "Yaoliang Yu"
        ],
        "published_date": "2019",
        "abstract": "In distributional reinforcement learning (RL), the estimated distribution of value function models both the parametric and intrinsic uncertainties. We propose a novel and efficient exploration method for deep RL that has two components. The first is a decaying schedule to suppress the intrinsic uncertainty. The second is an exploration bonus calculated from the upper quantiles of the learned distribution. In Atari 2600 games, our method outperforms QR-DQN in 12 out of 14 hard games (achieving 483 \\% average gain across 49 games in cumulative rewards over QR-DQN with a big win in Venture). We also compared our algorithm with QR-DQN in a challenging 3D driving simulator (CARLA). Results show that our algorithm achieves near-optimal safety rewards twice faster than QRDQN.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/f6b218453170edcbb51e49dd44ba2f83af53ef92.pdf",
        "venue": "International Conference on Machine Learning",
        "citationCount": 92,
        "score": 15.333333333333332
    },
    "1f4484086d210a2c44efe5eef0a2b42647822abf.pdf": {
        "title": "Breaking the Sample Complexity Barrier to Regret-Optimal Model-Free Reinforcement Learning",
        "authors": [
            "Gen Li",
            "Laixi Shi",
            "Yuxin Chen",
            "Yuantao Gu",
            "Yuejie Chi"
        ],
        "published_date": "2021",
        "abstract": "\n Achieving sample efficiency in online episodic reinforcement learning (RL) requires optimally balancing exploration and exploitation. When it comes to a finite-horizon episodic Markov decision process with $S$ states, $A$ actions and horizon length $H$, substantial progress has been achieved toward characterizing the minimax-optimal regret, which scales on the order of $\\sqrt{H^2SAT}$ (modulo log factors) with $T$ the total number of samples. While several competing solution paradigms have been proposed to minimize regret, they are either memory-inefficient, or fall short of optimality unless the sample size exceeds an enormous threshold (e.g. $S^6A^4 \\,\\mathrm{poly}(H)$ for existing model-free methods).\n To overcome such a large sample size barrier to efficient RL, we design a novel model-free algorithm, with space complexity $O(SAH)$, that achieves near-optimal regret as soon as the sample size exceeds the order of $SA\\,\\mathrm{poly}(H)$. In terms of this sample size requirement (also referred to the initial burn-in cost), our method improves\u2014by at least a factor of $S^5A^3$\u2014upon any prior memory-efficient algorithm that is asymptotically regret-optimal. Leveraging the recently introduced variance reduction strategy (also called reference-advantage decomposition), the proposed algorithm employs an early-settled reference update rule, with the aid of two Q-learning sequences with upper and lower confidence bounds. The design principle of our early-settled variance reduction method might be of independent interest to other RL settings that involve intricate exploration\u2013exploitation trade-offs.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/1f4484086d210a2c44efe5eef0a2b42647822abf.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 59,
        "score": 14.75
    },
    "04615a9955bce148aa7ba29e864389c26e10523a.pdf": {
        "title": "DEP-RL: Embodied Exploration for Reinforcement Learning in Overactuated and Musculoskeletal Systems",
        "authors": [
            "Pierre Schumacher",
            "D. Haeufle",
            "Dieter B\u00fcchler",
            "S. Schmitt",
            "G. Martius"
        ],
        "published_date": "2022",
        "abstract": "Muscle-actuated organisms are capable of learning an unparalleled diversity of dexterous movements despite their vast amount of muscles. Reinforcement learning (RL) on large musculoskeletal models, however, has not been able to show similar performance. We conjecture that ineffective exploration in large overactuated action spaces is a key problem. This is supported by the finding that common exploration noise strategies are inadequate in synthetic examples of overactuated systems. We identify differential extrinsic plasticity (DEP), a method from the domain of self-organization, as being able to induce state-space covering exploration within seconds of interaction. By integrating DEP into RL, we achieve fast learning of reaching and locomotion in musculoskeletal systems, outperforming current approaches in all considered tasks in sample efficiency and robustness.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/04615a9955bce148aa7ba29e864389c26e10523a.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 44,
        "score": 14.666666666666666
    },
    "c64a99ec780d5e1c6ba242a82e72e96eb0c3ef8a.pdf": {
        "title": "An Information-Theoretic Perspective on Intrinsic Motivation in Reinforcement Learning: A Survey",
        "authors": [
            "A. Aubret",
            "L. Matignon",
            "S. Hassas"
        ],
        "published_date": "2022",
        "abstract": "The reinforcement learning (RL) research area is very active, with an important number of new contributions, especially considering the emergent field of deep RL (DRL). However, a number of scientific and technical challenges still need to be resolved, among which we acknowledge the ability to abstract actions or the difficulty to explore the environment in sparse-reward settings which can be addressed by intrinsic motivation (IM). We propose to survey these research works through a new taxonomy based on information theory: we computationally revisit the notions of surprise, novelty, and skill-learning. This allows us to identify advantages and disadvantages of methods and exhibit current outlooks of research. Our analysis suggests that novelty and surprise can assist the building of a hierarchy of transferable skills which abstracts dynamics and makes the exploration process more robust.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/c64a99ec780d5e1c6ba242a82e72e96eb0c3ef8a.pdf",
        "venue": "Entropy",
        "citationCount": 44,
        "score": 14.666666666666666
    },
    "7d05987db045c56fa691da40e679cd328f0b68ef.pdf": {
        "title": "Study on the application of reinforcement learning in the operation optimization of HVAC system",
        "authors": [
            "Xiaolei Yuan",
            "Yiqun Pan",
            "Jianrong Yang",
            "Weitong Wang",
            "Zhizhong Huang"
        ],
        "published_date": "2020",
        "abstract": "",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/7d05987db045c56fa691da40e679cd328f0b68ef.pdf",
        "venue": "Building Simulation",
        "citationCount": 72,
        "score": 14.4
    },
    "399806e861a2ef960a81b37b593c2176a728c399.pdf": {
        "title": "Offline Reinforcement Learning as Anti-Exploration",
        "authors": [
            "Shideh Rezaeifar",
            "Robert Dadashi",
            "Nino Vieillard",
            "L'eonard Hussenot",
            "Olivier Bachem",
            "O. Pietquin",
            "M. Geist"
        ],
        "published_date": "2021",
        "abstract": "Offline Reinforcement Learning (RL) aims at learning an optimal control from a fixed dataset, without interactions with the system. An agent in this setting should avoid selecting actions whose consequences cannot be predicted from the data. This is the converse of exploration in RL, which favors such actions. We thus take inspiration from the literature on bonus-based exploration to design a new offline RL agent. The core idea is to subtract a prediction-based exploration bonus from the reward, instead of adding it for exploration. This allows the policy to stay close to the support of the dataset and practically extends some previous pessimism-based offline RL methods to a deep learning setting with arbitrary bonuses. We also connect this approach to a more common regularization of the learned policy towards the data. Instantiated with a bonus based on the prediction error of a variational autoencoder, we show that our simple agent is competitive with the state of the art on a set of continuous control locomotion and manipulation tasks.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/399806e861a2ef960a81b37b593c2176a728c399.pdf",
        "venue": "AAAI Conference on Artificial Intelligence",
        "citationCount": 57,
        "score": 14.25
    },
    "174be0bacee04d9eb13a698d484ab5ae441c1100.pdf": {
        "title": "Effective deep Q-networks (EDQN) strategy for resource allocation based on optimized reinforcement learning algorithm",
        "authors": [
            "Fatma M. Talaat"
        ],
        "published_date": "2022",
        "abstract": "The healthcare industry has always been an early adopter of new technology and a big benefactor of it. The use of reinforcement learning in the healthcare system has repeatedly resulted in improved outcomes.. Many challenges exist concerning the architecture of the RL method, measurement metrics, and model choice. More significantly, the validation of RL in authentic clinical settings needs further work. This paper presents a new Effective Resource Allocation Strategy (ERAS) for the Fog environment, which is suitable for Healthcare applications. ERAS tries to achieve effective resource management in the Fog environment via real-time resource allocating as well as prediction algorithms. Comparing the ERAS with the state-of-the-art algorithms, ERAS achieved the minimum Makespan as compared to previous resource allocation algorithms, while maximizing the Average Resource Utilization (ARU) and the Load Balancing Level (LBL). For each application, we further compared and contrasted the architecture of the RL models and the assessment metrics. In critical care, RL has tremendous potential to enhance decision-making. This paper presents two main contributions, (i) Optimization of the RL hyperparameters using PSO, and (ii) Using the optimized RL for the resource allocation and load balancing in the fog environment. Because of its exploitation, exploration, and capacity to get rid of local minima, the PSO has a significant significance when compared to other optimization methodologies.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/174be0bacee04d9eb13a698d484ab5ae441c1100.pdf",
        "venue": "Multimedia tools and applications",
        "citationCount": 42,
        "score": 14.0
    },
    "65587d4927fccc30788d3dfc9b639567721ff393.pdf": {
        "title": "Rethinking Reinforcement Learning for Recommendation: A Prompt Perspective",
        "authors": [
            "Xin Xin",
            "Tiago Pimentel",
            "Alexandros Karatzoglou",
            "Pengjie Ren",
            "Konstantina Christakopoulou",
            "Z. Ren"
        ],
        "published_date": "2022",
        "abstract": "Modern recommender systems aim to improve user experience. As reinforcement learning (RL) naturally fits this objective---maximizing an user's reward per session---it has become an emerging topic in recommender systems. Developing RL-based recommendation methods, however, is not trivial due to the offline training challenge. Specifically, the keystone of traditional RL is to train an agent with large amounts of online exploration making lots of 'errors' in the process. In the recommendation setting, though, we cannot afford the price of making 'errors' online. As a result, the agent needs to be trained through offline historical implicit feedback, collected under different recommendation policies; traditional RL algorithms may lead to sub-optimal policies under these offline training settings. Here we propose a new learning paradigm---namely Prompt-Based Reinforcement Learning (PRL)---for the offline training of RL-based recommendation agents. While traditional RL algorithms attempt to map state-action input pairs to their expected rewards (e.g., Q-values), PRL directly infers actions (i.e., recommended items) from state-reward inputs. In short, the agents are trained to predict a recommended item given the prior interactions and an observed reward value---with simple supervised learning. At deployment time, this historical (training) data acts as a knowledge base, while the state-reward pairs are used as a prompt. The agents are thus used to answer the question: Which item should be recommended given the prior interactions & the prompted reward value? We implement PRL with four notable recommendation models and conduct experiments on two real-world e-commerce datasets. Experimental results demonstrate the superior performance of our proposed methods.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/65587d4927fccc30788d3dfc9b639567721ff393.pdf",
        "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
        "citationCount": 41,
        "score": 13.666666666666666
    },
    "2fd42844445ec644c2c44c093c3522c08b59cb45.pdf": {
        "title": "Event-Triggered Model Predictive Control With Deep Reinforcement Learning for Autonomous Driving",
        "authors": [
            "Fengying Dang",
            "Dong Chen",
            "J. Chen",
            "Zhaojian Li"
        ],
        "published_date": "2022",
        "abstract": "Event-triggered model predictive control (eMPC) is a popular optimal control method with an aim to alleviate the computation and/or communication burden of MPC. However, it generally requires a priori knowledge of the closed-loop system behavior along with the communication characteristics for designing the event-trigger policy. This paper attempts to solve this challenge by proposing an efficient eMPC framework and demonstrates successful implementation of this framework on the autonomous vehicle path following. First of all, a model-free reinforcement learning (RL) agent is used to learn the optimal event-trigger policy without the need for a complete dynamical system and communication knowledge in this framework. Furthermore, techniques including prioritized experience replay (PER) buffer and long short-term memory (LSTM) are employed to foster exploration and improve training efficiency. In this paper, we use the proposed framework with three deep RL algorithms, i.e., Double Q-learning (DDQN), Proximal Policy Optimization (PPO), and Soft Actor-Critic (SAC), to solve this problem. Results show that all three deep RL-based eMPC (deep-RL-eMPC) can achieve better evaluation performance than the conventional threshold-based and previous linear Q-based approach in the autonomous path following. In particular, PPO-eMPC with LSTM and DDQN-eMPC with PER and LSTM obtain a superior balance between the closed-loop control performance and event-trigger frequency.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/2fd42844445ec644c2c44c093c3522c08b59cb45.pdf",
        "venue": "IEEE Transactions on Intelligent Vehicles",
        "citationCount": 39,
        "score": 13.0
    },
    "3e0925355554e3aeb99de8165c268582a82de3bb.pdf": {
        "title": "Smooth Exploration for Robotic Reinforcement Learning",
        "authors": [
            "A. Raffin",
            "Jens Kober",
            "F. Stulp"
        ],
        "published_date": "2020",
        "abstract": "Reinforcement learning (RL) enables robots to learn skills from interactions with the real world. In practice, the unstructured step-based exploration used in Deep RL -- often very successful in simulation -- leads to jerky motion patterns on real robots. Consequences of the resulting shaky behavior are poor exploration, or even damage to the robot. We address these issues by adapting state-dependent exploration (SDE) to current Deep RL algorithms. To enable this adaptation, we propose two extensions to the original SDE, using more general features and re-sampling the noise periodically, which leads to a new exploration method generalized state-dependent exploration (gSDE). We evaluate gSDE both in simulation, on PyBullet continuous control tasks, and directly on three different real robots: a tendon-driven elastic robot, a quadruped and an RC car. The noise sampling interval of gSDE permits to have a compromise between performance and smoothness, which allows training directly on the real robots without loss of performance. The code is available at https://github.com/DLR-RM/stable-baselines3.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/3e0925355554e3aeb99de8165c268582a82de3bb.pdf",
        "venue": "Conference on Robot Learning",
        "citationCount": 64,
        "score": 12.8
    },
    "1fa2a04bffc18cfb46650a11ab0e5696d711f58f.pdf": {
        "title": "An exploration strategy improves the diversity of de novo ligands using deep reinforcement learning: a case for the adenosine A2A receptor",
        "authors": [
            "Xuhan Liu",
            "K. Ye",
            "H. V. van Vlijmen",
            "A. IJzerman",
            "G. V. van Westen"
        ],
        "published_date": "2018",
        "abstract": "Over the last 5\u00a0years deep learning has progressed tremendously in both image recognition and natural language processing. Now it is increasingly applied to other data rich fields. In drug discovery, recurrent neural networks (RNNs) have been shown to be an effective method to generate novel chemical structures in the form of SMILES. However, ligands generated by current methods have so far provided relatively low diversity and do not fully cover the whole chemical space occupied by known ligands. Here, we propose a new method (DrugEx) to discover de novo drug-like molecules. DrugEx is an RNN model (generator) trained through reinforcement learning which was integrated with a special exploration strategy. As a case study we applied our method to design ligands against the adenosine A2A receptor. From ChEMBL data, a machine learning model (predictor) was created to predict whether generated molecules are active or not. Based on this predictor as the reward function, the generator was trained by reinforcement learning without any further data. We then compared the performance of our method with two previously published methods, REINVENT and ORGANIC. We found that candidate molecules our model designed, and predicted to be active, had a larger chemical diversity and better covered the chemical space of known ligands compared to the state-of-the-art.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/1fa2a04bffc18cfb46650a11ab0e5696d711f58f.pdf",
        "venue": "Journal of Cheminformatics",
        "citationCount": 77,
        "score": 11.0
    },
    "442e9f1e8f6218e68f944fd3028c5385691d4112.pdf": {
        "title": "Aggressive Quadrotor Flight Using Curiosity-Driven Reinforcement Learning",
        "authors": [
            "Qiyu Sun",
            "Jinbao Fang",
            "Weixing Zheng",
            "Yang Tang"
        ],
        "published_date": "2022",
        "abstract": "The ability to perform aggressive movements,which are called aggressive flights, is important for quadrotors during navigation. However, aggressive quadrotor flights are still a great challenge to practical applications. The existing solutions to aggressive flights heavily rely on a predefined trajectory, which is a time-consuming preprocessing step. To avoid such path planning, we propose a curiosity-driven reinforcement learning method for aggressive flight missions and a similarity-based curiosity module is introduced to speed up the training procedure. A branch structure exploration strategy is also applied to guarantee the robustness of the policy and to ensure the policy trained in simulations can be performed in real-world experiments directly. The experimental results in simulations demonstrate that our reinforcement learning algorithm performs well in aggressive flight tasks, speeds up the convergence process and improves the robustness of the policy. Besides, our algorithm shows a satisfactory simulated to real transferability and performs well in real-world experiments.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/442e9f1e8f6218e68f944fd3028c5385691d4112.pdf",
        "venue": "IEEE transactions on industrial electronics (1982. Print)",
        "citationCount": 32,
        "score": 10.666666666666666
    },
    "a25b645c3d24f91164230a0ac5bb2d4ec88c1538.pdf": {
        "title": "Information-Directed Exploration for Deep Reinforcement Learning",
        "authors": [
            "Nikolay Nikolov",
            "Johannes Kirschner",
            "Felix Berkenkamp",
            "Andreas Krause"
        ],
        "published_date": "2018",
        "abstract": "Efficient exploration remains a major challenge for reinforcement learning. One reason is that the variability of the returns often depends on the current state and action, and is therefore heteroscedastic. Classical exploration strategies such as upper confidence bound algorithms and Thompson sampling fail to appropriately account for heteroscedasticity, even in the bandit setting. Motivated by recent findings that address this issue in bandits, we propose to use Information-Directed Sampling (IDS) for exploration in reinforcement learning. As our main contribution, we build on recent advances in distributional reinforcement learning and propose a novel, tractable approximation of IDS for deep Q-learning. The resulting exploration strategy explicitly accounts for both parametric uncertainty and heteroscedastic observation noise. We evaluate our method on Atari games and demonstrate a significant improvement over alternative approaches.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/a25b645c3d24f91164230a0ac5bb2d4ec88c1538.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 74,
        "score": 10.571428571428571
    },
    "2fc993c78cd84dafe4c36d2ac1cc25a6256aaa9d.pdf": {
        "title": "Sample-Efficient Reinforcement Learning with loglog(T) Switching Cost",
        "authors": [
            "Dan Qiao",
            "Ming Yin",
            "Ming Min",
            "Yu-Xiang Wang"
        ],
        "published_date": "2022",
        "abstract": "We study the problem of reinforcement learning (RL) with low (policy) switching cost - a problem well-motivated by real-life RL applications in which deployments of new policies are costly and the number of policy updates must be low. In this paper, we propose a new algorithm based on stage-wise exploration and adaptive policy elimination that achieves a regret of $\\widetilde{O}(\\sqrt{H^4S^2AT})$ while requiring a switching cost of $O(HSA \\log\\log T)$. This is an exponential improvement over the best-known switching cost $O(H^2SA\\log T)$ among existing methods with $\\widetilde{O}(\\mathrm{poly}(H,S,A)\\sqrt{T})$ regret. In the above, $S,A$ denotes the number of states and actions in an $H$-horizon episodic Markov Decision Process model with unknown transitions, and $T$ is the number of steps. As a byproduct of our new techniques, we also derive a reward-free exploration algorithm with a switching cost of $O(HSA)$. Furthermore, we prove a pair of information-theoretical lower bounds which say that (1) Any no-regret algorithm must have a switching cost of $\\Omega(HSA)$; (2) Any $\\widetilde{O}(\\sqrt{T})$ regret algorithm must incur a switching cost of $\\Omega(HSA\\log\\log T)$. Both our algorithms are thus optimal in their switching costs.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/2fc993c78cd84dafe4c36d2ac1cc25a6256aaa9d.pdf",
        "venue": "International Conference on Machine Learning",
        "citationCount": 31,
        "score": 10.333333333333332
    },
    "0ba7f2e592dda172bc3d07f88cdcf2a57830deec.pdf": {
        "title": "Towards Safe Reinforcement Learning with a Safety Editor Policy",
        "authors": [
            "Haonan Yu",
            "Wei Xu",
            "Haichao Zhang"
        ],
        "published_date": "2022",
        "abstract": "We consider the safe reinforcement learning (RL) problem of maximizing utility with extremely low constraint violation rates. Assuming no prior knowledge or pre-training of the environment safety model given a task, an agent has to learn, via exploration, which states and actions are safe. A popular approach in this line of research is to combine a model-free RL algorithm with the Lagrangian method to adjust the weight of the constraint reward relative to the utility reward dynamically. It relies on a single policy to handle the conflict between utility and constraint rewards, which is often challenging. We present SEditor, a two-policy approach that learns a safety editor policy transforming potentially unsafe actions proposed by a utility maximizer policy into safe ones. The safety editor is trained to maximize the constraint reward while minimizing a hinge loss of the utility state-action values before and after an action is edited. SEditor extends existing safety layer designs that assume simplified safety models, to general safe RL scenarios where the safety model can in theory be arbitrarily complex. As a first-order method, it is easy to implement and efficient for both inference and training. On 12 Safety Gym tasks and 2 safe racing tasks, SEditor obtains much a higher overall safety-weighted-utility (SWU) score than the baselines, and demonstrates outstanding utility performance with constraint violation rates as low as once per 2k time steps, even in obstacle-dense environments. On some tasks, this low violation rate is up to 200 times lower than that of an unconstrained RL method with similar utility performance. Code is available at https://github.com/hnyu/seditor.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/0ba7f2e592dda172bc3d07f88cdcf2a57830deec.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 31,
        "score": 10.333333333333332
    },
    "09da56cd3bf72b632c43969be97874fa14a3765c.pdf": {
        "title": "The Challenges of Exploration for Offline Reinforcement Learning",
        "authors": [
            "Nathan Lambert",
            "Markus Wulfmeier",
            "William F. Whitney",
            "Arunkumar Byravan",
            "Michael Bloesch",
            "Vibhavari Dasagi",
            "Tim Hertweck",
            "Martin A. Riedmiller"
        ],
        "published_date": "2022",
        "abstract": "Offline Reinforcement Learning (ORL) enablesus to separately study the two interlinked processes of reinforcement learning: collecting informative experience and inferring optimal behaviour. The second step has been widely studied in the offline setting, but just as critical to data-efficient RL is the collection of informative data. The task-agnostic setting for data collection, where the task is not known a priori, is of particular interest due to the possibility of collecting a single dataset and using it to solve several downstream tasks as they arise. We investigate this setting via curiosity-based intrinsic motivation, a family of exploration methods which encourage the agent to explore those states or transitions it has not yet learned to model. With Explore2Offline, we propose to evaluate the quality of collected data by transferring the collected data and inferring policies with reward relabelling and standard offline RL algorithms. We evaluate a wide variety of data collection strategies, including a new exploration agent, Intrinsic Model Predictive Control (IMPC), using this scheme and demonstrate their performance on various tasks. We use this decoupled framework to strengthen intuitions about exploration and the data prerequisites for effective offline RL.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/09da56cd3bf72b632c43969be97874fa14a3765c.pdf",
        "venue": "arXiv.org",
        "citationCount": 30,
        "score": 10.0
    },
    "fc3b5dc5528e24dbbc8f4ec273e622ce40eec855.pdf": {
        "title": "Disentangling Transfer in Continual Reinforcement Learning",
        "authors": [
            "Maciej Wo\u0142czyk",
            "Michal Zajkac",
            "Razvan Pascanu",
            "Lukasz Kuci'nski",
            "Piotr Milo's"
        ],
        "published_date": "2022",
        "abstract": "The ability of continual learning systems to transfer knowledge from previously seen tasks in order to maximize performance on new tasks is a significant challenge for the field, limiting the applicability of continual learning solutions to realistic scenarios. Consequently, this study aims to broaden our understanding of transfer and its driving forces in the specific case of continual reinforcement learning. We adopt SAC as the underlying RL algorithm and Continual World as a suite of continuous control tasks. We systematically study how different components of SAC (the actor and the critic, exploration, and data) affect transfer efficacy, and we provide recommendations regarding various modeling options. The best set of choices, dubbed ClonEx-SAC, is evaluated on the recent Continual World benchmark. ClonEx-SAC achieves 87% final success rate compared to 80% of PackNet, the best method in the benchmark. Moreover, the transfer grows from 0.18 to 0.54 according to the metric provided by Continual World.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/fc3b5dc5528e24dbbc8f4ec273e622ce40eec855.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 30,
        "score": 10.0
    },
    "46eb68c585bdb8a1051dfda98b4b35610301264f.pdf": {
        "title": "Spacecraft Proximity Maneuvering and Rendezvous With Collision Avoidance Based on Reinforcement Learning",
        "authors": [
            "Qingyu Qu",
            "Kexin Liu",
            "Wei Wang",
            "Jinhu Lu"
        ],
        "published_date": "2022",
        "abstract": "The rapid development of the aerospace industry puts forward the urgent need for the evolution of autonomous spacecraft rendezvous technology, which has gained significant attention recently due to increased applications in various space missions. This article studies the relative position tracking problem of the autonomous spacecraft rendezvous under the requirement of collision avoidance. An exploration-adaptive deep deterministic policy gradient (DDPG) algorithm is proposed to train a definite control strategy for this mission. Similar to the DDPG algorithm, four neural networks are used in this method, where two of them are used to generate the deterministic policy, whereas the other two are used to score the obtained policy. Differently, adaptive noise is introduced to reduce the possibility of oscillations and divergences and to cut down the unnecessary computation by weakening the exploration of stabilization problems. In addition, in order to effectively and quickly adapt to some other similar scenarios, a metalearning-based idea is introduced by fine-tuning the prior strategy. Finally, two numerical simulations show that the trained control strategy can effectively avoid the oscillation phenomenon caused by the artificial potential function. Benefiting from this, the trained control strategy based on deep reinforcement learning technology can decrease the energy consumption by 16.44% during the close proximity phase, compared with the traditional artificial potential function method. Besides, after introducing the metalearning-based idea, a strategy available for some other perturbed scenarios can be trained in a relatively short period of time, which illustrates its adaptability.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/46eb68c585bdb8a1051dfda98b4b35610301264f.pdf",
        "venue": "IEEE Transactions on Aerospace and Electronic Systems",
        "citationCount": 29,
        "score": 9.666666666666666
    },
    "04efc9768a8e0c5f23b8c8504fb6db8803ffc071.pdf": {
        "title": "Generative Design by Using Exploration Approaches of Reinforcement Learning in Density-Based Structural Topology Optimization",
        "authors": [
            "H. Sun",
            "Ling Ma"
        ],
        "published_date": "2020",
        "abstract": "A central challenge in generative design is the exploration of vast number of solutions. In this work, we extend two major density-based structural topology optimization (STO) methods based on four classes of exploration algorithms of reinforcement learning (RL) to STO problems, which approaches generative design in a new way. The four methods are: first, using \u03b5 -greedy policy to disturb the search direction; second, using upper confidence bound (UCB) to add a bonus on sensitivity; last, using Thompson sampling (TS) as well as information-directed sampling (IDS) to direct the search, where the posterior function of reward is fitted by Beta distribution or Gaussian distribution. Those combined methods are evaluated on some structure compliance minimization tasks from 2D to 3D, including the variable thickness design problem of an atmospheric diving suit (ADS). We show that all methods can generate various acceptable design options by varying one or two parameters simply, except that IDS fails to reach the convergence for complex structures due to the limitation of computation ability. We also show that both Beta distribution and Gaussian distribution work well to describe the posterior probability.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/04efc9768a8e0c5f23b8c8504fb6db8803ffc071.pdf",
        "venue": "Designs",
        "citationCount": 48,
        "score": 9.600000000000001
    },
    "4df0238d5bfe0ab4cb8eaba67f4388c4e02dc2c8.pdf": {
        "title": "A Human-Machine Reinforcement Learning Method for Cooperative Energy Management",
        "authors": [
            "Yuechuan Tao",
            "Jing Qiu",
            "Shuying Lai",
            "Xian Zhang",
            "Yunqi Wang",
            "Guibin Wang"
        ],
        "published_date": "2022",
        "abstract": "The increasing penetration of distributed energy resources and a large volume of unprecedented data from smart metering infrastructure can help consumers transit to an active role in the smart grid. In this article, we propose a human-machine reinforcement learning (RL) framework in the smart grid context to formulate an energy management strategy for electric vehicles and thermostatically controlled loads aggregators. The proposed model-free method accelerates the decision-making speed by substituting the conventional optimization process, and it is more capable of coping with the diverse system environment via online learning. The human intervention is coordinated with machine learning to: 1) prevent the huge loss during the learning process; 2) realize emergency control; and 3) find preferable control policy. The performance of the proposed human-machine RL framework is verified in case studies. It can be concluded that our proposed method performs better than the conventional deep Q-learning and deep deterministic policy gradient in terms of convergence capability and preferable result exploration. Besides, the proposed method can better deal with emergent events, such as a sudden drop of photovoltaic (PV) output. Compared with the conventional model-based method, there are slight deviations between our method and the optimal solution, but the decision-making time is significantly reduced.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/4df0238d5bfe0ab4cb8eaba67f4388c4e02dc2c8.pdf",
        "venue": "IEEE Transactions on Industrial Informatics",
        "citationCount": 28,
        "score": 9.333333333333332
    },
    "fb3c6456708b0e143f545d77dc8ec804eb947395.pdf": {
        "title": "Curiosity-driven Exploration in Deep Reinforcement Learning via Bayesian Neural Networks",
        "authors": [
            "Rein Houthooft",
            "Xi Chen",
            "Yan Duan",
            "John Schulman",
            "F. Turck",
            "P. Abbeel"
        ],
        "published_date": "2016",
        "abstract": "Scalable and effective exploration remains a key challenge in reinforcement learning (RL). While there are methods with optimality guarantees in the setting of discrete state and action spaces, these methods cannot be applied in high-dimensional deep RL scenarios. As such, most contemporary RL relies on simple heuristics such as epsilon-greedy exploration or adding Gaussian noise to the controls. This paper introduces Variational Information Maximizing Exploration (VIME), an exploration strategy based on maximization of information gain about the agent's belief of environment dynamics. We propose a practical implementation, using variational inference in Bayesian neural networks which efficiently handles continuous state and action spaces. VIME modifies the MDP reward function, and can be applied with several different underlying RL algorithms. We demonstrate that VIME achieves significantly better performance compared to heuristic exploration methods across a variety of continuous control tasks and algorithms, including tasks with very sparse rewards.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/fb3c6456708b0e143f545d77dc8ec804eb947395.pdf",
        "venue": "arXiv.org",
        "citationCount": 79,
        "score": 8.777777777777777
    },
    "248a25d697fe0132840e9d03c00aefadf03408d8.pdf": {
        "title": "Offline Reinforcement Learning for Autonomous Driving with Safety and Exploration Enhancement",
        "authors": [
            "Tianyu Shi",
            "Dong Chen",
            "Kaian Chen",
            "Zhaojian Li"
        ],
        "published_date": "2021",
        "abstract": "Reinforcement learning (RL) is a powerful data-driven control method that has been largely explored in autonomous driving tasks. However, conventional RL approaches learn control policies through trial-and-error interactions with the environment and therefore may cause disastrous consequences such as collisions when testing in real-world traffic. Offline RL has recently emerged as a promising framework to learn effective policies from previously-collected, static datasets without the requirement of active interactions, making it especially appealing for autonomous driving applications. Despite promising, existing offline RL algorithms such as Batch-Constrained deep Q-learning (BCQ) generally lead to rather conservative policies with limited exploration efficiency. To address such issues, this paper presents an enhanced BCQ algorithm by employing a learnable parameter noise scheme in the perturbation model to increase the diversity of observed actions. In addition, a Lyapunov-based safety enhancement strategy is incorporated to constrain the explorable state space within a safe region. Experimental results in highway and parking traffic scenarios show that our approach outperforms the conventional RL method, as well as state-of-the-art offline RL algorithms.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/248a25d697fe0132840e9d03c00aefadf03408d8.pdf",
        "venue": "arXiv.org",
        "citationCount": 35,
        "score": 8.75
    },
    "7d1722451bff3b95e1d082864bb9b8437a0ddf41.pdf": {
        "title": "UAV Networks Against Multiple Maneuvering Smart Jamming With Knowledge-Based Reinforcement Learning",
        "authors": [
            "Zhiwei Li",
            "Yu Lu",
            "Xi Li",
            "Zeng-Guang Wang",
            "Wenxin Qiao",
            "Yicen Liu"
        ],
        "published_date": "2021",
        "abstract": "The unmanned aerial vehicles (UAVs) networks are very vulnerable to smart jammers that can choose their jamming strategy based on the ongoing channel state accordingly. Although reinforcement learning (RL) algorithms can give UAV networks the ability to make intelligent decisions, the high-dimensional state space makes it difficult for algorithms to converge quickly. This article proposes a knowledge-based RL method, which uses domain knowledge to compress the state space that the agent needs to explore and then improve the algorithm convergence speed. Specifically, we use the inertial law of the aircraft and the law of signal attenuation in free space to guide the highly efficient exploration of the UAVs in the state space. We incorporate the performance indicators of the receiver and the subjective value of the task into the design of the reward function, and build a virtual environment for pretraining to accelerate the convergence of anti-jamming decisions. In addition, the algorithm proposed is completely based on observable data, which is more realistic than those studies that assume the position or the channel strategy of the jammer. The simulation shows that the proposed algorithm can outperform the benchmarks of model-free RL algorithm in terms of converge speed and averaged reward.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/7d1722451bff3b95e1d082864bb9b8437a0ddf41.pdf",
        "venue": "IEEE Internet of Things Journal",
        "citationCount": 35,
        "score": 8.75
    },
    "b0d376434a528ee69d98174d75b4a571c53247ae.pdf": {
        "title": "Feature Augmentation with Reinforcement Learning",
        "authors": [
            "Jiabin Liu",
            "Chengliang Chai",
            "Yuyu Luo",
            "Yin Lou",
            "Jianhua Feng",
            "Nan Tang"
        ],
        "published_date": "2022",
        "abstract": "Sufficient good features are indispensable to train well-performed machine learning models. However, it is com-mon that good features are not always enough, where feature augmentation is necessary to enrich high-quality features by joining with other tables. There are two main challenges for the problem. Given a set of tables where we can augment features from, the first challenge is that there are a lot of ways of joining multiple tables and deciding which features (or attributes) to use - selecting the best set of features to augment is hard. Moreover, we may need to materialize the join results for different join options, doing full materialization might be time consuming - efficient but approximate methods are needed. In this paper, we first introduce the design space of the feature augmentation problem. Then, to address the above challenges, we propose a reinforcement learning based framework, namely AutoFeature, to augment the features following an exploration-exploitation strategy. AutoFeature keeps exploring the features in tables that have led to performance improvement. At the same time, AutoFeature also exploits the tables (features) that are rarely selected. In this way, the search space of tables (features) to be augmented can be well explored and a subset of good features can be selected. AutoFeature utilizes sampling techniques to achieve high efficiency. We implement two algorithms, one with multi-arm bandit and the other with branch Deep Q Networks (branch DQN), to realize the framework of AutoFeature. We conducted experiments on three real-world datasets School/XuetangE/Air using 16/23/34 candidate tables with 695/204/338 candidate features. Extensive results show that AutoFeature outperforms other methods by 12.4% and 9.8% on AUC values on two classification datasets (School and XuetangE) and by 0.113 on the MSE value on Air in terms of the model performance.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/b0d376434a528ee69d98174d75b4a571c53247ae.pdf",
        "venue": "IEEE International Conference on Data Engineering",
        "citationCount": 26,
        "score": 8.666666666666666
    },
    "2822e79cb293f54e05fd8a7cd2844cfcc683f5d8.pdf": {
        "title": "Plume Tracing via Model-Free Reinforcement Learning Method",
        "authors": [
            "Hangkai Hu",
            "Shiji Song",
            "C. L. Phillip Chen"
        ],
        "published_date": "2019",
        "abstract": "This paper studies the plume-tracing strategy for an autonomous underwater vehicle (AUV) in the deep-sea turbulent environment. The tracing problem is modeled as a partially observable Markov decision process with continuous state space and action space due to the spatio-temporal changes of environment. An long short-term memory-based reinforcement learning framework with full use of history information is proposed to generate a smooth strategy while the AUV interacting with the environment. Continuous temporal difference and deterministic policy gradient methods are employed to improve the strategy. To promote the performance of the algorithm, a supervised strategy generated by dynamic programming methods is utilized as transcendental knowledge of the agent. Historical searching trajectory\u2019s form and the exploration technology are specially designed to fit the algorithm. Simulation environments are established based on Reynolds-averaged Navier\u2013Stokes equations and the effectiveness of the learned plume-tracing strategy is validated with simulation experiments.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/2822e79cb293f54e05fd8a7cd2844cfcc683f5d8.pdf",
        "venue": "IEEE Transactions on Neural Networks and Learning Systems",
        "citationCount": 50,
        "score": 8.333333333333332
    },
    "fed0701afdfa6896057f7d04bd30ab1328eff110.pdf": {
        "title": "Evolutionary Diversity Optimization with Clustering-based Selection for Reinforcement Learning",
        "authors": [
            "Yutong Wang",
            "Ke Xue",
            "Chaojun Qian"
        ],
        "published_date": "2022",
        "abstract": "",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/fed0701afdfa6896057f7d04bd30ab1328eff110.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 25,
        "score": 8.333333333333332
    },
    "813f6e34feb3dc0346b6392d061af12ff186ba7e.pdf": {
        "title": "Learning Efficient Multi-Agent Cooperative Visual Exploration",
        "authors": [
            "Chao Yu",
            "Xinyi Yang",
            "Jiaxuan Gao",
            "Huazhong Yang",
            "Yu Wang",
            "Yi Wu"
        ],
        "published_date": "2021",
        "abstract": "We tackle the problem of cooperative visual exploration where multiple agents need to jointly explore unseen regions as fast as possible based on visual signals. Classical planning-based methods often suffer from expensive computation overhead at each step and a limited expressiveness of complex cooperation strategy. By contrast, reinforcement learning (RL) has recently become a popular paradigm for tackling this challenge due to its modeling capability of arbitrarily complex strategies and minimal inference overhead. In this paper, we extend the state-of-the-art single-agent visual navigation method, Active Neural SLAM (ANS), to the multi-agent setting by introducing a novel RL-based planning module, Multi-agent Spatial Planner (MSP).MSP leverages a transformer-based architecture, Spatial-TeamFormer, which effectively captures spatial relations and intra-agent interactions via hierarchical spatial self-attentions. In addition, we also implement a few multi-agent enhancements to process local information from each agent for an aligned spatial representation and more precise planning. Finally, we perform policy distillation to extract a meta policy to significantly improve the generalization capability of final policy. We call this overall solution, Multi-Agent Active Neural SLAM (MAANS). MAANS substantially outperforms classical planning-based baselines for the first time in a photo-realistic 3D simulator, Habitat. Code and videos can be found at https://sites.google.com/view/maans.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/813f6e34feb3dc0346b6392d061af12ff186ba7e.pdf",
        "venue": "European Conference on Computer Vision",
        "citationCount": 32,
        "score": 8.0
    },
    "21828b3f05acffbfdf7de7259ad8b8ebd57fa351.pdf": {
        "title": "Data-efficient deep reinforcement learning with expert demonstration for active flow control",
        "authors": [
            "Changdong Zheng",
            "Fangfang Xie",
            "Tingwei Ji",
            "Xinshuai Zhang",
            "Yufeng Lu",
            "Hongjie Zhou",
            "Yao Zheng"
        ],
        "published_date": "2022",
        "abstract": "Deep reinforcement learning (RL) is capable of identifying and modifying strategies for active flow control. However, the classic active formulation of deep RL requires lengthy active exploration. This paper describes the introduction of expert demonstration into a classic off-policy RL algorithm, the soft actor-critic algorithm, for application to vortex-induced vibration problems. This combined online-learning framework is applied to an oscillator wake environment and a Navier--Stokes environment, with expert demonstration obtained from the pole-placement method and surrogate model optimization. The results show that the soft actor--critic framework combined with expert demonstration enables rapid learning of active flow control strategies through a combination of prior demonstration data and online experience. The present study develops a new data-efficient RL approach for discovering active flow control strategies for vortex-induced vibration, providing a more practical methodology for industrial applications.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/21828b3f05acffbfdf7de7259ad8b8ebd57fa351.pdf",
        "venue": "The Physics of Fluids",
        "citationCount": 24,
        "score": 8.0
    },
    "714b7c61d81687d093fd8b7d6d6737d582a4c9b7.pdf": {
        "title": "Towards Applicable Reinforcement Learning: Improving the Generalization and Sample Efficiency with Policy Ensemble",
        "authors": [
            "Zhengyu Yang",
            "Kan Ren",
            "Xufang Luo",
            "Minghuan Liu",
            "Weiqing Liu",
            "J. Bian",
            "Weinan Zhang",
            "Dongsheng Li"
        ],
        "published_date": "2022",
        "abstract": "It is challenging for reinforcement learning (RL) algorithms to succeed in real-world applications. Take financial trading as an example, the market information is noisy yet imperfect and the macroeconomic regulation or other factors may shift between training and evaluation, thus it requires both generalization and high sample efficiency for resolving the task. However, directly applying typical RL algorithms can lead to poor performance in such scenarios. To derive a robust and applicable RL algorithm, in this work, we design a simple but effective method named Ensemble Proximal Policy Optimization (EPPO), which learns ensemble policies in an end-to-end manner. Notably, EPPO combines each policy and the policy ensemble organically and optimizes both simultaneously. In addition, EPPO adopts a diversity enhancement regularization over the policy space which helps to generalize to unseen states and promotes exploration. We theoretically prove that EPPO can increase exploration efficacy, and through comprehensive experimental evaluations on various tasks, we demonstrate that EPPO achieves higher efficiency and is robust for real-world applications compared with vanilla policy optimization algorithms and other ensemble methods. Code and supplemental materials are available at https://seqml.github.io/eppo.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/714b7c61d81687d093fd8b7d6d6737d582a4c9b7.pdf",
        "venue": "International Joint Conference on Artificial Intelligence",
        "citationCount": 23,
        "score": 7.666666666666666
    },
    "1e650cb12aaad371a49b0c8c4514e1b988a5178c.pdf": {
        "title": "Pareto Policy Pool for Model-based Offline Reinforcement Learning",
        "authors": [
            "Yijun Yang",
            "J. Jiang",
            "Tianyi Zhou",
            "Jie Ma",
            "Yuhui Shi"
        ],
        "published_date": "2022",
        "abstract": "",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/1e650cb12aaad371a49b0c8c4514e1b988a5178c.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 22,
        "score": 7.333333333333333
    },
    "1d2acae1d631e932c7ebe96fad3c3b04909e5cee.pdf": {
        "title": "A Deep Reinforcement Learning Strategy Combining Expert Experience Guidance for a Fruit-Picking Manipulator",
        "authors": [
            "Yuqi Liu",
            "Po Gao",
            "Change Zheng",
            "Lijing Tian",
            "Ye Tian"
        ],
        "published_date": "2022",
        "abstract": "When using deep reinforcement learning algorithms for path planning of a multi-DOF fruit-picking manipulator in unstructured environments, it is much too difficult for the multi-DOF manipulator to obtain high-value samples at the beginning of training, resulting in low learning and convergence efficiency. Aiming to reduce the inefficient exploration in unstructured environments, a reinforcement learning strategy combining expert experience guidance was first proposed in this paper. The ratios of expert experience to newly generated samples and the frequency of return visits to expert experience were studied by the simulation experiments. Some conclusions were that the ratio of expert experience, which declined from 0.45 to 0.35, was more effective in improving learning efficiency of the model than the constant ratio. Compared to an expert experience ratio of 0.35, the success rate increased by 1.26%, and compared to an expert experience ratio of 0.45, the success rate increased by 20.37%. The highest success rate was achieved when the frequency of return visits was 15 in 50 episodes, an improvement of 31.77%. The results showed that the proposed method can effectively improve the model performance and enhance the learning efficiency at the beginning of training in unstructured environments. This training method has implications for the training process of reinforcement learning in other domains.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/1d2acae1d631e932c7ebe96fad3c3b04909e5cee.pdf",
        "venue": "Electronics",
        "citationCount": 22,
        "score": 7.333333333333333
    },
    "fe7382db243694c67c667cf2ec80072577d2372b.pdf": {
        "title": "Rule-Aware Reinforcement Learning for Knowledge Graph Reasoning",
        "authors": [
            "Zhongni Hou",
            "Xiaolong Jin",
            "Zixuan Li",
            "Long Bai"
        ],
        "published_date": "2021",
        "abstract": "Multi-hop reasoning is an effective and ex-plainable approach to predicting missing facts in Knowledge Graphs (KGs). It usually adopts the Reinforcement Learning (RL) framework and searches over the KG to \ufb01nd an evidential path. However, due to the large exploration space, the RL-based model struggles with the serious sparse reward problem and needs to make a lot of trials. Moreover, its exploration can be biased towards spurious paths that coin-cidentally lead to correct answers. To solve both problems, we propose a simple but effective RL-based method called RARL (Rule-Aware RL). It injects high quality symbolic rules into the model\u2019s reasoning process and employs partially random beam search, which can not only increase the probability of paths getting rewards, but also alleviate the impact of spurious paths. Experimental results show that it outperforms existing multi-hop methods in terms of Hit@1 and MRR.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/fe7382db243694c67c667cf2ec80072577d2372b.pdf",
        "venue": "Findings",
        "citationCount": 29,
        "score": 7.25
    },
    "ba0e111b711e9b577932a02c9e40bc44b56cb88d.pdf": {
        "title": "Reinforcement Learning with Fast Stabilization in Linear Dynamical Systems",
        "authors": [
            "Sahin Lale",
            "K. Azizzadenesheli",
            "B. Hassibi",
            "Anima Anandkumar"
        ],
        "published_date": "2020",
        "abstract": "In this work, we study model-based reinforcement learning (RL) in unknown stabilizable linear dynamical systems. When learning a dynamical system, one needs to stabilize the unknown dynamics in order to avoid system blow-ups. We propose an algorithm that certifies fast stabilization of the underlying system by effectively exploring the environment with an improved exploration strategy. We show that the proposed algorithm attains $\\tilde{\\mathcal{O}}(\\sqrt{T})$ regret after $T$ time steps of agent-environment interaction. We also show that the regret of the proposed algorithm has only a polynomial dependence in the problem dimensions, which gives an exponential improvement over the prior methods. Our improved exploration method is simple, yet efficient, and it combines a sophisticated exploration policy in RL with an isotropic exploration strategy to achieve fast stabilization and improved regret. We empirically demonstrate that the proposed algorithm outperforms other popular methods in several adaptive control tasks.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/ba0e111b711e9b577932a02c9e40bc44b56cb88d.pdf",
        "venue": "International Conference on Artificial Intelligence and Statistics",
        "citationCount": 36,
        "score": 7.2
    },
    "cf628a42ee56c8f1b858790822a2bc0a61a49110.pdf": {
        "title": "Deep Black-Box Reinforcement Learning with Movement Primitives",
        "authors": [
            "Fabian Otto",
            "Onur \u00c7elik",
            "Hongyi Zhou",
            "Hanna Ziesche",
            "Ngo Anh Vien",
            "G. Neumann"
        ],
        "published_date": "2022",
        "abstract": "\\Episode-based reinforcement learning (ERL) algorithms treat reinforcement learning (RL) as a black-box optimization problem where we learn to select a parameter vector of a controller, often represented as a movement primitive, for a given task descriptor called a context. ERL offers several distinct benefits in comparison to step-based RL. It generates smooth control trajectories, can handle non-Markovian reward definitions, and the resulting exploration in parameter space is well suited for solving sparse reward settings. Yet, the high dimensionality of the movement primitive parameters has so far hampered the effective use of deep RL methods. In this paper, we present a new algorithm for deep ERL. It is based on differentiable trust region layers, a successful on-policy deep RL algorithm. These layers allow us to specify trust regions for the policy update that are solved exactly for each state using convex optimization, which enables policies learning with the high precision required for the ERL. We compare our ERL algorithm to state-of-the-art step-based algorithms in many complex simulated robotic control tasks. In doing so, we investigate different reward formulations - dense, sparse, and non-Markovian. While step-based algorithms perform well only on dense rewards, ERL performs favorably on sparse and non-Markovian rewards. Moreover, our results show that the sparse and the non-Markovian rewards are also often better suited to define the desired behavior, allowing us to obtain considerably higher quality policies compared to step-based RL.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/cf628a42ee56c8f1b858790822a2bc0a61a49110.pdf",
        "venue": "Conference on Robot Learning",
        "citationCount": 21,
        "score": 7.0
    },
    "fbcace16369032bb0292754bd78d03b68b554a95.pdf": {
        "title": "Stability-Preserving Automatic Tuning of PID Control with Reinforcement Learning",
        "authors": [
            "Ayub I. Lakhani",
            "Myisha A. Chowdhury",
            "Qiugang Lu"
        ],
        "published_date": "2021",
        "abstract": "Proportional-Integral-Derivative (PID) control has been the dominant control strategy in the process industry due to its simplicity in design and effectiveness in controlling a wide range of processes. However, most traditional PID tuning methods rely on trial and error for complex processes where insights about the system are limited and may not yield the optimal PID parameters. To address the issue, this work proposes an automatic PID tuning framework based on reinforcement learning (RL), particularly the deterministic policy gradient (DPG) method. Different from existing studies on using RL for PID tuning, in this work, we explicitly consider the closed-loop stability throughout the RL-based tuning process. In particular, we propose a novel episodic tuning framework that allows for an episodic closed-loop operation under selected PID parameters where the actor and critic networks are updated once at the end of each episode. To ensure the closed-loop stability during the tuning, we initialize the training with a conservative but stable baseline PID controller and the resultant reward is used as a benchmark score. A supervisor mechanism is used to monitor the running reward (e.g., tracking error) at each step in the episode. As soon as the running reward exceeds the benchmark score, the underlying controller is replaced by the baseline controller as an early correction to prevent instability. Moreover, we use layer normalization to standardize the input to each layer in actor and critic networks to overcome the issue of policy saturation at action bounds, to ensure the convergence to the optimum. The developed methods are validated through setpoint tracking experiments on a second-order plus dead-time system. Simulation results show that with our scheme, the closed-loop stability can be maintained throughout RL explorations and the explored PID parameters by the RL agent converge quickly to the optimum. Moreover, through simulation verification, the developed RL-based PID tuning method can adapt the PID parameters to changes in the process model automatically without requiring any knowledge about the underlying operating condition, in contrast to other adaptive methods such as the gain scheduling control.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/fbcace16369032bb0292754bd78d03b68b554a95.pdf",
        "venue": "Complex Engineering Systems",
        "citationCount": 28,
        "score": 7.0
    },
    "be33087668f98ac746e72999178d7641d27412f9.pdf": {
        "title": "A Multi-agent Reinforcement Learning Method for Swarm Robots in Space Collaborative Exploration",
        "authors": [
            "Yixin Huang",
            "Shufan Wu",
            "Z. Mu",
            "Xiangyu Long",
            "Sunhao Chu",
            "G. Zhao"
        ],
        "published_date": "2020",
        "abstract": "Deep-space exploration missions are known as particularly challenging with high risk and cost, as they operate in environments with high uncertainty. The fault of exploration robot can even cause the whole mission to failure. One of the solutions is to use swarm robots to operate missions collaboratively. Compared with a single capable robot, a swarm of less sophisticated robots can cooperate on multiple and complex tasks. Reinforcement learning (RL) has made a variety of progress in multi-agent system autonomous cooperative control domains. In this paper, we construct a collaborative exploration scenario, where a multi-robot system explores an unknown Mars surface. Tasks are assigned to robots by human scientists and each robot takes optimal policies autonomously. The method used to train policies is a multi-agent deep deterministic policy gradient algorithm (MADDPG) and we design an experience sample optimizer to improve this algorithm. The results show that, with the increase of robots and targets number, this method is more efficient than traditional deep RL algorithm in a multi-agent collaborative exploration environment.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/be33087668f98ac746e72999178d7641d27412f9.pdf",
        "venue": "2020 6th International Conference on Control, Automation and Robotics (ICCAR)",
        "citationCount": 34,
        "score": 6.800000000000001
    },
    "cae05340421a18c64ac0897d57bcdcc9a496a3b8.pdf": {
        "title": "Research on UCAV Maneuvering Decision Method Based on Heuristic Reinforcement Learning",
        "authors": [
            "Wang Yuan",
            "Z. Xiwen",
            "Zhou Rong",
            "Shangqin Tang",
            "Zhou Huan",
            "Dingkai Wei"
        ],
        "published_date": "2022",
        "abstract": "With the rapid development of unmanned combat aerial vehicle (UCAV)-related technologies, UCAVs are playing an increasingly important role in military operations. It has become an inevitable trend in the development of future air combat battlefields that UCAVs complete air combat tasks independently to acquire air superiority. In this paper, the UCAV maneuver decision problem in continuous action space is studied based on the deep reinforcement learning strategy optimization method. The UCAV platform model of continuous action space was established. Focusing on the problem of insufficient exploration ability of Ornstein\u2013Uhlenbeck (OU) exploration strategy in the deep deterministic policy gradient (DDPG) algorithm, a heuristic DDPG algorithm was proposed by introducing heuristic exploration strategy, and then a UCAV air combat maneuver decision method based on a heuristic DDPG algorithm is proposed. The superior performance of the algorithm is verified by comparison with different algorithms in the test environment, and the effectiveness of the decision method is verified by simulation of air combat tasks with different difficulty and attack modes.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/cae05340421a18c64ac0897d57bcdcc9a496a3b8.pdf",
        "venue": "Computational Intelligence and Neuroscience",
        "citationCount": 20,
        "score": 6.666666666666666
    },
    "cac7f83769836707b02adadb0cda8c791ca23c92.pdf": {
        "title": "Deep Reinforcement Learning-Based Driving Strategy for Avoidance of Chain Collisions and Its Safety Efficiency Analysis in Autonomous Vehicles",
        "authors": [
            "Abu Jafar Md. Muzahid",
            "Syafiq Fauzi Bin Kamarulzaman",
            "Md. Arafatur Rahman",
            "A. Alenezi"
        ],
        "published_date": "2022",
        "abstract": "Vehicle control in autonomous traffic flow is often handled using the best decision-making reinforcement learning methods. However, unexpected critical situations make the collisions more severe and, consequently, the chain collisions. In this work, we first review the leading causes of chain collisions and their subsequent chain events, which might provide an indication of how to prevent and mitigate the crash severity of chain collisions. Then, we consider the problem of chain collision avoidance as a Markov Decision Process problem in order to propose a reinforcement learning-based decision-making strategy and analyse the safety efficiency of existing methods in driving security. To address this, A reward function is being developed to deal with the challenge of multiple vehicle collision avoidance. A perception network structure based on formation and on actor-critic methodologies is employed to enhance the decision-making process. Finally, in the safety efficiency analysis phase, we investigated the safety efficiency performance of the agent vehicle in both single-agent and multi-agent autonomous driving environments. Three state-of-the-art contemporary actor-critic algorithms are used to create an extensive simulation in Unity3D. Moreover, to demonstrate the accuracy of the safety efficiency analysis, multiple training runs of the neural networks in respect of training performance, speed of training, success rate, and stability of rewards with a trade-off between exploitation and exploration during training are presented. Two aspects (single-agent and multi-agent) have assessed the efficiency of algorithms. Every aspect has been analyzed regarding the traffic flows: (1) the controlling efficiency of unexpected traffic situations by the sudden slowdown, (2) abrupt lane change, and (3) smoothly reaching the destination. All the findings of the analysis are intended to shed insight on the benefits of a greater, more reliable autonomous traffic set-up for academics and policymakers, and also to pave the way for the actual carry-out of a driver-less traffic world.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/cac7f83769836707b02adadb0cda8c791ca23c92.pdf",
        "venue": "IEEE Access",
        "citationCount": 20,
        "score": 6.666666666666666
    },
    "f4712cdb025ba4ab879bb47d5cf693a4923f1532.pdf": {
        "title": "Safe Building HVAC Control via Batch Reinforcement Learning",
        "authors": [
            "Chi Zhang",
            "S. Kuppannagari",
            "V. Prasanna"
        ],
        "published_date": "2022",
        "abstract": "In this paper, we study safe building HVAC control via batch reinforcement learning. Random exploration in building HVAC control is infeasible due to safety considerations. However, diverse states are necessary for RL algorithms to learn useful policies. To enable <italic>safety</italic> during exploration, we propose guided exploration by adding a Gaussian noise to a hand-crafted rule-based controller. Adjusting the variance of the noise provides a tradeoff between the <italic>diversity</italic> of the dataset and the <italic>safety</italic>. We apply Conservative Q Learning (CQL) to learn a policy. CQL ensures that the trained policy stays within the policy distribution used to collect the dataset, thereby guarantees safety at deployment. To select the optimal policy during the offline training, we apply model-based performance evaluation. We use the widely adopted CityLearn testbed to evaluate the performance of our proposed method. Compared with a rule-based controller, our approach obtains <inline-formula><tex-math notation=\"LaTeX\">$12\\%\\sim 35\\%$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>12</mml:mn><mml:mo>%</mml:mo><mml:mo>\u223c</mml:mo><mml:mn>35</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"zhang-ieq1-3164084.gif\"/></alternatives></inline-formula> reduction in ramping, <inline-formula><tex-math notation=\"LaTeX\">$3\\%\\sim 10\\%$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>3</mml:mn><mml:mo>%</mml:mo><mml:mo>\u223c</mml:mo><mml:mn>10</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"zhang-ieq2-3164084.gif\"/></alternatives></inline-formula> reduction in 1-load factor, <inline-formula><tex-math notation=\"LaTeX\">$3\\%\\sim 8\\%$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>3</mml:mn><mml:mo>%</mml:mo><mml:mo>\u223c</mml:mo><mml:mn>8</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"zhang-ieq3-3164084.gif\"/></alternatives></inline-formula> reduction in daily peak at deployment with less than <inline-formula><tex-math notation=\"LaTeX\">$10\\%$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>10</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"zhang-ieq4-3164084.gif\"/></alternatives></inline-formula> performance degradation during the exploration. On the contrary, the performance degradation of the state-of-the-art online reinforcement learning algorithm during exploration is around <inline-formula><tex-math notation=\"LaTeX\">$8\\%\\sim 18\\%$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>8</mml:mn><mml:mo>%</mml:mo><mml:mo>\u223c</mml:mo><mml:mn>18</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"zhang-ieq5-3164084.gif\"/></alternatives></inline-formula>. It also fails to surpass the performance of the rule-based controller at deployment.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/f4712cdb025ba4ab879bb47d5cf693a4923f1532.pdf",
        "venue": "IEEE Transactions on Sustainable Computing",
        "citationCount": 20,
        "score": 6.666666666666666
    },
    "b1be7ce8c639291adf7663535f0451f9ac03ed55.pdf": {
        "title": "Exploring Reward Strategies for Wind Turbine Pitch Control by Reinforcement Learning",
        "authors": [
            "J. E. Sierra-Garc\u00eda",
            "Matilde Santos"
        ],
        "published_date": "2020",
        "abstract": "In this work, a pitch controller of a wind turbine (WT) inspired by reinforcement learning (RL) is designed and implemented. The control system consists of a state estimator, a reward strategy, a policy table, and a policy update algorithm. Novel reward strategies related to the energy deviation from the rated power are defined. They are designed to improve the efficiency of the WT. Two new categories of reward strategies are proposed: \u201conly positive\u201d (O-P) and \u201cpositive-negative\u201d (P-N) rewards. The relationship of these categories with the exploration-exploitation dilemma, the use of \u03f5-greedy methods and the learning convergence are also introduced and linked to the WT control problem. In addition, an extensive analysis of the influence of the different rewards in the controller performance and in the learning speed is carried out. The controller is compared with a proportional-integral-derivative (PID) regulator for the same small wind turbine, obtaining better results. The simulations show how the P-N rewards improve the performance of the controller, stabilize the output power around the rated power, and reduce the error over time.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/b1be7ce8c639291adf7663535f0451f9ac03ed55.pdf",
        "venue": "Applied Sciences",
        "citationCount": 33,
        "score": 6.6000000000000005
    },
    "a064b8183d657178916ae21c43b5099bfef6804d.pdf": {
        "title": "A Reinforcement Learning Method for a Hybrid Flow-Shop Scheduling Problem",
        "authors": [
            "Wei Han",
            "Fang Guo",
            "Xi-chao Su"
        ],
        "published_date": "2019",
        "abstract": "The scheduling problems in mass production, manufacturing, assembly, synthesis, and transportation, as well as internet services, can partly be attributed to a hybrid flow-shop scheduling problem (HFSP). To solve the problem, a reinforcement learning (RL) method for HFSP is studied for the first time in this paper. HFSP is described and attributed to the Markov Decision Processes (MDP), for which the special states, actions, and reward function are designed. On this basis, the MDP framework is established. The Boltzmann exploration policy is adopted to trade-off the exploration and exploitation during choosing action in RL. Compared with the first-come-first-serve strategy that is frequently adopted when coding in most of the traditional intelligent algorithms, the rule in the RL method is first-come-first-choice, which is more conducive to achieving the global optimal solution. For validation, the RL method is utilized for scheduling in a metal processing workshop of an automobile engine factory. Then, the method is applied to the sortie scheduling of carrier aircraft in continuous dispatch. The results demonstrate that the machining and support scheduling obtained by this RL method are reasonable in result quality, real-time performance and complexity, indicating that this RL method is practical for HFSP.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/a064b8183d657178916ae21c43b5099bfef6804d.pdf",
        "venue": "Algorithms",
        "citationCount": 39,
        "score": 6.5
    },
    "bc98c81467ed3a6b21788f39c20cbe659014e551.pdf": {
        "title": "Safe exploration of nonlinear dynamical systems: A predictive safety filter for reinforcement learning",
        "authors": [
            "K. P. Wabersich",
            "M. Zeilinger"
        ],
        "published_date": "2018",
        "abstract": "",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/bc98c81467ed3a6b21788f39c20cbe659014e551.pdf",
        "venue": "arXiv.org",
        "citationCount": 44,
        "score": 6.285714285714286
    },
    "621d57c1243f055bc3850c1f3e38f351f53c947f.pdf": {
        "title": "Tightening Exploration in Upper Confidence Reinforcement Learning",
        "authors": [
            "Hippolyte Bourel",
            "Odalric-Ambrym Maillard",
            "M. S. Talebi"
        ],
        "published_date": "2020",
        "abstract": "The upper confidence reinforcement learning (UCRL2) strategy introduced in (Jaksch et al., 2010) is a popular method to perform regret minimization in unknown discrete Markov Decision Processes under the average-reward criterion. Despite its nice and generic theoretical regret guarantees, this strategy and its variants have remained until now mostly theoretical as numerical experiments on simple environments exhibit long burn-in phases before the learning takes place. Motivated by practical efficiency, we present UCRL3, following the lines of UCRL2, but with two key modifications: First, it uses state-of-the-art time-uniform concentration inequalities, to compute confidence sets on the reward and transition distributions for each state-action pair. To further tighten exploration, we introduce an adaptive computation of the support of each transition distributions. This enables to revisit the extended value iteration procedure to optimize over distributions with reduced support by disregarding low probability transitions, while still ensuring near-optimism. We demonstrate, through numerical experiments on standard environments, that reducing exploration this way yields a substantial numerical improvement compared to UCRL2 and its variants. On the theoretical side, these key modifications enable to derive a regret bound for UCRL3 improving on UCRL2, that for the first time makes appear a notion of local diameter and effective support, thanks to variance-aware concentration bounds.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/621d57c1243f055bc3850c1f3e38f351f53c947f.pdf",
        "venue": "International Conference on Machine Learning",
        "citationCount": 31,
        "score": 6.2
    },
    "9b5aa6a75d8e9f0e68d12e3b331acad6f32667d4.pdf": {
        "title": "Safe and Efficient Reinforcement Learning using Disturbance-Observer-Based Control Barrier Functions",
        "authors": [
            "Yikun Cheng",
            "Pan Zhao",
            "N. Hovakimyan"
        ],
        "published_date": "2022",
        "abstract": "Safe reinforcement learning (RL) with assured satisfaction of hard state constraints during training has recently received a lot of attention. Safety filters, e.g., based on control barrier functions (CBFs), provide a promising way for safe RL via modifying the unsafe actions of an RL agent on the fly. Existing safety filter-based approaches typically involve learning of uncertain dynamics and quantifying the learned model error, which leads to conservative filters before a large amount of data is collected to learn a good model, thereby preventing efficient exploration. This paper presents a method for safe and efficient RL using disturbance observers (DOBs) and control barrier functions (CBFs). Unlike most existing safe RL methods that deal with hard state constraints, our method does not involve model learning, and leverages DOBs to accurately estimate the pointwise value of the uncertainty, which is then incorporated into a robust CBF condition to generate safe actions. The DOB-based CBF can be used as a safety filter with model-free RL algorithms by minimally modifying the actions of an RL agent whenever necessary to ensure safety throughout the learning process. Simulation results on a unicycle and a 2D quadrotor demonstrate that the proposed method outperforms a state-of-the-art safe RL algorithm using CBFs and Gaussian processes-based model learning, in terms of safety violation rate, and sample and computational efficiency.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/9b5aa6a75d8e9f0e68d12e3b331acad6f32667d4.pdf",
        "venue": "Conference on Learning for Dynamics & Control",
        "citationCount": 18,
        "score": 6.0
    },
    "6c66fc8000da4d80bb57e60667e35a051016144a.pdf": {
        "title": "Safe reinforcement learning for multi-energy management systems with known constraint functions",
        "authors": [
            "Glenn Ceusters",
            "L. R. Camargo",
            "R. Franke",
            "Ann Now'e",
            "M. Messagie"
        ],
        "published_date": "2022",
        "abstract": ": Reinforcement learning (RL) is a promising optimal control technique for multi-energy management systems. It does not require a model a priori - reducing the upfront and ongoing project-speci\ufb01c engineering effort and is capable of learning better representations of the underlying system dynamics. However, vanilla RL does not provide constraint satisfaction guarantees - resulting in various unsafe interactions within its safety-critical environment. In this paper, we present two novel safe RL methods, namely SafeFallback and GiveSafe, where the safety constraint formulation is decoupled from the RL formulation and which provides hard-constraint satisfaction guarantees both during training (exploration) and exploitation of the (close-to) optimal policy. In a simulated multi-energy systems case study we have shown that both methods start with a signi\ufb01cantly higher utility (i.e. useful policy) compared to a vanilla RL benchmark (94,6% and 82,8% compared to 35,5%) and that the proposed SafeFallback method even can outperform the vanilla RL benchmark (102,9% to 100%). We conclude that both methods are viably safety constraint handling techniques capable beyond RL, as demonstrated with random agents while still providing hard-constraint guarantees. Finally, we propose fundamental future work to i.a. improve the constraint functions itself as more data becomes available.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/6c66fc8000da4d80bb57e60667e35a051016144a.pdf",
        "venue": "Energy and AI",
        "citationCount": 18,
        "score": 6.0
    },
    "5fd3ce235f5fcebd3d2807f710b060add527183b.pdf": {
        "title": "Deep Curiosity Search: Intra-Life Exploration Improves Performance on Challenging Deep Reinforcement Learning Problems",
        "authors": [
            "C. Stanton",
            "J. Clune"
        ],
        "published_date": "2018",
        "abstract": "Traditional exploration methods in RL require agents to perform random actions to find rewards. But these approaches struggle on sparse-reward domains like Montezuma's Revenge where the probability that any random action sequence leads to reward is extremely low. Recent algorithms have performed well on such tasks by encouraging agents to visit new states or perform new actions in relation to all prior training episodes (which we call across-training novelty). But such algorithms do not consider whether an agent exhibits intra-life novelty: doing something new within the current episode, regardless of whether those behaviors have been performed in previous episodes. We hypothesize that across-training novelty might discourage agents from revisiting initially non-rewarding states that could become important stepping stones later in training. We introduce Deep Curiosity Search (DeepCS), which encourages intra-life exploration by rewarding agents for visiting as many different states as possible within each episode, and show that DeepCS matches the performance of current state-of-the-art methods on Montezuma's Revenge. We further show that DeepCS improves exploration on Amidar, Freeway, Gravitar, and Tutankham (many of which are hard exploration games). Surprisingly, DeepCS doubles A2C performance on Seaquest, a game we would not have expected to benefit from intra-life exploration because the arena is small and already easily navigated by naive exploration techniques. In one run, DeepCS achieves a maximum training score of 80,000 points on Seaquest, higher than any methods other than Ape-X. The strong performance of DeepCS on these sparse- and dense-reward tasks suggests that encouraging intra-life novelty is an interesting, new approach for improving performance in Deep RL and motivates further research into hybridizing across-training and intra-life exploration methods.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/5fd3ce235f5fcebd3d2807f710b060add527183b.pdf",
        "venue": "arXiv.org",
        "citationCount": 42,
        "score": 6.0
    },
    "a011901fd788fc5ad452dd3d88f9f0970ea547a8.pdf": {
        "title": "QD-RL: Efficient Mixing of Quality and Diversity in Reinforcement Learning",
        "authors": [
            "Geoffrey Cideron",
            "Thomas Pierrot",
            "Nicolas Perrin",
            "Karim Beguir",
            "Olivier Sigaud"
        ],
        "published_date": "2020",
        "abstract": "",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/a011901fd788fc5ad452dd3d88f9f0970ea547a8.pdf",
        "venue": "arXiv.org",
        "citationCount": 29,
        "score": 5.800000000000001
    },
    "3efc894d0990faeb2f69194195d465ed64694104.pdf": {
        "title": "Feudal Latent Space Exploration for Coordinated Multi-Agent Reinforcement Learning",
        "authors": [
            "Xiangyu Liu",
            "Ying Tan"
        ],
        "published_date": "2022",
        "abstract": "",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/3efc894d0990faeb2f69194195d465ed64694104.pdf",
        "venue": "IEEE Transactions on Neural Networks and Learning Systems",
        "citationCount": 17,
        "score": 5.666666666666666
    },
    "46bd3c20e6f7a9b6e413ea9f3452965601fd6de9.pdf": {
        "title": "Unsupervised Reinforcement Learning for Transferable Manipulation Skill Discovery",
        "authors": [
            "Daesol Cho",
            "Jigang Kim",
            "H. J. Kim"
        ],
        "published_date": "2022",
        "abstract": "Current reinforcement learning (RL) in robotics often experiences difficulty in generalizing to new downstream tasks due to the innate task-specific training paradigm. To alleviate it, unsupervised RL, a framework that pre-trains the agent in a task-agnostic manner without access to the task-specific reward, leverages active exploration for distilling diverse experience into essential skills or reusable knowledge. For exploiting such benefits also in robotic manipulation, we propose an unsupervised method for transferable manipulation skill discovery that ties structured exploration toward interacting behavior and transferable skill learning. It not only enables the agent to learn interaction behavior, the key aspect of the robotic manipulation learning, without access to the environment reward, but also to generalize to arbitrary downstream manipulation tasks with the learned task-agnostic skills. Through comparative experiments, we show that our approach achieves the most diverse interacting behavior and significantly improves sample efficiency in downstream tasks including the extension to multi-object, multitask problems.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/46bd3c20e6f7a9b6e413ea9f3452965601fd6de9.pdf",
        "venue": "IEEE Robotics and Automation Letters",
        "citationCount": 17,
        "score": 5.666666666666666
    },
    "1d4288c47a7802575d2a0d231d1283a4f225a85b.pdf": {
        "title": "MetaCURE: Meta Reinforcement Learning with Empowerment-Driven Exploration",
        "authors": [
            "Jin Zhang",
            "Jianhao Wang",
            "Hao Hu",
            "Tong Chen",
            "Yingfeng Chen",
            "Changjie Fan",
            "Chongjie Zhang"
        ],
        "published_date": "2020",
        "abstract": "Meta reinforcement learning (meta-RL) extracts knowledge from previous tasks and achieves fast adaptation to new tasks. Despite recent progress, efficient exploration in meta-RL remains a key challenge in sparse-reward tasks, as it requires quickly finding informative task-relevant experiences in both meta-training and adaptation. To address this challenge, we explicitly model an exploration policy learning problem for meta-RL, which is separated from exploitation policy learning, and introduce a novel empowerment-driven exploration objective, which aims to maximize information gain for task identification. We derive a corresponding intrinsic reward and develop a new off-policy meta-RL framework, which efficiently learns separate context-aware exploration and exploitation policies by sharing the knowledge of task inference. Experimental evaluation shows that our meta-RL method significantly outperforms state-of-the-art baselines on various sparse-reward MuJoCo locomotion tasks and more complex sparse-reward Meta-World tasks.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/1d4288c47a7802575d2a0d231d1283a4f225a85b.pdf",
        "venue": "International Conference on Machine Learning",
        "citationCount": 28,
        "score": 5.6000000000000005
    },
    "1a39ad2d1553d07084b5e44a28878c8bb5018cef.pdf": {
        "title": "PC-MLP: Model-based Reinforcement Learning with Policy Cover Guided Exploration",
        "authors": [
            "Yuda Song",
            "Wen Sun"
        ],
        "published_date": "2021",
        "abstract": "Model-based Reinforcement Learning (RL) is a popular learning paradigm due to its potential sample efficiency compared to model-free RL. However, existing empirical model-based RL approaches lack the ability to explore. This work studies a computationally and statistically efficient model-based algorithm for both Kernelized Nonlinear Regulators (KNR) and linear Markov Decision Processes (MDPs). For both models, our algorithm guarantees polynomial sample complexity and only uses access to a planning oracle. Experimentally, we first demonstrate the flexibility and efficacy of our algorithm on a set of exploration challenging control tasks where existing empirical model-based RL approaches completely fail. We then show that our approach retains excellent performance even in common dense reward control benchmarks that do not require heavy exploration. Finally, we demonstrate that our method can also perform reward-free exploration efficiently. Our code can be found at https://github.com/yudasong/PCMLP.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/1a39ad2d1553d07084b5e44a28878c8bb5018cef.pdf",
        "venue": "International Conference on Machine Learning",
        "citationCount": 22,
        "score": 5.5
    },
    "ecf5dc817fd6326e943b759c889d1285e673b24a.pdf": {
        "title": "Digital Twin-Assisted Efficient Reinforcement Learning for Edge Task Scheduling",
        "authors": [
            "Xiucheng Wang",
            "Longfei Ma",
            "Hao Li",
            "Zhisheng Yin",
            "T. Luan",
            "Nan Cheng"
        ],
        "published_date": "2022",
        "abstract": "Task scheduling is a critical problem when one user offloads multiple different tasks to the edge server. When a user has multiple tasks to offload and only one task can be transmitted to server at a time, while server processes tasks according to the transmission order, the problem is NP-hard. However, it is difficult for traditional optimization methods to quickly obtain the optimal solution, while approaches based on reinforcement learning face with the challenge of excessively large action space and slow convergence. In this paper, we propose a Digital Twin (DT)-assisted RL-based task scheduling method in order to improve the performance and convergence of the RL. We use DT to simulate the results of different decisions made by the agent, so that one agent can try multiple actions at a time, or, similarly, multiple agents can interact with environment in parallel in DT. In this way, the exploration efficiency of RL can be significantly improved via DT, and thus RL can converges faster and local optimality is less likely to happen. Particularly, two algorithms are designed to made task scheduling decisions, i.e., DT-assisted asynchronous Q-learning (DTAQL) and DT-assisted exploring Q-learning (DTEQL). Simulation results show that both algorithms significantly improve the convergence speed of Q-learning by increasing the exploration efficiency.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/ecf5dc817fd6326e943b759c889d1285e673b24a.pdf",
        "venue": "IEEE Vehicular Technology Conference",
        "citationCount": 16,
        "score": 5.333333333333333
    },
    "02ad21eea9ec32783ba529487e74a76e85499a53.pdf": {
        "title": "Model-Based Offline Meta-Reinforcement Learning with Regularization",
        "authors": [
            "Sen Lin",
            "Jialin Wan",
            "Tengyu Xu",
            "Yingbin Liang",
            "Junshan Zhang"
        ],
        "published_date": "2022",
        "abstract": "Existing offline reinforcement learning (RL) methods face a few major challenges, particularly the distributional shift between the learned policy and the behavior policy. Offline Meta-RL is emerging as a promising approach to address these challenges, aiming to learn an informative meta-policy from a collection of tasks. Nevertheless, as shown in our empirical studies, offline Meta-RL could be outperformed by offline single-task RL methods on tasks with good quality of datasets, indicating that a right balance has to be delicately calibrated between\"exploring\"the out-of-distribution state-actions by following the meta-policy and\"exploiting\"the offline dataset by staying close to the behavior policy. Motivated by such empirical analysis, we explore model-based offline Meta-RL with regularized Policy Optimization (MerPO), which learns a meta-model for efficient task structure inference and an informative meta-policy for safe exploration of out-of-distribution state-actions. In particular, we devise a new meta-Regularized model-based Actor-Critic (RAC) method for within-task policy optimization, as a key building block of MerPO, using conservative policy evaluation and regularized policy improvement; and the intrinsic tradeoff therein is achieved via striking the right balance between two regularizers, one based on the behavior policy and the other on the meta-policy. We theoretically show that the learnt policy offers guaranteed improvement over both the behavior policy and the meta-policy, thus ensuring the performance improvement on new tasks via offline Meta-RL. Experiments corroborate the superior performance of MerPO over existing offline Meta-RL methods.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/02ad21eea9ec32783ba529487e74a76e85499a53.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 16,
        "score": 5.333333333333333
    },
    "a4a509d9019deac486087a0b10158ac115274de6.pdf": {
        "title": "UniRLTest: universal platform-independent testing with reinforcement learning via image understanding",
        "authors": [
            "Ziqian Zhang",
            "Yulei Liu",
            "Shengcheng Yu",
            "Xin Li",
            "Yexiao Yun",
            "Chunrong Fang",
            "Zhenyu Chen"
        ],
        "published_date": "2022",
        "abstract": "GUI testing has been prevailing in software testing. However, existing automated GUI testing tools mostly rely on frameworks of a specific platform. Testers have to fully understand platform features before developing platform-dependent GUI testing tools. Starting from the perspective of tester\u2019s vision, we observe that GUIs on different platforms share commonalities of widget images and layout designs, which can be leveraged to achieve platform-independent testing. We propose UniRLTest, an automated software testing framework, to achieve platform independence testing. UniRLTest utilizes computer vision techniques to capture all the widgets in the screenshot and constructs a widget tree for each page. A set of all the executable actions in each tree will be generated accordingly. UniRLTest adopts a Deep Q-Network, a reinforcement learning (RL) method, to the exploration process and formalize the Android GUI testing problem to a Marcov Decision Process (MDP), where RL could work. We have conducted evaluation experiments on 25 applications from different platforms. The result shows that UniRLTest outperforms baselines in terms of efficiency and effectiveness.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/a4a509d9019deac486087a0b10158ac115274de6.pdf",
        "venue": "International Symposium on Software Testing and Analysis",
        "citationCount": 16,
        "score": 5.333333333333333
    },
    "a17a7256c04afee68f9aa0b7bfdc67fbca998b9c.pdf": {
        "title": "Accelerating Reinforcement Learning for Autonomous Driving Using Task-Agnostic and Ego-Centric Motion Skills",
        "authors": [
            "Tong Zhou",
            "Letian Wang",
            "Ruobing Chen",
            "Wenshuo Wang",
            "Y. Liu"
        ],
        "published_date": "2022",
        "abstract": "Efficient and effective exploration in continuous space is a central problem in applying reinforcement learning (RL) to autonomous driving. Skills learned from expert demonstrations or designed for specific tasks can benefit the exploration, but they are usually costly-collected, unbalanced/suboptimal, or failing to transfer to diverse tasks. However, human drivers can adapt to varied driving tasks without demonstrations by taking efficient and structural explorations in the entire skill space rather than a limited space with task-specific skills. Inspired by the above fact, we propose an RL algorithm exploring all feasible motion skills instead of a limited set of task-specific and object-centric skills. Without demonstrations, our method can still perform well in diverse tasks. First, we build a task-agnostic and ego-centric (TaEc) motion skill library in a pure motion perspective, which is diverse enough to be reusable in different complex tasks. The motion skills are then encoded into a low-dimension latent skill space, in which RL can do exploration efficiently. Validations in various challenging driving scenarios demonstrate that our proposed method, TaEc-RL, outperforms its counterparts significantly in learning efficiency and task performance.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/a17a7256c04afee68f9aa0b7bfdc67fbca998b9c.pdf",
        "venue": "IEEE/RJS International Conference on Intelligent RObots and Systems",
        "citationCount": 16,
        "score": 5.333333333333333
    },
    "69bdc99655204190697067c3da5296e544e6865d.pdf": {
        "title": "Safe Model-Based Reinforcement Learning With an Uncertainty-Aware Reachability Certificate",
        "authors": [
            "Dongjie Yu",
            "Wenjun Zou",
            "Yujie Yang",
            "Haitong Ma",
            "Sheng Li",
            "Yuming Yin",
            "Jianyu Chen",
            "Jingliang Duan"
        ],
        "published_date": "2022",
        "abstract": "Safe reinforcement learning (RL) that solves constraint-satisfactory policies provides a promising way to the broader safety-critical applications of RL in real-world problems such as robotics. Among all safe RL approaches, model-based methods reduce training time violations further due to their high sample efficiency. However, lacking safety robustness against the model uncertainties remains an issue in safe model-based RL, especially in training time safety. In this paper, we propose a distributional reachability certificate (DRC) and its Bellman equation to address model uncertainties and characterize robust persistently safe states. Furthermore, we build a safe RL framework to resolve constraints required by the DRC and its corresponding shield policy. We also devise a line search method to maintain safety and reach higher returns simultaneously while leveraging the shield policy. Comprehensive experiments on classical benchmarks such as constrained tracking and navigation indicate that the proposed algorithm achieves comparable returns with much fewer constraint violations during training. Our code is available at https://github.com/ManUtdMoon/Distributional-Reachability-Policy-Optimization. Note to Practitioners\u2014Although it has been proven that RL can be applied in complex robotics control tasks, the training process of an RL control policy induces frequent failures because the agent needs to learn safety through constraint violations. This issue hinders the promotion of RL because a large amount of failure of robots is too expensive to afford. This paper aims to reduce the training-time violations of RL-based control methods, enabling RL to be leveraged in a broader application area. To achieve the goal, we first introduce a safety quantity describing the distribution of potential constraint violations in the long term. By imposing constraints on the quantile of the safety distribution, we can realize safety robust to the model uncertainty, which is necessary for real-world robot learning with environment uncertainty. Second, we further devise a shield policy aiming to minimize the constraint violation. The policy will intervene when the agent is about to violate state constraints, further enhancing exploration safety. Third, we implement a line search method to find an action pursuing near-optimal performance when fulfilling safety requirements strictly. Our experimental results indicate that the proposed algorithm reduces training-time violations significantly while maintaining competitive task performance. We make a step towards applying RL safely in real-world tasks. Our future work includes conducting physical verification on real robots to evaluate the algorithm and improving safety further by starting from an initially safe control policy that comes from domain knowledge.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/69bdc99655204190697067c3da5296e544e6865d.pdf",
        "venue": "IEEE Transactions on Automation Science and Engineering",
        "citationCount": 16,
        "score": 5.333333333333333
    },
    "abaa1a3a8468473fd2827e49623eabc36ffaf8fe.pdf": {
        "title": "Model-based reinforcement learning with parametrized physical models and optimism-driven exploration",
        "authors": [
            "Christopher Xie",
            "S. Patil",
            "T. Moldovan",
            "S. Levine",
            "P. Abbeel"
        ],
        "published_date": "2015",
        "abstract": "In this paper, we present a robotic model-based reinforcement learning method that combines ideas from model identification and model predictive control. We use a feature-based representation of the dynamics that allows the dynamics model to be fitted with a simple least squares procedure, and the features are identified from a high-level specification of the robot's morphology, consisting of the number and connectivity structure of its links. Model predictive control is then used to choose the actions under an optimistic model of the dynamics, which produces an efficient and goal-directed exploration strategy. We present real time experimental results on standard benchmark problems involving the pendulum, cartpole, and double pendulum systems. Experiments indicate that our method is able to learn a range of benchmark tasks substantially faster than the previous best methods. To evaluate our approach on a realistic robotic control task, we also demonstrate real time control of a simulated 7 degree of freedom arm.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/abaa1a3a8468473fd2827e49623eabc36ffaf8fe.pdf",
        "venue": "IEEE International Conference on Robotics and Automation",
        "citationCount": 50,
        "score": 5.0
    },
    "2029ebd195491dd845e14866045225b238f6c392.pdf": {
        "title": "Multi-Agent Deep Reinforcement Learning-Based Cooperative Spectrum Sensing With Upper Confidence Bound Exploration",
        "authors": [
            "Yu Zhang",
            "Peixiang Cai",
            "Changyong Pan",
            "Subing Zhang"
        ],
        "published_date": "2019",
        "abstract": "In this paper, a multi-agent deep reinforcement learning method was adopted to realize cooperative spectrum sensing in cognitive radio networks. Each secondary user learns an efficient sensing strategy from the sensing results of some of the selected spectra to avoid interference to the primary users and to coordinate with other secondary users. It is necessary to balance exploration and exploitation in the learning process when using deep reinforcement learning methods, helping explain that upper confidence bound with Hoeffding-style bonus has been adopted in this paper to improve the efficiency of exploration. The simulation results verify that the proposed algorithm, when compared with the conventional reinforcement learning methods with $\\varepsilon $ -greedy exploration, is much easier to achieve faster convergence speed and better reward performance.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/2029ebd195491dd845e14866045225b238f6c392.pdf",
        "venue": "IEEE Access",
        "citationCount": 29,
        "score": 4.833333333333333
    },
    "c447bc7891c2a3a775af4f8fb94e34e7968c1cb7.pdf": {
        "title": "A Reinforcement Learning-Based Reconstruction Method for Complex Defect Profiles in MFL Inspection",
        "authors": [
            "Zhenning Wu",
            "Yiming Deng",
            "Jinhai Liu",
            "Lixing Wang"
        ],
        "published_date": "2021",
        "abstract": "Magnetic flux leakage (MFL) inspection is one of the most commonly used nondestructive evaluation (NDT) methods for detecting anomalies of ferromagnetic materials. Sizing of the defect with MFL signal is the key problem of inspection, which is important for evaluating the health condition of the material. However, it is an ill-posed inverse problem that is hard to solve and harder to be accurate. Forward models that give a highly accurate simulated signal with a corresponding depth profile are widely used in solving the inverse problem iteratively. Unfortunately, the policy which determines the iteration process is hard to design. In this article, a reinforcement learning (RL)-based algorithm is proposed to reconstruct the depth of defects with a complex depth profile. Instead of designing the policy, the iterative process of the classic iteration-based method is embedded into the learning process of the RL-based algorithm proposed in this article. The policy is learned from the data generated during the reconstructing iteration. By designing the states, actions, and rewards in the RL structure, the most computationally costly part of calling the forward model during iteration is avoided. An adaptive limited exploration process is given to balance the exploration and exploitation in the inverse problem of MFL inspection in this article. The effectiveness of the proposed algorithm is demonstrated with simulation results under different noise levels. The results demonstrate that the proposed algorithm is robust with good reconstruction accuracy.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/c447bc7891c2a3a775af4f8fb94e34e7968c1cb7.pdf",
        "venue": "IEEE Transactions on Instrumentation and Measurement",
        "citationCount": 19,
        "score": 4.75
    },
    "70e1d6b227fdd605fe61239a953e803df97e521d.pdf": {
        "title": "Model-based Lifelong Reinforcement Learning with Bayesian Exploration",
        "authors": [
            "Haotian Fu",
            "Shangqun Yu",
            "Michael S. Littman",
            "G. Konidaris"
        ],
        "published_date": "2022",
        "abstract": "We propose a model-based lifelong reinforcement-learning approach that estimates a hierarchical Bayesian posterior distilling the common structure shared across different tasks. The learned posterior combined with a sample-based Bayesian exploration procedure increases the sample efficiency of learning across a family of related tasks. We first derive an analysis of the relationship between the sample complexity and the initialization quality of the posterior in the finite MDP setting. We next scale the approach to continuous-state domains by introducing a Variational Bayesian Lifelong Reinforcement Learning algorithm that can be combined with recent model-based deep RL methods, and that exhibits backward transfer. Experimental results on several challenging domains show that our algorithms achieve both better forward and backward transfer performance than state-of-the-art lifelong RL methods.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/70e1d6b227fdd605fe61239a953e803df97e521d.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 14,
        "score": 4.666666666666666
    },
    "a45df3efbd472d4d43ddc4072c61f7f674981ac6.pdf": {
        "title": "Causal Discovery and Reinforcement Learning: A Synergistic Integration",
        "authors": [
            "Arqu\u00edmides M\u00e9ndez-Molina",
            "E. Morales",
            "L. Sucar"
        ],
        "published_date": "2022",
        "abstract": "",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/a45df3efbd472d4d43ddc4072c61f7f674981ac6.pdf",
        "venue": "European Workshop on Probabilistic Graphical Models",
        "citationCount": 14,
        "score": 4.666666666666666
    },
    "5c0b56d9440ea70c79d1d0032d46c14e06f541f5.pdf": {
        "title": "Reactive Exploration to Cope with Non-Stationarity in Lifelong Reinforcement Learning",
        "authors": [
            "C. Steinparz",
            "Thomas Schmied",
            "Fabian Paischer",
            "Marius-Constantin Dinu",
            "Vihang Patil",
            "Angela Bitto-Nemling",
            "Hamid Eghbalzadeh",
            "Sepp Hochreiter"
        ],
        "published_date": "2022",
        "abstract": "In lifelong learning, an agent learns throughout its entire life without resets, in a constantly changing environment, as we humans do. Consequently, lifelong learning comes with a plethora of research problems such as continual domain shifts, which result in non-stationary rewards and environment dynamics. These non-stationarities are difficult to detect and cope with due to their continuous nature. Therefore, exploration strategies and learning methods are required that are capable of tracking the steady domain shifts, and adapting to them. We propose Reactive Exploration to track and react to continual domain shifts in lifelong reinforcement learning, and to update the policy correspondingly. To this end, we conduct experiments in order to investigate different exploration strategies. We empirically show that representatives of the policy-gradient family are better suited for lifelong learning, as they adapt more quickly to distribution shifts than Q-learning. Thereby, policy-gradient methods profit the most from Reactive Exploration and show good results in lifelong learning with continual domain shifts. Our code is available at: https://github.com/ml-jku/reactive-exploration.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/5c0b56d9440ea70c79d1d0032d46c14e06f541f5.pdf",
        "venue": "CoLLAs",
        "citationCount": 14,
        "score": 4.666666666666666
    },
    "0cdbd90989e5f60b6a42dae76b23bd489fcf65cc.pdf": {
        "title": "Robust Policy Optimization in Deep Reinforcement Learning",
        "authors": [
            "Md Masudur Rahman",
            "Yexiang Xue"
        ],
        "published_date": "2022",
        "abstract": "The policy gradient method enjoys the simplicity of the objective where the agent optimizes the cumulative reward directly. Moreover, in the continuous action domain, parameterized distribution of action distribution allows easy control of exploration, resulting from the variance of the representing distribution. Entropy can play an essential role in policy optimization by selecting the stochastic policy, which eventually helps better explore the environment in reinforcement learning (RL). However, the stochasticity often reduces as the training progresses; thus, the policy becomes less exploratory. Additionally, certain parametric distributions might only work for some environments and require extensive hyperparameter tuning. This paper aims to mitigate these issues. In particular, we propose an algorithm called Robust Policy Optimization (RPO), which leverages a perturbed distribution. We hypothesize that our method encourages high-entropy actions and provides a way to represent the action space better. We further provide empirical evidence to verify our hypothesis. We evaluated our methods on various continuous control tasks from DeepMind Control, OpenAI Gym, Pybullet, and IsaacGym. We observed that in many settings, RPO increases the policy entropy early in training and then maintains a certain level of entropy throughout the training period. Eventually, our agent RPO shows consistently improved performance compared to PPO and other techniques: entropy regularization, different distributions, and data augmentation. Furthermore, in several settings, our method stays robust in performance, while other baseline mechanisms fail to improve and even worsen the performance.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/0cdbd90989e5f60b6a42dae76b23bd489fcf65cc.pdf",
        "venue": "arXiv.org",
        "citationCount": 14,
        "score": 4.666666666666666
    },
    "5f3b337e74618a2364778222162b13bd55a15e27.pdf": {
        "title": "A Comparative Study of Deep Reinforcement Learning-based Transferable Energy Management Strategies for Hybrid Electric Vehicles",
        "authors": [
            "Jingyi Xu",
            "Zirui Li",
            "Li Gao",
            "Junyi Ma",
            "Qi Liu",
            "Yanan Zhao"
        ],
        "published_date": "2022",
        "abstract": "The deep reinforcement learning-based energy management strategies (EMS) have become a promising solution for hybrid electric vehicles (HEVs). When driving cycles are changed, the neural network will be retrained, which is a time-consuming and laborious task. A more efficient way of choosing EMS is to combine deep reinforcement learning (DRL) with transfer learning, which can transfer knowledge of one domain to the other new domain, making the network of the new domain reach convergence values quickly. Different exploration methods of DRL, including adding action space noise and parameter space noise, are compared against each other in the transfer learning process in this work. Results indicate that the network added parameter space noise is more stable and faster convergent than the others. In conclusion, the best exploration method for transferable EMS is to add noise in the parameter space, while the combination of action space noise and parameter space noise generally performs poorly. Our code is available at https://github.com/BIT-XJY/RL-based-Transferable-EMS.git.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/5f3b337e74618a2364778222162b13bd55a15e27.pdf",
        "venue": "2022 IEEE Intelligent Vehicles Symposium (IV)",
        "citationCount": 14,
        "score": 4.666666666666666
    },
    "b0c40766974df3eae8ff500379e66e5566cd16c9.pdf": {
        "title": "An Efficient Asynchronous Method for Integrating Evolutionary and Gradient-based Policy Search",
        "authors": [
            "Kyunghyun Lee",
            "Byeong-uk Lee",
            "Ukcheol Shin",
            "In-So Kweon"
        ],
        "published_date": "2020",
        "abstract": "Deep reinforcement learning (DRL) algorithms and evolution strategies (ES) have been applied to various tasks, showing excellent performances. These have the opposite properties, with DRL having good sample efficiency and poor stability, while ES being vice versa. Recently, there have been attempts to combine these algorithms, but these methods fully rely on synchronous update scheme, making it not ideal to maximize the benefits of the parallelism in ES. To solve this challenge, asynchronous update scheme was introduced, which is capable of good time-efficiency and diverse policy exploration. In this paper, we introduce an Asynchronous Evolution Strategy-Reinforcement Learning (AES-RL) that maximizes the parallel efficiency of ES and integrates it with policy gradient methods. Specifically, we propose 1) a novel framework to merge ES and DRL asynchronously and 2) various asynchronous update methods that can take all advantages of asynchronism, ES, and DRL, which are exploration and time efficiency, stability, and sample efficiency, respectively. The proposed framework and update methods are evaluated in continuous control benchmark work, showing superior performance as well as time efficiency compared to the previous methods.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/b0c40766974df3eae8ff500379e66e5566cd16c9.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 23,
        "score": 4.6000000000000005
    },
    "4c4d41a977e6d8d82b6cc19ac28e50de3c814fd8.pdf": {
        "title": "HyperDQN: A Randomized Exploration Method for Deep Reinforcement Learning",
        "authors": [
            "Ziniu Li",
            "Yingru Li",
            "Yushun Zhang",
            "Tong Zhang",
            "Zhimin Luo"
        ],
        "published_date": "2022",
        "abstract": "",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/4c4d41a977e6d8d82b6cc19ac28e50de3c814fd8.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 13,
        "score": 4.333333333333333
    },
    "3d3b0704c61d47c7bafb70ae2670b2786b8e4d81.pdf": {
        "title": "Efficient Exploration in Resource-Restricted Reinforcement Learning",
        "authors": [
            "Zhihai Wang",
            "Taoxing Pan",
            "Qi Zhou",
            "Jie Wang"
        ],
        "published_date": "2022",
        "abstract": "In many real-world applications of reinforcement learning (RL), performing actions requires consuming certain types of resources that are non-replenishable in each episode. Typical applications include robotic control with limited energy and video games with consumable items. In tasks with non-replenishable resources, we observe that popular RL methods such as soft actor critic suffer from poor sample efficiency. The major reason is that, they tend to exhaust resources fast and thus the subsequent exploration is severely restricted due to the absence of resources. To address this challenge, we first formalize the aforementioned problem as a resource-restricted reinforcement learning, and then propose a novel resource-aware exploration bonus (RAEB) to make reasonable usage of resources. An appealing feature of RAEB is that, it can significantly reduce unnecessary resource-consuming trials while effectively encouraging the agent to explore unvisited states. Experiments demonstrate that the proposed RAEB significantly outperforms state-of-the-art exploration strategies in resource-restricted reinforcement learning environments, improving the sample efficiency by up to an order of magnitude.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/3d3b0704c61d47c7bafb70ae2670b2786b8e4d81.pdf",
        "venue": "AAAI Conference on Artificial Intelligence",
        "citationCount": 13,
        "score": 4.333333333333333
    },
    "9e5fe2ba652774ba3b1127f626c192668a907132.pdf": {
        "title": "Decoupled Exploration and Exploitation Policies for Sample-Efficient Reinforcement Learning",
        "authors": [
            "William F. Whitney",
            "Michael Bloesch",
            "Jost Tobias Springenberg",
            "A. Abdolmaleki",
            "Kyunghyun Cho",
            "Martin A. Riedmiller"
        ],
        "published_date": "2021",
        "abstract": "Despite the close connection between exploration and sample efficiency, most state of the art reinforcement learning algorithms include no considerations for exploration beyond maximizing the entropy of the policy. In this work we address this seeming missed opportunity. We observe that the most common formulation of directed exploration in deep RL, known as bonus-based exploration (BBE), suffers from bias and slow coverage in the few-sample regime. This causes BBE to be actively detrimental to policy learning in many control tasks. We show that by decoupling the task policy from the exploration policy, directed exploration can be highly effective for sample-efficient continuous control. Our method, Decoupled Exploration and Exploitation Policies (DEEP), can be combined with any off-policy RL algorithm without modification. When used in conjunction with soft actor-critic, DEEP incurs no performance penalty in densely-rewarding environments. On sparse environments, DEEP gives a several-fold improvement in data efficiency due to better exploration.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/9e5fe2ba652774ba3b1127f626c192668a907132.pdf",
        "venue": "",
        "citationCount": 17,
        "score": 4.25
    },
    "678cd30fac3fcf215d0e714936f6907ecc6d4361.pdf": {
        "title": "Off-Policy Evolutionary Reinforcement Learning with Maximum Mutations",
        "authors": [
            "Karush Suri"
        ],
        "published_date": "2022",
        "abstract": "",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/678cd30fac3fcf215d0e714936f6907ecc6d4361.pdf",
        "venue": "Adaptive Agents and Multi-Agent Systems",
        "citationCount": 12,
        "score": 4.0
    },
    "103f1674121780097f896ffe525bab2c6ae0bcdc.pdf": {
        "title": "Exploration Entropy for Reinforcement Learning",
        "authors": [
            "Bo Xin",
            "Haixu Yu",
            "You Qin",
            "Qing Tang",
            "Zhangqing Zhu"
        ],
        "published_date": "2020",
        "abstract": "The training process analysis and termination condition of the training process of a Reinforcement Learning (RL) system have always been the key issues to train an RL agent. In this paper, a new approach based on State Entropy and Exploration Entropy is proposed to analyse the training process. The concept of State Entropy is used to denote the uncertainty for an RL agent to select the action at every state that the agent will traverse, while the Exploration Entropy denotes the action selection uncertainty of the whole system. Actually, the action selection uncertainty of a certain state or the whole system reflects the degree of exploration and the stage of the learning process for an agent. The Exploration Entropy is a new criterion to analyse and manage the training process of RL. The theoretical analysis and experiment results illustrate that the curve of Exploration Entropy contains more information than the existing analytical methods.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/103f1674121780097f896ffe525bab2c6ae0bcdc.pdf",
        "venue": "",
        "citationCount": 19,
        "score": 3.8000000000000003
    },
    "de93c8aed64229571b03e40b36499d4f07ce875d.pdf": {
        "title": "PBCS : Efficient Exploration and Exploitation Using a Synergy between Reinforcement Learning and Motion Planning",
        "authors": [
            "Guillaume Matheron",
            "Nicolas Perrin",
            "Olivier Sigaud"
        ],
        "published_date": "2020",
        "abstract": "The exploration-exploitation trade-off is at the heart of reinforcement learning (RL). However, most continuous control benchmarks used in recent RL research only require local exploration. This led to the development of algorithms that have basic exploration capabilities, and behave poorly in benchmarks that require more versatile exploration. For instance, as demonstrated in our empirical study, state-of-the-art RL algorithms such as DDPG and TD3 are unable to steer a point mass in even small 2D mazes. In this paper, we propose a new algorithm called \"Plan, Backplay, Chain Skills\" (PBCS) that combines motion planning and reinforcement learning to solve hard exploration environments. In a first phase, a motion planning algorithm is used to find a single good trajectory, then an RL algorithm is trained using a curriculum derived from the trajectory, by combining a variant of the Backplay algorithm and skill chaining. We show that this method outperforms state-of-the-art RL algorithms in 2D maze environments of various sizes, and is able to improve on the trajectory obtained by the motion planning phase.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/de93c8aed64229571b03e40b36499d4f07ce875d.pdf",
        "venue": "International Conference on Artificial Neural Networks",
        "citationCount": 19,
        "score": 3.8000000000000003
    },
    "9d1445f1845a2880ff9c752845660e9c294aa7b5.pdf": {
        "title": "Flow to Control: Offline Reinforcement Learning with Lossless Primitive Discovery",
        "authors": [
            "Yiqin Yang",
            "Haotian Hu",
            "Wenzhe Li",
            "Siyuan Li",
            "Jun Yang",
            "Qianchuan Zhao",
            "Chongjie Zhang"
        ],
        "published_date": "2022",
        "abstract": "Offline reinforcement learning (RL) enables the agent to effectively learn from logged data, which significantly extends the applicability of RL algorithms in real-world scenarios where exploration can be expensive or unsafe. Previous works have shown that extracting primitive skills from the recurring and temporally extended structures in the logged data yields better learning. However, these methods suffer greatly when the primitives have limited representation ability to recover the original policy space, especially in offline settings. In this paper, we give a quantitative characterization of the performance of offline hierarchical learning and highlight the importance of learning lossless primitives. To this end, we propose to use a flow-based structure as the representation for low-level policies. This allows us to represent the behaviors in the dataset faithfully while keeping the expression ability to recover the whole policy space. We show that such lossless primitives can drastically improve the performance of hierarchical policies. The experimental results and extensive ablation studies on the standard D4RL benchmark show that our method has a good representation ability for policies and achieves superior performance in most tasks.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/9d1445f1845a2880ff9c752845660e9c294aa7b5.pdf",
        "venue": "AAAI Conference on Artificial Intelligence",
        "citationCount": 11,
        "score": 3.6666666666666665
    },
    "83f1343500c9f0df62da0d61736738d8d7a9bba0.pdf": {
        "title": "Zero-Shot Policy Transfer with Disentangled Task Representation of Meta-Reinforcement Learning",
        "authors": [
            "Zheng Wu",
            "Yichen Xie",
            "Wenzhao Lian",
            "Changhao Wang",
            "Yanjiang Guo",
            "Jianyu Chen",
            "S. Schaal",
            "M. Tomizuka"
        ],
        "published_date": "2022",
        "abstract": "Humans are capable of abstracting various tasks as different combinations of multiple attributes. This perspective of compositionality is vital for human rapid learning and adaption since previous experiences from related tasks can be combined to generalize across novel compositional settings. In this work, we aim to achieve zero-shot policy generalization of Reinforcement Learning (RL) agents by leveraging the task compositionality. Our proposed method is a meta-RL algorithm with disentangled task representation, explicitly encoding different aspects of the tasks. Policy generalization is then performed by inferring unseen compositional task representations via the obtained disentanglement without extra exploration. The evaluation is conducted on three simulated tasks and a challenging real-world robotic insertion task. Experimental results demonstrate that our proposed method achieves policy generalization to unseen compositional tasks in a zero-shot manner.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/83f1343500c9f0df62da0d61736738d8d7a9bba0.pdf",
        "venue": "IEEE International Conference on Robotics and Automation",
        "citationCount": 11,
        "score": 3.6666666666666665
    },
    "2e1ca97d8f7604e3b334ff4903bfa67267379317.pdf": {
        "title": "The Surprising Effectiveness of Latent World Models for Continual Reinforcement Learning",
        "authors": [
            "Samuel Kessler",
            "Piotr Milo's",
            "Jack Parker-Holder",
            "Stephen J. Roberts"
        ],
        "published_date": "2022",
        "abstract": "",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/2e1ca97d8f7604e3b334ff4903bfa67267379317.pdf",
        "venue": "arXiv.org",
        "citationCount": 11,
        "score": 3.6666666666666665
    },
    "7551b7f7420087de59ad36e445cb3bdef9c382ee.pdf": {
        "title": "Generalized State-Dependent Exploration for Deep Reinforcement Learning in Robotics",
        "authors": [
            "A. Raffin",
            "F. Stulp"
        ],
        "published_date": "2020",
        "abstract": "",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/7551b7f7420087de59ad36e445cb3bdef9c382ee.pdf",
        "venue": "arXiv.org",
        "citationCount": 18,
        "score": 3.6
    },
    "b6ffc15ab55281dcf08b6a19d6f8bfcb190765b8.pdf": {
        "title": "Trust-Region Method with Deep Reinforcement Learning in Analog Design Space Exploration",
        "authors": [
            "Kai-En Yang",
            "Chia-Yu Tsai",
            "Hung-Hao Shen",
            "Chen-Feng Chiang",
            "Feng-Ming Tsai",
            "Chunguang Wang",
            "Yiju Ting",
            "Chia-Shun Yeh",
            "C. Lai"
        ],
        "published_date": "2020",
        "abstract": "This paper introduces new perspectives on analog design space search. To minimize the time-to-market, this endeavor better cast as constraint satisfaction problem than global optimization defined in prior arts. We incorporate model based agents, contrasted with model-free learning, to implement a trust-region strategy. As such, simple feed-forward networks can be trained with supervised learning, where the convergence is relatively trivial. Experiment results demonstrate orders of magnitude improvement on search iterations. Additionally, the unprecedented consideration of PVT conditions are accommodated. On circuits with TSMC 5/6nm process, our method achieve performance surpassing human designers. Furthermore, this framework is in production in industrial settings.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/b6ffc15ab55281dcf08b6a19d6f8bfcb190765b8.pdf",
        "venue": "Design Automation Conference",
        "citationCount": 17,
        "score": 3.4000000000000004
    },
    "06318722ebbac1d9b59e2408ecd3994eadb0ec6b.pdf": {
        "title": "Deep Reinforcement Learning Task Assignment Based on Domain Knowledge",
        "authors": [
            "Jiayi Liu",
            "Gang Wang",
            "Xiangke Guo",
            "Siyuan Wang",
            "Qiang Fu"
        ],
        "published_date": "2022",
        "abstract": "Deep Reinforcement Learning (DRL) methods are inefficient in the initial strategy exploration process due to the huge state space and action space in large-scale complex scenarios. This is becoming one of the bottlenecks in their application to large-scale game adversarial scenarios. This paper proposes a Safe reinforcement learning combined with Imitation learning for Task Assignment (SITA) method for a representative red-blue game confrontation scenario. Aiming at the problem of difficult sampling of Imitation Learning (IL), this paper combines human knowledge with adversarial rules to build a knowledge rule base; We propose the Imitation Learning with the Decoupled Network (ILDN) pre-training method to solve the problem of excessive initial invalid exploration; In order to reduce invalid exploration and improve the stability in the later stages of training, we incorporate Safe Reinforcement Learning (Safe RL) method after pre-training. Finally, we verified in the digital battlefield that the SITA method has higher training efficiency and strong generalization ability in large-scale complex scenarios.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/06318722ebbac1d9b59e2408ecd3994eadb0ec6b.pdf",
        "venue": "IEEE Access",
        "citationCount": 10,
        "score": 3.333333333333333
    },
    "1f577f6f0785c7967bef4ac6d0ab0370c2815b4d.pdf": {
        "title": "Occupancy Reward-Driven Exploration with Deep Reinforcement Learning for Mobile Robot System",
        "authors": [
            "A. Kamalova",
            "Suk-Gyu Lee",
            "Soon-H. Kwon"
        ],
        "published_date": "2022",
        "abstract": "This paper investigates the solution to a mobile-robot exploration problem following autonomous driving principles. The exploration task is formulated in this study as a process of building a map while a robot moves in an indoor environment beginning from full uncertainties. The sequence of robot decisions of how to move defines the strategy of the exploration that this paper aims to investigate, applying one of the Deep Reinforcement Learning methods, known as the Deep Deterministic Policy Gradient (DDPG) algorithm. A custom environment is created representing the mapping process with a map visualization, a robot model, and a reward function. The actor-critic network receives and sends input and output data, respectively, to the custom environment. The input is the data from the laser sensor, which is equipped on the robot. The output is the continuous actions of the robot in terms of linear and angular velocities. The training results of this study show the strengths and weaknesses of the DDPG algorithm for the robotic mapping problem. The implementation was developed in MATLAB platform using its corresponding toolboxes. A comparison with another exploration algorithm is also provided.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/1f577f6f0785c7967bef4ac6d0ab0370c2815b4d.pdf",
        "venue": "Applied Sciences",
        "citationCount": 10,
        "score": 3.333333333333333
    },
    "33e3f13087abd5241d55523140720f5e684b7bee.pdf": {
        "title": "Generative Planning for Temporally Coordinated Exploration in Reinforcement Learning",
        "authors": [
            "Haichao Zhang",
            "Wei Xu",
            "Haonan Yu"
        ],
        "published_date": "2022",
        "abstract": "Standard model-free reinforcement learning algorithms optimize a policy that generates the action to be taken in the current time step in order to maximize expected future return. While flexible, it faces difficulties arising from the inefficient exploration due to its single step nature. In this work, we present Generative Planning method (GPM), which can generate actions not only for the current step, but also for a number of future steps (thus termed as generative planning). This brings several benefits to GPM. Firstly, since GPM is trained by maximizing value, the plans generated from it can be regarded as intentional action sequences for reaching high value regions. GPM can therefore leverage its generated multi-step plans for temporally coordinated exploration towards high value regions, which is potentially more effective than a sequence of actions generated by perturbing each action at single step level, whose consistent movement decays exponentially with the number of exploration steps. Secondly, starting from a crude initial plan generator, GPM can refine it to be adaptive to the task, which, in return, benefits future explorations. This is potentially more effective than commonly used action-repeat strategy, which is non-adaptive in its form of plans. Additionally, since the multi-step plan can be interpreted as the intent of the agent from now to a span of time period into the future, it offers a more informative and intuitive signal for interpretation. Experiments are conducted on several benchmark environments and the results demonstrated its effectiveness compared with several baseline methods.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/33e3f13087abd5241d55523140720f5e684b7bee.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 10,
        "score": 3.333333333333333
    },
    "23e3f39ede3c45cb8cf456de1f56d7a8b8d4bc2e.pdf": {
        "title": "A Decision-Making Strategy for Car Following Based on Naturalist Driving Data via Deep Reinforcement Learning",
        "authors": [
            "Wenli Li",
            "Yousong Zhang",
            "Xiaohui Shi",
            "Fanke Qiu"
        ],
        "published_date": "2022",
        "abstract": "To improve the satisfaction and acceptance of automatic driving, we propose a deep reinforcement learning (DRL)-based autonomous car-following (CF) decision-making strategy using naturalist driving data (NDD). This study examines the traits of CF behavior using 1341 pairs of CF events taken from the Next Generation Simulation (NGSIM) data. Furthermore, in order to improve the random exploration of the agent\u2019s action, the dynamic characteristics of the speed-acceleration distribution are established in accordance with NDD. The action\u2019s varying constraints are achieved via a normal distribution 3\u03c3 boundary point-to-fit curve. A multiobjective reward function is designed considering safety, efficiency, and comfort, according to the time headway (THW) probability density distribution. The introduction of a penalty reward in mechanical energy allows the agent to internalize negative experiences. Next, a model of agent-environment interaction for CF decision-making control is built using the deep deterministic policy gradient (DDPG) method, which can explore complicated environments. Finally, extensive simulation experiments validate the effectiveness and accuracy of our proposal, and the driving strategy is learned through real-world driving data, which is better than human data.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/23e3f39ede3c45cb8cf456de1f56d7a8b8d4bc2e.pdf",
        "venue": "Italian National Conference on Sensors",
        "citationCount": 10,
        "score": 3.333333333333333
    },
    "97fe089acbe9d1f55753cd6b2ad6070e89521f6c.pdf": {
        "title": "Exposing Surveillance Detection Routes via Reinforcement Learning, Attack Graphs, and Cyber Terrain",
        "authors": [
            "Lanxiao Huang",
            "Tyler Cody",
            "Christopher Redino",
            "Abdul Rahman",
            "A. Kakkar",
            "Deepak Kushwaha",
            "Cheng Wang",
            "Ryan Clark",
            "Dan Radke",
            "P. Beling",
            "E. Bowen"
        ],
        "published_date": "2022",
        "abstract": "Reinforcement learning (RL) operating on attack graphs leveraging cyber terrain principles are used to develop reward and state associated with determination of surveillance detection routes (SDR). This work extends previous efforts on developing RL methods for path analysis within enterprise networks. This work focuses on building SDR where the routes focus on exploring the network services while trying to evade risk. RL is utilized to support the development of these routes by building a reward mechanism that would help in realization of these paths. The RL algorithm is modified to have a novel warm-up phase which decides in the initial exploration which areas of the network are safe to explore based on the rewards and penalty scale factor.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/97fe089acbe9d1f55753cd6b2ad6070e89521f6c.pdf",
        "venue": "International Conference on Machine Learning and Applications",
        "citationCount": 10,
        "score": 3.333333333333333
    },
    "6d97b81b3473492cb9986a63886cbb128496010c.pdf": {
        "title": "No-regret Exploration in Contextual Reinforcement Learning",
        "authors": [
            "Aditya Modi",
            "Ambuj Tewari"
        ],
        "published_date": "2019",
        "abstract": "",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/6d97b81b3473492cb9986a63886cbb128496010c.pdf",
        "venue": "Conference on Uncertainty in Artificial Intelligence",
        "citationCount": 19,
        "score": 3.1666666666666665
    },
    "807f377de905eda62e4cd2f0797153a59296adbb.pdf": {
        "title": "Partial Off-policy Learning: Balance Accuracy and Diversity for Human-Oriented Image Captioning",
        "authors": [
            "Jiahe Shi",
            "Yali Li",
            "Shengjin Wang"
        ],
        "published_date": "2021",
        "abstract": "Human-oriented image captioning with both high diversity and accuracy is a challenging task in vision+language modeling. The reinforcement learning (RL) based frameworks promote the accuracy of image captioning, yet seriously hurt the diversity. In contrast, other methods based on variational auto-encoder (VAE) or generative adversarial network (GAN) can produce diverse yet less accurate captions. In this work, we devote our attention to promote the diversity of RL-based image captioning. To be specific, we devise a partial off-policy learning scheme to balance accuracy and diversity. First, we keep the model exposed to varied candidate captions by sampling from the initial state before RL launched. Second, a novel criterion named max-CIDEr is proposed to serve as the reward for promoting diversity. We combine the above-mentioned offpolicy strategy with the on-policy one to moderate the exploration effect, further balancing the diversity and accuracy for human-like image captioning. Experiments show that our method locates the closest to human performance in the diversity-accuracy space, and achieves the highest Pearson correlation as 0.337 with human performance.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/807f377de905eda62e4cd2f0797153a59296adbb.pdf",
        "venue": "IEEE International Conference on Computer Vision",
        "citationCount": 12,
        "score": 3.0
    },
    "f14645d3a0740504ee632ab06f045cceaa5297bc.pdf": {
        "title": "RL-GEP: Symbolic Regression via Gene Expression Programming and Reinforcement Learning",
        "authors": [
            "Hengzhe Zhang",
            "Aimin Zhou"
        ],
        "published_date": "2021",
        "abstract": "Symbolic regression has become a hot topic in recent years due to the surging demand for interpretable machine learning methods. Traditionally, symbolic regression problems are mainly solved by genetic algorithms. Nonetheless, with the development of deep learning, reinforcement learning based symbolic regression methods have received attention gradually. Unfortunately, hardly any of those reinforcement learning based methods have been proven effectively to solve real world regression problems as genetic algorithm based methods. In this paper, we find a general reinforcement learning based symbolic regression method is difficult to solve real world problems since it is hard to balance between exploration and exploitation. To deal with this problem, we propose a hybrid method to use both genetic algorithm and reinforcement learning for solving symbolic regression problems. By doing so, we can combine the advantages of reinforcement learning and genetic algorithm and achieve better performance than using them alone. To validate the effectiveness of the proposed method, we apply the proposed method to ten benchmark datasets. The experimental results show that the proposed method achieves competitive performance compared with several well-known symbolic regression methods on those datasets.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/f14645d3a0740504ee632ab06f045cceaa5297bc.pdf",
        "venue": "IEEE International Joint Conference on Neural Network",
        "citationCount": 12,
        "score": 3.0
    },
    "f715558b65fd4f3c6966505c237d9a622947010b.pdf": {
        "title": "Sample Efficient Reinforcement Learning Method via High Efficient Episodic Memory",
        "authors": [
            "Dujia Yang",
            "Xiaowei Qin",
            "Xiaodong Xu",
            "Chensheng Li",
            "Guo Wei"
        ],
        "published_date": "2020",
        "abstract": "Reinforcement Learning (RL), especially Deep Reinforcement Learning (DRL), has made great progress in many areas, such as robots, video games and driving. However, sample inefficiency is a big obstacle to the widespread practical application of DRL. Inspired by the decision making in human brain, this problem can be solved by incorporating instance based learning, i.e. episodic memory. Many episodic memory based RL algorithms have emerged recently. However, these algorithms either only replace parametric DRL algorithm with episodic control or incorporate episodic memory in a single component of DRL. In contrast to preview works, this paper proposes a new sample-efficient reinforcement learning architecture which introduces a new episodic memory module and incorporates episodic thought into some key components of DRL: exploration, experience replay and loss function. Taking Deep Q-Network (DQN) algorithm for example, when combined with DQN, our algorithm is called High Efficient Episodic Memory DQN (HE-EMDQN). In HE-EMDQN, a new non-parametric episodic memory module is introduced to help calculate the loss and modify the predicted value for exploration. For the sake of accelerating the sample learning in experience replay, an auxiliary small buffer called percentile best episode replay memory is designed to compose a mixed mini-batch. We show across the testing environments that our algorithm is significantly more powerful and sample-efficient than DQN and the recent episodic memory deep q-network (EMDQN). This work provides a new perspective for other RL algorithms to improve sample efficiency by utilising episodic memory efficiently.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/f715558b65fd4f3c6966505c237d9a622947010b.pdf",
        "venue": "IEEE Access",
        "citationCount": 15,
        "score": 3.0
    },
    "e0d4af45fa3073dbfa9b6e464fa88d52e6f06928.pdf": {
        "title": "Energy-Efficient Slithering Gait Exploration for a Snake-like Robot based on Reinforcement Learning",
        "authors": [
            "Zhenshan Bing",
            "Christian Lemke",
            "Zhuangyi Jiang",
            "Kai Huang",
            "A. Knoll"
        ],
        "published_date": "2019",
        "abstract": "Similar to their counterparts in nature, the flexible bodies of snake-like robots enhance their movement capability and adaptability in diverse environments. However, this flexibility corresponds to a complex control task involving highly redundant degrees of freedom, where traditional model-based methods usually fail to propel the robots energy-efficiently. In this work, we present a novel approach for designing an energy-efficient slithering gait for a snake-like robot using a model-free reinforcement learning (RL) algorithm. Specifically, we present an RL-based controller for generating locomotion gaits at a wide range of velocities, which is trained using the proximal policy optimization (PPO) algorithm. Meanwhile, a traditional parameterized gait controller is presented and the parameter sets are optimized using the grid search and Bayesian optimization algorithms for the purposes of reasonable comparisons. Based on the analysis of the simulation results, we demonstrate that this RL-based controller exhibits very natural and adaptive movements, which are also substantially more energy-efficient than the gaits generated by the parameterized controller. Videos are shown at\u00a0https://videoviewsite.wixsite.com/rlsnake\u00a0.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/e0d4af45fa3073dbfa9b6e464fa88d52e6f06928.pdf",
        "venue": "International Joint Conference on Artificial Intelligence",
        "citationCount": 17,
        "score": 2.833333333333333
    },
    "f80432fa03c91283cadbf6c262a7bc6e45f4edd3.pdf": {
        "title": "Discretionary Lane Change Decision Making using Reinforcement Learning with Model-Based Exploration",
        "authors": [
            "Songan Zhang",
            "H. Peng",
            "S. Nageshrao",
            "H. E. Tseng"
        ],
        "published_date": "2019",
        "abstract": "Deep reinforcement learning (DRL) techniques have been used to solve a discretionary lane change decision-making problem and are showing promising results. However, since the input information for the discretionary lane change problem is continuous and can be in high dimension, it is an open challenge for DRL to optimize the exploration-exploitation trade-off. Conventional model-less exploration methods lack a systematic way to incorporate additional engineering or model-based knowledge of our application into consideration and as a result, the training can be inefficient and may dwell on a policy, e.g. lane change strategy that is impractical. In previous related work, many used the rule-based safety check policy to guide the exploration and collect input information data. However, it is not guaranteed to get the optimal policy and the performance is dependent on the safety check policy selected. In this paper, we developed an explicit statistical aggregated environment model using a conditional variational auto-encoder and a model-based exploration strategy leveraging it. The agent is guided to explore with surprise-based intrinsic reward derived from the environment model. The result is compared with annealing epsilon-greedy exploration and with rule-based safety check exploration. We demonstrate that the performance of the developed model-based exploration method is comparable with the best rule-based safety check exploration and much better than the epsilon-greedy exploration.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/f80432fa03c91283cadbf6c262a7bc6e45f4edd3.pdf",
        "venue": "International Conference on Machine Learning and Applications",
        "citationCount": 17,
        "score": 2.833333333333333
    },
    "117cd9e1dafc577e53e2d46897a784ed1e65996f.pdf": {
        "title": "Adaptive Exploration Strategy With Multi-Attribute Decision-Making for Reinforcement Learning",
        "authors": [
            "Chunyang Hu",
            "Meng Xu"
        ],
        "published_date": "2020",
        "abstract": "Reinforcement Learning (RL) agents often encounter the bottleneck of the performance when the dilemma of exploration and exploitation arises. In this study, an adaptive exploration strategy with multi-attribute decision-making is proposed to address the trade-off problem between exploration and exploitation. Firstly, the proposed method decomposes a complex task into several sub-tasks and trains each sub-task using the same training method individually. Then, the proposed method uses a multi-attribute decision-making method to develop an action policy integrating the training results of these trained sub-tasks. There are practical advantages to improve learning performance by allowing multiple learners to learn in parallel. An adaptive exploration strategy determines the probability of exploration depending on the information entropy instead of the suffocating work of empirical tuning. Finally, transfer learning extends the applicability of the proposed method. The experiment of the robot migration, the robot confrontation, and the real wheeled mobile robot are used to demonstrate the availability and practicability of the proposed method.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/117cd9e1dafc577e53e2d46897a784ed1e65996f.pdf",
        "venue": "IEEE Access",
        "citationCount": 14,
        "score": 2.8000000000000003
    },
    "46cbd8acfacca5fc74b8d05bb06264aff0d82a4e.pdf": {
        "title": "Graph-based Cluttered Scene Generation and Interactive Exploration using Deep Reinforcement Learning",
        "authors": [
            "K. N. Kumar",
            "Irfan Essa",
            "Sehoon Ha"
        ],
        "published_date": "2021",
        "abstract": "We introduce a novel method to teach a robotic agent to interactively explore cluttered yet structured scenes, such as kitchen pantries and grocery shelves, by leveraging the physical plausibility of the scene. We propose a novel learning framework to train an effective scene exploration policy to discover hidden objects with minimal interactions. First, we define a novel scene grammar to represent structured clutter. Then we train a Graph Neural Network (GNN) based Scene Generation agent using deep reinforcement learning (deep RL), to manipulate this Scene Grammar to create a diverse set of stable scenes, each containing multiple hidden objects. Given such cluttered scenes, we then train a Scene Exploration agent, using deep RL, to uncover hidden objects by interactively rearranging the scene. We show that our learned agents hide and discover significantly more objects than the baselines. We present quantitative results that prove the generalization capabilities of our agents. We also demonstrate sim-to-real transfer by successfully deploying the learned policy on a real UR10 robot to explore real-world cluttered scenes. The supplemental video can be found at: https://www.youtube.com/watch?v=T2Jo7wwaXss.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/46cbd8acfacca5fc74b8d05bb06264aff0d82a4e.pdf",
        "venue": "IEEE International Conference on Robotics and Automation",
        "citationCount": 11,
        "score": 2.75
    },
    "48de0dab9255ededce3cdaeac84f039d726f7f3e.pdf": {
        "title": "Controller exploitation-exploration reinforcement learning architecture for computing near-optimal policies",
        "authors": [
            "Erick Asiain",
            "J. Clempner",
            "A. Poznyak"
        ],
        "published_date": "2018",
        "abstract": "",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/48de0dab9255ededce3cdaeac84f039d726f7f3e.pdf",
        "venue": "Soft Computing - A Fusion of Foundations, Methodologies and Applications",
        "citationCount": 19,
        "score": 2.714285714285714
    },
    "071de005741bd666c7a9ccf40d5ed1d502f5282b.pdf": {
        "title": "Curiosity-Driven Exploration for Off-Policy Reinforcement Learning Methods*",
        "authors": [
            "Boyao Li",
            "Tao Lu",
            "Jiayi Li",
            "N. Lu",
            "Yinghao Cai",
            "Shuo Wang"
        ],
        "published_date": "2019",
        "abstract": "Deep reinforcement learning (DRL) has achieved remarkable results in many high-dimensional continuous control tasks. However, the RL agent still explores the environment randomly, resulting in low exploration efficiency and learning performance, especially in robotic manipulation tasks with sparse rewards. To address this problem, in this paper, we intro-duce a simplified Intrinsic Curiosity Module (S-ICM) into the off-policy RL methods to encourage the agent to pursue novel and surprising states for improving the exploration competence. This method can be combined with an arbitrary off-policy RL algorithm. We evaluate our approach on three challenging robotic manipulation tasks provided by OpenAI Gym. In our experiments, we combined our method with Deep Deterministic Policy Gradient (DDPG) with and without Hindsight Experience Replay (HER). The empirical results show that our proposed method significantly outperforms vanilla RL algorithms both in sample-efficiency and learning performance.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/071de005741bd666c7a9ccf40d5ed1d502f5282b.pdf",
        "venue": "IEEE International Conference on Robotics and Biomimetics",
        "citationCount": 14,
        "score": 2.333333333333333
    },
    "3f673101c2cac3b47639056e2988e018546c3c90.pdf": {
        "title": "Zeroth-Order Supervised Policy Improvement",
        "authors": [
            "Hao Sun",
            "Ziping Xu",
            "Yuhang Song",
            "Meng Fang",
            "Jiechao Xiong",
            "Bo Dai",
            "Zhengyou Zhang",
            "Bolei Zhou"
        ],
        "published_date": "2020",
        "abstract": "Despite the remarkable progress made by the policy gradient algorithms in reinforcement learning (RL), sub-optimal policies usually result from the local exploration property of the policy gradient update. In this work, we propose a method referred to as Zeroth-Order Supervised Policy Improvement (ZOSPI) that exploits the estimated value function Q globally while preserves the local exploitation of the policy gradient methods. We prove that with a good function structure, the zeroth-order optimization strategy combining both local and global samplings can find the global minima within a polynomial number of samples. To improve the exploration efficiency in unknown environments, ZOSPI is further combined with bootstrapped Q networks. Different from the standard policy gradient methods, the policy learning of ZOSPI is conducted in a self-supervision manner so that the policy can be implemented with gradient-free non-parametric models besides the neural network approximator. Experiments show that ZOSPI achieves competitive results on MuJoCo locomotion tasks with a remarkable sample efficiency.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/3f673101c2cac3b47639056e2988e018546c3c90.pdf",
        "venue": "arXiv.org",
        "citationCount": 10,
        "score": 2.0
    },
    "24107405a96a53d4c292b08608300a6c7e457ffe.pdf": {
        "title": "Stylistic Dialogue Generation via Information-Guided Reinforcement Learning Strategy",
        "authors": [
            "Yixuan Su",
            "Deng Cai",
            "Yan Wang",
            "Simon Baker",
            "A. Korhonen",
            "Nigel Collier",
            "Xiaojiang Liu"
        ],
        "published_date": "2020",
        "abstract": "Stylistic response generation is crucial for building an engaging dialogue system for industrial use. While it has attracted much research interest, existing methods often generate stylistic responses at the cost of the content quality (relevance and fluency). To enable better balance between the content quality and the style, we introduce a new training strategy, know as Information-Guided Reinforcement Learning (IG-RL). In IG-RL, a training model is encouraged to explore stylistic expressions while being constrained to maintain its content quality. This is achieved by adopting reinforcement learning strategy with statistical style information guidance for quality-preserving explorations. Experiments on two datasets show that the proposed approach outperforms several strong baselines in terms of the overall response performance.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/24107405a96a53d4c292b08608300a6c7e457ffe.pdf",
        "venue": "arXiv.org",
        "citationCount": 10,
        "score": 2.0
    },
    "57f2811d8d8da81f2b4ed80097f5e47a49d76d19.pdf": {
        "title": "WRFMR: A Multi-Agent Reinforcement Learning Method for Cooperative Tasks",
        "authors": [
            "Hui Liu",
            "Zhen Zhang",
            "Dongqing Wang"
        ],
        "published_date": "2020",
        "abstract": "Multi-agent reinforcement learning (MARL) for cooperative tasks has been extensively studied in recent years. The balance of exploration and exploitation is crucial to MARL algorithms\u2019 performance in terms of the learning speed and the quality of the obtained strategy. To this end, we propose an algorithm known as the weighted relative frequency of obtaining the maximal reward (WRFMR), which uses a weight parameter and the action probability to balance exploration and exploitation and accelerate convergence to the optimal joint action. For the WRFMR algorithm, each agent needs to share the state and the immediate reward and does not need to observe the actions of the other agents. Theoretical analysis on the model of WRFMR in cooperative repeated games shows that each optimal joint action is an asymptotically stable critical point if the component action of every optimal joint action is unique. The box-pushing task, the distributed sensor network (DSN) task, and a strategy game known as blood battlefield are used for empirical studies. Both the DSN task and the box-pushing task involve full cooperation, while blood battle comprises both cooperation and competition. The simulation results show that the WRFMR algorithm outperforms the other algorithms regarding the success rate and the learning speed.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/57f2811d8d8da81f2b4ed80097f5e47a49d76d19.pdf",
        "venue": "IEEE Access",
        "citationCount": 10,
        "score": 2.0
    },
    "bd38cbbb346a347cb5b60ac4a133b3d73cb44e07.pdf": {
        "title": "Efficient Online Reinforcement Learning with Offline Data",
        "authors": [
            "Philip J. Ball",
            "Laura M. Smith",
            "Ilya Kostrikov",
            "S. Levine"
        ],
        "published_date": "2023",
        "abstract": "Sample efficiency and exploration remain major challenges in online reinforcement learning (RL). A powerful approach that can be applied to address these issues is the inclusion of offline data, such as prior trajectories from a human expert or a sub-optimal exploration policy. Previous methods have relied on extensive modifications and additional complexity to ensure the effective use of this data. Instead, we ask: can we simply apply existing off-policy methods to leverage offline data when learning online? In this work, we demonstrate that the answer is yes; however, a set of minimal but important changes to existing off-policy RL algorithms are required to achieve reliable performance. We extensively ablate these design choices, demonstrating the key factors that most affect performance, and arrive at a set of recommendations that practitioners can readily apply, whether their data comprise a small number of expert demonstrations or large volumes of sub-optimal trajectories. We see that correct application of these simple recommendations can provide a $\\mathbf{2.5\\times}$ improvement over existing approaches across a diverse set of competitive benchmarks, with no additional computational overhead. We have released our code at https://github.com/ikostrikov/rlpd.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/bd38cbbb346a347cb5b60ac4a133b3d73cb44e07.pdf",
        "venue": "International Conference on Machine Learning",
        "citationCount": 220,
        "score": 110.0
    },
    "1b1efa2f9731ab3801c46bfc877695d41e437406.pdf": {
        "title": "An Online Reinforcement Learning-Based Energy Management Strategy for Microgrids With Centralized Control",
        "authors": [
            "Qing-ran Meng",
            "Sheharyar Hussain",
            "Fengzhang Luo",
            "Zhongguan Wang",
            "Xiaolong Jin"
        ],
        "published_date": "2025",
        "abstract": "To address the issue of significant unpredictability and intermittent nature of renewable energy sources, particularly wind and solar power, this paper introduces a novel optimization model based on online reinforcement learning. Initially, an energy management optimization model is designed to achieve plan adherence and minimize energy storage (ES) operation costs, taking into account the inherent challenges of wind power-photovoltaic energy storage systems (WPESS). An online reinforcement learning framework is employed, which defines various state variables, action variables, and reward functions for the energy management optimization model. The state-action-reward-state-action (SARSA) algorithm is applied to learn the joint scheduling strategy for the microgrid system, utilizing its iterative exploration mechanisms and interaction with the environment. This strategy aims to accomplish the goals of effective power tracking and reduction of storage charging and discharging. The proposed method's effectiveness is validated using a residential community with electric vehicle (EV) charging loads as a test case. Numerical analyses demonstrate that the approach is not reliant on traditional mathematical models and adapts well to the uncertainties and complex constraints of the WPESS, maintaining a low-cost requirement while achieving computational efficiency significantly higher than that of the model predictive control (MPC) and deep Q-network (DQN) algorithm.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/1b1efa2f9731ab3801c46bfc877695d41e437406.pdf",
        "venue": "IEEE transactions on industry applications",
        "citationCount": 62,
        "score": 62.0
    },
    "08e84c939b88fc50aaa74ef76e202e61a1ad940b.pdf": {
        "title": "StepCoder: Improve Code Generation with Reinforcement Learning from Compiler Feedback",
        "authors": [
            "Shihan Dou",
            "Yan Liu",
            "Haoxiang Jia",
            "Limao Xiong",
            "Enyu Zhou",
            "Junjie Shan",
            "Caishuang Huang",
            "Wei Shen",
            "Xiaoran Fan",
            "Zhiheng Xi",
            "Yuhao Zhou",
            "Tao Ji",
            "Rui Zheng",
            "Qi Zhang",
            "Xuanjing Huang",
            "Tao Gui"
        ],
        "published_date": "2024",
        "abstract": "The advancement of large language models (LLMs) has significantly propelled the field of code generation. Previous work integrated reinforcement learning (RL) with compiler feedback for exploring the output space of LLMs to enhance code generation quality. However, the lengthy code generated by LLMs in response to complex human requirements makes RL exploration a challenge. Also, since the unit tests may not cover the complicated code, optimizing LLMs by using these unexecuted code snippets is ineffective. To tackle these challenges, we introduce StepCoder, a novel RL framework for code generation, consisting of two main components: CCCS addresses the exploration challenge by breaking the long sequences code generation task into a Curriculum of Code Completion Subtasks, while FGO only optimizes the model by masking the unexecuted code segments to provide Fine-Grained Optimization. In addition, we furthermore construct the APPS+ dataset for RL training, which is manually verified to ensure the correctness of unit tests. Experimental results show that our method improves the ability to explore the output space and outperforms state-of-the-art approaches in corresponding benchmarks. Our dataset APPS+ and StepCoder are available online.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/08e84c939b88fc50aaa74ef76e202e61a1ad940b.pdf",
        "venue": "arXiv.org",
        "citationCount": 61,
        "score": 61.0
    },
    "5bac7d00035bc1e246a34f9ee3152b290f97bb92.pdf": {
        "title": "Supervised Pretraining Can Learn In-Context Reinforcement Learning",
        "authors": [
            "Jonathan Lee",
            "Annie Xie",
            "Aldo Pacchiano",
            "Yash Chandak",
            "Chelsea Finn",
            "Ofir Nachum",
            "E. Brunskill"
        ],
        "published_date": "2023",
        "abstract": "Large transformer models trained on diverse datasets have shown a remarkable ability to learn in-context, achieving high few-shot performance on tasks they were not explicitly trained to solve. In this paper, we study the in-context learning capabilities of transformers in decision-making problems, i.e., reinforcement learning (RL) for bandits and Markov decision processes. To do so, we introduce and study Decision-Pretrained Transformer (DPT), a supervised pretraining method where the transformer predicts an optimal action given a query state and an in-context dataset of interactions, across a diverse set of tasks. This procedure, while simple, produces a model with several surprising capabilities. We find that the pretrained transformer can be used to solve a range of RL problems in-context, exhibiting both exploration online and conservatism offline, despite not being explicitly trained to do so. The model also generalizes beyond the pretraining distribution to new tasks and automatically adapts its decision-making strategies to unknown structure. Theoretically, we show DPT can be viewed as an efficient implementation of Bayesian posterior sampling, a provably sample-efficient RL algorithm. We further leverage this connection to provide guarantees on the regret of the in-context algorithm yielded by DPT, and prove that it can learn faster than algorithms used to generate the pretraining data. These results suggest a promising yet simple path towards instilling strong in-context decision-making abilities in transformers.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/5bac7d00035bc1e246a34f9ee3152b290f97bb92.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 110,
        "score": 55.0
    },
    "043582a1ed2d2b7d08a804bafe9db188e9a65d96.pdf": {
        "title": "Neural Network Model-Based Reinforcement Learning Control for AUV 3-D Path Following",
        "authors": [
            "D. Ma",
            "Xi Chen",
            "Weihao Ma",
            "Huarong Zheng",
            "Fengzhong Qu"
        ],
        "published_date": "2024",
        "abstract": "Autonomous underwater vehicles (AUVs) have become important tools in the ocean exploration and have drawn considerable attention. Precise control for AUVs is the prerequisite to effectively execute underwater tasks. However, the classical control methods such as model predictive control (MPC) rely heavily on the dynamics model of the controlled system which is difficult to obtain for AUVs. To address this issue, a new reinforcement learning (RL) framework for AUV path-following control is proposed in this article. Specifically, we propose a novel actor-model-critic (AMC) architecture integrating a neural network model with the traditional actor-critic architecture. The neural network model is designed to learn the state transition function to explore the spatio-temporal change patterns of the AUV as well as the surrounding environment. Based on the AMC architecture, a RL-based controller agent named ModelPPO is constructed to control the AUV. With the required sailing speed achieved by a traditional proportional-integral (PI) controller, ModelPPO can control the rudder and elevator fins so that the AUV follows the desired path. Finally, a simulation platform is built to evaluate the performance of the proposed method that is compared with MPC and other RL-based methods. The obtained results demonstrate that the proposed method can achieve better performance than other methods, which demonstrate the great potential of the advanced artificial intelligence methods in solving the traditional motion control problems for intelligent vehicles.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/043582a1ed2d2b7d08a804bafe9db188e9a65d96.pdf",
        "venue": "IEEE Transactions on Intelligent Vehicles",
        "citationCount": 51,
        "score": 51.0
    },
    "139a84c6fc4887ce2374489d79af0df9e1e7e4d6.pdf": {
        "title": "Craftax: A Lightning-Fast Benchmark for Open-Ended Reinforcement Learning",
        "authors": [
            "Michael Matthews",
            "Michael Beukman",
            "Benjamin Ellis",
            "Mikayel Samvelyan",
            "Matthew Jackson",
            "Samuel Coward",
            "Jakob Foerster"
        ],
        "published_date": "2024",
        "abstract": "Benchmarks play a crucial role in the development and analysis of reinforcement learning (RL) algorithms. We identify that existing benchmarks used for research into open-ended learning fall into one of two categories. Either they are too slow for meaningful research to be performed without enormous computational resources, like Crafter, NetHack and Minecraft, or they are not complex enough to pose a significant challenge, like Minigrid and Procgen. To remedy this, we first present Craftax-Classic: a ground-up rewrite of Crafter in JAX that runs up to 250x faster than the Python-native original. A run of PPO using 1 billion environment interactions finishes in under an hour using only a single GPU and averages 90% of the optimal reward. To provide a more compelling challenge we present the main Craftax benchmark, a significant extension of the Crafter mechanics with elements inspired from NetHack. Solving Craftax requires deep exploration, long term planning and memory, as well as continual adaptation to novel situations as more of the world is discovered. We show that existing methods including global and episodic exploration, as well as unsupervised environment design fail to make material progress on the benchmark. We believe that Craftax can for the first time allow researchers to experiment in a complex, open-ended environment with limited computational resources.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/139a84c6fc4887ce2374489d79af0df9e1e7e4d6.pdf",
        "venue": "International Conference on Machine Learning",
        "citationCount": 47,
        "score": 47.0
    },
    "26662adf92cacf0810a14faa514360f270e97b53.pdf": {
        "title": "A Lightweight Reinforcement-Learning-Based Real-Time Path-Planning Method for Unmanned Aerial Vehicles",
        "authors": [
            "Meng Xi",
            "Huiao Dai",
            "Jingyi He",
            "Wenjie Li",
            "Jiabao Wen",
            "Shuai Xiao",
            "Jiachen Yang"
        ],
        "published_date": "2024",
        "abstract": "The unmanned aerial vehicles (UAVs) are competent to perform a variety of applications, possessing great potential and promise. The deep neural networks (DNN) technology has enabled the UAV-assisted paradigm, accelerated the construction of smart cities, and propelled the development of the Internet of Things (IoT). UAVs play an increasingly important role in various applications, such as surveillance, environmental monitoring, emergency rescue, and supplies delivery, for which a robust path-planning technique is the foundation and prerequisite. However, existing methods lack comprehensive consideration of the complicated urban environment and do not provide an overall assessment of the robustness and generalization. Meanwhile, due to the resource constraints and hardware limitations of UAVs, the complexity of deploying the network needs to be reduced. This article proposes a lightweight, reinforcement-learning-based (RL-based) real-time path-planning method for UAVs, adaptive SAC (ASAC) algorithm, which optimizing the training process, network architecture, and algorithmic models. First of all, we establish a framework of global training and local adaptation, where the structured environment model is constructed for interaction, and local dynamically varying information aids in improving generalization. Second, ASAC introduces a cross-layer connection approach that passes the original state information into the higher layers to avoid feature loss and improve learning efficiency. Finally, we propose an adaptive temperature coefficient, which flexibly adjusts the exploration probability of UAVs with the training phase and experience data accumulation. In addition, a series of comparison experiments have been conducted in conjunction with practical application requirements, and the results have fully proved the favorable superiority of ASAC.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/26662adf92cacf0810a14faa514360f270e97b53.pdf",
        "venue": "IEEE Internet of Things Journal",
        "citationCount": 44,
        "score": 44.0
    },
    "914eaadede7a95116362cd6982321f93044b3b19.pdf": {
        "title": "Multi-USV Task Planning Method Based on Improved Deep Reinforcement Learning",
        "authors": [
            "Jing Zhang",
            "Jian-Lin Ren",
            "Yani Cui",
            "Delong Fu",
            "Jingyu Cong"
        ],
        "published_date": "2024",
        "abstract": "A safe and reliable task planning method is a prerequisite for the collaborative execution of ocean observation data collection tasks by multiple unmanned surface vessels (multi-USVs). Deep reinforcement learning (DRL) combines the powerful nonlinear function-fitting capabilities of deep neural networks with the decision-making and control abilities of reinforcement learning, providing a novel approach to solving the multi-USV task planning problem. However, when applied to the field of multi-USV task planning, it faces challenges, such as a vast exploration space, extended training times, and unstable training process. To this end, this article proposes a multi-USV task planning method based on improved DRL. The proposed method draws on the idea of a value decomposition network, breaking down the multi-USV task planning problem into two subproblems: 1) task allocation and 2) autonomous collision avoidance. Different state spaces, action spaces, and reward functions are designed for the various subproblems. Based on this, a deep neural network is used to map the state space of each subproblem to the action space of each USV, and the generated strategy of the deep neural network is assessed based on the corresponding reward function. This successfully integrates task allocation and path planning into a comprehensive task planning framework. Deep neural networks consist of the Actor networks and the Critic networks. During the training phase of the Critic network, different methods are used to train different Critic networks to improve the convergence speed of the algorithm. An improved temporal difference error method is specifically applied to train the Critic network for evaluating autonomous collision avoidance strategies, resulting in improving the autonomous collision avoidance ability of USVs. At the same time, to improve the efficiency of task allocation, hierarchical mechanisms and regional division mechanisms are introduced to construct subsystem task planning models, which further decompose the task planning problem. A combination of successor features and an improved temporal difference error method is specifically applied to train another Critic network for evaluating the subsystem\u2019s task allocation schemes and collaborative motion trajectories, aiming to enhance the allocation efficiency of the subsystems. Furthermore, transfer learning is employed to merge the subsystem task planning, using it as a constraint to direct the exploration and assessment of both the cluster task allocation schemes and the cluster collaborative motion trajectories. This enables rapid and accurate learning for task allocation within the multi-USV cluster. During the training phase of the Actor network, the introduction of the experience replay method and target network technique is employed to enhance the proximal policy optimization algorithm. This facilitates distributed joint training of the Actor network, thereby improving the accuracy of the algorithm. Simulation results validate the effectiveness and superiority of this method.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/914eaadede7a95116362cd6982321f93044b3b19.pdf",
        "venue": "IEEE Internet of Things Journal",
        "citationCount": 41,
        "score": 41.0
    },
    "7d6168fbd3ed72f9098573007f4b8c2ec9e576b9.pdf": {
        "title": "Training Large Language Models for Reasoning through Reverse Curriculum Reinforcement Learning",
        "authors": [
            "Zhiheng Xi",
            "Wenxiang Chen",
            "Boyang Hong",
            "Senjie Jin",
            "Rui Zheng",
            "Wei He",
            "Yiwen Ding",
            "Shichun Liu",
            "Xin Guo",
            "Junzhe Wang",
            "Honglin Guo",
            "Wei Shen",
            "Xiaoran Fan",
            "Yuhao Zhou",
            "Shihan Dou",
            "Xiao Wang",
            "Xinbo Zhang",
            "Peng Sun",
            "Tao Gui",
            "Qi Zhang",
            "Xuanjing Huang"
        ],
        "published_date": "2024",
        "abstract": "In this paper, we propose R$^3$: Learning Reasoning through Reverse Curriculum Reinforcement Learning (RL), a novel method that employs only outcome supervision to achieve the benefits of process supervision for large language models. The core challenge in applying RL to complex reasoning is to identify a sequence of actions that result in positive rewards and provide appropriate supervision for optimization. Outcome supervision provides sparse rewards for final results without identifying error locations, whereas process supervision offers step-wise rewards but requires extensive manual annotation. R$^3$ overcomes these limitations by learning from correct demonstrations. Specifically, R$^3$ progressively slides the start state of reasoning from a demonstration's end to its beginning, facilitating easier model exploration at all stages. Thus, R$^3$ establishes a step-wise curriculum, allowing outcome supervision to offer step-level signals and precisely pinpoint errors. Using Llama2-7B, our method surpasses RL baseline on eight reasoning tasks by $4.1$ points on average. Notebaly, in program-based reasoning on GSM8K, it exceeds the baseline by $4.2$ points across three backbone models, and without any extra data, Codellama-7B + R$^3$ performs comparable to larger models or closed-source models.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/7d6168fbd3ed72f9098573007f4b8c2ec9e576b9.pdf",
        "venue": "International Conference on Machine Learning",
        "citationCount": 39,
        "score": 39.0
    },
    "a694a3fb49f3c6e63e2fa5ddb72fe3aa490592f1.pdf": {
        "title": "Toward Enhanced Reinforcement Learning-Based Resource Management via Digital Twin: Opportunities, Applications, and Challenges",
        "authors": [
            "Nan Cheng",
            "Xiucheng Wang",
            "Zan Li",
            "Zhisheng Yin",
            "Tom H. Luan",
            "Xuemin Shen"
        ],
        "published_date": "2024",
        "abstract": "This article presents a digital twin (DT)-enhanced reinforcement learning (RL) framework aimed at optimizing performance and reliability in network resource management, since the traditional RL methods face several unified challenges when applied to physical networks, including limited exploration efficiency, slow convergence, poor long-term performance, and safety concerns during the exploration phase. To deal with the above challenges, a comprehensive DT-based framework is proposed to enhance the convergence speed and performance for unified RL-based resource management. The proposed framework provides safe action exploration, more accurate estimates of long-term returns, faster training convergence, higher convergence performance, and real-time adaptation to varying network conditions. Then, two case studies on ultra-reliable and low-latency communication (URLLC) services and multiple unmanned aerial vehicles (UAV) network are presented, demonstrating improvements of the proposed framework in performance, convergence speed, and training cost reduction both on traditional RL and neural network based Deep RL (DRL). Finally, the article identifies and explores some of the research challenges and open issues in this rapidly evolving field.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/a694a3fb49f3c6e63e2fa5ddb72fe3aa490592f1.pdf",
        "venue": "IEEE Network",
        "citationCount": 21,
        "score": 21.0
    },
    "25db1b77bc330476c3cf6ce43236404c578b4372.pdf": {
        "title": "Model-Bellman Inconsistency for Model-based Offline Reinforcement Learning",
        "authors": [
            "Yihao Sun",
            "Jiajin Zhang",
            "Chengxing Jia",
            "Hao-Chu Lin",
            "Junyin Ye",
            "Yangze Yu"
        ],
        "published_date": "2023",
        "abstract": "",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/25db1b77bc330476c3cf6ce43236404c578b4372.pdf",
        "venue": "International Conference on Machine Learning",
        "citationCount": 42,
        "score": 21.0
    },
    "4e98282f5f3f1a388b8d95380473d4ef4878266e.pdf": {
        "title": "DrM: Mastering Visual Reinforcement Learning through Dormant Ratio Minimization",
        "authors": [
            "Guowei Xu",
            "Ruijie Zheng",
            "Yongyuan Liang",
            "Xiyao Wang",
            "Zhecheng Yuan",
            "Tianying Ji",
            "Yu Luo",
            "Xiaoyu Liu",
            "Jiaxin Yuan",
            "Pu Hua",
            "Shuzhen Li",
            "Yanjie Ze",
            "Hal Daum'e",
            "Furong Huang",
            "Huazhe Xu"
        ],
        "published_date": "2023",
        "abstract": "Visual reinforcement learning (RL) has shown promise in continuous control tasks. Despite its progress, current algorithms are still unsatisfactory in virtually every aspect of the performance such as sample efficiency, asymptotic performance, and their robustness to the choice of random seeds. In this paper, we identify a major shortcoming in existing visual RL methods that is the agents often exhibit sustained inactivity during early training, thereby limiting their ability to explore effectively. Expanding upon this crucial observation, we additionally unveil a significant correlation between the agents' inclination towards motorically inactive exploration and the absence of neuronal activity within their policy networks. To quantify this inactivity, we adopt dormant ratio as a metric to measure inactivity in the RL agent's network. Empirically, we also recognize that the dormant ratio can act as a standalone indicator of an agent's activity level, regardless of the received reward signals. Leveraging the aforementioned insights, we introduce DrM, a method that uses three core mechanisms to guide agents' exploration-exploitation trade-offs by actively minimizing the dormant ratio. Experiments demonstrate that DrM achieves significant improvements in sample efficiency and asymptotic performance with no broken seeds (76 seeds in total) across three continuous control benchmark environments, including DeepMind Control Suite, MetaWorld, and Adroit. Most importantly, DrM is the first model-free algorithm that consistently solves tasks in both the Dog and Manipulator domains from the DeepMind Control Suite as well as three dexterous hand manipulation tasks without demonstrations in Adroit, all based on pixel observations.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/4e98282f5f3f1a388b8d95380473d4ef4878266e.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 39,
        "score": 19.5
    },
    "abeb46288d537f98f76b979040a547ee81216377.pdf": {
        "title": "R1-Reward: Training Multimodal Reward Model Through Stable Reinforcement Learning",
        "authors": [
            "Yi-Fan Zhang",
            "Xingyu Lu",
            "Xiao Hu",
            "Chaoyou Fu",
            "Bin Wen",
            "Tianke Zhang",
            "Changyi Liu",
            "Kaiyu Jiang",
            "Kaibing Chen",
            "Kaiyu Tang",
            "Haojie Ding",
            "Jiankang Chen",
            "Fan Yang",
            "Zhang Zhang",
            "Tingting Gao",
            "Liang Wang"
        ],
        "published_date": "2025",
        "abstract": "Multimodal Reward Models (MRMs) play a crucial role in enhancing the performance of Multimodal Large Language Models (MLLMs). While recent advancements have primarily focused on improving the model structure and training data of MRMs, there has been limited exploration into the effectiveness of long-term reasoning capabilities for reward modeling and how to activate these capabilities in MRMs. In this paper, we explore how Reinforcement Learning (RL) can be used to improve reward modeling. Specifically, we reformulate the reward modeling problem as a rule-based RL task. However, we observe that directly applying existing RL algorithms, such as Reinforce++, to reward modeling often leads to training instability or even collapse due to the inherent limitations of these algorithms. To address this issue, we propose the StableReinforce algorithm, which refines the training loss, advantage estimation strategy, and reward design of existing RL methods. These refinements result in more stable training dynamics and superior performance. To facilitate MRM training, we collect 200K preference data from diverse datasets. Our reward model, R1-Reward, trained using the StableReinforce algorithm on this dataset, significantly improves performance on multimodal reward modeling benchmarks. Compared to previous SOTA models, R1-Reward achieves a $8.4\\%$ improvement on the VL Reward-Bench and a $14.3\\%$ improvement on the Multimodal Reward Bench. Moreover, with more inference compute, R1-Reward's performance is further enhanced, highlighting the potential of RL algorithms in optimizing MRMs.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/abeb46288d537f98f76b979040a547ee81216377.pdf",
        "venue": "arXiv.org",
        "citationCount": 19,
        "score": 19.0
    },
    "4a3e88d203564e547f5fb3f3d816a0b381492eae.pdf": {
        "title": "VLA-RL: Towards Masterful and General Robotic Manipulation with Scalable Reinforcement Learning",
        "authors": [
            "Guanxing Lu",
            "Wenkai Guo",
            "Chubin Zhang",
            "Yuheng Zhou",
            "Hao Jiang",
            "Zifeng Gao",
            "Yansong Tang",
            "Ziwei Wang"
        ],
        "published_date": "2025",
        "abstract": "Recent high-capacity vision-language-action (VLA) models have demonstrated impressive performance on a range of robotic manipulation tasks by imitating human demonstrations. However, exploiting offline data with limited visited states will cause execution failure in out-of-distribution scenarios. Intuitively, an exploration-based method that improves on online collected data at test time could address this limitation. We present VLA-RL, an algorithmic and systematic framework that leverages online reinforcement learning (RL) to improve pretrained auto-regressive VLAs in downstream tasks. Within a unified perspective, we first introduce a trajectory-level RL formulation for auto-regressive VLA training, which models general robotic manipulation trajectory as multi-modal multi-turn conversation. To address the challenge of sparse rewards, we fine-tune a pretrained vision-language model as a robotic process reward model, which is trained on pseudo reward labels annotated on automatically extracted task segments. To scale up, we identify several implementation findings that improve the stability and efficiency including curriculum selection strategy, GPU-balanced vectorized environments, batch decoding, and critic warmup. VLA-RL enables OpenVLA-7B to surpass the strongest finetuned baseline by 4.5% on 40 challenging robotic manipulation tasks in LIBERO, and even matches the performance of advanced commercial models such as $\\pi_0$-FAST. Notably, we observe that VLA-RL benefits from increased test-time optimization, indicating an early spark of inference scaling laws in robotics.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/4a3e88d203564e547f5fb3f3d816a0b381492eae.pdf",
        "venue": "arXiv.org",
        "citationCount": 18,
        "score": 18.0
    },
    "1a73038804052a40c12aae696848ece2168f6da7.pdf": {
        "title": "On the Importance of Exploration for Generalization in Reinforcement Learning",
        "authors": [
            "Yiding Jiang",
            "J. Z. Kolter",
            "R. Raileanu"
        ],
        "published_date": "2023",
        "abstract": "Existing approaches for improving generalization in deep reinforcement learning (RL) have mostly focused on representation learning, neglecting RL-specific aspects such as exploration. We hypothesize that the agent's exploration strategy plays a key role in its ability to generalize to new environments. Through a series of experiments in a tabular contextual MDP, we show that exploration is helpful not only for efficiently finding the optimal policy for the training environments but also for acquiring knowledge that helps decision making in unseen environments. Based on these observations, we propose EDE: Exploration via Distributional Ensemble, a method that encourages exploration of states with high epistemic uncertainty through an ensemble of Q-value distributions. Our algorithm is the first value-based approach to achieve state-of-the-art on both Procgen and Crafter, two benchmarks for generalization in RL with high-dimensional observations. The open-sourced implementation can be found at https://github.com/facebookresearch/ede .",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/1a73038804052a40c12aae696848ece2168f6da7.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 32,
        "score": 16.0
    },
    "79f923d6575bd8253e2f0b70813caa61a870ccee.pdf": {
        "title": "Entropy-regularized Diffusion Policy with Q-Ensembles for Offline Reinforcement Learning",
        "authors": [
            "Ruoqing Zhang",
            "Ziwei Luo",
            "Jens Sj\u00f6lund",
            "Thomas B. Sch\u00f6n",
            "Per Mattsson"
        ],
        "published_date": "2024",
        "abstract": "This paper presents advanced techniques of training diffusion policies for offline reinforcement learning (RL). At the core is a mean-reverting stochastic differential equation (SDE) that transfers a complex action distribution into a standard Gaussian and then samples actions conditioned on the environment state with a corresponding reverse-time SDE, like a typical diffusion policy. We show that such an SDE has a solution that we can use to calculate the log probability of the policy, yielding an entropy regularizer that improves the exploration of offline datasets. To mitigate the impact of inaccurate value functions from out-of-distribution data points, we further propose to learn the lower confidence bound of Q-ensembles for more robust policy improvement. By combining the entropy-regularized diffusion policy with Q-ensembles in offline RL, our method achieves state-of-the-art performance on most tasks in D4RL benchmarks. Code is available at https://github.com/ruoqizzz/Entropy-Regularized-Diffusion-Policy-with-QEnsemble.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/79f923d6575bd8253e2f0b70813caa61a870ccee.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 16,
        "score": 16.0
    },
    "ff87e30cb969d40b1d5c8ddc8932427793467f29.pdf": {
        "title": "Balance Reward and Safety Optimization for Safe Reinforcement Learning: A Perspective of Gradient Manipulation",
        "authors": [
            "Shangding Gu",
            "Bilgehan Sel",
            "Yuhao Ding",
            "Lu Wang",
            "Qingwei Lin",
            "Ming Jin",
            "Alois Knoll"
        ],
        "published_date": "2024",
        "abstract": "Ensuring the safety of Reinforcement Learning (RL) is crucial for its deployment in real-world applications. Nevertheless, managing the trade-off between reward and safety during exploration presents a significant challenge. Improving reward performance through policy adjustments may adversely affect safety performance. In this study, we aim to address this conflicting relation by leveraging the theory of gradient manipulation. Initially, we analyze the conflict between reward and safety gradients. Subsequently, we tackle the balance between reward and safety optimization by proposing a soft switching policy optimization method, for which we provide convergence analysis. Based on our theoretical examination, we provide a safe RL framework to overcome the aforementioned challenge, and we develop a Safety-MuJoCo Benchmark to assess the performance of safe RL algorithms. Finally, we evaluate the effectiveness of our method on the Safety-MuJoCo Benchmark and a popular safe benchmark, Omnisafe. Experimental results demonstrate that our algorithms outperform several state-of-the-art baselines in terms of balancing reward and safety optimization.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/ff87e30cb969d40b1d5c8ddc8932427793467f29.pdf",
        "venue": "AAAI Conference on Artificial Intelligence",
        "citationCount": 16,
        "score": 16.0
    },
    "aedd5c4bc11d8faf01e8456c91a183f2f76e5778.pdf": {
        "title": "ExploRLLM: Guiding Exploration in Reinforcement Learning with Large Language Models",
        "authors": [
            "Runyu Ma",
            "Jelle Luijkx",
            "Zlatan Ajanovi\u0107",
            "Jens Kober"
        ],
        "published_date": "2024",
        "abstract": "In robot manipulation, Reinforcement Learning (RL) often suffers from low sample efficiency and uncertain convergence, especially in large observation and action spaces. Foundation Models (FMs) offer an alternative, demonstrating promise in zero-shot and few-shot settings. However, they can be unreliable due to limited physical and spatial understanding. We introduce ExploRLLM, a method that combines the strengths of both paradigms. In our approach, FMs improve RL convergence by generating policy code and efficient representations, while a residual RL agent compensates for the FMs' limited physical understanding. We show that Explorllm outperforms both policies derived from FMs and RL baselines in table-top manipulation tasks. Additionally, real-world experiments show that the policies exhibit promising zero-shot sim-to-real transfer. Supplementary material is available at https://explorllm.github.io.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/aedd5c4bc11d8faf01e8456c91a183f2f76e5778.pdf",
        "venue": "IEEE International Conference on Robotics and Automation",
        "citationCount": 15,
        "score": 15.0
    },
    "e0bf76edd8e6b793b81c16a915ede48e377ebab6.pdf": {
        "title": "Efficient Reinforcement Learning with Large Language Model Priors",
        "authors": [
            "Xue Yan",
            "Yan Song",
            "Xidong Feng",
            "Mengyue Yang",
            "Haifeng Zhang",
            "H. Ammar",
            "Jun Wang"
        ],
        "published_date": "2024",
        "abstract": "In sequential decision-making (SDM) tasks, methods like reinforcement learning (RL) and heuristic search have made notable advances in specific cases. However, they often require extensive exploration and face challenges in generalizing across diverse environments due to their limited grasp of the underlying decision dynamics. In contrast, large language models (LLMs) have recently emerged as powerful general-purpose tools, due to their capacity to maintain vast amounts of domain-specific knowledge. To harness this rich prior knowledge for efficiently solving complex SDM tasks, we propose treating LLMs as prior action distributions and integrating them into RL frameworks through Bayesian inference methods, making use of variational inference and direct posterior sampling. The proposed approaches facilitate the seamless incorporation of fixed LLM priors into both policy-based and value-based RL frameworks. Our experiments show that incorporating LLM-based action priors significantly reduces exploration and optimization complexity, substantially improving sample efficiency compared to traditional RL techniques, e.g., using LLM priors decreases the number of required samples by over 90% in offline learning scenarios.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/e0bf76edd8e6b793b81c16a915ede48e377ebab6.pdf",
        "venue": "arXiv.org",
        "citationCount": 15,
        "score": 15.0
    },
    "5f5e9ec8bcc5e83eefecc0565bdc004f929f4723.pdf": {
        "title": "Self-Supervised Neuron Segmentation with Multi-Agent Reinforcement Learning",
        "authors": [
            "Yinda Chen",
            "Wei Huang",
            "Shenglong Zhou",
            "Qi Chen",
            "Zhiwei Xiong"
        ],
        "published_date": "2023",
        "abstract": "The performance of existing supervised neuron segmentation methods is highly dependent on the number of accurate annotations, especially when applied to large scale electron microscopy (EM) data. By extracting semantic information from unlabeled data, self-supervised methods can improve the performance of downstream tasks, among which the mask image model (MIM) has been widely used due to its simplicity and effectiveness in recovering original information from masked images. However, due to the high degree of structural locality in EM images, as well as the existence of considerable noise, many voxels contain little discriminative information, making MIM pretraining inefficient on the neuron segmentation task. To overcome this challenge, we propose a decision-based MIM that utilizes reinforcement learning (RL) to automatically search for optimal image masking ratio and masking strategy. Due to the vast exploration space, using single-agent RL for voxel prediction is impractical. Therefore, we treat each input patch as an agent with a shared behavior policy, allowing for multi-agent collaboration. Furthermore, this multi-agent model can capture dependencies between voxels, which is beneficial for the downstream segmentation task. Experiments conducted on representative EM datasets demonstrate that our approach has a significant advantage over alternative self-supervised methods on the task of neuron segmentation. Code is available at https://github.com/ydchen0806/dbMiM.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/5f5e9ec8bcc5e83eefecc0565bdc004f929f4723.pdf",
        "venue": "International Joint Conference on Artificial Intelligence",
        "citationCount": 28,
        "score": 14.0
    },
    "d60df0754df6ccb14c563f07f865f391da3cba2d.pdf": {
        "title": "An OCBA-Based Method for Efficient Sample Collection in Reinforcement Learning",
        "authors": [
            "Kuo Li",
            "Xinze Jin",
            "Qing-Shan Jia",
            "Dongchun Ren",
            "Huaxia Xia"
        ],
        "published_date": "2024",
        "abstract": "This work focuses on the sample collection in reinforcement learning (RL), where the interaction with the environment is typically time-consuming and extravagantly expensive. In order to collect samples in a more valuable way, we propose a confidence-based sampling strategy based on the optimal computing budget allocation algorithm (OCBA), which actively allocates the computing efforts to actions with different predictive uncertainties. We estimate the uncertainty with ensembles and generalize them from tabular representations to function approximations. The OCBA-based sampling strategy could be easily integrated into various off-policy RL algorithms, where we take Q-learning, DQN, and SAC as examples to show the incorporation. Besides, we provide the theoretical analysis towards convergence and evaluate the algorithms experimentally. According to the experiments, the incorporated algorithms obtain remarkable gains compared with modern ensemble-based RL algorithms. Note to Practitioners\u2014Reinforcement learning is a powerful tool for handling sequential decision-making problems, e.g., autonomous driving and robotics control, where the behaviors typically have a long-term effect on future events. However, although RL achieves human-level control in some tasks, it severely suffers from low sample efficiency. Therefore, implementing RL in some practical areas, e.g., healthcare and rescue, is extremely hard due to the requirement of massive samples. This work aims to enhance the exploration of RL by incorporating OCBA, which provides an asymptotically optimal data-collection strategy for simulation-based optimization. Based on ensemble-based uncertainty estimation and OCBA-based action selection, the incorporated RL algorithms show competitive performance on many benchmarks and significantly reduce the sampling efforts during iterations.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/d60df0754df6ccb14c563f07f865f391da3cba2d.pdf",
        "venue": "IEEE Transactions on Automation Science and Engineering",
        "citationCount": 13,
        "score": 13.0
    },
    "03c7f9a6b743eb1c9127b8d4552147b2b1a0c1e1.pdf": {
        "title": "Demonstration-Guided Reinforcement Learning with Efficient Exploration for Task Automation of Surgical Robot",
        "authors": [
            "Tao Huang",
            "Kai Chen",
            "Bin Li",
            "Yunhui Liu",
            "Qingxu Dou"
        ],
        "published_date": "2023",
        "abstract": "Task automation of surgical robot has the potentials to improve surgical efficiency. Recent reinforcement learning (RL) based approaches provide scalable solutions to surgical automation, but typically require extensive data collection to solve a task if no prior knowledge is given. This issue is known as the exploration challenge, which can be alleviated by providing expert demonstrations to an RL agent. Yet, how to make effective use of demonstration data to improve exploration efficiency still remains an open challenge. In this work, we introduce Demonstration-guided EXploration (DEX), an efficient reinforcement learning algorithm that aims to overcome the exploration problem with expert demonstrations for surgical automation. To effectively exploit demonstrations, our method estimates expert-like behaviors with higher values to facilitate productive interactions, and adopts non-parametric regression to enable such guidance at states unobserved in demonstration data. Extensive experiments on 10 surgical manipulation tasks from SurRoL, a comprehensive surgical simulation platform, demonstrate significant improvements in the exploration efficiency and task success rates of our method. Moreover, we also deploy the learned policies to the da Vinci Research Kit (dVRK) platform to show the effectiveness on the real robot. Code is available at https://github.com/med-air/DEX.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/03c7f9a6b743eb1c9127b8d4552147b2b1a0c1e1.pdf",
        "venue": "IEEE International Conference on Robotics and Automation",
        "citationCount": 26,
        "score": 13.0
    },
    "7bd4edf878976d329f326f3a12675a66cbc075e9.pdf": {
        "title": "Constrained reinforcement learning with statewise projection: a control barrier function approach",
        "authors": [
            "Xinze Jin",
            "Kuo Li",
            "Qing-Shan Jia"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/7bd4edf878976d329f326f3a12675a66cbc075e9.pdf",
        "venue": "Science China Information Sciences",
        "citationCount": 13,
        "score": 13.0
    },
    "c90aa0f206c6fd41c490c142f63f7ba046cae6b7.pdf": {
        "title": "Provable and Practical: Efficient Exploration in Reinforcement Learning via Langevin Monte Carlo",
        "authors": [
            "Haque Ishfaq",
            "Qingfeng Lan",
            "Pan Xu",
            "A. Mahmood",
            "Doina Precup",
            "Anima Anandkumar",
            "K. Azizzadenesheli"
        ],
        "published_date": "2023",
        "abstract": "We present a scalable and effective exploration strategy based on Thompson sampling for reinforcement learning (RL). One of the key shortcomings of existing Thompson sampling algorithms is the need to perform a Gaussian approximation of the posterior distribution, which is not a good surrogate in most practical settings. We instead directly sample the Q function from its posterior distribution, by using Langevin Monte Carlo, an efficient type of Markov Chain Monte Carlo (MCMC) method. Our method only needs to perform noisy gradient descent updates to learn the exact posterior distribution of the Q function, which makes our approach easy to deploy in deep RL. We provide a rigorous theoretical analysis for the proposed method and demonstrate that, in the linear Markov decision process (linear MDP) setting, it has a regret bound of $\\tilde{O}(d^{3/2}H^{3/2}\\sqrt{T})$, where $d$ is the dimension of the feature mapping, $H$ is the planning horizon, and $T$ is the total number of steps. We apply this approach to deep RL, by using Adam optimizer to perform gradient updates. Our approach achieves better or similar results compared with state-of-the-art deep RL algorithms on several challenging exploration tasks from the Atari57 suite.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/c90aa0f206c6fd41c490c142f63f7ba046cae6b7.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 24,
        "score": 12.0
    },
    "53db22a9d4ae77dd8218ba867184898adc84d1d1.pdf": {
        "title": "Sample Efficient Offline-to-Online Reinforcement Learning",
        "authors": [
            "Siyuan Guo",
            "Lixin Zou",
            "Hechang Chen",
            "B. Qu",
            "Haotian Chi",
            "Philip S. Yu",
            "Yi Chang"
        ],
        "published_date": "2024",
        "abstract": "Offline reinforcement learning (RL) makes it possible to train the agents entirely from a previously collected dataset. However, constrained by the quality of the offline dataset, offline RL agents typically have limited performance and cannot be directly deployed. Thus, it is desirable to further finetune the pretrained offline RL agents via online interactions with the environment. Existing offline-to-online RL algorithms suffer from the low sample efficiency issue, due to two inherent challenges, i.e., exploration limitation and distribution shift. To this end, we propose a sample-efficient offline-to-online RL algorithm via Optimistic Exploration and Meta Adaptation (OEMA). Specifically, we first propose an optimistic exploration strategy according to the principle of optimism in the face of uncertainty. This allows agents to sufficiently explore the environment in a stable manner. Moreover, we propose a meta learning based adaptation method, which can reduce the distribution shift and accelerate the offline-to-online adaptation process. We empirically demonstrate that OEMA improves the sample efficiency on D4RL benchmark. Besides, we provide in-depth analyses to verify the effectiveness of both optimistic exploration and meta adaptation.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/53db22a9d4ae77dd8218ba867184898adc84d1d1.pdf",
        "venue": "IEEE Transactions on Knowledge and Data Engineering",
        "citationCount": 12,
        "score": 12.0
    },
    "fbc0b5e1b822796d7ae97268def2e0993b5da644.pdf": {
        "title": "Algorithm Discovery With LLMs: Evolutionary Search Meets Reinforcement Learning",
        "authors": [
            "Anja Surina",
            "Amin Mansouri",
            "Lars Quaedvlieg",
            "Amal Seddas",
            "Maryna Viazovska",
            "Emmanuel Abbe",
            "Caglar Gulcehre"
        ],
        "published_date": "2025",
        "abstract": "Discovering efficient algorithms for solving complex problems has been an outstanding challenge in mathematics and computer science, requiring substantial human expertise over the years. Recent advancements in evolutionary search with large language models (LLMs) have shown promise in accelerating the discovery of algorithms across various domains, particularly in mathematics and optimization. However, existing approaches treat the LLM as a static generator, missing the opportunity to update the model with the signal obtained from evolutionary exploration. In this work, we propose to augment LLM-based evolutionary search by continuously refining the search operator - the LLM - through reinforcement learning (RL) fine-tuning. Our method leverages evolutionary search as an exploration strategy to discover improved algorithms, while RL optimizes the LLM policy based on these discoveries. Our experiments on combinatorial optimization tasks demonstrate that integrating RL with evolutionary search accelerates the discovery of superior algorithms, showcasing the potential of RL-enhanced evolutionary strategies for algorithm design.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/fbc0b5e1b822796d7ae97268def2e0993b5da644.pdf",
        "venue": "arXiv.org",
        "citationCount": 12,
        "score": 12.0
    },
    "97c7066b53b208b24001ad0dcc49a851fcf13bfd.pdf": {
        "title": "DIME:Diffusion-Based Maximum Entropy Reinforcement Learning",
        "authors": [
            "Onur Celik",
            "Zechu Li",
            "Denis Blessing",
            "Ge Li",
            "Daniel Palanicek",
            "Jan Peters",
            "G. Chalvatzaki",
            "Gerhard Neumann"
        ],
        "published_date": "2025",
        "abstract": "Maximum entropy reinforcement learning (MaxEnt-RL) has become the standard approach to RL due to its beneficial exploration properties. Traditionally, policies are parameterized using Gaussian distributions, which significantly limits their representational capacity. Diffusion-based policies offer a more expressive alternative, yet integrating them into MaxEnt-RL poses challenges-primarily due to the intractability of computing their marginal entropy. To overcome this, we propose Diffusion-Based Maximum Entropy RL (DIME). \\emph{DIME} leverages recent advances in approximate inference with diffusion models to derive a lower bound on the maximum entropy objective. Additionally, we propose a policy iteration scheme that provably converges to the optimal diffusion policy. Our method enables the use of expressive diffusion-based policies while retaining the principled exploration benefits of MaxEnt-RL, significantly outperforming other diffusion-based methods on challenging high-dimensional control benchmarks. It is also competitive with state-of-the-art non-diffusion based RL methods while requiring fewer algorithmic design choices and smaller update-to-data ratios, reducing computational complexity.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/97c7066b53b208b24001ad0dcc49a851fcf13bfd.pdf",
        "venue": "arXiv.org",
        "citationCount": 12,
        "score": 12.0
    },
    "81fdf2b3db2b3e7be90b51866a31b73587eecd30.pdf": {
        "title": "A Novel Reinforcement Learning-Based Robust Control Strategy for a Quadrotor",
        "authors": [
            "Hean Hua",
            "Yongchun Fang"
        ],
        "published_date": "2023",
        "abstract": "In this article, a novel reinforcement learning (RL)-based robust control approach is proposed for quadrotors, which guarantees efficient learning and satisfactory tracking performance by simultaneously evaluating the RL and the baseline method in training. Different from existing works, the key novelty is to design a practice-reliable RL control framework for quadrotors in a two-part cooperative manner. In the first part, based on the hierarchical property, a new robust integral of the signum of the error (RISE) design is proposed to ensure asymptotic convergence, which includes the nonlinear and the disturbance rejection terms. In the second part, a one-actor-dual-critic (OADC) learning framework is proposed, where the designed switching logic in the first part works as a benchmark to guide the learning. Specifically, the two critics independently evaluate the RL policy and the switching logic simultaneously, which are utilized for policy update, only when both are positive, corresponding to the remarkable actor-better exploration actions. The asymptotic RISE controller, together with the two critics in OADC learning framework, guarantees accurate judgment on every exploration. On this basis, the satisfactory performance of the RL policy is guaranteed by the actor-better exploration based learning while the chattering problem arisen from the switching logic is addressed completely. Plenty of comparative experimental tests are presented to illustrate the superior performance of the proposed RL controller in terms of tracking accuracy and robustness.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/81fdf2b3db2b3e7be90b51866a31b73587eecd30.pdf",
        "venue": "IEEE transactions on industrial electronics (1982. Print)",
        "citationCount": 23,
        "score": 11.5
    },
    "e4fef8d5864c5468100ca167639ef3fa374c0442.pdf": {
        "title": "MaxInfoRL: Boosting exploration in reinforcement learning through information gain maximization",
        "authors": [
            "Bhavya Sukhija",
            "Stelian Coros",
            "Andreas Krause",
            "Pieter Abbeel",
            "Carmelo Sferrazza"
        ],
        "published_date": "2024",
        "abstract": "Reinforcement learning (RL) algorithms aim to balance exploiting the current best strategy with exploring new options that could lead to higher rewards. Most common RL algorithms use undirected exploration, i.e., select random sequences of actions. Exploration can also be directed using intrinsic rewards, such as curiosity or model epistemic uncertainty. However, effectively balancing task and intrinsic rewards is challenging and often task-dependent. In this work, we introduce a framework, MaxInfoRL, for balancing intrinsic and extrinsic exploration. MaxInfoRL steers exploration towards informative transitions, by maximizing intrinsic rewards such as the information gain about the underlying task. When combined with Boltzmann exploration, this approach naturally trades off maximization of the value function with that of the entropy over states, rewards, and actions. We show that our approach achieves sublinear regret in the simplified setting of multi-armed bandits. We then apply this general formulation to a variety of off-policy model-free RL methods for continuous state-action spaces, yielding novel algorithms that achieve superior performance across hard exploration problems and complex scenarios such as visual control tasks.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/e4fef8d5864c5468100ca167639ef3fa374c0442.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 11,
        "score": 11.0
    },
    "7da3eebf5c2f481bc38ae6115cec3fc71dc87f2b.pdf": {
        "title": "Randomized Exploration in Cooperative Multi-Agent Reinforcement Learning",
        "authors": [
            "Hao-Lun Hsu",
            "Weixin Wang",
            "Miroslav Pajic",
            "Pan Xu"
        ],
        "published_date": "2024",
        "abstract": "We present the first study on provably efficient randomized exploration in cooperative multi-agent reinforcement learning (MARL). We propose a unified algorithm framework for randomized exploration in parallel Markov Decision Processes (MDPs), and two Thompson Sampling (TS)-type algorithms, CoopTS-PHE and CoopTS-LMC, incorporating the perturbed-history exploration (PHE) strategy and the Langevin Monte Carlo exploration (LMC) strategy, respectively, which are flexible in design and easy to implement in practice. For a special class of parallel MDPs where the transition is (approximately) linear, we theoretically prove that both CoopTS-PHE and CoopTS-LMC achieve a $\\widetilde{\\mathcal{O}}(d^{3/2}H^2\\sqrt{MK})$ regret bound with communication complexity $\\widetilde{\\mathcal{O}}(dHM^2)$, where $d$ is the feature dimension, $H$ is the horizon length, $M$ is the number of agents, and $K$ is the number of episodes. This is the first theoretical result for randomized exploration in cooperative MARL. We evaluate our proposed method on multiple parallel RL environments, including a deep exploration problem (i.e., $N$-chain), a video game, and a real-world problem in energy systems. Our experimental results support that our framework can achieve better performance, even under conditions of misspecified transition models. Additionally, we establish a connection between our unified framework and the practical application of federated learning.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/7da3eebf5c2f481bc38ae6115cec3fc71dc87f2b.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 11,
        "score": 11.0
    },
    "2e6534fbeb932fc058b9f865c0b25a3f201a0704.pdf": {
        "title": "Learning State-Specific Action Masks for Reinforcement Learning",
        "authors": [
            "Ziyi Wang",
            "Xinran Li",
            "Luoyang Sun",
            "Haifeng Zhang",
            "Hualin Liu",
            "Jun Wang"
        ],
        "published_date": "2024",
        "abstract": "Efficient yet sufficient exploration remains a critical challenge in reinforcement learning (RL), especially for Markov Decision Processes (MDPs) with vast action spaces. Previous approaches have commonly involved projecting the original action space into a latent space or employing environmental action masks to reduce the action possibilities. Nevertheless, these methods often lack interpretability or rely on expert knowledge. In this study, we introduce a novel method for automatically reducing the action space in environments with discrete action spaces while preserving interpretability. The proposed approach learns state-specific masks with a dual purpose: (1) eliminating actions with minimal influence on the MDP and (2) aggregating actions with identical behavioral consequences within the MDP. Specifically, we introduce a novel concept called Bisimulation Metrics on Actions by States (BMAS) to quantify the behavioral consequences of actions within the MDP and design a dedicated mask model to ensure their binary nature. Crucially, we present a practical learning procedure for training the mask model, leveraging transition data collected by any RL policy. Our method is designed to be plug-and-play and adaptable to all RL policies, and to validate its effectiveness, an integration into two prominent RL algorithms, DQN and PPO, is performed. Experimental results obtained from Maze, Atari, and \u03bcRTS2 reveal a substantial acceleration in the RL learning process and noteworthy performance improvements facilitated by the introduced approach.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/2e6534fbeb932fc058b9f865c0b25a3f201a0704.pdf",
        "venue": "Algorithms",
        "citationCount": 11,
        "score": 11.0
    },
    "01936f6df3c760d23df237d8d15cb7faadce9520.pdf": {
        "title": "Design of an Adaptive Robust PI Controller for DC/DC Boost Converter Using Reinforcement-Learning Technique and Snake Optimization Algorithm",
        "authors": [
            "S. Ghamari",
            "Mojtaba Hajihosseini",
            "D. Habibi",
            "Asma Aziz"
        ],
        "published_date": "2024",
        "abstract": "The DC/DC Boost converter exhibits a non-minimum phase system with a right half-plane zero structure, posing significant challenges for the design of effective control approaches. This article presents the design of a robust Proportional-Integral (PI) controller for this converter with an online adaptive mechanism based on the Reinforcement-Learning (RL) strategy. Classical PI controllers are simple and easy to build, but they need to be more robust against a wide range of disturbances and more adaptable to operational parameters. To address these issues, the RL adaptive strategy is used to optimize the performance of the PI controller. Some of the main advantages of the RL are lower sensitivity to error, more reliable results through the collection of data from the environment, an ideal model behavior within a specific context, and better frequency matching in real-time applications. Random exploration, nevertheless, can result in disastrous outcomes and surprising performance in real-world settings. Therefore, we opt for the Deterministic Policy Gradient (DPG) technique, which employs a deterministic action function as opposed to a stochastic one. DPG combines the benefits of actor-critics, deep Q-networks, and the deterministic policy gradient method. In addition, this method adopts the Snake Optimization (SO) algorithm to optimize the initial condition of gains, yielding more reliable results with faster dynamics. The SO method is known for its disciplined and nature-inspired approach, which results in faster decision-making and greater accuracy compared to other optimization algorithms. A structure using a hardware setup with CONTROLLINO MAXI Automation is built, which offers a more cost-effective and precise measurement method. Finally, the results achieved by simulations and experiments demonstrate the robustness of this approach.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/01936f6df3c760d23df237d8d15cb7faadce9520.pdf",
        "venue": "IEEE Access",
        "citationCount": 11,
        "score": 11.0
    },
    "c225c6e24526c160bbceaaa36b447df9d8e95f13.pdf": {
        "title": "Safe Reinforcement Learning in Autonomous Driving With Epistemic Uncertainty Estimation",
        "authors": [
            "Zhengran Zhang",
            "Qi Liu",
            "Yanjie Li",
            "Ke Lin",
            "Linyu Li"
        ],
        "published_date": "2024",
        "abstract": "Safety is one of the critical challenges in the autonomous driving task. Recent works address the safety by implementing a safe reinforcement learning (safe RL) mechanism. However, most approaches make conservative decisions without knowing the confidence of the actions, which ultimately causes traffic congestion and low travel efficiency. This paper proposes an uncertainty-augmented Lagrangian safe reinforcement algorithm (Lag-U) to improve exploration and safety performance for autonomous driving. First, epistemic uncertainty is introduced into safe RL by using deep ensemble. We use the estimated epistemic uncertainty to encourage exploration and to learn a risk-sensitive policy by adaptively modifying safety constraints. Second, we facilitate an intervention assurance to choose safer actions based on the quantified epistemic uncertainty during deployment. Experimental results prove that the proposed method outperforms other safe RL baselines. The trained vehicle can make a decent trade-off between high efficiency and avoiding risks, thus preventing ultra-conservative policy.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/c225c6e24526c160bbceaaa36b447df9d8e95f13.pdf",
        "venue": "IEEE transactions on intelligent transportation systems (Print)",
        "citationCount": 11,
        "score": 11.0
    },
    "c734971c6000e3f2769ab5165d00816af80dd76f.pdf": {
        "title": "In-context Exploration-Exploitation for Reinforcement Learning",
        "authors": [
            "Zhenwen Dai",
            "Federico Tomasi",
            "Sina Ghiassian"
        ],
        "published_date": "2024",
        "abstract": "In-context learning is a promising approach for online policy learning of offline reinforcement learning (RL) methods, which can be achieved at inference time without gradient optimization. However, this method is hindered by significant computational costs resulting from the gathering of large training trajectory sets and the need to train large Transformer models. We address this challenge by introducing an In-context Exploration-Exploitation (ICEE) algorithm, designed to optimize the efficiency of in-context policy learning. Unlike existing models, ICEE performs an exploration-exploitation trade-off at inference time within a Transformer model, without the need for explicit Bayesian inference. Consequently, ICEE can solve Bayesian optimization problems as efficiently as Gaussian process biased methods do, but in significantly less time. Through experiments in grid world environments, we demonstrate that ICEE can learn to solve new RL tasks using only tens of episodes, marking a substantial improvement over the hundreds of episodes needed by the previous in-context learning method.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/c734971c6000e3f2769ab5165d00816af80dd76f.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 10,
        "score": 10.0
    },
    "ed8ea3d06c173849f02ee8afcf8db07df0f31261.pdf": {
        "title": "Optimal Energy Management of Plug-In Hybrid Electric Vehicles Through Ensemble Reinforcement Learning With Exploration-to-Exploitation Ratio Control",
        "authors": [
            "Bin Shuai",
            "Min Hua",
            "Yanfei Li",
            "Shijin Shuai",
            "Hongming Xu",
            "Quan Zhou"
        ],
        "published_date": "2025",
        "abstract": "Reinforcement learning (RL) has demonstrated its advantages in the intelligent control of many vehicle systems. However, controlling the exploration-to-exploitation (E2E) ratio of RL for the best performance in real-world operations is a great challenge. To obtain the optimal E2E ratio for managing the energy flow of a plug-in hybrid electric vehicle (PHEV) in real-world driving, this paper proposes an ensemble learning scheme based on two independent Q-learning agents working back-to-back competitively. At each sampling time, these agents generate two candidate control actions based on state observation and their control policies. Three decay functions, including the widely-used exponential decay and two new decay methods i.e., reciprocal function-base decay and step-based decay, are introduced to formulate one-dimensional look-up tables for E2E control. Then the PHEV control action will be selected from the candidate actions by a learning automata module(LAM) developed in this paper. The combinations of the three decay methods and three ensemble strategies with maximum-based, random-based, and weighted-based methods are investigated with the considerations of their energy efficiency, real-time performance, and robustness. Experiments are carried out on software-in-loop and hardware-in-the-loop platforms to demonstrate the energy-saving potentials. By implementing the ensemble learning scheme based on the weighted-based ensemble method in the control of the studied PHEV, up to 1.04% of energy can be saved under the predefined real-world driving cycles compared to the conventional Q-learning scheme.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/ed8ea3d06c173849f02ee8afcf8db07df0f31261.pdf",
        "venue": "IEEE Transactions on Intelligent Vehicles",
        "citationCount": 10,
        "score": 10.0
    },
    "dcbafb5ec43572d6ca170d86569d09a5dd40a8f8.pdf": {
        "title": "Excluding the Irrelevant: Focusing Reinforcement Learning through Continuous Action Masking",
        "authors": [
            "Roland Stolz",
            "Hanna Krasowski",
            "Jakob Thumm",
            "Michael Eichelbeck",
            "Philipp Gassert",
            "Matthias Althoff"
        ],
        "published_date": "2024",
        "abstract": "Continuous action spaces in reinforcement learning (RL) are commonly defined as multidimensional intervals. While intervals usually reflect the action boundaries for tasks well, they can be challenging for learning because the typically large global action space leads to frequent exploration of irrelevant actions. Yet, little task knowledge can be sufficient to identify significantly smaller state-specific sets of relevant actions. Focusing learning on these relevant actions can significantly improve training efficiency and effectiveness. In this paper, we propose to focus learning on the set of relevant actions and introduce three continuous action masking methods for exactly mapping the action space to the state-dependent set of relevant actions. Thus, our methods ensure that only relevant actions are executed, enhancing the predictability of the RL agent and enabling its use in safety-critical applications. We further derive the implications of the proposed methods on the policy gradient. Using proximal policy optimization (PPO), we evaluate our methods on four control tasks, where the relevant action set is computed based on the system dynamics and a relevant state set. Our experiments show that the three action masking methods achieve higher final rewards and converge faster than the baseline without action masking.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/dcbafb5ec43572d6ca170d86569d09a5dd40a8f8.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 10,
        "score": 10.0
    },
    "492f441bc6fdbb5f4b9273197ae563126439abeb.pdf": {
        "title": "Learning and Repair of Deep Reinforcement Learning Policies from Fuzz-Testing Data",
        "authors": [
            "Martin Tappler",
            "Andrea Pferscher",
            "B. Aichernig",
            "Bettina K\u00f6nighofer"
        ],
        "published_date": "2024",
        "abstract": "Reinforcement learning from demonstrations (RLfD) is a promising approach to improve the exploration efficiency of reinforcement learning (RL) by learning from expert demonstrations in addition to interactions with the environment. In this paper, we propose a framework that combines techniques from search-based testing with RLfD with the goal to raise the level of dependability of RL policies and to reduce human engineering effort. Within our framework, we provide methods for efficiently training, evaluating, and repairing RL policies. Instead of relying on the costly collection of demonstrations from (human) experts, we automatically compute a diverse set of demonstrations via search-based fuzzing methods and use the fuzz demonstrations for RLfD. To evaluate the safety and robustness of the trained RL agent, we search for safety-critical scenarios in the black-box environment. Finally, when unsafe behavior is detected, we compute demonstrations through fuzz testing that represent safe behavior and use them to repair the policy. Our experiments show that our framework is able to efficiently learn high-performing and safe policies without requiring any expert knowledge.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/492f441bc6fdbb5f4b9273197ae563126439abeb.pdf",
        "venue": "International Conference on Software Engineering",
        "citationCount": 10,
        "score": 10.0
    },
    "4d3ffd8feca81d750aa30122b49cc1e874e70c1a.pdf": {
        "title": "MAMBA: an Effective World Model Approach for Meta-Reinforcement Learning",
        "authors": [
            "Zohar Rimon",
            "Tom Jurgenson",
            "Orr Krupnik",
            "Gilad Adler",
            "Aviv Tamar"
        ],
        "published_date": "2024",
        "abstract": "Meta-reinforcement learning (meta-RL) is a promising framework for tackling challenging domains requiring efficient exploration. Existing meta-RL algorithms are characterized by low sample efficiency, and mostly focus on low-dimensional task distributions. In parallel, model-based RL methods have been successful in solving partially observable MDPs, of which meta-RL is a special case. In this work, we leverage this success and propose a new model-based approach to meta-RL, based on elements from existing state-of-the-art model-based and meta-RL methods. We demonstrate the effectiveness of our approach on common meta-RL benchmark domains, attaining greater return with better sample efficiency (up to $15\\times$) while requiring very little hyperparameter tuning. In addition, we validate our approach on a slate of more challenging, higher-dimensional domains, taking a step towards real-world generalizing agents.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/4d3ffd8feca81d750aa30122b49cc1e874e70c1a.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 10,
        "score": 10.0
    },
    "40d1e0a1e8a861305f9354be747620782fc203ce.pdf": {
        "title": "Deep Reinforcement Learning: A Chronological Overview and Methods",
        "authors": [
            "Juan R. Terven"
        ],
        "published_date": "2025",
        "abstract": "Introduction: Deep reinforcement learning (deep RL) integrates the principles of reinforcement learning with deep neural networks, enabling agents to excel in diverse tasks ranging from playing board games such as Go and Chess to controlling robotic systems and autonomous vehicles. By leveraging foundational concepts of value functions, policy optimization, and temporal difference methods, deep RL has rapidly evolved and found applications in areas such as gaming, robotics, finance, and healthcare. Objective: This paper seeks to provide a comprehensive yet accessible overview of the evolution of deep RL and its leading algorithms. It aims to serve both as an introduction for newcomers to the field and as a practical guide for those seeking to select the most appropriate methods for specific problem domains. Methods: We begin by outlining fundamental reinforcement learning principles, followed by an exploration of early tabular Q-learning methods. We then trace the historical development of deep RL, highlighting key milestones such as the advent of deep Q-networks (DQN). The survey extends to policy gradient methods, actor\u2013critic architectures, and state-of-the-art algorithms such as proximal policy optimization, soft actor\u2013critic, and emerging model-based approaches. Throughout, we discuss the current challenges facing deep RL, including issues of sample efficiency, interpretability, and safety, as well as open research questions involving large-scale training, hierarchical architectures, and multi-task learning. Results: Our analysis demonstrates how critical breakthroughs have driven deep RL into increasingly complex application domains. We highlight existing limitations and ongoing bottlenecks, such as high data requirements and the need for more transparent, ethically aligned systems. Finally, we survey potential future directions, highlighting the importance of reliability and ethical considerations for real-world deployments.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/40d1e0a1e8a861305f9354be747620782fc203ce.pdf",
        "venue": "Applied Informatics",
        "citationCount": 9,
        "score": 9.0
    },
    "736c35ed3e8e86ce99e02ce117dba7ee77e60dda.pdf": {
        "title": "FastTuner: Transferable Physical Design Parameter Optimization using Fast Reinforcement Learning",
        "authors": [
            "Hao-Hsiang Hsiao",
            "Yi-Chen Lu",
            "Pruek Vanna-iampikul",
            "S. Lim"
        ],
        "published_date": "2024",
        "abstract": "Current state-of-the-art Design Space Exploration (DSE) methods in Physical Design (PD), including Bayesian optimization (BO) and Ant Colony Optimization (ACO), mainly rely on black-boxed rather than parametric (e.g., neural networks) approaches to improve end-of-flow Power, Performance, and Area (PPA) metrics, which often fail to generalize across unseen designs as netlist features are not properly leveraged. To overcome this issue, in this paper, we develop a Reinforcement Learning (RL) agent that leverages Graph Neural Networks (GNNs) and Transformers to perform \"fast\" DSE on unseen designs by sequentially encoding netlist features across different PD stages. Particularly, an attention-based encoder-decoder framework is devised for \"conditional\" parameter tuning, and a PPA estimator is introduced to predict end-of-flow PPA metrics for RL reward estimation. Extensive studies across 7 industrial designs under the TSMC 28nm technology node demonstrate that the proposed framework FastTuner, significantly outperforms existing state-of-the-art DSE techniques in both optimization quality and runtime. where we observe improvements up to 79.38% in Total Negative Slack (TNS), 12.22% in total power, and 50x in runtime.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/736c35ed3e8e86ce99e02ce117dba7ee77e60dda.pdf",
        "venue": "ACM International Symposium on Physical Design",
        "citationCount": 9,
        "score": 9.0
    },
    "a69adfee23dbc90b2292499ffb1bf9fbd7d656c5.pdf": {
        "title": "Reimagining space layout design through deep reinforcement learning",
        "authors": [
            "R. Kakooee",
            "B. Dillenburger"
        ],
        "published_date": "2024",
        "abstract": "\n Space layout design is a critical aspect of architectural design, influencing functionality and aesthetics. The inherent combinatorial nature of layout design poses challenges for traditional planning approaches; thus, it demands the exploration of novel methods. This paper presents a novel framework that leverages the potential of deep reinforcement learning (RL) algorithms to optimize space layouts. RL has demonstrated remarkable success in addressing complex decision-making problems, yet its application in the design process remains relatively unexplored. We argue that RL is particularly well-suited for the design process due to its ability to accommodate offline tasks and seamless integration with existing CAD software, effectively acting as a simulator for design exploration. Framing space layout design as an RL problem and employing RL methods allows for the automated exploration of the expansive design space, thereby enhancing the discovery of innovative solutions. This paper also elucidates the synergy between the design process and the RL problem, which opens new avenues for exploring the potential of RL algorithms in design. We aim to foster experimentation and collaboration within the RL and architecture communities. To facilitate our research, we have developed SpaceLayoutGym, an environment specifically designed for space layout design tasks. SpaceLayoutGym serves as a customizable environment that encapsulates the essential elements of the layout design process within an RL framework. To showcase the effectiveness of SpaceLayoutGym and the capabilities of RL as an artificial space layout designer, we employ the PPO algorithm to train the RL agent in selected design scenarios with both geometrical constraints and topological objectives. The study further extends to contrast the effectiveness of PPO agents with that of genetic algorithms, and also includes a comparative analysis with existing layouts. Our results demonstrate the potential of RL to optimize space layouts, offering a promising direction for the future of AI-aided design.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/a69adfee23dbc90b2292499ffb1bf9fbd7d656c5.pdf",
        "venue": "Journal of Computational Design and Engineering",
        "citationCount": 9,
        "score": 9.0
    },
    "e5d6cca71ea0fb216a25f86e96d3480886fdba27.pdf": {
        "title": "D5RL: Diverse Datasets for Data-Driven Deep Reinforcement Learning",
        "authors": [
            "Rafael Rafailov",
            "K. Hatch",
            "Anikait Singh",
            "Laura Smith",
            "Aviral Kumar",
            "Ilya Kostrikov",
            "Philippe Hansen-Estruch",
            "Victor Kolev",
            "Philip Ball",
            "Jiajun Wu",
            "Chelsea Finn",
            "Sergey Levine"
        ],
        "published_date": "2024",
        "abstract": "Offline reinforcement learning algorithms hold the promise of enabling data-driven RL methods that do not require costly or dangerous real-world exploration and benefit from large pre-collected datasets. This in turn can facilitate real-world applications, as well as a more standardized approach to RL research. Furthermore, offline RL methods can provide effective initializations for online finetuning to overcome challenges with exploration. However, evaluating progress on offline RL algorithms requires effective and challenging benchmarks that capture properties of real-world tasks, provide a range of task difficulties, and cover a range of challenges both in terms of the parameters of the domain (e.g., length of the horizon, sparsity of rewards) and the parameters of the data (e.g., narrow demonstration data or broad exploratory data). While considerable progress in offline RL in recent years has been enabled by simpler benchmark tasks, the most widely used datasets are increasingly saturating in performance and may fail to reflect properties of realistic tasks. We propose a new benchmark for offline RL that focuses on realistic simulations of robotic manipulation and locomotion environments, based on models of real-world robotic systems, and comprising a variety of data sources, including scripted data, play-style data collected by human teleoperators, and other data sources. Our proposed benchmark covers state-based and image-based domains, and supports both offline RL and online fine-tuning evaluation, with some of the tasks specifically designed to require both pre-training and fine-tuning. We hope that our proposed benchmark will facilitate further progress on both offline RL and fine-tuning algorithms. Website with code, examples, tasks, and data is available at \\url{https://sites.google.com/view/d5rl/}",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/e5d6cca71ea0fb216a25f86e96d3480886fdba27.pdf",
        "venue": "RLJ",
        "citationCount": 9,
        "score": 9.0
    },
    "5ecf53ab083f72f10421225a7dde25eb51cb6b22.pdf": {
        "title": "Can Learned Optimization Make Reinforcement Learning Less Difficult?",
        "authors": [
            "A. D. Goldie",
            "Chris Lu",
            "Matthew Jackson",
            "S. Whiteson",
            "J. Foerster"
        ],
        "published_date": "2024",
        "abstract": "While reinforcement learning (RL) holds great potential for decision making in the real world, it suffers from a number of unique difficulties which often need specific consideration. In particular: it is highly non-stationary; suffers from high degrees of plasticity loss; and requires exploration to prevent premature convergence to local optima and maximize return. In this paper, we consider whether learned optimization can help overcome these problems. Our method, Learned Optimization for Plasticity, Exploration and Non-stationarity (OPEN), meta-learns an update rule whose input features and output structure are informed by previously proposed solutions to these difficulties. We show that our parameterization is flexible enough to enable meta-learning in diverse learning contexts, including the ability to use stochasticity for exploration. Our experiments demonstrate that when meta-trained on single and small sets of environments, OPEN outperforms or equals traditionally used optimizers. Furthermore, OPEN shows strong generalization characteristics across a range of environments and agent architectures.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/5ecf53ab083f72f10421225a7dde25eb51cb6b22.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 9,
        "score": 9.0
    },
    "4186f74da6d793c91c5ca4d8a10cf2f8638eade6.pdf": {
        "title": "RLfOLD: Reinforcement Learning from Online Demonstrations in Urban Autonomous Driving",
        "authors": [
            "Daniel Coelho",
            "Miguel Oliveira",
            "Vitor Santos"
        ],
        "published_date": "2024",
        "abstract": "Reinforcement Learning from Demonstrations (RLfD) has emerged as an effective method by fusing expert demonstrations into Reinforcement Learning (RL) training, harnessing the strengths of both Imitation Learning (IL) and RL. However, existing algorithms rely on offline demonstrations, which can introduce a distribution gap between the demonstrations and the actual training environment, limiting their performance. In this paper, we propose a novel approach, Reinforcement Learning from Online Demonstrations (RLfOLD), that leverages online demonstrations to address this limitation, ensuring the agent learns from relevant and up-to-date scenarios, thus effectively bridging the distribution gap. Unlike conventional policy networks used in typical actor-critic algorithms, RLfOLD introduces a policy network that outputs two standard deviations: one for exploration and the other for IL training. This novel design allows the agent to adapt to varying levels of uncertainty inherent in both RL and IL. Furthermore, we introduce an exploration process guided by an online expert, incorporating an uncertainty-based technique. Our experiments on the CARLA NoCrash benchmark demonstrate the effectiveness and efficiency of RLfOLD. Notably, even with a significantly smaller encoder and a single camera setup, RLfOLD surpasses state-of-the-art methods in this evaluation. These results, achieved with limited resources, highlight RLfOLD as a highly promising solution for real-world applications.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/4186f74da6d793c91c5ca4d8a10cf2f8638eade6.pdf",
        "venue": "AAAI Conference on Artificial Intelligence",
        "citationCount": 9,
        "score": 9.0
    },
    "eb6b79b00d43fa03945ae2477c60e79edaf72d28.pdf": {
        "title": "Human Knowledge Enhanced Reinforcement Learning for Mandatory Lane-Change of Autonomous Vehicles in Congested Traffic",
        "authors": [
            "Yanjun Huang",
            "Yuxiao Gu",
            "Kang Yuan",
            "Shuo Yang",
            "Tao Liu",
            "Hong Chen"
        ],
        "published_date": "2024",
        "abstract": "Mandatory lane-change scenarios are often challenging for autonomous vehicles in complex environments. In this paper, a human-knowledge-enhanced reinforcement learning (RL) method for lane-change decision making is proposed, where the human intelligence is integrated with RL algorithm in a multiple manner. First, this paper constructs a complex ramp-off scenario with congested traffic flow to help agents master lane-change skills. On the basis of the Twin Delayed Deep Deterministic policy gradient (TD3) algorithm, the human prior experience is encoded into reward function and safety constraints offline, and the online guidance of experts is also introduced into the framework, which can limit the unsafe exploration during the training process and provide demonstration in complex scenarios. The experimental results indicate that our method can effectively improve the training efficiency and outperform typical RL method and expert drivers, without specific requirements on the expertise. The proposed method can enhance the learning ability of RL based driving strategies.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/eb6b79b00d43fa03945ae2477c60e79edaf72d28.pdf",
        "venue": "IEEE Transactions on Intelligent Vehicles",
        "citationCount": 9,
        "score": 9.0
    },
    "b093a3fa79512c48524f81c754bddec7b16afb17.pdf": {
        "title": "Outcome-directed Reinforcement Learning by Uncertainty & Temporal Distance-Aware Curriculum Goal Generation",
        "authors": [
            "Daesol Cho",
            "Seungjae Lee",
            "H. J. Kim"
        ],
        "published_date": "2023",
        "abstract": "Current reinforcement learning (RL) often suffers when solving a challenging exploration problem where the desired outcomes or high rewards are rarely observed. Even though curriculum RL, a framework that solves complex tasks by proposing a sequence of surrogate tasks, shows reasonable results, most of the previous works still have difficulty in proposing curriculum due to the absence of a mechanism for obtaining calibrated guidance to the desired outcome state without any prior domain knowledge. To alleviate it, we propose an uncertainty&temporal distance-aware curriculum goal generation method for the outcome-directed RL via solving a bipartite matching problem. It could not only provide precisely calibrated guidance of the curriculum to the desired outcome states but also bring much better sample efficiency and geometry-agnostic curriculum goal proposal capability compared to previous curriculum RL methods. We demonstrate that our algorithm significantly outperforms these prior methods in a variety of challenging navigation tasks and robotic manipulation tasks in a quantitative and qualitative way.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/b093a3fa79512c48524f81c754bddec7b16afb17.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 16,
        "score": 8.0
    },
    "9a67ff1d46d691f7741822d7a13587a517b1be14.pdf": {
        "title": "MobileGUI-RL: Advancing Mobile GUI Agent through Reinforcement Learning in Online Environment",
        "authors": [
            "Yucheng Shi",
            "Wenhao Yu",
            "Zaitang Li",
            "Yonglin Wang",
            "Hongming Zhang",
            "Ninghao Liu",
            "Haitao Mi",
            "Dong Yu"
        ],
        "published_date": "2025",
        "abstract": "Recently, there has been a surge of vision-based GUI agents designed to automate everyday mobile and web tasks. These agents interpret raw GUI screenshots and autonomously decide where to click, scroll, or type, which bypasses handcrafted rules and app-specific APIs. However, most existing methods trained GUI agent in the offline environment using pre-collected trajectories. This approach limits scalability, causes overfitting to specific UI templates, and leads to brittle policies when faced with unseen environment. We present MobileGUI-RL, a scalable framework that trains GUI agent in online environment. MobileGUI-RL contains two key components. It (i) synthesizes a curriculum of learnable tasks through self-exploration and filtering, and (ii) adapts GRPO to GUI navigation with trajectory-aware advantages and composite rewards that balance task success and execution efficiency. Experiments on three online mobile-agent benchmarks show consistent gains, validating the effectiveness of our approach.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/9a67ff1d46d691f7741822d7a13587a517b1be14.pdf",
        "venue": "arXiv.org",
        "citationCount": 8,
        "score": 8.0
    },
    "134acf45a016fced57cc8f8e171eeb6e0015b0b8.pdf": {
        "title": "Open the Black Box: Step-based Policy Updates for Temporally-Correlated Episodic Reinforcement Learning",
        "authors": [
            "Ge Li",
            "Hongyi Zhou",
            "Dominik Roth",
            "Serge Thilges",
            "Fabian Otto",
            "Rudolf Lioutikov",
            "Gerhard Neumann"
        ],
        "published_date": "2024",
        "abstract": "Current advancements in reinforcement learning (RL) have predominantly focused on learning step-based policies that generate actions for each perceived state. While these methods efficiently leverage step information from environmental interaction, they often ignore the temporal correlation between actions, resulting in inefficient exploration and unsmooth trajectories that are challenging to implement on real hardware. Episodic RL (ERL) seeks to overcome these challenges by exploring in parameters space that capture the correlation of actions. However, these approaches typically compromise data efficiency, as they treat trajectories as opaque \\emph{black boxes}. In this work, we introduce a novel ERL algorithm, Temporally-Correlated Episodic RL (TCE), which effectively utilizes step information in episodic policy updates, opening the 'black box' in existing ERL methods while retaining the smooth and consistent exploration in parameter space. TCE synergistically combines the advantages of step-based and episodic RL, achieving comparable performance to recent ERL methods while maintaining data efficiency akin to state-of-the-art (SoTA) step-based RL.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/134acf45a016fced57cc8f8e171eeb6e0015b0b8.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 8,
        "score": 8.0
    },
    "a9c896060fa85f01f289baaad346e98e94dbed4c.pdf": {
        "title": "A Comprehensive Survey of Reinforcement Learning: From Algorithms to Practical Challenges",
        "authors": [
            "Majid Ghasemi",
            "Amir Hossein Moosavi",
            "Dariush Ebrahimi"
        ],
        "published_date": "2024",
        "abstract": "Reinforcement Learning (RL) has emerged as a powerful paradigm in Artificial Intelligence (AI), enabling agents to learn optimal behaviors through interactions with their environments. Drawing from the foundations of trial and error, RL equips agents to make informed decisions through feedback in the form of rewards or penalties. This paper presents a comprehensive survey of RL, meticulously analyzing a wide range of algorithms, from foundational tabular methods to advanced Deep Reinforcement Learning (DRL) techniques. We categorize and evaluate these algorithms based on key criteria such as scalability, sample efficiency, and suitability. We compare the methods in the form of their strengths and weaknesses in diverse settings. Additionally, we offer practical insights into the selection and implementation of RL algorithms, addressing common challenges like convergence, stability, and the exploration-exploitation dilemma. This paper serves as a comprehensive reference for researchers and practitioners aiming to harness the full potential of RL in solving complex, real-world problems.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/a9c896060fa85f01f289baaad346e98e94dbed4c.pdf",
        "venue": "",
        "citationCount": 8,
        "score": 8.0
    },
    "68de2fdc9b516eba96b9726dfa0e77ee66336605.pdf": {
        "title": "A Motion Planning Method for Visual Servoing Using Deep Reinforcement Learning in Autonomous Robotic Assembly",
        "authors": [
            "Zhen-yu Liu",
            "Ke Wang",
            "Daxin Liu",
            "Qide Wang",
            "Jianrong Tan"
        ],
        "published_date": "2023",
        "abstract": "Assembly positioning by visual servoing (VS) is a basis for autonomous robotic assembly. In practice, VS control suffers potential stability and convergence problems due to image and physical constraints, e.g., field of view constraints, image local minima, obstacle collisions, and occlusion. Therefore, this article proposes a novel deep reinforcement learning-based hybrid visual servoing (DRL-HVS) controller for motion planning of VS tasks. DRL-HVS controller takes current observed image features and camera pose as inputs, and the core parameters of hybrid VS are dynamically optimized using a deep deterministic policy gradient (DDPG) algorithm to obtain an optimal motion scheme, considering image/physical constraints and robot motion performance. In addition, an adaptive exploration strategy is proposed to further improve the training efficiency by adaptively tuning the exploration noise parameters. In this way, the offline pretrained DRL-HVS controller in the virtual environment, where the DDPG actor\u2013critic network is continuously optimized, can be quickly deployed to a real robot system for real-time control. Experiments based on an eye-in-hand VS system are conducted with a calibrated HIKVISION RGB camera mounted on the end-effector of a GSK-RB03A1 six degree-of-freedom (6-DoF) robot. Basic VS task experiments show that the proposed controller achieves better performance than the existing methods: the servoing time is 24% smaller than that of the five-dimensional VS method, a 100% success rate with the perturbed ranges of the initial position within 25 mm for translation and 20\u00b0 for rotation, and a 48% efficiency improvement. Moreover, a planetary gear component assembly process case study, where the robot aims to automatically put the gears on the gear shafts, is conducted to demonstrate the applicability of the proposed method in practice.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/68de2fdc9b516eba96b9726dfa0e77ee66336605.pdf",
        "venue": "IEEE/ASME transactions on mechatronics",
        "citationCount": 15,
        "score": 7.5
    },
    "01f35fa70fc881ab80206121738380c57f8d2074.pdf": {
        "title": "Relative Entropy Regularized Sample-Efficient Reinforcement Learning With Continuous Actions",
        "authors": [
            "Zhiwei Shang",
            "Renxing Li",
            "Chunhuang Zheng",
            "Huiyun Li",
            "Yunduan Cui"
        ],
        "published_date": "2023",
        "abstract": "In this article, a novel reinforcement learning (RL) approach, continuous dynamic policy programming (CDPP), is proposed to tackle the issues of both learning stability and sample efficiency in the current RL methods with continuous actions. The proposed method naturally extends the relative entropy regularization from the value function-based framework to the actor\u2013critic (AC) framework of deep deterministic policy gradient (DDPG) to stabilize the learning process in continuous action space. It tackles the intractable softmax operation over continuous actions in the critic by Monte Carlo estimation and explores the practical advantages of the Mellowmax operator. A Boltzmann sampling policy is proposed to guide the exploration of actor following the relative entropy regularized critic for superior learning capability, exploration efficiency, and robustness. Evaluated by several benchmark and real-robot-based simulation tasks, the proposed method illustrates the positive impact of the relative entropy regularization including efficient exploration behavior and stable policy update in RL with continuous action space and successfully outperforms the related baseline approaches in both sample efficiency and learning stability.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/01f35fa70fc881ab80206121738380c57f8d2074.pdf",
        "venue": "IEEE Transactions on Neural Networks and Learning Systems",
        "citationCount": 15,
        "score": 7.5
    },
    "5dce3f2cfe018ada4bebeaa15aac6fe1d01b18ba.pdf": {
        "title": "Imagine, Initialize, and Explore: An Effective Exploration Method in Multi-Agent Reinforcement Learning",
        "authors": [
            "Zeyang Liu",
            "Lipeng Wan",
            "Xinrui Yang",
            "Zhuoran Chen",
            "Xingyu Chen",
            "Xuguang Lan"
        ],
        "published_date": "2024",
        "abstract": "Effective exploration is crucial to discovering optimal strategies for multi-agent reinforcement learning (MARL) in complex coordination tasks. Existing methods mainly utilize intrinsic rewards to enable committed exploration or use role-based learning for decomposing joint action spaces instead of directly conducting a collective search in the entire action-observation space. However, they often face challenges obtaining specific joint action sequences to reach successful states in long-horizon tasks. To address this limitation, we propose Imagine, Initialize, and Explore (IIE), a novel method that offers a promising solution for efficient multi-agent exploration in complex scenarios. IIE employs a transformer model to imagine how the agents reach a critical state that can influence each other's transition functions. Then, we initialize the environment at this state using a simulator before the exploration phase. We formulate the imagination as a sequence modeling problem, where the states, observations, prompts, actions, and rewards are predicted autoregressively. The prompt consists of timestep-to-go, return-to-go, influence value, and one-shot demonstration, specifying the desired state and trajectory as well as guiding the action generation. By initializing agents at the critical states, IIE significantly increases the likelihood of discovering potentially important under-explored regions. Despite its simplicity, empirical results demonstrate that our method outperforms multi-agent exploration baselines on the StarCraft Multi-Agent Challenge (SMAC) and SMACv2 environments. Particularly, IIE shows improved performance in the sparse-reward SMAC tasks and produces more effective curricula over the initialized states than other generative methods, such as CVAE-GAN and diffusion models.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/5dce3f2cfe018ada4bebeaa15aac6fe1d01b18ba.pdf",
        "venue": "AAAI Conference on Artificial Intelligence",
        "citationCount": 7,
        "score": 7.0
    },
    "390ba2d438a33bf0add55e0e7b2831fc034db41f.pdf": {
        "title": "Sample-Efficient Reinforcement Learning With Temporal Logic Objectives: Leveraging the Task Specification to Guide Exploration",
        "authors": [
            "Y. Kantaros",
            "Jun Wang"
        ],
        "published_date": "2024",
        "abstract": "In this article, we address the problem of learning optimal control policies for systems with uncertain dynamics and high-level control objectives specified as linear temporal logic (LTL) formulas. Uncertainty is considered in the workspace structure and the outcomes of control decisions giving rise to an unknown Markov decision process (MDP). Existing reinforcement learning (RL) algorithms for LTL tasks typically rely on exploring a product MDP state-space uniformly (using e.g., an $\\epsilon$-greedy policy) compromising sample-efficiency. This issue becomes more pronounced as the rewards get sparser and the MDP size or the task complexity increase. In this article, we propose an accelerated RL algorithm that can learn control policies significantly faster than competitive approaches. Its sample-efficiency relies on a novel task-driven exploration strategy that biases exploration toward directions that may contribute to task satisfaction. We provide theoretical analysis and extensive comparative experiments demonstrating the sample-efficiency of the proposed method. The benefit of our method becomes more evident as the task complexity or the MDP size increases.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/390ba2d438a33bf0add55e0e7b2831fc034db41f.pdf",
        "venue": "IEEE Transactions on Automatic Control",
        "citationCount": 7,
        "score": 7.0
    },
    "085dddfa3105967d4b9f09b1cd0fa7725779faf1.pdf": {
        "title": "MCMC: Multi-Constrained Model Compression via One-Stage Envelope Reinforcement Learning",
        "authors": [
            "Siqi Li",
            "Jun Chen",
            "Shanqi Liu",
            "Chengrui Zhu",
            "Guanzhong Tian",
            "Yong Liu"
        ],
        "published_date": "2024",
        "abstract": "Model compression methods are being developed to bridge the gap between the massive scale of neural networks and the limited hardware resources on edge devices. Since most real-world applications deployed on resource-limited hardware platforms typically have multiple hardware constraints simultaneously, most existing model compression approaches that only consider optimizing one single hardware objective are ineffective. In this article, we propose an automated pruning method called multi-constrained model compression (MCMC) that allows for the optimization of multiple hardware targets, such as latency, floating point operations (FLOPs), and memory usage, while minimizing the impact on accuracy. Specifically, we propose an improved multi-objective reinforcement learning (MORL) algorithm, the one-stage envelope deep deterministic policy gradient (DDPG) algorithm, to determine the pruning strategy for neural networks. Our improved one-stage envelope DDPG algorithm reduces exploration time and offers greater flexibility in adjusting target priorities, enhancing its suitability for pruning tasks. For instance, on the visual geometry group (VGG)-16 network, our method achieved an 80% reduction in FLOPs, a <inline-formula> <tex-math notation=\"LaTeX\">$2.31\\times $ </tex-math></inline-formula> reduction in memory usage, and a <inline-formula> <tex-math notation=\"LaTeX\">$1.92\\times $ </tex-math></inline-formula> acceleration, with an accuracy improvement of 0.09% compared with the baseline. For larger datasets, such as ImageNet, we reduced FLOPs by 50% for MobileNet-V1, resulting in a <inline-formula> <tex-math notation=\"LaTeX\">$4.7\\times $ </tex-math></inline-formula> faster speed and <inline-formula> <tex-math notation=\"LaTeX\">$1.48\\times $ </tex-math></inline-formula> memory compression, while maintaining the same accuracy. When applied to edge devices, such as JETSON XAVIER NX, our method resulted in a 71% reduction in FLOPs for MobileNet-V1, leading to a <inline-formula> <tex-math notation=\"LaTeX\">$1.63\\times $ </tex-math></inline-formula> faster speed, <inline-formula> <tex-math notation=\"LaTeX\">$1.64\\times $ </tex-math></inline-formula> memory compression, and an accuracy improvement.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/085dddfa3105967d4b9f09b1cd0fa7725779faf1.pdf",
        "venue": "IEEE Transactions on Neural Networks and Learning Systems",
        "citationCount": 7,
        "score": 7.0
    },
    "99d0760520eaa1dc81b5531cd45101e00474bbd1.pdf": {
        "title": "De Novo Drug Design Using Transformer-Based Machine Translation and Reinforcement Learning of an Adaptive Monte Carlo Tree Search",
        "authors": [
            "Dony Ang",
            "Cyril Rakovski",
            "H. Atamian"
        ],
        "published_date": "2024",
        "abstract": "The discovery of novel therapeutic compounds through de novo drug design represents a critical challenge in the field of pharmaceutical research. Traditional drug discovery approaches are often resource intensive and time consuming, leading researchers to explore innovative methods that harness the power of deep learning and reinforcement learning techniques. Here, we introduce a novel drug design approach called drugAI that leverages the Encoder\u2013Decoder Transformer architecture in tandem with Reinforcement Learning via a Monte Carlo Tree Search (RL-MCTS) to expedite the process of drug discovery while ensuring the production of valid small molecules with drug-like characteristics and strong binding affinities towards their targets. We successfully integrated the Encoder\u2013Decoder Transformer architecture, which generates molecular structures (drugs) from scratch with the RL-MCTS, serving as a reinforcement learning framework. The RL-MCTS combines the exploitation and exploration capabilities of a Monte Carlo Tree Search with the machine translation of a transformer-based Encoder\u2013Decoder model. This dynamic approach allows the model to iteratively refine its drug candidate generation process, ensuring that the generated molecules adhere to essential physicochemical and biological constraints and effectively bind to their targets. The results from drugAI showcase the effectiveness of the proposed approach across various benchmark datasets, demonstrating a significant improvement in both the validity and drug-likeness of the generated compounds, compared to two existing benchmark methods. Moreover, drugAI ensures that the generated molecules exhibit strong binding affinities to their respective targets. In summary, this research highlights the real-world applications of drugAI in drug discovery pipelines, potentially accelerating the identification of promising drug candidates for a wide range of diseases.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/99d0760520eaa1dc81b5531cd45101e00474bbd1.pdf",
        "venue": "Pharmaceuticals",
        "citationCount": 7,
        "score": 7.0
    },
    "3da8d2e6a0f8d5d62cbaae18235235483144c6a0.pdf": {
        "title": "Toward Optimized In\u2010Memory Reinforcement Learning: Leveraging 1/f Noise of Synaptic Ferroelectric Field\u2010Effect\u2010Transistors for Efficient Exploration",
        "authors": [
            "Jangsaeng Kim",
            "Wonjun Shin",
            "Jiyong Yim",
            "Dongseok Kwon",
            "Dae-Hun Kwon",
            "Jong\u2010Ho Lee"
        ],
        "published_date": "2024",
        "abstract": "Reinforcement learning (RL), exhibiting outstanding performance in various fields, requires large amounts of data for high performance. While exploration techniques address this requirement, conventional exploration methods have limitations: complexity of hardware implementation and significant hardware burden. Herein, in\u2010memory RL systems leveraging intrinsic 1/f noise of synaptic ferroelectric field\u2010effect\u2010transistors (FeFETs) for efficient exploration are proposed. The electrical characteristics of fabricated FeFETs with low\u2010power operation capability verify their suitability for neuromorphic systems. The proposed system achieves comparable performance to the conventional exploration method without additional circuits. The intrinsic 1/f noise of the FeFETs facilitates efficient exploration and offers significant advantages: efficiency in hardware implementation and simplicity in adjusting the 1/f noise level for optimal performance. This approach effectively addresses the challenges of conventional exploration methods. The operation mechanism of the exploration method utilizing the 1/f noise is systematically analyzed. The proposed in\u2010memory RL system demonstrates robustness and reliability to the device\u2010to\u2010device variation and the initial conductance distribution. This work provides further insights into the exploration methods of RL, paving the way for advanced in\u2010memory RL systems.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/3da8d2e6a0f8d5d62cbaae18235235483144c6a0.pdf",
        "venue": "Advanced Intelligent Systems",
        "citationCount": 6,
        "score": 6.0
    },
    "086a4d45a9deef5a10ee4febcd4c92c95a6305de.pdf": {
        "title": "Improving Offline Reinforcement Learning With in-Sample Advantage Regularization for Robot Manipulation",
        "authors": [
            "Chengzhong Ma",
            "Deyu Yang",
            "Tianyu Wu",
            "Zeyang Liu",
            "Houxue Yang",
            "Xingyu Chen",
            "Xuguang Lan",
            "Nanning Zheng"
        ],
        "published_date": "2024",
        "abstract": "Offline reinforcement learning (RL) aims to learn the possible policy from a fixed dataset without real-time interactions with the environment. By avoiding the risky exploration of the robot, this approach is expected to significantly improve the robot\u2019s learning efficiency and safety. However, due to errors in value estimation from out-of-distribution actions, most offline RL algorithms constrain or regularize the policy to the actions contained within the dataset. The cost of such methods is the introduction of new hyperparameters and additional complexity. In this article, we aim to adapt offline RL to robotic manipulation with minimal changes and to avoid evaluating out-of-distribution actions as much as possible. Therefore, we improve offline RL with in-sample advantage regularization (ISAR). To mitigate the impact of unseen actions, the ISAR learns the state-value function only with the dataset sample to regress the optimal action-value function. Our method calculates the advantage function of action-state pairs based on in-sample value estimation and adds a behavior cloning (BC) regularization term in the policy update. This improves sample efficiency with minimal changes, resulting in a simple and easy-to-implement method. The experiments of the D4RL robot benchmark and multigoal sparse rewards robotic tasks show that the ISAR achieves excellent performance comparable to current state-of-the-art algorithms without the need for complex parameter tuning and too much training time. In addition, we demonstrate the effectiveness of our method on a real-world robot platform.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/086a4d45a9deef5a10ee4febcd4c92c95a6305de.pdf",
        "venue": "IEEE Transactions on Neural Networks and Learning Systems",
        "citationCount": 6,
        "score": 6.0
    },
    "3dee83a4b0fadde414e00ff350940303eb859be1.pdf": {
        "title": "An Improved Distributed Maximum Power Point Tracking Technique in Photovoltaic Systems Based on Reinforcement Learning Algorithm",
        "authors": [
            "Zhihong Ge",
            "Xingshuo Li",
            "Fei Xu",
            "Haimeng Wu",
            "Ruichi Wang",
            "Shuye Ding"
        ],
        "published_date": "2024",
        "abstract": "The mismatch problem is commonly happened in photovoltaic systems due to partial shading conditions. Distributed maximum power point tracking architectures can be used to solve such problem. Reinforcement learning (RL) method, which is one of the advanced artificial intelligence methods is proposed to improve the tracking speed. However, the drawbacks, such as the lack of limited adaptability and exploration\u2013exploitation tradeoff theory, make the RL method low in efficiency. Therefore, this article combines the Beta method and $\\varepsilon$\u2013greedy algorithm with the RL method to address this problem. The simulation and experimental tests have been carried out and the result shows the efficiency of the proposed RL method is up to 96.85%, which verifies the superiority of the proposed scheme.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/3dee83a4b0fadde414e00ff350940303eb859be1.pdf",
        "venue": "IEEE Journal of Emerging and Selected Topics in Industrial Electronics",
        "citationCount": 6,
        "score": 6.0
    },
    "22c1ec46a81e9db6194b8784f4fe431f71953757.pdf": {
        "title": "Adaptive 3D UI Placement in Mixed Reality Using Deep Reinforcement Learning",
        "authors": [
            "Feiyu Lu",
            "Mengyu Chen",
            "Hsiang Hsu",
            "Pranav Deshpande",
            "Cheng Yao Wang",
            "Blair MacIntyre"
        ],
        "published_date": "2024",
        "abstract": "Mixed Reality (MR) could assist users\u2019 tasks by continuously integrating virtual content with their view of the physical environment. However, where and how to place these content to best support the users has been a challenging problem due to the dynamic nature of MR experiences. In contrast to prior work that investigates optimization-based methods, we are exploring how reinforcement learning (RL) could assist with continuous 3D content placement that is aware of users\u2019 poses and their surrounding environments. Through an initial exploration and preliminary evaluation, our results demonstrate the potential of RL to position content that maximizes the reward for users on the go. We further identify future directions for research that could harness the power of RL for personalized and optimized UI and content placement in MR.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/22c1ec46a81e9db6194b8784f4fe431f71953757.pdf",
        "venue": "CHI Extended Abstracts",
        "citationCount": 6,
        "score": 6.0
    },
    "9c7209f3d6b9bf362ea4d34730f34cf8511967f4.pdf": {
        "title": "EasySO: Exploration-enhanced Reinforcement Learning for Logic Synthesis Sequence Optimization and a Comprehensive RL Environment",
        "authors": [
            "Jianyong Yuan",
            "Peiyu Wang",
            "Junjie Ye",
            "Mingxuan Yuan",
            "Jianye Hao",
            "Junchi Yan"
        ],
        "published_date": "2023",
        "abstract": "Optimizing the quality of results (QoR) of a circuit during the logic synthesis (LS) phase in chip design is critical yet challenging. While most existing methods often mitigate the computational hardness by restricting the action space to a small set of operators and fixing the operator's parameters, they are susceptible to local minima and may not meet the high demand from industrial cases. In this paper, we develop a more comprehensive optimization approach via sample-efficient reinforcement learning (RL). Specifically, we first build a complete logic synthesis-RL environment, where the action space consists of three types of operators: logic optimization, technology mapping, and post-mapping, along with their associated continuouslbinary parameters for optimization as well. Based on this environment, we devise a hybrid proximal policy optimization (PPO) model to handle both discrete operators and parameters and design a distributed architecture to improve sample collection efficiency. Furthermore, we devise a dynamic exploration module to improve the exploration efficiency under the constraint of limited samples. We term our method as Exploration-enhanced RL for Logic Synthesis Sequence Optimization(EasySO). Results on the EPFL benchmark show that our method significantly outperforms current state-of-the-art models based on Bayesian optimization (BO) and the previous RL-based methods. Compared to resyn2, our EasySO achieves an average of 25.4% LUT-6 count optimization without sacrificing level values. Moreover, as of the time for this submission, we rank 26 first places among 40 optimization targets in the EPFL competition.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/9c7209f3d6b9bf362ea4d34730f34cf8511967f4.pdf",
        "venue": "2023 IEEE/ACM International Conference on Computer Aided Design (ICCAD)",
        "citationCount": 11,
        "score": 5.5
    },
    "200726cba07dec06a56ff46aa38836e9730a23a2.pdf": {
        "title": "Rethinking Population-assisted Off-policy Reinforcement Learning",
        "authors": [
            "Bowen Zheng",
            "Ran Cheng"
        ],
        "published_date": "2023",
        "abstract": "While off-policy reinforcement learning (RL) algorithms are sample efficient due to gradient-based updates and data reuse in the replay buffer, they struggle with convergence to local optima due to limited exploration. On the other hand, population-based algorithms offer a natural exploration strategy, but their heuristic black-box operators are inefficient. Recent algorithms have integrated these two methods, connecting them through a shared replay buffer. However, the effect of using diverse data from population optimization iterations on off-policy RL algorithms has not been thoroughly investigated. In this paper, we first analyze the use of off-policy RL algorithms in combination with population-based algorithms, showing that the use of population data could introduce an overlooked error and harm performance. To test this, we propose a uniform and scalable training design and conduct experiments on our tailored framework in robot locomotion tasks from the OpenAI gym. Our results substantiate that using population data in off-policy RL can cause instability during training and even degrade performance. To remedy this issue, we further propose a double replay buffer design that provides more on-policy data and show its effectiveness through experiments. Our results offer practical insights for training these hybrid methods.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/200726cba07dec06a56ff46aa38836e9730a23a2.pdf",
        "venue": "Annual Conference on Genetic and Evolutionary Computation",
        "citationCount": 11,
        "score": 5.5
    },
    "3c483c11f5fd9234576a93aea71a7ec1435b8514.pdf": {
        "title": "FlagVNE: A Flexible and Generalizable Reinforcement Learning Framework for Network Resource Allocation",
        "authors": [
            "Tianfu Wang",
            "Qilin Fan",
            "Chao Wang",
            "Long Yang",
            "Leilei Ding",
            "Nicholas Jing Yuan",
            "Hui Xiong"
        ],
        "published_date": "2024",
        "abstract": "Virtual network embedding (VNE) is an essential resource allocation task in network virtualization, aiming to map virtual network requests (VNRs) onto physical infrastructure. Reinforcement learning (RL) has recently emerged as a promising solution to this problem. However, existing RL-based VNE methods are limited by the unidirectional action design and one-size-fits-all training strategy, resulting in restricted searchability and generalizability. In this paper, we propose a flexible and generalizable RL framework for VNE, named FlagVNE. Specifically, we design a bidirectional action-based Markov decision process model that enables the joint selection of virtual and physical nodes, thus improving the exploration flexibility of solution space. To tackle the expansive and dynamic action space, we design a hierarchical decoder to generate adaptive action probability distributions and ensure high training efficiency. Furthermore, to overcome the generalization issue for varying VNR sizes, we propose a meta-RL-based training method with a curriculum scheduling strategy, facilitating specialized policy training for each VNR size. Finally, extensive experimental results show the effectiveness of FlagVNE across multiple key metrics. Our code is available at https://github.com/GeminiLight/flag-vne.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/3c483c11f5fd9234576a93aea71a7ec1435b8514.pdf",
        "venue": "International Joint Conference on Artificial Intelligence",
        "citationCount": 5,
        "score": 5.0
    },
    "d06737f9395e592f35ef251e09bea1c18037b096.pdf": {
        "title": "Random Latent Exploration for Deep Reinforcement Learning",
        "authors": [
            "Srinath Mahankali",
            "Zhang-Wei Hong",
            "Ayush Sekhari",
            "Alexander Rakhlin",
            "Pulkit Agrawal"
        ],
        "published_date": "2024",
        "abstract": "We introduce Random Latent Exploration (RLE), a simple yet effective exploration strategy in reinforcement learning (RL). On average, RLE outperforms noise-based methods, which perturb the agent's actions, and bonus-based exploration, which rewards the agent for attempting novel behaviors. The core idea of RLE is to encourage the agent to explore different parts of the environment by pursuing randomly sampled goals in a latent space. RLE is as simple as noise-based methods, as it avoids complex bonus calculations but retains the deep exploration benefits of bonus-based methods. Our experiments show that RLE improves performance on average in both discrete (e.g., Atari) and continuous control tasks (e.g., Isaac Gym), enhancing exploration while remaining a simple and general plug-in for existing RL algorithms. Project website and code: https://srinathm1359.github.io/random-latent-exploration",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/d06737f9395e592f35ef251e09bea1c18037b096.pdf",
        "venue": "International Conference on Machine Learning",
        "citationCount": 5,
        "score": 5.0
    },
    "3ef01975a45bcf78120d76c1bc73e8ec12e213bf.pdf": {
        "title": "Breadth-First Exploration on Adaptive Grid for Reinforcement Learning",
        "authors": [
            "Youngsik Yoon",
            "Gangbok Lee",
            "Sungsoo Ahn",
            "Jungseul Ok"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/3ef01975a45bcf78120d76c1bc73e8ec12e213bf.pdf",
        "venue": "International Conference on Machine Learning",
        "citationCount": 5,
        "score": 5.0
    },
    "39d2839aa4c3d8c0e64553891fe98ba261703154.pdf": {
        "title": "More Efficient Randomized Exploration for Reinforcement Learning via Approximate Sampling",
        "authors": [
            "Haque Ishfaq",
            "Yixin Tan",
            "Yu Yang",
            "Qingfeng Lan",
            "Jianfeng Lu",
            "A. R. Mahmood",
            "D. Precup",
            "Pan Xu"
        ],
        "published_date": "2024",
        "abstract": "Thompson sampling (TS) is one of the most popular exploration techniques in reinforcement learning (RL). However, most TS algorithms with theoretical guarantees are difficult to implement and not generalizable to Deep RL. While the emerging approximate sampling-based exploration schemes are promising, most existing algorithms are specific to linear Markov Decision Processes (MDP) with suboptimal regret bounds, or only use the most basic samplers such as Langevin Monte Carlo. In this work, we propose an algorithmic framework that incorporates different approximate sampling methods with the recently proposed Feel-Good Thompson Sampling (FGTS) approach (Zhang, 2022; Dann et al., 2021), which was previously known to be computationally intractable in general. When applied to linear MDPs, our regret analysis yields the best known dependency of regret on dimensionality, surpassing existing randomized algorithms. Additionally, we provide explicit sampling complexity for each employed sampler. Empirically, we show that in tasks where deep exploration is necessary, our proposed algorithms that combine FGTS and approximate sampling perform significantly better compared to other strong baselines. On several challenging games from the Atari 57 suite, our algorithms achieve performance that is either better than or on par with other strong baselines from the deep RL literature.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/39d2839aa4c3d8c0e64553891fe98ba261703154.pdf",
        "venue": "RLJ",
        "citationCount": 5,
        "score": 5.0
    },
    "cb7ea0176eec1f809f3bc3a34aeb279d44dda612.pdf": {
        "title": "Global Reinforcement Learning: Beyond Linear and Convex Rewards via Submodular Semi-gradient Methods",
        "authors": [
            "Ric De Santi",
            "Manish Prajapat",
            "Andreas Krause"
        ],
        "published_date": "2024",
        "abstract": "In classic Reinforcement Learning (RL), the agent maximizes an additive objective of the visited states, e.g., a value function. Unfortunately, objectives of this type cannot model many real-world applications such as experiment design, exploration, imitation learning, and risk-averse RL to name a few. This is due to the fact that additive objectives disregard interactions between states that are crucial for certain tasks. To tackle this problem, we introduce Global RL (GRL), where rewards are globally defined over trajectories instead of locally over states. Global rewards can capture negative interactions among states, e.g., in exploration, via submodularity, positive interactions, e.g., synergetic effects, via supermodularity, while mixed interactions via combinations of them. By exploiting ideas from submodular optimization, we propose a novel algorithmic scheme that converts any GRL problem to a sequence of classic RL problems and solves it efficiently with curvature-dependent approximation guarantees. We also provide hardness of approximation results and empirically demonstrate the effectiveness of our method on several GRL instances.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/cb7ea0176eec1f809f3bc3a34aeb279d44dda612.pdf",
        "venue": "International Conference on Machine Learning",
        "citationCount": 5,
        "score": 5.0
    },
    "b31c76815615c16cc8505dbb38d2921f921c029d.pdf": {
        "title": "A Coverage-Guided Fuzzing Method for Automatic Software Vulnerability Detection Using Reinforcement Learning-Enabled Multi-Level Input Mutation",
        "authors": [
            "Van-Hau Pham",
            "Do Thi Thu Hien",
            "Nguyen Phuc Chuong",
            "Pham Thanh Thai",
            "Phan The Duy"
        ],
        "published_date": "2024",
        "abstract": "Fuzzing is a popular and effective software testing technique that automatically generates or modifies inputs to test the stability and vulnerabilities of a software system, which has been widely applied and improved by security researchers and experts. The goal of fuzzing is to uncover potential weaknesses in software by providing unexpected and invalid inputs to the target program to monitor its behavior and identify errors or unintended outcomes. Recently, researchers have also integrated promising machine learning algorithms, such as reinforcement learning, to enhance the fuzzing process. Reinforcement learning (RL) has been proven to be able to improve the effectiveness of fuzzing by selecting and prioritizing transformation actions with higher coverage, which reduces the required effort to uncover vulnerabilities. However, RL-based fuzzing models also encounter certain limitations, including an imbalance between exploitation and exploration. In this study, we propose a coverage-guided RL-based fuzzing model that enhances grey-box fuzzing, in which we leverage deep Q-learning to predict and select input variations to maximize code coverage and use code coverage as a reward signal. This model is complemented by simple input selection and scheduling algorithms that promote a more balanced approach to exploiting and exploring software. Furthermore, we introduce a multi-level input mutation model combined with RL to create a sequence of actions for comprehensive input variation. The proposed model is compared to other fuzzing tools in testing various real-world programs, where the results indicate a notable enhancement in terms of code coverage, discovered paths, and execution speed of our solution.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/b31c76815615c16cc8505dbb38d2921f921c029d.pdf",
        "venue": "IEEE Access",
        "citationCount": 5,
        "score": 5.0
    },
    "c1844cda42b3732a5576d05bb6e007eb1db00919.pdf": {
        "title": "Incremental Reinforcement Learning with Dual-Adaptive \u03b5-Greedy Exploration",
        "authors": [
            "Wei Ding",
            "Siyang Jiang",
            "Hsi-Wen Chen",
            "Ming-Syan Chen"
        ],
        "published_date": "2023",
        "abstract": "Reinforcement learning (RL) has achieved impressive performance in various domains. However, most RL frameworks oversimplify the problem by assuming a fixed-yet-known environment and often have difficulty being generalized to real-world scenarios. In this paper, we address a new challenge with a more realistic setting, Incremental Reinforcement Learning, where the search space of the Markov Decision Process continually expands. While previous methods usually suffer from the lack of efficiency in exploring the unseen transitions, especially with increasing search space, we present a new exploration framework named Dual-Adaptive \u03f5-greedy Exploration (DAE) to address the challenge of Incremental RL. Specifically, DAE employs a Meta Policy and an Explorer to avoid redundant computation on those sufficiently\nlearned samples. Furthermore, we release a testbed based on a synthetic environment and the Atari benchmark to validate the effectiveness of any exploration algorithms under Incremental RL. Experimental results demonstrate that the proposed framework can efficiently learn the unseen transitions in new environments, leading to notable performance improvement, i.e., an average of more than 80%, over eight baselines examined.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/c1844cda42b3732a5576d05bb6e007eb1db00919.pdf",
        "venue": "AAAI Conference on Artificial Intelligence",
        "citationCount": 10,
        "score": 5.0
    },
    "8af5e79310ec1d8529eba38705e5f29dce789b00.pdf": {
        "title": "Advancements in Reinforcement Learning Algorithms for Autonomous Systems",
        "authors": [
            "Jesu Narkarunai Arasu Malaiyappan",
            "Sai Mani Krishna Sistla",
            "Jawaharbabu Jeyaraman"
        ],
        "published_date": "2024",
        "abstract": "Reinforcement learning, often known as RL, has developed as a strong paradigm to teach autonomous software agents to make choices in contexts that are both complicated and dynamic. This abstract investigates recent developments and uses of RL in a variety of fields, showing both its transformational potential and the constraints that it faces at present. Recent developments in reinforcement learning (RL) algorithms, in particular deep reinforcement learning (DRL), have made it possible to make major advancements in autonomous decision- making tasks. DRL algorithms can learn complicated representations of state-action spaces by using deep neural networks. This allows for more efficient exploration and exploitation methods to be implemented. Additionally, advancements in algorithmic enhancements, such as prioritized experience replay and distributional reinforcement learning, have improved the stability and sample efficiency of reinforcement learning algorithms, which has made it possible for these algorithms to be used in real-world applications. Robotics, autonomous cars, game playing, finance, and healthcare are just a few of the many fields that may benefit from the use of RL. In the field of robotics, reinforcement learning (RL) makes it possible for autonomous agents to learn how to navigate, manipulate, and move about in environments that are both complicated and unstructured. To improve both safety and efficiency on the road, autonomous cars make use of reinforcement learning (RL) to make decisions in dynamic traffic situations. In finance, RL algorithms are used for portfolio optimization, algorithmic trading, and risk management. These applications serve to improve investment techniques and decision-making procedures. Furthermore, in the field of healthcare, RL supports individualized treatment planning, clinical decision support, and medical image analysis, which enables physicians to provide patients with care that is specifically suited to their needs. Despite the promising improvements and applications, RL is still confronted with several difficulties that restrict its capacity to be widely adopted and scaled. Among these problems are the inefficiency of the sample, the trade-offs between exploration and exploitation, concerns about safety and dependability, and the need for explainability and interpretability in decision-making processes. To effectively address these difficulties, it is necessary to engage in collaborative efforts across several disciplines, conduct research on algorithmic developments, and establish extensive assessment frameworks (Anon, 2022).",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/8af5e79310ec1d8529eba38705e5f29dce789b00.pdf",
        "venue": "International Journal of Innovative Science and Research Technology",
        "citationCount": 5,
        "score": 5.0
    },
    "5221ba291d5901f950220f50d289d5e01d81b0c4.pdf": {
        "title": "Digital twin-enabled adaptive scheduling strategy based on deep reinforcement learning",
        "authors": [
            "Xuemei Gan",
            "Ying Zuo",
            "Ansi Zhang",
            "Shaobo Li",
            "Fei Tao"
        ],
        "published_date": "2023",
        "abstract": "",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/5221ba291d5901f950220f50d289d5e01d81b0c4.pdf",
        "venue": "Science China Technological Sciences",
        "citationCount": 8,
        "score": 4.0
    },
    "8ae6c7cff8bb2d766f5f9d3585cd262032378b33.pdf": {
        "title": "Ensemble-based Offline-to-Online Reinforcement Learning: From Pessimistic Learning to Optimistic Exploration",
        "authors": [
            "Kai-Wen Zhao",
            "Yi Ma",
            "Jinyi Liu",
            "Yan Zheng",
            "Zhaopeng Meng"
        ],
        "published_date": "2023",
        "abstract": "",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/8ae6c7cff8bb2d766f5f9d3585cd262032378b33.pdf",
        "venue": "arXiv.org",
        "citationCount": 8,
        "score": 4.0
    },
    "dad0fcbc6f2c9b7c4d7b7b1d4513d94d57ea63c6.pdf": {
        "title": "Federated Reinforcement Learning for Resource Allocation in V2X Networks",
        "authors": [
            "Kaidi Xu",
            "Shenglong Zhou",
            "Geoffrey Ye Li"
        ],
        "published_date": "2023",
        "abstract": "Resource allocation significantly impacts the performance of vehicle-to-everything (V2X) networks. Most existing algorithms for resource allocation are based on optimization or machine learning (e.g., reinforcement learning). In this paper, we explore resource allocation in a V2X network under the framework of federated reinforcement learning (FRL). On one hand, the usage of RL overcomes many challenges from the model-based optimization schemes. On the other hand, federated learning (FL) enables agents to deal with a number of practical issues, such as privacy, communication overhead, and exploration efficiency. The framework of FRL is then implemented by the in-exact alternative direction method of multipliers (ADMM), where subproblems are solved approximately using policy gradients and their second moments. The developed algorithm, FRLPGiA, has a nice numerical performance compared with some baseline methods for solving the resource allocation problem in a V2X network.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/dad0fcbc6f2c9b7c4d7b7b1d4513d94d57ea63c6.pdf",
        "venue": "IEEE Vehicular Technology Conference",
        "citationCount": 8,
        "score": 4.0
    },
    "ae47e4e2a4b749543c4de8624049c1d368cbc859.pdf": {
        "title": "Reinforcement learning with modified exploration strategy for mobile robot path planning",
        "authors": [
            "Nesrine Khlif",
            "Khraief-Hadded Nahla",
            "Belghith Safya"
        ],
        "published_date": "2023",
        "abstract": "Abstract Driven by the remarkable developments we have observed in recent years, path planning for mobile robots is a difficult part of robot navigation. Artificial intelligence applied to mobile robotics is also a distinct challenge; reinforcement learning (RL) is one of the most used algorithms in robotics. The exploration-exploitation dilemma is a motivating challenge for the performance of RL algorithms. The problem is balancing exploitation and exploration, as too much exploration leads to a decrease in cumulative reward, while too much exploitation locks the agent in a local optimum. This paper proposes a new path planning method for mobile robot based on Q-learning with an improved exploration strategy. In addition, a comparative study of Boltzmann distribution and \n$\\epsilon$\n -greedy politics is presented. Through simulations, the better performance of the proposed method in terms of execution time, path length, and cost function is confirmed.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/ae47e4e2a4b749543c4de8624049c1d368cbc859.pdf",
        "venue": "Robotica (Cambridge. Print)",
        "citationCount": 7,
        "score": 3.5
    },
    "c6ed1f7f478f9d004d5f6b9783df79f82d0d1464.pdf": {
        "title": "Optimistic Exploration in Reinforcement Learning Using Symbolic Model Estimates",
        "authors": [
            "S. Sreedharan",
            "Michael Katz"
        ],
        "published_date": "2023",
        "abstract": "",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/c6ed1f7f478f9d004d5f6b9783df79f82d0d1464.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 7,
        "score": 3.5
    },
    "2807f9c666335946113fb11dccadf36f8d78b772.pdf": {
        "title": "A Simple Unified Uncertainty-Guided Framework for Offline-to-Online Reinforcement Learning",
        "authors": [
            "Siyuan Guo",
            "Yanchao Sun",
            "Jifeng Hu",
            "Sili Huang",
            "Hechang Chen",
            "Haiyin Piao",
            "Lichao Sun",
            "Yi Chang"
        ],
        "published_date": "2023",
        "abstract": "Offline reinforcement learning (RL) provides a promising solution to learning an agent fully relying on a data-driven paradigm. However, constrained by the limited quality of the offline dataset, its performance is often sub-optimal. Therefore, it is desired to further finetune the agent via extra online interactions before deployment. Unfortunately, offline-to-online RL can be challenging due to two main challenges: constrained exploratory behavior and state-action distribution shift. To this end, we propose a Simple Unified uNcertainty-Guided (SUNG) framework, which naturally unifies the solution to both challenges with the tool of uncertainty. Specifically, SUNG quantifies uncertainty via a VAE-based state-action visitation density estimator. To facilitate efficient exploration, SUNG presents a practical optimistic exploration strategy to select informative actions with both high value and high uncertainty. Moreover, SUNG develops an adaptive exploitation method by applying conservative offline RL objectives to high-uncertainty samples and standard online RL objectives to low-uncertainty samples to smoothly bridge offline and online stages. SUNG achieves state-of-the-art online finetuning performance when combined with different offline RL methods, across various environments and datasets in D4RL benchmark.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/2807f9c666335946113fb11dccadf36f8d78b772.pdf",
        "venue": "arXiv.org",
        "citationCount": 7,
        "score": 3.5
    },
    "9eed36fb9b9bbb97579953d5e303dc0cf6c70a58.pdf": {
        "title": "Safe Reinforcement Learning With Dead-Ends Avoidance and Recovery",
        "authors": [
            "Xiao Zhang",
            "Hai Zhang",
            "Hongtu Zhou",
            "Chang Huang",
            "Di Zhang",
            "Chen Ye",
            "Junqiao Zhao"
        ],
        "published_date": "2023",
        "abstract": "Safety is one of the main challenges in applying reinforcement learning to tasks in realistic environments. To ensure safety during and after the training process, existing methods tend to adopt overly conservative policies to avoid unsafe situations. However, an overly conservative policy severely hinders exploration. In this letter, we propose a method to construct a boundary that discriminates between safe and unsafe states. The boundary we construct is equivalent to distinguishing dead-end states, indicating the maximum extent to which safe exploration is guaranteed, and thus has a minimum limitation on exploration. Similar to Recovery Reinforcement Learning, we utilize a decoupled RL framework to learn two policies, (1) a task policy that only considers improving the task performance, and (2) a recovery policy that maximizes safety. The recovery policy and a corresponding safety critic are pre-trained on an offline dataset, in which the safety critic evaluates the upper bound of safety in each state as awareness of environmental safety for the agent. During online training, a behavior correction mechanism is adopted, ensuring the agent interacts with the environment using safe actions only. Finally, experiments of continuous control tasks demonstrate that our approach has better task performance with fewer safety violations than state-of-the-art algorithms.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/9eed36fb9b9bbb97579953d5e303dc0cf6c70a58.pdf",
        "venue": "IEEE Robotics and Automation Letters",
        "citationCount": 7,
        "score": 3.5
    },
    "9c9a527ad885e664a31f0b70ec5f838d0c6ebfa6.pdf": {
        "title": "TA-Explore: Teacher-Assisted Exploration for Facilitating Fast Reinforcement Learning",
        "authors": [
            "Ali Beikmohammadi",
            "S. Magn\u00fasson"
        ],
        "published_date": "2023",
        "abstract": "",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/9c9a527ad885e664a31f0b70ec5f838d0c6ebfa6.pdf",
        "venue": "Adaptive Agents and Multi-Agent Systems",
        "citationCount": 6,
        "score": 3.0
    },
    "8ca9a74503c240b2746e351995ee0415657f1cd0.pdf": {
        "title": "Reinforcement Learning by Guided Safe Exploration",
        "authors": [
            "Qisong Yang",
            "T. D. Sim\u00e3o",
            "N. Jansen",
            "Simon Tindemans",
            "M. Spaan"
        ],
        "published_date": "2023",
        "abstract": "Safety is critical to broadening the application of reinforcement learning (RL). Often, we train RL agents in a controlled environment, such as a laboratory, before deploying them in the real world. However, the real-world target task might be unknown prior to deployment. Reward-free RL trains an agent without the reward to adapt quickly once the reward is revealed. We consider the constrained reward-free setting, where an agent (the guide) learns to explore safely without the reward signal. This agent is trained in a controlled environment, which allows unsafe interactions and still provides the safety signal. After the target task is revealed, safety violations are not allowed anymore. Thus, the guide is leveraged to compose a safe behaviour policy. Drawing from transfer learning, we also regularize a target policy (the student) towards the guide while the student is unreliable and gradually eliminate the influence of the guide as training progresses. The empirical analysis shows that this method can achieve safe transfer learning and helps the student solve the target task faster.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/8ca9a74503c240b2746e351995ee0415657f1cd0.pdf",
        "venue": "European Conference on Artificial Intelligence",
        "citationCount": 6,
        "score": 3.0
    },
    "06ffbf40c29daa4f6b4c5025d96ab1c27c0ea117.pdf": {
        "title": "Forest Fire Localization: From Reinforcement Learning Exploration to a Dynamic Drone Control",
        "authors": [
            "Jonatan Alvarez",
            "Assia Belbachir",
            "Faiza Belbachir",
            "Jamy Chahal",
            "Abdelhak Goudjil",
            "Johvany Gustave",
            "Ayb\u00fcke \u00d6zt\u00fcrk Suri"
        ],
        "published_date": "2023",
        "abstract": "",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/06ffbf40c29daa4f6b4c5025d96ab1c27c0ea117.pdf",
        "venue": "Journal of Intelligent and Robotic Systems",
        "citationCount": 6,
        "score": 3.0
    },
    "7f437f4af59ff994d97482ee1c12aaeb4b310e85.pdf": {
        "title": "Implicit Posteriori Parameter Distribution Optimization in Reinforcement Learning",
        "authors": [
            "Tianyi Li",
            "Gen-ke Yang",
            "Jian Chu"
        ],
        "published_date": "2023",
        "abstract": "Efficient and intelligent exploration remains a major challenge in the field of deep reinforcement learning (DRL). Bayesian inference with a distributional representation is usually an effective way to improve the exploration ability of the RL agent. However, when optimizing Bayesian neural networks (BNNs), most algorithms need to specify an explicit parameter distribution such as a multivariate Gaussian distribution. This may reduce the flexibility of model representation and affect the algorithm performance. Therefore, to improve sample efficiency and exploration based on Bayesian methods, we propose a novel implicit posteriori parameter distribution optimization (IPPDO) algorithm. First, we adopt a distributional perspective on the parameter and model it with an implicit distribution, which is approximated by generative models. Each model corresponds to a learned latent space, providing structured stochasticity for each layer in the network. Next, to make it possible to optimize an implicit posteriori parameter distribution, we build an energy-based model (EBM) with value function to represent the implicit distribution which is not constrained by any analytic density function. Then, we design a training algorithm based on amortized Stein variational gradient descent (SVGD) to improve the model learning efficiency. We compare IPPDO with other prevailing DRL algorithms on the OpenAI Gym, MuJoCo, and Box2D platforms. Experiments on various tasks demonstrate that the proposed algorithm can represent the parameter uncertainty implicitly for a learned policy and can consistently outperform competing approaches.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/7f437f4af59ff994d97482ee1c12aaeb4b310e85.pdf",
        "venue": "IEEE Transactions on Cybernetics",
        "citationCount": 6,
        "score": 3.0
    },
    "839395c4823ac8fff990485e7ce54e53c94bae6b.pdf": {
        "title": "Active Gamma-Ray Log Pattern Localization With Distributionally Robust Reinforcement Learning",
        "authors": [
            "Yuan Zi",
            "Lei Fan",
            "Xuqing Wu",
            "Jiefu Chen",
            "Shirui Wang",
            "Zhu Han"
        ],
        "published_date": "2023",
        "abstract": "Accurately localizing 1-D signal patterns, such as Gamma-ray well-log depth matching, is crucial in the oilfield service industry as it directly affects the quality of oil and gas exploration. However, traditional methods such as well-log curve analysis and pattern hand-picking matching are labor-intensive and heavily rely on human expertise, leading to inconsistent results. Although attempts have been made to automate this process, challenges such as low computational performance, nonrobustness, and nongeneralization remain unsolved. To address these challenges, we have developed a data-driven AI system that learns an active signal pattern localization strategy inspired by human attention. Our artificial intelligence system uses an offline reinforcement learning (RL) framework as its central component, which solves a highly abstracted Markov decision process (MDP) problem via offline training on human-labeled historical data. The RL agent uses top-down reasoning to determine the location of target signal fragments by deforming a bounding window using simple transformation actions. To overcome distribution shifts between logged data and real and ensure generalization, we propose a discrete distributionally robust soft actor-critic (SAC) RL framework (DRSAC-Discrete) to solve the MDP problem under uncertainty. By exploring unfamiliar environments in a restrictive manner, the DRSAC-Discrete algorithm provides a safe solution that can be used when data is limited during the early stage of this industrial application. We evaluated the RL-based localization system on augmented field Gamma-ray well-log datasets, and the results showed promising localization capability. Furthermore, the DRSAC-Discrete algorithm demonstrated relatively robust performance guarantees when facing data shortage.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/839395c4823ac8fff990485e7ce54e53c94bae6b.pdf",
        "venue": "IEEE Transactions on Geoscience and Remote Sensing",
        "citationCount": 5,
        "score": 2.5
    },
    "7825ea27ec1762f6ac41347603535500bcd121f7.pdf": {
        "title": "Reducing the Learning Time of Reinforcement Learning for the Supervisory Control of Discrete Event Systems",
        "authors": [
            "Junjun Yang",
            "Kaige Tan",
            "Lei Feng",
            "Ahmed M. El-Sherbeeny",
            "Zhiwu Li"
        ],
        "published_date": "2023",
        "abstract": "Reinforcement learning (RL) can obtain the supervisory controller for discrete-event systems modeled by finite automata and temporal logic. The published methods often have two limitations. First, a large number of training data are required to learn the RL controller. Second, the RL algorithms do not consider uncontrollable events, which are essential for supervisory control theory (SCT). To address the limitations, we first apply SCT to find the supervisors for the specifications modeled by automata. These supervisors remove illegal training data violating these specifications and hence reduce the exploration space of the RL algorithm. For the remaining specifications modeled by temporal logic, the RL algorithm is applied to search for the optimal control decision within the confined exploration space. Uncontrollable events are considered by the RL algorithm as uncertainties in the plant model. The proposed method can obtain a nonblocking supervisor for all specifications with less learning time than the published methods.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/7825ea27ec1762f6ac41347603535500bcd121f7.pdf",
        "venue": "IEEE Access",
        "citationCount": 5,
        "score": 2.5
    },
    "859172d8cb31ddebd9b18e3a7a7d982fab1d1341.pdf": {
        "title": "Reward Space Noise for Exploration in Deep Reinforcement Learning",
        "authors": [
            "Chuxiong Sun",
            "Rui Wang",
            "Qian Li",
            "Xiaohui Hu"
        ],
        "published_date": "2021",
        "abstract": "A fundamental challenge for reinforcement learning (RL) is how to achieve efficient exploration in initially unknown environments. Most state-of-the-art RL algorithms leverage action space noise to drive exploration. The classical strategies are computationally efficient and straightforward to implement. However, these methods may fail to perform effectively in complex environments. To address this issue, we propose a novel strategy named reward space noise (RSN) for farsighted and consistent exploration in RL. By introducing the stochasticity from reward space, we are able to change agent\u2019s understanding about environment and perturb its behaviors. We find that the simple RSN can achieve consistent exploration and scale to complex domains without intensive computational cost. To demonstrate the effectiveness and scalability of the proposed method, we implement a deep Q-learning agent with reward noise and evaluate its exploratory performance on a set of Atari games which are challenging for the naive [Formula: see text]-greedy strategy. The results show that reward noise outperforms action noise in most games and performs comparably in others. Concretely, we found that in the early training, the best exploratory performance of reward noise is obviously better than action noise, which demonstrates that the reward noise can quickly explore the valuable states and aid in finding the optimal policy. Moreover, the average scores and learning efficiency of reward noise are also higher than action noise through the whole training, which indicates that the reward noise can generate more stable and consistent performance.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/859172d8cb31ddebd9b18e3a7a7d982fab1d1341.pdf",
        "venue": "International journal of pattern recognition and artificial intelligence",
        "citationCount": 7,
        "score": 1.75
    },
    "f58ada4680d374f5dba96b5e7fcb5b7be11a6acc.pdf": {
        "title": "Survey of Reinforcement Learning based on Human Prior Knowledge",
        "authors": [
            "Zijing Guo",
            "Chendie Yao",
            "Yanghe Feng",
            "Yue Xu"
        ],
        "published_date": "2022",
        "abstract": "At present, task planning is mainly solved by rules or operational research methods. The time complexity and space complexity of these methods increase exponentially with the growth of problem size. Thus, it is challenging to solve large-scale problems. Moreover, it is helpless to solve dynamic task planning problems. Reinforcement learning (RL) is often used to solve the dynamic planning problem of continuous decision making. RL agent constantly interacts with the environment to achieve the expected goal. However, the RL method meets challenges when handling the problem with ample state space. More sampling and exploration are needed to update the strategy gradient, and the convergence speed is slow. Humans ensure a quick start of learning by using prior knowledge, which reduces the exploration time of problems. Thus, we review the methods which combine human prior knowledge with RL through temporal node classification of human prior knowledge combined with RL. In this way, agents effectively reduce the sampling and exploration of the environment and eventually get the optimal strategy faster. Finally, we propose the development direction.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/f58ada4680d374f5dba96b5e7fcb5b7be11a6acc.pdf",
        "venue": "Journal of Uncertain Systems",
        "citationCount": 5,
        "score": 1.6666666666666665
    },
    "aa65704a16138790678e2b9b59ae679b6c9353d7.pdf": {
        "title": "Knowledge-Guided Exploration in Deep Reinforcement Learning",
        "authors": [
            "Sahisnu Mazumder",
            "Bing Liu",
            "Shuai Wang",
            "Yingxuan Zhu",
            "Xiaotian Yin",
            "Lifeng Liu",
            "Jian Li"
        ],
        "published_date": "2022",
        "abstract": "This paper proposes a new method to drastically speed up deep reinforcement learning (deep RL) training for problems that have the property of state-action permissibility (SAP). Two types of permissibility are defined under SAP. The first type says that after an action $a_t$ is performed in a state $s_t$ and the agent has reached the new state $s_{t+1}$, the agent can decide whether $a_t$ is permissible or not permissible in $s_t$. The second type says that even without performing $a_t$ in $s_t$, the agent can already decide whether $a_t$ is permissible or not in $s_t$. An action is not permissible in a state if the action can never lead to an optimal solution and thus should not be tried (over and over again). We incorporate the proposed SAP property and encode action permissibility knowledge into two state-of-the-art deep RL algorithms to guide their state-action exploration together with a virtual stopping strategy. Results show that the SAP-based guidance can markedly speed up RL training.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/aa65704a16138790678e2b9b59ae679b6c9353d7.pdf",
        "venue": "arXiv.org",
        "citationCount": 5,
        "score": 1.6666666666666665
    },
    "bd8aaab29fa16f40ef016393ba7ca30127abab58.pdf": {
        "title": "Risk Perspective Exploration in Distributional Reinforcement Learning",
        "authors": [
            "Ji-Yun Oh",
            "Joonkee Kim",
            "Se-Young Yun"
        ],
        "published_date": "2022",
        "abstract": "Distributional reinforcement learning demonstrates state-of-the-art performance in continuous and discrete control settings with the features of variance and risk, which can be used to explore. However, the exploration method employing the risk property is hard to find, although numerous exploration methods in Distributional RL employ the variance of return distribution per action. In this paper, we present risk scheduling approaches that explore risk levels and optimistic behaviors from a risk perspective. We demonstrate the performance enhancement of the DMIX algorithm using risk scheduling in a multi-agent setting with comprehensive experiments.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/bd8aaab29fa16f40ef016393ba7ca30127abab58.pdf",
        "venue": "arXiv.org",
        "citationCount": 5,
        "score": 1.6666666666666665
    },
    "a6c5e49425ac01c534d9663a4453b28f6dc76f7c.pdf": {
        "title": "Accelerating Robot Trajectory Learning for Stochastic Tasks",
        "authors": [
            "J. Vidakovi\u0107",
            "B. Jerbi\u0107",
            "B. \u0160ekoranja",
            "M. \u0160vaco",
            "F. \u0160uligoj"
        ],
        "published_date": "2020",
        "abstract": "Learning from demonstration provides ways to transfer knowledge and skills from humans to robots. Models based solely on learning from demonstration often have very good generalization capabilities but are not completely accurate when adapting to new scenarios. This happens especially when learning stochastic tasks because of the correspondence problem and unmodeled physical properties of tasks. On the other hand, reinforcement learning (RL) methods such as policy search have the capability to refine an initial skill through exploration, where the learning process is often very dependent on the initialization strategy and is efficient in finding only local solutions. These two approaches are, therefore, frequently combined. In this paper, we present how the iterative learning of tasks can be accelerated by a learning from demonstration (LfD) method based on the extraction of via-points. The paper provides an evaluation of the approach on two different primitive motion tasks.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/a6c5e49425ac01c534d9663a4453b28f6dc76f7c.pdf",
        "venue": "IEEE Access",
        "citationCount": 6,
        "score": 1.2000000000000002
    },
    "d8bc75fbcfdecd5cd1952524d3e07db13722f5ff.pdf": {
        "title": "Optimal Volt/Var Control for Unbalanced Distribution Networks With Human-in-the-Loop Deep Reinforcement Learning",
        "authors": [
            "Xianzhuo Sun",
            "Zhao Xu",
            "Jing Qiu",
            "Huichuan Liu",
            "Huayi Wu",
            "Yuechuan Tao"
        ],
        "published_date": "2024",
        "abstract": "This paper proposes a human-in-the-loop deep reinforcement learning (HL-DRL)-based VVC strategy to simultaneously reduce power losses, mitigate voltage violations and compensate for voltage unbalance in three-phase unbalanced distribution networks. Instead of fully trusting DRL actions made by deep neural networks, a human intervention module is proposed to modify dangerous actions that violate operation constraints during offline training. This module refers to well-designed human guidance rules based on voltage-reactive power sensitivities, which regulate PV reactive power to sequentially address local voltage violation and unbalance issues to obtain safe transitions. To efficiently and safely learn the optimal control policy from these training samples, a human-in-the-loop soft actor-critic (HL-SAC) solution method is then developed. Different from the standard SAC algorithm, an online switch mechanism between action exploration and human intervention is designed. The actor network loss function is modified to incorporate human guidance terms, which alleviates the inconsistency of the updating direction of actor and critic networks. A hybrid experience replay buffer including both dangerous and safe transitions is also used to facilitate the learning process towards human actions. Comparative simulation results on a modified IEEE 123-bus unbalanced distribution system demonstrate the effectiveness and superiority of the proposed method in voltage control.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/d8bc75fbcfdecd5cd1952524d3e07db13722f5ff.pdf",
        "venue": "IEEE Transactions on Smart Grid",
        "citationCount": 21,
        "score": 21.0
    },
    "17682d6f13dcac703092e0f1500a4197e46bbf2c.pdf": {
        "title": "Safe Reinforcement Learning for Power System Control: A Review",
        "authors": [
            "Peipei Yu",
            "Zhen-yu Wang",
            "Hongcai Zhang",
            "Yonghua Song"
        ],
        "published_date": "2024",
        "abstract": "The large-scale integration of intermittent renewable energy resources introduces increased uncertainty and volatility to the supply side of power systems, thereby complicating system operation and control. Recently, data-driven approaches, particularly reinforcement learning (RL), have shown significant promise in addressing complex control challenges in power systems, because RL can learn from interactive feedback without needing prior knowledge of the system model. However, the training process of model-free RL methods relies heavily on random decisions for exploration, which may result in ``bad\"decisions that violate critical safety constraints and lead to catastrophic control outcomes. Due to the inability of RL methods to theoretically ensure decision safety in power systems, directly deploying traditional RL algorithms in the real world is deemed unacceptable. Consequently, the safety issue in RL applications, known as safe RL, has garnered considerable attention in recent years, leading to numerous important developments. This paper provides a comprehensive review of the state-of-the-art safe RL techniques and discusses how these techniques can be applied to power system control problems such as frequency regulation, voltage control, and energy management. We then present discussions on key challenges and future research directions, related to convergence and optimality, training efficiency, universality, and real-world deployment.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/17682d6f13dcac703092e0f1500a4197e46bbf2c.pdf",
        "venue": "arXiv.org",
        "citationCount": 8,
        "score": 8.0
    },
    "11c34b84c3ad6587529517c32923c446797c63e6.pdf": {
        "title": "Cooperative multi-agent reinforcement learning for multi-area integrated scheduling in wafer fabs",
        "authors": [
            "Ming Wang",
            "Jie Zhang",
            "Peng Zhang",
            "Mengyu Jin"
        ],
        "published_date": "2024",
        "abstract": "The existing scheduling methods of wafer fabs focus on single area, achieving local optimisation while failing to realise global optimisation due to neglecting the coordination of multi-area. Therefore, it is necessary to consider the complex opposing relationships between multi-area caused by constraints such as batch processing, re-entrance, and multiple residency times within and between areas to conduct integrated scheduling and shorten the production cycle time. For this issue, this paper proposes a cooperative multi-agent reinforcement learning for multi-area integrated scheduling. Aiming at the dynamic batching and scheduling considering the dynamic arrival lots in multi-area, a multi-agent reinforcement learning algorithm is presented to learn the optimal dynamic batching and scheduling policy firstly. Subsequently, a cooperative multi-agent framework is raised to achieve the global optimisation and coordination of multi-area. Furthermore, an adaptive exploration strategy is constructed to enhance the global exploration capability of the complex solution space caused by residency time constraints and re-entrant property. Moreover, a policy share enhanced Double DQN is employed to improve the generalisation and adaptability of the multi-agent. Finally, the experiments demonstrate that the proposed integrated scheduling method has better comprehensive performance compared to the previous area-separated scheduling methods.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/11c34b84c3ad6587529517c32923c446797c63e6.pdf",
        "venue": "International Journal of Production Research",
        "citationCount": 7,
        "score": 7.0
    },
    "d929e0bc4e30910527a714f239b5e5b37a7ec6ad.pdf": {
        "title": "Multi-UAV Cooperative Target Assignment Method Based on Reinforcement Learning",
        "authors": [
            "Yunlong Ding",
            "Minchi Kuang",
            "Heng Shi",
            "Jiazhan Gao"
        ],
        "published_date": "2024",
        "abstract": "To overcome the problems of traditional distributed target allocation algorithms in terms of lack of target strategic priority, poor scalability, and robustness, this paper proposes a proximal strategy optimization algorithm that combines threat assessment and attention mechanism (TAPPO). Based on the distributed training framework, the algorithm integrates a threat assessment and dynamic attention strategy and designs a dynamic reward function based on the current hit rate of the drone and the missile benefit ratio to improve the algorithm\u2019s exploration ability and scalability. Through an 8vs8 multi-UAV confrontation experiment in a digital twin simulation environment, the results show that the agent using the TAPPO algorithm for target allocation defeats the state machine with an 85% winning rate and is significantly better than other current mainstream target allocation algorithms, verifying the effectiveness of the algorithm.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/d929e0bc4e30910527a714f239b5e5b37a7ec6ad.pdf",
        "venue": "Drones",
        "citationCount": 6,
        "score": 6.0
    },
    "516c6ab3feab17bc158f12ef6768b26c603566b8.pdf": {
        "title": "Variable Speed Limit Intelligent Decision-Making Control Strategy Based on Deep Reinforcement Learning under Emergencies",
        "authors": [
            "Jingwen Yang",
            "Ping Wang",
            "Yongfeng Ju"
        ],
        "published_date": "2024",
        "abstract": "Uncertain emergency events are inevitable and occur unpredictably on the highway. Emergencies with lane capacity drops cause local congestion and can even cause a second accident if the response is not timely. To address this problem, a self-triggered variable speed limit (VSL) intelligent decision-making control strategy based on the improved deep deterministic policy gradient (DDPG) algorithm is proposed, which can eliminate or alleviate congestion in a timely manner. The action noise parameter is introduced to improve exploration efficiency and stability in the early stage of the algorithm training and then maximizes differential traffic flow as the control objective, taking the real-time traffic state as the input. The reward function is constructed to explore the values of the speed limit. The results show that in terms of safety, under different traffic flow levels, the proposed strategy has improved by over 28.30% compared to other methods. In terms of efficiency, except for being inferior to the no-control condition during low-traffic-flow conditions, our strategy has improved over 7.21% compared to the others. The proposed strategy greatly benefits traffic sustainability in Intelligent Transport Systems (ITSs).",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/516c6ab3feab17bc158f12ef6768b26c603566b8.pdf",
        "venue": "Sustainability",
        "citationCount": 5,
        "score": 5.0
    },
    "f5e09973834f852237a7d9db6583c7e6615a907d.pdf": {
        "title": "Reinforcement learning layout\u2010based optimal energy management in smart home: AI\u2010based approach",
        "authors": [
            "Sajjad Afroosheh",
            "Khodakhast Esapour",
            "Reza Khorram\u2010Nia",
            "Mazaher Karimi"
        ],
        "published_date": "2024",
        "abstract": "This research addresses the pressing need for enhanced energy management in smart homes, motivated by the inefficiencies of current methods in balancing power usage optimization with user comfort. By integrating reinforcement learning and a unique column\u2010and\u2010constraint generation strategy, the study aims to fill this gap and offer a comprehensive solution. Furthermore, the increasing adoption of renewable energy sources like solar panels underscores the importance of developing advanced energy management techniques, driving the exploration of innovative approaches such as the one proposed herein. The constraint coordination game (CCG) method is designed to efficiently manage the power usage of each appliance, including the charging and discharging of the energy storage system. Additionally, a deep learning model, specifically a deep neural network, is employed to forecast indoor temperatures, which significantly influence the energy demands of the air conditioning system. The synergistic combination of the CCG method with deep learning\u2010based indoor temperature forecasting promises significant reductions in homeowner energy expenses while maintaining optimal appliance performance and user satisfaction. Testing conducted in simulated environments demonstrates promising results, showcasing a 12% reduction in energy costs compared to conventional energy management strategies.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/f5e09973834f852237a7d9db6583c7e6615a907d.pdf",
        "venue": "IET Generation, Transmission &amp; Distribution",
        "citationCount": 5,
        "score": 5.0
    },
    "b43f8bacd80f32734cdff4f9d8c79e397872aed6.pdf": {
        "title": "RL-PLUS: Countering Capability Boundary Collapse of LLMs in Reinforcement Learning with Hybrid-policy Optimization",
        "authors": [
            "Yihong Dong",
            "Xue Jiang",
            "Yongding Tao",
            "Huanyu Liu",
            "Kechi Zhang",
            "Lili Mou",
            "Rongyu Cao",
            "Yingwei Ma",
            "Jue Chen",
            "Binhua Li",
            "Zhi Jin",
            "Fei Huang",
            "Yongbin Li",
            "Ge Li"
        ],
        "published_date": "2025",
        "abstract": "Reinforcement Learning with Verifiable Reward (RLVR) has significantly advanced the complex reasoning abilities of Large Language Models (LLMs). However, it struggles to break through the inherent capability boundaries of the base LLM, due to its essentially on-policy strategy coupled with LLM's immense action space and sparse reward. Critically, RLVR can lead to the capability boundary collapse, narrowing the LLM's problem-solving scope. To address this problem, we propose RL-PLUS, a novel hybrid-policy optimization approach for LLMs that synergizes internal exploitation with external data to achieve stronger reasoning capabilities and surpass the boundaries of base models. RL-PLUS integrates two core components, i.e., Multiple Importance Sampling to address distributional mismatch from external data, and Exploration-Based Advantage Function to guide the model towards high-value, unexplored reasoning paths. We provide both theoretical analysis and extensive experiments to demonstrate the superiority and generalizability of our approach. Compared with existing RLVR methods, RL-PLUS achieves 1) state-of-the-art performance on six math reasoning benchmarks; 2) superior performance on six out-of-distribution reasoning tasks; 3) consistent and significant gains across diverse model families, with average relative improvements up to 69.2\\%. Moreover, the analysis of Pass@k curves indicates that RL-PLUS effectively resolves the capability boundary collapse problem.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/b43f8bacd80f32734cdff4f9d8c79e397872aed6.pdf",
        "venue": "arXiv.org",
        "citationCount": 4,
        "score": 4.0
    },
    "8357670aac3c98a71b454ab5bca89558f265369d.pdf": {
        "title": "Two-Stage Evolutionary Reinforcement Learning for Enhancing Exploration and Exploitation",
        "authors": [
            "Qingling Zhu",
            "Xiaoqiang Wu",
            "Qiuzhen Lin",
            "Weineng Chen"
        ],
        "published_date": "2024",
        "abstract": "The integration of Evolutionary Algorithm (EA) and Reinforcement Learning (RL) has emerged as a promising approach for tackling some challenges in RL, such as sparse rewards, lack of exploration, and brittle convergence properties. However, existing methods often employ actor networks as individuals of EA, which may constrain their exploratory capabilities, as the entire actor population will stop evolution when the critic network in RL falls into local optimal. To alleviate this issue, this paper introduces a Two-stage Evolutionary Reinforcement Learning (TERL) framework that maintains a population containing both actor and critic networks. TERL divides the learning process into two stages. In the initial stage, individuals independently learn actor-critic networks, which are optimized alternatively by RL and Particle Swarm Optimization (PSO). This dual optimization fosters greater exploration, curbing susceptibility to local optima. Shared information from a common replay buffer and PSO algorithm substantially mitigates the computational load of training multiple agents. In the subsequent stage, TERL shifts to a refined exploitation phase. Here, only the best individual undergoes further refinement, while the rest individuals continue PSO-based optimization. This allocates more computational resources to the best individual for yielding superior performance. Empirical assessments, conducted across a range of continuous control problems, validate the efficacy of the proposed TERL paradigm.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/8357670aac3c98a71b454ab5bca89558f265369d.pdf",
        "venue": "AAAI Conference on Artificial Intelligence",
        "citationCount": 4,
        "score": 4.0
    },
    "58db2247187ac01acabc1c2fa02f9b189772729e.pdf": {
        "title": "An Intelligent Parameter Identification Method of DFIG Systems Using Hybrid Particle Swarm Optimization and Reinforcement Learning",
        "authors": [
            "Xuanchen Xiang",
            "Ruisheng Diao",
            "S. Bernadin",
            "Simon Y. Foo",
            "Fangyuan Sun",
            "Ayodeji S. Ogundana"
        ],
        "published_date": "2024",
        "abstract": "Precise modeling of power systems is vital to ensure stability, reliability, and secure operations. In power industrial settings, model parameters can become skewed over time due to prolonged device usage or modifications made to the control systems. Doubly-Fed Induction Generator (DFIG), one of the most prevalent generators in wind farms, is sensitive to transient occurrences. Consequently, parameter calibration of DFIG becomes a crucial focal point in power system planning and operational studies. In this paper, two baseline approaches are first developed to identify the potentially harmful parameters of the DFIG system, including the Particle Swarm Optimization (PSO) method and the state-of-the-art off-policy Reinforcement Learning (RL) method, Soft Actor-Critic (SAC). The outcomes demonstrated that the SAC method outperformed PSO, resulting in an impressive reduction of 74.67% Mean Squared Error (MSE) and a more efficient testing period. In further exploration, a novel hybrid approach called SAC-PSO is developed, with SAC being the teacher of PSO to tackle scenarios with multiple potential solutions. The results exhibited an even greater enhancement over using SAC alone, leading to a remarkable reduction of 87.84% MSE during the testing phase. The proposed method can also effectively apply to a power plant incorporating multiple wind generators.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/58db2247187ac01acabc1c2fa02f9b189772729e.pdf",
        "venue": "IEEE Access",
        "citationCount": 4,
        "score": 4.0
    },
    "e401ba782c2da93959582295089d3f04a051d6c1.pdf": {
        "title": "Reinforcement Learning and Sim-to-Real Transfer of Reorientation and Landing Control for Quadruped Robots on Asteroids",
        "authors": [
            "Ji Qi",
            "Haibo Gao",
            "Huanli Su",
            "M. Huo",
            "Haitao Yu",
            "Zongquan Deng"
        ],
        "published_date": "2024",
        "abstract": "In surface exploration missions, wheeled planetary vehicles have difficulty traveling on asteroids due to their weak gravitational fields. With the rapid development of hardware performance and control methods, quadruped robots have great potential to serve in asteroid exploration. When deploying or controlling the jumping motions of a quadruped robot on an asteroid, landing stability must be guaranteed. Under the weak and irregular gravitational fields of asteroids, the robot should be reoriented to an appropriate attitude for a smooth and stable landing. To achieve this objective, a model-free control method based on reinforcement learning (RL) was proposed. Sim-to-real transfer methods including domain randomization and transfer learning were proposed to address the sim-to-real problem. The original control model trained by RL was transferred to a new version that could be applied and tested on a real quadruped robot. An equivalent asteroid's weak gravitational environment experimental platform was for the first time designed and employed to test the performance of the proposed control scheme. Both the simulation and experimental results validated the effectiveness of the proposed controller training and sim-to-real transfer methods.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/e401ba782c2da93959582295089d3f04a051d6c1.pdf",
        "venue": "IEEE transactions on industrial electronics (1982. Print)",
        "citationCount": 4,
        "score": 4.0
    },
    "afa538f59cf2996837863be60a34eef5271a5ee9.pdf": {
        "title": "Offline Reinforcement Learning for Asynchronous Task Offloading in Mobile Edge Computing",
        "authors": [
            "Bolei Zhang",
            "Fu Xiao",
            "Lifa Wu"
        ],
        "published_date": "2024",
        "abstract": "Edge servers, which are located in close proximity to mobile users, have become key components for providing augmented computation and bandwidth. As the resources of edge servers are limited and shared, it is critical for the decentralized mobile users to determine the amount of offloaded workload, to avoid competition or waste of the public resources at the edge servers. Reinforcement learning (RL) methods, which are sequential and model-free, have been widely considered as a promising approach. However, directly deploying RL in edge computing remains elusive, since arbitrary exploration in real online environments often leads to poor user experience. To avoid the costly interactions, in this paper, we propose an offline RL framework which can be optimized by using a static offline dataset only. In essence, our method first trains a supervised offline model to simulate the edge computing environment dynamics, and then optimize the offloading policy in the offline environment with cost-free interactions. As the offloading requests are mostly asynchronous, we adopt a mean-field approach that treats all neighboring users as a single agent. The problem can then be simplified and reduced to a game between only two players. Moreover, we limit the length of the offline model rollout to ensure the simulated trajectories are accurate, so that the trained offloading policies can be generalized to unseen online environments. Theoretical analyses are conducted to validate the accuracy and convergence of our algorithm. In the experiments, we first train the offline simulation environment with a real historical data set, and then optimize the offloading policy in this environment model. The results show that our algorithm can converge very fast during training. In the execution, the algorithm still achieves high performance in the online environment.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/afa538f59cf2996837863be60a34eef5271a5ee9.pdf",
        "venue": "IEEE Transactions on Network and Service Management",
        "citationCount": 4,
        "score": 4.0
    },
    "88f98e3629a7aef2312f9e214ed2a75672113e4f.pdf": {
        "title": "A modified evolutionary reinforcement learning for multi-agent region protection with fewer defenders",
        "authors": [
            "Siqing Sun",
            "Huachao Dong",
            "Tianbo Li"
        ],
        "published_date": "2024",
        "abstract": "Autonomous region protection is a significant research area in multi-agent systems, aiming to empower defenders in preventing intruders from accessing specific regions. This paper presents a Multi-agent Region Protection Environment (MRPE) featuring fewer defenders, defender damages, and intruder evasion strategies targeting defenders. MRPE poses challenges for traditional protection methods due to its high nonstationarity and limited interception time window. To surmount these hurdles, we modify evolutionary reinforcement learning, giving rise to the corresponding multi-agent region protection method (MRPM). MRPM amalgamates the merits of evolutionary algorithms and deep reinforcement learning, specifically leveraging Differential Evolution (DE) and Multi-Agent Deep Deterministic Policy Gradient (MADDPG). DE facilitates diverse sample exploration and overcomes sparse rewards, while MADDPG trains defenders and expedites the DE convergence process. Additionally, an elite selection strategy tailored for multi-agent systems is devised to enhance defender collaboration. The paper also presents ingenious designs for the fitness and reward functions to effectively drive policy optimizations. Finally, extensive numerical simulations are conducted to validate the effectiveness of MRPM.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/88f98e3629a7aef2312f9e214ed2a75672113e4f.pdf",
        "venue": "Complex &amp; Intelligent Systems",
        "citationCount": 4,
        "score": 4.0
    },
    "c4aafb184f285d004d8c8072b5d6408e876428e1.pdf": {
        "title": "Reinforcement Learning for an Efficient and Effective Malware Investigation during Cyber Incident Response",
        "authors": [
            "Dipo Dunsin",
            "Mohamed Chahine Ghanem",
            "Karim Ouazzane",
            "Vassil T. Vassilev"
        ],
        "published_date": "2024",
        "abstract": "This research focused on enhancing post-incident malware forensic investigation using reinforcement learning RL. We proposed an advanced MDP post incident malware forensics investigation model and framework to expedite post incident forensics. We then implement our RL Malware Investigation Model based on structured MDP within the proposed framework. To identify malware artefacts, the RL agent acquires and examines forensics evidence files, iteratively improving its capabilities using Q Table and temporal difference learning. The Q learning algorithm significantly improved the agent ability to identify malware. An epsilon greedy exploration strategy and Q learning updates enabled efficient learning and decision making. Our experimental testing revealed that optimal learning rates depend on the MDP environment complexity, with simpler environments benefiting from higher rates for quicker convergence and complex ones requiring lower rates for stability. Our model performance in identifying and classifying malware reduced malware analysis time compared to human experts, demonstrating robustness and adaptability. The study highlighted the significance of hyper parameter tuning and suggested adaptive strategies for complex environments. Our RL based approach produced promising results and is validated as an alternative to traditional methods notably by offering continuous learning and adaptation to new and evolving malware threats which ultimately enhance the post incident forensics investigations.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/c4aafb184f285d004d8c8072b5d6408e876428e1.pdf",
        "venue": "arXiv.org",
        "citationCount": 4,
        "score": 4.0
    },
    "e825e6325dfb5b93066817e7cc6b226ba7d3b799.pdf": {
        "title": "Bayesian Design Principles for Offline-to-Online Reinforcement Learning",
        "authors": [
            "Haotian Hu",
            "Yiqin Yang",
            "Jianing Ye",
            "Chengjie Wu",
            "Ziqing Mai",
            "Yujing Hu",
            "Tangjie Lv",
            "Changjie Fan",
            "Qianchuan Zhao",
            "Chongjie Zhang"
        ],
        "published_date": "2024",
        "abstract": "Offline reinforcement learning (RL) is crucial for real-world applications where exploration can be costly or unsafe. However, offline learned policies are often suboptimal, and further online fine-tuning is required. In this paper, we tackle the fundamental dilemma of offline-to-online fine-tuning: if the agent remains pessimistic, it may fail to learn a better policy, while if it becomes optimistic directly, performance may suffer from a sudden drop. We show that Bayesian design principles are crucial in solving such a dilemma. Instead of adopting optimistic or pessimistic policies, the agent should act in a way that matches its belief in optimal policies. Such a probability-matching agent can avoid a sudden performance drop while still being guaranteed to find the optimal policy. Based on our theoretical findings, we introduce a novel algorithm that outperforms existing methods on various benchmarks, demonstrating the efficacy of our approach. Overall, the proposed approach provides a new perspective on offline-to-online RL that has the potential to enable more effective learning from offline data.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/e825e6325dfb5b93066817e7cc6b226ba7d3b799.pdf",
        "venue": "International Conference on Machine Learning",
        "citationCount": 4,
        "score": 4.0
    },
    "3416214ca1d4f790a048ece4229829333e836b4d.pdf": {
        "title": "Sparse Graph Representation Learning Based on Reinforcement Learning for Personalized Mild Cognitive Impairment (MCI) Diagnosis",
        "authors": [
            "Chang-Hoon Ji",
            "Dong-Hee Shin",
            "Young-Han Son",
            "Tae-Eui Kam"
        ],
        "published_date": "2024",
        "abstract": "Resting-state functional magnetic resonance imaging (rs-fMRI) has gained attention as a reliable technique for investigating the intrinsic function patterns of the brain. It facilitates the extraction of functional connectivity networks (FCNs) that capture synchronized activity patterns among regions of interest (ROIs). Analyzing FCNs enables the identification of distinctive connectivity patterns associated with mild cognitive impairment (MCI). For MCI diagnosis, various sparse representation techniques have been introduced, including statistical- and deep learning-based methods. However, these methods face limitations due to their reliance on supervised learning schemes, which restrict the exploration necessary for probing novel solutions. To overcome such limitation, prior work has incorporated reinforcement learning (RL) to dynamically select ROIs, but effective exploration remains challenging due to the vast search space during training. To tackle this issue, in this study, we propose an advanced RL-based framework that utilizes a divide-and-conquer approach to decompose the FCN construction task into smaller sub-problems in a subject-specific manner, enabling efficient exploration under each sub-problem condition. Additionally, we leverage the learned value function to determine the sparsity level of FCNs, considering individual characteristics of FCNs. We validate the effectiveness of our proposed framework by demonstrating its superior performance in MCI diagnosis on publicly available cohort datasets.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/3416214ca1d4f790a048ece4229829333e836b4d.pdf",
        "venue": "IEEE journal of biomedical and health informatics",
        "citationCount": 4,
        "score": 4.0
    },
    "6bf550ce7f10a4347d448eb810a92e1a5cfffa4b.pdf": {
        "title": "Beyond Optimism: Exploration With Partially Observable Rewards",
        "authors": [
            "Simone Parisi",
            "Alireza Kazemipour",
            "Michael Bowling"
        ],
        "published_date": "2024",
        "abstract": "Exploration in reinforcement learning (RL) remains an open challenge. RL algorithms rely on observing rewards to train the agent, and if informative rewards are sparse the agent learns slowly or may not learn at all. To improve exploration and reward discovery, popular algorithms rely on optimism. But what if sometimes rewards are unobservable, e.g., situations of partial monitoring in bandits and the recent formalism of monitored Markov decision process? In this case, optimism can lead to suboptimal behavior that does not explore further to collapse uncertainty. With this paper, we present a novel exploration strategy that overcomes the limitations of existing methods and guarantees convergence to an optimal policy even when rewards are not always observable. We further propose a collection of tabular environments for benchmarking exploration in RL (with and without unobservable rewards) and show that our method outperforms existing ones.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/6bf550ce7f10a4347d448eb810a92e1a5cfffa4b.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 3,
        "score": 3.0
    },
    "54cb726595cd8c03f59f49c9a97b76b4d3f932f2.pdf": {
        "title": "Rethinking Exploration in Reinforcement Learning with Effective Metric-Based Exploration Bonus",
        "authors": [
            "Yiming Wang",
            "Kaiyan Zhao",
            "Furui Liu",
            "Leong Hou"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/54cb726595cd8c03f59f49c9a97b76b4d3f932f2.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 3,
        "score": 3.0
    },
    "b44b0e8222021b51783c62b0bce071ef6fb3f12c.pdf": {
        "title": "OCEAN-MBRL: Offline Conservative Exploration for Model-Based Offline Reinforcement Learning",
        "authors": [
            "Fan Wu",
            "Rui Zhang",
            "Qi Yi",
            "Yunkai Gao",
            "Jiaming Guo",
            "Shaohui Peng",
            "Siming Lan",
            "Husheng Han",
            "Yansong Pan",
            "Kaizhao Yuan",
            "Pengwei Jin",
            "Rui Chen",
            "Yunji Chen",
            "Ling Li"
        ],
        "published_date": "2024",
        "abstract": "Model-based offline reinforcement learning (RL) algorithms have emerged as a promising paradigm for offline RL.\nThese algorithms usually learn a dynamics model from a static dataset of transitions, use the model to generate synthetic trajectories, and perform conservative policy optimization within these trajectories. \nHowever, our observations indicate that policy optimization methods used in these model-based offline RL algorithms are not effective at exploring the learned model and induce biased exploration, which ultimately impairs the performance of the algorithm.\nTo address this issue, we propose Offline Conservative ExplorAtioN (OCEAN), a novel rollout approach to model-based offline RL.\nIn our method, we incorporate additional exploration techniques and introduce three conservative constraints based on uncertainty estimation to mitigate the potential impact of significant dynamic errors resulting from exploratory transitions. \nOur work is a plug-in method and can be combined with classical model-based RL algorithms, such as MOPO, COMBO, and RAMBO.\nExperiment results of our method on the D4RL MuJoCo benchmark show that OCEAN significantly improves the performance of existing algorithms.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/b44b0e8222021b51783c62b0bce071ef6fb3f12c.pdf",
        "venue": "AAAI Conference on Artificial Intelligence",
        "citationCount": 3,
        "score": 3.0
    },
    "5ac5f3827ee4c32cebadbc9de8a892853575efb9.pdf": {
        "title": "Active Exploration Deep Reinforcement Learning for Continuous Action Space with Forward Prediction",
        "authors": [
            "Dongfang Zhao",
            "Huanshi Xu",
            "Zhang Xun"
        ],
        "published_date": "2024",
        "abstract": "The application of reinforcement learning (RL) to the field of autonomous robotics has high requirements about sample efficiency, since the agent expends for interaction with the environment. One method for sample efficiency is to extract knowledge from existing samples and used to exploration. Typical RL algorithms achieve exploration using task-specific knowledge or adding exploration noise. These methods are limited to current policy improvement level and lack of long-term planning. We propose a novel active exploration deep RL algorithm for the continuous action space problem named active exploration deep reinforcement learning (AEDRL). Our method uses the Gaussian process to model dynamic model, enabling the probability description of prediction sample. Action selection is formulated as the solution of the optimization problem. Thus, the optimization objective is specifically designed for selecting samples that can minimize the uncertainty of the dynamic model. Active exploration is achieved through long-term optimized action selection. This long-term considered action exploration method is more guidance for learning. Enable intelligent agents to explore more interesting action spaces. The proposed AEDRL algorithm is evaluated on several robotic control task including classic pendulum problem and five complex articulated robots. The AEDRL can learn a controller using fewer episodes and demonstrates performance and sample efficiency.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/5ac5f3827ee4c32cebadbc9de8a892853575efb9.pdf",
        "venue": "International Journal of Computational Intelligence Systems",
        "citationCount": 3,
        "score": 3.0
    },
    "cce1245ba1ec154120b3b256faf7bf28f769b505.pdf": {
        "title": "A Novel Guided Deep Reinforcement Learning Tracking Control Strategy for Multirotors",
        "authors": [
            "Hean Hua",
            "Yaonan Wang",
            "Hang Zhong",
            "Hui Zhang",
            "Yongchun Fang"
        ],
        "published_date": "2025",
        "abstract": "This paper presents an intelligent control scheme for multirotors, where accurate trajectory tracking, strong robustness and reliable generalization are guaranteed by the dual-feedback sliding-mode (DFSM) guided deep reinforcement learning (RL). Different from current solutions, the proposed method explores optimal learning strategy on the sliding surface according to the DFSM demonstrations, where the elegantly designed parallel evaluation takes full advantage of model knowledge and learning exploration. Specifically, the intelligent tracking control is achieved in a two-step design. First, the DFSM algorithm is designed for multirotors, where the linear and nonlinear feedback terms work cooperatively. Second, the DFSM-guided deep RL is put forward to achieve intelligent switching on the sliding surface, where position and velocity errors are both considered to generate accurate switching decisions. In the framework, explorations and the DFSM demonstrations are evaluated in parallel, where only the explorations that are better than the DFSM baseline, are kept for policy improvement. In this way, the DFSM algorithm keeps pushing the RL policy to explore better strategy, where the unavoidable bad experiences arisen from exploration are identified accurately. Practical comparative experimental results are included to verify the effectiveness of the proposed strategy. Note to Practitioners\u2014This paper is motivated by the practical problem of controlling multirotor system in uncertain environments. Up until now, most existing approaches are proposed without taking full advantage of model knowledge and deep learning techniques simultaneously, which lacks of reliability in practical application. To deal with the problem, a new dual-feedback sliding-mode (DFSM) guided deep reinforcement learning (RL) strategy is proposed, where the dual feedback and guided RL are designed to achieve satisfactory tracking control and simultaneously handle uncertainties. Specifically, by introducing double-check framework, the RL strategy explores optimal switching policy on the sliding surface according to the DFSM demonstrations, guaranteeing strong robustness and reliable generalization of the obtained RL policy in uncertain environments. The key feature of the framework is that the DFSM-driven training guarantees practice-oriented tracking control in a DFSM-RL cooperative manner. Comparative experiments are implemented to verify the tracking performance of the proposed intelligent control strategy.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/cce1245ba1ec154120b3b256faf7bf28f769b505.pdf",
        "venue": "IEEE Transactions on Automation Science and Engineering",
        "citationCount": 3,
        "score": 3.0
    },
    "37fe2a997bf07a972473abd079d175335940e6bd.pdf": {
        "title": "CDE: Curiosity-Driven Exploration for Efficient Reinforcement Learning in Large Language Models",
        "authors": [
            "Runpeng Dai",
            "Linfeng Song",
            "Haolin Liu",
            "Zhenwen Liang",
            "Dian Yu",
            "Haitao Mi",
            "Zhaopeng Tu",
            "Rui Liu",
            "Tong Zheng",
            "Hongtu Zhu",
            "Dong Yu"
        ],
        "published_date": "2025",
        "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) is a powerful paradigm for enhancing the reasoning ability of Large Language Models (LLMs). Yet current RLVR methods often explore poorly, leading to premature convergence and entropy collapse. To address this challenge, we introduce Curiosity-Driven Exploration (CDE), a framework that leverages the model's own intrinsic sense of curiosity to guide exploration. We formalize curiosity with signals from both the actor and the critic: for the actor, we use perplexity over its generated response, and for the critic, we use the variance of value estimates from a multi-head architecture. Both signals serve as an exploration bonus within the RLVR framework to guide the model. Our theoretical analysis shows that the actor-wise bonus inherently penalizes overconfident errors and promotes diversity among correct responses; moreover, we connect the critic-wise bonus to the well-established count-based exploration bonus in RL. Empirically, our method achieves an approximate +3 point improvement over standard RLVR using GRPO/PPO on AIME benchmarks. Further analysis identifies a calibration collapse mechanism within RLVR, shedding light on common LLM failure modes.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/37fe2a997bf07a972473abd079d175335940e6bd.pdf",
        "venue": "",
        "citationCount": 3,
        "score": 3.0
    },
    "e32e28a8a06739997957113b7fa1bd033f6801ba.pdf": {
        "title": "An Improved Reinforcement Learning Method Based on Unsupervised Learning",
        "authors": [
            "Xin Chang",
            "Yanbin Li",
            "Guanjie Zhang",
            "Donghui Liu",
            "Changjun Fu"
        ],
        "published_date": "2024",
        "abstract": "The approach of directly combining clustering method and reinforcement learning (RL) will lead to encounter the issue where states may have different state transition processes under the same action, resulting in poor policy performance. To address this challenge with multi-dimensional continuous observation data, an improved reinforcement learning method based on unsupervised learning is proposed with a novel framework. Instead of dimensionality reduction methods, unsupervised clustering is employed to indirectly capture the underlying structure of the data. First, the proposed framework incorporates multi-dimensional information, including the current observation data, the next observation data and reward information, during the clustering process, leading to a more accurate and comprehensive low-dimensional discrete representation of the observation data while retaining preserving transition of Markov decision process. Second, by compressing the observation data into a well-defined state space, the resulting cluster labels serve as the low-dimensional discrete label-states for reinforcement learning to generate more effective and robust policies. Comparative analysis with state-of-the-art RL methods demonstrates that the improved RL methods base on framework achieves higher rewards, indicating its superior performance. Furthermore, the framework exhibits computational efficiency, as evidenced by its reasonable time complexity. This structural innovation allows for better exploration and exploitation of the transition, leading to improved policy performance in engineering applications.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/e32e28a8a06739997957113b7fa1bd033f6801ba.pdf",
        "venue": "IEEE Access",
        "citationCount": 3,
        "score": 3.0
    },
    "c81a06446fa1df12cc043d9d9c8cfd5775090c59.pdf": {
        "title": "Enhancing Scalability in Reinforcement Learning for Open Spaces",
        "authors": [
            "J. Janjua",
            "Shagufta Kousar",
            "Areeba Khan",
            "Anaum Ihsan",
            "Tahir Abbas",
            "Ali Q Saeed"
        ],
        "published_date": "2024",
        "abstract": "Reinforcement Learning (RL) has been successful when the environment has specific objectives and boundaries. But with the emerging focus on open-world application which makes all or some of the rules or purpose go to naught, it makes traditional methods of RL a bit more difficult. This paper goes over various advancements and changes in Reinforcement Learning which can be employed for open-ended environments. Among the other strategies, hierarchical reinforcement learning, intrinsic motivation-based exploration, meta-learning and unsupervised skill acquisition are also among the ones that are examined. As a result, such a position based on the technology argues the promising future of open-ended methods for the management of complex problems and high level of uncertainty associated with the preset target or purpose. Also, we study cases in video games, robotics and autonomous systems, where RL is implemented in an open-ended and dynamic environment. We also outline existing limitations and perspectives, highlighting the need for more flexible methods and inter-scientific collaboration to fully realize RL's potential in open-ended contexts.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/c81a06446fa1df12cc043d9d9c8cfd5775090c59.pdf",
        "venue": "2024 International Conference on Decision Aid Sciences and Applications (DASA)",
        "citationCount": 3,
        "score": 3.0
    },
    "66c5bed508f9dce55b2beeaaf36a29929c0bc2ac.pdf": {
        "title": "Water Age Control for Water Distribution Networks via Safe Reinforcement Learning",
        "authors": [
            "Jorge Val Ledesma",
            "Rafa\u0142 Wi\u015bniewski",
            "C. Kalles\u00f8e",
            "Agisilaos Tsouvalas"
        ],
        "published_date": "2024",
        "abstract": "Reinforcement learning (RL) is a widely used control technique that finds an optimal policy using the feedback of its actions. The search for the optimal policy requires that the system explores a broad region of the state space. This search puts at risk the safe operation, since some of the explored regions might be near the physical system limits. Implementing learning methods in industrial applications is limited because of its uncertain behavior when finding an optimal policy. This work proposes an RL control algorithm with a filter that supervises the safety of the exploration based on a nominal model. The performance of this safety filter is increased by modeling the uncertainty with a Gaussian process (GP) regression. This method is applied to the optimization of the management of a water distribution network (WDN) with an elevated reservoir; the management objectives are to regulate the tank filling while maintaining an adequate water turnover. The proposed method is validated in a laboratory setup that emulates the hydraulic features of a WDN.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/66c5bed508f9dce55b2beeaaf36a29929c0bc2ac.pdf",
        "venue": "IEEE Transactions on Control Systems Technology",
        "citationCount": 3,
        "score": 3.0
    },
    "2389fafc2a97e13fa810c4014babe73bd886c06f.pdf": {
        "title": "Offline Reinforcement Learning with Failure Under Sparse Reward Environments",
        "authors": [
            "Mingkang Wu",
            "Umer Siddique",
            "Abhinav Sinha",
            "Yongcan Cao"
        ],
        "published_date": "2024",
        "abstract": "This paper presents a new reinforcement learning approach that leverages failed experiences in sparse reward environments. Unlike traditional reinforcement learning methods that rely on successful experiences or expert demonstrations, the proposed approach utilizes failed experiences to guide the policy update during the learning process. The primary objective behind this work is to develop a method that can efficiently utilize failed experiences to guide the search direction as directional cues from successful experiences may be limited in sparse environments. To achieve this objective, we introduce a new objective function that aims to maximize the dissimilarity between the RL agent's actions and actions from failed experiences. This discrepancy serves as a valuable indicator to guide the agent's exploration. In other words, our method focuses on achieving the desired objectives by leveraging failed experiences to provide a significant opportunity for the agent to refine its policy. We further employ hindsight experience replay (HER) to enhance the directional search by creating and achieving potential subgoals that align with the primary objectives. To assess the effectiveness of our method, we conduct experiments on three sparse reward environments. Our findings demonstrate that the proposed approach significantly enhances the agent's learning efficiency and improves robustness to variations in demonstration quality compared to conventional reinforcement learning techniques.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/2389fafc2a97e13fa810c4014babe73bd886c06f.pdf",
        "venue": "International Conference on Multimodal Interaction",
        "citationCount": 3,
        "score": 3.0
    },
    "b2d827c286e32dbf0739e8c796b119b1074809b4.pdf": {
        "title": "Meta SAC-Lag: Towards Deployable Safe Reinforcement Learning via MetaGradient-based Hyperparameter Tuning",
        "authors": [
            "Homayoun Honari",
            "Amir M. Soufi Enayati",
            "Mehran Ghafarian Tamizi",
            "Homayoun Najjaran"
        ],
        "published_date": "2024",
        "abstract": "Safe Reinforcement Learning (Safe RL) is one of the prevalently studied subcategories of trial-and-error-based methods with the intention to be deployed on real-world systems. In safe RL, the goal is to maximize reward performance while minimizing constraints, often achieved by setting bounds on constraint functions and utilizing the Lagrangian method. However, deploying Lagrangian-based safe RL in real-world scenarios is challenging due to the necessity of threshold fine-tuning, as imprecise adjustments may lead to suboptimal policy convergence. To mitigate this challenge, we propose a unified Lagrangian-based model-free architecture called Meta Soft Actor-Critic Lagrangian (Meta SAC-Lag). Meta SAC-Lag uses meta-gradient optimization to automatically update the safety-related hyperparameters. The proposed method is designed to address safe exploration and threshold adjustment with minimal hyperparameter tuning requirement. In our pipeline, the inner parameters are updated through the conventional formulation and the hyperparameters are adjusted using the meta-objectives which are defined based on the updated parameters. Our results show that the agent can reliably adjust the safety performance due to the relatively fast convergence rate of the safety threshold. We evaluate the performance of Meta SAC-Lag in five simulated environments against Lagrangian baselines, and the results demonstrate its capability to create synergy between parameters, yielding better or competitive results. Furthermore, we conduct a real-world experiment involving a robotic arm tasked with pouring coffee into a cup without spillage. Meta SAC-Lag is successfully trained to execute the task, while minimizing effort constraints. The success of Meta SAC-Lag in performing the experiment is intended to be a step toward practical deployment of safe RL algorithms to learn the control process of safety-critical real-world systems without explicit engineering.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/b2d827c286e32dbf0739e8c796b119b1074809b4.pdf",
        "venue": "IEEE/RJS International Conference on Intelligent RObots and Systems",
        "citationCount": 3,
        "score": 3.0
    },
    "fae722ae17483aeef3485f0177346ba3ce332ea9.pdf": {
        "title": "Task-Driven Autonomous Driving: Balanced Strategies Integrating Curriculum Reinforcement Learning and Residual Policy",
        "authors": [
            "Jiamin Shi",
            "Tangyike Zhang",
            "Ziqi Zong",
            "Shitao Chen",
            "Jingmin Xin",
            "Nanning Zheng"
        ],
        "published_date": "2024",
        "abstract": "Achieving fully autonomous driving in urban traffic scenarios is a significant challenge that necessitates balancing safety, efficiency, and compliance with traffic regulations. In this letter, we introduce a novel Curriculum Residual Hierarchical Reinforcement Learning (CR-HRL) framework. It integrates a rule-based planning model as a guiding mechanism, while a deep reinforcement learning algorithm generates supplementary residual strategies. This combination enables the RL agent to perform safe and efficient overtaking in complex traffic scenarios. Furthermore, we implement a detailed three-stage curriculum learning strategy that enhances the training process. By progressively increasing task complexity, the curriculum strategy effectively guides the exploration of autonomous vehicles and improves the reusability of sub-strategies. The effectiveness of the CR-HRL framework is confirmed through ablation experiments. Comparative experiments further highlight the superior efficiency and decision-making capabilities of our framework over traditional rule-based and RL baseline methods. Tests conducted with actual vehicles also demonstrate its practical applicability in real-world settings.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/fae722ae17483aeef3485f0177346ba3ce332ea9.pdf",
        "venue": "IEEE Robotics and Automation Letters",
        "citationCount": 3,
        "score": 3.0
    },
    "dca6c5f8f6c64b4188212e16c2faf461ffd4e49a.pdf": {
        "title": "Reinforcement learning-based dynamic field exploration and reconstruction using multi-robot systems for environmental monitoring",
        "authors": [
            "Thinh Lu",
            "Divyam Sobti",
            "Deepak Talwar",
            "Wencen Wu"
        ],
        "published_date": "2025",
        "abstract": "In the realm of real-time environmental monitoring and hazard detection, multi-robot systems present a promising solution for exploring and mapping dynamic fields, particularly in scenarios where human intervention poses safety risks. This research introduces a strategy for path planning and control of a group of mobile sensing robots to efficiently explore and reconstruct a dynamic field consisting of multiple non-overlapping diffusion sources. Our approach integrates a reinforcement learning-based path planning algorithm to guide the multi-robot formation in identifying diffusion sources, with a clustering-based method for destination selection once a new source is detected, to enhance coverage and accelerate exploration in unknown environments. Simulation results and real-world laboratory experiments demonstrate the effectiveness of our approach in exploring and reconstructing dynamic fields. This study advances the field of multi-robot systems in environmental monitoring and has practical implications for rescue missions and field explorations.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/dca6c5f8f6c64b4188212e16c2faf461ffd4e49a.pdf",
        "venue": "Frontiers Robotics AI",
        "citationCount": 3,
        "score": 3.0
    },
    "9ac3f4e74e9837cb47f43211cf143d4c7115e7f1.pdf": {
        "title": "``Give Me an Example Like This'': Episodic Active Reinforcement Learning from Demonstrations",
        "authors": [
            "Muhan Hou",
            "Koen V. Hindriks",
            "Guszti Eiben",
            "Kim Baraka"
        ],
        "published_date": "2024",
        "abstract": "Reinforcement Learning (RL) has achieved great success in sequential decision-making problems but often requires extensive agent-environment interactions. To improve sample efficiency, methods like Reinforcement Learning from Expert Demonstrations (RLED) incorporate external expert demonstrations to aid agent exploration during the learning process. However, these demonstrations, typically collected from human users, are costly and thus often limited in quantity. Therefore, how to select the optimal set of human demonstrations that most effectively aids learning becomes a critical concern. This paper introduces EARLY (Episodic Active Learning from demonstration querY), an algorithm designed to enable a learning agent to generate optimized queries for expert demonstrations in a trajectory-based feature space. EARLY employs a trajectory-level estimate of uncertainty in the agent\u2019s current policy to determine the optimal timing and content for feature-based queries. By querying episodic demonstrations instead of isolated state-action pairs, EARLY enhances the human teaching experience and achieves better learning performance. We validate the effectiveness of our method across three simulated navigation tasks of increasing difficulty. Results indicate that our method achieves expert-level performance in all three tasks, converging over 50% faster than other four baseline methods when demonstrations are generated by simulated oracle policies. A follow-up pilot user study (N = 18) further supports that our method maintains significantly better convergence with human expert demonstrators, while also providing a better user experience in terms of perceived task load and requiring significantly less human time.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/9ac3f4e74e9837cb47f43211cf143d4c7115e7f1.pdf",
        "venue": "International Conference on Human-Agent Interaction",
        "citationCount": 3,
        "score": 3.0
    }
}