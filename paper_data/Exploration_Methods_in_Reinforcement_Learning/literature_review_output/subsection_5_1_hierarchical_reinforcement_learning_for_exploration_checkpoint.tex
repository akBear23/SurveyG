\subsection{Hierarchical Reinforcement Learning for Exploration}

Effective exploration is a critical bottleneck in reinforcement learning (RL), particularly in complex environments characterized by vast state-action spaces, sparse rewards, and long task horizons. Hierarchical Reinforcement Learning (HRL) offers a powerful and principled framework to address these challenges by decomposing large, intractable problems into a hierarchy of more manageable sub-problems. This decomposition allows agents to learn and explore at different levels of temporal abstraction, significantly enhancing exploration efficiency and robustness.

The foundational concept underpinning HRL for exploration is the "Options Framework" introduced by \cite{Sutton1999}. Building upon earlier notions of skill chaining by \cite{Singh1995}, the Options Framework formalizes "options" as temporally extended actions, or sub-policies, that execute for multiple time steps. An option consists of an initiation set (states where it can be taken), a policy (how to act while the option is active), and a termination condition (when the option ends). This framework allows a high-level policy to choose among options, while a low-level policy executes the primitive actions within a chosen option. This mechanism fundamentally aids exploration by enabling agents to traverse large portions of the state space more efficiently than with primitive actions alone. Instead of exploring individual steps, the agent explores sequences of actions (options), effectively reducing the effective search space and allowing for more directed movement towards relevant subgoals or novel regions. For instance, an agent might learn an "open door" option, which, once selected, reliably executes the necessary primitive actions to open a door, allowing the high-level policy to explore the consequences of being on the other side of the door, rather than stumbling upon the correct sequence of door-opening actions by chance.

In the era of deep reinforcement learning, various architectures have been proposed to implement hierarchical control and leverage its benefits for exploration. FeUdal Networks (FuN) \cite{Vezhnevets2017} introduced a two-level hierarchy with a "Manager" module that sets goals in a latent space and a "Worker" module that executes primitive actions to achieve those goals. The Manager explores the space of goals, while the Worker explores the primitive action space conditioned on the current goal. This explicit goal-setting mechanism intrinsically guides exploration towards achieving meaningful sub-objectives. Similarly, Hierarchical Reinforcement Learning with Off-policy Correction (HIRO) \cite{Nachum2018} enables efficient learning of both high-level and low-level policies by correcting for off-policy data, allowing the high-level policy to explore by setting goals in the state space, and the low-level policy to learn how to reach those goals. Hierarchical Actor-Critic (HAC) \cite{Levy2019} further refines this by using multiple layers of actor-critic agents, where higher levels set goals for lower levels, and a "hindsight experience replay" mechanism allows agents to learn from failed attempts to reach goals, thereby improving exploration by making better use of suboptimal trajectories. These deep HRL methods demonstrate how learning goal-conditioned policies at different levels of abstraction can significantly accelerate exploration in complex, high-dimensional environments.

A critical aspect of HRL for exploration is the autonomous discovery of useful options or skills, often driven by intrinsic motivation. Rather than manually defining options, agents can learn them through self-supervision. Methods like Diversity is All You Need (DIAYN) \cite{Eysenbach2018} and Variational Option Discovery (VALOR) \cite{Gregor2017} learn a diverse set of skills by maximizing the mutual information between the skill executed and the resulting state trajectory, effectively rewarding the agent for discovering distinct behaviors. These learned skills then serve as valuable options for a higher-level policy, enabling more structured and efficient exploration. The survey by \cite{aubret2022inh} highlights how information-theoretic intrinsic motivation, particularly novelty and surprise, can assist in building a hierarchy of transferable skills, making the exploration process more robust. Furthermore, \cite{janjua2024yhk} emphasizes unsupervised skill acquisition as a key advancement for enhancing scalability in open-ended environments, where HRL provides a natural framework for organizing these learned behaviors.

HRL also facilitates temporally coordinated and goal-conditioned exploration. \cite{zhang2022p0b} proposed Generative Planning Method (GPM), which generates multi-step action plans, effectively acting as temporally extended options. These plans guide exploration towards high-value regions more consistently than single-step perturbations, and the plan generator can adapt to the task, further benefiting future explorations. This aligns with HRL's ability to create intentional action sequences for reaching specific subgoals. Similarly, Random Latent Exploration (RLE) \cite{mahankali20248dx}, while not strictly HRL, encourages exploration by pursuing randomly sampled goals in a latent space. This goal-conditioned exploration paradigm is inherently compatible with HRL, where the high-level policy can sample latent goals for the low-level policy to achieve, fostering diverse and deep exploration.

The utility of hierarchical exploration extends significantly to multi-agent systems, where coordination and efficient search are paramount. \cite{hu2020qwm} designed a cooperative exploration strategy for multiple mobile robots using a hierarchical control architecture, where a high-level decision-making layer coordinates exploration to minimize redundancy, and a low-level layer handles target tracking and collision avoidance. This demonstrates how HRL can structure complex multi-robot behaviors for efficient, coordinated exploration. More recently, \cite{yu20213c1} tackled cooperative visual exploration for multiple agents with a Multi-agent Spatial Planner (MSP) leveraging a transformer-based architecture with hierarchical spatial self-attentions, enabling agents to capture spatial relations and plan cooperatively based on visual signals. \cite{liu2024xkk} further advances multi-agent exploration with "Imagine, Initialize, and Explore" (IIE), which uses a transformer to imagine critical states and then initializes agents at these states for targeted exploration. This approach, while not explicitly called HRL, embodies hierarchical decomposition by first identifying high-level critical states (subgoals) and then focusing low-level exploration from those points, enhancing the discovery of successful joint action sequences in long-horizon tasks.

In summary, HRL provides a powerful framework for managing the complexity of exploration in large state-action spaces. By enabling agents to learn and compose temporally extended actions (options) or to decompose complex tasks into sub-problems, HRL significantly reduces the dimensionality of the exploration problem. This structured approach allows agents to focus on achieving meaningful subgoals, leading to more directed and sample-efficient discovery of optimal policies in long-horizon tasks. Despite these advancements, challenges persist, particularly in the autonomous discovery of optimal and diverse skill sets, the robust learning of high-level policies that effectively coordinate lower-level skills, and ensuring seamless communication and transfer of information across hierarchical levels. Future research will likely focus on developing more adaptive and autonomous methods for skill acquisition and composition, further integrating HRL with robust intrinsic motivation and meta-learning to create agents capable of truly open-ended and efficient exploration.