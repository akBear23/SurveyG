\subsection{Model-Based Planning and Experience Replay}

Efficiently navigating and learning within complex environments is a fundamental challenge in reinforcement learning (RL), often exacerbated by the high cost of real-world interactions. Model-based planning and experience replay address this by making more efficient use of collected experience, implicitly aiding exploration by accelerating learning and propagating information more widely across the state space.

A foundational approach in this domain is the Dyna architecture, introduced by \cite{Sutton1990}. Dyna-Q integrates direct reinforcement learning with planning by concurrently learning an environmental model (transitions and rewards) from real experiences. This learned model is then used to generate simulated experiences, allowing the agent to perform "mental rehearsals" and update its value function from both real and imagined interactions. This process significantly accelerates value function updates and propagates information more widely, making each real interaction more valuable and implicitly encouraging exploration by quickly refining the agent's understanding of the environment. Complementing this, \cite{Lin1992} highlighted the importance of experience replay, a mechanism where past experiences are stored and re-used for learning. By replaying previously collected data, agents can learn more efficiently from a fixed set of interactions, reducing the need for extensive new exploration and improving sample efficiency, particularly in off-policy learning settings.

To further enhance the efficiency of model-based planning, \cite{Sutton1993} and \cite{Moore1993} introduced prioritized sweeping. This method refines the planning process by prioritizing updates to state-action pairs whose values are likely to change significantly, or which have a large impact on other states. By focusing computational resources on the most informative simulated experiences, prioritized sweeping dramatically accelerates learning and value propagation compared to uniform sweeping, ensuring that the agent's understanding of the environment is refined more quickly and effectively. Addressing the challenge of scaling model-based methods to larger state spaces, \cite{Singh1992} proposed reinforcement learning with a hierarchy of abstract models. This approach leverages structural decomposition to manage complexity, allowing exploration and planning to occur at different levels of temporal abstraction, which can make the learning problem more tractable. Similarly, \cite{Kaelbling1993} explored methods for blending planning and direct reinforcement learning, demonstrating how a learned model can be actively used to guide exploration by evaluating hypothetical scenarios and informing action selection, thereby making exploration more directed and less random.

Beyond direct model learning for planning, advancements in state representation also contribute to the efficiency of model-based approaches. \cite{Dayan1993} introduced the successor representation, which models the expected future state occupancies rather than immediate transitions. While not a direct planning mechanism in the Dyna sense, this representation provides a more generalized understanding of state relationships, improving generalization for temporal difference learning and implicitly aiding exploration by making value estimates more robust and transferable across similar states.

In more recent deep reinforcement learning contexts, the principles of model-based planning continue to evolve. \cite{stadie20158af} demonstrated how deep predictive models can be used to incentivize exploration by assigning exploration bonuses based on the uncertainty or novelty derived from the learned dynamics. This approach leverages the representational power of neural networks to build scalable models in high-dimensional domains, guiding exploration towards areas where the model's predictions are less confident. Extending this, \cite{fu20220cl} proposed a model-based lifelong reinforcement learning approach that estimates a hierarchical Bayesian posterior to distill common structures across tasks. By combining this learned posterior with Bayesian exploration, their method significantly increases the sample efficiency of learning across related tasks, showcasing how sophisticated model learning can facilitate principled exploration and transfer. Furthermore, \cite{ma2024r2p} integrated neural network models into an actor-critic architecture (ModelPPO) for AUV path-following control. Their neural network model learns the state transition function, allowing the agent to explore spatio-temporal patterns and achieve superior performance compared to traditional model predictive control and other RL methods, underscoring the enduring utility of learned models in complex control tasks.

Despite their significant advantages in sample efficiency and information propagation, model-based planning and experience replay methods face critical limitations. Their performance heavily relies on the accuracy and learnability of the environmental model. In complex, high-dimensional, or non-stationary domains, learning an accurate and robust model can be exceedingly challenging, and errors in the model can compound, leading to "model bias" and potentially suboptimal policies or misleading exploration. Nevertheless, these foundational and evolving model-based approaches remain crucial for accelerating learning and making efficient use of collected experience, thereby implicitly guiding agents towards more effective exploration strategies.