\subsection*{Prediction Error and Self-Supervised Curiosity}
Effective exploration remains a cornerstone challenge in reinforcement learning (RL), particularly in environments characterized by sparse rewards and high-dimensional observations. To address this, intrinsic motivation methods have emerged, where agents generate their own reward signals to drive discovery. A prominent approach within this paradigm is self-supervised curiosity, which leverages the agent's ability to predict future states or features, using prediction error as an intrinsic reward to guide exploration. This strategy incentivizes agents to seek out situations where their internal models of the world are inaccurate, thereby driving learning about the environment's underlying dynamics.

The foundational concept of curiosity as a driver for learning can be traced back to early work by \cite{schmidhuber1997}, which proposed that agents could be intrinsically motivated to explore by optimizing the predictability of their sensory inputs. This early idea laid the groundwork for defining curiosity as a measure of surprise or novelty. Expanding on this, \cite{stadie20158af} demonstrated that deep predictive models could be effectively used to assign exploration bonuses in complex domains like Atari games, by rewarding states where the agent's learned dynamics model exhibited high uncertainty. This represented an important step in scaling prediction-error-based curiosity to high-dimensional visual inputs, moving beyond simpler tabular settings.

A significant advancement in this direction was the Intrinsic Curiosity Module (ICM) proposed by \cite{pathak2017}. ICM defines curiosity as the error in predicting the consequence of an agent's own actions within a learned feature space. Specifically, it trains a self-supervised forward dynamics model to predict the next latent state given the current latent state and action. The magnitude of this prediction error then serves as the intrinsic reward. This design incentivizes the agent to explore states where its internal model is inaccurate, thereby driving learning about the environment's dynamics. Crucially, by operating in a learned feature space rather than raw pixels, ICM made an initial attempt to mitigate the "noisy TV problem," where agents might be perpetually drawn to uncontrollable stochastic elements (like static on a TV screen) that generate high prediction error but offer no meaningful learning progress. This focus on learnable and controllable aspects of the environment represented a significant step towards scalable curiosity in high-dimensional visual environments.

Despite ICM's success, its reliance on learning an accurate forward dynamics model can still be problematic, particularly in highly stochastic environments. In such settings, genuine environmental noise or inherent unpredictability can lead to consistently high prediction errors, which are uninformative for learning and can still distract the agent, leading to inefficient exploration. This limitation highlights a critical distinction: prediction error can arise from either the agent's lack of knowledge (epistemic uncertainty) or from inherent environmental stochasticity (aleatoric uncertainty). ICM, by primarily measuring the error of a single forward model, struggles to differentiate between these two sources, potentially leading to spurious curiosity signals.

To address this challenge and provide more robust curiosity signals, alternative prediction-error-based approaches have emerged. One prominent direction involves leveraging ensembles of models to quantify epistemic uncertainty more explicitly. For instance, methods like Exploration via Distributional Ensemble (EDE) \cite{jiang2023qmw} encourage exploration of states with high epistemic uncertainty by using an ensemble of Q-value distributions. The disagreement or variance among the predictions of these ensemble members provides a more reliable signal of what the agent truly "doesn't know," rather than simply what is unpredictable due to noise. This ensemble-based approach offers a principled way to direct exploration towards areas where the agent's understanding of the environment is weakest, promoting more efficient knowledge acquisition. The concept of using prediction error as a curiosity signal is also versatile, extending to domains like Large Language Models, where signals such as perplexity over generated responses or variance of value estimates from multi-head architectures can serve as intrinsic exploration bonuses \cite{dai2025h8g}. From an information-theoretic perspective, these methods align with the idea of maximizing information gain, where surprise (prediction error) and novelty drive the building of abstract dynamics models and transferable skills \cite{aubret2022inh}.

The versatility of these prediction-error-based curiosity mechanisms has led to their integration into various RL frameworks. For instance, \cite{li2019tj1} demonstrated how a simplified version of ICM could be effectively combined with off-policy RL methods, such as Deep Deterministic Policy Gradient (DDPG) and Hindsight Experience Replay (HER), significantly improving exploration efficiency and learning performance in robotic manipulation tasks with sparse rewards. Furthermore, the utility of curiosity-based intrinsic motivation extends to the challenging domain of offline reinforcement learning. \cite{lambert202277x} investigated how such curiosity-driven methods could be used to collect informative datasets in a task-agnostic manner, which could then be leveraged by offline RL algorithms, highlighting their role in generating high-quality data for subsequent learning.

While prediction error and self-supervised curiosity have proven highly effective in driving exploration by incentivizing agents to learn about their environment's dynamics, challenges remain. The primary limitation lies in distinguishing between genuine uncertainty that can be resolved through exploration and inherent environmental stochasticity that offers no meaningful learning progress. This distinction is crucial for designing intrinsic reward functions that consistently align with meaningful exploration, especially in complex, hierarchical tasks. The balance between intrinsic and extrinsic rewards, and the potential for agents to get stuck in "perpetual exploration" loops without making tangible task progress, are also critical considerations. Addressing the robustness of these intrinsic signals against uninformative stochasticity is a key area of ongoing research, motivating the development of more sophisticated methods that will be discussed in the subsequent section.