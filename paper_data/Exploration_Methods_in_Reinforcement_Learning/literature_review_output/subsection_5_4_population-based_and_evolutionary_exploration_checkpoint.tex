\subsection{Population-Based and Evolutionary Exploration}

The inherent challenge of the exploration-exploitation dilemma in Reinforcement Learning (RL) often leads single agents to converge prematurely to sub-optimal policies, particularly in environments characterized by sparse rewards or complex, multi-modal reward landscapes. To overcome these limitations, a distinct paradigm has emerged that leverages populations of agents or evolutionary algorithms to foster broader, more robust exploration and facilitate global search. These approaches represent a meta-level solution, structuring the learning system itself for enhanced discovery.

Early precursors to population-based exploration can be found in Neuroevolution, where neural network architectures and weights are optimized using evolutionary algorithms. Methods like NEAT (NeuroEvolution of Augmenting Topologies) \cite{stanley2002evolving} demonstrated the power of evolving diverse populations of networks to solve complex control tasks, implicitly performing exploration by searching a vast hypothesis space. More recently, Evolution Strategies (ES) have gained prominence as a scalable black-box optimization technique for deep RL, capable of training deep neural networks efficiently due to their high parallelizability \cite{salimans2017evolution}. However, standard ES can struggle with sparse or deceptive reward landscapes, necessitating directed exploration. To address this, \cite{conti2017cr2} introduced methods like Novelty Search with Evolution Strategies (NS-ES) and Quality Diversity (QD) algorithms hybridized with ES. These approaches maintain a population of novelty-seeking agents, rewarding exploration of novel behaviors rather than just high performance, thereby enabling ES to avoid local optima and achieve higher performance on challenging deep RL tasks like Atari games and simulated robot locomotion.

A significant advancement in population-based methods for deep RL is Population Based Training (PBT) \cite{jaderberg2017population}. PBT concurrently trains a population of agents, each with its own set of hyperparameters and model weights. Unlike traditional grid search or random search, PBT dynamically adapts hyperparameters during training by periodically evaluating agents, exploiting well-performing ones (copying their weights and hyperparameters) and exploring new hyperparameter configurations for underperforming ones. This asynchronous "exploit-and-explore" strategy allows PBT to discover robust hyperparameter schedules and model weights simultaneously, leading to faster training and improved final performance across diverse tasks, including complex deep RL benchmarks. PBT's strength lies in its ability to adaptively tune both learning processes and agent policies, making it highly effective for complex, high-dimensional problems.

Building on the strengths of both evolutionary algorithms and gradient-based RL, Evolutionary Reinforcement Learning (ERL) frameworks have emerged as a powerful hybrid paradigm. These methods typically combine the global search capabilities of evolutionary algorithms with the local optimization efficiency of gradient-based RL. Early ERL approaches, such as those by \cite{khadka2018evolutionary} and CEM-RL \cite{pourchot2019cemrl}, often involve evolving a population of actor networks, while a shared critic network (trained via gradient descent) provides value estimates to guide both the evolutionary process and individual actor updates. This hybrid approach aims to leverage the exploration benefits of evolution (e.g., escaping local optima, maintaining diversity) and the sample efficiency of RL. For instance, \cite{sun2024edc} proposed a modified ERL method for multi-agent region protection, amalgamating Differential Evolution (DE) for diverse sample exploration and overcoming sparse rewards with Multi-Agent Deep Deterministic Policy Gradient (MADDPG) for training defenders and expediting DE convergence.

A more recent advancement in this line is the Two-Stage Evolutionary Reinforcement Learning (TERL) framework proposed by \cite{zhu2024sb0}. TERL addresses a key limitation of prior ERL methods, which often evolve only actor networks, thereby constraining exploration if a single critic network falls into local optima. Instead, TERL maintains and optimizes a population of *complete RL agents*, each comprising both an actor and a critic network. This design enables more independent and diverse exploration by each individual, mitigating the risk of premature convergence to suboptimal policies dictated by a flawed shared critic. The TERL framework operates through a novel two-stage learning process: an initial "Exploration Stage" where all individuals learn independently, optimized by a hybrid approach combining gradient-based RL updates with meta-optimization techniques like Particle Swarm Optimization (PSO). This stage emphasizes diversification, with agents sharing information efficiently through a common replay buffer, which helps propagate beneficial experiences across the population. Following this, the "Exploitation Stage" focuses on refining the best-performing individual from the population through concentrated RL-based updates, while the remaining individuals continue to undergo PSO to further diversify the replay buffer. This dynamic allocation of computational resources and tailored optimization strategies across stages allows TERL to effectively balance the exploration-exploitation dilemma.

Despite their advantages, population-based and evolutionary methods introduce their own set of challenges. Computational cost is a primary concern, as maintaining and training multiple agents or evolving large populations can be resource-intensive, although parallelization strategies (like those in ES and PBT) mitigate this. Furthermore, the integration of diverse data from population optimization into off-policy RL algorithms, particularly through shared replay buffers, can introduce instability and even degrade performance, as highlighted by \cite{zheng2023u9k}. This issue arises because population data, while diverse, might not align with the on-policy distribution expected by some RL algorithms, leading to an "overlooked error." To remedy this, \cite{zheng2023u9k} proposed a double replay buffer design to provide more on-policy data, demonstrating the need for careful architectural considerations when combining these paradigms. The choice between PBT's hyperparameter evolution and ERL's policy evolution also presents a trade-off: PBT excels at finding robust training configurations, while ERL directly optimizes policy parameters, often leading to more direct policy improvement.

In conclusion, population-based and evolutionary exploration methods offer a compelling meta-level solution to the challenges of exploration in complex RL environments. By evolving populations of complete RL agents, dynamically adapting hyperparameters, or employing hybrid optimization strategies, these approaches enable more diverse learning trajectories and a more robust search for optimal policies, moving beyond the limitations of single-agent exploration heuristics. Future research could explore more sophisticated mechanisms for inter-agent information sharing, investigate adaptive intrinsic motivation signals within these population-based frameworks, or extend these concepts to multi-task and open-ended learning scenarios, further enhancing the adaptability and generalization capabilities of RL agents.