\subsection{Early Explicit Exploration Bonuses}
The fundamental challenge of exploration in reinforcement learning (RL) necessitates strategies that transcend purely random actions to efficiently discover optimal policies, particularly in environments characterized by sparse or delayed rewards. This subsection focuses on the pioneering methods that introduced explicit incentives for agents to explore novel or less-visited states, thereby laying the groundwork for more sophisticated intrinsic motivation techniques. These early approaches were crucial in demonstrating the power of directed exploration beyond mere stochasticity.

A seminal contribution to explicit exploration bonuses came from \cite{Thrun1992}, who introduced count-based exploration. In this paradigm, agents receive an additional, intrinsic reward for visiting states or taking actions less frequently encountered. The core idea is straightforward: by incentivizing novelty based on visitation frequency, the agent is directly encouraged to explore uncharted regions of the state space. This approach effectively transforms the problem of undirected search into a directed quest for new experiences, ensuring broader state space coverage in tabular settings. This principle aligns with the broader concept of "optimism in the face of uncertainty," where less-known options are optimistically valued higher to encourage their selection \cite{SuttonBarto2018}. Such count-based mechanisms share conceptual roots with strategies employed in the multi-armed bandit problem, where algorithms like Upper Confidence Bound (UCB) leverage visitation counts (or estimates of uncertainty) to balance exploitation of known good options with exploration of less-tried ones, thereby providing a theoretical basis for directed exploration in simpler settings.

Complementing frequency-based methods, the concept of \textit{recency-based} exploration also emerged as a valuable heuristic in early RL. While less formally enshrined in a single seminal work compared to count-based methods, the underlying idea was to grant exploration bonuses based on the time elapsed since a state was last visited, or to prioritize states that were recently discovered but not yet thoroughly explored. These approaches aimed to prevent agents from getting stuck in local optima by encouraging them to refresh their knowledge about "stale" or neglected parts of the environment. For instance, an agent might receive a bonus inversely proportional to the number of timesteps since its last visit to a particular state, ensuring that even frequently visited states are eventually re-explored if they haven't been seen for a while. Both count-based and recency-based methods, while distinct in their temporal focus, shared the common goal of directing exploration by explicitly rewarding the agent for interacting with less familiar parts of the environment, moving beyond the undirected nature of $\epsilon$-greedy exploration.

Despite their groundbreaking nature and effectiveness in controlled, tabular, or low-dimensional environments, these early explicit exploration bonuses faced significant limitations, primarily due to the curse of dimensionality. Count-based methods, by their very definition, require maintaining an accurate count for each unique state-action pair. In environments with large, continuous, or high-dimensional state spaces (e.g., visual observations from images), enumerating and tracking every distinct state becomes computationally infeasible and memory-prohibitive. The notion of a "unique state" itself becomes ill-defined in continuous spaces, making direct counting impossible. Similarly, recency-based methods also struggle in such complex settings, as tracking the last visit time for an astronomically large or continuous state space is equally impractical. The inability of these foundational explicit bonuses to scale effectively to real-world complexity underscored the need for more generalized and robust intrinsic motivation techniques that could approximate novelty in high-dimensional settings without explicit state enumeration.

In conclusion, early explicit exploration bonuses, encompassing both count-based (frequency) and recency-based heuristics, provided a critical foundation for directed exploration in reinforcement learning. They successfully demonstrated the power of incentivizing novelty to overcome the limitations of purely random search in environments where states could be distinctly enumerated. However, their inherent reliance on explicit state representations severely limited their applicability to tabular or low-dimensional environments. These fundamental scalability challenges, driven by the curse of dimensionality, highlighted the need for two distinct paths forward: firstly, the development of theoretically grounded approaches that could offer provable guarantees on learning efficiency in tractable domains (as discussed in Section 3); and secondly, the creation of more advanced intrinsic motivation techniques that could generalize the notion of a "count" or "novelty" to complex, high-dimensional domains without explicit state enumeration (as will be explored in Section 4.2).