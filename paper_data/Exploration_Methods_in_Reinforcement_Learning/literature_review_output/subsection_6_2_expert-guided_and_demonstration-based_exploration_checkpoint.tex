\subsection{Expert-Guided and Demonstration-Based Exploration}
Efficient exploration remains a formidable challenge in reinforcement learning (RL), particularly in complex, high-dimensional, and sparse-reward environments. To mitigate this, a significant body of research focuses on leveraging expert knowledge or demonstrations to guide and accelerate the exploration process, thereby improving sample efficiency and policy robustness. This paradigm, often termed Reinforcement Learning from Demonstrations (RLfD), seeks to bridge the gap between purely autonomous trial-and-error and the wealth of human or simulated expertise.

Early efforts established the foundational utility of demonstrations in overcoming exploration hurdles. \cite{nair2017crs} demonstrated that even a small set of expert demonstrations, when integrated with RL algorithms like Deep Deterministic Policy Gradients (DDPG) and Hindsight Experience Replay (HER), could provide an order of magnitude speedup in learning complex, continuous control robotics tasks with sparse rewards. These methods often relied on static, \textit{offline} datasets of expert trajectories, which provided initial guidance but presented inherent limitations. Building upon this, \cite{uchendu20221h1} proposed Jump-Start Reinforcement Learning (JSRL), which uses a "guide-policy" derived from offline data or demonstrations to form a curriculum of starting states for an "exploration-policy," significantly improving sample complexity, especially in data-scarce regimes. Similarly, \cite{hansen2022jm2} highlighted key ingredients for accelerating visual model-based RL with demonstrations, including policy pretraining, targeted exploration, and oversampling demonstration data, leading to substantial performance gains in sparse reward tasks.

Beyond direct trajectory imitation, expert knowledge can also be integrated through symbolic rules or domain-specific insights. For instance, \cite{hou2021c2r} introduced Rule-Aware Reinforcement Learning (RARL) for knowledge graph reasoning, injecting high-quality symbolic rules into the model's reasoning process to alleviate sparse rewards and prevent spurious paths. \cite{mazumder2022deb} proposed using "state-action permissibility" knowledge to guide exploration, drastically speeding up deep RL training by identifying and avoiding impermissible actions. In a similar vein, \cite{liu20228r4} combined human knowledge-based rule bases with imitation learning pre-training (ILDN) and safe RL to enhance efficiency and generalization in large-scale adversarial scenarios. Furthermore, theoretical frameworks have explored how exploration itself can be framed as a utility to be optimized, which demonstrations can implicitly help achieve \cite{zhang2020o5t, santi2024hct}.

Despite the benefits of offline demonstrations, a persistent challenge is the "distribution gap" \cite{coelho2024oa6}. This occurs when the agent's policy deviates from the expert's, leading it into states not covered by the static demonstration dataset, thereby hindering generalization and robustness. To address this, research has shifted towards more dynamic and interactive integration of expert knowledge. \cite{ball20235zm} showed that with minimal modifications, existing off-policy RL algorithms could effectively leverage offline data (including demonstrations) for online learning, achieving significant performance improvements. This offline-to-online fine-tuning paradigm is crucial for real-world applications \cite{rafailov2024wtw, hu2024085}, where policies pre-trained on demonstrations need to adapt to novel online experiences. For example, \cite{lu2025j7f} presented VLA-RL, an approach that uses online reinforcement learning to improve pretrained Vision-Language-Action (VLA) models, specifically addressing out-of-distribution failures that arise from limited offline data in robotic manipulation. Even leveraging "negative demonstrations" or failed experiences can guide exploration by showing what \textit{not} to do, as demonstrated by \cite{wu20248f9} in sparse reward environments.

A significant advancement in bridging the distribution gap and enhancing exploration efficiency comes from dynamically interacting with experts. \cite{hou20248b2} introduced EARLY, an active RL from demonstrations algorithm where the agent intelligently queries for episodic demonstrations based on its trajectory-level uncertainty. This approach makes expert guidance more targeted and resource-efficient by requesting help only when needed. The pinnacle of this dynamic interaction is exemplified by \cite{coelho2024oa6} with RLfOLD (Reinforcement Learning from Online Demonstrations) for urban autonomous driving. RLfOLD introduces the novel concept of \textit{online demonstrations}, which are dynamically collected from a simulator's privileged information during the agent's active exploration. These demonstrations are seamlessly integrated into a single replay buffer alongside agent experiences, directly addressing the distribution gap by providing contextually relevant expert guidance. The framework utilizes a modified Soft Actor-Critic (SAC) algorithm with a dual standard deviation policy network, outputting distinct $\sigma_{RL}$ for exploration and $\sigma_{IL}$ for imitation, allowing for adaptive balancing of these learning objectives. Furthermore, an uncertainty-based mechanism selectively invokes the online expert to guide the agent in challenging situations, making exploration more targeted and efficient. RLfOLD demonstrated superior performance in the CARLA NoCrash benchmark with significantly fewer resources, highlighting its effectiveness and efficiency in complex, real-time domains.

In conclusion, the intellectual trajectory of expert-guided exploration has evolved from static, offline demonstration datasets to dynamic, interactive, and online expert guidance. This progression effectively addresses critical challenges such as the distribution gap and sample inefficiency, particularly in safety-critical applications like autonomous driving and robotics. While methods leveraging offline demonstrations provide crucial initial boosts, the trend towards online and active expert interaction, exemplified by frameworks like RLfOLD, represents a significant step towards more robust, adaptive, and generalizable RL systems. Future research will likely focus on refining the mechanisms for generating and integrating online expert guidance, especially in real-world scenarios where "privileged information" is unavailable, and developing more sophisticated expert models that can provide nuanced and context-aware interventions.