{
  "title": "A Comprehensive Literature Review with Self-Reflection",
  "papers_processed": 240,
  "paper_list": [
    "c28ec2a40a2c77e20d64cf1c85dc931106df8e83.pdf",
    "0fcb2034e31a2bc2f12a2b1363d0d77baf445fdf.pdf",
    "45f573f302dc7e77cbc5d1a74ccbac3564bbebc8.pdf",
    "f8d8e192979ab5d8d593e76a0c4e2c7581778732.pdf",
    "2470fcf0f89082de874ac9133ccb3a8667dd89a8.pdf",
    "68c108795deef06fa929d1f6e96b75dbf7ce8531.pdf",
    "431dc05ac25510de6264084434254cca877f9ab3.pdf",
    "2c23085488337c4c1b5673b8d0f4ac95bda73529.pdf",
    "2064020586d5832b55f80a7dffea1fd90a5d94dd.pdf",
    "1c35807e1a4c24e2013fa0a090cee9cc4716a5f5.pdf",
    "f3a63a840185fa8c5e0db4bbe12afa7d3de7d029.pdf",
    "52a6657cf1cb3cc847695a386bd65b5eea34bc13.pdf",
    "12075ea34f5fbe32ec5582786761ab34d401209b.pdf",
    "dc05886db1e6f17f4489d867477b38fe13e31783.pdf",
    "6bf9b58b8f418fb2922762550fb78bb22c83c0f8.pdf",
    "6ce21379ffac786207632d16ea7d6e3eb150f910.pdf",
    "2e5dc4b028e670d1b9ac0b4e15580cdb7197cb68.pdf",
    "f593dc96b20ce8427182e773e3b2192d707706a8.pdf",
    "cc9f2fd320a279741403c4bfbeb91179803c428c.pdf",
    "3c3093f5f8e9601637612fcfb8f160f116fa30e4.pdf",
    "b2004f4f19bc6ccae8e4afc554f39142870100f5.pdf",
    "61f371768cdc093828f432660e22f7a17f22e2af.pdf",
    "1d6beec78e720beab5adb0a5fce6fa6b846aed1a.pdf",
    "ba44a95f1a8bc5765438d03c01137799e930c88d.pdf",
    "d27edce419e1c731c0aeb9e1842d1e022b2cc6ab.pdf",
    "116fc10b798e3294b00e92f2b8053d0c89ad9182.pdf",
    "0f810eb4777fd05317951ebaa7a3f5835ee84cf4.pdf",
    "468a3e85a2da0908c6f371ad83f6bcf6b6fdf193.pdf",
    "535d184eadf47fa17ce4073b6e2f180783e85300.pdf",
    "0d82360a4da311a277607db355dda3f196e8eb3d.pdf",
    "f6b218453170edcbb51e49dd44ba2f83af53ef92.pdf",
    "1f4484086d210a2c44efe5eef0a2b42647822abf.pdf",
    "04615a9955bce148aa7ba29e864389c26e10523a.pdf",
    "c64a99ec780d5e1c6ba242a82e72e96eb0c3ef8a.pdf",
    "7d05987db045c56fa691da40e679cd328f0b68ef.pdf",
    "399806e861a2ef960a81b37b593c2176a728c399.pdf",
    "174be0bacee04d9eb13a698d484ab5ae441c1100.pdf",
    "65587d4927fccc30788d3dfc9b639567721ff393.pdf",
    "2fd42844445ec644c2c44c093c3522c08b59cb45.pdf",
    "3e0925355554e3aeb99de8165c268582a82de3bb.pdf",
    "1fa2a04bffc18cfb46650a11ab0e5696d711f58f.pdf",
    "442e9f1e8f6218e68f944fd3028c5385691d4112.pdf",
    "a25b645c3d24f91164230a0ac5bb2d4ec88c1538.pdf",
    "2fc993c78cd84dafe4c36d2ac1cc25a6256aaa9d.pdf",
    "0ba7f2e592dda172bc3d07f88cdcf2a57830deec.pdf",
    "09da56cd3bf72b632c43969be97874fa14a3765c.pdf",
    "fc3b5dc5528e24dbbc8f4ec273e622ce40eec855.pdf",
    "46eb68c585bdb8a1051dfda98b4b35610301264f.pdf",
    "04efc9768a8e0c5f23b8c8504fb6db8803ffc071.pdf",
    "4df0238d5bfe0ab4cb8eaba67f4388c4e02dc2c8.pdf",
    "fb3c6456708b0e143f545d77dc8ec804eb947395.pdf",
    "248a25d697fe0132840e9d03c00aefadf03408d8.pdf",
    "7d1722451bff3b95e1d082864bb9b8437a0ddf41.pdf",
    "b0d376434a528ee69d98174d75b4a571c53247ae.pdf",
    "2822e79cb293f54e05fd8a7cd2844cfcc683f5d8.pdf",
    "fed0701afdfa6896057f7d04bd30ab1328eff110.pdf",
    "813f6e34feb3dc0346b6392d061af12ff186ba7e.pdf",
    "21828b3f05acffbfdf7de7259ad8b8ebd57fa351.pdf",
    "714b7c61d81687d093fd8b7d6d6737d582a4c9b7.pdf",
    "1e650cb12aaad371a49b0c8c4514e1b988a5178c.pdf",
    "1d2acae1d631e932c7ebe96fad3c3b04909e5cee.pdf",
    "fe7382db243694c67c667cf2ec80072577d2372b.pdf",
    "ba0e111b711e9b577932a02c9e40bc44b56cb88d.pdf",
    "cf628a42ee56c8f1b858790822a2bc0a61a49110.pdf",
    "fbcace16369032bb0292754bd78d03b68b554a95.pdf",
    "be33087668f98ac746e72999178d7641d27412f9.pdf",
    "cae05340421a18c64ac0897d57bcdcc9a496a3b8.pdf",
    "cac7f83769836707b02adadb0cda8c791ca23c92.pdf",
    "f4712cdb025ba4ab879bb47d5cf693a4923f1532.pdf",
    "b1be7ce8c639291adf7663535f0451f9ac03ed55.pdf",
    "a064b8183d657178916ae21c43b5099bfef6804d.pdf",
    "bc98c81467ed3a6b21788f39c20cbe659014e551.pdf",
    "621d57c1243f055bc3850c1f3e38f351f53c947f.pdf",
    "9b5aa6a75d8e9f0e68d12e3b331acad6f32667d4.pdf",
    "6c66fc8000da4d80bb57e60667e35a051016144a.pdf",
    "5fd3ce235f5fcebd3d2807f710b060add527183b.pdf",
    "a011901fd788fc5ad452dd3d88f9f0970ea547a8.pdf",
    "3efc894d0990faeb2f69194195d465ed64694104.pdf",
    "46bd3c20e6f7a9b6e413ea9f3452965601fd6de9.pdf",
    "1d4288c47a7802575d2a0d231d1283a4f225a85b.pdf",
    "1a39ad2d1553d07084b5e44a28878c8bb5018cef.pdf",
    "ecf5dc817fd6326e943b759c889d1285e673b24a.pdf",
    "02ad21eea9ec32783ba529487e74a76e85499a53.pdf",
    "a4a509d9019deac486087a0b10158ac115274de6.pdf",
    "a17a7256c04afee68f9aa0b7bfdc67fbca998b9c.pdf",
    "69bdc99655204190697067c3da5296e544e6865d.pdf",
    "abaa1a3a8468473fd2827e49623eabc36ffaf8fe.pdf",
    "2029ebd195491dd845e14866045225b238f6c392.pdf",
    "c447bc7891c2a3a775af4f8fb94e34e7968c1cb7.pdf",
    "70e1d6b227fdd605fe61239a953e803df97e521d.pdf",
    "a45df3efbd472d4d43ddc4072c61f7f674981ac6.pdf",
    "5c0b56d9440ea70c79d1d0032d46c14e06f541f5.pdf",
    "0cdbd90989e5f60b6a42dae76b23bd489fcf65cc.pdf",
    "5f3b337e74618a2364778222162b13bd55a15e27.pdf",
    "b0c40766974df3eae8ff500379e66e5566cd16c9.pdf",
    "4c4d41a977e6d8d82b6cc19ac28e50de3c814fd8.pdf",
    "3d3b0704c61d47c7bafb70ae2670b2786b8e4d81.pdf",
    "9e5fe2ba652774ba3b1127f626c192668a907132.pdf",
    "678cd30fac3fcf215d0e714936f6907ecc6d4361.pdf",
    "103f1674121780097f896ffe525bab2c6ae0bcdc.pdf",
    "de93c8aed64229571b03e40b36499d4f07ce875d.pdf",
    "9d1445f1845a2880ff9c752845660e9c294aa7b5.pdf",
    "83f1343500c9f0df62da0d61736738d8d7a9bba0.pdf",
    "2e1ca97d8f7604e3b334ff4903bfa67267379317.pdf",
    "7551b7f7420087de59ad36e445cb3bdef9c382ee.pdf",
    "b6ffc15ab55281dcf08b6a19d6f8bfcb190765b8.pdf",
    "06318722ebbac1d9b59e2408ecd3994eadb0ec6b.pdf",
    "1f577f6f0785c7967bef4ac6d0ab0370c2815b4d.pdf",
    "33e3f13087abd5241d55523140720f5e684b7bee.pdf",
    "23e3f39ede3c45cb8cf456de1f56d7a8b8d4bc2e.pdf",
    "97fe089acbe9d1f55753cd6b2ad6070e89521f6c.pdf",
    "6d97b81b3473492cb9986a63886cbb128496010c.pdf",
    "807f377de905eda62e4cd2f0797153a59296adbb.pdf",
    "f14645d3a0740504ee632ab06f045cceaa5297bc.pdf",
    "f715558b65fd4f3c6966505c237d9a622947010b.pdf",
    "e0d4af45fa3073dbfa9b6e464fa88d52e6f06928.pdf",
    "f80432fa03c91283cadbf6c262a7bc6e45f4edd3.pdf",
    "117cd9e1dafc577e53e2d46897a784ed1e65996f.pdf",
    "46cbd8acfacca5fc74b8d05bb06264aff0d82a4e.pdf",
    "48de0dab9255ededce3cdaeac84f039d726f7f3e.pdf",
    "071de005741bd666c7a9ccf40d5ed1d502f5282b.pdf",
    "3f673101c2cac3b47639056e2988e018546c3c90.pdf",
    "24107405a96a53d4c292b08608300a6c7e457ffe.pdf",
    "57f2811d8d8da81f2b4ed80097f5e47a49d76d19.pdf",
    "bd38cbbb346a347cb5b60ac4a133b3d73cb44e07.pdf",
    "1b1efa2f9731ab3801c46bfc877695d41e437406.pdf",
    "08e84c939b88fc50aaa74ef76e202e61a1ad940b.pdf",
    "5bac7d00035bc1e246a34f9ee3152b290f97bb92.pdf",
    "043582a1ed2d2b7d08a804bafe9db188e9a65d96.pdf",
    "139a84c6fc4887ce2374489d79af0df9e1e7e4d6.pdf",
    "26662adf92cacf0810a14faa514360f270e97b53.pdf",
    "914eaadede7a95116362cd6982321f93044b3b19.pdf",
    "7d6168fbd3ed72f9098573007f4b8c2ec9e576b9.pdf",
    "a694a3fb49f3c6e63e2fa5ddb72fe3aa490592f1.pdf",
    "25db1b77bc330476c3cf6ce43236404c578b4372.pdf",
    "4e98282f5f3f1a388b8d95380473d4ef4878266e.pdf",
    "abeb46288d537f98f76b979040a547ee81216377.pdf",
    "4a3e88d203564e547f5fb3f3d816a0b381492eae.pdf",
    "1a73038804052a40c12aae696848ece2168f6da7.pdf",
    "79f923d6575bd8253e2f0b70813caa61a870ccee.pdf",
    "ff87e30cb969d40b1d5c8ddc8932427793467f29.pdf",
    "aedd5c4bc11d8faf01e8456c91a183f2f76e5778.pdf",
    "e0bf76edd8e6b793b81c16a915ede48e377ebab6.pdf",
    "5f5e9ec8bcc5e83eefecc0565bdc004f929f4723.pdf",
    "d60df0754df6ccb14c563f07f865f391da3cba2d.pdf",
    "03c7f9a6b743eb1c9127b8d4552147b2b1a0c1e1.pdf",
    "7bd4edf878976d329f326f3a12675a66cbc075e9.pdf",
    "c90aa0f206c6fd41c490c142f63f7ba046cae6b7.pdf",
    "53db22a9d4ae77dd8218ba867184898adc84d1d1.pdf",
    "fbc0b5e1b822796d7ae97268def2e0993b5da644.pdf",
    "97c7066b53b208b24001ad0dcc49a851fcf13bfd.pdf",
    "81fdf2b3db2b3e7be90b51866a31b73587eecd30.pdf",
    "e4fef8d5864c5468100ca167639ef3fa374c0442.pdf",
    "7da3eebf5c2f481bc38ae6115cec3fc71dc87f2b.pdf",
    "2e6534fbeb932fc058b9f865c0b25a3f201a0704.pdf",
    "01936f6df3c760d23df237d8d15cb7faadce9520.pdf",
    "c225c6e24526c160bbceaaa36b447df9d8e95f13.pdf",
    "c734971c6000e3f2769ab5165d00816af80dd76f.pdf",
    "ed8ea3d06c173849f02ee8afcf8db07df0f31261.pdf",
    "dcbafb5ec43572d6ca170d86569d09a5dd40a8f8.pdf",
    "492f441bc6fdbb5f4b9273197ae563126439abeb.pdf",
    "4d3ffd8feca81d750aa30122b49cc1e874e70c1a.pdf",
    "40d1e0a1e8a861305f9354be747620782fc203ce.pdf",
    "736c35ed3e8e86ce99e02ce117dba7ee77e60dda.pdf",
    "a69adfee23dbc90b2292499ffb1bf9fbd7d656c5.pdf",
    "e5d6cca71ea0fb216a25f86e96d3480886fdba27.pdf",
    "5ecf53ab083f72f10421225a7dde25eb51cb6b22.pdf",
    "4186f74da6d793c91c5ca4d8a10cf2f8638eade6.pdf",
    "eb6b79b00d43fa03945ae2477c60e79edaf72d28.pdf",
    "b093a3fa79512c48524f81c754bddec7b16afb17.pdf",
    "9a67ff1d46d691f7741822d7a13587a517b1be14.pdf",
    "134acf45a016fced57cc8f8e171eeb6e0015b0b8.pdf",
    "a9c896060fa85f01f289baaad346e98e94dbed4c.pdf",
    "68de2fdc9b516eba96b9726dfa0e77ee66336605.pdf",
    "01f35fa70fc881ab80206121738380c57f8d2074.pdf",
    "5dce3f2cfe018ada4bebeaa15aac6fe1d01b18ba.pdf",
    "390ba2d438a33bf0add55e0e7b2831fc034db41f.pdf",
    "085dddfa3105967d4b9f09b1cd0fa7725779faf1.pdf",
    "99d0760520eaa1dc81b5531cd45101e00474bbd1.pdf",
    "3da8d2e6a0f8d5d62cbaae18235235483144c6a0.pdf",
    "086a4d45a9deef5a10ee4febcd4c92c95a6305de.pdf",
    "3dee83a4b0fadde414e00ff350940303eb859be1.pdf",
    "22c1ec46a81e9db6194b8784f4fe431f71953757.pdf",
    "9c7209f3d6b9bf362ea4d34730f34cf8511967f4.pdf",
    "200726cba07dec06a56ff46aa38836e9730a23a2.pdf",
    "3c483c11f5fd9234576a93aea71a7ec1435b8514.pdf",
    "d06737f9395e592f35ef251e09bea1c18037b096.pdf",
    "3ef01975a45bcf78120d76c1bc73e8ec12e213bf.pdf",
    "39d2839aa4c3d8c0e64553891fe98ba261703154.pdf",
    "cb7ea0176eec1f809f3bc3a34aeb279d44dda612.pdf",
    "b31c76815615c16cc8505dbb38d2921f921c029d.pdf",
    "c1844cda42b3732a5576d05bb6e007eb1db00919.pdf",
    "8af5e79310ec1d8529eba38705e5f29dce789b00.pdf",
    "5221ba291d5901f950220f50d289d5e01d81b0c4.pdf",
    "8ae6c7cff8bb2d766f5f9d3585cd262032378b33.pdf",
    "dad0fcbc6f2c9b7c4d7b7b1d4513d94d57ea63c6.pdf",
    "ae47e4e2a4b749543c4de8624049c1d368cbc859.pdf",
    "c6ed1f7f478f9d004d5f6b9783df79f82d0d1464.pdf",
    "2807f9c666335946113fb11dccadf36f8d78b772.pdf",
    "9eed36fb9b9bbb97579953d5e303dc0cf6c70a58.pdf",
    "9c9a527ad885e664a31f0b70ec5f838d0c6ebfa6.pdf",
    "8ca9a74503c240b2746e351995ee0415657f1cd0.pdf",
    "06ffbf40c29daa4f6b4c5025d96ab1c27c0ea117.pdf",
    "7f437f4af59ff994d97482ee1c12aaeb4b310e85.pdf",
    "839395c4823ac8fff990485e7ce54e53c94bae6b.pdf",
    "7825ea27ec1762f6ac41347603535500bcd121f7.pdf",
    "859172d8cb31ddebd9b18e3a7a7d982fab1d1341.pdf",
    "f58ada4680d374f5dba96b5e7fcb5b7be11a6acc.pdf",
    "aa65704a16138790678e2b9b59ae679b6c9353d7.pdf",
    "bd8aaab29fa16f40ef016393ba7ca30127abab58.pdf",
    "a6c5e49425ac01c534d9663a4453b28f6dc76f7c.pdf",
    "d8bc75fbcfdecd5cd1952524d3e07db13722f5ff.pdf",
    "17682d6f13dcac703092e0f1500a4197e46bbf2c.pdf",
    "11c34b84c3ad6587529517c32923c446797c63e6.pdf",
    "d929e0bc4e30910527a714f239b5e5b37a7ec6ad.pdf",
    "516c6ab3feab17bc158f12ef6768b26c603566b8.pdf",
    "f5e09973834f852237a7d9db6583c7e6615a907d.pdf",
    "b43f8bacd80f32734cdff4f9d8c79e397872aed6.pdf",
    "8357670aac3c98a71b454ab5bca89558f265369d.pdf",
    "58db2247187ac01acabc1c2fa02f9b189772729e.pdf",
    "e401ba782c2da93959582295089d3f04a051d6c1.pdf",
    "afa538f59cf2996837863be60a34eef5271a5ee9.pdf",
    "88f98e3629a7aef2312f9e214ed2a75672113e4f.pdf",
    "c4aafb184f285d004d8c8072b5d6408e876428e1.pdf",
    "e825e6325dfb5b93066817e7cc6b226ba7d3b799.pdf",
    "3416214ca1d4f790a048ece4229829333e836b4d.pdf",
    "6bf550ce7f10a4347d448eb810a92e1a5cfffa4b.pdf",
    "54cb726595cd8c03f59f49c9a97b76b4d3f932f2.pdf",
    "b44b0e8222021b51783c62b0bce071ef6fb3f12c.pdf",
    "5ac5f3827ee4c32cebadbc9de8a892853575efb9.pdf",
    "cce1245ba1ec154120b3b256faf7bf28f769b505.pdf",
    "37fe2a997bf07a972473abd079d175335940e6bd.pdf",
    "e32e28a8a06739997957113b7fa1bd033f6801ba.pdf",
    "c81a06446fa1df12cc043d9d9c8cfd5775090c59.pdf",
    "66c5bed508f9dce55b2beeaaf36a29929c0bc2ac.pdf",
    "2389fafc2a97e13fa810c4014babe73bd886c06f.pdf",
    "b2d827c286e32dbf0739e8c796b119b1074809b4.pdf",
    "fae722ae17483aeef3485f0177346ba3ce332ea9.pdf",
    "dca6c5f8f6c64b4188212e16c2faf461ffd4e49a.pdf",
    "9ac3f4e74e9837cb47f43211cf143d4c7115e7f1.pdf"
  ],
  "citations_map": {
    "c28ec2a40a2c77e20d64cf1c85dc931106df8e83.pdf": "nair2017crs",
    "0fcb2034e31a2bc2f12a2b1363d0d77baf445fdf.pdf": "tang20166wr",
    "45f573f302dc7e77cbc5d1a74ccbac3564bbebc8.pdf": "lee2021qzk",
    "f8d8e192979ab5d8d593e76a0c4e2c7581778732.pdf": "hu2020qwm",
    "2470fcf0f89082de874ac9133ccb3a8667dd89a8.pdf": "stadie20158af",
    "68c108795deef06fa929d1f6e96b75dbf7ce8531.pdf": "gupta2018rge",
    "431dc05ac25510de6264084434254cca877f9ab3.pdf": "thananjeyan2020d20",
    "2c23085488337c4c1b5673b8d0f4ac95bda73529.pdf": "wu2021r67",
    "2064020586d5832b55f80a7dffea1fd90a5d94dd.pdf": "conti2017cr2",
    "1c35807e1a4c24e2013fa0a090cee9cc4716a5f5.pdf": "seo2022cjf",
    "f3a63a840185fa8c5e0db4bbe12afa7d3de7d029.pdf": "uchendu20221h1",
    "52a6657cf1cb3cc847695a386bd65b5eea34bc13.pdf": "li2020r8r",
    "12075ea34f5fbe32ec5582786761ab34d401209b.pdf": "yang2021ngm",
    "dc05886db1e6f17f4489d867477b38fe13e31783.pdf": "lee2019hnz",
    "6bf9b58b8f418fb2922762550fb78bb22c83c0f8.pdf": "zhang2020o5t",
    "6ce21379ffac786207632d16ea7d6e3eb150f910.pdf": "chang20221gc",
    "2e5dc4b028e670d1b9ac0b4e15580cdb7197cb68.pdf": "yang2021psl",
    "f593dc96b20ce8427182e773e3b2192d707706a8.pdf": "li2022ktf",
    "cc9f2fd320a279741403c4bfbeb91179803c428c.pdf": "liang20226ix",
    "3c3093f5f8e9601637612fcfb8f160f116fa30e4.pdf": "hong20182pr",
    "b2004f4f19bc6ccae8e4afc554f39142870100f5.pdf": "hansen2022jm2",
    "61f371768cdc093828f432660e22f7a17f22e2af.pdf": "pong2021i4o",
    "1d6beec78e720beab5adb0a5fce6fa6b846aed1a.pdf": "jia2021kxs",
    "ba44a95f1a8bc5765438d03c01137799e930c88d.pdf": "zhang2022dgg",
    "d27edce419e1c731c0aeb9e1842d1e022b2cc6ab.pdf": "dorfman20216nq",
    "116fc10b798e3294b00e92f2b8053d0c89ad9182.pdf": "tai2016bp8",
    "0f810eb4777fd05317951ebaa7a3f5835ee84cf4.pdf": "martin2017bgt",
    "468a3e85a2da0908c6f371ad83f6bcf6b6fdf193.pdf": "rckin2021yud",
    "535d184eadf47fa17ce4073b6e2f180783e85300.pdf": "zhelo2018wi8",
    "0d82360a4da311a277607db355dda3f196e8eb3d.pdf": "zhang2020bse",
    "f6b218453170edcbb51e49dd44ba2f83af53ef92.pdf": "mavrin2019iqm",
    "1f4484086d210a2c44efe5eef0a2b42647822abf.pdf": "li2021w3q",
    "04615a9955bce148aa7ba29e864389c26e10523a.pdf": "schumacher2022x3f",
    "c64a99ec780d5e1c6ba242a82e72e96eb0c3ef8a.pdf": "aubret2022inh",
    "7d05987db045c56fa691da40e679cd328f0b68ef.pdf": "yuan2020epo",
    "399806e861a2ef960a81b37b593c2176a728c399.pdf": "rezaeifar20211eu",
    "174be0bacee04d9eb13a698d484ab5ae441c1100.pdf": "talaat2022ywa",
    "65587d4927fccc30788d3dfc9b639567721ff393.pdf": "xin2022qcl",
    "2fd42844445ec644c2c44c093c3522c08b59cb45.pdf": "dang2022kwh",
    "3e0925355554e3aeb99de8165c268582a82de3bb.pdf": "raffin2020o1a",
    "1fa2a04bffc18cfb46650a11ab0e5696d711f58f.pdf": "liu2018jde",
    "442e9f1e8f6218e68f944fd3028c5385691d4112.pdf": "sun2022ul9",
    "a25b645c3d24f91164230a0ac5bb2d4ec88c1538.pdf": "nikolov20184g9",
    "2fc993c78cd84dafe4c36d2ac1cc25a6256aaa9d.pdf": "qiao20220gx",
    "0ba7f2e592dda172bc3d07f88cdcf2a57830deec.pdf": "yu20222xi",
    "09da56cd3bf72b632c43969be97874fa14a3765c.pdf": "lambert202277x",
    "fc3b5dc5528e24dbbc8f4ec273e622ce40eec855.pdf": "woczyk20220mn",
    "46eb68c585bdb8a1051dfda98b4b35610301264f.pdf": "qu2022uym",
    "04efc9768a8e0c5f23b8c8504fb6db8803ffc071.pdf": "sun2020zjg",
    "4df0238d5bfe0ab4cb8eaba67f4388c4e02dc2c8.pdf": "tao202294e",
    "fb3c6456708b0e143f545d77dc8ec804eb947395.pdf": "houthooft2016yee",
    "248a25d697fe0132840e9d03c00aefadf03408d8.pdf": "shi20215fg",
    "7d1722451bff3b95e1d082864bb9b8437a0ddf41.pdf": "li2021l92",
    "b0d376434a528ee69d98174d75b4a571c53247ae.pdf": "liu20220g4",
    "2822e79cb293f54e05fd8a7cd2844cfcc683f5d8.pdf": "hu20195n2",
    "fed0701afdfa6896057f7d04bd30ab1328eff110.pdf": "wang2022boj",
    "813f6e34feb3dc0346b6392d061af12ff186ba7e.pdf": "yu20213c1",
    "21828b3f05acffbfdf7de7259ad8b8ebd57fa351.pdf": "zheng2022816",
    "714b7c61d81687d093fd8b7d6d6737d582a4c9b7.pdf": "yang2022mx5",
    "1e650cb12aaad371a49b0c8c4514e1b988a5178c.pdf": "yang2022fou",
    "1d2acae1d631e932c7ebe96fad3c3b04909e5cee.pdf": "liu2022uiv",
    "fe7382db243694c67c667cf2ec80072577d2372b.pdf": "hou2021c2r",
    "ba0e111b711e9b577932a02c9e40bc44b56cb88d.pdf": "lale2020xqs",
    "cf628a42ee56c8f1b858790822a2bc0a61a49110.pdf": "otto2022qef",
    "fbcace16369032bb0292754bd78d03b68b554a95.pdf": "lakhani2021217",
    "be33087668f98ac746e72999178d7641d27412f9.pdf": "huang2020wll",
    "cae05340421a18c64ac0897d57bcdcc9a496a3b8.pdf": "yuan2022hto",
    "cac7f83769836707b02adadb0cda8c791ca23c92.pdf": "muzahid2022fyb",
    "f4712cdb025ba4ab879bb47d5cf693a4923f1532.pdf": "zhang20229rg",
    "b1be7ce8c639291adf7663535f0451f9ac03ed55.pdf": "sierragarca2020g35",
    "a064b8183d657178916ae21c43b5099bfef6804d.pdf": "han20199g2",
    "bc98c81467ed3a6b21788f39c20cbe659014e551.pdf": "wabersich2018t86",
    "621d57c1243f055bc3850c1f3e38f351f53c947f.pdf": "bourel2020tnm",
    "9b5aa6a75d8e9f0e68d12e3b331acad6f32667d4.pdf": "cheng20224w2",
    "6c66fc8000da4d80bb57e60667e35a051016144a.pdf": "ceusters2022drp",
    "5fd3ce235f5fcebd3d2807f710b060add527183b.pdf": "stanton20183fs",
    "a011901fd788fc5ad452dd3d88f9f0970ea547a8.pdf": "cideron2020kdj",
    "3efc894d0990faeb2f69194195d465ed64694104.pdf": "liu2022nhx",
    "46bd3c20e6f7a9b6e413ea9f3452965601fd6de9.pdf": "cho2022o2c",
    "1d4288c47a7802575d2a0d231d1283a4f225a85b.pdf": "zhang2020xq9",
    "1a39ad2d1553d07084b5e44a28878c8bb5018cef.pdf": "song2021elb",
    "ecf5dc817fd6326e943b759c889d1285e673b24a.pdf": "wang20229ce",
    "02ad21eea9ec32783ba529487e74a76e85499a53.pdf": "lin2022vqo",
    "a4a509d9019deac486087a0b10158ac115274de6.pdf": "zhang2022egf",
    "a17a7256c04afee68f9aa0b7bfdc67fbca998b9c.pdf": "zhou2022fny",
    "69bdc99655204190697067c3da5296e544e6865d.pdf": "yu2022bo5",
    "abaa1a3a8468473fd2827e49623eabc36ffaf8fe.pdf": "xie2015vwy",
    "2029ebd195491dd845e14866045225b238f6c392.pdf": "zhang2019yjm",
    "c447bc7891c2a3a775af4f8fb94e34e7968c1cb7.pdf": "wu2021mht",
    "70e1d6b227fdd605fe61239a953e803df97e521d.pdf": "fu20220cl",
    "a45df3efbd472d4d43ddc4072c61f7f674981ac6.pdf": "mndezmolina2022ec5",
    "5c0b56d9440ea70c79d1d0032d46c14e06f541f5.pdf": "steinparz20220nl",
    "0cdbd90989e5f60b6a42dae76b23bd489fcf65cc.pdf": "rahman2022p7b",
    "5f3b337e74618a2364778222162b13bd55a15e27.pdf": "xu2022cgd",
    "b0c40766974df3eae8ff500379e66e5566cd16c9.pdf": "lee2020k9k",
    "4c4d41a977e6d8d82b6cc19ac28e50de3c814fd8.pdf": "li2022ec4",
    "3d3b0704c61d47c7bafb70ae2670b2786b8e4d81.pdf": "wang2022t55",
    "9e5fe2ba652774ba3b1127f626c192668a907132.pdf": "whitney2021xlu",
    "678cd30fac3fcf215d0e714936f6907ecc6d4361.pdf": "suri20226rr",
    "103f1674121780097f896ffe525bab2c6ae0bcdc.pdf": "xin2020y4j",
    "de93c8aed64229571b03e40b36499d4f07ce875d.pdf": "matheron2020zmh",
    "9d1445f1845a2880ff9c752845660e9c294aa7b5.pdf": "yang2022j0z",
    "83f1343500c9f0df62da0d61736738d8d7a9bba0.pdf": "wu2022sot",
    "2e1ca97d8f7604e3b334ff4903bfa67267379317.pdf": "kessler202295l",
    "7551b7f7420087de59ad36e445cb3bdef9c382ee.pdf": "raffin2020ka2",
    "b6ffc15ab55281dcf08b6a19d6f8bfcb190765b8.pdf": "yang20206wi",
    "06318722ebbac1d9b59e2408ecd3994eadb0ec6b.pdf": "liu20228r4",
    "1f577f6f0785c7967bef4ac6d0ab0370c2815b4d.pdf": "kamalova2022jpm",
    "33e3f13087abd5241d55523140720f5e684b7bee.pdf": "zhang2022p0b",
    "23e3f39ede3c45cb8cf456de1f56d7a8b8d4bc2e.pdf": "li20227ss",
    "97fe089acbe9d1f55753cd6b2ad6070e89521f6c.pdf": "huang2022or8",
    "6d97b81b3473492cb9986a63886cbb128496010c.pdf": "modi2019fs3",
    "807f377de905eda62e4cd2f0797153a59296adbb.pdf": "shi20215ek",
    "f14645d3a0740504ee632ab06f045cceaa5297bc.pdf": "zhang2021qq6",
    "f715558b65fd4f3c6966505c237d9a622947010b.pdf": "yang2020dxb",
    "e0d4af45fa3073dbfa9b6e464fa88d52e6f06928.pdf": "bing2019py7",
    "f80432fa03c91283cadbf6c262a7bc6e45f4edd3.pdf": "zhang20192ef",
    "117cd9e1dafc577e53e2d46897a784ed1e65996f.pdf": "hu2020yhq",
    "46cbd8acfacca5fc74b8d05bb06264aff0d82a4e.pdf": "kumar20216sy",
    "48de0dab9255ededce3cdaeac84f039d726f7f3e.pdf": "asiain2018wxr",
    "071de005741bd666c7a9ccf40d5ed1d502f5282b.pdf": "li2019tj1",
    "3f673101c2cac3b47639056e2988e018546c3c90.pdf": "sun2020c1p",
    "24107405a96a53d4c292b08608300a6c7e457ffe.pdf": "su2020k2m",
    "57f2811d8d8da81f2b4ed80097f5e47a49d76d19.pdf": "liu2020o0c",
    "bd38cbbb346a347cb5b60ac4a133b3d73cb44e07.pdf": "ball20235zm",
    "1b1efa2f9731ab3801c46bfc877695d41e437406.pdf": "meng2025l1q",
    "08e84c939b88fc50aaa74ef76e202e61a1ad940b.pdf": "dou2024kjg",
    "5bac7d00035bc1e246a34f9ee3152b290f97bb92.pdf": "lee202337c",
    "043582a1ed2d2b7d08a804bafe9db188e9a65d96.pdf": "ma2024r2p",
    "139a84c6fc4887ce2374489d79af0df9e1e7e4d6.pdf": "matthews20241yx",
    "26662adf92cacf0810a14faa514360f270e97b53.pdf": "xi2024e2i",
    "914eaadede7a95116362cd6982321f93044b3b19.pdf": "zhang20242te",
    "7d6168fbd3ed72f9098573007f4b8c2ec9e576b9.pdf": "xi2024tj9",
    "a694a3fb49f3c6e63e2fa5ddb72fe3aa490592f1.pdf": "cheng2024vjq",
    "25db1b77bc330476c3cf6ce43236404c578b4372.pdf": "sun20238u5",
    "4e98282f5f3f1a388b8d95380473d4ef4878266e.pdf": "xu2023t6r",
    "abeb46288d537f98f76b979040a547ee81216377.pdf": "zhang2025wku",
    "4a3e88d203564e547f5fb3f3d816a0b381492eae.pdf": "lu2025j7f",
    "1a73038804052a40c12aae696848ece2168f6da7.pdf": "jiang2023qmw",
    "79f923d6575bd8253e2f0b70813caa61a870ccee.pdf": "zhang20244ty",
    "ff87e30cb969d40b1d5c8ddc8932427793467f29.pdf": "gu2024fu3",
    "aedd5c4bc11d8faf01e8456c91a183f2f76e5778.pdf": "ma2024b33",
    "e0bf76edd8e6b793b81c16a915ede48e377ebab6.pdf": "yan2024p3y",
    "5f5e9ec8bcc5e83eefecc0565bdc004f929f4723.pdf": "chen2023ymk",
    "d60df0754df6ccb14c563f07f865f391da3cba2d.pdf": "li2024drs",
    "03c7f9a6b743eb1c9127b8d4552147b2b1a0c1e1.pdf": "huang202366f",
    "7bd4edf878976d329f326f3a12675a66cbc075e9.pdf": "jin2024035",
    "c90aa0f206c6fd41c490c142f63f7ba046cae6b7.pdf": "ishfaq20235fo",
    "53db22a9d4ae77dd8218ba867184898adc84d1d1.pdf": "guo2024sba",
    "fbc0b5e1b822796d7ae97268def2e0993b5da644.pdf": "surina2025smk",
    "97c7066b53b208b24001ad0dcc49a851fcf13bfd.pdf": "celik202575j",
    "81fdf2b3db2b3e7be90b51866a31b73587eecd30.pdf": "hua2023omp",
    "e4fef8d5864c5468100ca167639ef3fa374c0442.pdf": "sukhija2024zz8",
    "7da3eebf5c2f481bc38ae6115cec3fc71dc87f2b.pdf": "hsu2024tqd",
    "2e6534fbeb932fc058b9f865c0b25a3f201a0704.pdf": "wang20248rm",
    "01936f6df3c760d23df237d8d15cb7faadce9520.pdf": "ghamari2024bbm",
    "c225c6e24526c160bbceaaa36b447df9d8e95f13.pdf": "zhang2024ppn",
    "c734971c6000e3f2769ab5165d00816af80dd76f.pdf": "dai2024x3l",
    "ed8ea3d06c173849f02ee8afcf8db07df0f31261.pdf": "shuai2025fq3",
    "dcbafb5ec43572d6ca170d86569d09a5dd40a8f8.pdf": "stolz20240y2",
    "492f441bc6fdbb5f4b9273197ae563126439abeb.pdf": "tappler2024nm1",
    "4d3ffd8feca81d750aa30122b49cc1e874e70c1a.pdf": "rimon20243o6",
    "40d1e0a1e8a861305f9354be747620782fc203ce.pdf": "terven202599m",
    "736c35ed3e8e86ce99e02ce117dba7ee77e60dda.pdf": "hsiao2024wps",
    "a69adfee23dbc90b2292499ffb1bf9fbd7d656c5.pdf": "kakooee2024w9m",
    "e5d6cca71ea0fb216a25f86e96d3480886fdba27.pdf": "rafailov2024wtw",
    "5ecf53ab083f72f10421225a7dde25eb51cb6b22.pdf": "goldie2024cuf",
    "4186f74da6d793c91c5ca4d8a10cf2f8638eade6.pdf": "coelho2024oa6",
    "eb6b79b00d43fa03945ae2477c60e79edaf72d28.pdf": "huang2024nh4",
    "b093a3fa79512c48524f81c754bddec7b16afb17.pdf": "cho2023z4l",
    "9a67ff1d46d691f7741822d7a13587a517b1be14.pdf": "shi20258tu",
    "134acf45a016fced57cc8f8e171eeb6e0015b0b8.pdf": "li2024ge1",
    "a9c896060fa85f01f289baaad346e98e94dbed4c.pdf": "ghasemi2024j43",
    "68de2fdc9b516eba96b9726dfa0e77ee66336605.pdf": "liu2023729",
    "01f35fa70fc881ab80206121738380c57f8d2074.pdf": "shang202305k",
    "5dce3f2cfe018ada4bebeaa15aac6fe1d01b18ba.pdf": "liu2024xkk",
    "390ba2d438a33bf0add55e0e7b2831fc034db41f.pdf": "kantaros2024sgn",
    "085dddfa3105967d4b9f09b1cd0fa7725779faf1.pdf": "li2024zix",
    "99d0760520eaa1dc81b5531cd45101e00474bbd1.pdf": "ang2024t27",
    "3da8d2e6a0f8d5d62cbaae18235235483144c6a0.pdf": "kim2024qde",
    "086a4d45a9deef5a10ee4febcd4c92c95a6305de.pdf": "ma2024jej",
    "3dee83a4b0fadde414e00ff350940303eb859be1.pdf": "ge20243g0",
    "22c1ec46a81e9db6194b8784f4fe431f71953757.pdf": "lu2024ush",
    "9c7209f3d6b9bf362ea4d34730f34cf8511967f4.pdf": "yuan2023m1m",
    "200726cba07dec06a56ff46aa38836e9730a23a2.pdf": "zheng2023u9k",
    "3c483c11f5fd9234576a93aea71a7ec1435b8514.pdf": "wang20241f3",
    "d06737f9395e592f35ef251e09bea1c18037b096.pdf": "mahankali20248dx",
    "3ef01975a45bcf78120d76c1bc73e8ec12e213bf.pdf": "yoon2024lff",
    "39d2839aa4c3d8c0e64553891fe98ba261703154.pdf": "ishfaq20245to",
    "cb7ea0176eec1f809f3bc3a34aeb279d44dda612.pdf": "santi2024hct",
    "b31c76815615c16cc8505dbb38d2921f921c029d.pdf": "pham2024j80",
    "c1844cda42b3732a5576d05bb6e007eb1db00919.pdf": "ding2023whs",
    "8af5e79310ec1d8529eba38705e5f29dce789b00.pdf": "malaiyappan20245bh",
    "5221ba291d5901f950220f50d289d5e01d81b0c4.pdf": "gan2023o50",
    "8ae6c7cff8bb2d766f5f9d3585cd262032378b33.pdf": "zhao2023cay",
    "dad0fcbc6f2c9b7c4d7b7b1d4513d94d57ea63c6.pdf": "xu2023m9r",
    "ae47e4e2a4b749543c4de8624049c1d368cbc859.pdf": "khlif2023zg3",
    "c6ed1f7f478f9d004d5f6b9783df79f82d0d1464.pdf": "sreedharan2023nae",
    "2807f9c666335946113fb11dccadf36f8d78b772.pdf": "guo20233sd",
    "9eed36fb9b9bbb97579953d5e303dc0cf6c70a58.pdf": "zhang2023wqi",
    "9c9a527ad885e664a31f0b70ec5f838d0c6ebfa6.pdf": "beikmohammadi2023v6w",
    "8ca9a74503c240b2746e351995ee0415657f1cd0.pdf": "yang2023n56",
    "06ffbf40c29daa4f6b4c5025d96ab1c27c0ea117.pdf": "alvarez2023v09",
    "7f437f4af59ff994d97482ee1c12aaeb4b310e85.pdf": "li2023kgk",
    "839395c4823ac8fff990485e7ce54e53c94bae6b.pdf": "zi20238ug",
    "7825ea27ec1762f6ac41347603535500bcd121f7.pdf": "yang2023w3h",
    "859172d8cb31ddebd9b18e3a7a7d982fab1d1341.pdf": "sun20219kr",
    "f58ada4680d374f5dba96b5e7fcb5b7be11a6acc.pdf": "guo2022y6b",
    "aa65704a16138790678e2b9b59ae679b6c9353d7.pdf": "mazumder2022deb",
    "bd8aaab29fa16f40ef016393ba7ca30127abab58.pdf": "oh2022cei",
    "a6c5e49425ac01c534d9663a4453b28f6dc76f7c.pdf": "vidakovi2020q23",
    "d8bc75fbcfdecd5cd1952524d3e07db13722f5ff.pdf": "sun2024kxq",
    "17682d6f13dcac703092e0f1500a4197e46bbf2c.pdf": "yu2024x53",
    "11c34b84c3ad6587529517c32923c446797c63e6.pdf": "wang2024htz",
    "d929e0bc4e30910527a714f239b5e5b37a7ec6ad.pdf": "ding20246hx",
    "516c6ab3feab17bc158f12ef6768b26c603566b8.pdf": "yang2024yh9",
    "f5e09973834f852237a7d9db6583c7e6615a907d.pdf": "afroosheh2024id4",
    "b43f8bacd80f32734cdff4f9d8c79e397872aed6.pdf": "dong2025887",
    "8357670aac3c98a71b454ab5bca89558f265369d.pdf": "zhu2024sb0",
    "58db2247187ac01acabc1c2fa02f9b189772729e.pdf": "xiang2024qhz",
    "e401ba782c2da93959582295089d3f04a051d6c1.pdf": "qi2024hxq",
    "afa538f59cf2996837863be60a34eef5271a5ee9.pdf": "zhang2024wgo",
    "88f98e3629a7aef2312f9e214ed2a75672113e4f.pdf": "sun2024edc",
    "c4aafb184f285d004d8c8072b5d6408e876428e1.pdf": "dunsin2024e5w",
    "e825e6325dfb5b93066817e7cc6b226ba7d3b799.pdf": "hu2024085",
    "3416214ca1d4f790a048ece4229829333e836b4d.pdf": "ji2024gkw",
    "6bf550ce7f10a4347d448eb810a92e1a5cfffa4b.pdf": "parisi2024u3o",
    "54cb726595cd8c03f59f49c9a97b76b4d3f932f2.pdf": "wang2024anu",
    "b44b0e8222021b51783c62b0bce071ef6fb3f12c.pdf": "wu2024mak",
    "5ac5f3827ee4c32cebadbc9de8a892853575efb9.pdf": "zhao2024714",
    "cce1245ba1ec154120b3b256faf7bf28f769b505.pdf": "hua2025fq5",
    "37fe2a997bf07a972473abd079d175335940e6bd.pdf": "dai2025h8g",
    "e32e28a8a06739997957113b7fa1bd033f6801ba.pdf": "chang2024u7x",
    "c81a06446fa1df12cc043d9d9c8cfd5775090c59.pdf": "janjua2024yhk",
    "66c5bed508f9dce55b2beeaaf36a29929c0bc2ac.pdf": "ledesma2024zzm",
    "2389fafc2a97e13fa810c4014babe73bd886c06f.pdf": "wu20248f9",
    "b2d827c286e32dbf0739e8c796b119b1074809b4.pdf": "honari202473t",
    "fae722ae17483aeef3485f0177346ba3ce332ea9.pdf": "shi2024g6o",
    "dca6c5f8f6c64b4188212e16c2faf461ffd4e49a.pdf": "lu2025caz",
    "9ac3f4e74e9837cb47f43211cf143d4c7115e7f1.pdf": "hou20248b2"
  },
  "sections": {
    "Introduction to Exploration in Reinforcement Learning": "\\section{Introduction to Exploration in Reinforcement Learning}\n\\label{sec:introduction_to_exploration_in_reinforcement_learning}\n\n\n\n\\subsection{The Exploration-Exploitation Dilemma}\n\\label{sec:1_1_the_exploration-exploitation_dilemma}\n\nThe exploration-exploitation dilemma represents a foundational and ubiquitous challenge in Reinforcement Learning (RL), fundamentally shaping how an autonomous agent acquires knowledge and optimizes its behavior within an uncertain environment \\cite{sutton2018reinforcement}. At its core, this dilemma encapsulates the inherent tension between two conflicting objectives: an agent must judiciously decide whether to \\textit{exploit} its current understanding to select actions that are known to yield high immediate rewards, or to \\textit{explore} unknown actions and states, which, despite immediate uncertainty, may lead to the discovery of significantly greater long-term rewards \\cite{robbins1952some, sutton2018reinforcement}. This delicate balance is critical for the design of effective RL agents, as an imbalanced trade-off can profoundly impact learning efficiency and the ultimate optimality of the learned policy.\n\nTo formally illustrate this core dilemma, consider the classic multi-armed bandit (MAB) problem, a simplified yet powerful model for sequential decision-making under uncertainty \\cite{robbins1952some}. In a MAB setting, an agent faces $K$ distinct \"arms,\" each associated with an unknown probability distribution over rewards. At each time step $t$, the agent selects an arm $a_t \\in \\{1, \\dots, K\\}$ and observes a reward $r_t \\sim \\mathcal{D}_{a_t}$. The objective is to maximize the cumulative reward over a sequence of $T$ pulls, or equivalently, to minimize \"regret.\" Regret, formally defined as the difference between the expected cumulative reward of an optimal policy (always pulling the best arm) and the agent's actual cumulative reward, is given by $R_T = \\sum_{t=1}^T (\\mu^* - \\mu_{a_t})$, where $\\mu^*$ is the expected reward of the optimal arm and $\\mu_{a_t}$ is the expected reward of the arm chosen at time $t$ \\cite{lai1985asymptotically}. Pulling an arm with a high estimated mean reward is an act of exploitation. Conversely, choosing an arm that has been sampled infrequently, or whose reward distribution is highly uncertain, constitutes exploration. Pure exploitation risks converging to a suboptimal arm if initial samples were misleading, while pure exploration, such as random arm selection, wastes opportunities to collect known rewards, leading to high regret \\cite{auer2002finite}. Modern approaches to MABs continue to refine this balance, often by maximizing information gain or balancing intrinsic and extrinsic rewards to achieve sublinear regret \\cite{sukhija2024zz8}.\n\nExtending from the simplified MAB framework to full Markov Decision Processes (MDPs), the exploration-exploitation dilemma becomes significantly more intricate. In MDPs, an agent's action not only yields an immediate reward but also transitions the agent to a new state, influencing future rewards. The state space can be high-dimensional or continuous, and the environmental dynamics are often unknown. This means that the value of an action is not independent but depends on the subsequent states it might lead to. Furthermore, rewards can be delayed, making it challenging to attribute positive or negative outcomes to specific exploratory actions taken much earlier in a sequence. Partial observability, where the agent does not have complete information about the true state of the environment, further exacerbates the challenge, as \"unknown\" can refer to truly unvisited states or merely unobserved aspects of the current state \\cite{parisi2024u3o}. These complexities necessitate more sophisticated, directed exploration strategies that move beyond simple random action selection.\n\nConceptual approaches to managing this dilemma in complex RL settings generally fall into several categories. One prominent principle is \"optimism in the face of uncertainty\" (OFU), where agents are incentivized to explore states or actions about which their knowledge is limited, by optimistically assuming these unknown options will yield maximal rewards \\cite{auer2002finite, brafman2002r}. This encourages the agent to gather sufficient data to reduce uncertainty, as exemplified by algorithms like UCB (Upper Confidence Bound) in MABs and R-Max in MDPs \\cite{auer2002finite, brafman2002r}. Another approach involves explicitly valuing the discovery of novel states or actions, often by assigning intrinsic rewards for visiting less-frequented regions of the state-action space. Furthermore, information-theoretic methods guide exploration by maximizing the expected reduction in uncertainty about the environment's dynamics or the optimal policy, thereby prioritizing experiences that yield the most significant knowledge gain \\cite{sukhija2024zz8}. These conceptual frameworks highlight the diverse ways researchers have sought to formalize and address the fundamental trade-off.\n\nThis subsection has established the exploration-exploitation dilemma as a central challenge in RL, defining its core tenets, illustrating it with the MAB problem, and extending its complexities to MDPs. It has also introduced foundational conceptual approaches to its management. The subsequent sections of this literature review will systematically delve into the diverse methodologies developed to tackle this challenge, tracing their evolution from foundational heuristic approaches and theoretically grounded algorithms to advanced intrinsic motivation techniques, adaptive strategies, and their specialized applications. Each method offers a unique perspective on how to navigate this central trade-off, collectively advancing the field towards more intelligent and autonomous learning agents.\n\\subsection{Historical Context and Evolution of Exploration Research}\n\\label{sec:1_2_historical_context__and__evolution_of_exploration_research}\n\nThe fundamental challenge of balancing exploration and exploitation, where an agent must gather sufficient information about its environment to make optimal decisions while simultaneously leveraging its current knowledge, has been a cornerstone of Reinforcement Learning (RL) since its inception \\cite{Sutton1998}. The historical trajectory of exploration research reflects a continuous effort to overcome the inherent complexities of unknown environments, evolving from rudimentary heuristics in simplified settings to sophisticated, scalable strategies for complex, high-dimensional domains. This evolution has been driven by both conceptual shifts and technological advancements, particularly the rise of deep learning.\n\nThe intellectual origins of principled exploration can be traced to the multi-armed bandit (MAB) problem, the simplest setting where the exploration-exploitation dilemma is starkly presented. In MABs, an agent chooses from a set of actions (arms) with unknown reward distributions, aiming to maximize cumulative reward. Foundational algorithms like Upper Confidence Bound (UCB) \\cite{Auer2002} emerged from the principle of \"optimism in the face of uncertainty\" (OFU), which encourages agents to explore actions whose true values are uncertain by optimistically assuming they might yield high rewards. Concurrently, Bayesian approaches, notably Thompson Sampling \\cite{Thompson1933}, provided a probabilistic framework for exploration by sampling from a posterior distribution over action values, effectively balancing exploration and exploitation by favoring actions with high potential given current uncertainty. These early MAB solutions laid the theoretical bedrock for later exploration strategies in full Markov Decision Processes (MDPs) \\cite{parisi2024u3o}.\n\nTransitioning to full MDPs, early RL exploration strategies were often heuristic. The $\\epsilon$-greedy policy, a direct extension of MAB ideas, randomly selects actions with a small probability ($\\epsilon$) to discover new state-action values, while otherwise exploiting current knowledge \\cite{Sutton1998}. While simple, its undirected nature proved inefficient in larger state spaces. To address this, early research in tabular settings introduced explicit exploration bonuses, often count-based, which incentivized agents to visit less-frequented states or take less-tried actions by augmenting the reward signal. These methods aimed for broader state space coverage, but their direct reliance on explicit state-action enumeration rendered them impractical for environments with continuous or very large discrete state spaces, foreshadowing the pervasive \"curse of dimensionality.\" The role of dynamic programming in this era was primarily to compute optimal policies *given* a known model, highlighting the critical need for effective exploration to *learn* such models in unknown environments.\n\nA significant conceptual shift emerged with the development of theoretically grounded exploration methods for finite MDPs, aiming to provide provable guarantees on learning efficiency. The OFU principle, originating from MABs, became a cornerstone, leading to algorithms like UCRL2 \\cite{Strehl2009}. Within the PAC-MDP (Probably Approximately Correct-MDP) framework, these methods provided provable bounds on the number of samples required to learn a near-optimal policy. While offering robust theoretical guarantees on sample complexity, these approaches were computationally demanding and inherently limited by their reliance on explicit state-action enumeration, making them largely inapplicable to the high-dimensional problems prevalent in modern RL. Efforts continued to refine these theoretical methods, with works like UCRL3 \\cite{bourel2020tnm} introducing tighter concentration inequalities and adaptive computation of transition supports to improve practical efficiency within the theoretical paradigm. However, the fundamental trade-off persisted: rigorous theoretical guarantees often came at the cost of scalability, necessitating a paradigm shift for complex, real-world domains.\n\nThe advent of deep learning provided a new impetus for exploration research, shifting the focus towards scalable solutions for complex, high-dimensional observation spaces where traditional counting or explicit model-learning became intractable. This era saw the emergence of intrinsic motivation, a paradigm where agents generate internal reward signals for novel or surprising experiences, independent of external task rewards. The challenge was to generalize the notion of \"visitation count\" or \"novelty\" to continuous, high-dimensional state spaces. Breakthroughs included the concept of pseudo-counts \\cite{Bellemare2016}, derived from density models, which allowed agents to quantify novelty in high-dimensional state spaces. Complementing this, \\cite{martin2017bgt} introduced $\\phi$-pseudocounts, generalizing state visit-counts by exploiting the same feature representation used for value function approximation, thereby rewarding exploration in feature space rather than the untransformed state space. Similarly, \\cite{Tang2016} demonstrated that even a simple generalization of classic count-based methods, using hash codes to count state occurrences, could achieve competitive performance in deep RL benchmarks, underscoring the power of novelty-seeking in complex environments.\n\nDespite their success, early intrinsic motivation methods faced challenges, such as the \"noisy TV problem,\" where agents might be perpetually distracted by uncontrollable stochastic elements that offer no meaningful learning. To address this, \\cite{Pathak2017} introduced the Intrinsic Curiosity Module (ICM), which generates intrinsic rewards based on the agent's prediction error of its own actions' consequences in a learned feature space, thereby focusing exploration on controllable and learnable aspects of the environment. Further refining this, \\cite{Burda2019} proposed Random Network Distillation (RND), a simpler and more robust intrinsic reward mechanism that measures prediction error between a policy network and a fixed, randomly initialized target network. RND proved less susceptible to environmental stochasticity, providing a more reliable signal for true novelty and significantly improving exploration stability. Simultaneously, efforts were made to bridge the gap between theoretical rigor and deep RL scalability. For instance, \\cite{nikolov20184g9} introduced Information-Directed Sampling (IDS) for deep Q-learning, providing a tractable approximation that explicitly accounts for both parametric uncertainty and heteroscedastic observation noise, further enhancing the theoretical grounding of exploration in deep RL.\n\nIn conclusion, the evolution of exploration research in RL reflects a continuous effort to overcome the inherent challenges of unknown environments. This journey moved from foundational theoretical guarantees in simplified settings like MABs, through heuristic and theoretically-grounded methods for tabular MDPs, and ultimately to practical, scalable, and increasingly robust solutions for complex, high-dimensional domains enabled by deep learning. This historical trajectory, marked by shifts from heuristic to theoretically grounded, and then to intrinsically motivated and uncertainty-aware deep learning approaches, sets the stage for the detailed methodological discussions of advanced and adaptive strategies that follow.\n\\subsection{Motivation for Effective Exploration}\n\\label{sec:1_3_motivation_for_effective_exploration}\n\n\nEffective exploration stands as a cornerstone for the successful application of Reinforcement Learning (RL) agents, particularly in the intricate and often unforgiving landscape of real-world environments. Its crucial role stems from the inherent challenges that frequently impede learning: the scarcity of informative reward signals, the vastness of high-dimensional state and action spaces, the prevalence of deceptive local optima that can trap agents in suboptimal behaviors, and the critical need for policies that generalize beyond training data while remaining sample-efficient. Without well-designed exploration strategies, RL agents risk converging to inferior policies, failing to discover optimal solutions, or even remaining inert in complex tasks, thereby underscoring the continuous drive for innovation in this research domain.\n\nOne of the primary motivations for robust exploration arises from the pervasive issue of **sparse reward signals** and the **curse of dimensionality**. In many practical scenarios, agents receive meaningful feedback only after achieving specific, often distant, goals. This sparsity makes naive trial-and-error exploration highly inefficient or even impossible. Early attempts to address this, such as model-based planning with Dyna-Q \\cite{Sutton90} and subsequent works \\cite{Kaelbling93,Singh2004}, aimed to improve sample efficiency by leveraging learned environmental models to generate synthetic experiences. Similarly, count-based methods \\cite{Thrun92} offered explicit incentives for visiting less-known states. However, these foundational approaches often struggled to scale to high-dimensional or continuous state spaces, where explicit state enumeration or precise model learning becomes intractable. This limitation fundamentally motivated the development of **intrinsic motivation** techniques, which empower agents to generate their own internal reward signals, independent of external task rewards. Pioneering ideas of \"curiosity\" based on prediction error \\cite{Schmidhuber91,Schmidhuber97} and learning progress \\cite{Singh00} provided conceptual breakthroughs. These concepts were subsequently scaled to deep RL through methods like pseudo-counts for high-dimensional spaces \\cite{Bellemare16}, exploration bonuses derived from deep predictive models \\cite{stadie20158af}, and hash-based count methods \\cite{tang20166wr}. Such advancements have proven vital for tasks requiring extensive discovery in visually rich or complex environments, such as mapless navigation for mobile robots \\cite{zhelo2018wi8}.\n\nBeyond simply finding rewards, effective exploration is essential to overcome the **peril of deceptive local optima**. Many environments present reward landscapes with numerous suboptimal peaks, where a greedy agent might become trapped, never discovering the globally optimal policy. This challenge necessitates exploration strategies that actively encourage agents to venture beyond seemingly good but ultimately inferior solutions. Information-theoretic approaches, such as Variational Information Maximizing Exploration (VIME) \\cite{Houthooft2016}, address this by guiding agents to states that maximize information gain about the environment's dynamics, thereby reducing uncertainty and facilitating escape from local traps. More recent intrinsic motivation methods, like the Intrinsic Curiosity Module (ICM) \\cite{Pathak17} and Random Network Distillation (RND) \\cite{Burda18}, provide robust novelty signals by rewarding prediction errors in learned feature spaces or against random targets. These methods are crucial for preventing agents from being perpetually attracted to uninformative stochastic elements (the \"noisy TV\" problem) that could otherwise lead to spurious curiosity and suboptimal convergence. Furthermore, approaches like diversity-driven exploration \\cite{hong20182pr} and novelty-seeking in evolutionary strategies \\cite{conti2017cr2} explicitly aim to prevent policies from being trapped in local optima by encouraging a wide range of behaviors and exploring diverse solution spaces. Robust policy optimization techniques, such as Robust Policy Optimization (RPO) \\cite{rahman2022p7b}, also contribute by maintaining sufficient policy entropy, ensuring continuous and broad exploration to avoid premature convergence.\n\nThe imperative for **sample efficiency** and **generalization** further underscores the critical need for sophisticated exploration. In real-world applications, data collection can be costly, time-consuming, or even risky, making inefficient exploration a significant bottleneck. Moreover, agents must often perform reliably in environments that differ subtly or significantly from their training conditions. This motivates exploration strategies that not only discover optimal policies quickly but also acquire knowledge transferable to unseen scenarios. For instance, \\cite{whitney2021xlu} highlights that simple policy entropy maximization is often insufficient for sample-efficient continuous control, advocating for decoupled exploration and exploitation policies. Leveraging existing data, such as expert demonstrations \\cite{nair2017crs} or large volumes of offline trajectories \\cite{ball20235zm}, can dramatically accelerate learning by guiding exploration towards promising regions of the state-action space, thus improving sample efficiency. The importance of exploration for *generalization* itself is a key motivation for methods like EDE (Exploration via Distributional Ensemble) \\cite{jiang2023qmw}, which encourages exploration of states with high epistemic uncertainty to acquire knowledge that aids decision-making in novel environments. Meta-learning exploration strategies \\cite{gupta2018rge} enable agents to learn *how* to explore effectively across a distribution of tasks, fostering rapid adaptation and generalization. Crucially, in safety-critical domains, exploration must be conducted within predefined safe boundaries or with learned recovery mechanisms, as explored by Recovery RL \\cite{thananjeyan2020d20}, ensuring that the pursuit of optimal behavior does not lead to catastrophic outcomes.\n\nIn conclusion, the motivation for effective exploration in Reinforcement Learning is deeply rooted in the fundamental challenges of the field. It is indispensable for navigating sparse reward landscapes, conquering high-dimensional complexities, escaping deceptive local optima, and achieving both sample efficiency and robust generalization in dynamic, real-world settings. The continuous evolution of exploration strategies, from basic heuristics to advanced intrinsic motivation, diversity-driven methods, and meta-learning approaches, reflects its non-negotiable status as a core component for unlocking the full potential of intelligent agents. Addressing these challenges drives ongoing research to develop more robust, theoretically grounded, and computationally efficient exploration methods that can seamlessly integrate with the demands of practical applications.\n",
    "Foundational Concepts and Early Approaches to Exploration": "\\section{Foundational Concepts and Early Approaches to Exploration}\n\\label{sec:foundational_concepts__and__early_approaches_to_exploration}\n\n\n\n\\subsection{Basic Exploration Heuristics}\n\\label{sec:2_1_basic_exploration_heuristics}\n\nThe fundamental challenge of exploration in reinforcement learning (RL) lies in efficiently discovering optimal policies within an environment while simultaneously exploiting currently known good actions. The earliest and most straightforward attempts to address this exploration-exploitation dilemma centered around simple, yet widely adopted, heuristics, primarily the $\\epsilon$-greedy policy. This approach serves as a crucial baseline from which more sophisticated and targeted exploration strategies have evolved, highlighting the initial attempts to balance this fundamental trade-off.\n\nThe $\\epsilon$-greedy policy operates on a simple principle: with a small probability $\\epsilon$ (epsilon), the agent selects an action uniformly at random, thereby exploring the environment. With a higher probability of $1-\\epsilon$, the agent chooses the action that maximizes its current estimated value (the greedy action), thus exploiting its learned knowledge. This method's appeal lies in its conceptual simplicity and ease of implementation, making it a foundational component in many early RL algorithms \\cite{yang2021ngm, aubret2022inh}. It ensures that every action has a non-zero probability of being selected, preventing the agent from getting permanently stuck in suboptimal policies.\n\nDespite its widespread use, basic $\\epsilon$-greedy exploration suffers from several inherent limitations that have motivated the development of more advanced techniques. A primary drawback is its undirected nature \\cite{sukhija2024zz8}. The random actions taken during exploration are not guided by any sense of novelty, uncertainty, or potential for high reward. This leads to inefficient exploration, particularly in environments with large state-action spaces, sparse rewards, or deceptive local optima \\cite{stadie20158af, houthooft2016yee}. For instance, in complex domains requiring processing raw pixel inputs, simple $\\epsilon$-greedy methods are often impractical due to their reliance on enumerating or uniformly sampling a vast, high-dimensional state-action space \\cite{stadie20158af}. The lack of direction means the agent might spend considerable time revisiting well-understood states or exploring unpromising regions, rather than focusing on truly unknown or potentially rewarding areas \\cite{stanton20183fs}.\n\nFurthermore, $\\epsilon$-greedy policies struggle to distinguish between actions that are truly unknown and those that are known to be suboptimal \\cite{gupta2018rge}. Every action, regardless of how much is known about its outcomes, receives the same random exploration probability. This uniform randomness can be particularly problematic in real-world settings, where \"random exploration, nevertheless, can result in disastrous outcomes and surprising performance\" \\cite{ghamari2024bbm}. The method fails to leverage the agent's uncertainty about its value estimates, a critical piece of information for efficient exploration. This limitation highlighted the need for strategies that could generalize uncertainty and direct exploration towards states or actions with high epistemic uncertainty, as explored by methods like count-based exploration in feature space \\cite{martin2017bgt, tang20166wr}. These later approaches aimed to provide exploration bonuses based on how frequently states or features were visited, offering a more nuanced way to encourage novelty than simple uniform random action selection.\n\nThe inefficiency of $\\epsilon$-greedy becomes even more pronounced in large or continuous state spaces, where the probability of revisiting any specific state becomes infinitesimally small, rendering simple visit counts ineffective \\cite{tang20166wr}. This \"curse of dimensionality\" necessitated methods that could generalize exploration across similar states or learn representations of novelty, moving beyond the direct, uninformative randomness of $\\epsilon$-greedy.\n\nHowever, the concept of $\\epsilon$-greedy has not been entirely abandoned. Its simplicity has made it a foundational element that has been significantly refined and adapted. For example, in the context of Incremental Reinforcement Learning, where state and action spaces continually expand, a Dual-Adaptive $\\epsilon$-greedy Exploration (DAE) method has been proposed \\cite{ding2023whs}. This advanced variant dynamically adjusts the exploration probability $\\epsilon$ based on the convergence of value estimates for specific states (Meta Policy) and guides the agent to prioritize \"least-tried\" actions (Explorer). This evolution demonstrates how the core idea of balancing exploration and exploitation, first introduced by basic $\\epsilon$-greedy, can be made significantly more sophisticated and targeted to address the challenges of dynamic and expanding environments, moving beyond its initial undirected and inefficient form.\n\nIn conclusion, while basic exploration heuristics like $\\epsilon$-greedy provided a crucial initial framework for addressing the exploration-exploitation trade-off, their inherent limitationsundirected exploration, inefficiency in large state spaces, and inability to distinguish between truly unknown and well-understood but suboptimal actionsunderscored the necessity for more sophisticated and targeted exploration strategies. These early methods laid the groundwork, serving as a fundamental baseline from which the rich and diverse landscape of modern exploration techniques has emerged.\n\\subsection{Model-Based Planning and Experience Replay}\n\\label{sec:2_2_model-based_planning__and__experience_replay}\n\n\nEfficiently navigating and learning within complex environments is a fundamental challenge in reinforcement learning (RL), often exacerbated by the high cost of real-world interactions. Model-based planning and experience replay address this by making more efficient use of collected experience, implicitly aiding exploration by accelerating learning and propagating information more widely across the state space.\n\nA foundational approach in this domain is the Dyna architecture, introduced by \\cite{Sutton1990}. Dyna-Q integrates direct reinforcement learning with planning by concurrently learning an environmental model (transitions and rewards) from real experiences. This learned model is then used to generate simulated experiences, allowing the agent to perform \"mental rehearsals\" and update its value function from both real and imagined interactions. This process significantly accelerates value function updates and propagates information more widely, making each real interaction more valuable and implicitly encouraging exploration by quickly refining the agent's understanding of the environment. Complementing this, \\cite{Lin1992} highlighted the importance of experience replay, a mechanism where past experiences are stored and re-used for learning. By replaying previously collected data, agents can learn more efficiently from a fixed set of interactions, reducing the need for extensive new exploration and improving sample efficiency, particularly in off-policy learning settings.\n\nTo further enhance the efficiency of model-based planning, \\cite{Sutton1993} and \\cite{Moore1993} introduced prioritized sweeping. This method refines the planning process by prioritizing updates to state-action pairs whose values are likely to change significantly, or which have a large impact on other states. By focusing computational resources on the most informative simulated experiences, prioritized sweeping dramatically accelerates learning and value propagation compared to uniform sweeping, ensuring that the agent's understanding of the environment is refined more quickly and effectively. Addressing the challenge of scaling model-based methods to larger state spaces, \\cite{Singh1992} proposed reinforcement learning with a hierarchy of abstract models. This approach leverages structural decomposition to manage complexity, allowing exploration and planning to occur at different levels of temporal abstraction, which can make the learning problem more tractable. Similarly, \\cite{Kaelbling1993} explored methods for blending planning and direct reinforcement learning, demonstrating how a learned model can be actively used to guide exploration by evaluating hypothetical scenarios and informing action selection, thereby making exploration more directed and less random.\n\nBeyond direct model learning for planning, advancements in state representation also contribute to the efficiency of model-based approaches. \\cite{Dayan1993} introduced the successor representation, which models the expected future state occupancies rather than immediate transitions. While not a direct planning mechanism in the Dyna sense, this representation provides a more generalized understanding of state relationships, improving generalization for temporal difference learning and implicitly aiding exploration by making value estimates more robust and transferable across similar states.\n\nIn more recent deep reinforcement learning contexts, the principles of model-based planning continue to evolve. \\cite{stadie20158af} demonstrated how deep predictive models can be used to incentivize exploration by assigning exploration bonuses based on the uncertainty or novelty derived from the learned dynamics. This approach leverages the representational power of neural networks to build scalable models in high-dimensional domains, guiding exploration towards areas where the model's predictions are less confident. Extending this, \\cite{fu20220cl} proposed a model-based lifelong reinforcement learning approach that estimates a hierarchical Bayesian posterior to distill common structures across tasks. By combining this learned posterior with Bayesian exploration, their method significantly increases the sample efficiency of learning across related tasks, showcasing how sophisticated model learning can facilitate principled exploration and transfer. Furthermore, \\cite{ma2024r2p} integrated neural network models into an actor-critic architecture (ModelPPO) for AUV path-following control. Their neural network model learns the state transition function, allowing the agent to explore spatio-temporal patterns and achieve superior performance compared to traditional model predictive control and other RL methods, underscoring the enduring utility of learned models in complex control tasks.\n\nDespite their significant advantages in sample efficiency and information propagation, model-based planning and experience replay methods face critical limitations. Their performance heavily relies on the accuracy and learnability of the environmental model. In complex, high-dimensional, or non-stationary domains, learning an accurate and robust model can be exceedingly challenging, and errors in the model can compound, leading to \"model bias\" and potentially suboptimal policies or misleading exploration. Nevertheless, these foundational and evolving model-based approaches remain crucial for accelerating learning and making efficient use of collected experience, thereby implicitly guiding agents towards more effective exploration strategies.\n\\subsection{Early Explicit Exploration Bonuses}\n\\label{sec:2_3_early_explicit_exploration_bonuses}\n\nThe fundamental challenge of exploration in reinforcement learning (RL) necessitates strategies that transcend purely random actions to efficiently discover optimal policies, particularly in environments characterized by sparse or delayed rewards. This subsection focuses on the pioneering methods that introduced explicit incentives for agents to explore novel or less-visited states, thereby laying the groundwork for more sophisticated intrinsic motivation techniques. These early approaches were crucial in demonstrating the power of directed exploration beyond mere stochasticity.\n\nA seminal contribution to explicit exploration bonuses came from \\cite{Thrun1992}, who introduced count-based exploration. In this paradigm, agents receive an additional, intrinsic reward for visiting states or taking actions less frequently encountered. The core idea is straightforward: by incentivizing novelty based on visitation frequency, the agent is directly encouraged to explore uncharted regions of the state space. This approach effectively transforms the problem of undirected search into a directed quest for new experiences, ensuring broader state space coverage in tabular settings. This principle aligns with the broader concept of \"optimism in the face of uncertainty,\" where less-known options are optimistically valued higher to encourage their selection \\cite{SuttonBarto2018}. Such count-based mechanisms share conceptual roots with strategies employed in the multi-armed bandit problem, where algorithms like Upper Confidence Bound (UCB) leverage visitation counts (or estimates of uncertainty) to balance exploitation of known good options with exploration of less-tried ones, thereby providing a theoretical basis for directed exploration in simpler settings.\n\nComplementing frequency-based methods, the concept of \\textit{recency-based} exploration also emerged as a valuable heuristic in early RL. While less formally enshrined in a single seminal work compared to count-based methods, the underlying idea was to grant exploration bonuses based on the time elapsed since a state was last visited, or to prioritize states that were recently discovered but not yet thoroughly explored. These approaches aimed to prevent agents from getting stuck in local optima by encouraging them to refresh their knowledge about \"stale\" or neglected parts of the environment. For instance, an agent might receive a bonus inversely proportional to the number of timesteps since its last visit to a particular state, ensuring that even frequently visited states are eventually re-explored if they haven't been seen for a while. Both count-based and recency-based methods, while distinct in their temporal focus, shared the common goal of directing exploration by explicitly rewarding the agent for interacting with less familiar parts of the environment, moving beyond the undirected nature of $\\epsilon$-greedy exploration.\n\nDespite their groundbreaking nature and effectiveness in controlled, tabular, or low-dimensional environments, these early explicit exploration bonuses faced significant limitations, primarily due to the curse of dimensionality. Count-based methods, by their very definition, require maintaining an accurate count for each unique state-action pair. In environments with large, continuous, or high-dimensional state spaces (e.g., visual observations from images), enumerating and tracking every distinct state becomes computationally infeasible and memory-prohibitive. The notion of a \"unique state\" itself becomes ill-defined in continuous spaces, making direct counting impossible. Similarly, recency-based methods also struggle in such complex settings, as tracking the last visit time for an astronomically large or continuous state space is equally impractical. The inability of these foundational explicit bonuses to scale effectively to real-world complexity underscored the need for more generalized and robust intrinsic motivation techniques that could approximate novelty in high-dimensional settings without explicit state enumeration.\n\nIn conclusion, early explicit exploration bonuses, encompassing both count-based (frequency) and recency-based heuristics, provided a critical foundation for directed exploration in reinforcement learning. They successfully demonstrated the power of incentivizing novelty to overcome the limitations of purely random search in environments where states could be distinctly enumerated. However, their inherent reliance on explicit state representations severely limited their applicability to tabular or low-dimensional environments. These fundamental scalability challenges, driven by the curse of dimensionality, highlighted the need for two distinct paths forward: firstly, the development of theoretically grounded approaches that could offer provable guarantees on learning efficiency in tractable domains (as discussed in Section 3); and secondly, the creation of more advanced intrinsic motivation techniques that could generalize the notion of a \"count\" or \"novelty\" to complex, high-dimensional domains without explicit state enumeration (as will be explored in Section 4.2).\n",
    "Theoretically Grounded Exploration Strategies": "\\section{Theoretically Grounded Exploration Strategies}\n\\label{sec:theoretically_grounded_exploration_strategies}\n\n\n\n\\subsection{Optimism in the Face of Uncertainty (OFU) and PAC-MDP}\n\\label{sec:3_1_optimism_in_the_face_of_uncertainty_(ofu)__and__pac-mdp}\n\n\nThe principle of \"optimism in the face of uncertainty\" (OFU) stands as a foundational pillar for theoretically grounded exploration in reinforcement learning. This paradigm dictates that when an agent faces uncertainty about the true value of a state-action pair, it should optimistically assume the highest possible reward, thereby actively incentivizing exploration of unknown or poorly understood regions of the environment. This inherent bias towards unexplored options ensures that the agent gathers sufficient data to accurately estimate values, ultimately facilitating convergence to an optimal policy. OFU is intrinsically linked to the concept of Probably Approximately Correct (PAC-MDP) guarantees, which provide strong theoretical assurances that an agent can learn a near-optimal policy with high probability within a polynomial number of interactions \\cite{kearns2002near}.\n\nThe historical development of OFU principles can be traced from the simpler multi-armed bandit (MAB) setting to full Markov Decision Processes (MDPs). In MABs, Upper Confidence Bound (UCB) algorithms, such as UCB1 \\cite{auer2002finite}, exemplify OFU by selecting actions that maximize an upper confidence bound on their estimated value. This bound is typically a sum of the empirical mean reward and a bonus term that scales with the uncertainty (e.g., inversely proportional to the square root of the number of times the arm has been pulled). This strategy ensures that arms with potentially high, but uncertain, returns are sufficiently explored.\n\nExtending this principle to the more complex MDP setting, algorithms like R-Max \\cite{brafman2002r} and UCRL (Upper Confidence Reinforcement Learning) \\cite{auer2009ucrl2} operationalize OFU to provide PAC-MDP guarantees. R-Max constructs an explicit model of the MDP and, for any state-action pair that has not been sampled a sufficient number of times, it optimistically assigns a maximal reward ($R_{max}$) and models a self-loop transition. This design effectively \"forces\" the planning algorithm to prioritize exploration of these unknown regions, as they appear maximally rewarding. Once a state-action pair has been visited enough times, its estimated reward and transition dynamics are considered reliable, and the optimism is removed. Similarly, UCRL algorithms maintain confidence intervals over the estimated transition probabilities and reward functions of the MDP. At each step, UCRL computes an \"optimistic\" MDP whose parameters lie within these confidence intervals and whose optimal policy yields the highest possible value. The agent then acts optimally with respect to this optimistic model, ensuring that actions leading to uncertain but potentially high-reward outcomes are chosen. Another notable algorithm, UCB-Value Iteration (UCB-VI), also leverages confidence bounds on value functions to guide optimistic exploration \\cite{kearns2002near}.\n\nThese OFU-based algorithms are celebrated for their robust theoretical bounds on sample complexity, guaranteeing that an agent will find an $\\epsilon$-optimal policy (a policy whose value is within $\\epsilon$ of the optimal value) within a number of interactions that scales polynomially with the size of the state space, action space, and the desired accuracy. This makes them a strong foundation for efficient learning in environments where such guarantees are paramount. However, a critical limitation arises from their reliance on explicit state enumeration and accurate model estimation, which becomes intractable in high-dimensional or continuous state spaces. As highlighted by \\cite{stadie20158af}, while \"Bayesian and PAC-MDP approaches to the exploration problem offer strong formal guarantees,\" they often become \"impractical in higher dimensions due to their reliance on enumerating the state-action space.\" This \"curse of dimensionality\" severely restricts their direct applicability to complex, real-world environments, a challenge reinforced by comprehensive surveys on deep reinforcement learning exploration \\cite{yang2021psl}.\n\nDespite these scalability challenges, the core tenets of OFU continue to inform contemporary research. Modern model-based RL algorithms still strive for similar guarantees, even if they employ approximations to handle larger state spaces. For instance, \\cite{song2021elb} introduces PC-MLP, a model-based RL algorithm that aims for polynomial sample complexity in both Kernelized Nonlinear Regulators and linear MDPs, demonstrating that the pursuit of theoretically efficient exploration remains active. This work, like its predecessors, relies on a planning oracle, a common assumption in algorithms with strong theoretical bounds. Furthermore, recent work by \\cite{sreedharan2023nae} explores optimistic exploration using symbolic model estimates, showcasing how OFU principles can be adapted to structured environments where symbolic representations can mitigate some of the dimensionality issues, thereby making optimistic planning more tractable.\n\nIn conclusion, the principle of optimism in the face of uncertainty, coupled with PAC-MDP guarantees, provides a robust theoretical framework for efficient exploration in reinforcement learning. These methods offer strong bounds on sample complexity and ensure convergence to near-optimal policies by systematically exploring uncertain but potentially rewarding avenues. However, their inherent reliance on explicit model construction and finite state-action spaces limits their direct applicability to the vast, high-dimensional environments common in modern deep RL. This fundamental trade-off between theoretical rigor and practical scalability has motivated the development of alternative exploration strategies, such as intrinsic motivation and approximate methods, which often sacrifice explicit PAC-MDP assurances for greater applicability in complex domains.\n\\subsection{Bayesian Approaches to Exploration}\n\\label{sec:3_2_bayesian_approaches_to_exploration}\n\n\nBayesian approaches offer a principled and theoretically grounded framework for tackling the exploration-exploitation dilemma in reinforcement learning by explicitly quantifying and managing uncertainty. These methods maintain a posterior distribution over possible models of the environment, value functions, or policies, leveraging this uncertainty to guide decision-making. The fundamental premise is that actions are not merely chosen based on their immediate expected reward, but also for their potential to reduce epistemic uncertainty, thereby leading to more informed and efficient learning over the long term. This subsection explores key techniques, from foundational posterior sampling methods like Thompson Sampling to scalable approximations using deep ensembles and Monte Carlo dropout, highlighting their mechanisms for balancing exploration and exploitation.\n\nHistorically, the concept of Bayesian reinforcement learning dates back to early theoretical works, where agents would explicitly maintain a posterior over the entire Markov Decision Process (MDP) parameters \\cite{strens2000bayesian}. While providing strong theoretical guarantees, the computational intractability of maintaining and updating exact posterior distributions, especially in high-dimensional state and action spaces or with complex, non-linear dynamics models common in deep reinforcement learning (DRL), severely limited their practical application. Exact Bayesian inference often requires complex computations over continuous or high-dimensional parameter spaces, making it prohibitive for real-world scenarios.\n\nA cornerstone technique that exemplifies the Bayesian principle is Thompson Sampling. It operates by sampling a model (or a Q-function, or a policy) from the current posterior distribution and then acting optimally with respect to that sampled entity for a period. This mechanism inherently balances exploration and exploitation: models that are highly uncertain or have not been sufficiently explored are more likely to be sampled, leading to exploration, while well-understood models guide exploitation. The elegance of Thompson Sampling lies in its ability to implicitly direct exploration towards promising yet uncertain areas. Recent advancements have focused on making Thompson Sampling more scalable and provably efficient for DRL. For instance, \\textcite{ishfaq20235fo} present a scalable Thompson Sampling strategy for RL that directly samples the Q-function from its posterior distribution using Langevin Monte Carlo, an efficient Markov Chain Monte Carlo (MCMC) method. This approach bypasses the need for restrictive Gaussian approximations, offering a more accurate representation of the posterior and demonstrating a regret bound of $\\tilde{O}(d^{3/2}H^{3/2}\\sqrt{T})$ in linear Markov Decision Processes (MDPs), making it deployable in deep RL with standard optimizers. Building on this, \\textcite{ishfaq20245to} further enhance randomized exploration for RL by proposing an algorithmic framework that incorporates various approximate sampling methods with the computationally challenging Feel-Good Thompson Sampling (FGTS) approach. Their work yields improved regret bounds for linear MDPs and shows significant empirical gains in challenging deep exploration tasks within the Atari 57 suite, underscoring the potential of efficient approximate sampling to unlock the power of Thompson Sampling in complex environments.\n\nGiven the challenges of exact Bayesian inference, much research in DRL has focused on practical approximations for estimating epistemic uncertainty, which is crucial for effective Bayesian exploration. Deep ensembles have emerged as a prominent and effective method. By training multiple neural networks with different random initializations or data subsets, the disagreement among their predictions can serve as a proxy for epistemic uncertainty. This disagreement can then be used to generate intrinsic rewards, encouraging the agent to explore states where the ensemble's predictions diverge significantly. \\textcite{jiang2023qmw} propose Exploration via Distributional Ensemble (EDE), a method that encourages exploration of states with high epistemic uncertainty through an ensemble of Q-value distributions. EDE demonstrates state-of-the-art performance on benchmarks like Procgen and Crafter, highlighting the importance of exploration for generalization and the efficacy of ensemble-based uncertainty. Similarly, \\textcite{yang2022mx5} introduce Ensemble Proximal Policy Optimization (EPPO), which learns ensemble policies and incorporates a diversity enhancement regularization over the policy space. This regularization helps to generalize to unseen states and promotes exploration by encouraging the ensemble members to maintain diverse behaviors, thereby covering a broader range of the state-action space. In safety-critical applications, \\textcite{zhang2024ppn} leverage deep ensembles to estimate epistemic uncertainty within a safe reinforcement learning framework. Their Uncertainty-augmented Lagrangian (Lag-U) algorithm uses this uncertainty to encourage exploration and adaptively modify safety constraints, enabling a better trade-off between efficiency and risk avoidance in autonomous driving.\n\nAnother practical method for approximating Bayesian uncertainty in deep neural networks is Monte Carlo dropout. By applying dropout during inference, multiple forward passes can be performed to obtain a distribution of predictions, from which uncertainty (e.g., variance) can be estimated. This technique provides a computationally efficient way to quantify epistemic uncertainty without training multiple separate models. \\textcite{wu2021r67} utilize a practical and effective dropout-based uncertainty estimation method in their Uncertainty Weighted Actor-Critic (UWAC) algorithm. While primarily applied to offline reinforcement learning to detect and down-weight out-of-distribution state-action pairs, the underlying principle of using dropout to estimate uncertainty is directly applicable to guiding exploration in online settings by incentivizing visits to states where uncertainty is high.\n\nDespite their theoretical elegance and principled approach, a common limitation of explicit Bayesian methods remains the computational complexity associated with maintaining and updating posterior distributions. While modern approximations like MCMC, deep ensembles, and Monte Carlo dropout significantly improve scalability, they introduce their own trade-offs. Deep ensembles require training and maintaining multiple neural networks, which can be computationally expensive and memory-intensive. Monte Carlo dropout, while efficient, relies on specific assumptions about the network architecture and may not always accurately capture the true posterior uncertainty. The accuracy of these approximations directly impacts the effectiveness of the exploration strategy and the theoretical guarantees. Future research continues to focus on developing more scalable, computationally efficient, and theoretically robust Bayesian approximations that can harness the full potential of uncertainty-driven exploration in complex, high-dimensional, and real-world reinforcement learning scenarios, moving beyond heuristic exploration towards more informed and adaptive learning.\n\\subsection{Information-Theoretic Exploration}\n\\label{sec:3_3_information-theoretic_exploration}\n\n\nInformation-theoretic exploration strategies offer a principled framework for addressing the exploration-exploitation dilemma in Reinforcement Learning (RL) by explicitly quantifying and maximizing the expected information gain. Unlike heuristic or purely novelty-seeking approaches, these methods guide agents towards experiences that are most likely to reduce uncertainty about the environment's dynamics, the optimal policy, or the value function. This section delineates various facets of information-theoretic exploration, emphasizing how they provide a sophisticated understanding of learning progress and model improvement.\n\nA fundamental concept in this domain is the maximization of mutual information. A seminal work, Variational Information Maximizing Exploration (VIME) \\cite{Houthooft2016}, exemplifies this by proposing an intrinsic reward signal derived from the mutual information between the agent's actions and the learned parameters of its environment dynamics model. VIME leverages variational inference to estimate this information gain, thereby incentivizing the agent to take actions that maximally reduce its uncertainty about how the environment functions. This approach moves beyond simple state visitation counts, actively driving the agent to improve its internal model of the world by seeking out states and actions that are most informative for model learning. The strength of VIME lies in its explicit link between exploration and model improvement, but its computational complexity, particularly in estimating mutual information and maintaining accurate posterior distributions over model parameters in high-dimensional settings, can be a significant challenge.\n\nBeyond uncertainty about the environment model, information-theoretic approaches also focus on reducing uncertainty about the optimal policy or value function. Information-Directed Sampling (IDS) is a prominent example, which explicitly quantifies the value of information by maximizing the \"information ratio\" \\cite{russo2014learning}. This ratio balances the expected reduction in regret (or increase in reward) from gaining information against the cost of exploration. Unlike Thompson Sampling (which samples a policy from a posterior and acts greedily), IDS directly optimizes for the value of information, making it a more explicit information-theoretic strategy. While initially developed for bandit problems, IDS has been extended to Deep RL, as demonstrated by \\cite{nikolov20184g9}. This work proposes a tractable approximation of IDS for deep Q-learning, which explicitly accounts for both parametric uncertainty and heteroscedastic observation noise. By leveraging distributional reinforcement learning, this approach provides a robust exploration strategy that is particularly effective in environments with varying levels of uncertainty, outperforming traditional methods that struggle with non-uniform return variability. The application of IDS also extends beyond traditional RL control tasks, as seen in \\cite{sun2020zjg}, where it was used in density-based structural topology optimization to efficiently direct the search towards optimal designs by maximizing the expected value of information in generative design problems.\n\nAnother significant information-theoretic concept is empowerment, which defines an intrinsic reward as the channel capacity between an agent's actions and its future states \\cite{salge2014empowerment, mohamed2015variational}. Maximizing empowerment encourages an agent to explore states where it has greater control or influence over its future, effectively driving it towards regions of the state space that offer more diverse and controllable outcomes. This perspective aligns with the broader idea of intrinsic motivation, where agents are driven by an innate desire to understand and control their environment. As highlighted by \\cite{aubret2022inh}, information theory provides a rich taxonomy for intrinsic motivation, encompassing concepts like surprise (reduction in predictive uncertainty), novelty (information gain about unfamiliar states), and skill-learning (maximizing control over future states, i.e., empowerment). These different facets underscore how information-theoretic principles can be applied to various aspects of learning and exploration.\n\nWhile Bayesian methods, discussed in Section 3.2, inherently align with information-theoretic principles by maintaining and reducing uncertainty, information-theoretic exploration distinguishes itself by explicitly formulating exploration as an optimization problem over information gain. For instance, Thompson Sampling implicitly reduces uncertainty by sampling from a posterior, but IDS or VIME directly compute or approximate the value of information. The computational overhead of precisely quantifying mutual information or channel capacity remains a primary challenge for information-theoretic methods, especially in complex, high-dimensional, and non-stationary environments. Approximations, such as those used in VIME or the Deep RL extension of IDS \\cite{nikolov20184g9}, are crucial for scalability.\n\nIn conclusion, information-theoretic exploration offers a powerful and principled lens through which to design effective exploration strategies. By explicitly valuing information gainwhether about the environment's dynamics (VIME), the optimal policy (IDS), or the agent's control over its future (empowerment)these methods move beyond simple heuristics to foster truly intelligent and directed discovery. Despite challenges related to computational tractability and the accurate estimation of information-theoretic quantities in complex settings, advancements in approximation techniques continue to enhance their practical applicability. Future research will likely focus on developing more efficient and robust approximations for information gain, potentially integrating with meta-learning to adaptively select optimal information-seeking strategies, and further exploring their utility in multi-agent and open-ended learning scenarios.\n",
    "Intrinsic Motivation: Novelty, Curiosity, and Prediction Error": "\\section{Intrinsic Motivation: Novelty, Curiosity, and Prediction Error}\n\\label{sec:intrinsic_motivation:_novelty,_curiosity,__and__prediction_error}\n\n\n\n\\subsection{Early Concepts of Intrinsic Curiosity}\n\\label{sec:4_1_early_concepts_of_intrinsic_curiosity}\n\nThe challenge of exploration in reinforcement learning, particularly in environments characterized by sparse or delayed extrinsic rewards, led to the development of intrinsic motivation. This paradigm shift moved beyond solely relying on external reward signals, proposing that agents could be driven by an internal 'curiosity' or 'novelty' derived from their own learning progress or model improvement. These foundational concepts laid the theoretical and conceptual groundwork for later, more sophisticated curiosity-driven and novelty-seeking exploration methods.\n\nOne of the earliest proponents of intrinsic curiosity was \\textcite{Schmidhuber1997}, who introduced the idea of rewarding an agent for improving its world model's predictive accuracy. The agent is intrinsically motivated to explore states where its current model makes inaccurate predictions, thus seeking out \"surprising\" observations to reduce its uncertainty and improve its understanding of the environment. Building on this, \\textcite{Singh2004} further formalized the notion of intrinsic motivation, comparing and contrasting different intrinsic signals such as novelty (unfamiliarity) and surprise (prediction error), providing a more theoretical framework for these internal drives.\n\nAs reinforcement learning moved towards more complex, high-dimensional domains, the challenge became scaling these intrinsic curiosity concepts. \\textcite{Stadie2015} addressed this by proposing an exploration method that assigned bonuses from a concurrently learned deep predictive model of the system dynamics. This work demonstrated how the early ideas of prediction-error-based curiosity could be extended to tasks requiring raw pixel inputs, like Atari games, by leveraging deep neural networks to parameterize the world model. Further refining the theoretical underpinnings, \\textcite{Houthooft2016} introduced Variational Information Maximizing Exploration (VIME), a principled, Bayesian approach that encourages agents to explore by maximizing the information gain about the environment's dynamics model. This method provides a more formal way to quantify and reduce epistemic uncertainty, guiding exploration towards states that are most informative for improving the agent's internal model.\n\nDespite these advancements, prediction-error-based curiosity methods faced a challenge known as the \"noisy TV problem,\" where agents could be perpetually distracted by unlearnable stochastic elements in the environment that constantly generated high prediction errors. To address this, \\textcite{Pathak2017} proposed the Intrinsic Curiosity Module (ICM), which computes intrinsic rewards based on the prediction error of future states in a *learned feature space* rather than raw pixel space. By learning a feature representation that is invariant to factors beyond the agent's control, ICM effectively filters out unlearnable stochasticity, allowing curiosity to focus on aspects of the environment that the agent can influence. \\textcite{Burda2018} further simplified and improved the robustness of curiosity-driven exploration with Random Network Distillation (RND). RND measures novelty as the prediction error of a fixed, randomly initialized target network's output by a trained prediction network, providing a highly effective intrinsic reward signal that is largely immune to the noisy TV problem because the prediction target is independent of the environment's true dynamics.\n\nThe practical utility of these curiosity-driven approaches has been demonstrated across various applications. For instance, \\textcite{Li2019tj1} showed how a simplified Intrinsic Curiosity Module (S-ICM) could be effectively integrated with off-policy reinforcement learning methods, significantly improving exploration efficiency and learning performance in robotic manipulation tasks with sparse rewards. Similarly, \\textcite{Zhelo2018wi8} applied curiosity-driven exploration to mapless navigation for mobile robots, validating its crucial role in improving deep reinforcement learning performance in tasks with challenging exploration requirements and enhancing generalization capabilities in unseen environments. More recently, \\textcite{Sun2022ul9} utilized a similarity-based curiosity module to enable aggressive quadrotor flights, demonstrating how intrinsic motivation can accelerate training and improve the robustness of policies in complex control tasks.\n\nIn conclusion, the early concepts of intrinsic curiosity marked a fundamental shift in reinforcement learning, moving from external reward dependence to internal drives based on predictability, surprise, and learning progress. These pioneering ideas, from \\textcite{Schmidhuber1997}'s initial formulation of prediction error as a motivator to the more robust and scalable deep learning-driven methods like ICM \\textcite{Pathak2017} and RND \\textcite{Burda2018}, have provided effective solutions for exploration in sparse-reward environments. While significant progress has been made in making these methods robust to irrelevant stochasticity, ongoing research continues to explore how to design intrinsic reward functions that consistently align with efficient and meaningful exploration across diverse, open-ended domains, and how to balance these internal drives with external task objectives.\n\\subsection{Count-Based and Density-Based Novelty}\n\\label{sec:4_2_count-based__and__density-based_novelty}\n\nEffective exploration is paramount in reinforcement learning, particularly when agents operate in environments characterized by sparse extrinsic rewards or vast, high-dimensional state spaces. A prominent class of intrinsic motivation methods addresses this by quantifying the 'novelty' or 'unvisitedness' of states, generating internal reward signals that encourage agents to venture into less-frequented regions. This approach aims to foster broad state space coverage, which is often crucial for discovering optimal policies.\n\nThe foundational concept of count-based exploration, as pioneered by \\cite{thrun1992efficient}, involves assigning an intrinsic bonus to states inversely proportional to their visitation frequency. In tabular or low-dimensional discrete environments, these methods provide a theoretically sound mechanism for directed exploration, ensuring that agents sufficiently explore all reachable states. However, as discussed in Section 2.3, traditional count-based approaches face a critical limitation: the curse of dimensionality. In high-dimensional or continuous state spaces, the probability of revisiting any exact state becomes infinitesimally small. This renders direct state counting impractical, as most states are encountered only once, leading to uniformly high novelty bonuses that fail to guide exploration effectively.\n\nTo bridge this gap and enable count-based exploration in deep reinforcement learning, researchers developed sophisticated techniques to approximate state visitation frequencies. Early efforts, such as those by \\cite{stadie20158af}, demonstrated that deep predictive models could generate intrinsic exploration bonuses based on learned system dynamics in complex visual environments like Atari games. While not strictly count-based, this work highlighted the potential of neural networks to process high-dimensional observations and produce meaningful intrinsic signals, setting the stage for more direct approximations of novelty.\n\nA pivotal breakthrough in scaling count-based exploration was the introduction of pseudo-counts and density models. \\cite{bellemare2016unifying} proposed a unified framework that generalizes count-based exploration by estimating state visitation frequencies using density models, thereby generating \"pseudo-counts\" for high-dimensional observations. Instead of exact state matching, this approach leverages the statistical likelihood of observing a state given past experiences. States that are less probable under the learned density model are considered more novel and receive higher intrinsic rewards. This effectively overcomes the limitations of exact state enumeration by providing a continuous and differentiable measure of novelty.\n\nBuilding on this principle, various practical implementations emerged. \\cite{tang20166wr} introduced \\texttt{\\#Exploration}, a surprisingly effective yet simple method that maps high-dimensional states to hash codes. By counting the occurrences of these hash codes, the approach approximates state visitation frequencies, allowing for scalable pseudo-counting. This demonstrated that even a relatively crude approximation of state novelty, when combined with deep reinforcement learning, could yield near state-of-the-art performance on challenging benchmarks. However, the effectiveness of hash-based methods can be sensitive to the choice of hash function and the potential for hash collisions, which might conflate distinct states. Further refining the use of neural networks for density estimation, \\cite{ostrovski2017count} explicitly employed neural density models to compute pseudo-counts. This provided a more principled and robust statistical approach to estimate state novelty in complex visual environments, as the density model can learn more meaningful representations of states and their relationships. The challenge with such methods lies in the computational complexity of training accurate density models in high-dimensional spaces and ensuring that the learned density truly reflects meaningful novelty rather than irrelevant stochasticity.\n\nWhile count-based and density-based methods primarily focus on the frequency of state visitation, other intrinsic motivation techniques, such as curiosity-driven exploration, leverage prediction error as a proxy for novelty. Methods like the Intrinsic Curiosity Module (ICM) by \\cite{pathak2017curiosity} and Random Network Distillation (RND) by \\cite{burda2018exploration} reward agents for encountering states where their internal predictive models are inaccurate or for states that lead to unpredictable outcomes. These approaches offer an alternative perspective on novelty, focusing on the agent's learning progress or uncertainty about environmental dynamics rather than mere visitation frequency. Although distinct in their underlying signals, both paradigms share the common goal of generating intrinsic rewards to drive exploration in sparse-reward, high-dimensional settings, with density-based methods providing a statistical measure of \"unvisitedness\" and prediction-error methods focusing on \"unpredictability.\"\n\nIn summary, count-based and density-based novelty methods have undergone a significant evolution, transforming from simple heuristics for discrete environments into sophisticated deep learning techniques capable of scaling to complex, high-dimensional state spaces. The transition from direct state counting to pseudo-counts derived from neural density models has been critical for enabling robust exploration in deep reinforcement learning. These techniques provide a practical and often effective way to incentivize broad state space coverage and discover new areas. Nevertheless, challenges persist, including the computational overhead of training accurate density models, the sensitivity to state representation, and the difficulty of ensuring that the quantified novelty aligns with task-relevant exploration rather than being misled by uninformative stochasticity. Future research continues to refine these methods, often by integrating insights from both visitation statistics and predictive uncertainty, to develop more adaptive and robust novelty-seeking agents.\n\\subsection{Prediction Error and Self-Supervised Curiosity}\n\\label{sec:4_3_prediction_error__and__self-supervised_curiosity}\n\nEffective exploration remains a cornerstone challenge in reinforcement learning (RL), particularly in environments characterized by sparse rewards and high-dimensional observations. To address this, intrinsic motivation methods have emerged, where agents generate their own reward signals to drive discovery. A prominent approach within this paradigm is self-supervised curiosity, which leverages the agent's ability to predict future states or features, using prediction error as an intrinsic reward to guide exploration. This strategy incentivizes agents to seek out situations where their internal models of the world are inaccurate, thereby driving learning about the environment's underlying dynamics.\n\nThe foundational concept of curiosity as a driver for learning can be traced back to early work by \\cite{schmidhuber1997}, which proposed that agents could be intrinsically motivated to explore by optimizing the predictability of their sensory inputs. This early idea laid the groundwork for defining curiosity as a measure of surprise or novelty. Expanding on this, \\cite{stadie20158af} demonstrated that deep predictive models could be effectively used to assign exploration bonuses in complex domains like Atari games, by rewarding states where the agent's learned dynamics model exhibited high uncertainty. This represented an important step in scaling prediction-error-based curiosity to high-dimensional visual inputs, moving beyond simpler tabular settings.\n\nA significant advancement in this direction was the Intrinsic Curiosity Module (ICM) proposed by \\cite{pathak2017}. ICM defines curiosity as the error in predicting the consequence of an agent's own actions within a learned feature space. Specifically, it trains a self-supervised forward dynamics model to predict the next latent state given the current latent state and action. The magnitude of this prediction error then serves as the intrinsic reward. This design incentivizes the agent to explore states where its internal model is inaccurate, thereby driving learning about the environment's dynamics. Crucially, by operating in a learned feature space rather than raw pixels, ICM made an initial attempt to mitigate the \"noisy TV problem,\" where agents might be perpetually drawn to uncontrollable stochastic elements (like static on a TV screen) that generate high prediction error but offer no meaningful learning progress. This focus on learnable and controllable aspects of the environment represented a significant step towards scalable curiosity in high-dimensional visual environments.\n\nDespite ICM's success, its reliance on learning an accurate forward dynamics model can still be problematic, particularly in highly stochastic environments. In such settings, genuine environmental noise or inherent unpredictability can lead to consistently high prediction errors, which are uninformative for learning and can still distract the agent, leading to inefficient exploration. This limitation highlights a critical distinction: prediction error can arise from either the agent's lack of knowledge (epistemic uncertainty) or from inherent environmental stochasticity (aleatoric uncertainty). ICM, by primarily measuring the error of a single forward model, struggles to differentiate between these two sources, potentially leading to spurious curiosity signals.\n\nTo address this challenge and provide more robust curiosity signals, alternative prediction-error-based approaches have emerged. One prominent direction involves leveraging ensembles of models to quantify epistemic uncertainty more explicitly. For instance, methods like Exploration via Distributional Ensemble (EDE) \\cite{jiang2023qmw} encourage exploration of states with high epistemic uncertainty by using an ensemble of Q-value distributions. The disagreement or variance among the predictions of these ensemble members provides a more reliable signal of what the agent truly \"doesn't know,\" rather than simply what is unpredictable due to noise. This ensemble-based approach offers a principled way to direct exploration towards areas where the agent's understanding of the environment is weakest, promoting more efficient knowledge acquisition. The concept of using prediction error as a curiosity signal is also versatile, extending to domains like Large Language Models, where signals such as perplexity over generated responses or variance of value estimates from multi-head architectures can serve as intrinsic exploration bonuses \\cite{dai2025h8g}. From an information-theoretic perspective, these methods align with the idea of maximizing information gain, where surprise (prediction error) and novelty drive the building of abstract dynamics models and transferable skills \\cite{aubret2022inh}.\n\nThe versatility of these prediction-error-based curiosity mechanisms has led to their integration into various RL frameworks. For instance, \\cite{li2019tj1} demonstrated how a simplified version of ICM could be effectively combined with off-policy RL methods, such as Deep Deterministic Policy Gradient (DDPG) and Hindsight Experience Replay (HER), significantly improving exploration efficiency and learning performance in robotic manipulation tasks with sparse rewards. Furthermore, the utility of curiosity-based intrinsic motivation extends to the challenging domain of offline reinforcement learning. \\cite{lambert202277x} investigated how such curiosity-driven methods could be used to collect informative datasets in a task-agnostic manner, which could then be leveraged by offline RL algorithms, highlighting their role in generating high-quality data for subsequent learning.\n\nWhile prediction error and self-supervised curiosity have proven highly effective in driving exploration by incentivizing agents to learn about their environment's dynamics, challenges remain. The primary limitation lies in distinguishing between genuine uncertainty that can be resolved through exploration and inherent environmental stochasticity that offers no meaningful learning progress. This distinction is crucial for designing intrinsic reward functions that consistently align with meaningful exploration, especially in complex, hierarchical tasks. The balance between intrinsic and extrinsic rewards, and the potential for agents to get stuck in \"perpetual exploration\" loops without making tangible task progress, are also critical considerations. Addressing the robustness of these intrinsic signals against uninformative stochasticity is a key area of ongoing research, motivating the development of more sophisticated methods that will be discussed in the subsequent section.\n\\subsection{Robust Intrinsic Rewards: Addressing the Noisy TV Problem}\n\\label{sec:4_4_robust_intrinsic_rewards:_addressing_the_noisy_tv_problem}\n\nThe quest for effective exploration in reinforcement learning (RL) is profoundly challenged by environments offering sparse extrinsic rewards. While intrinsic motivation methods emerged as a promising avenue to generate internal curiosity signals and alleviate this sparsity, early prediction-error approaches frequently succumbed to the \"noisy TV problem.\" This phenomenon describes scenarios where agents are perpetually drawn to unpredictable yet uninformative stochastic elements within the environment, such as random pixel noise on a screen or flickering lights. These elements generate consistently high prediction errors, leading to spurious curiosity that distracts the agent from truly novel and learnable aspects of the environment, thereby hindering focused exploration.\n\nInitial efforts, such as those by \\cite{stadie20158af}, explored incentivizing exploration through bonuses derived from concurrently learned deep predictive models of system dynamics. This aimed to reward agents for encountering states that challenged their current environmental understanding. A prominent example of this paradigm is the Intrinsic Curiosity Module (ICM) \\cite{Pathak2017}. ICM trains a forward dynamics model to predict the next state's features given the current state's features and the executed action. The magnitude of this prediction error then serves as an intrinsic reward, encouraging the agent to visit states where its internal model is inaccurate. While ICM provided a scalable solution for high-dimensional visual inputs by operating in a learned feature space, its reliance on predicting *future states* made it inherently susceptible to the noisy TV problem. In environments with uninformative stochasticity, the forward dynamics model would consistently fail to predict these truly random, irreducible changes, resulting in persistently high prediction errors. This spurious curiosity would then cause the agent to repeatedly visit these uninformative areas, diverting computational resources and hindering progress towards task-relevant exploration.\n\nTo overcome the critical limitations posed by the noisy TV problem, \\cite{Burda2018} introduced Random Network Distillation (RND), a pivotal innovation in robust intrinsic reward generation. RND fundamentally redefines novelty detection by decoupling the intrinsic reward from the learnability of the environment's true dynamics. Instead of predicting future states, RND employs two neural networks: a fixed, randomly initialized target network ($f$) and a predictor network ($\\hat{f}$). Both networks receive the current state $s_t$ as input. The predictor network is trained to predict the output of the target network, i.e., $\\hat{f}(s_t) \\approx f(s_t)$. The intrinsic reward is then defined as the mean squared error between the outputs of these two networks: $r_i = ||\\hat{f}(s_t) - f(s_t)||^2$.\n\nThe key insight of RND lies in its design: for any given state $s_t$, even one containing uninformative stochasticity (like a noisy TV screen), the randomly initialized target network $f(s_t)$ produces a *fixed and deterministic* output embedding. The predictor network $\\hat{f}(s_t)$ is then trained to learn this fixed mapping. If an agent repeatedly visits a state $s_t$, the predictor $\\hat{f}$ will eventually learn to accurately map $s_t$ to $f(s_t)$, causing the prediction error and thus the intrinsic reward to *decay*. This mechanism effectively filters out irrelevant stochasticity because the reward is based on the *novelty of the state itself* (i.e., how well the predictor has learned to map that specific state to its fixed target), rather than the unpredictability of state transitions. Consequently, even a noisy TV state, despite its visual randomness, yields a fixed target output from $f$. Once the agent has sufficiently explored this state, $\\hat{f}$ learns the mapping, and the curiosity bonus diminishes, preventing perpetual attraction. This contrasts sharply with ICM, where the prediction error for a truly stochastic transition remains irreducible, leading to persistent curiosity.\n\nRND's robust and simpler mechanism for novelty detection proved highly effective, leading to significantly more efficient and directed exploration in complex visual domains, such as Atari games, where it substantially outperformed prior methods on hard exploration tasks \\cite{Burda2018}. Its success underscored the importance of designing intrinsic reward signals that are resilient to environmental noise and focus on aspects of the environment truly conducive to learning. Subsequent works, such as \\cite{li2019tj1}, further explored simplified variants of curiosity modules, demonstrating how architectural or methodological simplifications could enhance the practical utility and integration of prediction-error based intrinsic motivation, particularly for off-policy reinforcement learning methods.\n\nWhile RND provided a powerful solution to the noisy TV problem, it is not the sole robust intrinsic motivation strategy. Other approaches also aim to mitigate the effects of uninformative stochasticity or enhance exploration in complementary ways. For instance, methods based on model disagreement or ensembles, which reward exploration in states where multiple learned dynamics models disagree, can implicitly discount uncontrollable noise by focusing on areas where the agent's *learnable* understanding is inconsistent. Information-theoretic perspectives, as surveyed by \\cite{aubret2022inh}, often emphasize maximizing *useful* information gain, which aligns with RND's implicit discounting of unlearnable noise. Furthermore, diversity-driven exploration strategies, which incentivize agents to visit states that are distinct from previously encountered ones \\cite{hong20182pr}, or Random Latent Exploration (RLE), which encourages agents to pursue randomly sampled goals in a latent space \\cite{mahankali20248dx}, offer alternative mechanisms for robust and deep exploration without relying solely on prediction error. Deep Curiosity Search (DeepCS) \\cite{stanton20183fs} also introduced the concept of \"intra-life novelty,\" rewarding exploration within a single episode, which can complement RND's \"across-training novelty\" by encouraging immediate discovery.\n\nDespite these significant advancements, challenges persist. While RND effectively addresses the noisy TV problem by focusing on learnable novelty, the intrinsic reward signal can still be somewhat undirected, potentially leading to exploration of areas that are novel but not necessarily relevant to the extrinsic task. Future research continues to explore how to imbue intrinsic rewards with more task-relevant directionality, perhaps through goal-conditioned or hierarchical approaches, or how to combine them with other exploration strategies to achieve even more efficient and purposeful exploration in increasingly complex and open-ended environments. The integration of RND into advanced agents like Agent57 \\cite{Badia2020Agent57} further demonstrates its lasting impact as a foundational component for achieving state-of-the-art performance in diverse and challenging domains.\n",
    "Advanced and Adaptive Exploration Strategies": "\\section{Advanced and Adaptive Exploration Strategies}\n\\label{sec:advanced__and__adaptive_exploration_strategies}\n\n\n\n\\subsection{Hierarchical Reinforcement Learning for Exploration}\n\\label{sec:5_1_hierarchical_reinforcement_learning_for_exploration}\n\n\nEffective exploration is a critical bottleneck in reinforcement learning (RL), particularly in complex environments characterized by vast state-action spaces, sparse rewards, and long task horizons. Hierarchical Reinforcement Learning (HRL) offers a powerful and principled framework to address these challenges by decomposing large, intractable problems into a hierarchy of more manageable sub-problems. This decomposition allows agents to learn and explore at different levels of temporal abstraction, significantly enhancing exploration efficiency and robustness.\n\nThe foundational concept underpinning HRL for exploration is the \"Options Framework\" introduced by \\cite{Sutton1999}. Building upon earlier notions of skill chaining by \\cite{Singh1995}, the Options Framework formalizes \"options\" as temporally extended actions, or sub-policies, that execute for multiple time steps. An option consists of an initiation set (states where it can be taken), a policy (how to act while the option is active), and a termination condition (when the option ends). This framework allows a high-level policy to choose among options, while a low-level policy executes the primitive actions within a chosen option. This mechanism fundamentally aids exploration by enabling agents to traverse large portions of the state space more efficiently than with primitive actions alone. Instead of exploring individual steps, the agent explores sequences of actions (options), effectively reducing the effective search space and allowing for more directed movement towards relevant subgoals or novel regions. For instance, an agent might learn an \"open door\" option, which, once selected, reliably executes the necessary primitive actions to open a door, allowing the high-level policy to explore the consequences of being on the other side of the door, rather than stumbling upon the correct sequence of door-opening actions by chance.\n\nIn the era of deep reinforcement learning, various architectures have been proposed to implement hierarchical control and leverage its benefits for exploration. FeUdal Networks (FuN) \\cite{Vezhnevets2017} introduced a two-level hierarchy with a \"Manager\" module that sets goals in a latent space and a \"Worker\" module that executes primitive actions to achieve those goals. The Manager explores the space of goals, while the Worker explores the primitive action space conditioned on the current goal. This explicit goal-setting mechanism intrinsically guides exploration towards achieving meaningful sub-objectives. Similarly, Hierarchical Reinforcement Learning with Off-policy Correction (HIRO) \\cite{Nachum2018} enables efficient learning of both high-level and low-level policies by correcting for off-policy data, allowing the high-level policy to explore by setting goals in the state space, and the low-level policy to learn how to reach those goals. Hierarchical Actor-Critic (HAC) \\cite{Levy2019} further refines this by using multiple layers of actor-critic agents, where higher levels set goals for lower levels, and a \"hindsight experience replay\" mechanism allows agents to learn from failed attempts to reach goals, thereby improving exploration by making better use of suboptimal trajectories. These deep HRL methods demonstrate how learning goal-conditioned policies at different levels of abstraction can significantly accelerate exploration in complex, high-dimensional environments.\n\nA critical aspect of HRL for exploration is the autonomous discovery of useful options or skills, often driven by intrinsic motivation. Rather than manually defining options, agents can learn them through self-supervision. Methods like Diversity is All You Need (DIAYN) \\cite{Eysenbach2018} and Variational Option Discovery (VALOR) \\cite{Gregor2017} learn a diverse set of skills by maximizing the mutual information between the skill executed and the resulting state trajectory, effectively rewarding the agent for discovering distinct behaviors. These learned skills then serve as valuable options for a higher-level policy, enabling more structured and efficient exploration. The survey by \\cite{aubret2022inh} highlights how information-theoretic intrinsic motivation, particularly novelty and surprise, can assist in building a hierarchy of transferable skills, making the exploration process more robust. Furthermore, \\cite{janjua2024yhk} emphasizes unsupervised skill acquisition as a key advancement for enhancing scalability in open-ended environments, where HRL provides a natural framework for organizing these learned behaviors.\n\nHRL also facilitates temporally coordinated and goal-conditioned exploration. \\cite{zhang2022p0b} proposed Generative Planning Method (GPM), which generates multi-step action plans, effectively acting as temporally extended options. These plans guide exploration towards high-value regions more consistently than single-step perturbations, and the plan generator can adapt to the task, further benefiting future explorations. This aligns with HRL's ability to create intentional action sequences for reaching specific subgoals. Similarly, Random Latent Exploration (RLE) \\cite{mahankali20248dx}, while not strictly HRL, encourages exploration by pursuing randomly sampled goals in a latent space. This goal-conditioned exploration paradigm is inherently compatible with HRL, where the high-level policy can sample latent goals for the low-level policy to achieve, fostering diverse and deep exploration.\n\nThe utility of hierarchical exploration extends significantly to multi-agent systems, where coordination and efficient search are paramount. \\cite{hu2020qwm} designed a cooperative exploration strategy for multiple mobile robots using a hierarchical control architecture, where a high-level decision-making layer coordinates exploration to minimize redundancy, and a low-level layer handles target tracking and collision avoidance. This demonstrates how HRL can structure complex multi-robot behaviors for efficient, coordinated exploration. More recently, \\cite{yu20213c1} tackled cooperative visual exploration for multiple agents with a Multi-agent Spatial Planner (MSP) leveraging a transformer-based architecture with hierarchical spatial self-attentions, enabling agents to capture spatial relations and plan cooperatively based on visual signals. \\cite{liu2024xkk} further advances multi-agent exploration with \"Imagine, Initialize, and Explore\" (IIE), which uses a transformer to imagine critical states and then initializes agents at these states for targeted exploration. This approach, while not explicitly called HRL, embodies hierarchical decomposition by first identifying high-level critical states (subgoals) and then focusing low-level exploration from those points, enhancing the discovery of successful joint action sequences in long-horizon tasks.\n\nIn summary, HRL provides a powerful framework for managing the complexity of exploration in large state-action spaces. By enabling agents to learn and compose temporally extended actions (options) or to decompose complex tasks into sub-problems, HRL significantly reduces the dimensionality of the exploration problem. This structured approach allows agents to focus on achieving meaningful subgoals, leading to more directed and sample-efficient discovery of optimal policies in long-horizon tasks. Despite these advancements, challenges persist, particularly in the autonomous discovery of optimal and diverse skill sets, the robust learning of high-level policies that effectively coordinate lower-level skills, and ensuring seamless communication and transfer of information across hierarchical levels. Future research will likely focus on developing more adaptive and autonomous methods for skill acquisition and composition, further integrating HRL with robust intrinsic motivation and meta-learning to create agents capable of truly open-ended and efficient exploration.\n\\subsection{Learning Exploration Policies (Meta-Exploration)}\n\\label{sec:5_2_learning_exploration_policies_(meta-exploration)}\n\n\nA pivotal advancement in reinforcement learning (RL) exploration shifts the paradigm from relying on hand-crafted heuristics or fixed intrinsic reward functions to enabling agents to autonomously learn their own exploration strategies. This sophisticated approach, termed meta-exploration, involves an outer-loop optimization process that trains a meta-controller or a recurrent policy to generate adaptive exploration behaviors, aiming to maximize long-term returns across a distribution of tasks or episodes. By learning \"how to explore,\" these methods can dynamically adjust the exploration-exploitation trade-off, leading to more efficient, task-relevant discovery and robust performance, particularly in novel and complex environments \\cite{duan2016rll, wang2016learning}. This represents a significant stride towards autonomous and intelligent exploration, where agents generalize their exploration capabilities rather than relearning them from scratch for each new task.\n\nThe foundational concept of meta-exploration was significantly advanced by works demonstrating that recurrent neural networks (RNNs) can serve as meta-learners. \\cite{duan2016rll} introduced RL$^2$ (Reinforcement Learning to Reinforce Learn), a seminal framework where an RNN-based agent is trained to solve a distribution of tasks. The RNN's hidden state effectively encodes task-specific information and past experience, allowing it to learn an exploration strategy that adapts within a single episode and across multiple episodes of a new, unseen task. This enables the agent to exhibit rapid adaptation and efficient exploration behaviors, such as directed search or uncertainty-driven probing, without explicit hand-engineered exploration bonuses. Similarly, \\cite{wang2016learning} explored the idea of \"Learning to Reinforce Learn,\" where an RNN acts as a meta-learner to discover an entire RL algorithm, including its exploration component, by processing sequences of observations, actions, and rewards. These approaches highlight the power of recurrent architectures to implicitly capture and execute sophisticated exploration policies that generalize across related tasks.\n\nBuilding on these foundations, subsequent research has focused on learning more structured and informed exploration strategies. \\cite{gupta2018rge} proposed Model Agnostic Exploration with Structured Noise (MAESN), a gradient-based meta-learning algorithm that learns exploration strategies from prior experience. MAESN leverages prior tasks to initialize a policy and acquire a latent exploration space, which injects structured stochasticity into the policy. This allows for exploration strategies that are informed by previous knowledge, moving beyond simple action-space noise and proving more effective than task-agnostic methods. This work underscores the benefit of meta-learning not just the policy, but the *mechanism* of exploration itself, enabling more targeted and efficient discovery.\n\nMeta-learning has also been applied to the generation and refinement of intrinsic motivation signals for exploration. \\cite{zhang2020xq9} introduced MetaCURE (Meta Reinforcement Learning with Empowerment-Driven Exploration), which explicitly models an exploration policy learning problem separate from the exploitation policy. MetaCURE employs a novel empowerment-driven exploration objective that aims to maximize information gain for task identification, deriving a corresponding intrinsic reward. By learning separate, context-aware exploration and exploitation policies and sharing task inference knowledge, MetaCURE significantly enhances exploration efficiency in sparse-reward meta-RL tasks. This demonstrates how meta-learning can discover effective intrinsic reward functions that guide exploration towards truly informative experiences, addressing a key challenge in intrinsic motivation.\n\nFurthermore, meta-exploration has been integrated with other advanced RL paradigms. \\cite{rimon20243o6} presented MAMBA, a model-based approach to meta-RL that leverages world models for efficient exploration. By learning an internal model of the environment, MAMBA can plan and explore more effectively, leading to greater return and significantly improved sample efficiency (up to 15x) compared to existing meta-RL algorithms, especially in higher-dimensional domains. This highlights the synergy between learning environmental models and meta-learning exploration strategies. Complementing this, \\cite{goldie2024cuf} introduced Learned Optimization for Plasticity, Exploration and Non-stationarity (OPEN), which meta-learns an update rule (an optimizer) whose input features and output structure are informed by solutions to common RL difficulties, including exploration. OPEN's parameterization is flexible enough to use stochasticity for exploration, demonstrating that meta-learning can discover effective policy update mechanisms that inherently promote efficient exploration.\n\nIt is crucial to distinguish meta-exploration from merely adaptive exploration, where exploration parameters are tuned within a single learning process. While methods that dynamically adjust exploration probabilities based on metrics like information entropy \\cite{hu2020yhq} or ensemble learning for balancing exploration-exploitation ratios \\cite{shuai2025fq3} are valuable, they typically do not involve an outer-loop meta-training process to learn a generalizable exploration *strategy* across tasks. Such adaptive approaches are better categorized under integrated and adaptive exploration frameworks (Section 5.3), which focus on dynamic parameter adjustment rather than learning the exploration algorithm itself.\n\nIn conclusion, the shift towards learning exploration policies through meta-exploration represents a profound step towards truly autonomous and intelligent agents. These approaches, ranging from recurrent policies learning exploration behaviors across episodes to meta-learning structured exploration noise, intrinsic motivation signals, or even entire optimization rules, empower agents to generalize their exploration capabilities and achieve robust performance across diverse problem settings. However, significant challenges persist, including the computational expense of meta-training, the difficulty of defining appropriate and diverse task distributions for meta-learning, ensuring the learned exploration strategies generalize to truly novel and out-of-distribution tasks, and designing effective meta-objectives that capture desirable exploration properties. Future research will likely focus on developing more robust, sample-efficient, and generalizable meta-learning algorithms that can discover truly novel and effective exploration strategies across a wide spectrum of tasks, pushing the boundaries of autonomous discovery.\n\\subsection{Integrated and Adaptive Exploration Frameworks}\n\\label{sec:5_3_integrated__and__adaptive_exploration_frameworks}\n\n\nEffective exploration in complex, high-dimensional environments often transcends the capabilities of a single, static strategy, necessitating frameworks that dynamically combine multiple exploration techniques and adaptively select or blend them based on the current task, state, or learning progress. Moving beyond a 'one-size-fits-all' approach, these integrated frameworks aim to create highly versatile and robust agents capable of tackling a wide spectrum of exploration challenges, representing a key direction for developing general-purpose reinforcement learning (RL) agents. Robust intrinsic motivation, as discussed in Section 4, frequently serves as a foundational component within these integrated systems, providing internal reward signals (e.g., RND-style novelty \\cite{Burda2018} or episodic novelty \\cite{NGU}) that are then managed or orchestrated by higher-level adaptive mechanisms.\n\nA prominent approach to achieving adaptive exploration involves the use of meta-controllers that explicitly orchestrate a portfolio of exploration-exploitation policies. The seminal work of \\cite{Badia2020} on Agent57 exemplifies this paradigm, achieving state-of-the-art performance across diverse Atari games. Agent57 integrates multiple intrinsic motivation signals, including Random Network Distillation (RND) for life-long novelty and value-discrepancy-based novelty, with an adaptive exploration strategy managed by a meta-controller. This meta-controller dynamically selects from a range of exploration-exploitation policies, allowing the agent to adjust its behavior on the fly to suit the specific demands of each game and phase of learning. The strength of this approach lies in its ability to explicitly learn *how* to explore by selecting appropriate behaviors from a predefined set, offering significant flexibility. However, a limitation is its reliance on a hand-designed portfolio of base policies and the complexity of meta-learning the controller, which can be computationally intensive and may struggle if the optimal strategy is not represented within the initial portfolio.\n\nBeyond explicit meta-controllers, other integrated frameworks leverage ensemble methods or decoupled architectures to achieve robust and adaptive exploration. Ensemble-based approaches, for instance, integrate multiple policies or value functions to capture uncertainty or promote diversity. \\cite{jiang2023qmw} introduced Exploration via Distributional Ensemble (EDE), a method that encourages exploration of states with high epistemic uncertainty through an ensemble of Q-value distributions. This integration of multiple distributional estimates allows for a more nuanced understanding of uncertainty, leading to improved generalization in unseen environments. Similarly, \\cite{yang2022mx5} proposed Ensemble Proximal Policy Optimization (EPPO), which learns ensemble policies in an end-to-end manner, combining individual policies and the ensemble organically. EPPO adopts a diversity enhancement regularization over the policy space, which theoretically increases exploration efficacy and promotes generalization. In a different vein, \\cite{whitney2021xlu} proposed Decoupled Exploration and Exploitation Policies (DEEP), which structurally separates the task policy from the exploration policy. This decoupling allows for directed exploration to be highly effective for sample-efficient continuous control without incurring performance penalties in densely-rewarding environments. In contrast to Agent57's explicit switching mechanism, these ensemble and decoupled architectures offer a more implicit form of adaptation, either through aggregation of diverse perspectives or a clear separation of learning objectives, providing robustness but potentially less fine-grained, task-specific adaptation.\n\nFurther advancements in adaptive exploration leverage probabilistic, Bayesian, and reactive mechanisms to guide behavior based on uncertainty or environmental shifts. Bayesian approaches provide a principled framework for managing uncertainty, which can be directly used to drive exploration. \\cite{fu20220cl} introduced a model-based lifelong RL approach that estimates a hierarchical Bayesian posterior, which, combined with a sample-based Bayesian exploration procedure, adaptively increases sample efficiency across related tasks. Extending this, \\cite{li2023kgk} explored Bayesian exploration with Implicit Posteriori Parameter Distribution Optimization (IPPDO), modeling parameter uncertainty with an implicit distribution approximated by generative models, offering greater flexibility and improved sample efficiency. In the realm of randomized exploration, \\cite{ishfaq20235fo} and \\cite{ishfaq20245to} developed scalable Thompson sampling strategies using Langevin Monte Carlo and approximate sampling, respectively. These methods directly sample the Q-function from its posterior distribution, providing provably efficient and adaptive exploration by inherently balancing uncertainty and reward. For dynamic environments, \\cite{steinparz20220nl} proposed Reactive Exploration, designed to track and react to continual domain shifts in lifelong reinforcement learning, demonstrating adaptive policy updates in non-stationary settings. These probabilistic and reactive methods offer strong theoretical grounding for continuous adaptation but often face computational challenges in high-dimensional settings, requiring efficient approximation techniques.\n\nEmerging trends also point towards exploration as an emergent property from learned optimizers or large pre-trained models. \\cite{goldie2024cuf} introduced Learned Optimization for Plasticity, Exploration and Non-stationarity (OPEN), a meta-learned update rule whose input features and output structure are informed by solutions to RL difficulties, including the ability to use stochasticity for exploration. This represents a shift towards learning the optimization process itself, which implicitly includes adaptive exploration. In the context of large foundation models, \\cite{lee202337c} demonstrated that Decision-Pretrained Transformers (DPT) can exhibit emergent online exploration capabilities in-context, without explicit training for it. Building on this, \\cite{dai2024x3l} proposed In-context Exploration-Exploitation (ICEE), which optimizes the efficiency of in-context policy learning by performing exploration-exploitation trade-offs at inference time within a Transformer model. These approaches suggest a future where exploration is less explicitly engineered and more implicitly learned or emergent, potentially leading to highly generalizable agents, but raise questions about control, interpretability, and the sample efficiency required for pre-training.\n\nIn conclusion, integrated and adaptive exploration frameworks represent a significant leap towards developing general-purpose RL agents. By combining robust intrinsic motivation signals, explicit meta-controllers, ensemble and decoupled architectures, principled Bayesian and probabilistic methods, and leveraging emergent capabilities from learned optimizers and large models, these frameworks move beyond static heuristics to dynamically adjust their exploration behaviors. While significant progress has been made, future directions include developing more theoretically grounded guarantees for these complex adaptive systems, enhancing their computational efficiency and scalability, and ensuring robust generalization to truly novel and open-ended environments where the optimal exploration strategy might not be easily predefined or learned from limited prior experience. A key unresolved challenge is the automated discovery of an optimal portfolio of exploration strategies for meta-controllers, as current approaches often rely on hand-designed components.\n\\subsection{Population-Based and Evolutionary Exploration}\n\\label{sec:5_4_population-based__and__evolutionary_exploration}\n\n\nThe inherent challenge of the exploration-exploitation dilemma in Reinforcement Learning (RL) often leads single agents to converge prematurely to sub-optimal policies, particularly in environments characterized by sparse rewards or complex, multi-modal reward landscapes. To overcome these limitations, a distinct paradigm has emerged that leverages populations of agents or evolutionary algorithms to foster broader, more robust exploration and facilitate global search. These approaches represent a meta-level solution, structuring the learning system itself for enhanced discovery.\n\nEarly precursors to population-based exploration can be found in Neuroevolution, where neural network architectures and weights are optimized using evolutionary algorithms. Methods like NEAT (NeuroEvolution of Augmenting Topologies) \\cite{stanley2002evolving} demonstrated the power of evolving diverse populations of networks to solve complex control tasks, implicitly performing exploration by searching a vast hypothesis space. More recently, Evolution Strategies (ES) have gained prominence as a scalable black-box optimization technique for deep RL, capable of training deep neural networks efficiently due to their high parallelizability \\cite{salimans2017evolution}. However, standard ES can struggle with sparse or deceptive reward landscapes, necessitating directed exploration. To address this, \\cite{conti2017cr2} introduced methods like Novelty Search with Evolution Strategies (NS-ES) and Quality Diversity (QD) algorithms hybridized with ES. These approaches maintain a population of novelty-seeking agents, rewarding exploration of novel behaviors rather than just high performance, thereby enabling ES to avoid local optima and achieve higher performance on challenging deep RL tasks like Atari games and simulated robot locomotion.\n\nA significant advancement in population-based methods for deep RL is Population Based Training (PBT) \\cite{jaderberg2017population}. PBT concurrently trains a population of agents, each with its own set of hyperparameters and model weights. Unlike traditional grid search or random search, PBT dynamically adapts hyperparameters during training by periodically evaluating agents, exploiting well-performing ones (copying their weights and hyperparameters) and exploring new hyperparameter configurations for underperforming ones. This asynchronous \"exploit-and-explore\" strategy allows PBT to discover robust hyperparameter schedules and model weights simultaneously, leading to faster training and improved final performance across diverse tasks, including complex deep RL benchmarks. PBT's strength lies in its ability to adaptively tune both learning processes and agent policies, making it highly effective for complex, high-dimensional problems.\n\nBuilding on the strengths of both evolutionary algorithms and gradient-based RL, Evolutionary Reinforcement Learning (ERL) frameworks have emerged as a powerful hybrid paradigm. These methods typically combine the global search capabilities of evolutionary algorithms with the local optimization efficiency of gradient-based RL. Early ERL approaches, such as those by \\cite{khadka2018evolutionary} and CEM-RL \\cite{pourchot2019cemrl}, often involve evolving a population of actor networks, while a shared critic network (trained via gradient descent) provides value estimates to guide both the evolutionary process and individual actor updates. This hybrid approach aims to leverage the exploration benefits of evolution (e.g., escaping local optima, maintaining diversity) and the sample efficiency of RL. For instance, \\cite{sun2024edc} proposed a modified ERL method for multi-agent region protection, amalgamating Differential Evolution (DE) for diverse sample exploration and overcoming sparse rewards with Multi-Agent Deep Deterministic Policy Gradient (MADDPG) for training defenders and expediting DE convergence.\n\nA more recent advancement in this line is the Two-Stage Evolutionary Reinforcement Learning (TERL) framework proposed by \\cite{zhu2024sb0}. TERL addresses a key limitation of prior ERL methods, which often evolve only actor networks, thereby constraining exploration if a single critic network falls into local optima. Instead, TERL maintains and optimizes a population of *complete RL agents*, each comprising both an actor and a critic network. This design enables more independent and diverse exploration by each individual, mitigating the risk of premature convergence to suboptimal policies dictated by a flawed shared critic. The TERL framework operates through a novel two-stage learning process: an initial \"Exploration Stage\" where all individuals learn independently, optimized by a hybrid approach combining gradient-based RL updates with meta-optimization techniques like Particle Swarm Optimization (PSO). This stage emphasizes diversification, with agents sharing information efficiently through a common replay buffer, which helps propagate beneficial experiences across the population. Following this, the \"Exploitation Stage\" focuses on refining the best-performing individual from the population through concentrated RL-based updates, while the remaining individuals continue to undergo PSO to further diversify the replay buffer. This dynamic allocation of computational resources and tailored optimization strategies across stages allows TERL to effectively balance the exploration-exploitation dilemma.\n\nDespite their advantages, population-based and evolutionary methods introduce their own set of challenges. Computational cost is a primary concern, as maintaining and training multiple agents or evolving large populations can be resource-intensive, although parallelization strategies (like those in ES and PBT) mitigate this. Furthermore, the integration of diverse data from population optimization into off-policy RL algorithms, particularly through shared replay buffers, can introduce instability and even degrade performance, as highlighted by \\cite{zheng2023u9k}. This issue arises because population data, while diverse, might not align with the on-policy distribution expected by some RL algorithms, leading to an \"overlooked error.\" To remedy this, \\cite{zheng2023u9k} proposed a double replay buffer design to provide more on-policy data, demonstrating the need for careful architectural considerations when combining these paradigms. The choice between PBT's hyperparameter evolution and ERL's policy evolution also presents a trade-off: PBT excels at finding robust training configurations, while ERL directly optimizes policy parameters, often leading to more direct policy improvement.\n\nIn conclusion, population-based and evolutionary exploration methods offer a compelling meta-level solution to the challenges of exploration in complex RL environments. By evolving populations of complete RL agents, dynamically adapting hyperparameters, or employing hybrid optimization strategies, these approaches enable more diverse learning trajectories and a more robust search for optimal policies, moving beyond the limitations of single-agent exploration heuristics. Future research could explore more sophisticated mechanisms for inter-agent information sharing, investigate adaptive intrinsic motivation signals within these population-based frameworks, or extend these concepts to multi-task and open-ended learning scenarios, further enhancing the adaptability and generalization capabilities of RL agents.\n",
    "Specialized Contexts and Applications of Exploration": "\\section{Specialized Contexts and Applications of Exploration}\n\\label{sec:specialized_contexts__and__applications_of_exploration}\n\n\n\n\\subsection{Exploration in Offline Reinforcement Learning}\n\\label{sec:6_1_exploration_in_offline_reinforcement_learning}\n\n\nIn offline Reinforcement Learning (RL), the agent is tasked with learning an optimal policy solely from a fixed, pre-collected dataset, fundamentally precluding any further active interaction with the environment. This paradigm shift introduces unique challenges for exploration, as traditional active exploration strategies, which involve generating new experiences, are inherently impossible. Instead, the core problem transforms into managing *distributional shift* and preventing the policy from querying actions that are out-of-distribution (OOD) relative to the dataset. The focus shifts from active exploration to ensuring 'conservative learning' within the boundaries of the existing data distribution, aiming to identify reliable regions for policy improvement while rigorously avoiding OOD actions that could lead to unreliable value estimates or unsafe behaviors due to extrapolation errors. This conservative approach is paramount for real-world applications where data collection is costly, risky, or simply not feasible during the learning process.\n\nThe foundational approaches to offline RL primarily address the distributional shift problem through two main mechanisms: policy constraints and value function pessimism. Seminal works like Batch-Constrained Q-Learning (BCQ) \\cite{fujimoto2019off} introduced explicit policy constraints, regularizing the learned policy to stay close to the behavior policy that generated the dataset. This is typically achieved by adding a regularization term (e.g., KL-divergence) to the policy objective, ensuring that the agent does not venture into unobserved state-action pairs. Similarly, Behavior Regularized Actor Critic (BRAC) \\cite{wu2019behavior} further explored various forms of behavior regularization to mitigate the distributional shift. While effective in keeping the policy close to the data, these methods can sometimes limit the discovery of truly optimal policies if the behavior policy was suboptimal.\n\nA complementary and highly influential approach is Conservative Q-Learning (CQL) \\cite{kumar2020conservative}. CQL tackles the distributional shift by explicitly enforcing pessimism in the value function estimation. It achieves this by adding a penalty to the Q-values of OOD actions, ensuring that the learned Q-function provides a lower bound on the true Q-values. This prevents overestimation of action values for unseen actions, which is a common failure mode in offline RL. While BCQ primarily constrains the policy directly, CQL intervenes at the value function level, indirectly shaping the policy by making OOD actions less attractive. This distinction highlights different points of intervention for ensuring conservatism.\n\nBuilding upon these foundations, subsequent methods have leveraged uncertainty estimation to guide conservative learning more explicitly. The Uncertainty Weighted Actor-Critic (UWAC) \\cite{wu2021r67} explicitly incorporates uncertainty treatment by detecting OOD state-action pairs and down-weighting their contribution in the training objectives. Utilizing a practical dropout-based uncertainty estimation, UWAC laid groundwork for robust learning by mitigating the impact of unreliable data points. Similarly, \\cite{rezaeifar20211eu} conceptualized offline RL as \"anti-exploration,\" proposing to subtract a prediction-based exploration bonus from the reward. This innovative approach encourages the policy to remain within the support of the dataset by penalizing actions whose consequences cannot be reliably predicted, effectively extending pessimism-based offline RL methods to deep learning settings. Further, \\cite{zhang20244ty} introduced an Entropy-regularized Diffusion Policy with Q-Ensembles, where robust policy improvement is achieved by learning the lower confidence bound of Q-ensembles. This implicitly accounts for uncertainty in value estimates, mitigating the impact of inaccurate value functions from OOD data, while an entropy regularizer improves \"exploration\" within the offline dataset by encouraging diverse actions within the observed distribution.\n\nSimpler yet effective mechanisms have also emerged to ensure in-sample learning. \\cite{ma2024jej} proposed Improving Offline Reinforcement Learning with in-Sample Advantage Regularization (ISAR), which adapts offline RL to robotic manipulation with minimal changes. ISAR learns the state-value function exclusively from dataset samples, then calculates the advantage function based on this in-sample estimation and adds a behavior cloning regularization term. This method effectively mitigates the impact of unseen actions without introducing complex hyperparameters, offering a straightforward approach to conservative learning that implicitly handles OOD issues.\n\nA significant challenge, particularly in model-based offline RL (MBRL), is addressing biased exploration during the synthetic trajectory generation phase. Standard maximum entropy exploration mechanisms, often adopted from online RL, can lead to skewed data distributions and impaired performance when applied to learned dynamics models. To tackle this, \\cite{wu2024mak} introduced OCEAN-MBRL (Offline Conservative ExplorAtioN for Model-Based Offline Reinforcement Learning), a novel plug-in rollout approach. OCEAN explicitly decouples exploration from exploitation, introducing a principled, conservative exploration strategy guided by an ensemble of dynamics models for uncertainty estimation. It employs three key constraints: a state evaluation constraint to explore only in low-uncertainty regions, an exploration range constraint to select conservative actions, and a trajectory truncation constraint to limit rollouts in high-uncertainty areas. This comprehensive approach significantly enhances the stability and performance of existing MBRL algorithms by ensuring that exploration within the learned model remains reliable and does not generate new, potentially unsafe, out-of-distribution experiences.\n\nThe transition from offline learning to online fine-tuning presents another critical juncture for conservative exploration. While initial conservatism is vital for stable offline learning, a purely pessimistic policy might fail to discover better actions during online interaction. The Simple Unified uNcertainty-Guided (SUNG) framework for offline-to-online RL \\cite{guo20233sd} quantifies uncertainty via a VAE-based state-action visitation density estimator. SUNG's adaptive exploitation method applies conservative offline RL objectives to high-uncertainty samples and standard online RL objectives to low-uncertainty samples, demonstrating a sophisticated use of uncertainty to dynamically adjust the degree of conservatism. Building on this, \\cite{guo2024sba} proposed Optimistic Exploration and Meta Adaptation (OEMA) for sample-efficient offline-to-online RL. OEMA employs an optimistic exploration strategy, adhering to the principle of optimism in the face of uncertainty, allowing agents to sufficiently explore while reducing distributional shift through meta-learning. Providing a theoretical underpinning for such adaptive strategies, \\cite{hu2024085} showed that Bayesian design principles are crucial for offline-to-online fine-tuning, suggesting that a probability-matching agent, rather than purely optimistic or pessimistic ones, can avoid sudden performance drops while being guaranteed to find the optimal policy.\n\nThe literature on exploration in offline RL has thus evolved from foundational methods that strictly constrain policies or penalize OOD value estimates to sophisticated, explicit conservative exploration strategies, particularly in model-based settings and during the offline-to-online transition. While significant progress has been made in ensuring learning remains robust and effective without active interaction, a persistent challenge lies in the accuracy, reliability, and computational efficiency of uncertainty estimation itself. Future research directions could focus on developing more robust and scalable uncertainty quantification methods, as well as adaptive mechanisms that dynamically adjust conservative exploration constraints based on the evolving confidence in the learned policy and model, effectively balancing the need for safety with the potential for performance improvement.\n\\subsection{Expert-Guided and Demonstration-Based Exploration}\n\\label{sec:6_2_expert-guided__and__demonstration-based_exploration}\n\nEfficient exploration remains a formidable challenge in reinforcement learning (RL), particularly in complex, high-dimensional, and sparse-reward environments. To mitigate this, a significant body of research focuses on leveraging expert knowledge or demonstrations to guide and accelerate the exploration process, thereby improving sample efficiency and policy robustness. This paradigm, often termed Reinforcement Learning from Demonstrations (RLfD), seeks to bridge the gap between purely autonomous trial-and-error and the wealth of human or simulated expertise.\n\nEarly efforts established the foundational utility of demonstrations in overcoming exploration hurdles. \\cite{nair2017crs} demonstrated that even a small set of expert demonstrations, when integrated with RL algorithms like Deep Deterministic Policy Gradients (DDPG) and Hindsight Experience Replay (HER), could provide an order of magnitude speedup in learning complex, continuous control robotics tasks with sparse rewards. These methods often relied on static, \\textit{offline} datasets of expert trajectories, which provided initial guidance but presented inherent limitations. Building upon this, \\cite{uchendu20221h1} proposed Jump-Start Reinforcement Learning (JSRL), which uses a \"guide-policy\" derived from offline data or demonstrations to form a curriculum of starting states for an \"exploration-policy,\" significantly improving sample complexity, especially in data-scarce regimes. Similarly, \\cite{hansen2022jm2} highlighted key ingredients for accelerating visual model-based RL with demonstrations, including policy pretraining, targeted exploration, and oversampling demonstration data, leading to substantial performance gains in sparse reward tasks.\n\nBeyond direct trajectory imitation, expert knowledge can also be integrated through symbolic rules or domain-specific insights. For instance, \\cite{hou2021c2r} introduced Rule-Aware Reinforcement Learning (RARL) for knowledge graph reasoning, injecting high-quality symbolic rules into the model's reasoning process to alleviate sparse rewards and prevent spurious paths. \\cite{mazumder2022deb} proposed using \"state-action permissibility\" knowledge to guide exploration, drastically speeding up deep RL training by identifying and avoiding impermissible actions. In a similar vein, \\cite{liu20228r4} combined human knowledge-based rule bases with imitation learning pre-training (ILDN) and safe RL to enhance efficiency and generalization in large-scale adversarial scenarios. Furthermore, theoretical frameworks have explored how exploration itself can be framed as a utility to be optimized, which demonstrations can implicitly help achieve \\cite{zhang2020o5t, santi2024hct}.\n\nDespite the benefits of offline demonstrations, a persistent challenge is the \"distribution gap\" \\cite{coelho2024oa6}. This occurs when the agent's policy deviates from the expert's, leading it into states not covered by the static demonstration dataset, thereby hindering generalization and robustness. To address this, research has shifted towards more dynamic and interactive integration of expert knowledge. \\cite{ball20235zm} showed that with minimal modifications, existing off-policy RL algorithms could effectively leverage offline data (including demonstrations) for online learning, achieving significant performance improvements. This offline-to-online fine-tuning paradigm is crucial for real-world applications \\cite{rafailov2024wtw, hu2024085}, where policies pre-trained on demonstrations need to adapt to novel online experiences. For example, \\cite{lu2025j7f} presented VLA-RL, an approach that uses online reinforcement learning to improve pretrained Vision-Language-Action (VLA) models, specifically addressing out-of-distribution failures that arise from limited offline data in robotic manipulation. Even leveraging \"negative demonstrations\" or failed experiences can guide exploration by showing what \\textit{not} to do, as demonstrated by \\cite{wu20248f9} in sparse reward environments.\n\nA significant advancement in bridging the distribution gap and enhancing exploration efficiency comes from dynamically interacting with experts. \\cite{hou20248b2} introduced EARLY, an active RL from demonstrations algorithm where the agent intelligently queries for episodic demonstrations based on its trajectory-level uncertainty. This approach makes expert guidance more targeted and resource-efficient by requesting help only when needed. The pinnacle of this dynamic interaction is exemplified by \\cite{coelho2024oa6} with RLfOLD (Reinforcement Learning from Online Demonstrations) for urban autonomous driving. RLfOLD introduces the novel concept of \\textit{online demonstrations}, which are dynamically collected from a simulator's privileged information during the agent's active exploration. These demonstrations are seamlessly integrated into a single replay buffer alongside agent experiences, directly addressing the distribution gap by providing contextually relevant expert guidance. The framework utilizes a modified Soft Actor-Critic (SAC) algorithm with a dual standard deviation policy network, outputting distinct $\\sigma_{RL}$ for exploration and $\\sigma_{IL}$ for imitation, allowing for adaptive balancing of these learning objectives. Furthermore, an uncertainty-based mechanism selectively invokes the online expert to guide the agent in challenging situations, making exploration more targeted and efficient. RLfOLD demonstrated superior performance in the CARLA NoCrash benchmark with significantly fewer resources, highlighting its effectiveness and efficiency in complex, real-time domains.\n\nIn conclusion, the intellectual trajectory of expert-guided exploration has evolved from static, offline demonstration datasets to dynamic, interactive, and online expert guidance. This progression effectively addresses critical challenges such as the distribution gap and sample inefficiency, particularly in safety-critical applications like autonomous driving and robotics. While methods leveraging offline demonstrations provide crucial initial boosts, the trend towards online and active expert interaction, exemplified by frameworks like RLfOLD, represents a significant step towards more robust, adaptive, and generalizable RL systems. Future research will likely focus on refining the mechanisms for generating and integrating online expert guidance, especially in real-world scenarios where \"privileged information\" is unavailable, and developing more sophisticated expert models that can provide nuanced and context-aware interventions.\n\\subsection{Exploration in Dynamic and Expanding Environments}\n\\label{sec:6_3_exploration_in_dynamic__and__exp_and_ing_environments}\n\n\nThe challenge of efficient exploration intensifies significantly in Reinforcement Learning (RL) when agents must operate in environments where the state and action spaces are not static but continually expand or evolve \\cite{yang2021ngm}. Unlike traditional Markov Decision Processes (MDPs) that assume fixed state and action sets, real-world systems often undergo updates, introducing novel states, actions, or even entire sub-environments. This dynamic nature necessitates exploration strategies that can efficiently discover and integrate new information without incurring computationally prohibitive retraining costs or suffering from catastrophic forgetting of previously acquired knowledge. This specialized context forms a crucial subset of lifelong and continual learning, where agents must adapt to an unending stream of tasks or environmental changes \\cite{janjua2024yhk, fu20220cl}.\n\nThe concept of Incremental Reinforcement Learning (Incremental RL) has emerged to specifically address this challenge, focusing on how agents can efficiently adapt their policies to newly introduced states and actions. While lifelong learning broadly concerns sequential task learning and knowledge transfer \\cite{fu20220cl, woczyk20220mn}, Incremental RL distinguishes itself by tackling the explicit *expansion* of the underlying MDP structure. A seminal contribution by \\cite{ding2023whs} formally defines Incremental RL and proposes the Dual-Adaptive $\\epsilon$-greedy Exploration (DAE) algorithm. This approach confronts the inherent inefficiency of standard exploration methods and the strong inductive biases that can arise from extensive prior learning, which often hinder adaptation to expanding environments. DAE employs a Meta Policy ($\\Psi$) to adaptively determine a state-dependent $\\epsilon$ by assessing the exploration convergence of a state (via TD-Error rate), thereby deciding *when* to explore. Concurrently, an Explorer ($\\Phi$) guides the agent to prioritize \"least-tried\" actions by estimating their relative frequencies, addressing *what* to explore. Crucially, DAE also incorporates strategies for incrementally adapting deep Q-networks by reusing trained policies and intelligently initializing new neurons and Q-values. This architectural flexibility, combined with the dual-adaptive exploration mechanism, significantly reduces training overhead and enables robust adaptation to expanding search spaces without retraining from scratch.\n\nThe need for adaptive exploration in dynamic settings is also highlighted by research into non-stationary environments, which share some conceptual overlaps with expanding environments, though they differ mechanistically. For instance, \\cite{steinparz20220nl} introduces Reactive Exploration to cope with continual domain shifts in lifelong reinforcement learning. This work demonstrates that policy-gradient methods benefit from strategies that track and react to non-stationarities, such as changes in reward functions or environmental dynamics, within an otherwise fixed state-action space. While Reactive Exploration focuses on adapting to *changes* in existing elements, DAE specifically addresses the *addition* of new states and actions. However, both underscore the broader necessity for exploration strategies that can actively adapt to environmental changes, rather than relying on static or pre-defined exploration schedules. Similarly, the importance of exploration for generalization to new, unseen environments, as explored by \\cite{jiang2023qmw}, aligns with the goals of Incremental RL. Their Exploration via Distributional Ensemble (EDE) method encourages exploration of states with high epistemic uncertainty, which is crucial for acquiring knowledge that aids decision-making in novel situations. While EDE aims to generalize within a potentially vast but fixed environment, DAE's focus is on efficiently integrating entirely new components into the agent's operational space, a distinction critical for truly open-ended learning systems \\cite{janjua2024yhk}.\n\nOther advanced exploration techniques, such as those leveraging intrinsic motivation \\cite{houthooft2016yee} or information gain maximization \\cite{aubret2022inh}, aim to improve exploration efficiency by incentivizing agents to visit novel states or reduce uncertainty about the environment dynamics. For example, Variational Information Maximizing Exploration (VIME) \\cite{houthooft2016yee} uses Bayesian neural networks to maximize information gain about environment dynamics. While powerful in static high-dimensional environments, their direct applicability and scalability to *continually expanding* state and action spaces present unique challenges. Prediction-error-based methods, like those underlying many intrinsic motivation approaches, may struggle when the underlying dynamics model requires continuous architectural restructuring rather than just parameter updates. The sudden introduction of entirely new states or actions can render existing prediction models inaccurate or incomplete, requiring significant re-learning or architectural modifications that are not inherently handled by these methods. Count-based or density-based novelty methods, while effective for discovering unvisited regions, would need robust mechanisms to distinguish truly *new* states/actions from merely *unvisited* ones within the previously known space, and to efficiently update their density estimations for an ever-growing space. DAE's explicit focus on identifying and prioritizing newly available actions and states, overcoming the inductive bias from extensive prior learning, offers a more targeted solution to these architectural and knowledge-transfer challenges.\n\nThe integration of such adaptive exploration strategies with broader lifelong learning frameworks is a critical direction. Lifelong RL methods, such as model-based Bayesian approaches that estimate a hierarchical posterior to distill common task structures \\cite{fu20220cl}, offer mechanisms for backward transfer and efficient learning across related tasks. However, these often assume a fixed set of potential tasks or a stable underlying model structure. The challenge of Incremental RL lies in the dynamic *growth* of this structure, requiring not just transfer but also efficient architectural expansion and robust exploration of truly novel elements. Learned optimization methods, which meta-learn update rules to handle non-stationarity, plasticity loss, and exploration \\cite{goldie2024cuf}, offer a promising avenue by building adaptability directly into the learning process, potentially complementing DAE's specific exploration mechanisms.\n\nIn conclusion, the progression towards \"Exploration in Dynamic and Expanding Environments\" marks a crucial intellectual shift in Reinforcement Learning, moving beyond the static MDP assumption towards more realistic, evolving systems. While foundational exploration methods provide general tools, the work on Incremental RL, particularly the Dual-Adaptive $\\epsilon$-greedy Exploration \\cite{ding2023whs}, offers a targeted solution for environments where state and action spaces continually grow. Future research in this area will likely focus on extending these adaptive exploration strategies to more complex, partially observable, or even multi-agent expanding environments, further enhancing the lifelong learning capabilities of RL agents in truly dynamic real-world scenarios, and integrating them more deeply with meta-learning and continual learning paradigms to address catastrophic forgetting and efficient knowledge transfer in ever-growing systems.\n\\subsection{Safety-Aware Exploration}\n\\label{sec:6_4_safety-aware_exploration}\n\nThe exploration phase in reinforcement learning (RL) is critical for discovering optimal policies, yet in real-world, safety-critical applications, unconstrained exploration can lead to catastrophic outcomes and raise significant ethical concerns regarding accountability, fairness in decision-making under risk, and the potential for unforeseen negative side-effects in human-inhabited environments. This subsection delves into methods designed to ensure safety during exploration, navigating the inherent tension between the need for aggressive exploration to achieve optimal performance and the imperative to maintain safe operation. The problem is often formally cast within the framework of Constrained Markov Decision Processes (CMDPs), where an agent aims to maximize cumulative reward while simultaneously satisfying constraints on cumulative costs, such as safety violations \\cite{altman1999constrained}. This formalism provides a robust theoretical foundation for developing algorithms that provide safety guarantees during the learning process.\n\nEarly efforts to integrate safety into RL exploration focused on establishing explicit boundaries and constraints. A foundational approach involves \"safety layers\" or \"shielding,\" which act as guardians, restricting the agent's actions or states to predefined safe regions, thereby preventing the agent from entering hazardous situations during learning \\cite{Stachurski2008}. While early works laid the groundwork, modern deep RL has seen significant advancements, notably with methods like those proposed by \\cite{alshiekh2018safe}, which formally integrate shielding into deep RL agents. These methods enforce explicit safety constraints, ensuring exploration is guided within a permissible operational envelope, effectively mitigating the risk of catastrophic failures. However, a key limitation of static safety layers is their potential to be overly conservative, which can severely restrict the agent's exploration capabilities and prevent the discovery of truly optimal, yet initially unknown, safe behaviors. This conservativeness often stems from the difficulty of accurately predefining safe regions in complex, high-dimensional environments, or from worst-case assumptions made to guarantee safety.\n\nAddressing the limitations of static constraints and the inherent conservativeness, more recent research has explored dynamic and learned safety mechanisms, often decoupling the concerns of task performance and safety. \\cite{yu20222xi} introduced SEditor, a two-policy approach that learns a \"safety editor policy\" to transform potentially unsafe actions proposed by a utility-maximizing policy into safe ones. SEditor represents a conceptual shift from static, predefined shields to a learned, dynamic safety filter, allowing for more nuanced and state-dependent safety interventions. This method moves beyond simplified safety models, enabling the safety editor to learn complex safety functions, effectively acting as a dynamic shield. While SEditor demonstrates significantly lower constraint violation rates and maintains high utility, its effectiveness relies on the ability to train an accurate safety editor policy, which can be challenging in highly dynamic or unpredictable environments.\n\nFurther advancing dynamic safety, \\cite{thananjeyan2020d20} introduced Recovery RL, which first leverages offline data to learn about constraint-violating zones. It then employs a task policy for reward maximization and a dedicated recovery policy that activates to guide the agent back to safety when constraint violation is likely. This dual-policy structure allows for more aggressive exploration by the task policy, relying on the learned recovery mechanism to prevent unsafe outcomes. Unlike SEditor, which modifies actions *before* execution, Recovery RL focuses on *recovering* from potentially unsafe trajectories, offering a different trade-off between proactive prevention and reactive correction. Recovery RL demonstrates superior efficiency in balancing task success and constraint adherence in complex, contact-rich manipulation tasks and image-based navigation, even on physical robots, by allowing the task policy greater freedom. Similarly, \\cite{zhang2023wqi} proposed a method for safe RL with dead-ends avoidance and recovery. This approach constructs a boundary to discriminate between safe and unsafe states, equivalent to distinguishing dead-end states, thereby ensuring maximum safe exploration with minimal limitation. Like Recovery RL, it utilizes a decoupled framework with a task policy and a pre-trained recovery policy, along with a safety critic, to ensure safe actions during online training. This strategy aims to achieve better task performance with fewer safety violations by carefully delineating the extent of guaranteed safe exploration, offering a more precise definition of \"safe\" exploration boundaries.\n\nAnother powerful paradigm for guaranteeing safety, particularly in continuous control systems, draws from control theory: Lyapunov stability and Control Barrier Functions (CBFs). These methods provide formal guarantees that a system's state will remain within a predefined safe set. Control Barrier Functions (CBFs) are functions of the state that define a safe set and whose derivatives can be constrained to render this set forward-invariant, thus preventing the agent from leaving it. \\cite{zhang2022dgg} proposed a Barrier Lyapunov Function-based safe RL (BLF-SRL) algorithm for autonomous vehicles. This approach integrates BLF items into an optimized backstepping control method, constraining state variables within a designed safety region during learning. By decomposing optimal control with BLF items, it achieves safe exploration while learning adaptive uncertain items, ensuring both safety and performance optimization in safety-critical domains. While control-theoretic methods like BLF-SRL offer strong, often deterministic, safety guarantees, they typically require an accurate model of the system dynamics and specific assumptions about the environment, a limitation not shared by model-free, data-driven approaches like Recovery RL, which in turn provide only probabilistic safety assurances. Addressing this model dependency, \\cite{cheng20224w2} presented a method using Disturbance-Observer-Based Control Barrier Functions (DOB-CBFs). This approach avoids explicit model learning by leveraging disturbance observers to accurately estimate the pointwise value of uncertainty, which is then incorporated into a robust CBF condition. This allows for less conservative safety filters, especially during early learning, by effectively handling unknown disturbances without requiring extensive model training.\n\nThe inherent uncertainty in exploration necessitates risk-sensitive approaches. Building on Bayesian exploration principles (as discussed in Section 3.2), some approaches use uncertainty to define credible intervals for constraint satisfaction, leading to more principled conservative exploration. \\cite{zhang2024ppn} introduced Lag-U, an uncertainty-augmented Lagrangian safe RL algorithm for autonomous driving. This method uses deep ensembles to estimate epistemic uncertainty, which is then used to encourage exploration and learn a risk-sensitive policy by adaptively modifying safety constraints. Furthermore, it incorporates an intervention assurance mechanism based on quantified uncertainty to select safer actions during deployment. This allows for a better trade-off between efficiency and risk avoidance, preventing overly conservative policies by making safety decisions based on the agent's confidence in its predictions. Complementing this, \\cite{yu2022bo5} proposed a distributional reachability certificate (DRC) to address model uncertainties and characterize robust persistently safe states. Their framework builds a shield policy based on DRC to minimize constraint violations, especially during training, by considering the distribution of potential long-term constraint violations, thereby enhancing safety robustness against model uncertainty. Beyond explicit uncertainty estimation, some methods directly manipulate the learning process to balance reward and safety. \\cite{gu2024fu3} addressed the conflict between reward and safety gradients, proposing a soft switching policy optimization method. By analyzing and manipulating these gradients, their framework aims to achieve a better balance between optimizing for rewards and adhering to safety constraints, offering a more direct way to mitigate the inherent conflict compared to simply adding penalty terms in CMDPs.\n\nIn scenarios where some safety signals are available in a controlled environment, \"guided safe exploration\" can be employed to facilitate safe transfer learning. \\cite{yang2023n56} proposed a method where a \"guide\" agent learns to explore safely without external rewards in a controlled environment where safety signals are available. This guide then helps compose a safe behavior policy for a \"student\" agent in a target task where safety violations are prohibited. The student policy is regularized towards the guide while it is unreliable, gradually reducing the guide's influence as it learns. This approach enables safe transfer learning and faster problem-solving in the target task, highlighting the utility of leveraging prior safety knowledge to bootstrap safe exploration in novel, safety-critical settings.\n\nIn summary, safety-aware exploration has evolved significantly, moving from static, predefined safety layers and formal constraint satisfaction (CMDPs) to dynamic, learned recovery mechanisms, control-theoretic guarantees (BLFs, CBFs), and risk-sensitive approaches leveraging uncertainty quantification. The field continues to grapple with the fundamental tension between encouraging sufficient exploration for optimal learning and guaranteeing safety in real-world, safety-critical applications. Future directions will likely involve integrating more sophisticated formal verification techniques, developing robust and scalable uncertainty quantification for proactive safety prediction, designing intrinsically safe exploration strategies that adhere to ethical guidelines from the outset, and further refining gradient manipulation techniques to optimally balance conflicting objectives. The goal remains to enable autonomous agents to learn effectively without compromising human or environmental safety, ensuring responsible deployment in increasingly complex and sensitive domains.\n",
    "Conclusion and Future Directions": "\\section{Conclusion and Future Directions}\n\\label{sec:conclusion__and__future_directions}\n\n\n\n\\subsection{Summary of Key Advancements}\n\\label{sec:7_1_summary_of_key_advancements}\n\n\nThe journey of exploration in reinforcement learning (RL) reflects a continuous and sophisticated evolution, driven by the persistent challenge of balancing the acquisition of new information with the exploitation of known optimal actions. This review has traced a narrative arc from foundational heuristics to theoretically grounded algorithms, and subsequently to scalable intrinsic motivation techniques, culminating in adaptive, learned exploration strategies tailored for specialized contexts. This progression underscores the field's relentless innovation in addressing the increasing complexity of environments and the inherent difficulties of the exploration-exploitation dilemma.\n\nEarly attempts to navigate the exploration-exploitation trade-off, as discussed in Section 2, centered on basic heuristics like $\\epsilon$-greedy and the implicit exploration benefits of model-based planning architectures such as Dyna-Q. Concurrently, explicit exploration bonuses, often count-based, provided direct incentives for visiting novel states. While effective in tabular settings, these methods faced significant scalability challenges in high-dimensional or continuous state spaces. For instance, early attempts to generalize count-based exploration to deep RL, such as mapping states to hash codes \\cite{tang20166wr}, provided a surprisingly strong baseline but highlighted the need for more robust, scalable novelty detection mechanisms.\n\nA pivotal shift occurred with the development of theoretically grounded exploration strategies (Section 3), which sought to provide provable guarantees on learning efficiency. Principles like \"optimism in the face of uncertainty\" (OFU), embodied in algorithms like R-Max, offered PAC-MDP guarantees by optimistically valuing unexplored regions. Bayesian approaches, notably Thompson Sampling, provided a principled framework for managing uncertainty by maintaining posterior distributions over models or value functions. Further advancing this theoretical rigor, information-theoretic methods emerged, guiding exploration by maximizing knowledge gain about the environment or optimal policy. Techniques such as Variational Information Maximizing Exploration (VIME) \\cite{houthooft2016yee} and MaxInfoRL \\cite{sukhija2024zz8} exemplify this, rewarding agents for transitions that significantly reduce uncertainty or improve the agent's internal model. This information-centric view, as surveyed by \\cite{aubret2022inh}, moved beyond simple novelty to a more sophisticated understanding of learning progress. However, the computational complexity of maintaining accurate uncertainty estimates and the limitations of optimism in scenarios with partially observable rewards \\cite{parisi2024u3o} often restricted their applicability to simpler environments.\n\nThe advent of deep reinforcement learning necessitated a paradigm shift, leading to the widespread adoption of intrinsic motivation techniques (Section 4). These methods generate internal reward signals to drive exploration, proving crucial in environments with sparse external rewards. Initial concepts of curiosity and novelty-seeking evolved into scalable approaches. Count-based methods were adapted for high-dimensional spaces using pseudo-counts and density models. A significant breakthrough came with prediction-error based curiosity, where agents are rewarded for encountering surprising or unpredictable observations, as seen in the Intrinsic Curiosity Module (ICM) \\cite{li2019tj1, zhelo2018wi8}. This directed exploration towards aspects of the environment that improve the agent's internal dynamics model. To address the \"noisy TV\" problem, where agents are perpetually attracted to uninformative stochasticity, robust intrinsic reward mechanisms like Random Network Distillation (RND) were developed, effectively filtering out irrelevant noise and focusing exploration on truly learnable novelty. The introduction of Random Latent Exploration (RLE) \\cite{mahankali20248dx} further simplified this, offering deep exploration benefits by pursuing randomly sampled goals in a latent space without complex bonus calculations.\n\nThe field has continued to push towards more advanced and adaptive exploration strategies (Section 5). Hierarchical Reinforcement Learning (HRL) enabled structured exploration by decomposing tasks into sub-problems, allowing agents to explore at different levels of temporal abstraction. A particularly impactful direction is meta-learning for exploration, where agents learn *how* to explore effectively across diverse tasks. Algorithms like MAESN \\cite{gupta2018rge} demonstrate this by learning structured exploration strategies and latent exploration spaces from prior experience, injecting informed stochasticity into policies and outperforming task-agnostic methods. Furthermore, the development of decoupled exploration and exploitation policies (DEEP) \\cite{whitney2021xlu} highlighted that separating these concerns can significantly boost sample efficiency, particularly in sparse reward settings. Integrated frameworks, such as Agent57, combine multiple techniques like RND, episodic memory, and adaptive meta-controllers to achieve state-of-the-art performance across a wide range of challenging environments. Diversity-driven exploration strategies \\cite{hong20182pr} also contribute to preventing policies from being trapped in local optima by encouraging varied behaviors. Population-based and evolutionary methods offer a meta-level solution, leveraging multiple agents or meta-optimization to achieve more robust and global exploration in complex reward landscapes.\n\nFinally, the application of these sophisticated exploration methods has expanded into specialized contexts (Section 6), demonstrating their versatility and practical impact. In offline reinforcement learning, where active exploration is impossible, the focus shifted to conservative exploration within fixed datasets, employing uncertainty estimation to avoid out-of-distribution actions \\cite{wu2021r67, zi20238ug}. Expert demonstrations and human feedback have been leveraged to guide exploration, significantly improving sample efficiency and overcoming sparse reward challenges in domains like robotics \\cite{nair2017crs, lee2021qzk}. Safety-aware exploration has become critical for real-world applications, incorporating constraints and recovery policies to prevent hazardous actions \\cite{thananjeyan2020d20}. The challenges of dynamic and open-ended environments, which demand continuous adaptation and robust discovery, are also being addressed \\cite{janjua2024yhk, matthews20241yx}. Emerging trends, such as the use of Decision-Pretrained Transformers (DPT) for in-context learning and adaptive exploration \\cite{lee202337c}, hint at a future where powerful foundation models might inherently possess sophisticated exploration capabilities.\n\nIn summary, the field has progressed from simple, often undirected, exploration heuristics to theoretically grounded methods, then to scalable intrinsic motivation for deep RL, and finally to highly adaptive, learned, and integrated strategies. This continuous innovation has enabled RL agents to achieve unprecedented performance and robustness in increasingly complex and diverse tasks, while also addressing critical concerns like safety and data efficiency. The trajectory reflects a deep commitment to overcoming the persistent challenges of the exploration-exploitation dilemma, paving the way for more intelligent and autonomous learning systems.\n\\subsection{Open Challenges and Theoretical Gaps}\n\\label{sec:7_2_open_challenges__and__theoretical_gaps}\n\nDespite significant advancements in reinforcement learning (RL) exploration, several fundamental challenges persist, particularly concerning scalability, robustness, sample efficiency, and the enduring gap between theoretical guarantees and practical applicability in complex, real-world settings. Addressing these issues is crucial for enabling RL agents to operate effectively in high-dimensional, stochastic, and open-ended environments.\n\nA primary challenge lies in scaling exploration strategies to extremely high-dimensional or continuous state-action spaces, where traditional methods struggle due to the curse of dimensionality. Early count-based exploration, while effective in tabular settings \\cite{Thrun1992}, quickly becomes infeasible. Efforts to bridge this gap include methods that generalize counts to high-dimensional spaces, such as using hash codes \\cite{Tang20166wr} or pseudo-counts derived from density models \\cite{Bellemare2016}. Concurrently, intrinsic motivation, often based on prediction error, emerged as a scalable heuristic. For instance, \\cite{Stadie20158af} utilized deep predictive models to generate exploration bonuses in Atari games, demonstrating the potential of learned models to guide exploration in complex visual domains. However, these prediction-error methods, like the Intrinsic Curiosity Module (ICM) \\cite{Pathak2017}, often suffer from the \"noisy TV problem,\" where uninformative stochasticity in the environment can generate spurious intrinsic rewards, leading to inefficient exploration. \\cite{Burda2018} addressed this by proposing Random Network Distillation (RND), which uses the prediction error of a fixed random network, proving more robust to environmental stochasticity and focusing exploration on learnable aspects of the environment. More recently, \\cite{Li2023kgk} proposed Implicit Posteriori Parameter Distribution Optimization (IPPDO) to improve exploration by modeling parameter uncertainty with implicit distributions, aiming for more flexible and efficient exploration in deep RL. Similarly, \\cite{Rahman2022p7b} introduced Robust Policy Optimization (RPO) to maintain high policy entropy throughout training, preventing premature convergence and ensuring sustained exploration in continuous action spaces.\n\nAnother persistent gap exists between methods offering strong theoretical guarantees and those providing practical scalability. Algorithms like R-Max \\cite{Strehl2008} provide provable bounds on sample complexity and regret, but their reliance on explicit model learning or tabular representations limits their application to simpler, finite Markov Decision Processes (MDPs). Bridging this gap requires developing principled yet adaptable solutions for real-world complexity. \\cite{Song2021elb} attempts this by proposing a computationally and statistically efficient model-based RL algorithm for specific model classes (Kernelized Nonlinear Regulators and linear MDPs) with polynomial sample complexity guarantees. In practical control applications, where accurate models are hard to obtain, methods like ModelPPO \\cite{Ma2024r2p} integrate neural network models into actor-critic architectures for AUV path following, demonstrating improved performance over traditional and model-free RL by learning state transition functions. For dynamic systems like microgrids, \\cite{Meng2025l1q} employs online RL with SARSA to adapt to uncertainties without relying on traditional mathematical models, prioritizing computational efficiency and real-time adaptability. Similarly, \\cite{Xi2024e2i} proposes a lightweight, adaptive SAC algorithm for UAV path planning, which adjusts exploration probability dynamically to balance efficiency and generalization in resource-constrained environments. These works highlight the ongoing tension between theoretical optimality and the need for practical, robust solutions in complex engineering domains.\n\nThe challenge of designing exploration strategies that are both sample-efficient and theoretically optimal across diverse tasks, especially in open-ended learning scenarios, remains largely unresolved. Meta-reinforcement learning (meta-RL) offers a promising avenue by learning exploration strategies from prior experience. \\cite{Gupta2018rge} introduced MAESN (Model Agnostic Exploration with Structured Noise) to learn structured exploration strategies, which are more effective than task-agnostic noise. \\cite{Zhang2020xq9} further developed MetaCURE, an empowerment-driven exploration method for meta-RL that maximizes information gain for task identification in sparse-reward settings. Leveraging offline data can also significantly boost sample efficiency. \\cite{Nair2017crs} demonstrated that incorporating demonstrations can overcome exploration difficulties in sparse-reward robotics tasks, while \\cite{Ball20235zm} showed how to effectively integrate offline data into online RL with minimal modifications. Offline RL itself faces challenges with out-of-distribution actions, which \\cite{Wu2021r67} addresses with Uncertainty Weighted Actor-Critic (UWAC) by down-weighting contributions from uncertain state-action pairs. More recent work explores in-context learning with large transformer models: \\cite{Lee202337c} introduced Decision-Pretrained Transformer (DPT), which can learn in-context exploration and conservatism from diverse datasets, generalizing to new tasks. Building on this, \\cite{Dai2024x3l} proposed In-context Exploration-Exploitation (ICEE) to optimize this trade-off at inference time, enhancing efficiency. The development of benchmarks like Craftax \\cite{Matthews20241yx} further underscores the current limitations of existing methods in achieving deep exploration, long-term planning, and continual adaptation required for truly open-ended learning. Cooperative exploration in multi-agent systems \\cite{Hu2020qwm, Yu20213c1} and autonomous navigation \\cite{Li2020r8r, Zhelo2018wi8, Kamalova2022jpm} also represent significant real-world complexities demanding robust and adaptable exploration.\n\nFinally, integrating safety constraints into exploration is a critical, yet often conflicting, requirement for real-world deployment. Extensive exploration, while necessary for learning, can lead to dangerous situations. \\cite{Thananjeyan2020d20} proposed Recovery RL, which decouples task and recovery policies and learns constraint-violating zones from offline data to safely navigate this trade-off. \\cite{Yu20222xi} introduced SEditor, a two-policy approach where a safety editor policy transforms potentially unsafe actions into safe ones, achieving extremely low constraint violation rates. \\cite{Zhang2023wqi} advanced safe RL by identifying and avoiding dead-end states, providing a minimal limitation on safe exploration. In reward-free settings, \\cite{Yang2023n56} proposed a \"guide\" agent to learn safe exploration which then regularizes a \"student\" policy. For deployable safe RL, \\cite{Honari202473t} developed Meta SAC-Lag, using meta-gradient optimization to automatically tune safety-related hyperparameters. Addressing model uncertainties for robust safety, \\cite{Yu2022bo5} introduced a distributional reachability certificate for safe model-based RL. Furthermore, \\cite{Zi20238ug} applied distributionally robust RL for active signal pattern localization, enabling safe exploration in unfamiliar environments with limited data. These efforts highlight the ongoing challenge of balancing the need for exploration with stringent safety requirements, often requiring complex architectural designs or meta-learning approaches.\n\nIn conclusion, while the field has made substantial progress from tabular, theoretically-grounded methods to scalable, deep learning-based intrinsic motivation, significant open challenges remain. The persistent gap between methods with strong theoretical guarantees (often for simpler settings) and those providing practical scalability (often heuristic-driven) underscores the critical need for principled yet adaptable solutions. Future research must focus on developing exploration strategies that are robust to environmental stochasticity, sample-efficient across diverse tasks, capable of deep exploration in open-ended environments, and inherently safe for real-world deployment, potentially through hybrid approaches that combine the strengths of model-based reasoning, intrinsic motivation, and meta-learning with strong theoretical foundations.\n\\subsection{Emerging Trends and Ethical Considerations}\n\\label{sec:7_3_emerging_trends__and__ethical_considerations}\n\n\nThe frontier of reinforcement learning (RL) exploration is characterized by a dual pursuit: developing increasingly sophisticated agents capable of understanding and navigating complex, open-ended environments, and simultaneously ensuring these autonomous systems operate ethically and safely, particularly in human-interactive or safety-critical domains. This subsection explores cutting-edge research directions, including the transformative integration of large foundation models, the development of truly general-purpose and adaptive exploration agents, the increasing focus on learning better representations, and the critical ethical implications of deploying such intelligent systems.\n\nA pivotal emerging trend is the integration of **large foundation models (LFMs)**, such as Large Language Models (LLMs) and Vision-Language Models (VLMs), to imbue RL agents with more sophisticated world understanding, high-level planning capabilities, and common-sense priors. Traditional RL often struggles with extensive exploration in complex, semantically rich environments due to its limited grasp of underlying decision dynamics. LLMs, with their vast domain-specific knowledge, can serve as powerful prior action distributions, significantly reducing exploration and optimization complexity when integrated into RL frameworks through Bayesian inference methods \\cite{yan2024p3y}. This approach can decrease the number of required samples by over 90\\% in offline learning scenarios, demonstrating the immense potential of leveraging pre-trained knowledge to guide exploration. Furthermore, LLMs are being directly utilized to generate intrinsic curiosity signals. For instance, Curiosity-Driven Exploration (CDE) for LLMs in Reinforcement Learning with Verifiable Rewards (RLVR) leverages the model's own perplexity (from the actor) and the variance of value estimates (from the critic) as intrinsic bonuses \\cite{dai2025h8g}. This framework guides exploration by penalizing overconfident errors and promoting diversity, addressing issues like premature convergence and entropy collapse common in LLM-based RL. Beyond direct policy guidance, LLMs can act as adaptive search operators within meta-learning frameworks, where evolutionary search discovers improved algorithms, and RL fine-tunes the LLM policy based on these discoveries, accelerating algorithm design for complex combinatorial optimization tasks \\cite{surina2025smk}. These advancements highlight LFMs' capacity to elevate exploration from low-level state space coverage to high-level conceptual discovery and informed decision-making.\n\nComplementing the rise of LFMs, there is an increasing focus on **learning better representations** to facilitate more informed and efficient exploration. Robust representations are crucial for defining novelty, quantifying uncertainty, and building accurate world models in high-dimensional observation spaces. While earlier methods like the simplified Intrinsic Curiosity Module (S-ICM) \\cite{li2019tj1} and its predecessor ICM \\cite{pathak2017} leveraged prediction error in learned feature spaces to incentivize novelty, contemporary research pushes for more sophisticated self-supervised techniques that disentangle factors of variation and capture epistemic uncertainty. For example, the Actor-Model-Critic (AMC) architecture for Autonomous Underwater Vehicle (AUV) path-following explicitly learns the state transition function via a neural network, enabling the agent to anticipate environmental dynamics and guide exploration towards informative regions \\cite{ma2024r2p}. Beyond explicit model learning, methods like Exploration via Distributional Ensemble (EDE) emphasize the importance of exploration for generalization, not just optimal policy finding \\cite{jiang2023qmw}. EDE encourages exploration of states with high epistemic uncertainty using an ensemble of Q-value distributions, implicitly relying on robust representations to quantify this uncertainty effectively. Similarly, Decoupled Exploration and Exploitation Policies (DEEP) demonstrate that separating the task policy from the exploration policy can yield significant sample efficiency improvements in sparse environments, a benefit often amplified by representations that allow for meaningful novelty detection and uncertainty estimation \\cite{whitney2021xlu}. These approaches underscore that the quality of learned representations directly impacts an agent's ability to discern truly novel or uncertain aspects of an environment, leading to more directed and less wasteful exploration.\n\nThe drive towards **truly general-purpose exploration agents** capable of tackling open-ended problems is leading to more adaptive, robust, and scalable strategies. Rather than relying on fixed heuristics, recent work focuses on agents that can dynamically adjust their exploration behavior. Adaptive exploration strategies, such as those using multi-attribute decision-making based on information entropy and task decomposition, allow for more flexible and context-aware exploration \\cite{hu2020yhq}. Further advancing this, ensemble learning schemes with explicit \"exploration-to-exploitation (E2E) ratio control\" via multiple Q-learning agents and adaptive decay functions enable more nuanced balancing of exploration and exploitation, crucial for real-world applications requiring continuous adaptation \\cite{shuai2025fq3}. The scalability and theoretical guarantees of exploration are also paramount for such agents. Thompson sampling-based methods, employing Langevin Monte Carlo (LMC) and approximate sampling, offer provably efficient and scalable exploration in deep RL with theoretical regret bounds, ensuring reliability in autonomous systems \\cite{ishfaq20235fo, ishfaq20245to}. This extends to collaborative settings, where randomized exploration in cooperative multi-agent RL (MARL) with methods like CoopTS-PHE and CoopTS-LMC provides theoretical guarantees for regret bounds and communication complexity, essential for complex multi-agent environments \\cite{hsu2024tqd}. Moreover, simple yet effective strategies like Random Latent Exploration (RLE), which pursues randomly sampled goals in a latent space, demonstrate that deep exploration can be achieved without complex bonus calculations, promoting broader applicability as a general plug-in for existing RL algorithms \\cite{mahankali20248dx}. The concept of meta-learning how to explore is also gaining traction, with approaches like Learned Optimization for Plasticity, Exploration and Non-stationarity (OPEN) meta-learning update rules that incorporate stochasticity for exploration, showing strong generalization across diverse environments and agent architectures \\cite{goldie2024cuf}. These advancements, alongside broader discussions on open-ended RL emphasizing hierarchical learning, intrinsic motivation, and unsupervised skill acquisition \\cite{janjua2024yhk}, signify a shift towards agents that can autonomously learn and adapt their exploration strategies across a wide spectrum of tasks.\n\nAlongside these advancements in exploration capabilities, the **ethical considerations** surrounding autonomous exploration are gaining increasing prominence, especially in safety-critical or human-interactive environments. The inherent trial-and-error nature of RL exploration can lead to \"bad decisions\" that violate critical safety constraints, as highlighted in reviews of safe RL for power system control \\cite{yu2024x53}. This necessitates responsible development and deployment, emphasizing alignment with human values and safety standards. A direct response to this challenge is the \"human-in-the-loop deep reinforcement learning (HL-DRL)\" approach for optimal Volt/Var control in unbalanced distribution networks \\cite{sun2024kxq}. This method allows human intervention to modify dangerous actions during offline training and integrates human guidance into the actor network's loss function, ensuring the learned policy adheres to operational constraints and human safety guidelines. Broader advancements in RL for autonomous systems also explicitly identify safety, dependability, and explainability as critical constraints that limit wide adoption \\cite{malaiyappan20245bh}. The imperative is to develop exploration strategies that not only discover optimal behaviors but do so within predefined safe regions, learn to recover from unsafe situations, and provide transparent decision-making processes. This ensures that the learning process does not lead to catastrophic outcomes and adheres to ethical considerations, bridging the gap between autonomous learning and responsible societal impact.\n\nIn conclusion, the field is rapidly advancing towards more intelligent, adaptive, and generalizable exploration strategies. This progress is driven by the transformative potential of large foundation models for high-level understanding and goal generation, the continuous refinement of learned representations for informed low-level novelty detection and uncertainty quantification, and the development of meta-learning approaches for truly general-purpose agents. Simultaneously, the increasing power and autonomy of these systems amplify the imperative to address ethical implications, particularly in safety-critical domains. Future research must continue to bridge the gap between theoretical guarantees and practical deployment in highly dynamic real-world scenarios, further integrating human oversight, value alignment, and explainability into the design of autonomous exploration systems to ensure their beneficial and responsible societal impact.\n"
  },
  "subsections": {
    "Robust Intrinsic Rewards: Addressing the Noisy TV Problem": "\\subsection{Robust Intrinsic Rewards: Addressing the Noisy TV Problem}\nThe quest for effective exploration in reinforcement learning (RL) is profoundly challenged by environments offering sparse extrinsic rewards. While intrinsic motivation methods emerged as a promising avenue to generate internal curiosity signals and alleviate this sparsity, early prediction-error approaches frequently succumbed to the \"noisy TV problem.\" This phenomenon describes scenarios where agents are perpetually drawn to unpredictable yet uninformative stochastic elements within the environment, such as random pixel noise on a screen or flickering lights. These elements generate consistently high prediction errors, leading to spurious curiosity that distracts the agent from truly novel and learnable aspects of the environment, thereby hindering focused exploration.\n\nInitial efforts, such as those by \\cite{stadie20158af}, explored incentivizing exploration through bonuses derived from concurrently learned deep predictive models of system dynamics. This aimed to reward agents for encountering states that challenged their current environmental understanding. A prominent example of this paradigm is the Intrinsic Curiosity Module (ICM) \\cite{Pathak2017}. ICM trains a forward dynamics model to predict the next state's features given the current state's features and the executed action. The magnitude of this prediction error then serves as an intrinsic reward, encouraging the agent to visit states where its internal model is inaccurate. While ICM provided a scalable solution for high-dimensional visual inputs by operating in a learned feature space, its reliance on predicting *future states* made it inherently susceptible to the noisy TV problem. In environments with uninformative stochasticity, the forward dynamics model would consistently fail to predict these truly random, irreducible changes, resulting in persistently high prediction errors. This spurious curiosity would then cause the agent to repeatedly visit these uninformative areas, diverting computational resources and hindering progress towards task-relevant exploration.\n\nTo overcome the critical limitations posed by the noisy TV problem, \\cite{Burda2018} introduced Random Network Distillation (RND), a pivotal innovation in robust intrinsic reward generation. RND fundamentally redefines novelty detection by decoupling the intrinsic reward from the learnability of the environment's true dynamics. Instead of predicting future states, RND employs two neural networks: a fixed, randomly initialized target network ($f$) and a predictor network ($\\hat{f}$). Both networks receive the current state $s_t$ as input. The predictor network is trained to predict the output of the target network, i.e., $\\hat{f}(s_t) \\approx f(s_t)$. The intrinsic reward is then defined as the mean squared error between the outputs of these two networks: $r_i = ||\\hat{f}(s_t) - f(s_t)||^2$.\n\nThe key insight of RND lies in its design: for any given state $s_t$, even one containing uninformative stochasticity (like a noisy TV screen), the randomly initialized target network $f(s_t)$ produces a *fixed and deterministic* output embedding. The predictor network $\\hat{f}(s_t)$ is then trained to learn this fixed mapping. If an agent repeatedly visits a state $s_t$, the predictor $\\hat{f}$ will eventually learn to accurately map $s_t$ to $f(s_t)$, causing the prediction error and thus the intrinsic reward to *decay*. This mechanism effectively filters out irrelevant stochasticity because the reward is based on the *novelty of the state itself* (i.e., how well the predictor has learned to map that specific state to its fixed target), rather than the unpredictability of state transitions. Consequently, even a noisy TV state, despite its visual randomness, yields a fixed target output from $f$. Once the agent has sufficiently explored this state, $\\hat{f}$ learns the mapping, and the curiosity bonus diminishes, preventing perpetual attraction. This contrasts sharply with ICM, where the prediction error for a truly stochastic transition remains irreducible, leading to persistent curiosity.\n\nRND's robust and simpler mechanism for novelty detection proved highly effective, leading to significantly more efficient and directed exploration in complex visual domains, such as Atari games, where it substantially outperformed prior methods on hard exploration tasks \\cite{Burda2018}. Its success underscored the importance of designing intrinsic reward signals that are resilient to environmental noise and focus on aspects of the environment truly conducive to learning. Subsequent works, such as \\cite{li2019tj1}, further explored simplified variants of curiosity modules, demonstrating how architectural or methodological simplifications could enhance the practical utility and integration of prediction-error based intrinsic motivation, particularly for off-policy reinforcement learning methods.\n\nWhile RND provided a powerful solution to the noisy TV problem, it is not the sole robust intrinsic motivation strategy. Other approaches also aim to mitigate the effects of uninformative stochasticity or enhance exploration in complementary ways. For instance, methods based on model disagreement or ensembles, which reward exploration in states where multiple learned dynamics models disagree, can implicitly discount uncontrollable noise by focusing on areas where the agent's *learnable* understanding is inconsistent. Information-theoretic perspectives, as surveyed by \\cite{aubret2022inh}, often emphasize maximizing *useful* information gain, which aligns with RND's implicit discounting of unlearnable noise. Furthermore, diversity-driven exploration strategies, which incentivize agents to visit states that are distinct from previously encountered ones \\cite{hong20182pr}, or Random Latent Exploration (RLE), which encourages agents to pursue randomly sampled goals in a latent space \\cite{mahankali20248dx}, offer alternative mechanisms for robust and deep exploration without relying solely on prediction error. Deep Curiosity Search (DeepCS) \\cite{stanton20183fs} also introduced the concept of \"intra-life novelty,\" rewarding exploration within a single episode, which can complement RND's \"across-training novelty\" by encouraging immediate discovery.\n\nDespite these significant advancements, challenges persist. While RND effectively addresses the noisy TV problem by focusing on learnable novelty, the intrinsic reward signal can still be somewhat undirected, potentially leading to exploration of areas that are novel but not necessarily relevant to the extrinsic task. Future research continues to explore how to imbue intrinsic rewards with more task-relevant directionality, perhaps through goal-conditioned or hierarchical approaches, or how to combine them with other exploration strategies to achieve even more efficient and purposeful exploration in increasingly complex and open-ended environments. The integration of RND into advanced agents like Agent57 \\cite{Badia2020Agent57} further demonstrates its lasting impact as a foundational component for achieving state-of-the-art performance in diverse and challenging domains.",
    "Hierarchical Reinforcement Learning for Exploration": "\\subsection{Hierarchical Reinforcement Learning for Exploration}\n\nEffective exploration is a critical bottleneck in reinforcement learning (RL), particularly in complex environments characterized by vast state-action spaces, sparse rewards, and long task horizons. Hierarchical Reinforcement Learning (HRL) offers a powerful and principled framework to address these challenges by decomposing large, intractable problems into a hierarchy of more manageable sub-problems. This decomposition allows agents to learn and explore at different levels of temporal abstraction, significantly enhancing exploration efficiency and robustness.\n\nThe foundational concept underpinning HRL for exploration is the \"Options Framework\" introduced by \\cite{Sutton1999}. Building upon earlier notions of skill chaining by \\cite{Singh1995}, the Options Framework formalizes \"options\" as temporally extended actions, or sub-policies, that execute for multiple time steps. An option consists of an initiation set (states where it can be taken), a policy (how to act while the option is active), and a termination condition (when the option ends). This framework allows a high-level policy to choose among options, while a low-level policy executes the primitive actions within a chosen option. This mechanism fundamentally aids exploration by enabling agents to traverse large portions of the state space more efficiently than with primitive actions alone. Instead of exploring individual steps, the agent explores sequences of actions (options), effectively reducing the effective search space and allowing for more directed movement towards relevant subgoals or novel regions. For instance, an agent might learn an \"open door\" option, which, once selected, reliably executes the necessary primitive actions to open a door, allowing the high-level policy to explore the consequences of being on the other side of the door, rather than stumbling upon the correct sequence of door-opening actions by chance.\n\nIn the era of deep reinforcement learning, various architectures have been proposed to implement hierarchical control and leverage its benefits for exploration. FeUdal Networks (FuN) \\cite{Vezhnevets2017} introduced a two-level hierarchy with a \"Manager\" module that sets goals in a latent space and a \"Worker\" module that executes primitive actions to achieve those goals. The Manager explores the space of goals, while the Worker explores the primitive action space conditioned on the current goal. This explicit goal-setting mechanism intrinsically guides exploration towards achieving meaningful sub-objectives. Similarly, Hierarchical Reinforcement Learning with Off-policy Correction (HIRO) \\cite{Nachum2018} enables efficient learning of both high-level and low-level policies by correcting for off-policy data, allowing the high-level policy to explore by setting goals in the state space, and the low-level policy to learn how to reach those goals. Hierarchical Actor-Critic (HAC) \\cite{Levy2019} further refines this by using multiple layers of actor-critic agents, where higher levels set goals for lower levels, and a \"hindsight experience replay\" mechanism allows agents to learn from failed attempts to reach goals, thereby improving exploration by making better use of suboptimal trajectories. These deep HRL methods demonstrate how learning goal-conditioned policies at different levels of abstraction can significantly accelerate exploration in complex, high-dimensional environments.\n\nA critical aspect of HRL for exploration is the autonomous discovery of useful options or skills, often driven by intrinsic motivation. Rather than manually defining options, agents can learn them through self-supervision. Methods like Diversity is All You Need (DIAYN) \\cite{Eysenbach2018} and Variational Option Discovery (VALOR) \\cite{Gregor2017} learn a diverse set of skills by maximizing the mutual information between the skill executed and the resulting state trajectory, effectively rewarding the agent for discovering distinct behaviors. These learned skills then serve as valuable options for a higher-level policy, enabling more structured and efficient exploration. The survey by \\cite{aubret2022inh} highlights how information-theoretic intrinsic motivation, particularly novelty and surprise, can assist in building a hierarchy of transferable skills, making the exploration process more robust. Furthermore, \\cite{janjua2024yhk} emphasizes unsupervised skill acquisition as a key advancement for enhancing scalability in open-ended environments, where HRL provides a natural framework for organizing these learned behaviors.\n\nHRL also facilitates temporally coordinated and goal-conditioned exploration. \\cite{zhang2022p0b} proposed Generative Planning Method (GPM), which generates multi-step action plans, effectively acting as temporally extended options. These plans guide exploration towards high-value regions more consistently than single-step perturbations, and the plan generator can adapt to the task, further benefiting future explorations. This aligns with HRL's ability to create intentional action sequences for reaching specific subgoals. Similarly, Random Latent Exploration (RLE) \\cite{mahankali20248dx}, while not strictly HRL, encourages exploration by pursuing randomly sampled goals in a latent space. This goal-conditioned exploration paradigm is inherently compatible with HRL, where the high-level policy can sample latent goals for the low-level policy to achieve, fostering diverse and deep exploration.\n\nThe utility of hierarchical exploration extends significantly to multi-agent systems, where coordination and efficient search are paramount. \\cite{hu2020qwm} designed a cooperative exploration strategy for multiple mobile robots using a hierarchical control architecture, where a high-level decision-making layer coordinates exploration to minimize redundancy, and a low-level layer handles target tracking and collision avoidance. This demonstrates how HRL can structure complex multi-robot behaviors for efficient, coordinated exploration. More recently, \\cite{yu20213c1} tackled cooperative visual exploration for multiple agents with a Multi-agent Spatial Planner (MSP) leveraging a transformer-based architecture with hierarchical spatial self-attentions, enabling agents to capture spatial relations and plan cooperatively based on visual signals. \\cite{liu2024xkk} further advances multi-agent exploration with \"Imagine, Initialize, and Explore\" (IIE), which uses a transformer to imagine critical states and then initializes agents at these states for targeted exploration. This approach, while not explicitly called HRL, embodies hierarchical decomposition by first identifying high-level critical states (subgoals) and then focusing low-level exploration from those points, enhancing the discovery of successful joint action sequences in long-horizon tasks.\n\nIn summary, HRL provides a powerful framework for managing the complexity of exploration in large state-action spaces. By enabling agents to learn and compose temporally extended actions (options) or to decompose complex tasks into sub-problems, HRL significantly reduces the dimensionality of the exploration problem. This structured approach allows agents to focus on achieving meaningful subgoals, leading to more directed and sample-efficient discovery of optimal policies in long-horizon tasks. Despite these advancements, challenges persist, particularly in the autonomous discovery of optimal and diverse skill sets, the robust learning of high-level policies that effectively coordinate lower-level skills, and ensuring seamless communication and transfer of information across hierarchical levels. Future research will likely focus on developing more adaptive and autonomous methods for skill acquisition and composition, further integrating HRL with robust intrinsic motivation and meta-learning to create agents capable of truly open-ended and efficient exploration.",
    "Learning Exploration Policies (Meta-Exploration)": "\\subsection{Learning Exploration Policies (Meta-Exploration)}\n\nA pivotal advancement in reinforcement learning (RL) exploration shifts the paradigm from relying on hand-crafted heuristics or fixed intrinsic reward functions to enabling agents to autonomously learn their own exploration strategies. This sophisticated approach, termed meta-exploration, involves an outer-loop optimization process that trains a meta-controller or a recurrent policy to generate adaptive exploration behaviors, aiming to maximize long-term returns across a distribution of tasks or episodes. By learning \"how to explore,\" these methods can dynamically adjust the exploration-exploitation trade-off, leading to more efficient, task-relevant discovery and robust performance, particularly in novel and complex environments \\cite{duan2016rll, wang2016learning}. This represents a significant stride towards autonomous and intelligent exploration, where agents generalize their exploration capabilities rather than relearning them from scratch for each new task.\n\nThe foundational concept of meta-exploration was significantly advanced by works demonstrating that recurrent neural networks (RNNs) can serve as meta-learners. \\cite{duan2016rll} introduced RL$^2$ (Reinforcement Learning to Reinforce Learn), a seminal framework where an RNN-based agent is trained to solve a distribution of tasks. The RNN's hidden state effectively encodes task-specific information and past experience, allowing it to learn an exploration strategy that adapts within a single episode and across multiple episodes of a new, unseen task. This enables the agent to exhibit rapid adaptation and efficient exploration behaviors, such as directed search or uncertainty-driven probing, without explicit hand-engineered exploration bonuses. Similarly, \\cite{wang2016learning} explored the idea of \"Learning to Reinforce Learn,\" where an RNN acts as a meta-learner to discover an entire RL algorithm, including its exploration component, by processing sequences of observations, actions, and rewards. These approaches highlight the power of recurrent architectures to implicitly capture and execute sophisticated exploration policies that generalize across related tasks.\n\nBuilding on these foundations, subsequent research has focused on learning more structured and informed exploration strategies. \\cite{gupta2018rge} proposed Model Agnostic Exploration with Structured Noise (MAESN), a gradient-based meta-learning algorithm that learns exploration strategies from prior experience. MAESN leverages prior tasks to initialize a policy and acquire a latent exploration space, which injects structured stochasticity into the policy. This allows for exploration strategies that are informed by previous knowledge, moving beyond simple action-space noise and proving more effective than task-agnostic methods. This work underscores the benefit of meta-learning not just the policy, but the *mechanism* of exploration itself, enabling more targeted and efficient discovery.\n\nMeta-learning has also been applied to the generation and refinement of intrinsic motivation signals for exploration. \\cite{zhang2020xq9} introduced MetaCURE (Meta Reinforcement Learning with Empowerment-Driven Exploration), which explicitly models an exploration policy learning problem separate from the exploitation policy. MetaCURE employs a novel empowerment-driven exploration objective that aims to maximize information gain for task identification, deriving a corresponding intrinsic reward. By learning separate, context-aware exploration and exploitation policies and sharing task inference knowledge, MetaCURE significantly enhances exploration efficiency in sparse-reward meta-RL tasks. This demonstrates how meta-learning can discover effective intrinsic reward functions that guide exploration towards truly informative experiences, addressing a key challenge in intrinsic motivation.\n\nFurthermore, meta-exploration has been integrated with other advanced RL paradigms. \\cite{rimon20243o6} presented MAMBA, a model-based approach to meta-RL that leverages world models for efficient exploration. By learning an internal model of the environment, MAMBA can plan and explore more effectively, leading to greater return and significantly improved sample efficiency (up to 15x) compared to existing meta-RL algorithms, especially in higher-dimensional domains. This highlights the synergy between learning environmental models and meta-learning exploration strategies. Complementing this, \\cite{goldie2024cuf} introduced Learned Optimization for Plasticity, Exploration and Non-stationarity (OPEN), which meta-learns an update rule (an optimizer) whose input features and output structure are informed by solutions to common RL difficulties, including exploration. OPEN's parameterization is flexible enough to use stochasticity for exploration, demonstrating that meta-learning can discover effective policy update mechanisms that inherently promote efficient exploration.\n\nIt is crucial to distinguish meta-exploration from merely adaptive exploration, where exploration parameters are tuned within a single learning process. While methods that dynamically adjust exploration probabilities based on metrics like information entropy \\cite{hu2020yhq} or ensemble learning for balancing exploration-exploitation ratios \\cite{shuai2025fq3} are valuable, they typically do not involve an outer-loop meta-training process to learn a generalizable exploration *strategy* across tasks. Such adaptive approaches are better categorized under integrated and adaptive exploration frameworks (Section 5.3), which focus on dynamic parameter adjustment rather than learning the exploration algorithm itself.\n\nIn conclusion, the shift towards learning exploration policies through meta-exploration represents a profound step towards truly autonomous and intelligent agents. These approaches, ranging from recurrent policies learning exploration behaviors across episodes to meta-learning structured exploration noise, intrinsic motivation signals, or even entire optimization rules, empower agents to generalize their exploration capabilities and achieve robust performance across diverse problem settings. However, significant challenges persist, including the computational expense of meta-training, the difficulty of defining appropriate and diverse task distributions for meta-learning, ensuring the learned exploration strategies generalize to truly novel and out-of-distribution tasks, and designing effective meta-objectives that capture desirable exploration properties. Future research will likely focus on developing more robust, sample-efficient, and generalizable meta-learning algorithms that can discover truly novel and effective exploration strategies across a wide spectrum of tasks, pushing the boundaries of autonomous discovery.",
    "Integrated and Adaptive Exploration Frameworks": "\\subsection{Integrated and Adaptive Exploration Frameworks}\n\nEffective exploration in complex, high-dimensional environments often transcends the capabilities of a single, static strategy, necessitating frameworks that dynamically combine multiple exploration techniques and adaptively select or blend them based on the current task, state, or learning progress. Moving beyond a 'one-size-fits-all' approach, these integrated frameworks aim to create highly versatile and robust agents capable of tackling a wide spectrum of exploration challenges, representing a key direction for developing general-purpose reinforcement learning (RL) agents. Robust intrinsic motivation, as discussed in Section 4, frequently serves as a foundational component within these integrated systems, providing internal reward signals (e.g., RND-style novelty \\cite{Burda2018} or episodic novelty \\cite{NGU}) that are then managed or orchestrated by higher-level adaptive mechanisms.\n\nA prominent approach to achieving adaptive exploration involves the use of meta-controllers that explicitly orchestrate a portfolio of exploration-exploitation policies. The seminal work of \\cite{Badia2020} on Agent57 exemplifies this paradigm, achieving state-of-the-art performance across diverse Atari games. Agent57 integrates multiple intrinsic motivation signals, including Random Network Distillation (RND) for life-long novelty and value-discrepancy-based novelty, with an adaptive exploration strategy managed by a meta-controller. This meta-controller dynamically selects from a range of exploration-exploitation policies, allowing the agent to adjust its behavior on the fly to suit the specific demands of each game and phase of learning. The strength of this approach lies in its ability to explicitly learn *how* to explore by selecting appropriate behaviors from a predefined set, offering significant flexibility. However, a limitation is its reliance on a hand-designed portfolio of base policies and the complexity of meta-learning the controller, which can be computationally intensive and may struggle if the optimal strategy is not represented within the initial portfolio.\n\nBeyond explicit meta-controllers, other integrated frameworks leverage ensemble methods or decoupled architectures to achieve robust and adaptive exploration. Ensemble-based approaches, for instance, integrate multiple policies or value functions to capture uncertainty or promote diversity. \\cite{jiang2023qmw} introduced Exploration via Distributional Ensemble (EDE), a method that encourages exploration of states with high epistemic uncertainty through an ensemble of Q-value distributions. This integration of multiple distributional estimates allows for a more nuanced understanding of uncertainty, leading to improved generalization in unseen environments. Similarly, \\cite{yang2022mx5} proposed Ensemble Proximal Policy Optimization (EPPO), which learns ensemble policies in an end-to-end manner, combining individual policies and the ensemble organically. EPPO adopts a diversity enhancement regularization over the policy space, which theoretically increases exploration efficacy and promotes generalization. In a different vein, \\cite{whitney2021xlu} proposed Decoupled Exploration and Exploitation Policies (DEEP), which structurally separates the task policy from the exploration policy. This decoupling allows for directed exploration to be highly effective for sample-efficient continuous control without incurring performance penalties in densely-rewarding environments. In contrast to Agent57's explicit switching mechanism, these ensemble and decoupled architectures offer a more implicit form of adaptation, either through aggregation of diverse perspectives or a clear separation of learning objectives, providing robustness but potentially less fine-grained, task-specific adaptation.\n\nFurther advancements in adaptive exploration leverage probabilistic, Bayesian, and reactive mechanisms to guide behavior based on uncertainty or environmental shifts. Bayesian approaches provide a principled framework for managing uncertainty, which can be directly used to drive exploration. \\cite{fu20220cl} introduced a model-based lifelong RL approach that estimates a hierarchical Bayesian posterior, which, combined with a sample-based Bayesian exploration procedure, adaptively increases sample efficiency across related tasks. Extending this, \\cite{li2023kgk} explored Bayesian exploration with Implicit Posteriori Parameter Distribution Optimization (IPPDO), modeling parameter uncertainty with an implicit distribution approximated by generative models, offering greater flexibility and improved sample efficiency. In the realm of randomized exploration, \\cite{ishfaq20235fo} and \\cite{ishfaq20245to} developed scalable Thompson sampling strategies using Langevin Monte Carlo and approximate sampling, respectively. These methods directly sample the Q-function from its posterior distribution, providing provably efficient and adaptive exploration by inherently balancing uncertainty and reward. For dynamic environments, \\cite{steinparz20220nl} proposed Reactive Exploration, designed to track and react to continual domain shifts in lifelong reinforcement learning, demonstrating adaptive policy updates in non-stationary settings. These probabilistic and reactive methods offer strong theoretical grounding for continuous adaptation but often face computational challenges in high-dimensional settings, requiring efficient approximation techniques.\n\nEmerging trends also point towards exploration as an emergent property from learned optimizers or large pre-trained models. \\cite{goldie2024cuf} introduced Learned Optimization for Plasticity, Exploration and Non-stationarity (OPEN), a meta-learned update rule whose input features and output structure are informed by solutions to RL difficulties, including the ability to use stochasticity for exploration. This represents a shift towards learning the optimization process itself, which implicitly includes adaptive exploration. In the context of large foundation models, \\cite{lee202337c} demonstrated that Decision-Pretrained Transformers (DPT) can exhibit emergent online exploration capabilities in-context, without explicit training for it. Building on this, \\cite{dai2024x3l} proposed In-context Exploration-Exploitation (ICEE), which optimizes the efficiency of in-context policy learning by performing exploration-exploitation trade-offs at inference time within a Transformer model. These approaches suggest a future where exploration is less explicitly engineered and more implicitly learned or emergent, potentially leading to highly generalizable agents, but raise questions about control, interpretability, and the sample efficiency required for pre-training.\n\nIn conclusion, integrated and adaptive exploration frameworks represent a significant leap towards developing general-purpose RL agents. By combining robust intrinsic motivation signals, explicit meta-controllers, ensemble and decoupled architectures, principled Bayesian and probabilistic methods, and leveraging emergent capabilities from learned optimizers and large models, these frameworks move beyond static heuristics to dynamically adjust their exploration behaviors. While significant progress has been made, future directions include developing more theoretically grounded guarantees for these complex adaptive systems, enhancing their computational efficiency and scalability, and ensuring robust generalization to truly novel and open-ended environments where the optimal exploration strategy might not be easily predefined or learned from limited prior experience. A key unresolved challenge is the automated discovery of an optimal portfolio of exploration strategies for meta-controllers, as current approaches often rely on hand-designed components.",
    "Population-Based and Evolutionary Exploration": "\\subsection{Population-Based and Evolutionary Exploration}\n\nThe inherent challenge of the exploration-exploitation dilemma in Reinforcement Learning (RL) often leads single agents to converge prematurely to sub-optimal policies, particularly in environments characterized by sparse rewards or complex, multi-modal reward landscapes. To overcome these limitations, a distinct paradigm has emerged that leverages populations of agents or evolutionary algorithms to foster broader, more robust exploration and facilitate global search. These approaches represent a meta-level solution, structuring the learning system itself for enhanced discovery.\n\nEarly precursors to population-based exploration can be found in Neuroevolution, where neural network architectures and weights are optimized using evolutionary algorithms. Methods like NEAT (NeuroEvolution of Augmenting Topologies) \\cite{stanley2002evolving} demonstrated the power of evolving diverse populations of networks to solve complex control tasks, implicitly performing exploration by searching a vast hypothesis space. More recently, Evolution Strategies (ES) have gained prominence as a scalable black-box optimization technique for deep RL, capable of training deep neural networks efficiently due to their high parallelizability \\cite{salimans2017evolution}. However, standard ES can struggle with sparse or deceptive reward landscapes, necessitating directed exploration. To address this, \\cite{conti2017cr2} introduced methods like Novelty Search with Evolution Strategies (NS-ES) and Quality Diversity (QD) algorithms hybridized with ES. These approaches maintain a population of novelty-seeking agents, rewarding exploration of novel behaviors rather than just high performance, thereby enabling ES to avoid local optima and achieve higher performance on challenging deep RL tasks like Atari games and simulated robot locomotion.\n\nA significant advancement in population-based methods for deep RL is Population Based Training (PBT) \\cite{jaderberg2017population}. PBT concurrently trains a population of agents, each with its own set of hyperparameters and model weights. Unlike traditional grid search or random search, PBT dynamically adapts hyperparameters during training by periodically evaluating agents, exploiting well-performing ones (copying their weights and hyperparameters) and exploring new hyperparameter configurations for underperforming ones. This asynchronous \"exploit-and-explore\" strategy allows PBT to discover robust hyperparameter schedules and model weights simultaneously, leading to faster training and improved final performance across diverse tasks, including complex deep RL benchmarks. PBT's strength lies in its ability to adaptively tune both learning processes and agent policies, making it highly effective for complex, high-dimensional problems.\n\nBuilding on the strengths of both evolutionary algorithms and gradient-based RL, Evolutionary Reinforcement Learning (ERL) frameworks have emerged as a powerful hybrid paradigm. These methods typically combine the global search capabilities of evolutionary algorithms with the local optimization efficiency of gradient-based RL. Early ERL approaches, such as those by \\cite{khadka2018evolutionary} and CEM-RL \\cite{pourchot2019cemrl}, often involve evolving a population of actor networks, while a shared critic network (trained via gradient descent) provides value estimates to guide both the evolutionary process and individual actor updates. This hybrid approach aims to leverage the exploration benefits of evolution (e.g., escaping local optima, maintaining diversity) and the sample efficiency of RL. For instance, \\cite{sun2024edc} proposed a modified ERL method for multi-agent region protection, amalgamating Differential Evolution (DE) for diverse sample exploration and overcoming sparse rewards with Multi-Agent Deep Deterministic Policy Gradient (MADDPG) for training defenders and expediting DE convergence.\n\nA more recent advancement in this line is the Two-Stage Evolutionary Reinforcement Learning (TERL) framework proposed by \\cite{zhu2024sb0}. TERL addresses a key limitation of prior ERL methods, which often evolve only actor networks, thereby constraining exploration if a single critic network falls into local optima. Instead, TERL maintains and optimizes a population of *complete RL agents*, each comprising both an actor and a critic network. This design enables more independent and diverse exploration by each individual, mitigating the risk of premature convergence to suboptimal policies dictated by a flawed shared critic. The TERL framework operates through a novel two-stage learning process: an initial \"Exploration Stage\" where all individuals learn independently, optimized by a hybrid approach combining gradient-based RL updates with meta-optimization techniques like Particle Swarm Optimization (PSO). This stage emphasizes diversification, with agents sharing information efficiently through a common replay buffer, which helps propagate beneficial experiences across the population. Following this, the \"Exploitation Stage\" focuses on refining the best-performing individual from the population through concentrated RL-based updates, while the remaining individuals continue to undergo PSO to further diversify the replay buffer. This dynamic allocation of computational resources and tailored optimization strategies across stages allows TERL to effectively balance the exploration-exploitation dilemma.\n\nDespite their advantages, population-based and evolutionary methods introduce their own set of challenges. Computational cost is a primary concern, as maintaining and training multiple agents or evolving large populations can be resource-intensive, although parallelization strategies (like those in ES and PBT) mitigate this. Furthermore, the integration of diverse data from population optimization into off-policy RL algorithms, particularly through shared replay buffers, can introduce instability and even degrade performance, as highlighted by \\cite{zheng2023u9k}. This issue arises because population data, while diverse, might not align with the on-policy distribution expected by some RL algorithms, leading to an \"overlooked error.\" To remedy this, \\cite{zheng2023u9k} proposed a double replay buffer design to provide more on-policy data, demonstrating the need for careful architectural considerations when combining these paradigms. The choice between PBT's hyperparameter evolution and ERL's policy evolution also presents a trade-off: PBT excels at finding robust training configurations, while ERL directly optimizes policy parameters, often leading to more direct policy improvement.\n\nIn conclusion, population-based and evolutionary exploration methods offer a compelling meta-level solution to the challenges of exploration in complex RL environments. By evolving populations of complete RL agents, dynamically adapting hyperparameters, or employing hybrid optimization strategies, these approaches enable more diverse learning trajectories and a more robust search for optimal policies, moving beyond the limitations of single-agent exploration heuristics. Future research could explore more sophisticated mechanisms for inter-agent information sharing, investigate adaptive intrinsic motivation signals within these population-based frameworks, or extend these concepts to multi-task and open-ended learning scenarios, further enhancing the adaptability and generalization capabilities of RL agents.",
    "Exploration in Offline Reinforcement Learning": "\\subsection*{Exploration in Offline Reinforcement Learning}\n\nIn offline Reinforcement Learning (RL), the agent is tasked with learning an optimal policy solely from a fixed, pre-collected dataset, fundamentally precluding any further active interaction with the environment. This paradigm shift introduces unique challenges for exploration, as traditional active exploration strategies, which involve generating new experiences, are inherently impossible. Instead, the core problem transforms into managing *distributional shift* and preventing the policy from querying actions that are out-of-distribution (OOD) relative to the dataset. The focus shifts from active exploration to ensuring 'conservative learning' within the boundaries of the existing data distribution, aiming to identify reliable regions for policy improvement while rigorously avoiding OOD actions that could lead to unreliable value estimates or unsafe behaviors due to extrapolation errors. This conservative approach is paramount for real-world applications where data collection is costly, risky, or simply not feasible during the learning process.\n\nThe foundational approaches to offline RL primarily address the distributional shift problem through two main mechanisms: policy constraints and value function pessimism. Seminal works like Batch-Constrained Q-Learning (BCQ) \\cite{fujimoto2019off} introduced explicit policy constraints, regularizing the learned policy to stay close to the behavior policy that generated the dataset. This is typically achieved by adding a regularization term (e.g., KL-divergence) to the policy objective, ensuring that the agent does not venture into unobserved state-action pairs. Similarly, Behavior Regularized Actor Critic (BRAC) \\cite{wu2019behavior} further explored various forms of behavior regularization to mitigate the distributional shift. While effective in keeping the policy close to the data, these methods can sometimes limit the discovery of truly optimal policies if the behavior policy was suboptimal.\n\nA complementary and highly influential approach is Conservative Q-Learning (CQL) \\cite{kumar2020conservative}. CQL tackles the distributional shift by explicitly enforcing pessimism in the value function estimation. It achieves this by adding a penalty to the Q-values of OOD actions, ensuring that the learned Q-function provides a lower bound on the true Q-values. This prevents overestimation of action values for unseen actions, which is a common failure mode in offline RL. While BCQ primarily constrains the policy directly, CQL intervenes at the value function level, indirectly shaping the policy by making OOD actions less attractive. This distinction highlights different points of intervention for ensuring conservatism.\n\nBuilding upon these foundations, subsequent methods have leveraged uncertainty estimation to guide conservative learning more explicitly. The Uncertainty Weighted Actor-Critic (UWAC) \\cite{wu2021r67} explicitly incorporates uncertainty treatment by detecting OOD state-action pairs and down-weighting their contribution in the training objectives. Utilizing a practical dropout-based uncertainty estimation, UWAC laid groundwork for robust learning by mitigating the impact of unreliable data points. Similarly, \\cite{rezaeifar20211eu} conceptualized offline RL as \"anti-exploration,\" proposing to subtract a prediction-based exploration bonus from the reward. This innovative approach encourages the policy to remain within the support of the dataset by penalizing actions whose consequences cannot be reliably predicted, effectively extending pessimism-based offline RL methods to deep learning settings. Further, \\cite{zhang20244ty} introduced an Entropy-regularized Diffusion Policy with Q-Ensembles, where robust policy improvement is achieved by learning the lower confidence bound of Q-ensembles. This implicitly accounts for uncertainty in value estimates, mitigating the impact of inaccurate value functions from OOD data, while an entropy regularizer improves \"exploration\" within the offline dataset by encouraging diverse actions within the observed distribution.\n\nSimpler yet effective mechanisms have also emerged to ensure in-sample learning. \\cite{ma2024jej} proposed Improving Offline Reinforcement Learning with in-Sample Advantage Regularization (ISAR), which adapts offline RL to robotic manipulation with minimal changes. ISAR learns the state-value function exclusively from dataset samples, then calculates the advantage function based on this in-sample estimation and adds a behavior cloning regularization term. This method effectively mitigates the impact of unseen actions without introducing complex hyperparameters, offering a straightforward approach to conservative learning that implicitly handles OOD issues.\n\nA significant challenge, particularly in model-based offline RL (MBRL), is addressing biased exploration during the synthetic trajectory generation phase. Standard maximum entropy exploration mechanisms, often adopted from online RL, can lead to skewed data distributions and impaired performance when applied to learned dynamics models. To tackle this, \\cite{wu2024mak} introduced OCEAN-MBRL (Offline Conservative ExplorAtioN for Model-Based Offline Reinforcement Learning), a novel plug-in rollout approach. OCEAN explicitly decouples exploration from exploitation, introducing a principled, conservative exploration strategy guided by an ensemble of dynamics models for uncertainty estimation. It employs three key constraints: a state evaluation constraint to explore only in low-uncertainty regions, an exploration range constraint to select conservative actions, and a trajectory truncation constraint to limit rollouts in high-uncertainty areas. This comprehensive approach significantly enhances the stability and performance of existing MBRL algorithms by ensuring that exploration within the learned model remains reliable and does not generate new, potentially unsafe, out-of-distribution experiences.\n\nThe transition from offline learning to online fine-tuning presents another critical juncture for conservative exploration. While initial conservatism is vital for stable offline learning, a purely pessimistic policy might fail to discover better actions during online interaction. The Simple Unified uNcertainty-Guided (SUNG) framework for offline-to-online RL \\cite{guo20233sd} quantifies uncertainty via a VAE-based state-action visitation density estimator. SUNG's adaptive exploitation method applies conservative offline RL objectives to high-uncertainty samples and standard online RL objectives to low-uncertainty samples, demonstrating a sophisticated use of uncertainty to dynamically adjust the degree of conservatism. Building on this, \\cite{guo2024sba} proposed Optimistic Exploration and Meta Adaptation (OEMA) for sample-efficient offline-to-online RL. OEMA employs an optimistic exploration strategy, adhering to the principle of optimism in the face of uncertainty, allowing agents to sufficiently explore while reducing distributional shift through meta-learning. Providing a theoretical underpinning for such adaptive strategies, \\cite{hu2024085} showed that Bayesian design principles are crucial for offline-to-online fine-tuning, suggesting that a probability-matching agent, rather than purely optimistic or pessimistic ones, can avoid sudden performance drops while being guaranteed to find the optimal policy.\n\nThe literature on exploration in offline RL has thus evolved from foundational methods that strictly constrain policies or penalize OOD value estimates to sophisticated, explicit conservative exploration strategies, particularly in model-based settings and during the offline-to-online transition. While significant progress has been made in ensuring learning remains robust and effective without active interaction, a persistent challenge lies in the accuracy, reliability, and computational efficiency of uncertainty estimation itself. Future research directions could focus on developing more robust and scalable uncertainty quantification methods, as well as adaptive mechanisms that dynamically adjust conservative exploration constraints based on the evolving confidence in the learned policy and model, effectively balancing the need for safety with the potential for performance improvement.",
    "Expert-Guided and Demonstration-Based Exploration": "\\subsection{Expert-Guided and Demonstration-Based Exploration}\nEfficient exploration remains a formidable challenge in reinforcement learning (RL), particularly in complex, high-dimensional, and sparse-reward environments. To mitigate this, a significant body of research focuses on leveraging expert knowledge or demonstrations to guide and accelerate the exploration process, thereby improving sample efficiency and policy robustness. This paradigm, often termed Reinforcement Learning from Demonstrations (RLfD), seeks to bridge the gap between purely autonomous trial-and-error and the wealth of human or simulated expertise.\n\nEarly efforts established the foundational utility of demonstrations in overcoming exploration hurdles. \\cite{nair2017crs} demonstrated that even a small set of expert demonstrations, when integrated with RL algorithms like Deep Deterministic Policy Gradients (DDPG) and Hindsight Experience Replay (HER), could provide an order of magnitude speedup in learning complex, continuous control robotics tasks with sparse rewards. These methods often relied on static, \\textit{offline} datasets of expert trajectories, which provided initial guidance but presented inherent limitations. Building upon this, \\cite{uchendu20221h1} proposed Jump-Start Reinforcement Learning (JSRL), which uses a \"guide-policy\" derived from offline data or demonstrations to form a curriculum of starting states for an \"exploration-policy,\" significantly improving sample complexity, especially in data-scarce regimes. Similarly, \\cite{hansen2022jm2} highlighted key ingredients for accelerating visual model-based RL with demonstrations, including policy pretraining, targeted exploration, and oversampling demonstration data, leading to substantial performance gains in sparse reward tasks.\n\nBeyond direct trajectory imitation, expert knowledge can also be integrated through symbolic rules or domain-specific insights. For instance, \\cite{hou2021c2r} introduced Rule-Aware Reinforcement Learning (RARL) for knowledge graph reasoning, injecting high-quality symbolic rules into the model's reasoning process to alleviate sparse rewards and prevent spurious paths. \\cite{mazumder2022deb} proposed using \"state-action permissibility\" knowledge to guide exploration, drastically speeding up deep RL training by identifying and avoiding impermissible actions. In a similar vein, \\cite{liu20228r4} combined human knowledge-based rule bases with imitation learning pre-training (ILDN) and safe RL to enhance efficiency and generalization in large-scale adversarial scenarios. Furthermore, theoretical frameworks have explored how exploration itself can be framed as a utility to be optimized, which demonstrations can implicitly help achieve \\cite{zhang2020o5t, santi2024hct}.\n\nDespite the benefits of offline demonstrations, a persistent challenge is the \"distribution gap\" \\cite{coelho2024oa6}. This occurs when the agent's policy deviates from the expert's, leading it into states not covered by the static demonstration dataset, thereby hindering generalization and robustness. To address this, research has shifted towards more dynamic and interactive integration of expert knowledge. \\cite{ball20235zm} showed that with minimal modifications, existing off-policy RL algorithms could effectively leverage offline data (including demonstrations) for online learning, achieving significant performance improvements. This offline-to-online fine-tuning paradigm is crucial for real-world applications \\cite{rafailov2024wtw, hu2024085}, where policies pre-trained on demonstrations need to adapt to novel online experiences. For example, \\cite{lu2025j7f} presented VLA-RL, an approach that uses online reinforcement learning to improve pretrained Vision-Language-Action (VLA) models, specifically addressing out-of-distribution failures that arise from limited offline data in robotic manipulation. Even leveraging \"negative demonstrations\" or failed experiences can guide exploration by showing what \\textit{not} to do, as demonstrated by \\cite{wu20248f9} in sparse reward environments.\n\nA significant advancement in bridging the distribution gap and enhancing exploration efficiency comes from dynamically interacting with experts. \\cite{hou20248b2} introduced EARLY, an active RL from demonstrations algorithm where the agent intelligently queries for episodic demonstrations based on its trajectory-level uncertainty. This approach makes expert guidance more targeted and resource-efficient by requesting help only when needed. The pinnacle of this dynamic interaction is exemplified by \\cite{coelho2024oa6} with RLfOLD (Reinforcement Learning from Online Demonstrations) for urban autonomous driving. RLfOLD introduces the novel concept of \\textit{online demonstrations}, which are dynamically collected from a simulator's privileged information during the agent's active exploration. These demonstrations are seamlessly integrated into a single replay buffer alongside agent experiences, directly addressing the distribution gap by providing contextually relevant expert guidance. The framework utilizes a modified Soft Actor-Critic (SAC) algorithm with a dual standard deviation policy network, outputting distinct $\\sigma_{RL}$ for exploration and $\\sigma_{IL}$ for imitation, allowing for adaptive balancing of these learning objectives. Furthermore, an uncertainty-based mechanism selectively invokes the online expert to guide the agent in challenging situations, making exploration more targeted and efficient. RLfOLD demonstrated superior performance in the CARLA NoCrash benchmark with significantly fewer resources, highlighting its effectiveness and efficiency in complex, real-time domains.\n\nIn conclusion, the intellectual trajectory of expert-guided exploration has evolved from static, offline demonstration datasets to dynamic, interactive, and online expert guidance. This progression effectively addresses critical challenges such as the distribution gap and sample inefficiency, particularly in safety-critical applications like autonomous driving and robotics. While methods leveraging offline demonstrations provide crucial initial boosts, the trend towards online and active expert interaction, exemplified by frameworks like RLfOLD, represents a significant step towards more robust, adaptive, and generalizable RL systems. Future research will likely focus on refining the mechanisms for generating and integrating online expert guidance, especially in real-world scenarios where \"privileged information\" is unavailable, and developing more sophisticated expert models that can provide nuanced and context-aware interventions.",
    "Exploration in Dynamic and Expanding Environments": "\\subsection*{Exploration in Dynamic and Expanding Environments}\n\nThe challenge of efficient exploration intensifies significantly in Reinforcement Learning (RL) when agents must operate in environments where the state and action spaces are not static but continually expand or evolve \\cite{yang2021ngm}. Unlike traditional Markov Decision Processes (MDPs) that assume fixed state and action sets, real-world systems often undergo updates, introducing novel states, actions, or even entire sub-environments. This dynamic nature necessitates exploration strategies that can efficiently discover and integrate new information without incurring computationally prohibitive retraining costs or suffering from catastrophic forgetting of previously acquired knowledge. This specialized context forms a crucial subset of lifelong and continual learning, where agents must adapt to an unending stream of tasks or environmental changes \\cite{janjua2024yhk, fu20220cl}.\n\nThe concept of Incremental Reinforcement Learning (Incremental RL) has emerged to specifically address this challenge, focusing on how agents can efficiently adapt their policies to newly introduced states and actions. While lifelong learning broadly concerns sequential task learning and knowledge transfer \\cite{fu20220cl, woczyk20220mn}, Incremental RL distinguishes itself by tackling the explicit *expansion* of the underlying MDP structure. A seminal contribution by \\cite{ding2023whs} formally defines Incremental RL and proposes the Dual-Adaptive $\\epsilon$-greedy Exploration (DAE) algorithm. This approach confronts the inherent inefficiency of standard exploration methods and the strong inductive biases that can arise from extensive prior learning, which often hinder adaptation to expanding environments. DAE employs a Meta Policy ($\\Psi$) to adaptively determine a state-dependent $\\epsilon$ by assessing the exploration convergence of a state (via TD-Error rate), thereby deciding *when* to explore. Concurrently, an Explorer ($\\Phi$) guides the agent to prioritize \"least-tried\" actions by estimating their relative frequencies, addressing *what* to explore. Crucially, DAE also incorporates strategies for incrementally adapting deep Q-networks by reusing trained policies and intelligently initializing new neurons and Q-values. This architectural flexibility, combined with the dual-adaptive exploration mechanism, significantly reduces training overhead and enables robust adaptation to expanding search spaces without retraining from scratch.\n\nThe need for adaptive exploration in dynamic settings is also highlighted by research into non-stationary environments, which share some conceptual overlaps with expanding environments, though they differ mechanistically. For instance, \\cite{steinparz20220nl} introduces Reactive Exploration to cope with continual domain shifts in lifelong reinforcement learning. This work demonstrates that policy-gradient methods benefit from strategies that track and react to non-stationarities, such as changes in reward functions or environmental dynamics, within an otherwise fixed state-action space. While Reactive Exploration focuses on adapting to *changes* in existing elements, DAE specifically addresses the *addition* of new states and actions. However, both underscore the broader necessity for exploration strategies that can actively adapt to environmental changes, rather than relying on static or pre-defined exploration schedules. Similarly, the importance of exploration for generalization to new, unseen environments, as explored by \\cite{jiang2023qmw}, aligns with the goals of Incremental RL. Their Exploration via Distributional Ensemble (EDE) method encourages exploration of states with high epistemic uncertainty, which is crucial for acquiring knowledge that aids decision-making in novel situations. While EDE aims to generalize within a potentially vast but fixed environment, DAE's focus is on efficiently integrating entirely new components into the agent's operational space, a distinction critical for truly open-ended learning systems \\cite{janjua2024yhk}.\n\nOther advanced exploration techniques, such as those leveraging intrinsic motivation \\cite{houthooft2016yee} or information gain maximization \\cite{aubret2022inh}, aim to improve exploration efficiency by incentivizing agents to visit novel states or reduce uncertainty about the environment dynamics. For example, Variational Information Maximizing Exploration (VIME) \\cite{houthooft2016yee} uses Bayesian neural networks to maximize information gain about environment dynamics. While powerful in static high-dimensional environments, their direct applicability and scalability to *continually expanding* state and action spaces present unique challenges. Prediction-error-based methods, like those underlying many intrinsic motivation approaches, may struggle when the underlying dynamics model requires continuous architectural restructuring rather than just parameter updates. The sudden introduction of entirely new states or actions can render existing prediction models inaccurate or incomplete, requiring significant re-learning or architectural modifications that are not inherently handled by these methods. Count-based or density-based novelty methods, while effective for discovering unvisited regions, would need robust mechanisms to distinguish truly *new* states/actions from merely *unvisited* ones within the previously known space, and to efficiently update their density estimations for an ever-growing space. DAE's explicit focus on identifying and prioritizing newly available actions and states, overcoming the inductive bias from extensive prior learning, offers a more targeted solution to these architectural and knowledge-transfer challenges.\n\nThe integration of such adaptive exploration strategies with broader lifelong learning frameworks is a critical direction. Lifelong RL methods, such as model-based Bayesian approaches that estimate a hierarchical posterior to distill common task structures \\cite{fu20220cl}, offer mechanisms for backward transfer and efficient learning across related tasks. However, these often assume a fixed set of potential tasks or a stable underlying model structure. The challenge of Incremental RL lies in the dynamic *growth* of this structure, requiring not just transfer but also efficient architectural expansion and robust exploration of truly novel elements. Learned optimization methods, which meta-learn update rules to handle non-stationarity, plasticity loss, and exploration \\cite{goldie2024cuf}, offer a promising avenue by building adaptability directly into the learning process, potentially complementing DAE's specific exploration mechanisms.\n\nIn conclusion, the progression towards \"Exploration in Dynamic and Expanding Environments\" marks a crucial intellectual shift in Reinforcement Learning, moving beyond the static MDP assumption towards more realistic, evolving systems. While foundational exploration methods provide general tools, the work on Incremental RL, particularly the Dual-Adaptive $\\epsilon$-greedy Exploration \\cite{ding2023whs}, offers a targeted solution for environments where state and action spaces continually grow. Future research in this area will likely focus on extending these adaptive exploration strategies to more complex, partially observable, or even multi-agent expanding environments, further enhancing the lifelong learning capabilities of RL agents in truly dynamic real-world scenarios, and integrating them more deeply with meta-learning and continual learning paradigms to address catastrophic forgetting and efficient knowledge transfer in ever-growing systems.",
    "Safety-Aware Exploration": "\\subsection{Safety-Aware Exploration}\nThe exploration phase in reinforcement learning (RL) is critical for discovering optimal policies, yet in real-world, safety-critical applications, unconstrained exploration can lead to catastrophic outcomes and raise significant ethical concerns regarding accountability, fairness in decision-making under risk, and the potential for unforeseen negative side-effects in human-inhabited environments. This subsection delves into methods designed to ensure safety during exploration, navigating the inherent tension between the need for aggressive exploration to achieve optimal performance and the imperative to maintain safe operation. The problem is often formally cast within the framework of Constrained Markov Decision Processes (CMDPs), where an agent aims to maximize cumulative reward while simultaneously satisfying constraints on cumulative costs, such as safety violations \\cite{altman1999constrained}. This formalism provides a robust theoretical foundation for developing algorithms that provide safety guarantees during the learning process.\n\nEarly efforts to integrate safety into RL exploration focused on establishing explicit boundaries and constraints. A foundational approach involves \"safety layers\" or \"shielding,\" which act as guardians, restricting the agent's actions or states to predefined safe regions, thereby preventing the agent from entering hazardous situations during learning \\cite{Stachurski2008}. While early works laid the groundwork, modern deep RL has seen significant advancements, notably with methods like those proposed by \\cite{alshiekh2018safe}, which formally integrate shielding into deep RL agents. These methods enforce explicit safety constraints, ensuring exploration is guided within a permissible operational envelope, effectively mitigating the risk of catastrophic failures. However, a key limitation of static safety layers is their potential to be overly conservative, which can severely restrict the agent's exploration capabilities and prevent the discovery of truly optimal, yet initially unknown, safe behaviors. This conservativeness often stems from the difficulty of accurately predefining safe regions in complex, high-dimensional environments, or from worst-case assumptions made to guarantee safety.\n\nAddressing the limitations of static constraints and the inherent conservativeness, more recent research has explored dynamic and learned safety mechanisms, often decoupling the concerns of task performance and safety. \\cite{yu20222xi} introduced SEditor, a two-policy approach that learns a \"safety editor policy\" to transform potentially unsafe actions proposed by a utility-maximizing policy into safe ones. SEditor represents a conceptual shift from static, predefined shields to a learned, dynamic safety filter, allowing for more nuanced and state-dependent safety interventions. This method moves beyond simplified safety models, enabling the safety editor to learn complex safety functions, effectively acting as a dynamic shield. While SEditor demonstrates significantly lower constraint violation rates and maintains high utility, its effectiveness relies on the ability to train an accurate safety editor policy, which can be challenging in highly dynamic or unpredictable environments.\n\nFurther advancing dynamic safety, \\cite{thananjeyan2020d20} introduced Recovery RL, which first leverages offline data to learn about constraint-violating zones. It then employs a task policy for reward maximization and a dedicated recovery policy that activates to guide the agent back to safety when constraint violation is likely. This dual-policy structure allows for more aggressive exploration by the task policy, relying on the learned recovery mechanism to prevent unsafe outcomes. Unlike SEditor, which modifies actions *before* execution, Recovery RL focuses on *recovering* from potentially unsafe trajectories, offering a different trade-off between proactive prevention and reactive correction. Recovery RL demonstrates superior efficiency in balancing task success and constraint adherence in complex, contact-rich manipulation tasks and image-based navigation, even on physical robots, by allowing the task policy greater freedom. Similarly, \\cite{zhang2023wqi} proposed a method for safe RL with dead-ends avoidance and recovery. This approach constructs a boundary to discriminate between safe and unsafe states, equivalent to distinguishing dead-end states, thereby ensuring maximum safe exploration with minimal limitation. Like Recovery RL, it utilizes a decoupled framework with a task policy and a pre-trained recovery policy, along with a safety critic, to ensure safe actions during online training. This strategy aims to achieve better task performance with fewer safety violations by carefully delineating the extent of guaranteed safe exploration, offering a more precise definition of \"safe\" exploration boundaries.\n\nAnother powerful paradigm for guaranteeing safety, particularly in continuous control systems, draws from control theory: Lyapunov stability and Control Barrier Functions (CBFs). These methods provide formal guarantees that a system's state will remain within a predefined safe set. Control Barrier Functions (CBFs) are functions of the state that define a safe set and whose derivatives can be constrained to render this set forward-invariant, thus preventing the agent from leaving it. \\cite{zhang2022dgg} proposed a Barrier Lyapunov Function-based safe RL (BLF-SRL) algorithm for autonomous vehicles. This approach integrates BLF items into an optimized backstepping control method, constraining state variables within a designed safety region during learning. By decomposing optimal control with BLF items, it achieves safe exploration while learning adaptive uncertain items, ensuring both safety and performance optimization in safety-critical domains. While control-theoretic methods like BLF-SRL offer strong, often deterministic, safety guarantees, they typically require an accurate model of the system dynamics and specific assumptions about the environment, a limitation not shared by model-free, data-driven approaches like Recovery RL, which in turn provide only probabilistic safety assurances. Addressing this model dependency, \\cite{cheng20224w2} presented a method using Disturbance-Observer-Based Control Barrier Functions (DOB-CBFs). This approach avoids explicit model learning by leveraging disturbance observers to accurately estimate the pointwise value of uncertainty, which is then incorporated into a robust CBF condition. This allows for less conservative safety filters, especially during early learning, by effectively handling unknown disturbances without requiring extensive model training.\n\nThe inherent uncertainty in exploration necessitates risk-sensitive approaches. Building on Bayesian exploration principles (as discussed in Section 3.2), some approaches use uncertainty to define credible intervals for constraint satisfaction, leading to more principled conservative exploration. \\cite{zhang2024ppn} introduced Lag-U, an uncertainty-augmented Lagrangian safe RL algorithm for autonomous driving. This method uses deep ensembles to estimate epistemic uncertainty, which is then used to encourage exploration and learn a risk-sensitive policy by adaptively modifying safety constraints. Furthermore, it incorporates an intervention assurance mechanism based on quantified uncertainty to select safer actions during deployment. This allows for a better trade-off between efficiency and risk avoidance, preventing overly conservative policies by making safety decisions based on the agent's confidence in its predictions. Complementing this, \\cite{yu2022bo5} proposed a distributional reachability certificate (DRC) to address model uncertainties and characterize robust persistently safe states. Their framework builds a shield policy based on DRC to minimize constraint violations, especially during training, by considering the distribution of potential long-term constraint violations, thereby enhancing safety robustness against model uncertainty. Beyond explicit uncertainty estimation, some methods directly manipulate the learning process to balance reward and safety. \\cite{gu2024fu3} addressed the conflict between reward and safety gradients, proposing a soft switching policy optimization method. By analyzing and manipulating these gradients, their framework aims to achieve a better balance between optimizing for rewards and adhering to safety constraints, offering a more direct way to mitigate the inherent conflict compared to simply adding penalty terms in CMDPs.\n\nIn scenarios where some safety signals are available in a controlled environment, \"guided safe exploration\" can be employed to facilitate safe transfer learning. \\cite{yang2023n56} proposed a method where a \"guide\" agent learns to explore safely without external rewards in a controlled environment where safety signals are available. This guide then helps compose a safe behavior policy for a \"student\" agent in a target task where safety violations are prohibited. The student policy is regularized towards the guide while it is unreliable, gradually reducing the guide's influence as it learns. This approach enables safe transfer learning and faster problem-solving in the target task, highlighting the utility of leveraging prior safety knowledge to bootstrap safe exploration in novel, safety-critical settings.\n\nIn summary, safety-aware exploration has evolved significantly, moving from static, predefined safety layers and formal constraint satisfaction (CMDPs) to dynamic, learned recovery mechanisms, control-theoretic guarantees (BLFs, CBFs), and risk-sensitive approaches leveraging uncertainty quantification. The field continues to grapple with the fundamental tension between encouraging sufficient exploration for optimal learning and guaranteeing safety in real-world, safety-critical applications. Future directions will likely involve integrating more sophisticated formal verification techniques, developing robust and scalable uncertainty quantification for proactive safety prediction, designing intrinsically safe exploration strategies that adhere to ethical guidelines from the outset, and further refining gradient manipulation techniques to optimally balance conflicting objectives. The goal remains to enable autonomous agents to learn effectively without compromising human or environmental safety, ensuring responsible deployment in increasingly complex and sensitive domains.",
    "Summary of Key Advancements": "\\subsection*{Summary of Key Advancements}\n\nThe journey of exploration in reinforcement learning (RL) reflects a continuous and sophisticated evolution, driven by the persistent challenge of balancing the acquisition of new information with the exploitation of known optimal actions. This review has traced a narrative arc from foundational heuristics to theoretically grounded algorithms, and subsequently to scalable intrinsic motivation techniques, culminating in adaptive, learned exploration strategies tailored for specialized contexts. This progression underscores the field's relentless innovation in addressing the increasing complexity of environments and the inherent difficulties of the exploration-exploitation dilemma.\n\nEarly attempts to navigate the exploration-exploitation trade-off, as discussed in Section 2, centered on basic heuristics like $\\epsilon$-greedy and the implicit exploration benefits of model-based planning architectures such as Dyna-Q. Concurrently, explicit exploration bonuses, often count-based, provided direct incentives for visiting novel states. While effective in tabular settings, these methods faced significant scalability challenges in high-dimensional or continuous state spaces. For instance, early attempts to generalize count-based exploration to deep RL, such as mapping states to hash codes \\cite{tang20166wr}, provided a surprisingly strong baseline but highlighted the need for more robust, scalable novelty detection mechanisms.\n\nA pivotal shift occurred with the development of theoretically grounded exploration strategies (Section 3), which sought to provide provable guarantees on learning efficiency. Principles like \"optimism in the face of uncertainty\" (OFU), embodied in algorithms like R-Max, offered PAC-MDP guarantees by optimistically valuing unexplored regions. Bayesian approaches, notably Thompson Sampling, provided a principled framework for managing uncertainty by maintaining posterior distributions over models or value functions. Further advancing this theoretical rigor, information-theoretic methods emerged, guiding exploration by maximizing knowledge gain about the environment or optimal policy. Techniques such as Variational Information Maximizing Exploration (VIME) \\cite{houthooft2016yee} and MaxInfoRL \\cite{sukhija2024zz8} exemplify this, rewarding agents for transitions that significantly reduce uncertainty or improve the agent's internal model. This information-centric view, as surveyed by \\cite{aubret2022inh}, moved beyond simple novelty to a more sophisticated understanding of learning progress. However, the computational complexity of maintaining accurate uncertainty estimates and the limitations of optimism in scenarios with partially observable rewards \\cite{parisi2024u3o} often restricted their applicability to simpler environments.\n\nThe advent of deep reinforcement learning necessitated a paradigm shift, leading to the widespread adoption of intrinsic motivation techniques (Section 4). These methods generate internal reward signals to drive exploration, proving crucial in environments with sparse external rewards. Initial concepts of curiosity and novelty-seeking evolved into scalable approaches. Count-based methods were adapted for high-dimensional spaces using pseudo-counts and density models. A significant breakthrough came with prediction-error based curiosity, where agents are rewarded for encountering surprising or unpredictable observations, as seen in the Intrinsic Curiosity Module (ICM) \\cite{li2019tj1, zhelo2018wi8}. This directed exploration towards aspects of the environment that improve the agent's internal dynamics model. To address the \"noisy TV\" problem, where agents are perpetually attracted to uninformative stochasticity, robust intrinsic reward mechanisms like Random Network Distillation (RND) were developed, effectively filtering out irrelevant noise and focusing exploration on truly learnable novelty. The introduction of Random Latent Exploration (RLE) \\cite{mahankali20248dx} further simplified this, offering deep exploration benefits by pursuing randomly sampled goals in a latent space without complex bonus calculations.\n\nThe field has continued to push towards more advanced and adaptive exploration strategies (Section 5). Hierarchical Reinforcement Learning (HRL) enabled structured exploration by decomposing tasks into sub-problems, allowing agents to explore at different levels of temporal abstraction. A particularly impactful direction is meta-learning for exploration, where agents learn *how* to explore effectively across diverse tasks. Algorithms like MAESN \\cite{gupta2018rge} demonstrate this by learning structured exploration strategies and latent exploration spaces from prior experience, injecting informed stochasticity into policies and outperforming task-agnostic methods. Furthermore, the development of decoupled exploration and exploitation policies (DEEP) \\cite{whitney2021xlu} highlighted that separating these concerns can significantly boost sample efficiency, particularly in sparse reward settings. Integrated frameworks, such as Agent57, combine multiple techniques like RND, episodic memory, and adaptive meta-controllers to achieve state-of-the-art performance across a wide range of challenging environments. Diversity-driven exploration strategies \\cite{hong20182pr} also contribute to preventing policies from being trapped in local optima by encouraging varied behaviors. Population-based and evolutionary methods offer a meta-level solution, leveraging multiple agents or meta-optimization to achieve more robust and global exploration in complex reward landscapes.\n\nFinally, the application of these sophisticated exploration methods has expanded into specialized contexts (Section 6), demonstrating their versatility and practical impact. In offline reinforcement learning, where active exploration is impossible, the focus shifted to conservative exploration within fixed datasets, employing uncertainty estimation to avoid out-of-distribution actions \\cite{wu2021r67, zi20238ug}. Expert demonstrations and human feedback have been leveraged to guide exploration, significantly improving sample efficiency and overcoming sparse reward challenges in domains like robotics \\cite{nair2017crs, lee2021qzk}. Safety-aware exploration has become critical for real-world applications, incorporating constraints and recovery policies to prevent hazardous actions \\cite{thananjeyan2020d20}. The challenges of dynamic and open-ended environments, which demand continuous adaptation and robust discovery, are also being addressed \\cite{janjua2024yhk, matthews20241yx}. Emerging trends, such as the use of Decision-Pretrained Transformers (DPT) for in-context learning and adaptive exploration \\cite{lee202337c}, hint at a future where powerful foundation models might inherently possess sophisticated exploration capabilities.\n\nIn summary, the field has progressed from simple, often undirected, exploration heuristics to theoretically grounded methods, then to scalable intrinsic motivation for deep RL, and finally to highly adaptive, learned, and integrated strategies. This continuous innovation has enabled RL agents to achieve unprecedented performance and robustness in increasingly complex and diverse tasks, while also addressing critical concerns like safety and data efficiency. The trajectory reflects a deep commitment to overcoming the persistent challenges of the exploration-exploitation dilemma, paving the way for more intelligent and autonomous learning systems.",
    "Open Challenges and Theoretical Gaps": "\\subsection{Open Challenges and Theoretical Gaps}\nDespite significant advancements in reinforcement learning (RL) exploration, several fundamental challenges persist, particularly concerning scalability, robustness, sample efficiency, and the enduring gap between theoretical guarantees and practical applicability in complex, real-world settings. Addressing these issues is crucial for enabling RL agents to operate effectively in high-dimensional, stochastic, and open-ended environments.\n\nA primary challenge lies in scaling exploration strategies to extremely high-dimensional or continuous state-action spaces, where traditional methods struggle due to the curse of dimensionality. Early count-based exploration, while effective in tabular settings \\cite{Thrun1992}, quickly becomes infeasible. Efforts to bridge this gap include methods that generalize counts to high-dimensional spaces, such as using hash codes \\cite{Tang20166wr} or pseudo-counts derived from density models \\cite{Bellemare2016}. Concurrently, intrinsic motivation, often based on prediction error, emerged as a scalable heuristic. For instance, \\cite{Stadie20158af} utilized deep predictive models to generate exploration bonuses in Atari games, demonstrating the potential of learned models to guide exploration in complex visual domains. However, these prediction-error methods, like the Intrinsic Curiosity Module (ICM) \\cite{Pathak2017}, often suffer from the \"noisy TV problem,\" where uninformative stochasticity in the environment can generate spurious intrinsic rewards, leading to inefficient exploration. \\cite{Burda2018} addressed this by proposing Random Network Distillation (RND), which uses the prediction error of a fixed random network, proving more robust to environmental stochasticity and focusing exploration on learnable aspects of the environment. More recently, \\cite{Li2023kgk} proposed Implicit Posteriori Parameter Distribution Optimization (IPPDO) to improve exploration by modeling parameter uncertainty with implicit distributions, aiming for more flexible and efficient exploration in deep RL. Similarly, \\cite{Rahman2022p7b} introduced Robust Policy Optimization (RPO) to maintain high policy entropy throughout training, preventing premature convergence and ensuring sustained exploration in continuous action spaces.\n\nAnother persistent gap exists between methods offering strong theoretical guarantees and those providing practical scalability. Algorithms like R-Max \\cite{Strehl2008} provide provable bounds on sample complexity and regret, but their reliance on explicit model learning or tabular representations limits their application to simpler, finite Markov Decision Processes (MDPs). Bridging this gap requires developing principled yet adaptable solutions for real-world complexity. \\cite{Song2021elb} attempts this by proposing a computationally and statistically efficient model-based RL algorithm for specific model classes (Kernelized Nonlinear Regulators and linear MDPs) with polynomial sample complexity guarantees. In practical control applications, where accurate models are hard to obtain, methods like ModelPPO \\cite{Ma2024r2p} integrate neural network models into actor-critic architectures for AUV path following, demonstrating improved performance over traditional and model-free RL by learning state transition functions. For dynamic systems like microgrids, \\cite{Meng2025l1q} employs online RL with SARSA to adapt to uncertainties without relying on traditional mathematical models, prioritizing computational efficiency and real-time adaptability. Similarly, \\cite{Xi2024e2i} proposes a lightweight, adaptive SAC algorithm for UAV path planning, which adjusts exploration probability dynamically to balance efficiency and generalization in resource-constrained environments. These works highlight the ongoing tension between theoretical optimality and the need for practical, robust solutions in complex engineering domains.\n\nThe challenge of designing exploration strategies that are both sample-efficient and theoretically optimal across diverse tasks, especially in open-ended learning scenarios, remains largely unresolved. Meta-reinforcement learning (meta-RL) offers a promising avenue by learning exploration strategies from prior experience. \\cite{Gupta2018rge} introduced MAESN (Model Agnostic Exploration with Structured Noise) to learn structured exploration strategies, which are more effective than task-agnostic noise. \\cite{Zhang2020xq9} further developed MetaCURE, an empowerment-driven exploration method for meta-RL that maximizes information gain for task identification in sparse-reward settings. Leveraging offline data can also significantly boost sample efficiency. \\cite{Nair2017crs} demonstrated that incorporating demonstrations can overcome exploration difficulties in sparse-reward robotics tasks, while \\cite{Ball20235zm} showed how to effectively integrate offline data into online RL with minimal modifications. Offline RL itself faces challenges with out-of-distribution actions, which \\cite{Wu2021r67} addresses with Uncertainty Weighted Actor-Critic (UWAC) by down-weighting contributions from uncertain state-action pairs. More recent work explores in-context learning with large transformer models: \\cite{Lee202337c} introduced Decision-Pretrained Transformer (DPT), which can learn in-context exploration and conservatism from diverse datasets, generalizing to new tasks. Building on this, \\cite{Dai2024x3l} proposed In-context Exploration-Exploitation (ICEE) to optimize this trade-off at inference time, enhancing efficiency. The development of benchmarks like Craftax \\cite{Matthews20241yx} further underscores the current limitations of existing methods in achieving deep exploration, long-term planning, and continual adaptation required for truly open-ended learning. Cooperative exploration in multi-agent systems \\cite{Hu2020qwm, Yu20213c1} and autonomous navigation \\cite{Li2020r8r, Zhelo2018wi8, Kamalova2022jpm} also represent significant real-world complexities demanding robust and adaptable exploration.\n\nFinally, integrating safety constraints into exploration is a critical, yet often conflicting, requirement for real-world deployment. Extensive exploration, while necessary for learning, can lead to dangerous situations. \\cite{Thananjeyan2020d20} proposed Recovery RL, which decouples task and recovery policies and learns constraint-violating zones from offline data to safely navigate this trade-off. \\cite{Yu20222xi} introduced SEditor, a two-policy approach where a safety editor policy transforms potentially unsafe actions into safe ones, achieving extremely low constraint violation rates. \\cite{Zhang2023wqi} advanced safe RL by identifying and avoiding dead-end states, providing a minimal limitation on safe exploration. In reward-free settings, \\cite{Yang2023n56} proposed a \"guide\" agent to learn safe exploration which then regularizes a \"student\" policy. For deployable safe RL, \\cite{Honari202473t} developed Meta SAC-Lag, using meta-gradient optimization to automatically tune safety-related hyperparameters. Addressing model uncertainties for robust safety, \\cite{Yu2022bo5} introduced a distributional reachability certificate for safe model-based RL. Furthermore, \\cite{Zi20238ug} applied distributionally robust RL for active signal pattern localization, enabling safe exploration in unfamiliar environments with limited data. These efforts highlight the ongoing challenge of balancing the need for exploration with stringent safety requirements, often requiring complex architectural designs or meta-learning approaches.\n\nIn conclusion, while the field has made substantial progress from tabular, theoretically-grounded methods to scalable, deep learning-based intrinsic motivation, significant open challenges remain. The persistent gap between methods with strong theoretical guarantees (often for simpler settings) and those providing practical scalability (often heuristic-driven) underscores the critical need for principled yet adaptable solutions. Future research must focus on developing exploration strategies that are robust to environmental stochasticity, sample-efficient across diverse tasks, capable of deep exploration in open-ended environments, and inherently safe for real-world deployment, potentially through hybrid approaches that combine the strengths of model-based reasoning, intrinsic motivation, and meta-learning with strong theoretical foundations.",
    "Emerging Trends and Ethical Considerations": "\\subsection*{Emerging Trends and Ethical Considerations}\n\nThe frontier of reinforcement learning (RL) exploration is characterized by a dual pursuit: developing increasingly sophisticated agents capable of understanding and navigating complex, open-ended environments, and simultaneously ensuring these autonomous systems operate ethically and safely, particularly in human-interactive or safety-critical domains. This subsection explores cutting-edge research directions, including the transformative integration of large foundation models, the development of truly general-purpose and adaptive exploration agents, the increasing focus on learning better representations, and the critical ethical implications of deploying such intelligent systems.\n\nA pivotal emerging trend is the integration of **large foundation models (LFMs)**, such as Large Language Models (LLMs) and Vision-Language Models (VLMs), to imbue RL agents with more sophisticated world understanding, high-level planning capabilities, and common-sense priors. Traditional RL often struggles with extensive exploration in complex, semantically rich environments due to its limited grasp of underlying decision dynamics. LLMs, with their vast domain-specific knowledge, can serve as powerful prior action distributions, significantly reducing exploration and optimization complexity when integrated into RL frameworks through Bayesian inference methods \\cite{yan2024p3y}. This approach can decrease the number of required samples by over 90\\% in offline learning scenarios, demonstrating the immense potential of leveraging pre-trained knowledge to guide exploration. Furthermore, LLMs are being directly utilized to generate intrinsic curiosity signals. For instance, Curiosity-Driven Exploration (CDE) for LLMs in Reinforcement Learning with Verifiable Rewards (RLVR) leverages the model's own perplexity (from the actor) and the variance of value estimates (from the critic) as intrinsic bonuses \\cite{dai2025h8g}. This framework guides exploration by penalizing overconfident errors and promoting diversity, addressing issues like premature convergence and entropy collapse common in LLM-based RL. Beyond direct policy guidance, LLMs can act as adaptive search operators within meta-learning frameworks, where evolutionary search discovers improved algorithms, and RL fine-tunes the LLM policy based on these discoveries, accelerating algorithm design for complex combinatorial optimization tasks \\cite{surina2025smk}. These advancements highlight LFMs' capacity to elevate exploration from low-level state space coverage to high-level conceptual discovery and informed decision-making.\n\nComplementing the rise of LFMs, there is an increasing focus on **learning better representations** to facilitate more informed and efficient exploration. Robust representations are crucial for defining novelty, quantifying uncertainty, and building accurate world models in high-dimensional observation spaces. While earlier methods like the simplified Intrinsic Curiosity Module (S-ICM) \\cite{li2019tj1} and its predecessor ICM \\cite{pathak2017} leveraged prediction error in learned feature spaces to incentivize novelty, contemporary research pushes for more sophisticated self-supervised techniques that disentangle factors of variation and capture epistemic uncertainty. For example, the Actor-Model-Critic (AMC) architecture for Autonomous Underwater Vehicle (AUV) path-following explicitly learns the state transition function via a neural network, enabling the agent to anticipate environmental dynamics and guide exploration towards informative regions \\cite{ma2024r2p}. Beyond explicit model learning, methods like Exploration via Distributional Ensemble (EDE) emphasize the importance of exploration for generalization, not just optimal policy finding \\cite{jiang2023qmw}. EDE encourages exploration of states with high epistemic uncertainty using an ensemble of Q-value distributions, implicitly relying on robust representations to quantify this uncertainty effectively. Similarly, Decoupled Exploration and Exploitation Policies (DEEP) demonstrate that separating the task policy from the exploration policy can yield significant sample efficiency improvements in sparse environments, a benefit often amplified by representations that allow for meaningful novelty detection and uncertainty estimation \\cite{whitney2021xlu}. These approaches underscore that the quality of learned representations directly impacts an agent's ability to discern truly novel or uncertain aspects of an environment, leading to more directed and less wasteful exploration.\n\nThe drive towards **truly general-purpose exploration agents** capable of tackling open-ended problems is leading to more adaptive, robust, and scalable strategies. Rather than relying on fixed heuristics, recent work focuses on agents that can dynamically adjust their exploration behavior. Adaptive exploration strategies, such as those using multi-attribute decision-making based on information entropy and task decomposition, allow for more flexible and context-aware exploration \\cite{hu2020yhq}. Further advancing this, ensemble learning schemes with explicit \"exploration-to-exploitation (E2E) ratio control\" via multiple Q-learning agents and adaptive decay functions enable more nuanced balancing of exploration and exploitation, crucial for real-world applications requiring continuous adaptation \\cite{shuai2025fq3}. The scalability and theoretical guarantees of exploration are also paramount for such agents. Thompson sampling-based methods, employing Langevin Monte Carlo (LMC) and approximate sampling, offer provably efficient and scalable exploration in deep RL with theoretical regret bounds, ensuring reliability in autonomous systems \\cite{ishfaq20235fo, ishfaq20245to}. This extends to collaborative settings, where randomized exploration in cooperative multi-agent RL (MARL) with methods like CoopTS-PHE and CoopTS-LMC provides theoretical guarantees for regret bounds and communication complexity, essential for complex multi-agent environments \\cite{hsu2024tqd}. Moreover, simple yet effective strategies like Random Latent Exploration (RLE), which pursues randomly sampled goals in a latent space, demonstrate that deep exploration can be achieved without complex bonus calculations, promoting broader applicability as a general plug-in for existing RL algorithms \\cite{mahankali20248dx}. The concept of meta-learning how to explore is also gaining traction, with approaches like Learned Optimization for Plasticity, Exploration and Non-stationarity (OPEN) meta-learning update rules that incorporate stochasticity for exploration, showing strong generalization across diverse environments and agent architectures \\cite{goldie2024cuf}. These advancements, alongside broader discussions on open-ended RL emphasizing hierarchical learning, intrinsic motivation, and unsupervised skill acquisition \\cite{janjua2024yhk}, signify a shift towards agents that can autonomously learn and adapt their exploration strategies across a wide spectrum of tasks.\n\nAlongside these advancements in exploration capabilities, the **ethical considerations** surrounding autonomous exploration are gaining increasing prominence, especially in safety-critical or human-interactive environments. The inherent trial-and-error nature of RL exploration can lead to \"bad decisions\" that violate critical safety constraints, as highlighted in reviews of safe RL for power system control \\cite{yu2024x53}. This necessitates responsible development and deployment, emphasizing alignment with human values and safety standards. A direct response to this challenge is the \"human-in-the-loop deep reinforcement learning (HL-DRL)\" approach for optimal Volt/Var control in unbalanced distribution networks \\cite{sun2024kxq}. This method allows human intervention to modify dangerous actions during offline training and integrates human guidance into the actor network's loss function, ensuring the learned policy adheres to operational constraints and human safety guidelines. Broader advancements in RL for autonomous systems also explicitly identify safety, dependability, and explainability as critical constraints that limit wide adoption \\cite{malaiyappan20245bh}. The imperative is to develop exploration strategies that not only discover optimal behaviors but do so within predefined safe regions, learn to recover from unsafe situations, and provide transparent decision-making processes. This ensures that the learning process does not lead to catastrophic outcomes and adheres to ethical considerations, bridging the gap between autonomous learning and responsible societal impact.\n\nIn conclusion, the field is rapidly advancing towards more intelligent, adaptive, and generalizable exploration strategies. This progress is driven by the transformative potential of large foundation models for high-level understanding and goal generation, the continuous refinement of learned representations for informed low-level novelty detection and uncertainty quantification, and the development of meta-learning approaches for truly general-purpose agents. Simultaneously, the increasing power and autonomy of these systems amplify the imperative to address ethical implications, particularly in safety-critical domains. Future research must continue to bridge the gap between theoretical guarantees and practical deployment in highly dynamic real-world scenarios, further integrating human oversight, value alignment, and explainability into the design of autonomous exploration systems to ensure their beneficial and responsible societal impact."
  },
  "latex_document": "\\documentclass[12pt,a4paper]{article}\n    \\usepackage[utf8]{inputenc}\n    \\usepackage[T1]{fontenc}\n    \\usepackage{amsmath,amsfonts,amssymb}\n    \\usepackage{graphicx}\n    \\usepackage[margin=2.5cm]{geometry}\n    \\usepackage{setspace}\n    \\usepackage{natbib}\n    \\usepackage{url}\n    \\usepackage{hyperref}\n    \\usepackage{booktabs}\n    \\usepackage{longtable}\n    \\usepackage{array}\n    \\usepackage{multirow}\n    \\usepackage{wrapfig}\n    \\usepackage{float}\n    \\usepackage{colortbl}\n    \\usepackage{pdflscape}\n    \\usepackage{tabu}\n    \\usepackage{threeparttable}\n    \\usepackage{threeparttablex}\n    \\usepackage[normalem]{ulem}\n    \\usepackage{makecell}\n    \\usepackage{xcolor}\n\n    % Set line spacing\n    \\doublespacing\n\n    % Configure hyperref\n    \\hypersetup{\n        colorlinks=true,\n        linkcolor=blue,\n        filecolor=magenta,      \n        urlcolor=cyan,\n        citecolor=red,\n    }\n\n    % Title and author information\n    \\title{A Comprehensive Literature Review with Self-Reflection}\n    \\author{Literature Review}\n    \\date{\\today}\n\n    \\begin{document}\n\n    \\maketitle\n\n    % Abstract (optional)\n    \\begin{abstract}\n    This literature review provides a comprehensive analysis of recent research in the field. The review synthesizes findings from 240 research papers, identifying key themes, methodological approaches, and future research directions.\n    \\end{abstract}\n\n    \\newpage\n    \\tableofcontents\n    \\newpage\n\n    \\label{sec:introduction_to_exploration_in_reinforcement_learning}\n\n\\section{Introduction to Exploration in Reinforcement Learning}\n\\label{sec:introduction\\_to\\_exploration\\_in\\_reinforcement\\_learning}\n\n\\subsection{The Exploration-Exploitation Dilemma}\n\\label{sec:1\\_1\\_the\\_exploration-exploitation\\_dilemma}\n\nThe exploration-exploitation dilemma represents a foundational and ubiquitous challenge in Reinforcement Learning (RL), fundamentally shaping how an autonomous agent acquires knowledge and optimizes its behavior within an uncertain environment \\cite{sutton2018reinforcement}. At its core, this dilemma encapsulates the inherent tension between two conflicting objectives: an agent must judiciously decide whether to \\textit{exploit} its current understanding to select actions that are known to yield high immediate rewards, or to \\textit{explore} unknown actions and states, which, despite immediate uncertainty, may lead to the discovery of significantly greater long-term rewards \\cite{robbins1952some, sutton2018reinforcement}. This delicate balance is critical for the design of effective RL agents, as an imbalanced trade-off can profoundly impact learning efficiency and the ultimate optimality of the learned policy.\n\nTo formally illustrate this core dilemma, consider the classic multi-armed bandit (MAB) problem, a simplified yet powerful model for sequential decision-making under uncertainty \\cite{robbins1952some}. In a MAB setting, an agent faces $K$ distinct \"arms,\" each associated with an unknown probability distribution over rewards. At each time step $t$, the agent selects an arm $a\\_t \\in \\{1, \\dots, K\\}$ and observes a reward $r\\_t \\sim \\mathcal{D}\\_{a\\_t}$. The objective is to maximize the cumulative reward over a sequence of $T$ pulls, or equivalently, to minimize \"regret.\" Regret, formally defined as the difference between the expected cumulative reward of an optimal policy (always pulling the best arm) and the agent's actual cumulative reward, is given by $R\\_T = \\sum\\_{t=1}^T (\\mu^\\textit{ - \\mu\\_{a\\_t})$, where $\\mu^}$ is the expected reward of the optimal arm and $\\mu\\_{a\\_t}$ is the expected reward of the arm chosen at time $t$ \\cite{lai1985asymptotically}. Pulling an arm with a high estimated mean reward is an act of exploitation. Conversely, choosing an arm that has been sampled infrequently, or whose reward distribution is highly uncertain, constitutes exploration. Pure exploitation risks converging to a suboptimal arm if initial samples were misleading, while pure exploration, such as random arm selection, wastes opportunities to collect known rewards, leading to high regret \\cite{auer2002finite}. Modern approaches to MABs continue to refine this balance, often by maximizing information gain or balancing intrinsic and extrinsic rewards to achieve sublinear regret \\cite{sukhija2024zz8}.\n\nExtending from the simplified MAB framework to full Markov Decision Processes (MDPs), the exploration-exploitation dilemma becomes significantly more intricate. In MDPs, an agent's action not only yields an immediate reward but also transitions the agent to a new state, influencing future rewards. The state space can be high-dimensional or continuous, and the environmental dynamics are often unknown. This means that the value of an action is not independent but depends on the subsequent states it might lead to. Furthermore, rewards can be delayed, making it challenging to attribute positive or negative outcomes to specific exploratory actions taken much earlier in a sequence. Partial observability, where the agent does not have complete information about the true state of the environment, further exacerbates the challenge, as \"unknown\" can refer to truly unvisited states or merely unobserved aspects of the current state \\cite{parisi2024u3o}. These complexities necessitate more sophisticated, directed exploration strategies that move beyond simple random action selection.\n\nConceptual approaches to managing this dilemma in complex RL settings generally fall into several categories. One prominent principle is \"optimism in the face of uncertainty\" (OFU), where agents are incentivized to explore states or actions about which their knowledge is limited, by optimistically assuming these unknown options will yield maximal rewards \\cite{auer2002finite, brafman2002r}. This encourages the agent to gather sufficient data to reduce uncertainty, as exemplified by algorithms like UCB (Upper Confidence Bound) in MABs and R-Max in MDPs \\cite{auer2002finite, brafman2002r}. Another approach involves explicitly valuing the discovery of novel states or actions, often by assigning intrinsic rewards for visiting less-frequented regions of the state-action space. Furthermore, information-theoretic methods guide exploration by maximizing the expected reduction in uncertainty about the environment's dynamics or the optimal policy, thereby prioritizing experiences that yield the most significant knowledge gain \\cite{sukhija2024zz8}. These conceptual frameworks highlight the diverse ways researchers have sought to formalize and address the fundamental trade-off.\n\nThis subsection has established the exploration-exploitation dilemma as a central challenge in RL, defining its core tenets, illustrating it with the MAB problem, and extending its complexities to MDPs. It has also introduced foundational conceptual approaches to its management. The subsequent sections of this literature review will systematically delve into the diverse methodologies developed to tackle this challenge, tracing their evolution from foundational heuristic approaches and theoretically grounded algorithms to advanced intrinsic motivation techniques, adaptive strategies, and their specialized applications. Each method offers a unique perspective on how to navigate this central trade-off, collectively advancing the field towards more intelligent and autonomous learning agents.\n\\subsection{Historical Context and Evolution of Exploration Research}\n\\label{sec:1\\_2\\_historical\\_context\\_\\_and\\_\\_evolution\\_of\\_exploration\\_research}\n\nThe fundamental challenge of balancing exploration and exploitation, where an agent must gather sufficient information about its environment to make optimal decisions while simultaneously leveraging its current knowledge, has been a cornerstone of Reinforcement Learning (RL) since its inception \\cite{Sutton1998}. The historical trajectory of exploration research reflects a continuous effort to overcome the inherent complexities of unknown environments, evolving from rudimentary heuristics in simplified settings to sophisticated, scalable strategies for complex, high-dimensional domains. This evolution has been driven by both conceptual shifts and technological advancements, particularly the rise of deep learning.\n\nThe intellectual origins of principled exploration can be traced to the multi-armed bandit (MAB) problem, the simplest setting where the exploration-exploitation dilemma is starkly presented. In MABs, an agent chooses from a set of actions (arms) with unknown reward distributions, aiming to maximize cumulative reward. Foundational algorithms like Upper Confidence Bound (UCB) \\cite{Auer2002} emerged from the principle of \"optimism in the face of uncertainty\" (OFU), which encourages agents to explore actions whose true values are uncertain by optimistically assuming they might yield high rewards. Concurrently, Bayesian approaches, notably Thompson Sampling \\cite{Thompson1933}, provided a probabilistic framework for exploration by sampling from a posterior distribution over action values, effectively balancing exploration and exploitation by favoring actions with high potential given current uncertainty. These early MAB solutions laid the theoretical bedrock for later exploration strategies in full Markov Decision Processes (MDPs) \\cite{parisi2024u3o}.\n\nTransitioning to full MDPs, early RL exploration strategies were often heuristic. The $\\epsilon$-greedy policy, a direct extension of MAB ideas, randomly selects actions with a small probability ($\\epsilon$) to discover new state-action values, while otherwise exploiting current knowledge \\cite{Sutton1998}. While simple, its undirected nature proved inefficient in larger state spaces. To address this, early research in tabular settings introduced explicit exploration bonuses, often count-based, which incentivized agents to visit less-frequented states or take less-tried actions by augmenting the reward signal. These methods aimed for broader state space coverage, but their direct reliance on explicit state-action enumeration rendered them impractical for environments with continuous or very large discrete state spaces, foreshadowing the pervasive \"curse of dimensionality.\" The role of dynamic programming in this era was primarily to compute optimal policies \\textit{given} a known model, highlighting the critical need for effective exploration to \\textit{learn} such models in unknown environments.\n\nA significant conceptual shift emerged with the development of theoretically grounded exploration methods for finite MDPs, aiming to provide provable guarantees on learning efficiency. The OFU principle, originating from MABs, became a cornerstone, leading to algorithms like UCRL2 \\cite{Strehl2009}. Within the PAC-MDP (Probably Approximately Correct-MDP) framework, these methods provided provable bounds on the number of samples required to learn a near-optimal policy. While offering robust theoretical guarantees on sample complexity, these approaches were computationally demanding and inherently limited by their reliance on explicit state-action enumeration, making them largely inapplicable to the high-dimensional problems prevalent in modern RL. Efforts continued to refine these theoretical methods, with works like UCRL3 \\cite{bourel2020tnm} introducing tighter concentration inequalities and adaptive computation of transition supports to improve practical efficiency within the theoretical paradigm. However, the fundamental trade-off persisted: rigorous theoretical guarantees often came at the cost of scalability, necessitating a paradigm shift for complex, real-world domains.\n\nThe advent of deep learning provided a new impetus for exploration research, shifting the focus towards scalable solutions for complex, high-dimensional observation spaces where traditional counting or explicit model-learning became intractable. This era saw the emergence of intrinsic motivation, a paradigm where agents generate internal reward signals for novel or surprising experiences, independent of external task rewards. The challenge was to generalize the notion of \"visitation count\" or \"novelty\" to continuous, high-dimensional state spaces. Breakthroughs included the concept of pseudo-counts \\cite{Bellemare2016}, derived from density models, which allowed agents to quantify novelty in high-dimensional state spaces. Complementing this, \\cite{martin2017bgt} introduced $\\phi$-pseudocounts, generalizing state visit-counts by exploiting the same feature representation used for value function approximation, thereby rewarding exploration in feature space rather than the untransformed state space. Similarly, \\cite{Tang2016} demonstrated that even a simple generalization of classic count-based methods, using hash codes to count state occurrences, could achieve competitive performance in deep RL benchmarks, underscoring the power of novelty-seeking in complex environments.\n\nDespite their success, early intrinsic motivation methods faced challenges, such as the \"noisy TV problem,\" where agents might be perpetually distracted by uncontrollable stochastic elements that offer no meaningful learning. To address this, \\cite{Pathak2017} introduced the Intrinsic Curiosity Module (ICM), which generates intrinsic rewards based on the agent's prediction error of its own actions' consequences in a learned feature space, thereby focusing exploration on controllable and learnable aspects of the environment. Further refining this, \\cite{Burda2019} proposed Random Network Distillation (RND), a simpler and more robust intrinsic reward mechanism that measures prediction error between a policy network and a fixed, randomly initialized target network. RND proved less susceptible to environmental stochasticity, providing a more reliable signal for true novelty and significantly improving exploration stability. Simultaneously, efforts were made to bridge the gap between theoretical rigor and deep RL scalability. For instance, \\cite{nikolov20184g9} introduced Information-Directed Sampling (IDS) for deep Q-learning, providing a tractable approximation that explicitly accounts for both parametric uncertainty and heteroscedastic observation noise, further enhancing the theoretical grounding of exploration in deep RL.\n\nIn conclusion, the evolution of exploration research in RL reflects a continuous effort to overcome the inherent challenges of unknown environments. This journey moved from foundational theoretical guarantees in simplified settings like MABs, through heuristic and theoretically-grounded methods for tabular MDPs, and ultimately to practical, scalable, and increasingly robust solutions for complex, high-dimensional domains enabled by deep learning. This historical trajectory, marked by shifts from heuristic to theoretically grounded, and then to intrinsically motivated and uncertainty-aware deep learning approaches, sets the stage for the detailed methodological discussions of advanced and adaptive strategies that follow.\n\\subsection{Motivation for Effective Exploration}\n\\label{sec:1\\_3\\_motivation\\_for\\_effective\\_exploration}\n\nEffective exploration stands as a cornerstone for the successful application of Reinforcement Learning (RL) agents, particularly in the intricate and often unforgiving landscape of real-world environments. Its crucial role stems from the inherent challenges that frequently impede learning: the scarcity of informative reward signals, the vastness of high-dimensional state and action spaces, the prevalence of deceptive local optima that can trap agents in suboptimal behaviors, and the critical need for policies that generalize beyond training data while remaining sample-efficient. Without well-designed exploration strategies, RL agents risk converging to inferior policies, failing to discover optimal solutions, or even remaining inert in complex tasks, thereby underscoring the continuous drive for innovation in this research domain.\n\nOne of the primary motivations for robust exploration arises from the pervasive issue of \\textbf{sparse reward signals} and the \\textbf{curse of dimensionality}. In many practical scenarios, agents receive meaningful feedback only after achieving specific, often distant, goals. This sparsity makes naive trial-and-error exploration highly inefficient or even impossible. Early attempts to address this, such as model-based planning with Dyna-Q \\cite{Sutton90} and subsequent works \\cite{Kaelbling93,Singh2004}, aimed to improve sample efficiency by leveraging learned environmental models to generate synthetic experiences. Similarly, count-based methods \\cite{Thrun92} offered explicit incentives for visiting less-known states. However, these foundational approaches often struggled to scale to high-dimensional or continuous state spaces, where explicit state enumeration or precise model learning becomes intractable. This limitation fundamentally motivated the development of \\textbf{intrinsic motivation} techniques, which empower agents to generate their own internal reward signals, independent of external task rewards. Pioneering ideas of \"curiosity\" based on prediction error \\cite{Schmidhuber91,Schmidhuber97} and learning progress \\cite{Singh00} provided conceptual breakthroughs. These concepts were subsequently scaled to deep RL through methods like pseudo-counts for high-dimensional spaces \\cite{Bellemare16}, exploration bonuses derived from deep predictive models \\cite{stadie20158af}, and hash-based count methods \\cite{tang20166wr}. Such advancements have proven vital for tasks requiring extensive discovery in visually rich or complex environments, such as mapless navigation for mobile robots \\cite{zhelo2018wi8}.\n\nBeyond simply finding rewards, effective exploration is essential to overcome the \\textbf{peril of deceptive local optima}. Many environments present reward landscapes with numerous suboptimal peaks, where a greedy agent might become trapped, never discovering the globally optimal policy. This challenge necessitates exploration strategies that actively encourage agents to venture beyond seemingly good but ultimately inferior solutions. Information-theoretic approaches, such as Variational Information Maximizing Exploration (VIME) \\cite{Houthooft2016}, address this by guiding agents to states that maximize information gain about the environment's dynamics, thereby reducing uncertainty and facilitating escape from local traps. More recent intrinsic motivation methods, like the Intrinsic Curiosity Module (ICM) \\cite{Pathak17} and Random Network Distillation (RND) \\cite{Burda18}, provide robust novelty signals by rewarding prediction errors in learned feature spaces or against random targets. These methods are crucial for preventing agents from being perpetually attracted to uninformative stochastic elements (the \"noisy TV\" problem) that could otherwise lead to spurious curiosity and suboptimal convergence. Furthermore, approaches like diversity-driven exploration \\cite{hong20182pr} and novelty-seeking in evolutionary strategies \\cite{conti2017cr2} explicitly aim to prevent policies from being trapped in local optima by encouraging a wide range of behaviors and exploring diverse solution spaces. Robust policy optimization techniques, such as Robust Policy Optimization (RPO) \\cite{rahman2022p7b}, also contribute by maintaining sufficient policy entropy, ensuring continuous and broad exploration to avoid premature convergence.\n\nThe imperative for \\textbf{sample efficiency} and \\textbf{generalization} further underscores the critical need for sophisticated exploration. In real-world applications, data collection can be costly, time-consuming, or even risky, making inefficient exploration a significant bottleneck. Moreover, agents must often perform reliably in environments that differ subtly or significantly from their training conditions. This motivates exploration strategies that not only discover optimal policies quickly but also acquire knowledge transferable to unseen scenarios. For instance, \\cite{whitney2021xlu} highlights that simple policy entropy maximization is often insufficient for sample-efficient continuous control, advocating for decoupled exploration and exploitation policies. Leveraging existing data, such as expert demonstrations \\cite{nair2017crs} or large volumes of offline trajectories \\cite{ball20235zm}, can dramatically accelerate learning by guiding exploration towards promising regions of the state-action space, thus improving sample efficiency. The importance of exploration for \\textit{generalization} itself is a key motivation for methods like EDE (Exploration via Distributional Ensemble) \\cite{jiang2023qmw}, which encourages exploration of states with high epistemic uncertainty to acquire knowledge that aids decision-making in novel environments. Meta-learning exploration strategies \\cite{gupta2018rge} enable agents to learn \\textit{how} to explore effectively across a distribution of tasks, fostering rapid adaptation and generalization. Crucially, in safety-critical domains, exploration must be conducted within predefined safe boundaries or with learned recovery mechanisms, as explored by Recovery RL \\cite{thananjeyan2020d20}, ensuring that the pursuit of optimal behavior does not lead to catastrophic outcomes.\n\nIn conclusion, the motivation for effective exploration in Reinforcement Learning is deeply rooted in the fundamental challenges of the field. It is indispensable for navigating sparse reward landscapes, conquering high-dimensional complexities, escaping deceptive local optima, and achieving both sample efficiency and robust generalization in dynamic, real-world settings. The continuous evolution of exploration strategies, from basic heuristics to advanced intrinsic motivation, diversity-driven methods, and meta-learning approaches, reflects its non-negotiable status as a core component for unlocking the full potential of intelligent agents. Addressing these challenges drives ongoing research to develop more robust, theoretically grounded, and computationally efficient exploration methods that can seamlessly integrate with the demands of practical applications.\n\n\n\\label{sec:foundational_concepts_and_early_approaches_to_exploration}\n\n\\section{Foundational Concepts and Early Approaches to Exploration}\n\\label{sec:foundational\\_concepts\\_\\_and\\_\\_early\\_approaches\\_to\\_exploration}\n\n\\subsection{Basic Exploration Heuristics}\n\\label{sec:2\\_1\\_basic\\_exploration\\_heuristics}\n\nThe fundamental challenge of exploration in reinforcement learning (RL) lies in efficiently discovering optimal policies within an environment while simultaneously exploiting currently known good actions. The earliest and most straightforward attempts to address this exploration-exploitation dilemma centered around simple, yet widely adopted, heuristics, primarily the $\\epsilon$-greedy policy. This approach serves as a crucial baseline from which more sophisticated and targeted exploration strategies have evolved, highlighting the initial attempts to balance this fundamental trade-off.\n\nThe $\\epsilon$-greedy policy operates on a simple principle: with a small probability $\\epsilon$ (epsilon), the agent selects an action uniformly at random, thereby exploring the environment. With a higher probability of $1-\\epsilon$, the agent chooses the action that maximizes its current estimated value (the greedy action), thus exploiting its learned knowledge. This method's appeal lies in its conceptual simplicity and ease of implementation, making it a foundational component in many early RL algorithms \\cite{yang2021ngm, aubret2022inh}. It ensures that every action has a non-zero probability of being selected, preventing the agent from getting permanently stuck in suboptimal policies.\n\nDespite its widespread use, basic $\\epsilon$-greedy exploration suffers from several inherent limitations that have motivated the development of more advanced techniques. A primary drawback is its undirected nature \\cite{sukhija2024zz8}. The random actions taken during exploration are not guided by any sense of novelty, uncertainty, or potential for high reward. This leads to inefficient exploration, particularly in environments with large state-action spaces, sparse rewards, or deceptive local optima \\cite{stadie20158af, houthooft2016yee}. For instance, in complex domains requiring processing raw pixel inputs, simple $\\epsilon$-greedy methods are often impractical due to their reliance on enumerating or uniformly sampling a vast, high-dimensional state-action space \\cite{stadie20158af}. The lack of direction means the agent might spend considerable time revisiting well-understood states or exploring unpromising regions, rather than focusing on truly unknown or potentially rewarding areas \\cite{stanton20183fs}.\n\nFurthermore, $\\epsilon$-greedy policies struggle to distinguish between actions that are truly unknown and those that are known to be suboptimal \\cite{gupta2018rge}. Every action, regardless of how much is known about its outcomes, receives the same random exploration probability. This uniform randomness can be particularly problematic in real-world settings, where \"random exploration, nevertheless, can result in disastrous outcomes and surprising performance\" \\cite{ghamari2024bbm}. The method fails to leverage the agent's uncertainty about its value estimates, a critical piece of information for efficient exploration. This limitation highlighted the need for strategies that could generalize uncertainty and direct exploration towards states or actions with high epistemic uncertainty, as explored by methods like count-based exploration in feature space \\cite{martin2017bgt, tang20166wr}. These later approaches aimed to provide exploration bonuses based on how frequently states or features were visited, offering a more nuanced way to encourage novelty than simple uniform random action selection.\n\nThe inefficiency of $\\epsilon$-greedy becomes even more pronounced in large or continuous state spaces, where the probability of revisiting any specific state becomes infinitesimally small, rendering simple visit counts ineffective \\cite{tang20166wr}. This \"curse of dimensionality\" necessitated methods that could generalize exploration across similar states or learn representations of novelty, moving beyond the direct, uninformative randomness of $\\epsilon$-greedy.\n\nHowever, the concept of $\\epsilon$-greedy has not been entirely abandoned. Its simplicity has made it a foundational element that has been significantly refined and adapted. For example, in the context of Incremental Reinforcement Learning, where state and action spaces continually expand, a Dual-Adaptive $\\epsilon$-greedy Exploration (DAE) method has been proposed \\cite{ding2023whs}. This advanced variant dynamically adjusts the exploration probability $\\epsilon$ based on the convergence of value estimates for specific states (Meta Policy) and guides the agent to prioritize \"least-tried\" actions (Explorer). This evolution demonstrates how the core idea of balancing exploration and exploitation, first introduced by basic $\\epsilon$-greedy, can be made significantly more sophisticated and targeted to address the challenges of dynamic and expanding environments, moving beyond its initial undirected and inefficient form.\n\nIn conclusion, while basic exploration heuristics like $\\epsilon$-greedy provided a crucial initial framework for addressing the exploration-exploitation trade-off, their inherent limitationsundirected exploration, inefficiency in large state spaces, and inability to distinguish between truly unknown and well-understood but suboptimal actionsunderscored the necessity for more sophisticated and targeted exploration strategies. These early methods laid the groundwork, serving as a fundamental baseline from which the rich and diverse landscape of modern exploration techniques has emerged.\n\\subsection{Model-Based Planning and Experience Replay}\n\\label{sec:2\\_2\\_model-based\\_planning\\_\\_and\\_\\_experience\\_replay}\n\nEfficiently navigating and learning within complex environments is a fundamental challenge in reinforcement learning (RL), often exacerbated by the high cost of real-world interactions. Model-based planning and experience replay address this by making more efficient use of collected experience, implicitly aiding exploration by accelerating learning and propagating information more widely across the state space.\n\nA foundational approach in this domain is the Dyna architecture, introduced by \\cite{Sutton1990}. Dyna-Q integrates direct reinforcement learning with planning by concurrently learning an environmental model (transitions and rewards) from real experiences. This learned model is then used to generate simulated experiences, allowing the agent to perform \"mental rehearsals\" and update its value function from both real and imagined interactions. This process significantly accelerates value function updates and propagates information more widely, making each real interaction more valuable and implicitly encouraging exploration by quickly refining the agent's understanding of the environment. Complementing this, \\cite{Lin1992} highlighted the importance of experience replay, a mechanism where past experiences are stored and re-used for learning. By replaying previously collected data, agents can learn more efficiently from a fixed set of interactions, reducing the need for extensive new exploration and improving sample efficiency, particularly in off-policy learning settings.\n\nTo further enhance the efficiency of model-based planning, \\cite{Sutton1993} and \\cite{Moore1993} introduced prioritized sweeping. This method refines the planning process by prioritizing updates to state-action pairs whose values are likely to change significantly, or which have a large impact on other states. By focusing computational resources on the most informative simulated experiences, prioritized sweeping dramatically accelerates learning and value propagation compared to uniform sweeping, ensuring that the agent's understanding of the environment is refined more quickly and effectively. Addressing the challenge of scaling model-based methods to larger state spaces, \\cite{Singh1992} proposed reinforcement learning with a hierarchy of abstract models. This approach leverages structural decomposition to manage complexity, allowing exploration and planning to occur at different levels of temporal abstraction, which can make the learning problem more tractable. Similarly, \\cite{Kaelbling1993} explored methods for blending planning and direct reinforcement learning, demonstrating how a learned model can be actively used to guide exploration by evaluating hypothetical scenarios and informing action selection, thereby making exploration more directed and less random.\n\nBeyond direct model learning for planning, advancements in state representation also contribute to the efficiency of model-based approaches. \\cite{Dayan1993} introduced the successor representation, which models the expected future state occupancies rather than immediate transitions. While not a direct planning mechanism in the Dyna sense, this representation provides a more generalized understanding of state relationships, improving generalization for temporal difference learning and implicitly aiding exploration by making value estimates more robust and transferable across similar states.\n\nIn more recent deep reinforcement learning contexts, the principles of model-based planning continue to evolve. \\cite{stadie20158af} demonstrated how deep predictive models can be used to incentivize exploration by assigning exploration bonuses based on the uncertainty or novelty derived from the learned dynamics. This approach leverages the representational power of neural networks to build scalable models in high-dimensional domains, guiding exploration towards areas where the model's predictions are less confident. Extending this, \\cite{fu20220cl} proposed a model-based lifelong reinforcement learning approach that estimates a hierarchical Bayesian posterior to distill common structures across tasks. By combining this learned posterior with Bayesian exploration, their method significantly increases the sample efficiency of learning across related tasks, showcasing how sophisticated model learning can facilitate principled exploration and transfer. Furthermore, \\cite{ma2024r2p} integrated neural network models into an actor-critic architecture (ModelPPO) for AUV path-following control. Their neural network model learns the state transition function, allowing the agent to explore spatio-temporal patterns and achieve superior performance compared to traditional model predictive control and other RL methods, underscoring the enduring utility of learned models in complex control tasks.\n\nDespite their significant advantages in sample efficiency and information propagation, model-based planning and experience replay methods face critical limitations. Their performance heavily relies on the accuracy and learnability of the environmental model. In complex, high-dimensional, or non-stationary domains, learning an accurate and robust model can be exceedingly challenging, and errors in the model can compound, leading to \"model bias\" and potentially suboptimal policies or misleading exploration. Nevertheless, these foundational and evolving model-based approaches remain crucial for accelerating learning and making efficient use of collected experience, thereby implicitly guiding agents towards more effective exploration strategies.\n\\subsection{Early Explicit Exploration Bonuses}\n\\label{sec:2\\_3\\_early\\_explicit\\_exploration\\_bonuses}\n\nThe fundamental challenge of exploration in reinforcement learning (RL) necessitates strategies that transcend purely random actions to efficiently discover optimal policies, particularly in environments characterized by sparse or delayed rewards. This subsection focuses on the pioneering methods that introduced explicit incentives for agents to explore novel or less-visited states, thereby laying the groundwork for more sophisticated intrinsic motivation techniques. These early approaches were crucial in demonstrating the power of directed exploration beyond mere stochasticity.\n\nA seminal contribution to explicit exploration bonuses came from \\cite{Thrun1992}, who introduced count-based exploration. In this paradigm, agents receive an additional, intrinsic reward for visiting states or taking actions less frequently encountered. The core idea is straightforward: by incentivizing novelty based on visitation frequency, the agent is directly encouraged to explore uncharted regions of the state space. This approach effectively transforms the problem of undirected search into a directed quest for new experiences, ensuring broader state space coverage in tabular settings. This principle aligns with the broader concept of \"optimism in the face of uncertainty,\" where less-known options are optimistically valued higher to encourage their selection \\cite{SuttonBarto2018}. Such count-based mechanisms share conceptual roots with strategies employed in the multi-armed bandit problem, where algorithms like Upper Confidence Bound (UCB) leverage visitation counts (or estimates of uncertainty) to balance exploitation of known good options with exploration of less-tried ones, thereby providing a theoretical basis for directed exploration in simpler settings.\n\nComplementing frequency-based methods, the concept of \\textit{recency-based} exploration also emerged as a valuable heuristic in early RL. While less formally enshrined in a single seminal work compared to count-based methods, the underlying idea was to grant exploration bonuses based on the time elapsed since a state was last visited, or to prioritize states that were recently discovered but not yet thoroughly explored. These approaches aimed to prevent agents from getting stuck in local optima by encouraging them to refresh their knowledge about \"stale\" or neglected parts of the environment. For instance, an agent might receive a bonus inversely proportional to the number of timesteps since its last visit to a particular state, ensuring that even frequently visited states are eventually re-explored if they haven't been seen for a while. Both count-based and recency-based methods, while distinct in their temporal focus, shared the common goal of directing exploration by explicitly rewarding the agent for interacting with less familiar parts of the environment, moving beyond the undirected nature of $\\epsilon$-greedy exploration.\n\nDespite their groundbreaking nature and effectiveness in controlled, tabular, or low-dimensional environments, these early explicit exploration bonuses faced significant limitations, primarily due to the curse of dimensionality. Count-based methods, by their very definition, require maintaining an accurate count for each unique state-action pair. In environments with large, continuous, or high-dimensional state spaces (e.g., visual observations from images), enumerating and tracking every distinct state becomes computationally infeasible and memory-prohibitive. The notion of a \"unique state\" itself becomes ill-defined in continuous spaces, making direct counting impossible. Similarly, recency-based methods also struggle in such complex settings, as tracking the last visit time for an astronomically large or continuous state space is equally impractical. The inability of these foundational explicit bonuses to scale effectively to real-world complexity underscored the need for more generalized and robust intrinsic motivation techniques that could approximate novelty in high-dimensional settings without explicit state enumeration.\n\nIn conclusion, early explicit exploration bonuses, encompassing both count-based (frequency) and recency-based heuristics, provided a critical foundation for directed exploration in reinforcement learning. They successfully demonstrated the power of incentivizing novelty to overcome the limitations of purely random search in environments where states could be distinctly enumerated. However, their inherent reliance on explicit state representations severely limited their applicability to tabular or low-dimensional environments. These fundamental scalability challenges, driven by the curse of dimensionality, highlighted the need for two distinct paths forward: firstly, the development of theoretically grounded approaches that could offer provable guarantees on learning efficiency in tractable domains (as discussed in Section 3); and secondly, the creation of more advanced intrinsic motivation techniques that could generalize the notion of a \"count\" or \"novelty\" to complex, high-dimensional domains without explicit state enumeration (as will be explored in Section 4.2).\n\n\n\\label{sec:theoretically_grounded_exploration_strategies}\n\n\\section{Theoretically Grounded Exploration Strategies}\n\\label{sec:theoretically\\_grounded\\_exploration\\_strategies}\n\n\\subsection{Optimism in the Face of Uncertainty (OFU) and PAC-MDP}\n\\label{sec:3\\_1\\_optimism\\_in\\_the\\_face\\_of\\_uncertainty\\_(ofu)\\_\\_and\\_\\_pac-mdp}\n\nThe principle of \"optimism in the face of uncertainty\" (OFU) stands as a foundational pillar for theoretically grounded exploration in reinforcement learning. This paradigm dictates that when an agent faces uncertainty about the true value of a state-action pair, it should optimistically assume the highest possible reward, thereby actively incentivizing exploration of unknown or poorly understood regions of the environment. This inherent bias towards unexplored options ensures that the agent gathers sufficient data to accurately estimate values, ultimately facilitating convergence to an optimal policy. OFU is intrinsically linked to the concept of Probably Approximately Correct (PAC-MDP) guarantees, which provide strong theoretical assurances that an agent can learn a near-optimal policy with high probability within a polynomial number of interactions \\cite{kearns2002near}.\n\nThe historical development of OFU principles can be traced from the simpler multi-armed bandit (MAB) setting to full Markov Decision Processes (MDPs). In MABs, Upper Confidence Bound (UCB) algorithms, such as UCB1 \\cite{auer2002finite}, exemplify OFU by selecting actions that maximize an upper confidence bound on their estimated value. This bound is typically a sum of the empirical mean reward and a bonus term that scales with the uncertainty (e.g., inversely proportional to the square root of the number of times the arm has been pulled). This strategy ensures that arms with potentially high, but uncertain, returns are sufficiently explored.\n\nExtending this principle to the more complex MDP setting, algorithms like R-Max \\cite{brafman2002r} and UCRL (Upper Confidence Reinforcement Learning) \\cite{auer2009ucrl2} operationalize OFU to provide PAC-MDP guarantees. R-Max constructs an explicit model of the MDP and, for any state-action pair that has not been sampled a sufficient number of times, it optimistically assigns a maximal reward ($R\\_{max}$) and models a self-loop transition. This design effectively \"forces\" the planning algorithm to prioritize exploration of these unknown regions, as they appear maximally rewarding. Once a state-action pair has been visited enough times, its estimated reward and transition dynamics are considered reliable, and the optimism is removed. Similarly, UCRL algorithms maintain confidence intervals over the estimated transition probabilities and reward functions of the MDP. At each step, UCRL computes an \"optimistic\" MDP whose parameters lie within these confidence intervals and whose optimal policy yields the highest possible value. The agent then acts optimally with respect to this optimistic model, ensuring that actions leading to uncertain but potentially high-reward outcomes are chosen. Another notable algorithm, UCB-Value Iteration (UCB-VI), also leverages confidence bounds on value functions to guide optimistic exploration \\cite{kearns2002near}.\n\nThese OFU-based algorithms are celebrated for their robust theoretical bounds on sample complexity, guaranteeing that an agent will find an $\\epsilon$-optimal policy (a policy whose value is within $\\epsilon$ of the optimal value) within a number of interactions that scales polynomially with the size of the state space, action space, and the desired accuracy. This makes them a strong foundation for efficient learning in environments where such guarantees are paramount. However, a critical limitation arises from their reliance on explicit state enumeration and accurate model estimation, which becomes intractable in high-dimensional or continuous state spaces. As highlighted by \\cite{stadie20158af}, while \"Bayesian and PAC-MDP approaches to the exploration problem offer strong formal guarantees,\" they often become \"impractical in higher dimensions due to their reliance on enumerating the state-action space.\" This \"curse of dimensionality\" severely restricts their direct applicability to complex, real-world environments, a challenge reinforced by comprehensive surveys on deep reinforcement learning exploration \\cite{yang2021psl}.\n\nDespite these scalability challenges, the core tenets of OFU continue to inform contemporary research. Modern model-based RL algorithms still strive for similar guarantees, even if they employ approximations to handle larger state spaces. For instance, \\cite{song2021elb} introduces PC-MLP, a model-based RL algorithm that aims for polynomial sample complexity in both Kernelized Nonlinear Regulators and linear MDPs, demonstrating that the pursuit of theoretically efficient exploration remains active. This work, like its predecessors, relies on a planning oracle, a common assumption in algorithms with strong theoretical bounds. Furthermore, recent work by \\cite{sreedharan2023nae} explores optimistic exploration using symbolic model estimates, showcasing how OFU principles can be adapted to structured environments where symbolic representations can mitigate some of the dimensionality issues, thereby making optimistic planning more tractable.\n\nIn conclusion, the principle of optimism in the face of uncertainty, coupled with PAC-MDP guarantees, provides a robust theoretical framework for efficient exploration in reinforcement learning. These methods offer strong bounds on sample complexity and ensure convergence to near-optimal policies by systematically exploring uncertain but potentially rewarding avenues. However, their inherent reliance on explicit model construction and finite state-action spaces limits their direct applicability to the vast, high-dimensional environments common in modern deep RL. This fundamental trade-off between theoretical rigor and practical scalability has motivated the development of alternative exploration strategies, such as intrinsic motivation and approximate methods, which often sacrifice explicit PAC-MDP assurances for greater applicability in complex domains.\n\\subsection{Bayesian Approaches to Exploration}\n\\label{sec:3\\_2\\_bayesian\\_approaches\\_to\\_exploration}\n\nBayesian approaches offer a principled and theoretically grounded framework for tackling the exploration-exploitation dilemma in reinforcement learning by explicitly quantifying and managing uncertainty. These methods maintain a posterior distribution over possible models of the environment, value functions, or policies, leveraging this uncertainty to guide decision-making. The fundamental premise is that actions are not merely chosen based on their immediate expected reward, but also for their potential to reduce epistemic uncertainty, thereby leading to more informed and efficient learning over the long term. This subsection explores key techniques, from foundational posterior sampling methods like Thompson Sampling to scalable approximations using deep ensembles and Monte Carlo dropout, highlighting their mechanisms for balancing exploration and exploitation.\n\nHistorically, the concept of Bayesian reinforcement learning dates back to early theoretical works, where agents would explicitly maintain a posterior over the entire Markov Decision Process (MDP) parameters \\cite{strens2000bayesian}. While providing strong theoretical guarantees, the computational intractability of maintaining and updating exact posterior distributions, especially in high-dimensional state and action spaces or with complex, non-linear dynamics models common in deep reinforcement learning (DRL), severely limited their practical application. Exact Bayesian inference often requires complex computations over continuous or high-dimensional parameter spaces, making it prohibitive for real-world scenarios.\n\nA cornerstone technique that exemplifies the Bayesian principle is Thompson Sampling. It operates by sampling a model (or a Q-function, or a policy) from the current posterior distribution and then acting optimally with respect to that sampled entity for a period. This mechanism inherently balances exploration and exploitation: models that are highly uncertain or have not been sufficiently explored are more likely to be sampled, leading to exploration, while well-understood models guide exploitation. The elegance of Thompson Sampling lies in its ability to implicitly direct exploration towards promising yet uncertain areas. Recent advancements have focused on making Thompson Sampling more scalable and provably efficient for DRL. For instance, \\textcite{ishfaq20235fo} present a scalable Thompson Sampling strategy for RL that directly samples the Q-function from its posterior distribution using Langevin Monte Carlo, an efficient Markov Chain Monte Carlo (MCMC) method. This approach bypasses the need for restrictive Gaussian approximations, offering a more accurate representation of the posterior and demonstrating a regret bound of $\\tilde{O}(d^{3/2}H^{3/2}\\sqrt{T})$ in linear Markov Decision Processes (MDPs), making it deployable in deep RL with standard optimizers. Building on this, \\textcite{ishfaq20245to} further enhance randomized exploration for RL by proposing an algorithmic framework that incorporates various approximate sampling methods with the computationally challenging Feel-Good Thompson Sampling (FGTS) approach. Their work yields improved regret bounds for linear MDPs and shows significant empirical gains in challenging deep exploration tasks within the Atari 57 suite, underscoring the potential of efficient approximate sampling to unlock the power of Thompson Sampling in complex environments.\n\nGiven the challenges of exact Bayesian inference, much research in DRL has focused on practical approximations for estimating epistemic uncertainty, which is crucial for effective Bayesian exploration. Deep ensembles have emerged as a prominent and effective method. By training multiple neural networks with different random initializations or data subsets, the disagreement among their predictions can serve as a proxy for epistemic uncertainty. This disagreement can then be used to generate intrinsic rewards, encouraging the agent to explore states where the ensemble's predictions diverge significantly. \\textcite{jiang2023qmw} propose Exploration via Distributional Ensemble (EDE), a method that encourages exploration of states with high epistemic uncertainty through an ensemble of Q-value distributions. EDE demonstrates state-of-the-art performance on benchmarks like Procgen and Crafter, highlighting the importance of exploration for generalization and the efficacy of ensemble-based uncertainty. Similarly, \\textcite{yang2022mx5} introduce Ensemble Proximal Policy Optimization (EPPO), which learns ensemble policies and incorporates a diversity enhancement regularization over the policy space. This regularization helps to generalize to unseen states and promotes exploration by encouraging the ensemble members to maintain diverse behaviors, thereby covering a broader range of the state-action space. In safety-critical applications, \\textcite{zhang2024ppn} leverage deep ensembles to estimate epistemic uncertainty within a safe reinforcement learning framework. Their Uncertainty-augmented Lagrangian (Lag-U) algorithm uses this uncertainty to encourage exploration and adaptively modify safety constraints, enabling a better trade-off between efficiency and risk avoidance in autonomous driving.\n\nAnother practical method for approximating Bayesian uncertainty in deep neural networks is Monte Carlo dropout. By applying dropout during inference, multiple forward passes can be performed to obtain a distribution of predictions, from which uncertainty (e.g., variance) can be estimated. This technique provides a computationally efficient way to quantify epistemic uncertainty without training multiple separate models. \\textcite{wu2021r67} utilize a practical and effective dropout-based uncertainty estimation method in their Uncertainty Weighted Actor-Critic (UWAC) algorithm. While primarily applied to offline reinforcement learning to detect and down-weight out-of-distribution state-action pairs, the underlying principle of using dropout to estimate uncertainty is directly applicable to guiding exploration in online settings by incentivizing visits to states where uncertainty is high.\n\nDespite their theoretical elegance and principled approach, a common limitation of explicit Bayesian methods remains the computational complexity associated with maintaining and updating posterior distributions. While modern approximations like MCMC, deep ensembles, and Monte Carlo dropout significantly improve scalability, they introduce their own trade-offs. Deep ensembles require training and maintaining multiple neural networks, which can be computationally expensive and memory-intensive. Monte Carlo dropout, while efficient, relies on specific assumptions about the network architecture and may not always accurately capture the true posterior uncertainty. The accuracy of these approximations directly impacts the effectiveness of the exploration strategy and the theoretical guarantees. Future research continues to focus on developing more scalable, computationally efficient, and theoretically robust Bayesian approximations that can harness the full potential of uncertainty-driven exploration in complex, high-dimensional, and real-world reinforcement learning scenarios, moving beyond heuristic exploration towards more informed and adaptive learning.\n\\subsection{Information-Theoretic Exploration}\n\\label{sec:3\\_3\\_information-theoretic\\_exploration}\n\nInformation-theoretic exploration strategies offer a principled framework for addressing the exploration-exploitation dilemma in Reinforcement Learning (RL) by explicitly quantifying and maximizing the expected information gain. Unlike heuristic or purely novelty-seeking approaches, these methods guide agents towards experiences that are most likely to reduce uncertainty about the environment's dynamics, the optimal policy, or the value function. This section delineates various facets of information-theoretic exploration, emphasizing how they provide a sophisticated understanding of learning progress and model improvement.\n\nA fundamental concept in this domain is the maximization of mutual information. A seminal work, Variational Information Maximizing Exploration (VIME) \\cite{Houthooft2016}, exemplifies this by proposing an intrinsic reward signal derived from the mutual information between the agent's actions and the learned parameters of its environment dynamics model. VIME leverages variational inference to estimate this information gain, thereby incentivizing the agent to take actions that maximally reduce its uncertainty about how the environment functions. This approach moves beyond simple state visitation counts, actively driving the agent to improve its internal model of the world by seeking out states and actions that are most informative for model learning. The strength of VIME lies in its explicit link between exploration and model improvement, but its computational complexity, particularly in estimating mutual information and maintaining accurate posterior distributions over model parameters in high-dimensional settings, can be a significant challenge.\n\nBeyond uncertainty about the environment model, information-theoretic approaches also focus on reducing uncertainty about the optimal policy or value function. Information-Directed Sampling (IDS) is a prominent example, which explicitly quantifies the value of information by maximizing the \"information ratio\" \\cite{russo2014learning}. This ratio balances the expected reduction in regret (or increase in reward) from gaining information against the cost of exploration. Unlike Thompson Sampling (which samples a policy from a posterior and acts greedily), IDS directly optimizes for the value of information, making it a more explicit information-theoretic strategy. While initially developed for bandit problems, IDS has been extended to Deep RL, as demonstrated by \\cite{nikolov20184g9}. This work proposes a tractable approximation of IDS for deep Q-learning, which explicitly accounts for both parametric uncertainty and heteroscedastic observation noise. By leveraging distributional reinforcement learning, this approach provides a robust exploration strategy that is particularly effective in environments with varying levels of uncertainty, outperforming traditional methods that struggle with non-uniform return variability. The application of IDS also extends beyond traditional RL control tasks, as seen in \\cite{sun2020zjg}, where it was used in density-based structural topology optimization to efficiently direct the search towards optimal designs by maximizing the expected value of information in generative design problems.\n\nAnother significant information-theoretic concept is empowerment, which defines an intrinsic reward as the channel capacity between an agent's actions and its future states \\cite{salge2014empowerment, mohamed2015variational}. Maximizing empowerment encourages an agent to explore states where it has greater control or influence over its future, effectively driving it towards regions of the state space that offer more diverse and controllable outcomes. This perspective aligns with the broader idea of intrinsic motivation, where agents are driven by an innate desire to understand and control their environment. As highlighted by \\cite{aubret2022inh}, information theory provides a rich taxonomy for intrinsic motivation, encompassing concepts like surprise (reduction in predictive uncertainty), novelty (information gain about unfamiliar states), and skill-learning (maximizing control over future states, i.e., empowerment). These different facets underscore how information-theoretic principles can be applied to various aspects of learning and exploration.\n\nWhile Bayesian methods, discussed in Section 3.2, inherently align with information-theoretic principles by maintaining and reducing uncertainty, information-theoretic exploration distinguishes itself by explicitly formulating exploration as an optimization problem over information gain. For instance, Thompson Sampling implicitly reduces uncertainty by sampling from a posterior, but IDS or VIME directly compute or approximate the value of information. The computational overhead of precisely quantifying mutual information or channel capacity remains a primary challenge for information-theoretic methods, especially in complex, high-dimensional, and non-stationary environments. Approximations, such as those used in VIME or the Deep RL extension of IDS \\cite{nikolov20184g9}, are crucial for scalability.\n\nIn conclusion, information-theoretic exploration offers a powerful and principled lens through which to design effective exploration strategies. By explicitly valuing information gainwhether about the environment's dynamics (VIME), the optimal policy (IDS), or the agent's control over its future (empowerment)these methods move beyond simple heuristics to foster truly intelligent and directed discovery. Despite challenges related to computational tractability and the accurate estimation of information-theoretic quantities in complex settings, advancements in approximation techniques continue to enhance their practical applicability. Future research will likely focus on developing more efficient and robust approximations for information gain, potentially integrating with meta-learning to adaptively select optimal information-seeking strategies, and further exploring their utility in multi-agent and open-ended learning scenarios.\n\n\n\\label{sec:intrinsic_motivation:_novelty,_curiosity,_and_prediction_error}\n\n\\section{Intrinsic Motivation: Novelty, Curiosity, and Prediction Error}\n\\label{sec:intrinsic\\_motivation:\\_novelty,\\_curiosity,\\_\\_and\\_\\_prediction\\_error}\n\n\\subsection{Early Concepts of Intrinsic Curiosity}\n\\label{sec:4\\_1\\_early\\_concepts\\_of\\_intrinsic\\_curiosity}\n\nThe challenge of exploration in reinforcement learning, particularly in environments characterized by sparse or delayed extrinsic rewards, led to the development of intrinsic motivation. This paradigm shift moved beyond solely relying on external reward signals, proposing that agents could be driven by an internal 'curiosity' or 'novelty' derived from their own learning progress or model improvement. These foundational concepts laid the theoretical and conceptual groundwork for later, more sophisticated curiosity-driven and novelty-seeking exploration methods.\n\nOne of the earliest proponents of intrinsic curiosity was \\textcite{Schmidhuber1997}, who introduced the idea of rewarding an agent for improving its world model's predictive accuracy. The agent is intrinsically motivated to explore states where its current model makes inaccurate predictions, thus seeking out \"surprising\" observations to reduce its uncertainty and improve its understanding of the environment. Building on this, \\textcite{Singh2004} further formalized the notion of intrinsic motivation, comparing and contrasting different intrinsic signals such as novelty (unfamiliarity) and surprise (prediction error), providing a more theoretical framework for these internal drives.\n\nAs reinforcement learning moved towards more complex, high-dimensional domains, the challenge became scaling these intrinsic curiosity concepts. \\textcite{Stadie2015} addressed this by proposing an exploration method that assigned bonuses from a concurrently learned deep predictive model of the system dynamics. This work demonstrated how the early ideas of prediction-error-based curiosity could be extended to tasks requiring raw pixel inputs, like Atari games, by leveraging deep neural networks to parameterize the world model. Further refining the theoretical underpinnings, \\textcite{Houthooft2016} introduced Variational Information Maximizing Exploration (VIME), a principled, Bayesian approach that encourages agents to explore by maximizing the information gain about the environment's dynamics model. This method provides a more formal way to quantify and reduce epistemic uncertainty, guiding exploration towards states that are most informative for improving the agent's internal model.\n\nDespite these advancements, prediction-error-based curiosity methods faced a challenge known as the \"noisy TV problem,\" where agents could be perpetually distracted by unlearnable stochastic elements in the environment that constantly generated high prediction errors. To address this, \\textcite{Pathak2017} proposed the Intrinsic Curiosity Module (ICM), which computes intrinsic rewards based on the prediction error of future states in a \\textit{learned feature space} rather than raw pixel space. By learning a feature representation that is invariant to factors beyond the agent's control, ICM effectively filters out unlearnable stochasticity, allowing curiosity to focus on aspects of the environment that the agent can influence. \\textcite{Burda2018} further simplified and improved the robustness of curiosity-driven exploration with Random Network Distillation (RND). RND measures novelty as the prediction error of a fixed, randomly initialized target network's output by a trained prediction network, providing a highly effective intrinsic reward signal that is largely immune to the noisy TV problem because the prediction target is independent of the environment's true dynamics.\n\nThe practical utility of these curiosity-driven approaches has been demonstrated across various applications. For instance, \\textcite{Li2019tj1} showed how a simplified Intrinsic Curiosity Module (S-ICM) could be effectively integrated with off-policy reinforcement learning methods, significantly improving exploration efficiency and learning performance in robotic manipulation tasks with sparse rewards. Similarly, \\textcite{Zhelo2018wi8} applied curiosity-driven exploration to mapless navigation for mobile robots, validating its crucial role in improving deep reinforcement learning performance in tasks with challenging exploration requirements and enhancing generalization capabilities in unseen environments. More recently, \\textcite{Sun2022ul9} utilized a similarity-based curiosity module to enable aggressive quadrotor flights, demonstrating how intrinsic motivation can accelerate training and improve the robustness of policies in complex control tasks.\n\nIn conclusion, the early concepts of intrinsic curiosity marked a fundamental shift in reinforcement learning, moving from external reward dependence to internal drives based on predictability, surprise, and learning progress. These pioneering ideas, from \\textcite{Schmidhuber1997}'s initial formulation of prediction error as a motivator to the more robust and scalable deep learning-driven methods like ICM \\textcite{Pathak2017} and RND \\textcite{Burda2018}, have provided effective solutions for exploration in sparse-reward environments. While significant progress has been made in making these methods robust to irrelevant stochasticity, ongoing research continues to explore how to design intrinsic reward functions that consistently align with efficient and meaningful exploration across diverse, open-ended domains, and how to balance these internal drives with external task objectives.\n\\subsection{Count-Based and Density-Based Novelty}\n\\label{sec:4\\_2\\_count-based\\_\\_and\\_\\_density-based\\_novelty}\n\nEffective exploration is paramount in reinforcement learning, particularly when agents operate in environments characterized by sparse extrinsic rewards or vast, high-dimensional state spaces. A prominent class of intrinsic motivation methods addresses this by quantifying the 'novelty' or 'unvisitedness' of states, generating internal reward signals that encourage agents to venture into less-frequented regions. This approach aims to foster broad state space coverage, which is often crucial for discovering optimal policies.\n\nThe foundational concept of count-based exploration, as pioneered by \\cite{thrun1992efficient}, involves assigning an intrinsic bonus to states inversely proportional to their visitation frequency. In tabular or low-dimensional discrete environments, these methods provide a theoretically sound mechanism for directed exploration, ensuring that agents sufficiently explore all reachable states. However, as discussed in Section 2.3, traditional count-based approaches face a critical limitation: the curse of dimensionality. In high-dimensional or continuous state spaces, the probability of revisiting any exact state becomes infinitesimally small. This renders direct state counting impractical, as most states are encountered only once, leading to uniformly high novelty bonuses that fail to guide exploration effectively.\n\nTo bridge this gap and enable count-based exploration in deep reinforcement learning, researchers developed sophisticated techniques to approximate state visitation frequencies. Early efforts, such as those by \\cite{stadie20158af}, demonstrated that deep predictive models could generate intrinsic exploration bonuses based on learned system dynamics in complex visual environments like Atari games. While not strictly count-based, this work highlighted the potential of neural networks to process high-dimensional observations and produce meaningful intrinsic signals, setting the stage for more direct approximations of novelty.\n\nA pivotal breakthrough in scaling count-based exploration was the introduction of pseudo-counts and density models. \\cite{bellemare2016unifying} proposed a unified framework that generalizes count-based exploration by estimating state visitation frequencies using density models, thereby generating \"pseudo-counts\" for high-dimensional observations. Instead of exact state matching, this approach leverages the statistical likelihood of observing a state given past experiences. States that are less probable under the learned density model are considered more novel and receive higher intrinsic rewards. This effectively overcomes the limitations of exact state enumeration by providing a continuous and differentiable measure of novelty.\n\nBuilding on this principle, various practical implementations emerged. \\cite{tang20166wr} introduced \\texttt{\\#Exploration}, a surprisingly effective yet simple method that maps high-dimensional states to hash codes. By counting the occurrences of these hash codes, the approach approximates state visitation frequencies, allowing for scalable pseudo-counting. This demonstrated that even a relatively crude approximation of state novelty, when combined with deep reinforcement learning, could yield near state-of-the-art performance on challenging benchmarks. However, the effectiveness of hash-based methods can be sensitive to the choice of hash function and the potential for hash collisions, which might conflate distinct states. Further refining the use of neural networks for density estimation, \\cite{ostrovski2017count} explicitly employed neural density models to compute pseudo-counts. This provided a more principled and robust statistical approach to estimate state novelty in complex visual environments, as the density model can learn more meaningful representations of states and their relationships. The challenge with such methods lies in the computational complexity of training accurate density models in high-dimensional spaces and ensuring that the learned density truly reflects meaningful novelty rather than irrelevant stochasticity.\n\nWhile count-based and density-based methods primarily focus on the frequency of state visitation, other intrinsic motivation techniques, such as curiosity-driven exploration, leverage prediction error as a proxy for novelty. Methods like the Intrinsic Curiosity Module (ICM) by \\cite{pathak2017curiosity} and Random Network Distillation (RND) by \\cite{burda2018exploration} reward agents for encountering states where their internal predictive models are inaccurate or for states that lead to unpredictable outcomes. These approaches offer an alternative perspective on novelty, focusing on the agent's learning progress or uncertainty about environmental dynamics rather than mere visitation frequency. Although distinct in their underlying signals, both paradigms share the common goal of generating intrinsic rewards to drive exploration in sparse-reward, high-dimensional settings, with density-based methods providing a statistical measure of \"unvisitedness\" and prediction-error methods focusing on \"unpredictability.\"\n\nIn summary, count-based and density-based novelty methods have undergone a significant evolution, transforming from simple heuristics for discrete environments into sophisticated deep learning techniques capable of scaling to complex, high-dimensional state spaces. The transition from direct state counting to pseudo-counts derived from neural density models has been critical for enabling robust exploration in deep reinforcement learning. These techniques provide a practical and often effective way to incentivize broad state space coverage and discover new areas. Nevertheless, challenges persist, including the computational overhead of training accurate density models, the sensitivity to state representation, and the difficulty of ensuring that the quantified novelty aligns with task-relevant exploration rather than being misled by uninformative stochasticity. Future research continues to refine these methods, often by integrating insights from both visitation statistics and predictive uncertainty, to develop more adaptive and robust novelty-seeking agents.\n\\subsection{Prediction Error and Self-Supervised Curiosity}\n\\label{sec:4\\_3\\_prediction\\_error\\_\\_and\\_\\_self-supervised\\_curiosity}\n\nEffective exploration remains a cornerstone challenge in reinforcement learning (RL), particularly in environments characterized by sparse rewards and high-dimensional observations. To address this, intrinsic motivation methods have emerged, where agents generate their own reward signals to drive discovery. A prominent approach within this paradigm is self-supervised curiosity, which leverages the agent's ability to predict future states or features, using prediction error as an intrinsic reward to guide exploration. This strategy incentivizes agents to seek out situations where their internal models of the world are inaccurate, thereby driving learning about the environment's underlying dynamics.\n\nThe foundational concept of curiosity as a driver for learning can be traced back to early work by \\cite{schmidhuber1997}, which proposed that agents could be intrinsically motivated to explore by optimizing the predictability of their sensory inputs. This early idea laid the groundwork for defining curiosity as a measure of surprise or novelty. Expanding on this, \\cite{stadie20158af} demonstrated that deep predictive models could be effectively used to assign exploration bonuses in complex domains like Atari games, by rewarding states where the agent's learned dynamics model exhibited high uncertainty. This represented an important step in scaling prediction-error-based curiosity to high-dimensional visual inputs, moving beyond simpler tabular settings.\n\nA significant advancement in this direction was the Intrinsic Curiosity Module (ICM) proposed by \\cite{pathak2017}. ICM defines curiosity as the error in predicting the consequence of an agent's own actions within a learned feature space. Specifically, it trains a self-supervised forward dynamics model to predict the next latent state given the current latent state and action. The magnitude of this prediction error then serves as the intrinsic reward. This design incentivizes the agent to explore states where its internal model is inaccurate, thereby driving learning about the environment's dynamics. Crucially, by operating in a learned feature space rather than raw pixels, ICM made an initial attempt to mitigate the \"noisy TV problem,\" where agents might be perpetually drawn to uncontrollable stochastic elements (like static on a TV screen) that generate high prediction error but offer no meaningful learning progress. This focus on learnable and controllable aspects of the environment represented a significant step towards scalable curiosity in high-dimensional visual environments.\n\nDespite ICM's success, its reliance on learning an accurate forward dynamics model can still be problematic, particularly in highly stochastic environments. In such settings, genuine environmental noise or inherent unpredictability can lead to consistently high prediction errors, which are uninformative for learning and can still distract the agent, leading to inefficient exploration. This limitation highlights a critical distinction: prediction error can arise from either the agent's lack of knowledge (epistemic uncertainty) or from inherent environmental stochasticity (aleatoric uncertainty). ICM, by primarily measuring the error of a single forward model, struggles to differentiate between these two sources, potentially leading to spurious curiosity signals.\n\nTo address this challenge and provide more robust curiosity signals, alternative prediction-error-based approaches have emerged. One prominent direction involves leveraging ensembles of models to quantify epistemic uncertainty more explicitly. For instance, methods like Exploration via Distributional Ensemble (EDE) \\cite{jiang2023qmw} encourage exploration of states with high epistemic uncertainty by using an ensemble of Q-value distributions. The disagreement or variance among the predictions of these ensemble members provides a more reliable signal of what the agent truly \"doesn't know,\" rather than simply what is unpredictable due to noise. This ensemble-based approach offers a principled way to direct exploration towards areas where the agent's understanding of the environment is weakest, promoting more efficient knowledge acquisition. The concept of using prediction error as a curiosity signal is also versatile, extending to domains like Large Language Models, where signals such as perplexity over generated responses or variance of value estimates from multi-head architectures can serve as intrinsic exploration bonuses \\cite{dai2025h8g}. From an information-theoretic perspective, these methods align with the idea of maximizing information gain, where surprise (prediction error) and novelty drive the building of abstract dynamics models and transferable skills \\cite{aubret2022inh}.\n\nThe versatility of these prediction-error-based curiosity mechanisms has led to their integration into various RL frameworks. For instance, \\cite{li2019tj1} demonstrated how a simplified version of ICM could be effectively combined with off-policy RL methods, such as Deep Deterministic Policy Gradient (DDPG) and Hindsight Experience Replay (HER), significantly improving exploration efficiency and learning performance in robotic manipulation tasks with sparse rewards. Furthermore, the utility of curiosity-based intrinsic motivation extends to the challenging domain of offline reinforcement learning. \\cite{lambert202277x} investigated how such curiosity-driven methods could be used to collect informative datasets in a task-agnostic manner, which could then be leveraged by offline RL algorithms, highlighting their role in generating high-quality data for subsequent learning.\n\nWhile prediction error and self-supervised curiosity have proven highly effective in driving exploration by incentivizing agents to learn about their environment's dynamics, challenges remain. The primary limitation lies in distinguishing between genuine uncertainty that can be resolved through exploration and inherent environmental stochasticity that offers no meaningful learning progress. This distinction is crucial for designing intrinsic reward functions that consistently align with meaningful exploration, especially in complex, hierarchical tasks. The balance between intrinsic and extrinsic rewards, and the potential for agents to get stuck in \"perpetual exploration\" loops without making tangible task progress, are also critical considerations. Addressing the robustness of these intrinsic signals against uninformative stochasticity is a key area of ongoing research, motivating the development of more sophisticated methods that will be discussed in the subsequent section.\n\\subsection{Robust Intrinsic Rewards: Addressing the Noisy TV Problem}\n\\label{sec:4\\_4\\_robust\\_intrinsic\\_rewards:\\_addressing\\_the\\_noisy\\_tv\\_problem}\n\nThe quest for effective exploration in reinforcement learning (RL) is profoundly challenged by environments offering sparse extrinsic rewards. While intrinsic motivation methods emerged as a promising avenue to generate internal curiosity signals and alleviate this sparsity, early prediction-error approaches frequently succumbed to the \"noisy TV problem.\" This phenomenon describes scenarios where agents are perpetually drawn to unpredictable yet uninformative stochastic elements within the environment, such as random pixel noise on a screen or flickering lights. These elements generate consistently high prediction errors, leading to spurious curiosity that distracts the agent from truly novel and learnable aspects of the environment, thereby hindering focused exploration.\n\nInitial efforts, such as those by \\cite{stadie20158af}, explored incentivizing exploration through bonuses derived from concurrently learned deep predictive models of system dynamics. This aimed to reward agents for encountering states that challenged their current environmental understanding. A prominent example of this paradigm is the Intrinsic Curiosity Module (ICM) \\cite{Pathak2017}. ICM trains a forward dynamics model to predict the next state's features given the current state's features and the executed action. The magnitude of this prediction error then serves as an intrinsic reward, encouraging the agent to visit states where its internal model is inaccurate. While ICM provided a scalable solution for high-dimensional visual inputs by operating in a learned feature space, its reliance on predicting \\textit{future states} made it inherently susceptible to the noisy TV problem. In environments with uninformative stochasticity, the forward dynamics model would consistently fail to predict these truly random, irreducible changes, resulting in persistently high prediction errors. This spurious curiosity would then cause the agent to repeatedly visit these uninformative areas, diverting computational resources and hindering progress towards task-relevant exploration.\n\nTo overcome the critical limitations posed by the noisy TV problem, \\cite{Burda2018} introduced Random Network Distillation (RND), a pivotal innovation in robust intrinsic reward generation. RND fundamentally redefines novelty detection by decoupling the intrinsic reward from the learnability of the environment's true dynamics. Instead of predicting future states, RND employs two neural networks: a fixed, randomly initialized target network ($f$) and a predictor network ($\\hat{f}$). Both networks receive the current state $s\\_t$ as input. The predictor network is trained to predict the output of the target network, i.e., $\\hat{f}(s\\_t) \\approx f(s\\_t)$. The intrinsic reward is then defined as the mean squared error between the outputs of these two networks: $r\\_i = ||\\hat{f}(s\\_t) - f(s\\_t)||^2$.\n\nThe key insight of RND lies in its design: for any given state $s\\_t$, even one containing uninformative stochasticity (like a noisy TV screen), the randomly initialized target network $f(s\\_t)$ produces a \\textit{fixed and deterministic} output embedding. The predictor network $\\hat{f}(s\\_t)$ is then trained to learn this fixed mapping. If an agent repeatedly visits a state $s\\_t$, the predictor $\\hat{f}$ will eventually learn to accurately map $s\\_t$ to $f(s\\_t)$, causing the prediction error and thus the intrinsic reward to \\textit{decay}. This mechanism effectively filters out irrelevant stochasticity because the reward is based on the \\textit{novelty of the state itself} (i.e., how well the predictor has learned to map that specific state to its fixed target), rather than the unpredictability of state transitions. Consequently, even a noisy TV state, despite its visual randomness, yields a fixed target output from $f$. Once the agent has sufficiently explored this state, $\\hat{f}$ learns the mapping, and the curiosity bonus diminishes, preventing perpetual attraction. This contrasts sharply with ICM, where the prediction error for a truly stochastic transition remains irreducible, leading to persistent curiosity.\n\nRND's robust and simpler mechanism for novelty detection proved highly effective, leading to significantly more efficient and directed exploration in complex visual domains, such as Atari games, where it substantially outperformed prior methods on hard exploration tasks \\cite{Burda2018}. Its success underscored the importance of designing intrinsic reward signals that are resilient to environmental noise and focus on aspects of the environment truly conducive to learning. Subsequent works, such as \\cite{li2019tj1}, further explored simplified variants of curiosity modules, demonstrating how architectural or methodological simplifications could enhance the practical utility and integration of prediction-error based intrinsic motivation, particularly for off-policy reinforcement learning methods.\n\nWhile RND provided a powerful solution to the noisy TV problem, it is not the sole robust intrinsic motivation strategy. Other approaches also aim to mitigate the effects of uninformative stochasticity or enhance exploration in complementary ways. For instance, methods based on model disagreement or ensembles, which reward exploration in states where multiple learned dynamics models disagree, can implicitly discount uncontrollable noise by focusing on areas where the agent's \\textit{learnable} understanding is inconsistent. Information-theoretic perspectives, as surveyed by \\cite{aubret2022inh}, often emphasize maximizing \\textit{useful} information gain, which aligns with RND's implicit discounting of unlearnable noise. Furthermore, diversity-driven exploration strategies, which incentivize agents to visit states that are distinct from previously encountered ones \\cite{hong20182pr}, or Random Latent Exploration (RLE), which encourages agents to pursue randomly sampled goals in a latent space \\cite{mahankali20248dx}, offer alternative mechanisms for robust and deep exploration without relying solely on prediction error. Deep Curiosity Search (DeepCS) \\cite{stanton20183fs} also introduced the concept of \"intra-life novelty,\" rewarding exploration within a single episode, which can complement RND's \"across-training novelty\" by encouraging immediate discovery.\n\nDespite these significant advancements, challenges persist. While RND effectively addresses the noisy TV problem by focusing on learnable novelty, the intrinsic reward signal can still be somewhat undirected, potentially leading to exploration of areas that are novel but not necessarily relevant to the extrinsic task. Future research continues to explore how to imbue intrinsic rewards with more task-relevant directionality, perhaps through goal-conditioned or hierarchical approaches, or how to combine them with other exploration strategies to achieve even more efficient and purposeful exploration in increasingly complex and open-ended environments. The integration of RND into advanced agents like Agent57 \\cite{Badia2020Agent57} further demonstrates its lasting impact as a foundational component for achieving state-of-the-art performance in diverse and challenging domains.\n\n\n\\label{sec:advanced_and_adaptive_exploration_strategies}\n\n\\section{Advanced and Adaptive Exploration Strategies}\n\\label{sec:advanced\\_\\_and\\_\\_adaptive\\_exploration\\_strategies}\n\n\\subsection{Hierarchical Reinforcement Learning for Exploration}\n\\label{sec:5\\_1\\_hierarchical\\_reinforcement\\_learning\\_for\\_exploration}\n\nEffective exploration is a critical bottleneck in reinforcement learning (RL), particularly in complex environments characterized by vast state-action spaces, sparse rewards, and long task horizons. Hierarchical Reinforcement Learning (HRL) offers a powerful and principled framework to address these challenges by decomposing large, intractable problems into a hierarchy of more manageable sub-problems. This decomposition allows agents to learn and explore at different levels of temporal abstraction, significantly enhancing exploration efficiency and robustness.\n\nThe foundational concept underpinning HRL for exploration is the \"Options Framework\" introduced by \\cite{Sutton1999}. Building upon earlier notions of skill chaining by \\cite{Singh1995}, the Options Framework formalizes \"options\" as temporally extended actions, or sub-policies, that execute for multiple time steps. An option consists of an initiation set (states where it can be taken), a policy (how to act while the option is active), and a termination condition (when the option ends). This framework allows a high-level policy to choose among options, while a low-level policy executes the primitive actions within a chosen option. This mechanism fundamentally aids exploration by enabling agents to traverse large portions of the state space more efficiently than with primitive actions alone. Instead of exploring individual steps, the agent explores sequences of actions (options), effectively reducing the effective search space and allowing for more directed movement towards relevant subgoals or novel regions. For instance, an agent might learn an \"open door\" option, which, once selected, reliably executes the necessary primitive actions to open a door, allowing the high-level policy to explore the consequences of being on the other side of the door, rather than stumbling upon the correct sequence of door-opening actions by chance.\n\nIn the era of deep reinforcement learning, various architectures have been proposed to implement hierarchical control and leverage its benefits for exploration. FeUdal Networks (FuN) \\cite{Vezhnevets2017} introduced a two-level hierarchy with a \"Manager\" module that sets goals in a latent space and a \"Worker\" module that executes primitive actions to achieve those goals. The Manager explores the space of goals, while the Worker explores the primitive action space conditioned on the current goal. This explicit goal-setting mechanism intrinsically guides exploration towards achieving meaningful sub-objectives. Similarly, Hierarchical Reinforcement Learning with Off-policy Correction (HIRO) \\cite{Nachum2018} enables efficient learning of both high-level and low-level policies by correcting for off-policy data, allowing the high-level policy to explore by setting goals in the state space, and the low-level policy to learn how to reach those goals. Hierarchical Actor-Critic (HAC) \\cite{Levy2019} further refines this by using multiple layers of actor-critic agents, where higher levels set goals for lower levels, and a \"hindsight experience replay\" mechanism allows agents to learn from failed attempts to reach goals, thereby improving exploration by making better use of suboptimal trajectories. These deep HRL methods demonstrate how learning goal-conditioned policies at different levels of abstraction can significantly accelerate exploration in complex, high-dimensional environments.\n\nA critical aspect of HRL for exploration is the autonomous discovery of useful options or skills, often driven by intrinsic motivation. Rather than manually defining options, agents can learn them through self-supervision. Methods like Diversity is All You Need (DIAYN) \\cite{Eysenbach2018} and Variational Option Discovery (VALOR) \\cite{Gregor2017} learn a diverse set of skills by maximizing the mutual information between the skill executed and the resulting state trajectory, effectively rewarding the agent for discovering distinct behaviors. These learned skills then serve as valuable options for a higher-level policy, enabling more structured and efficient exploration. The survey by \\cite{aubret2022inh} highlights how information-theoretic intrinsic motivation, particularly novelty and surprise, can assist in building a hierarchy of transferable skills, making the exploration process more robust. Furthermore, \\cite{janjua2024yhk} emphasizes unsupervised skill acquisition as a key advancement for enhancing scalability in open-ended environments, where HRL provides a natural framework for organizing these learned behaviors.\n\nHRL also facilitates temporally coordinated and goal-conditioned exploration. \\cite{zhang2022p0b} proposed Generative Planning Method (GPM), which generates multi-step action plans, effectively acting as temporally extended options. These plans guide exploration towards high-value regions more consistently than single-step perturbations, and the plan generator can adapt to the task, further benefiting future explorations. This aligns with HRL's ability to create intentional action sequences for reaching specific subgoals. Similarly, Random Latent Exploration (RLE) \\cite{mahankali20248dx}, while not strictly HRL, encourages exploration by pursuing randomly sampled goals in a latent space. This goal-conditioned exploration paradigm is inherently compatible with HRL, where the high-level policy can sample latent goals for the low-level policy to achieve, fostering diverse and deep exploration.\n\nThe utility of hierarchical exploration extends significantly to multi-agent systems, where coordination and efficient search are paramount. \\cite{hu2020qwm} designed a cooperative exploration strategy for multiple mobile robots using a hierarchical control architecture, where a high-level decision-making layer coordinates exploration to minimize redundancy, and a low-level layer handles target tracking and collision avoidance. This demonstrates how HRL can structure complex multi-robot behaviors for efficient, coordinated exploration. More recently, \\cite{yu20213c1} tackled cooperative visual exploration for multiple agents with a Multi-agent Spatial Planner (MSP) leveraging a transformer-based architecture with hierarchical spatial self-attentions, enabling agents to capture spatial relations and plan cooperatively based on visual signals. \\cite{liu2024xkk} further advances multi-agent exploration with \"Imagine, Initialize, and Explore\" (IIE), which uses a transformer to imagine critical states and then initializes agents at these states for targeted exploration. This approach, while not explicitly called HRL, embodies hierarchical decomposition by first identifying high-level critical states (subgoals) and then focusing low-level exploration from those points, enhancing the discovery of successful joint action sequences in long-horizon tasks.\n\nIn summary, HRL provides a powerful framework for managing the complexity of exploration in large state-action spaces. By enabling agents to learn and compose temporally extended actions (options) or to decompose complex tasks into sub-problems, HRL significantly reduces the dimensionality of the exploration problem. This structured approach allows agents to focus on achieving meaningful subgoals, leading to more directed and sample-efficient discovery of optimal policies in long-horizon tasks. Despite these advancements, challenges persist, particularly in the autonomous discovery of optimal and diverse skill sets, the robust learning of high-level policies that effectively coordinate lower-level skills, and ensuring seamless communication and transfer of information across hierarchical levels. Future research will likely focus on developing more adaptive and autonomous methods for skill acquisition and composition, further integrating HRL with robust intrinsic motivation and meta-learning to create agents capable of truly open-ended and efficient exploration.\n\\subsection{Learning Exploration Policies (Meta-Exploration)}\n\\label{sec:5\\_2\\_learning\\_exploration\\_policies\\_(meta-exploration)}\n\nA pivotal advancement in reinforcement learning (RL) exploration shifts the paradigm from relying on hand-crafted heuristics or fixed intrinsic reward functions to enabling agents to autonomously learn their own exploration strategies. This sophisticated approach, termed meta-exploration, involves an outer-loop optimization process that trains a meta-controller or a recurrent policy to generate adaptive exploration behaviors, aiming to maximize long-term returns across a distribution of tasks or episodes. By learning \"how to explore,\" these methods can dynamically adjust the exploration-exploitation trade-off, leading to more efficient, task-relevant discovery and robust performance, particularly in novel and complex environments \\cite{duan2016rll, wang2016learning}. This represents a significant stride towards autonomous and intelligent exploration, where agents generalize their exploration capabilities rather than relearning them from scratch for each new task.\n\nThe foundational concept of meta-exploration was significantly advanced by works demonstrating that recurrent neural networks (RNNs) can serve as meta-learners. \\cite{duan2016rll} introduced RL$^2$ (Reinforcement Learning to Reinforce Learn), a seminal framework where an RNN-based agent is trained to solve a distribution of tasks. The RNN's hidden state effectively encodes task-specific information and past experience, allowing it to learn an exploration strategy that adapts within a single episode and across multiple episodes of a new, unseen task. This enables the agent to exhibit rapid adaptation and efficient exploration behaviors, such as directed search or uncertainty-driven probing, without explicit hand-engineered exploration bonuses. Similarly, \\cite{wang2016learning} explored the idea of \"Learning to Reinforce Learn,\" where an RNN acts as a meta-learner to discover an entire RL algorithm, including its exploration component, by processing sequences of observations, actions, and rewards. These approaches highlight the power of recurrent architectures to implicitly capture and execute sophisticated exploration policies that generalize across related tasks.\n\nBuilding on these foundations, subsequent research has focused on learning more structured and informed exploration strategies. \\cite{gupta2018rge} proposed Model Agnostic Exploration with Structured Noise (MAESN), a gradient-based meta-learning algorithm that learns exploration strategies from prior experience. MAESN leverages prior tasks to initialize a policy and acquire a latent exploration space, which injects structured stochasticity into the policy. This allows for exploration strategies that are informed by previous knowledge, moving beyond simple action-space noise and proving more effective than task-agnostic methods. This work underscores the benefit of meta-learning not just the policy, but the \\textit{mechanism} of exploration itself, enabling more targeted and efficient discovery.\n\nMeta-learning has also been applied to the generation and refinement of intrinsic motivation signals for exploration. \\cite{zhang2020xq9} introduced MetaCURE (Meta Reinforcement Learning with Empowerment-Driven Exploration), which explicitly models an exploration policy learning problem separate from the exploitation policy. MetaCURE employs a novel empowerment-driven exploration objective that aims to maximize information gain for task identification, deriving a corresponding intrinsic reward. By learning separate, context-aware exploration and exploitation policies and sharing task inference knowledge, MetaCURE significantly enhances exploration efficiency in sparse-reward meta-RL tasks. This demonstrates how meta-learning can discover effective intrinsic reward functions that guide exploration towards truly informative experiences, addressing a key challenge in intrinsic motivation.\n\nFurthermore, meta-exploration has been integrated with other advanced RL paradigms. \\cite{rimon20243o6} presented MAMBA, a model-based approach to meta-RL that leverages world models for efficient exploration. By learning an internal model of the environment, MAMBA can plan and explore more effectively, leading to greater return and significantly improved sample efficiency (up to 15x) compared to existing meta-RL algorithms, especially in higher-dimensional domains. This highlights the synergy between learning environmental models and meta-learning exploration strategies. Complementing this, \\cite{goldie2024cuf} introduced Learned Optimization for Plasticity, Exploration and Non-stationarity (OPEN), which meta-learns an update rule (an optimizer) whose input features and output structure are informed by solutions to common RL difficulties, including exploration. OPEN's parameterization is flexible enough to use stochasticity for exploration, demonstrating that meta-learning can discover effective policy update mechanisms that inherently promote efficient exploration.\n\nIt is crucial to distinguish meta-exploration from merely adaptive exploration, where exploration parameters are tuned within a single learning process. While methods that dynamically adjust exploration probabilities based on metrics like information entropy \\cite{hu2020yhq} or ensemble learning for balancing exploration-exploitation ratios \\cite{shuai2025fq3} are valuable, they typically do not involve an outer-loop meta-training process to learn a generalizable exploration \\textit{strategy} across tasks. Such adaptive approaches are better categorized under integrated and adaptive exploration frameworks (Section 5.3), which focus on dynamic parameter adjustment rather than learning the exploration algorithm itself.\n\nIn conclusion, the shift towards learning exploration policies through meta-exploration represents a profound step towards truly autonomous and intelligent agents. These approaches, ranging from recurrent policies learning exploration behaviors across episodes to meta-learning structured exploration noise, intrinsic motivation signals, or even entire optimization rules, empower agents to generalize their exploration capabilities and achieve robust performance across diverse problem settings. However, significant challenges persist, including the computational expense of meta-training, the difficulty of defining appropriate and diverse task distributions for meta-learning, ensuring the learned exploration strategies generalize to truly novel and out-of-distribution tasks, and designing effective meta-objectives that capture desirable exploration properties. Future research will likely focus on developing more robust, sample-efficient, and generalizable meta-learning algorithms that can discover truly novel and effective exploration strategies across a wide spectrum of tasks, pushing the boundaries of autonomous discovery.\n\\subsection{Integrated and Adaptive Exploration Frameworks}\n\\label{sec:5\\_3\\_integrated\\_\\_and\\_\\_adaptive\\_exploration\\_frameworks}\n\nEffective exploration in complex, high-dimensional environments often transcends the capabilities of a single, static strategy, necessitating frameworks that dynamically combine multiple exploration techniques and adaptively select or blend them based on the current task, state, or learning progress. Moving beyond a 'one-size-fits-all' approach, these integrated frameworks aim to create highly versatile and robust agents capable of tackling a wide spectrum of exploration challenges, representing a key direction for developing general-purpose reinforcement learning (RL) agents. Robust intrinsic motivation, as discussed in Section 4, frequently serves as a foundational component within these integrated systems, providing internal reward signals (e.g., RND-style novelty \\cite{Burda2018} or episodic novelty \\cite{NGU}) that are then managed or orchestrated by higher-level adaptive mechanisms.\n\nA prominent approach to achieving adaptive exploration involves the use of meta-controllers that explicitly orchestrate a portfolio of exploration-exploitation policies. The seminal work of \\cite{Badia2020} on Agent57 exemplifies this paradigm, achieving state-of-the-art performance across diverse Atari games. Agent57 integrates multiple intrinsic motivation signals, including Random Network Distillation (RND) for life-long novelty and value-discrepancy-based novelty, with an adaptive exploration strategy managed by a meta-controller. This meta-controller dynamically selects from a range of exploration-exploitation policies, allowing the agent to adjust its behavior on the fly to suit the specific demands of each game and phase of learning. The strength of this approach lies in its ability to explicitly learn \\textit{how} to explore by selecting appropriate behaviors from a predefined set, offering significant flexibility. However, a limitation is its reliance on a hand-designed portfolio of base policies and the complexity of meta-learning the controller, which can be computationally intensive and may struggle if the optimal strategy is not represented within the initial portfolio.\n\nBeyond explicit meta-controllers, other integrated frameworks leverage ensemble methods or decoupled architectures to achieve robust and adaptive exploration. Ensemble-based approaches, for instance, integrate multiple policies or value functions to capture uncertainty or promote diversity. \\cite{jiang2023qmw} introduced Exploration via Distributional Ensemble (EDE), a method that encourages exploration of states with high epistemic uncertainty through an ensemble of Q-value distributions. This integration of multiple distributional estimates allows for a more nuanced understanding of uncertainty, leading to improved generalization in unseen environments. Similarly, \\cite{yang2022mx5} proposed Ensemble Proximal Policy Optimization (EPPO), which learns ensemble policies in an end-to-end manner, combining individual policies and the ensemble organically. EPPO adopts a diversity enhancement regularization over the policy space, which theoretically increases exploration efficacy and promotes generalization. In a different vein, \\cite{whitney2021xlu} proposed Decoupled Exploration and Exploitation Policies (DEEP), which structurally separates the task policy from the exploration policy. This decoupling allows for directed exploration to be highly effective for sample-efficient continuous control without incurring performance penalties in densely-rewarding environments. In contrast to Agent57's explicit switching mechanism, these ensemble and decoupled architectures offer a more implicit form of adaptation, either through aggregation of diverse perspectives or a clear separation of learning objectives, providing robustness but potentially less fine-grained, task-specific adaptation.\n\nFurther advancements in adaptive exploration leverage probabilistic, Bayesian, and reactive mechanisms to guide behavior based on uncertainty or environmental shifts. Bayesian approaches provide a principled framework for managing uncertainty, which can be directly used to drive exploration. \\cite{fu20220cl} introduced a model-based lifelong RL approach that estimates a hierarchical Bayesian posterior, which, combined with a sample-based Bayesian exploration procedure, adaptively increases sample efficiency across related tasks. Extending this, \\cite{li2023kgk} explored Bayesian exploration with Implicit Posteriori Parameter Distribution Optimization (IPPDO), modeling parameter uncertainty with an implicit distribution approximated by generative models, offering greater flexibility and improved sample efficiency. In the realm of randomized exploration, \\cite{ishfaq20235fo} and \\cite{ishfaq20245to} developed scalable Thompson sampling strategies using Langevin Monte Carlo and approximate sampling, respectively. These methods directly sample the Q-function from its posterior distribution, providing provably efficient and adaptive exploration by inherently balancing uncertainty and reward. For dynamic environments, \\cite{steinparz20220nl} proposed Reactive Exploration, designed to track and react to continual domain shifts in lifelong reinforcement learning, demonstrating adaptive policy updates in non-stationary settings. These probabilistic and reactive methods offer strong theoretical grounding for continuous adaptation but often face computational challenges in high-dimensional settings, requiring efficient approximation techniques.\n\nEmerging trends also point towards exploration as an emergent property from learned optimizers or large pre-trained models. \\cite{goldie2024cuf} introduced Learned Optimization for Plasticity, Exploration and Non-stationarity (OPEN), a meta-learned update rule whose input features and output structure are informed by solutions to RL difficulties, including the ability to use stochasticity for exploration. This represents a shift towards learning the optimization process itself, which implicitly includes adaptive exploration. In the context of large foundation models, \\cite{lee202337c} demonstrated that Decision-Pretrained Transformers (DPT) can exhibit emergent online exploration capabilities in-context, without explicit training for it. Building on this, \\cite{dai2024x3l} proposed In-context Exploration-Exploitation (ICEE), which optimizes the efficiency of in-context policy learning by performing exploration-exploitation trade-offs at inference time within a Transformer model. These approaches suggest a future where exploration is less explicitly engineered and more implicitly learned or emergent, potentially leading to highly generalizable agents, but raise questions about control, interpretability, and the sample efficiency required for pre-training.\n\nIn conclusion, integrated and adaptive exploration frameworks represent a significant leap towards developing general-purpose RL agents. By combining robust intrinsic motivation signals, explicit meta-controllers, ensemble and decoupled architectures, principled Bayesian and probabilistic methods, and leveraging emergent capabilities from learned optimizers and large models, these frameworks move beyond static heuristics to dynamically adjust their exploration behaviors. While significant progress has been made, future directions include developing more theoretically grounded guarantees for these complex adaptive systems, enhancing their computational efficiency and scalability, and ensuring robust generalization to truly novel and open-ended environments where the optimal exploration strategy might not be easily predefined or learned from limited prior experience. A key unresolved challenge is the automated discovery of an optimal portfolio of exploration strategies for meta-controllers, as current approaches often rely on hand-designed components.\n\\subsection{Population-Based and Evolutionary Exploration}\n\\label{sec:5\\_4\\_population-based\\_\\_and\\_\\_evolutionary\\_exploration}\n\nThe inherent challenge of the exploration-exploitation dilemma in Reinforcement Learning (RL) often leads single agents to converge prematurely to sub-optimal policies, particularly in environments characterized by sparse rewards or complex, multi-modal reward landscapes. To overcome these limitations, a distinct paradigm has emerged that leverages populations of agents or evolutionary algorithms to foster broader, more robust exploration and facilitate global search. These approaches represent a meta-level solution, structuring the learning system itself for enhanced discovery.\n\nEarly precursors to population-based exploration can be found in Neuroevolution, where neural network architectures and weights are optimized using evolutionary algorithms. Methods like NEAT (NeuroEvolution of Augmenting Topologies) \\cite{stanley2002evolving} demonstrated the power of evolving diverse populations of networks to solve complex control tasks, implicitly performing exploration by searching a vast hypothesis space. More recently, Evolution Strategies (ES) have gained prominence as a scalable black-box optimization technique for deep RL, capable of training deep neural networks efficiently due to their high parallelizability \\cite{salimans2017evolution}. However, standard ES can struggle with sparse or deceptive reward landscapes, necessitating directed exploration. To address this, \\cite{conti2017cr2} introduced methods like Novelty Search with Evolution Strategies (NS-ES) and Quality Diversity (QD) algorithms hybridized with ES. These approaches maintain a population of novelty-seeking agents, rewarding exploration of novel behaviors rather than just high performance, thereby enabling ES to avoid local optima and achieve higher performance on challenging deep RL tasks like Atari games and simulated robot locomotion.\n\nA significant advancement in population-based methods for deep RL is Population Based Training (PBT) \\cite{jaderberg2017population}. PBT concurrently trains a population of agents, each with its own set of hyperparameters and model weights. Unlike traditional grid search or random search, PBT dynamically adapts hyperparameters during training by periodically evaluating agents, exploiting well-performing ones (copying their weights and hyperparameters) and exploring new hyperparameter configurations for underperforming ones. This asynchronous \"exploit-and-explore\" strategy allows PBT to discover robust hyperparameter schedules and model weights simultaneously, leading to faster training and improved final performance across diverse tasks, including complex deep RL benchmarks. PBT's strength lies in its ability to adaptively tune both learning processes and agent policies, making it highly effective for complex, high-dimensional problems.\n\nBuilding on the strengths of both evolutionary algorithms and gradient-based RL, Evolutionary Reinforcement Learning (ERL) frameworks have emerged as a powerful hybrid paradigm. These methods typically combine the global search capabilities of evolutionary algorithms with the local optimization efficiency of gradient-based RL. Early ERL approaches, such as those by \\cite{khadka2018evolutionary} and CEM-RL \\cite{pourchot2019cemrl}, often involve evolving a population of actor networks, while a shared critic network (trained via gradient descent) provides value estimates to guide both the evolutionary process and individual actor updates. This hybrid approach aims to leverage the exploration benefits of evolution (e.g., escaping local optima, maintaining diversity) and the sample efficiency of RL. For instance, \\cite{sun2024edc} proposed a modified ERL method for multi-agent region protection, amalgamating Differential Evolution (DE) for diverse sample exploration and overcoming sparse rewards with Multi-Agent Deep Deterministic Policy Gradient (MADDPG) for training defenders and expediting DE convergence.\n\nA more recent advancement in this line is the Two-Stage Evolutionary Reinforcement Learning (TERL) framework proposed by \\cite{zhu2024sb0}. TERL addresses a key limitation of prior ERL methods, which often evolve only actor networks, thereby constraining exploration if a single critic network falls into local optima. Instead, TERL maintains and optimizes a population of \\textit{complete RL agents}, each comprising both an actor and a critic network. This design enables more independent and diverse exploration by each individual, mitigating the risk of premature convergence to suboptimal policies dictated by a flawed shared critic. The TERL framework operates through a novel two-stage learning process: an initial \"Exploration Stage\" where all individuals learn independently, optimized by a hybrid approach combining gradient-based RL updates with meta-optimization techniques like Particle Swarm Optimization (PSO). This stage emphasizes diversification, with agents sharing information efficiently through a common replay buffer, which helps propagate beneficial experiences across the population. Following this, the \"Exploitation Stage\" focuses on refining the best-performing individual from the population through concentrated RL-based updates, while the remaining individuals continue to undergo PSO to further diversify the replay buffer. This dynamic allocation of computational resources and tailored optimization strategies across stages allows TERL to effectively balance the exploration-exploitation dilemma.\n\nDespite their advantages, population-based and evolutionary methods introduce their own set of challenges. Computational cost is a primary concern, as maintaining and training multiple agents or evolving large populations can be resource-intensive, although parallelization strategies (like those in ES and PBT) mitigate this. Furthermore, the integration of diverse data from population optimization into off-policy RL algorithms, particularly through shared replay buffers, can introduce instability and even degrade performance, as highlighted by \\cite{zheng2023u9k}. This issue arises because population data, while diverse, might not align with the on-policy distribution expected by some RL algorithms, leading to an \"overlooked error.\" To remedy this, \\cite{zheng2023u9k} proposed a double replay buffer design to provide more on-policy data, demonstrating the need for careful architectural considerations when combining these paradigms. The choice between PBT's hyperparameter evolution and ERL's policy evolution also presents a trade-off: PBT excels at finding robust training configurations, while ERL directly optimizes policy parameters, often leading to more direct policy improvement.\n\nIn conclusion, population-based and evolutionary exploration methods offer a compelling meta-level solution to the challenges of exploration in complex RL environments. By evolving populations of complete RL agents, dynamically adapting hyperparameters, or employing hybrid optimization strategies, these approaches enable more diverse learning trajectories and a more robust search for optimal policies, moving beyond the limitations of single-agent exploration heuristics. Future research could explore more sophisticated mechanisms for inter-agent information sharing, investigate adaptive intrinsic motivation signals within these population-based frameworks, or extend these concepts to multi-task and open-ended learning scenarios, further enhancing the adaptability and generalization capabilities of RL agents.\n\n\n\\label{sec:specialized_contexts_and_applications_of_exploration}\n\n\\section{Specialized Contexts and Applications of Exploration}\n\\label{sec:specialized\\_contexts\\_\\_and\\_\\_applications\\_of\\_exploration}\n\n\\subsection{Exploration in Offline Reinforcement Learning}\n\\label{sec:6\\_1\\_exploration\\_in\\_offline\\_reinforcement\\_learning}\n\nIn offline Reinforcement Learning (RL), the agent is tasked with learning an optimal policy solely from a fixed, pre-collected dataset, fundamentally precluding any further active interaction with the environment. This paradigm shift introduces unique challenges for exploration, as traditional active exploration strategies, which involve generating new experiences, are inherently impossible. Instead, the core problem transforms into managing \\textit{distributional shift} and preventing the policy from querying actions that are out-of-distribution (OOD) relative to the dataset. The focus shifts from active exploration to ensuring 'conservative learning' within the boundaries of the existing data distribution, aiming to identify reliable regions for policy improvement while rigorously avoiding OOD actions that could lead to unreliable value estimates or unsafe behaviors due to extrapolation errors. This conservative approach is paramount for real-world applications where data collection is costly, risky, or simply not feasible during the learning process.\n\nThe foundational approaches to offline RL primarily address the distributional shift problem through two main mechanisms: policy constraints and value function pessimism. Seminal works like Batch-Constrained Q-Learning (BCQ) \\cite{fujimoto2019off} introduced explicit policy constraints, regularizing the learned policy to stay close to the behavior policy that generated the dataset. This is typically achieved by adding a regularization term (e.g., KL-divergence) to the policy objective, ensuring that the agent does not venture into unobserved state-action pairs. Similarly, Behavior Regularized Actor Critic (BRAC) \\cite{wu2019behavior} further explored various forms of behavior regularization to mitigate the distributional shift. While effective in keeping the policy close to the data, these methods can sometimes limit the discovery of truly optimal policies if the behavior policy was suboptimal.\n\nA complementary and highly influential approach is Conservative Q-Learning (CQL) \\cite{kumar2020conservative}. CQL tackles the distributional shift by explicitly enforcing pessimism in the value function estimation. It achieves this by adding a penalty to the Q-values of OOD actions, ensuring that the learned Q-function provides a lower bound on the true Q-values. This prevents overestimation of action values for unseen actions, which is a common failure mode in offline RL. While BCQ primarily constrains the policy directly, CQL intervenes at the value function level, indirectly shaping the policy by making OOD actions less attractive. This distinction highlights different points of intervention for ensuring conservatism.\n\nBuilding upon these foundations, subsequent methods have leveraged uncertainty estimation to guide conservative learning more explicitly. The Uncertainty Weighted Actor-Critic (UWAC) \\cite{wu2021r67} explicitly incorporates uncertainty treatment by detecting OOD state-action pairs and down-weighting their contribution in the training objectives. Utilizing a practical dropout-based uncertainty estimation, UWAC laid groundwork for robust learning by mitigating the impact of unreliable data points. Similarly, \\cite{rezaeifar20211eu} conceptualized offline RL as \"anti-exploration,\" proposing to subtract a prediction-based exploration bonus from the reward. This innovative approach encourages the policy to remain within the support of the dataset by penalizing actions whose consequences cannot be reliably predicted, effectively extending pessimism-based offline RL methods to deep learning settings. Further, \\cite{zhang20244ty} introduced an Entropy-regularized Diffusion Policy with Q-Ensembles, where robust policy improvement is achieved by learning the lower confidence bound of Q-ensembles. This implicitly accounts for uncertainty in value estimates, mitigating the impact of inaccurate value functions from OOD data, while an entropy regularizer improves \"exploration\" within the offline dataset by encouraging diverse actions within the observed distribution.\n\nSimpler yet effective mechanisms have also emerged to ensure in-sample learning. \\cite{ma2024jej} proposed Improving Offline Reinforcement Learning with in-Sample Advantage Regularization (ISAR), which adapts offline RL to robotic manipulation with minimal changes. ISAR learns the state-value function exclusively from dataset samples, then calculates the advantage function based on this in-sample estimation and adds a behavior cloning regularization term. This method effectively mitigates the impact of unseen actions without introducing complex hyperparameters, offering a straightforward approach to conservative learning that implicitly handles OOD issues.\n\nA significant challenge, particularly in model-based offline RL (MBRL), is addressing biased exploration during the synthetic trajectory generation phase. Standard maximum entropy exploration mechanisms, often adopted from online RL, can lead to skewed data distributions and impaired performance when applied to learned dynamics models. To tackle this, \\cite{wu2024mak} introduced OCEAN-MBRL (Offline Conservative ExplorAtioN for Model-Based Offline Reinforcement Learning), a novel plug-in rollout approach. OCEAN explicitly decouples exploration from exploitation, introducing a principled, conservative exploration strategy guided by an ensemble of dynamics models for uncertainty estimation. It employs three key constraints: a state evaluation constraint to explore only in low-uncertainty regions, an exploration range constraint to select conservative actions, and a trajectory truncation constraint to limit rollouts in high-uncertainty areas. This comprehensive approach significantly enhances the stability and performance of existing MBRL algorithms by ensuring that exploration within the learned model remains reliable and does not generate new, potentially unsafe, out-of-distribution experiences.\n\nThe transition from offline learning to online fine-tuning presents another critical juncture for conservative exploration. While initial conservatism is vital for stable offline learning, a purely pessimistic policy might fail to discover better actions during online interaction. The Simple Unified uNcertainty-Guided (SUNG) framework for offline-to-online RL \\cite{guo20233sd} quantifies uncertainty via a VAE-based state-action visitation density estimator. SUNG's adaptive exploitation method applies conservative offline RL objectives to high-uncertainty samples and standard online RL objectives to low-uncertainty samples, demonstrating a sophisticated use of uncertainty to dynamically adjust the degree of conservatism. Building on this, \\cite{guo2024sba} proposed Optimistic Exploration and Meta Adaptation (OEMA) for sample-efficient offline-to-online RL. OEMA employs an optimistic exploration strategy, adhering to the principle of optimism in the face of uncertainty, allowing agents to sufficiently explore while reducing distributional shift through meta-learning. Providing a theoretical underpinning for such adaptive strategies, \\cite{hu2024085} showed that Bayesian design principles are crucial for offline-to-online fine-tuning, suggesting that a probability-matching agent, rather than purely optimistic or pessimistic ones, can avoid sudden performance drops while being guaranteed to find the optimal policy.\n\nThe literature on exploration in offline RL has thus evolved from foundational methods that strictly constrain policies or penalize OOD value estimates to sophisticated, explicit conservative exploration strategies, particularly in model-based settings and during the offline-to-online transition. While significant progress has been made in ensuring learning remains robust and effective without active interaction, a persistent challenge lies in the accuracy, reliability, and computational efficiency of uncertainty estimation itself. Future research directions could focus on developing more robust and scalable uncertainty quantification methods, as well as adaptive mechanisms that dynamically adjust conservative exploration constraints based on the evolving confidence in the learned policy and model, effectively balancing the need for safety with the potential for performance improvement.\n\\subsection{Expert-Guided and Demonstration-Based Exploration}\n\\label{sec:6\\_2\\_expert-guided\\_\\_and\\_\\_demonstration-based\\_exploration}\n\nEfficient exploration remains a formidable challenge in reinforcement learning (RL), particularly in complex, high-dimensional, and sparse-reward environments. To mitigate this, a significant body of research focuses on leveraging expert knowledge or demonstrations to guide and accelerate the exploration process, thereby improving sample efficiency and policy robustness. This paradigm, often termed Reinforcement Learning from Demonstrations (RLfD), seeks to bridge the gap between purely autonomous trial-and-error and the wealth of human or simulated expertise.\n\nEarly efforts established the foundational utility of demonstrations in overcoming exploration hurdles. \\cite{nair2017crs} demonstrated that even a small set of expert demonstrations, when integrated with RL algorithms like Deep Deterministic Policy Gradients (DDPG) and Hindsight Experience Replay (HER), could provide an order of magnitude speedup in learning complex, continuous control robotics tasks with sparse rewards. These methods often relied on static, \\textit{offline} datasets of expert trajectories, which provided initial guidance but presented inherent limitations. Building upon this, \\cite{uchendu20221h1} proposed Jump-Start Reinforcement Learning (JSRL), which uses a \"guide-policy\" derived from offline data or demonstrations to form a curriculum of starting states for an \"exploration-policy,\" significantly improving sample complexity, especially in data-scarce regimes. Similarly, \\cite{hansen2022jm2} highlighted key ingredients for accelerating visual model-based RL with demonstrations, including policy pretraining, targeted exploration, and oversampling demonstration data, leading to substantial performance gains in sparse reward tasks.\n\nBeyond direct trajectory imitation, expert knowledge can also be integrated through symbolic rules or domain-specific insights. For instance, \\cite{hou2021c2r} introduced Rule-Aware Reinforcement Learning (RARL) for knowledge graph reasoning, injecting high-quality symbolic rules into the model's reasoning process to alleviate sparse rewards and prevent spurious paths. \\cite{mazumder2022deb} proposed using \"state-action permissibility\" knowledge to guide exploration, drastically speeding up deep RL training by identifying and avoiding impermissible actions. In a similar vein, \\cite{liu20228r4} combined human knowledge-based rule bases with imitation learning pre-training (ILDN) and safe RL to enhance efficiency and generalization in large-scale adversarial scenarios. Furthermore, theoretical frameworks have explored how exploration itself can be framed as a utility to be optimized, which demonstrations can implicitly help achieve \\cite{zhang2020o5t, santi2024hct}.\n\nDespite the benefits of offline demonstrations, a persistent challenge is the \"distribution gap\" \\cite{coelho2024oa6}. This occurs when the agent's policy deviates from the expert's, leading it into states not covered by the static demonstration dataset, thereby hindering generalization and robustness. To address this, research has shifted towards more dynamic and interactive integration of expert knowledge. \\cite{ball20235zm} showed that with minimal modifications, existing off-policy RL algorithms could effectively leverage offline data (including demonstrations) for online learning, achieving significant performance improvements. This offline-to-online fine-tuning paradigm is crucial for real-world applications \\cite{rafailov2024wtw, hu2024085}, where policies pre-trained on demonstrations need to adapt to novel online experiences. For example, \\cite{lu2025j7f} presented VLA-RL, an approach that uses online reinforcement learning to improve pretrained Vision-Language-Action (VLA) models, specifically addressing out-of-distribution failures that arise from limited offline data in robotic manipulation. Even leveraging \"negative demonstrations\" or failed experiences can guide exploration by showing what \\textit{not} to do, as demonstrated by \\cite{wu20248f9} in sparse reward environments.\n\nA significant advancement in bridging the distribution gap and enhancing exploration efficiency comes from dynamically interacting with experts. \\cite{hou20248b2} introduced EARLY, an active RL from demonstrations algorithm where the agent intelligently queries for episodic demonstrations based on its trajectory-level uncertainty. This approach makes expert guidance more targeted and resource-efficient by requesting help only when needed. The pinnacle of this dynamic interaction is exemplified by \\cite{coelho2024oa6} with RLfOLD (Reinforcement Learning from Online Demonstrations) for urban autonomous driving. RLfOLD introduces the novel concept of \\textit{online demonstrations}, which are dynamically collected from a simulator's privileged information during the agent's active exploration. These demonstrations are seamlessly integrated into a single replay buffer alongside agent experiences, directly addressing the distribution gap by providing contextually relevant expert guidance. The framework utilizes a modified Soft Actor-Critic (SAC) algorithm with a dual standard deviation policy network, outputting distinct $\\sigma\\_{RL}$ for exploration and $\\sigma\\_{IL}$ for imitation, allowing for adaptive balancing of these learning objectives. Furthermore, an uncertainty-based mechanism selectively invokes the online expert to guide the agent in challenging situations, making exploration more targeted and efficient. RLfOLD demonstrated superior performance in the CARLA NoCrash benchmark with significantly fewer resources, highlighting its effectiveness and efficiency in complex, real-time domains.\n\nIn conclusion, the intellectual trajectory of expert-guided exploration has evolved from static, offline demonstration datasets to dynamic, interactive, and online expert guidance. This progression effectively addresses critical challenges such as the distribution gap and sample inefficiency, particularly in safety-critical applications like autonomous driving and robotics. While methods leveraging offline demonstrations provide crucial initial boosts, the trend towards online and active expert interaction, exemplified by frameworks like RLfOLD, represents a significant step towards more robust, adaptive, and generalizable RL systems. Future research will likely focus on refining the mechanisms for generating and integrating online expert guidance, especially in real-world scenarios where \"privileged information\" is unavailable, and developing more sophisticated expert models that can provide nuanced and context-aware interventions.\n\\subsection{Exploration in Dynamic and Expanding Environments}\n\\label{sec:6\\_3\\_exploration\\_in\\_dynamic\\_\\_and\\_\\_exp\\_and\\_ing\\_environments}\n\nThe challenge of efficient exploration intensifies significantly in Reinforcement Learning (RL) when agents must operate in environments where the state and action spaces are not static but continually expand or evolve \\cite{yang2021ngm}. Unlike traditional Markov Decision Processes (MDPs) that assume fixed state and action sets, real-world systems often undergo updates, introducing novel states, actions, or even entire sub-environments. This dynamic nature necessitates exploration strategies that can efficiently discover and integrate new information without incurring computationally prohibitive retraining costs or suffering from catastrophic forgetting of previously acquired knowledge. This specialized context forms a crucial subset of lifelong and continual learning, where agents must adapt to an unending stream of tasks or environmental changes \\cite{janjua2024yhk, fu20220cl}.\n\nThe concept of Incremental Reinforcement Learning (Incremental RL) has emerged to specifically address this challenge, focusing on how agents can efficiently adapt their policies to newly introduced states and actions. While lifelong learning broadly concerns sequential task learning and knowledge transfer \\cite{fu20220cl, woczyk20220mn}, Incremental RL distinguishes itself by tackling the explicit \\textit{expansion} of the underlying MDP structure. A seminal contribution by \\cite{ding2023whs} formally defines Incremental RL and proposes the Dual-Adaptive $\\epsilon$-greedy Exploration (DAE) algorithm. This approach confronts the inherent inefficiency of standard exploration methods and the strong inductive biases that can arise from extensive prior learning, which often hinder adaptation to expanding environments. DAE employs a Meta Policy ($\\Psi$) to adaptively determine a state-dependent $\\epsilon$ by assessing the exploration convergence of a state (via TD-Error rate), thereby deciding \\textit{when} to explore. Concurrently, an Explorer ($\\Phi$) guides the agent to prioritize \"least-tried\" actions by estimating their relative frequencies, addressing \\textit{what} to explore. Crucially, DAE also incorporates strategies for incrementally adapting deep Q-networks by reusing trained policies and intelligently initializing new neurons and Q-values. This architectural flexibility, combined with the dual-adaptive exploration mechanism, significantly reduces training overhead and enables robust adaptation to expanding search spaces without retraining from scratch.\n\nThe need for adaptive exploration in dynamic settings is also highlighted by research into non-stationary environments, which share some conceptual overlaps with expanding environments, though they differ mechanistically. For instance, \\cite{steinparz20220nl} introduces Reactive Exploration to cope with continual domain shifts in lifelong reinforcement learning. This work demonstrates that policy-gradient methods benefit from strategies that track and react to non-stationarities, such as changes in reward functions or environmental dynamics, within an otherwise fixed state-action space. While Reactive Exploration focuses on adapting to \\textit{changes} in existing elements, DAE specifically addresses the \\textit{addition} of new states and actions. However, both underscore the broader necessity for exploration strategies that can actively adapt to environmental changes, rather than relying on static or pre-defined exploration schedules. Similarly, the importance of exploration for generalization to new, unseen environments, as explored by \\cite{jiang2023qmw}, aligns with the goals of Incremental RL. Their Exploration via Distributional Ensemble (EDE) method encourages exploration of states with high epistemic uncertainty, which is crucial for acquiring knowledge that aids decision-making in novel situations. While EDE aims to generalize within a potentially vast but fixed environment, DAE's focus is on efficiently integrating entirely new components into the agent's operational space, a distinction critical for truly open-ended learning systems \\cite{janjua2024yhk}.\n\nOther advanced exploration techniques, such as those leveraging intrinsic motivation \\cite{houthooft2016yee} or information gain maximization \\cite{aubret2022inh}, aim to improve exploration efficiency by incentivizing agents to visit novel states or reduce uncertainty about the environment dynamics. For example, Variational Information Maximizing Exploration (VIME) \\cite{houthooft2016yee} uses Bayesian neural networks to maximize information gain about environment dynamics. While powerful in static high-dimensional environments, their direct applicability and scalability to \\textit{continually expanding} state and action spaces present unique challenges. Prediction-error-based methods, like those underlying many intrinsic motivation approaches, may struggle when the underlying dynamics model requires continuous architectural restructuring rather than just parameter updates. The sudden introduction of entirely new states or actions can render existing prediction models inaccurate or incomplete, requiring significant re-learning or architectural modifications that are not inherently handled by these methods. Count-based or density-based novelty methods, while effective for discovering unvisited regions, would need robust mechanisms to distinguish truly \\textit{new} states/actions from merely \\textit{unvisited} ones within the previously known space, and to efficiently update their density estimations for an ever-growing space. DAE's explicit focus on identifying and prioritizing newly available actions and states, overcoming the inductive bias from extensive prior learning, offers a more targeted solution to these architectural and knowledge-transfer challenges.\n\nThe integration of such adaptive exploration strategies with broader lifelong learning frameworks is a critical direction. Lifelong RL methods, such as model-based Bayesian approaches that estimate a hierarchical posterior to distill common task structures \\cite{fu20220cl}, offer mechanisms for backward transfer and efficient learning across related tasks. However, these often assume a fixed set of potential tasks or a stable underlying model structure. The challenge of Incremental RL lies in the dynamic \\textit{growth} of this structure, requiring not just transfer but also efficient architectural expansion and robust exploration of truly novel elements. Learned optimization methods, which meta-learn update rules to handle non-stationarity, plasticity loss, and exploration \\cite{goldie2024cuf}, offer a promising avenue by building adaptability directly into the learning process, potentially complementing DAE's specific exploration mechanisms.\n\nIn conclusion, the progression towards \"Exploration in Dynamic and Expanding Environments\" marks a crucial intellectual shift in Reinforcement Learning, moving beyond the static MDP assumption towards more realistic, evolving systems. While foundational exploration methods provide general tools, the work on Incremental RL, particularly the Dual-Adaptive $\\epsilon$-greedy Exploration \\cite{ding2023whs}, offers a targeted solution for environments where state and action spaces continually grow. Future research in this area will likely focus on extending these adaptive exploration strategies to more complex, partially observable, or even multi-agent expanding environments, further enhancing the lifelong learning capabilities of RL agents in truly dynamic real-world scenarios, and integrating them more deeply with meta-learning and continual learning paradigms to address catastrophic forgetting and efficient knowledge transfer in ever-growing systems.\n\\subsection{Safety-Aware Exploration}\n\\label{sec:6\\_4\\_safety-aware\\_exploration}\n\nThe exploration phase in reinforcement learning (RL) is critical for discovering optimal policies, yet in real-world, safety-critical applications, unconstrained exploration can lead to catastrophic outcomes and raise significant ethical concerns regarding accountability, fairness in decision-making under risk, and the potential for unforeseen negative side-effects in human-inhabited environments. This subsection delves into methods designed to ensure safety during exploration, navigating the inherent tension between the need for aggressive exploration to achieve optimal performance and the imperative to maintain safe operation. The problem is often formally cast within the framework of Constrained Markov Decision Processes (CMDPs), where an agent aims to maximize cumulative reward while simultaneously satisfying constraints on cumulative costs, such as safety violations \\cite{altman1999constrained}. This formalism provides a robust theoretical foundation for developing algorithms that provide safety guarantees during the learning process.\n\nEarly efforts to integrate safety into RL exploration focused on establishing explicit boundaries and constraints. A foundational approach involves \"safety layers\" or \"shielding,\" which act as guardians, restricting the agent's actions or states to predefined safe regions, thereby preventing the agent from entering hazardous situations during learning \\cite{Stachurski2008}. While early works laid the groundwork, modern deep RL has seen significant advancements, notably with methods like those proposed by \\cite{alshiekh2018safe}, which formally integrate shielding into deep RL agents. These methods enforce explicit safety constraints, ensuring exploration is guided within a permissible operational envelope, effectively mitigating the risk of catastrophic failures. However, a key limitation of static safety layers is their potential to be overly conservative, which can severely restrict the agent's exploration capabilities and prevent the discovery of truly optimal, yet initially unknown, safe behaviors. This conservativeness often stems from the difficulty of accurately predefining safe regions in complex, high-dimensional environments, or from worst-case assumptions made to guarantee safety.\n\nAddressing the limitations of static constraints and the inherent conservativeness, more recent research has explored dynamic and learned safety mechanisms, often decoupling the concerns of task performance and safety. \\cite{yu20222xi} introduced SEditor, a two-policy approach that learns a \"safety editor policy\" to transform potentially unsafe actions proposed by a utility-maximizing policy into safe ones. SEditor represents a conceptual shift from static, predefined shields to a learned, dynamic safety filter, allowing for more nuanced and state-dependent safety interventions. This method moves beyond simplified safety models, enabling the safety editor to learn complex safety functions, effectively acting as a dynamic shield. While SEditor demonstrates significantly lower constraint violation rates and maintains high utility, its effectiveness relies on the ability to train an accurate safety editor policy, which can be challenging in highly dynamic or unpredictable environments.\n\nFurther advancing dynamic safety, \\cite{thananjeyan2020d20} introduced Recovery RL, which first leverages offline data to learn about constraint-violating zones. It then employs a task policy for reward maximization and a dedicated recovery policy that activates to guide the agent back to safety when constraint violation is likely. This dual-policy structure allows for more aggressive exploration by the task policy, relying on the learned recovery mechanism to prevent unsafe outcomes. Unlike SEditor, which modifies actions \\textit{before} execution, Recovery RL focuses on \\textit{recovering} from potentially unsafe trajectories, offering a different trade-off between proactive prevention and reactive correction. Recovery RL demonstrates superior efficiency in balancing task success and constraint adherence in complex, contact-rich manipulation tasks and image-based navigation, even on physical robots, by allowing the task policy greater freedom. Similarly, \\cite{zhang2023wqi} proposed a method for safe RL with dead-ends avoidance and recovery. This approach constructs a boundary to discriminate between safe and unsafe states, equivalent to distinguishing dead-end states, thereby ensuring maximum safe exploration with minimal limitation. Like Recovery RL, it utilizes a decoupled framework with a task policy and a pre-trained recovery policy, along with a safety critic, to ensure safe actions during online training. This strategy aims to achieve better task performance with fewer safety violations by carefully delineating the extent of guaranteed safe exploration, offering a more precise definition of \"safe\" exploration boundaries.\n\nAnother powerful paradigm for guaranteeing safety, particularly in continuous control systems, draws from control theory: Lyapunov stability and Control Barrier Functions (CBFs). These methods provide formal guarantees that a system's state will remain within a predefined safe set. Control Barrier Functions (CBFs) are functions of the state that define a safe set and whose derivatives can be constrained to render this set forward-invariant, thus preventing the agent from leaving it. \\cite{zhang2022dgg} proposed a Barrier Lyapunov Function-based safe RL (BLF-SRL) algorithm for autonomous vehicles. This approach integrates BLF items into an optimized backstepping control method, constraining state variables within a designed safety region during learning. By decomposing optimal control with BLF items, it achieves safe exploration while learning adaptive uncertain items, ensuring both safety and performance optimization in safety-critical domains. While control-theoretic methods like BLF-SRL offer strong, often deterministic, safety guarantees, they typically require an accurate model of the system dynamics and specific assumptions about the environment, a limitation not shared by model-free, data-driven approaches like Recovery RL, which in turn provide only probabilistic safety assurances. Addressing this model dependency, \\cite{cheng20224w2} presented a method using Disturbance-Observer-Based Control Barrier Functions (DOB-CBFs). This approach avoids explicit model learning by leveraging disturbance observers to accurately estimate the pointwise value of uncertainty, which is then incorporated into a robust CBF condition. This allows for less conservative safety filters, especially during early learning, by effectively handling unknown disturbances without requiring extensive model training.\n\nThe inherent uncertainty in exploration necessitates risk-sensitive approaches. Building on Bayesian exploration principles (as discussed in Section 3.2), some approaches use uncertainty to define credible intervals for constraint satisfaction, leading to more principled conservative exploration. \\cite{zhang2024ppn} introduced Lag-U, an uncertainty-augmented Lagrangian safe RL algorithm for autonomous driving. This method uses deep ensembles to estimate epistemic uncertainty, which is then used to encourage exploration and learn a risk-sensitive policy by adaptively modifying safety constraints. Furthermore, it incorporates an intervention assurance mechanism based on quantified uncertainty to select safer actions during deployment. This allows for a better trade-off between efficiency and risk avoidance, preventing overly conservative policies by making safety decisions based on the agent's confidence in its predictions. Complementing this, \\cite{yu2022bo5} proposed a distributional reachability certificate (DRC) to address model uncertainties and characterize robust persistently safe states. Their framework builds a shield policy based on DRC to minimize constraint violations, especially during training, by considering the distribution of potential long-term constraint violations, thereby enhancing safety robustness against model uncertainty. Beyond explicit uncertainty estimation, some methods directly manipulate the learning process to balance reward and safety. \\cite{gu2024fu3} addressed the conflict between reward and safety gradients, proposing a soft switching policy optimization method. By analyzing and manipulating these gradients, their framework aims to achieve a better balance between optimizing for rewards and adhering to safety constraints, offering a more direct way to mitigate the inherent conflict compared to simply adding penalty terms in CMDPs.\n\nIn scenarios where some safety signals are available in a controlled environment, \"guided safe exploration\" can be employed to facilitate safe transfer learning. \\cite{yang2023n56} proposed a method where a \"guide\" agent learns to explore safely without external rewards in a controlled environment where safety signals are available. This guide then helps compose a safe behavior policy for a \"student\" agent in a target task where safety violations are prohibited. The student policy is regularized towards the guide while it is unreliable, gradually reducing the guide's influence as it learns. This approach enables safe transfer learning and faster problem-solving in the target task, highlighting the utility of leveraging prior safety knowledge to bootstrap safe exploration in novel, safety-critical settings.\n\nIn summary, safety-aware exploration has evolved significantly, moving from static, predefined safety layers and formal constraint satisfaction (CMDPs) to dynamic, learned recovery mechanisms, control-theoretic guarantees (BLFs, CBFs), and risk-sensitive approaches leveraging uncertainty quantification. The field continues to grapple with the fundamental tension between encouraging sufficient exploration for optimal learning and guaranteeing safety in real-world, safety-critical applications. Future directions will likely involve integrating more sophisticated formal verification techniques, developing robust and scalable uncertainty quantification for proactive safety prediction, designing intrinsically safe exploration strategies that adhere to ethical guidelines from the outset, and further refining gradient manipulation techniques to optimally balance conflicting objectives. The goal remains to enable autonomous agents to learn effectively without compromising human or environmental safety, ensuring responsible deployment in increasingly complex and sensitive domains.\n\n\n\\label{sec:conclusion_and_future_directions}\n\n\\section{Conclusion and Future Directions}\n\\label{sec:conclusion\\_\\_and\\_\\_future\\_directions}\n\n\\subsection{Summary of Key Advancements}\n\\label{sec:7\\_1\\_summary\\_of\\_key\\_advancements}\n\nThe journey of exploration in reinforcement learning (RL) reflects a continuous and sophisticated evolution, driven by the persistent challenge of balancing the acquisition of new information with the exploitation of known optimal actions. This review has traced a narrative arc from foundational heuristics to theoretically grounded algorithms, and subsequently to scalable intrinsic motivation techniques, culminating in adaptive, learned exploration strategies tailored for specialized contexts. This progression underscores the field's relentless innovation in addressing the increasing complexity of environments and the inherent difficulties of the exploration-exploitation dilemma.\n\nEarly attempts to navigate the exploration-exploitation trade-off, as discussed in Section 2, centered on basic heuristics like $\\epsilon$-greedy and the implicit exploration benefits of model-based planning architectures such as Dyna-Q. Concurrently, explicit exploration bonuses, often count-based, provided direct incentives for visiting novel states. While effective in tabular settings, these methods faced significant scalability challenges in high-dimensional or continuous state spaces. For instance, early attempts to generalize count-based exploration to deep RL, such as mapping states to hash codes \\cite{tang20166wr}, provided a surprisingly strong baseline but highlighted the need for more robust, scalable novelty detection mechanisms.\n\nA pivotal shift occurred with the development of theoretically grounded exploration strategies (Section 3), which sought to provide provable guarantees on learning efficiency. Principles like \"optimism in the face of uncertainty\" (OFU), embodied in algorithms like R-Max, offered PAC-MDP guarantees by optimistically valuing unexplored regions. Bayesian approaches, notably Thompson Sampling, provided a principled framework for managing uncertainty by maintaining posterior distributions over models or value functions. Further advancing this theoretical rigor, information-theoretic methods emerged, guiding exploration by maximizing knowledge gain about the environment or optimal policy. Techniques such as Variational Information Maximizing Exploration (VIME) \\cite{houthooft2016yee} and MaxInfoRL \\cite{sukhija2024zz8} exemplify this, rewarding agents for transitions that significantly reduce uncertainty or improve the agent's internal model. This information-centric view, as surveyed by \\cite{aubret2022inh}, moved beyond simple novelty to a more sophisticated understanding of learning progress. However, the computational complexity of maintaining accurate uncertainty estimates and the limitations of optimism in scenarios with partially observable rewards \\cite{parisi2024u3o} often restricted their applicability to simpler environments.\n\nThe advent of deep reinforcement learning necessitated a paradigm shift, leading to the widespread adoption of intrinsic motivation techniques (Section 4). These methods generate internal reward signals to drive exploration, proving crucial in environments with sparse external rewards. Initial concepts of curiosity and novelty-seeking evolved into scalable approaches. Count-based methods were adapted for high-dimensional spaces using pseudo-counts and density models. A significant breakthrough came with prediction-error based curiosity, where agents are rewarded for encountering surprising or unpredictable observations, as seen in the Intrinsic Curiosity Module (ICM) \\cite{li2019tj1, zhelo2018wi8}. This directed exploration towards aspects of the environment that improve the agent's internal dynamics model. To address the \"noisy TV\" problem, where agents are perpetually attracted to uninformative stochasticity, robust intrinsic reward mechanisms like Random Network Distillation (RND) were developed, effectively filtering out irrelevant noise and focusing exploration on truly learnable novelty. The introduction of Random Latent Exploration (RLE) \\cite{mahankali20248dx} further simplified this, offering deep exploration benefits by pursuing randomly sampled goals in a latent space without complex bonus calculations.\n\nThe field has continued to push towards more advanced and adaptive exploration strategies (Section 5). Hierarchical Reinforcement Learning (HRL) enabled structured exploration by decomposing tasks into sub-problems, allowing agents to explore at different levels of temporal abstraction. A particularly impactful direction is meta-learning for exploration, where agents learn \\textit{how} to explore effectively across diverse tasks. Algorithms like MAESN \\cite{gupta2018rge} demonstrate this by learning structured exploration strategies and latent exploration spaces from prior experience, injecting informed stochasticity into policies and outperforming task-agnostic methods. Furthermore, the development of decoupled exploration and exploitation policies (DEEP) \\cite{whitney2021xlu} highlighted that separating these concerns can significantly boost sample efficiency, particularly in sparse reward settings. Integrated frameworks, such as Agent57, combine multiple techniques like RND, episodic memory, and adaptive meta-controllers to achieve state-of-the-art performance across a wide range of challenging environments. Diversity-driven exploration strategies \\cite{hong20182pr} also contribute to preventing policies from being trapped in local optima by encouraging varied behaviors. Population-based and evolutionary methods offer a meta-level solution, leveraging multiple agents or meta-optimization to achieve more robust and global exploration in complex reward landscapes.\n\nFinally, the application of these sophisticated exploration methods has expanded into specialized contexts (Section 6), demonstrating their versatility and practical impact. In offline reinforcement learning, where active exploration is impossible, the focus shifted to conservative exploration within fixed datasets, employing uncertainty estimation to avoid out-of-distribution actions \\cite{wu2021r67, zi20238ug}. Expert demonstrations and human feedback have been leveraged to guide exploration, significantly improving sample efficiency and overcoming sparse reward challenges in domains like robotics \\cite{nair2017crs, lee2021qzk}. Safety-aware exploration has become critical for real-world applications, incorporating constraints and recovery policies to prevent hazardous actions \\cite{thananjeyan2020d20}. The challenges of dynamic and open-ended environments, which demand continuous adaptation and robust discovery, are also being addressed \\cite{janjua2024yhk, matthews20241yx}. Emerging trends, such as the use of Decision-Pretrained Transformers (DPT) for in-context learning and adaptive exploration \\cite{lee202337c}, hint at a future where powerful foundation models might inherently possess sophisticated exploration capabilities.\n\nIn summary, the field has progressed from simple, often undirected, exploration heuristics to theoretically grounded methods, then to scalable intrinsic motivation for deep RL, and finally to highly adaptive, learned, and integrated strategies. This continuous innovation has enabled RL agents to achieve unprecedented performance and robustness in increasingly complex and diverse tasks, while also addressing critical concerns like safety and data efficiency. The trajectory reflects a deep commitment to overcoming the persistent challenges of the exploration-exploitation dilemma, paving the way for more intelligent and autonomous learning systems.\n\\subsection{Open Challenges and Theoretical Gaps}\n\\label{sec:7\\_2\\_open\\_challenges\\_\\_and\\_\\_theoretical\\_gaps}\n\nDespite significant advancements in reinforcement learning (RL) exploration, several fundamental challenges persist, particularly concerning scalability, robustness, sample efficiency, and the enduring gap between theoretical guarantees and practical applicability in complex, real-world settings. Addressing these issues is crucial for enabling RL agents to operate effectively in high-dimensional, stochastic, and open-ended environments.\n\nA primary challenge lies in scaling exploration strategies to extremely high-dimensional or continuous state-action spaces, where traditional methods struggle due to the curse of dimensionality. Early count-based exploration, while effective in tabular settings \\cite{Thrun1992}, quickly becomes infeasible. Efforts to bridge this gap include methods that generalize counts to high-dimensional spaces, such as using hash codes \\cite{Tang20166wr} or pseudo-counts derived from density models \\cite{Bellemare2016}. Concurrently, intrinsic motivation, often based on prediction error, emerged as a scalable heuristic. For instance, \\cite{Stadie20158af} utilized deep predictive models to generate exploration bonuses in Atari games, demonstrating the potential of learned models to guide exploration in complex visual domains. However, these prediction-error methods, like the Intrinsic Curiosity Module (ICM) \\cite{Pathak2017}, often suffer from the \"noisy TV problem,\" where uninformative stochasticity in the environment can generate spurious intrinsic rewards, leading to inefficient exploration. \\cite{Burda2018} addressed this by proposing Random Network Distillation (RND), which uses the prediction error of a fixed random network, proving more robust to environmental stochasticity and focusing exploration on learnable aspects of the environment. More recently, \\cite{Li2023kgk} proposed Implicit Posteriori Parameter Distribution Optimization (IPPDO) to improve exploration by modeling parameter uncertainty with implicit distributions, aiming for more flexible and efficient exploration in deep RL. Similarly, \\cite{Rahman2022p7b} introduced Robust Policy Optimization (RPO) to maintain high policy entropy throughout training, preventing premature convergence and ensuring sustained exploration in continuous action spaces.\n\nAnother persistent gap exists between methods offering strong theoretical guarantees and those providing practical scalability. Algorithms like R-Max \\cite{Strehl2008} provide provable bounds on sample complexity and regret, but their reliance on explicit model learning or tabular representations limits their application to simpler, finite Markov Decision Processes (MDPs). Bridging this gap requires developing principled yet adaptable solutions for real-world complexity. \\cite{Song2021elb} attempts this by proposing a computationally and statistically efficient model-based RL algorithm for specific model classes (Kernelized Nonlinear Regulators and linear MDPs) with polynomial sample complexity guarantees. In practical control applications, where accurate models are hard to obtain, methods like ModelPPO \\cite{Ma2024r2p} integrate neural network models into actor-critic architectures for AUV path following, demonstrating improved performance over traditional and model-free RL by learning state transition functions. For dynamic systems like microgrids, \\cite{Meng2025l1q} employs online RL with SARSA to adapt to uncertainties without relying on traditional mathematical models, prioritizing computational efficiency and real-time adaptability. Similarly, \\cite{Xi2024e2i} proposes a lightweight, adaptive SAC algorithm for UAV path planning, which adjusts exploration probability dynamically to balance efficiency and generalization in resource-constrained environments. These works highlight the ongoing tension between theoretical optimality and the need for practical, robust solutions in complex engineering domains.\n\nThe challenge of designing exploration strategies that are both sample-efficient and theoretically optimal across diverse tasks, especially in open-ended learning scenarios, remains largely unresolved. Meta-reinforcement learning (meta-RL) offers a promising avenue by learning exploration strategies from prior experience. \\cite{Gupta2018rge} introduced MAESN (Model Agnostic Exploration with Structured Noise) to learn structured exploration strategies, which are more effective than task-agnostic noise. \\cite{Zhang2020xq9} further developed MetaCURE, an empowerment-driven exploration method for meta-RL that maximizes information gain for task identification in sparse-reward settings. Leveraging offline data can also significantly boost sample efficiency. \\cite{Nair2017crs} demonstrated that incorporating demonstrations can overcome exploration difficulties in sparse-reward robotics tasks, while \\cite{Ball20235zm} showed how to effectively integrate offline data into online RL with minimal modifications. Offline RL itself faces challenges with out-of-distribution actions, which \\cite{Wu2021r67} addresses with Uncertainty Weighted Actor-Critic (UWAC) by down-weighting contributions from uncertain state-action pairs. More recent work explores in-context learning with large transformer models: \\cite{Lee202337c} introduced Decision-Pretrained Transformer (DPT), which can learn in-context exploration and conservatism from diverse datasets, generalizing to new tasks. Building on this, \\cite{Dai2024x3l} proposed In-context Exploration-Exploitation (ICEE) to optimize this trade-off at inference time, enhancing efficiency. The development of benchmarks like Craftax \\cite{Matthews20241yx} further underscores the current limitations of existing methods in achieving deep exploration, long-term planning, and continual adaptation required for truly open-ended learning. Cooperative exploration in multi-agent systems \\cite{Hu2020qwm, Yu20213c1} and autonomous navigation \\cite{Li2020r8r, Zhelo2018wi8, Kamalova2022jpm} also represent significant real-world complexities demanding robust and adaptable exploration.\n\nFinally, integrating safety constraints into exploration is a critical, yet often conflicting, requirement for real-world deployment. Extensive exploration, while necessary for learning, can lead to dangerous situations. \\cite{Thananjeyan2020d20} proposed Recovery RL, which decouples task and recovery policies and learns constraint-violating zones from offline data to safely navigate this trade-off. \\cite{Yu20222xi} introduced SEditor, a two-policy approach where a safety editor policy transforms potentially unsafe actions into safe ones, achieving extremely low constraint violation rates. \\cite{Zhang2023wqi} advanced safe RL by identifying and avoiding dead-end states, providing a minimal limitation on safe exploration. In reward-free settings, \\cite{Yang2023n56} proposed a \"guide\" agent to learn safe exploration which then regularizes a \"student\" policy. For deployable safe RL, \\cite{Honari202473t} developed Meta SAC-Lag, using meta-gradient optimization to automatically tune safety-related hyperparameters. Addressing model uncertainties for robust safety, \\cite{Yu2022bo5} introduced a distributional reachability certificate for safe model-based RL. Furthermore, \\cite{Zi20238ug} applied distributionally robust RL for active signal pattern localization, enabling safe exploration in unfamiliar environments with limited data. These efforts highlight the ongoing challenge of balancing the need for exploration with stringent safety requirements, often requiring complex architectural designs or meta-learning approaches.\n\nIn conclusion, while the field has made substantial progress from tabular, theoretically-grounded methods to scalable, deep learning-based intrinsic motivation, significant open challenges remain. The persistent gap between methods with strong theoretical guarantees (often for simpler settings) and those providing practical scalability (often heuristic-driven) underscores the critical need for principled yet adaptable solutions. Future research must focus on developing exploration strategies that are robust to environmental stochasticity, sample-efficient across diverse tasks, capable of deep exploration in open-ended environments, and inherently safe for real-world deployment, potentially through hybrid approaches that combine the strengths of model-based reasoning, intrinsic motivation, and meta-learning with strong theoretical foundations.\n\\subsection{Emerging Trends and Ethical Considerations}\n\\label{sec:7\\_3\\_emerging\\_trends\\_\\_and\\_\\_ethical\\_considerations}\n\nThe frontier of reinforcement learning (RL) exploration is characterized by a dual pursuit: developing increasingly sophisticated agents capable of understanding and navigating complex, open-ended environments, and simultaneously ensuring these autonomous systems operate ethically and safely, particularly in human-interactive or safety-critical domains. This subsection explores cutting-edge research directions, including the transformative integration of large foundation models, the development of truly general-purpose and adaptive exploration agents, the increasing focus on learning better representations, and the critical ethical implications of deploying such intelligent systems.\n\nA pivotal emerging trend is the integration of \\textbf{large foundation models (LFMs)}, such as Large Language Models (LLMs) and Vision-Language Models (VLMs), to imbue RL agents with more sophisticated world understanding, high-level planning capabilities, and common-sense priors. Traditional RL often struggles with extensive exploration in complex, semantically rich environments due to its limited grasp of underlying decision dynamics. LLMs, with their vast domain-specific knowledge, can serve as powerful prior action distributions, significantly reducing exploration and optimization complexity when integrated into RL frameworks through Bayesian inference methods \\cite{yan2024p3y}. This approach can decrease the number of required samples by over 90\\\\% in offline learning scenarios, demonstrating the immense potential of leveraging pre-trained knowledge to guide exploration. Furthermore, LLMs are being directly utilized to generate intrinsic curiosity signals. For instance, Curiosity-Driven Exploration (CDE) for LLMs in Reinforcement Learning with Verifiable Rewards (RLVR) leverages the model's own perplexity (from the actor) and the variance of value estimates (from the critic) as intrinsic bonuses \\cite{dai2025h8g}. This framework guides exploration by penalizing overconfident errors and promoting diversity, addressing issues like premature convergence and entropy collapse common in LLM-based RL. Beyond direct policy guidance, LLMs can act as adaptive search operators within meta-learning frameworks, where evolutionary search discovers improved algorithms, and RL fine-tunes the LLM policy based on these discoveries, accelerating algorithm design for complex combinatorial optimization tasks \\cite{surina2025smk}. These advancements highlight LFMs' capacity to elevate exploration from low-level state space coverage to high-level conceptual discovery and informed decision-making.\n\nComplementing the rise of LFMs, there is an increasing focus on \\textbf{learning better representations} to facilitate more informed and efficient exploration. Robust representations are crucial for defining novelty, quantifying uncertainty, and building accurate world models in high-dimensional observation spaces. While earlier methods like the simplified Intrinsic Curiosity Module (S-ICM) \\cite{li2019tj1} and its predecessor ICM \\cite{pathak2017} leveraged prediction error in learned feature spaces to incentivize novelty, contemporary research pushes for more sophisticated self-supervised techniques that disentangle factors of variation and capture epistemic uncertainty. For example, the Actor-Model-Critic (AMC) architecture for Autonomous Underwater Vehicle (AUV) path-following explicitly learns the state transition function via a neural network, enabling the agent to anticipate environmental dynamics and guide exploration towards informative regions \\cite{ma2024r2p}. Beyond explicit model learning, methods like Exploration via Distributional Ensemble (EDE) emphasize the importance of exploration for generalization, not just optimal policy finding \\cite{jiang2023qmw}. EDE encourages exploration of states with high epistemic uncertainty using an ensemble of Q-value distributions, implicitly relying on robust representations to quantify this uncertainty effectively. Similarly, Decoupled Exploration and Exploitation Policies (DEEP) demonstrate that separating the task policy from the exploration policy can yield significant sample efficiency improvements in sparse environments, a benefit often amplified by representations that allow for meaningful novelty detection and uncertainty estimation \\cite{whitney2021xlu}. These approaches underscore that the quality of learned representations directly impacts an agent's ability to discern truly novel or uncertain aspects of an environment, leading to more directed and less wasteful exploration.\n\nThe drive towards \\textbf{truly general-purpose exploration agents} capable of tackling open-ended problems is leading to more adaptive, robust, and scalable strategies. Rather than relying on fixed heuristics, recent work focuses on agents that can dynamically adjust their exploration behavior. Adaptive exploration strategies, such as those using multi-attribute decision-making based on information entropy and task decomposition, allow for more flexible and context-aware exploration \\cite{hu2020yhq}. Further advancing this, ensemble learning schemes with explicit \"exploration-to-exploitation (E2E) ratio control\" via multiple Q-learning agents and adaptive decay functions enable more nuanced balancing of exploration and exploitation, crucial for real-world applications requiring continuous adaptation \\cite{shuai2025fq3}. The scalability and theoretical guarantees of exploration are also paramount for such agents. Thompson sampling-based methods, employing Langevin Monte Carlo (LMC) and approximate sampling, offer provably efficient and scalable exploration in deep RL with theoretical regret bounds, ensuring reliability in autonomous systems \\cite{ishfaq20235fo, ishfaq20245to}. This extends to collaborative settings, where randomized exploration in cooperative multi-agent RL (MARL) with methods like CoopTS-PHE and CoopTS-LMC provides theoretical guarantees for regret bounds and communication complexity, essential for complex multi-agent environments \\cite{hsu2024tqd}. Moreover, simple yet effective strategies like Random Latent Exploration (RLE), which pursues randomly sampled goals in a latent space, demonstrate that deep exploration can be achieved without complex bonus calculations, promoting broader applicability as a general plug-in for existing RL algorithms \\cite{mahankali20248dx}. The concept of meta-learning how to explore is also gaining traction, with approaches like Learned Optimization for Plasticity, Exploration and Non-stationarity (OPEN) meta-learning update rules that incorporate stochasticity for exploration, showing strong generalization across diverse environments and agent architectures \\cite{goldie2024cuf}. These advancements, alongside broader discussions on open-ended RL emphasizing hierarchical learning, intrinsic motivation, and unsupervised skill acquisition \\cite{janjua2024yhk}, signify a shift towards agents that can autonomously learn and adapt their exploration strategies across a wide spectrum of tasks.\n\nAlongside these advancements in exploration capabilities, the \\textbf{ethical considerations} surrounding autonomous exploration are gaining increasing prominence, especially in safety-critical or human-interactive environments. The inherent trial-and-error nature of RL exploration can lead to \"bad decisions\" that violate critical safety constraints, as highlighted in reviews of safe RL for power system control \\cite{yu2024x53}. This necessitates responsible development and deployment, emphasizing alignment with human values and safety standards. A direct response to this challenge is the \"human-in-the-loop deep reinforcement learning (HL-DRL)\" approach for optimal Volt/Var control in unbalanced distribution networks \\cite{sun2024kxq}. This method allows human intervention to modify dangerous actions during offline training and integrates human guidance into the actor network's loss function, ensuring the learned policy adheres to operational constraints and human safety guidelines. Broader advancements in RL for autonomous systems also explicitly identify safety, dependability, and explainability as critical constraints that limit wide adoption \\cite{malaiyappan20245bh}. The imperative is to develop exploration strategies that not only discover optimal behaviors but do so within predefined safe regions, learn to recover from unsafe situations, and provide transparent decision-making processes. This ensures that the learning process does not lead to catastrophic outcomes and adheres to ethical considerations, bridging the gap between autonomous learning and responsible societal impact.\n\nIn conclusion, the field is rapidly advancing towards more intelligent, adaptive, and generalizable exploration strategies. This progress is driven by the transformative potential of large foundation models for high-level understanding and goal generation, the continuous refinement of learned representations for informed low-level novelty detection and uncertainty quantification, and the development of meta-learning approaches for truly general-purpose agents. Simultaneously, the increasing power and autonomy of these systems amplify the imperative to address ethical implications, particularly in safety-critical domains. Future research must continue to bridge the gap between theoretical guarantees and practical deployment in highly dynamic real-world scenarios, further integrating human oversight, value alignment, and explainability into the design of autonomous exploration systems to ensure their beneficial and responsible societal impact.\n\n\n\\newpage\n\\section*{References}\n\\addcontentsline{toc}{section}{References}\n\n\\begin{thebibliography}{240}\n\n\\bibitem{nair2017crs}\nAshvin Nair, Bob McGrew, Marcin Andrychowicz, et al. (2017). \\textit{Overcoming Exploration in Reinforcement Learning with Demonstrations}. IEEE International Conference on Robotics and Automation.\n\n\\bibitem{tang20166wr}\nHaoran Tang, Rein Houthooft, Davis Foote, et al. (2016). \\textit{#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning}. Neural Information Processing Systems.\n\n\\bibitem{lee2021qzk}\nKimin Lee, Laura M. Smith, and P. Abbeel (2021). \\textit{PEBBLE: Feedback-Efficient Interactive Reinforcement Learning via Relabeling Experience and Unsupervised Pre-training}. International Conference on Machine Learning.\n\n\\bibitem{hu2020qwm}\nJunyan Hu, Hanlin Niu, J. Carrasco, et al. (2020). \\textit{Voronoi-Based Multi-Robot Autonomous Exploration in Unknown Environments via Deep Reinforcement Learning}. IEEE Transactions on Vehicular Technology.\n\n\\bibitem{stadie20158af}\nBradly C. Stadie, S. Levine, and P. Abbeel (2015). \\textit{Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models}. arXiv.org.\n\n\\bibitem{gupta2018rge}\nAbhishek Gupta, Russell Mendonca, Yuxuan Liu, et al. (2018). \\textit{Meta-Reinforcement Learning of Structured Exploration Strategies}. Neural Information Processing Systems.\n\n\\bibitem{thananjeyan2020d20}\nBrijen Thananjeyan, A. Balakrishna, Suraj Nair, et al. (2020). \\textit{Recovery RL: Safe Reinforcement Learning With Learned Recovery Zones}. IEEE Robotics and Automation Letters.\n\n\\bibitem{wu2021r67}\nYue Wu, Shuangfei Zhai, Nitish Srivastava, et al. (2021). \\textit{Uncertainty Weighted Actor-Critic for Offline Reinforcement Learning}. International Conference on Machine Learning.\n\n\\bibitem{conti2017cr2}\nEdoardo Conti, Vashisht Madhavan, F. Such, et al. (2017). \\textit{Improving Exploration in Evolution Strategies for Deep Reinforcement Learning via a Population of Novelty-Seeking Agents}. Neural Information Processing Systems.\n\n\\bibitem{seo2022cjf}\nYounggyo Seo, Kimin Lee, Stephen James, et al. (2022). \\textit{Reinforcement Learning with Action-Free Pre-Training from Videos}. International Conference on Machine Learning.\n\n\\bibitem{uchendu20221h1}\nIkechukwu Uchendu, Ted Xiao, Yao Lu, et al. (2022). \\textit{Jump-Start Reinforcement Learning}. International Conference on Machine Learning.\n\n\\bibitem{li2020r8r}\nHaoran Li, Qichao Zhang, and Dongbin Zhao (2020). \\textit{Deep Reinforcement Learning-Based Automatic Exploration for Navigation in Unknown Environment}. IEEE Transactions on Neural Networks and Learning Systems.\n\n\\bibitem{yang2021ngm}\nTianpei Yang, Hongyao Tang, Chenjia Bai, et al. (2021). \\textit{Exploration in Deep Reinforcement Learning: From Single-Agent to Multiagent Domain}. IEEE Transactions on Neural Networks and Learning Systems.\n\n\\bibitem{lee2019hnz}\nKimin Lee, Kibok Lee, Jinwoo Shin, et al. (2019). \\textit{Network Randomization: A Simple Technique for Generalization in Deep Reinforcement Learning}. International Conference on Learning Representations.\n\n\\bibitem{zhang2020o5t}\nJunyu Zhang, Alec Koppel, A. S. Bedi, et al. (2020). \\textit{Variational Policy Gradient Method for Reinforcement Learning with General Utilities}. Neural Information Processing Systems.\n\n\\bibitem{chang20221gc}\nJingru Chang, Dong Yu, Y. Hu, et al. (2022). \\textit{Deep Reinforcement Learning for Dynamic Flexible Job Shop Scheduling with Random Job Arrival}. Processes.\n\n\\bibitem{yang2021psl}\nTianpei Yang, Hongyao Tang, Chenjia Bai, et al. (2021). \\textit{Exploration in Deep Reinforcement Learning: A Comprehensive Survey}. arXiv.org.\n\n\\bibitem{li2022ktf}\nJinning Li, Chen Tang, M. Tomizuka, et al. (2022). \\textit{Hierarchical Planning Through Goal-Conditioned Offline Reinforcement Learning}. IEEE Robotics and Automation Letters.\n\n\\bibitem{liang20226ix}\nXi-Xi Liang, Katherine Shu, Kimin Lee, et al. (2022). \\textit{Reward Uncertainty for Exploration in Preference-based Reinforcement Learning}. International Conference on Learning Representations.\n\n\\bibitem{hong20182pr}\nZhang-Wei Hong, Tzu-Yun Shann, Shih-Yang Su, et al. (2018). \\textit{Diversity-Driven Exploration Strategy for Deep Reinforcement Learning}. Neural Information Processing Systems.\n\n\\bibitem{hansen2022jm2}\nNicklas Hansen, Yixin Lin, H. Su, et al. (2022). \\textit{MoDem: Accelerating Visual Model-Based Reinforcement Learning with Demonstrations}. International Conference on Learning Representations.\n\n\\bibitem{pong2021i4o}\nVitchyr H. Pong, Ashvin Nair, Laura M. Smith, et al. (2021). \\textit{Offline Meta-Reinforcement Learning with Online Self-Supervision}. International Conference on Machine Learning.\n\n\\bibitem{jia2021kxs}\nDanyang Jia, Hao Guo, Z. Song, et al. (2021). \\textit{Local and global stimuli in reinforcement learning}. New Journal of Physics.\n\n\\bibitem{zhang2022dgg}\nYuxiang Zhang, Xiaoling Liang, Dongyu Li, et al. (2022). \\textit{Barrier Lyapunov Function-Based Safe Reinforcement Learning for Autonomous Vehicles With Optimized Backstepping}. IEEE Transactions on Neural Networks and Learning Systems.\n\n\\bibitem{dorfman20216nq}\nRon Dorfman, Idan Shenfeld, and Aviv Tamar (2021). \\textit{Offline Meta Reinforcement Learning - Identifiability Challenges and Effective Data Collection Strategies}. Neural Information Processing Systems.\n\n\\bibitem{tai2016bp8}\nL. Tai, and Ming Liu (2016). \\textit{A robot exploration strategy based on Q-learning network}. International Conference on Real-time Computing and Robotics.\n\n\\bibitem{martin2017bgt}\nJarryd Martin, S. N. Sasikumar, Tom Everitt, et al. (2017). \\textit{Count-Based Exploration in Feature Space for Reinforcement Learning}. International Joint Conference on Artificial Intelligence.\n\n\\bibitem{rckin2021yud}\nJulius Rckin, Liren Jin, and Marija Popovic (2021). \\textit{Adaptive Informative Path Planning Using Deep Reinforcement Learning for UAV-based Active Sensing}. IEEE International Conference on Robotics and Automation.\n\n\\bibitem{zhelo2018wi8}\nOleksii Zhelo, Jingwei Zhang, L. Tai, et al. (2018). \\textit{Curiosity-driven Exploration for Mapless Navigation with Deep Reinforcement Learning}. arXiv.org.\n\n\\bibitem{zhang2020bse}\nQilei Zhang, Jinying Lin, Q. Sha, et al. (2020). \\textit{Deep Interactive Reinforcement Learning for Path Following of Autonomous Underwater Vehicle}. IEEE Access.\n\n\\bibitem{mavrin2019iqm}\nB. Mavrin, Shangtong Zhang, Hengshuai Yao, et al. (2019). \\textit{Distributional Reinforcement Learning for Efficient Exploration}. International Conference on Machine Learning.\n\n\\bibitem{li2021w3q}\nGen Li, Laixi Shi, Yuxin Chen, et al. (2021). \\textit{Breaking the Sample Complexity Barrier to Regret-Optimal Model-Free Reinforcement Learning}. Neural Information Processing Systems.\n\n\\bibitem{schumacher2022x3f}\nPierre Schumacher, D. Haeufle, Dieter Bchler, et al. (2022). \\textit{DEP-RL: Embodied Exploration for Reinforcement Learning in Overactuated and Musculoskeletal Systems}. International Conference on Learning Representations.\n\n\\bibitem{aubret2022inh}\nA. Aubret, L. Matignon, and S. Hassas (2022). \\textit{An Information-Theoretic Perspective on Intrinsic Motivation in Reinforcement Learning: A Survey}. Entropy.\n\n\\bibitem{yuan2020epo}\nXiaolei Yuan, Yiqun Pan, Jianrong Yang, et al. (2020). \\textit{Study on the application of reinforcement learning in the operation optimization of HVAC system}. Building Simulation.\n\n\\bibitem{rezaeifar20211eu}\nShideh Rezaeifar, Robert Dadashi, Nino Vieillard, et al. (2021). \\textit{Offline Reinforcement Learning as Anti-Exploration}. AAAI Conference on Artificial Intelligence.\n\n\\bibitem{talaat2022ywa}\nFatma M. Talaat (2022). \\textit{Effective deep Q-networks (EDQN) strategy for resource allocation based on optimized reinforcement learning algorithm}. Multimedia tools and applications.\n\n\\bibitem{xin2022qcl}\nXin Xin, Tiago Pimentel, Alexandros Karatzoglou, et al. (2022). \\textit{Rethinking Reinforcement Learning for Recommendation: A Prompt Perspective}. Annual International ACM SIGIR Conference on Research and Development in Information Retrieval.\n\n\\bibitem{dang2022kwh}\nFengying Dang, Dong Chen, J. Chen, et al. (2022). \\textit{Event-Triggered Model Predictive Control With Deep Reinforcement Learning for Autonomous Driving}. IEEE Transactions on Intelligent Vehicles.\n\n\\bibitem{raffin2020o1a}\nA. Raffin, Jens Kober, and F. Stulp (2020). \\textit{Smooth Exploration for Robotic Reinforcement Learning}. Conference on Robot Learning.\n\n\\bibitem{liu2018jde}\nXuhan Liu, K. Ye, H. V. van Vlijmen, et al. (2018). \\textit{An exploration strategy improves the diversity of de novo ligands using deep reinforcement learning: a case for the adenosine A2A receptor}. Journal of Cheminformatics.\n\n\\bibitem{sun2022ul9}\nQiyu Sun, Jinbao Fang, Weixing Zheng, et al. (2022). \\textit{Aggressive Quadrotor Flight Using Curiosity-Driven Reinforcement Learning}. IEEE transactions on industrial electronics (1982. Print).\n\n\\bibitem{nikolov20184g9}\nNikolay Nikolov, Johannes Kirschner, Felix Berkenkamp, et al. (2018). \\textit{Information-Directed Exploration for Deep Reinforcement Learning}. International Conference on Learning Representations.\n\n\\bibitem{qiao20220gx}\nDan Qiao, Ming Yin, Ming Min, et al. (2022). \\textit{Sample-Efficient Reinforcement Learning with loglog(T) Switching Cost}. International Conference on Machine Learning.\n\n\\bibitem{yu20222xi}\nHaonan Yu, Wei Xu, and Haichao Zhang (2022). \\textit{Towards Safe Reinforcement Learning with a Safety Editor Policy}. Neural Information Processing Systems.\n\n\\bibitem{lambert202277x}\nNathan Lambert, Markus Wulfmeier, William F. Whitney, et al. (2022). \\textit{The Challenges of Exploration for Offline Reinforcement Learning}. arXiv.org.\n\n\\bibitem{woczyk20220mn}\nMaciej Woczyk, Michal Zajkac, Razvan Pascanu, et al. (2022). \\textit{Disentangling Transfer in Continual Reinforcement Learning}. Neural Information Processing Systems.\n\n\\bibitem{qu2022uym}\nQingyu Qu, Kexin Liu, Wei Wang, et al. (2022). \\textit{Spacecraft Proximity Maneuvering and Rendezvous With Collision Avoidance Based on Reinforcement Learning}. IEEE Transactions on Aerospace and Electronic Systems.\n\n\\bibitem{sun2020zjg}\nH. Sun, and Ling Ma (2020). \\textit{Generative Design by Using Exploration Approaches of Reinforcement Learning in Density-Based Structural Topology Optimization}. Designs.\n\n\\bibitem{tao202294e}\nYuechuan Tao, Jing Qiu, Shuying Lai, et al. (2022). \\textit{A Human-Machine Reinforcement Learning Method for Cooperative Energy Management}. IEEE Transactions on Industrial Informatics.\n\n\\bibitem{houthooft2016yee}\nRein Houthooft, Xi Chen, Yan Duan, et al. (2016). \\textit{Curiosity-driven Exploration in Deep Reinforcement Learning via Bayesian Neural Networks}. arXiv.org.\n\n\\bibitem{shi20215fg}\nTianyu Shi, Dong Chen, Kaian Chen, et al. (2021). \\textit{Offline Reinforcement Learning for Autonomous Driving with Safety and Exploration Enhancement}. arXiv.org.\n\n\\bibitem{li2021l92}\nZhiwei Li, Yu Lu, Xi Li, et al. (2021). \\textit{UAV Networks Against Multiple Maneuvering Smart Jamming With Knowledge-Based Reinforcement Learning}. IEEE Internet of Things Journal.\n\n\\bibitem{liu20220g4}\nJiabin Liu, Chengliang Chai, Yuyu Luo, et al. (2022). \\textit{Feature Augmentation with Reinforcement Learning}. IEEE International Conference on Data Engineering.\n\n\\bibitem{hu20195n2}\nHangkai Hu, Shiji Song, and C. L. Phillip Chen (2019). \\textit{Plume Tracing via Model-Free Reinforcement Learning Method}. IEEE Transactions on Neural Networks and Learning Systems.\n\n\\bibitem{wang2022boj}\nYutong Wang, Ke Xue, and Chaojun Qian (2022). \\textit{Evolutionary Diversity Optimization with Clustering-based Selection for Reinforcement Learning}. International Conference on Learning Representations.\n\n\\bibitem{yu20213c1}\nChao Yu, Xinyi Yang, Jiaxuan Gao, et al. (2021). \\textit{Learning Efficient Multi-Agent Cooperative Visual Exploration}. European Conference on Computer Vision.\n\n\\bibitem{zheng2022816}\nChangdong Zheng, Fangfang Xie, Tingwei Ji, et al. (2022). \\textit{Data-efficient deep reinforcement learning with expert demonstration for active flow control}. The Physics of Fluids.\n\n\\bibitem{yang2022mx5}\nZhengyu Yang, Kan Ren, Xufang Luo, et al. (2022). \\textit{Towards Applicable Reinforcement Learning: Improving the Generalization and Sample Efficiency with Policy Ensemble}. International Joint Conference on Artificial Intelligence.\n\n\\bibitem{yang2022fou}\nYijun Yang, J. Jiang, Tianyi Zhou, et al. (2022). \\textit{Pareto Policy Pool for Model-based Offline Reinforcement Learning}. International Conference on Learning Representations.\n\n\\bibitem{liu2022uiv}\nYuqi Liu, Po Gao, Change Zheng, et al. (2022). \\textit{A Deep Reinforcement Learning Strategy Combining Expert Experience Guidance for a Fruit-Picking Manipulator}. Electronics.\n\n\\bibitem{hou2021c2r}\nZhongni Hou, Xiaolong Jin, Zixuan Li, et al. (2021). \\textit{Rule-Aware Reinforcement Learning for Knowledge Graph Reasoning}. Findings.\n\n\\bibitem{lale2020xqs}\nSahin Lale, K. Azizzadenesheli, B. Hassibi, et al. (2020). \\textit{Reinforcement Learning with Fast Stabilization in Linear Dynamical Systems}. International Conference on Artificial Intelligence and Statistics.\n\n\\bibitem{otto2022qef}\nFabian Otto, Onur elik, Hongyi Zhou, et al. (2022). \\textit{Deep Black-Box Reinforcement Learning with Movement Primitives}. Conference on Robot Learning.\n\n\\bibitem{lakhani2021217}\nAyub I. Lakhani, Myisha A. Chowdhury, and Qiugang Lu (2021). \\textit{Stability-Preserving Automatic Tuning of PID Control with Reinforcement Learning}. Complex Engineering Systems.\n\n\\bibitem{huang2020wll}\nYixin Huang, Shufan Wu, Z. Mu, et al. (2020). \\textit{A Multi-agent Reinforcement Learning Method for Swarm Robots in Space Collaborative Exploration}. 2020 6th International Conference on Control, Automation and Robotics (ICCAR).\n\n\\bibitem{yuan2022hto}\nWang Yuan, Z. Xiwen, Zhou Rong, et al. (2022). \\textit{Research on UCAV Maneuvering Decision Method Based on Heuristic Reinforcement Learning}. Computational Intelligence and Neuroscience.\n\n\\bibitem{muzahid2022fyb}\nAbu Jafar Md. Muzahid, Syafiq Fauzi Bin Kamarulzaman, Md. Arafatur Rahman, et al. (2022). \\textit{Deep Reinforcement Learning-Based Driving Strategy for Avoidance of Chain Collisions and Its Safety Efficiency Analysis in Autonomous Vehicles}. IEEE Access.\n\n\\bibitem{zhang20229rg}\nChi Zhang, S. Kuppannagari, and V. Prasanna (2022). \\textit{Safe Building HVAC Control via Batch Reinforcement Learning}. IEEE Transactions on Sustainable Computing.\n\n\\bibitem{sierragarca2020g35}\nJ. E. Sierra-Garca, and Matilde Santos (2020). \\textit{Exploring Reward Strategies for Wind Turbine Pitch Control by Reinforcement Learning}. Applied Sciences.\n\n\\bibitem{han20199g2}\nWei Han, Fang Guo, and Xi-chao Su (2019). \\textit{A Reinforcement Learning Method for a Hybrid Flow-Shop Scheduling Problem}. Algorithms.\n\n\\bibitem{wabersich2018t86}\nK. P. Wabersich, and M. Zeilinger (2018). \\textit{Safe exploration of nonlinear dynamical systems: A predictive safety filter for reinforcement learning}. arXiv.org.\n\n\\bibitem{bourel2020tnm}\nHippolyte Bourel, Odalric-Ambrym Maillard, and M. S. Talebi (2020). \\textit{Tightening Exploration in Upper Confidence Reinforcement Learning}. International Conference on Machine Learning.\n\n\\bibitem{cheng20224w2}\nYikun Cheng, Pan Zhao, and N. Hovakimyan (2022). \\textit{Safe and Efficient Reinforcement Learning using Disturbance-Observer-Based Control Barrier Functions}. Conference on Learning for Dynamics & Control.\n\n\\bibitem{ceusters2022drp}\nGlenn Ceusters, L. R. Camargo, R. Franke, et al. (2022). \\textit{Safe reinforcement learning for multi-energy management systems with known constraint functions}. Energy and AI.\n\n\\bibitem{stanton20183fs}\nC. Stanton, and J. Clune (2018). \\textit{Deep Curiosity Search: Intra-Life Exploration Improves Performance on Challenging Deep Reinforcement Learning Problems}. arXiv.org.\n\n\\bibitem{cideron2020kdj}\nGeoffrey Cideron, Thomas Pierrot, Nicolas Perrin, et al. (2020). \\textit{QD-RL: Efficient Mixing of Quality and Diversity in Reinforcement Learning}. arXiv.org.\n\n\\bibitem{liu2022nhx}\nXiangyu Liu, and Ying Tan (2022). \\textit{Feudal Latent Space Exploration for Coordinated Multi-Agent Reinforcement Learning}. IEEE Transactions on Neural Networks and Learning Systems.\n\n\\bibitem{cho2022o2c}\nDaesol Cho, Jigang Kim, and H. J. Kim (2022). \\textit{Unsupervised Reinforcement Learning for Transferable Manipulation Skill Discovery}. IEEE Robotics and Automation Letters.\n\n\\bibitem{zhang2020xq9}\nJin Zhang, Jianhao Wang, Hao Hu, et al. (2020). \\textit{MetaCURE: Meta Reinforcement Learning with Empowerment-Driven Exploration}. International Conference on Machine Learning.\n\n\\bibitem{song2021elb}\nYuda Song, and Wen Sun (2021). \\textit{PC-MLP: Model-based Reinforcement Learning with Policy Cover Guided Exploration}. International Conference on Machine Learning.\n\n\\bibitem{wang20229ce}\nXiucheng Wang, Longfei Ma, Hao Li, et al. (2022). \\textit{Digital Twin-Assisted Efficient Reinforcement Learning for Edge Task Scheduling}. IEEE Vehicular Technology Conference.\n\n\\bibitem{lin2022vqo}\nSen Lin, Jialin Wan, Tengyu Xu, et al. (2022). \\textit{Model-Based Offline Meta-Reinforcement Learning with Regularization}. International Conference on Learning Representations.\n\n\\bibitem{zhang2022egf}\nZiqian Zhang, Yulei Liu, Shengcheng Yu, et al. (2022). \\textit{UniRLTest: universal platform-independent testing with reinforcement learning via image understanding}. International Symposium on Software Testing and Analysis.\n\n\\bibitem{zhou2022fny}\nTong Zhou, Letian Wang, Ruobing Chen, et al. (2022). \\textit{Accelerating Reinforcement Learning for Autonomous Driving Using Task-Agnostic and Ego-Centric Motion Skills}. IEEE/RJS International Conference on Intelligent RObots and Systems.\n\n\\bibitem{yu2022bo5}\nDongjie Yu, Wenjun Zou, Yujie Yang, et al. (2022). \\textit{Safe Model-Based Reinforcement Learning With an Uncertainty-Aware Reachability Certificate}. IEEE Transactions on Automation Science and Engineering.\n\n\\bibitem{xie2015vwy}\nChristopher Xie, S. Patil, T. Moldovan, et al. (2015). \\textit{Model-based reinforcement learning with parametrized physical models and optimism-driven exploration}. IEEE International Conference on Robotics and Automation.\n\n\\bibitem{zhang2019yjm}\nYu Zhang, Peixiang Cai, Changyong Pan, et al. (2019). \\textit{Multi-Agent Deep Reinforcement Learning-Based Cooperative Spectrum Sensing With Upper Confidence Bound Exploration}. IEEE Access.\n\n\\bibitem{wu2021mht}\nZhenning Wu, Yiming Deng, Jinhai Liu, et al. (2021). \\textit{A Reinforcement Learning-Based Reconstruction Method for Complex Defect Profiles in MFL Inspection}. IEEE Transactions on Instrumentation and Measurement.\n\n\\bibitem{fu20220cl}\nHaotian Fu, Shangqun Yu, Michael S. Littman, et al. (2022). \\textit{Model-based Lifelong Reinforcement Learning with Bayesian Exploration}. Neural Information Processing Systems.\n\n\\bibitem{mndezmolina2022ec5}\nArqumides Mndez-Molina, E. Morales, and L. Sucar (2022). \\textit{Causal Discovery and Reinforcement Learning: A Synergistic Integration}. European Workshop on Probabilistic Graphical Models.\n\n\\bibitem{steinparz20220nl}\nC. Steinparz, Thomas Schmied, Fabian Paischer, et al. (2022). \\textit{Reactive Exploration to Cope with Non-Stationarity in Lifelong Reinforcement Learning}. CoLLAs.\n\n\\bibitem{rahman2022p7b}\nMd Masudur Rahman, and Yexiang Xue (2022). \\textit{Robust Policy Optimization in Deep Reinforcement Learning}. arXiv.org.\n\n\\bibitem{xu2022cgd}\nJingyi Xu, Zirui Li, Li Gao, et al. (2022). \\textit{A Comparative Study of Deep Reinforcement Learning-based Transferable Energy Management Strategies for Hybrid Electric Vehicles}. 2022 IEEE Intelligent Vehicles Symposium (IV).\n\n\\bibitem{lee2020k9k}\nKyunghyun Lee, Byeong-uk Lee, Ukcheol Shin, et al. (2020). \\textit{An Efficient Asynchronous Method for Integrating Evolutionary and Gradient-based Policy Search}. Neural Information Processing Systems.\n\n\\bibitem{li2022ec4}\nZiniu Li, Yingru Li, Yushun Zhang, et al. (2022). \\textit{HyperDQN: A Randomized Exploration Method for Deep Reinforcement Learning}. International Conference on Learning Representations.\n\n\\bibitem{wang2022t55}\nZhihai Wang, Taoxing Pan, Qi Zhou, et al. (2022). \\textit{Efficient Exploration in Resource-Restricted Reinforcement Learning}. AAAI Conference on Artificial Intelligence.\n\n\\bibitem{whitney2021xlu}\nWilliam F. Whitney, Michael Bloesch, Jost Tobias Springenberg, et al. (2021). \\textit{Decoupled Exploration and Exploitation Policies for Sample-Efficient Reinforcement Learning}. Unpublished manuscript.\n\n\\bibitem{suri20226rr}\nKarush Suri (2022). \\textit{Off-Policy Evolutionary Reinforcement Learning with Maximum Mutations}. Adaptive Agents and Multi-Agent Systems.\n\n\\bibitem{xin2020y4j}\nBo Xin, Haixu Yu, You Qin, et al. (2020). \\textit{Exploration Entropy for Reinforcement Learning}. Unpublished manuscript.\n\n\\bibitem{matheron2020zmh}\nGuillaume Matheron, Nicolas Perrin, and Olivier Sigaud (2020). \\textit{PBCS : Efficient Exploration and Exploitation Using a Synergy between Reinforcement Learning and Motion Planning}. International Conference on Artificial Neural Networks.\n\n\\bibitem{yang2022j0z}\nYiqin Yang, Haotian Hu, Wenzhe Li, et al. (2022). \\textit{Flow to Control: Offline Reinforcement Learning with Lossless Primitive Discovery}. AAAI Conference on Artificial Intelligence.\n\n\\bibitem{wu2022sot}\nZheng Wu, Yichen Xie, Wenzhao Lian, et al. (2022). \\textit{Zero-Shot Policy Transfer with Disentangled Task Representation of Meta-Reinforcement Learning}. IEEE International Conference on Robotics and Automation.\n\n\\bibitem{kessler202295l}\nSamuel Kessler, Piotr Milo's, Jack Parker-Holder, et al. (2022). \\textit{The Surprising Effectiveness of Latent World Models for Continual Reinforcement Learning}. arXiv.org.\n\n\\bibitem{raffin2020ka2}\nA. Raffin, and F. Stulp (2020). \\textit{Generalized State-Dependent Exploration for Deep Reinforcement Learning in Robotics}. arXiv.org.\n\n\\bibitem{yang20206wi}\nKai-En Yang, Chia-Yu Tsai, Hung-Hao Shen, et al. (2020). \\textit{Trust-Region Method with Deep Reinforcement Learning in Analog Design Space Exploration}. Design Automation Conference.\n\n\\bibitem{liu20228r4}\nJiayi Liu, Gang Wang, Xiangke Guo, et al. (2022). \\textit{Deep Reinforcement Learning Task Assignment Based on Domain Knowledge}. IEEE Access.\n\n\\bibitem{kamalova2022jpm}\nA. Kamalova, Suk-Gyu Lee, and Soon-H. Kwon (2022). \\textit{Occupancy Reward-Driven Exploration with Deep Reinforcement Learning for Mobile Robot System}. Applied Sciences.\n\n\\bibitem{zhang2022p0b}\nHaichao Zhang, Wei Xu, and Haonan Yu (2022). \\textit{Generative Planning for Temporally Coordinated Exploration in Reinforcement Learning}. International Conference on Learning Representations.\n\n\\bibitem{li20227ss}\nWenli Li, Yousong Zhang, Xiaohui Shi, et al. (2022). \\textit{A Decision-Making Strategy for Car Following Based on Naturalist Driving Data via Deep Reinforcement Learning}. Italian National Conference on Sensors.\n\n\\bibitem{huang2022or8}\nLanxiao Huang, Tyler Cody, Christopher Redino, et al. (2022). \\textit{Exposing Surveillance Detection Routes via Reinforcement Learning, Attack Graphs, and Cyber Terrain}. International Conference on Machine Learning and Applications.\n\n\\bibitem{modi2019fs3}\nAditya Modi, and Ambuj Tewari (2019). \\textit{No-regret Exploration in Contextual Reinforcement Learning}. Conference on Uncertainty in Artificial Intelligence.\n\n\\bibitem{shi20215ek}\nJiahe Shi, Yali Li, and Shengjin Wang (2021). \\textit{Partial Off-policy Learning: Balance Accuracy and Diversity for Human-Oriented Image Captioning}. IEEE International Conference on Computer Vision.\n\n\\bibitem{zhang2021qq6}\nHengzhe Zhang, and Aimin Zhou (2021). \\textit{RL-GEP: Symbolic Regression via Gene Expression Programming and Reinforcement Learning}. IEEE International Joint Conference on Neural Network.\n\n\\bibitem{yang2020dxb}\nDujia Yang, Xiaowei Qin, Xiaodong Xu, et al. (2020). \\textit{Sample Efficient Reinforcement Learning Method via High Efficient Episodic Memory}. IEEE Access.\n\n\\bibitem{bing2019py7}\nZhenshan Bing, Christian Lemke, Zhuangyi Jiang, et al. (2019). \\textit{Energy-Efficient Slithering Gait Exploration for a Snake-like Robot based on Reinforcement Learning}. International Joint Conference on Artificial Intelligence.\n\n\\bibitem{zhang20192ef}\nSongan Zhang, H. Peng, S. Nageshrao, et al. (2019). \\textit{Discretionary Lane Change Decision Making using Reinforcement Learning with Model-Based Exploration}. International Conference on Machine Learning and Applications.\n\n\\bibitem{hu2020yhq}\nChunyang Hu, and Meng Xu (2020). \\textit{Adaptive Exploration Strategy With Multi-Attribute Decision-Making for Reinforcement Learning}. IEEE Access.\n\n\\bibitem{kumar20216sy}\nK. N. Kumar, Irfan Essa, and Sehoon Ha (2021). \\textit{Graph-based Cluttered Scene Generation and Interactive Exploration using Deep Reinforcement Learning}. IEEE International Conference on Robotics and Automation.\n\n\\bibitem{asiain2018wxr}\nErick Asiain, J. Clempner, and A. Poznyak (2018). \\textit{Controller exploitation-exploration reinforcement learning architecture for computing near-optimal policies}. Soft Computing - A Fusion of Foundations, Methodologies and Applications.\n\n\\bibitem{li2019tj1}\nBoyao Li, Tao Lu, Jiayi Li, et al. (2019). \\textit{Curiosity-Driven Exploration for Off-Policy Reinforcement Learning Methods*}. IEEE International Conference on Robotics and Biomimetics.\n\n\\bibitem{sun2020c1p}\nHao Sun, Ziping Xu, Yuhang Song, et al. (2020). \\textit{Zeroth-Order Supervised Policy Improvement}. arXiv.org.\n\n\\bibitem{su2020k2m}\nYixuan Su, Deng Cai, Yan Wang, et al. (2020). \\textit{Stylistic Dialogue Generation via Information-Guided Reinforcement Learning Strategy}. arXiv.org.\n\n\\bibitem{liu2020o0c}\nHui Liu, Zhen Zhang, and Dongqing Wang (2020). \\textit{WRFMR: A Multi-Agent Reinforcement Learning Method for Cooperative Tasks}. IEEE Access.\n\n\\bibitem{ball20235zm}\nPhilip J. Ball, Laura M. Smith, Ilya Kostrikov, et al. (2023). \\textit{Efficient Online Reinforcement Learning with Offline Data}. International Conference on Machine Learning.\n\n\\bibitem{meng2025l1q}\nQing-ran Meng, Sheharyar Hussain, Fengzhang Luo, et al. (2025). \\textit{An Online Reinforcement Learning-Based Energy Management Strategy for Microgrids With Centralized Control}. IEEE transactions on industry applications.\n\n\\bibitem{dou2024kjg}\nShihan Dou, Yan Liu, Haoxiang Jia, et al. (2024). \\textit{StepCoder: Improve Code Generation with Reinforcement Learning from Compiler Feedback}. arXiv.org.\n\n\\bibitem{lee202337c}\nJonathan Lee, Annie Xie, Aldo Pacchiano, et al. (2023). \\textit{Supervised Pretraining Can Learn In-Context Reinforcement Learning}. Neural Information Processing Systems.\n\n\\bibitem{ma2024r2p}\nD. Ma, Xi Chen, Weihao Ma, et al. (2024). \\textit{Neural Network Model-Based Reinforcement Learning Control for AUV 3-D Path Following}. IEEE Transactions on Intelligent Vehicles.\n\n\\bibitem{matthews20241yx}\nMichael Matthews, Michael Beukman, Benjamin Ellis, et al. (2024). \\textit{Craftax: A Lightning-Fast Benchmark for Open-Ended Reinforcement Learning}. International Conference on Machine Learning.\n\n\\bibitem{xi2024e2i}\nMeng Xi, Huiao Dai, Jingyi He, et al. (2024). \\textit{A Lightweight Reinforcement-Learning-Based Real-Time Path-Planning Method for Unmanned Aerial Vehicles}. IEEE Internet of Things Journal.\n\n\\bibitem{zhang20242te}\nJing Zhang, Jian-Lin Ren, Yani Cui, et al. (2024). \\textit{Multi-USV Task Planning Method Based on Improved Deep Reinforcement Learning}. IEEE Internet of Things Journal.\n\n\\bibitem{xi2024tj9}\nZhiheng Xi, Wenxiang Chen, Boyang Hong, et al. (2024). \\textit{Training Large Language Models for Reasoning through Reverse Curriculum Reinforcement Learning}. International Conference on Machine Learning.\n\n\\bibitem{cheng2024vjq}\nNan Cheng, Xiucheng Wang, Zan Li, et al. (2024). \\textit{Toward Enhanced Reinforcement Learning-Based Resource Management via Digital Twin: Opportunities, Applications, and Challenges}. IEEE Network.\n\n\\bibitem{sun20238u5}\nYihao Sun, Jiajin Zhang, Chengxing Jia, et al. (2023). \\textit{Model-Bellman Inconsistency for Model-based Offline Reinforcement Learning}. International Conference on Machine Learning.\n\n\\bibitem{xu2023t6r}\nGuowei Xu, Ruijie Zheng, Yongyuan Liang, et al. (2023). \\textit{DrM: Mastering Visual Reinforcement Learning through Dormant Ratio Minimization}. International Conference on Learning Representations.\n\n\\bibitem{zhang2025wku}\nYi-Fan Zhang, Xingyu Lu, Xiao Hu, et al. (2025). \\textit{R1-Reward: Training Multimodal Reward Model Through Stable Reinforcement Learning}. arXiv.org.\n\n\\bibitem{lu2025j7f}\nGuanxing Lu, Wenkai Guo, Chubin Zhang, et al. (2025). \\textit{VLA-RL: Towards Masterful and General Robotic Manipulation with Scalable Reinforcement Learning}. arXiv.org.\n\n\\bibitem{jiang2023qmw}\nYiding Jiang, J. Z. Kolter, and R. Raileanu (2023). \\textit{On the Importance of Exploration for Generalization in Reinforcement Learning}. Neural Information Processing Systems.\n\n\\bibitem{zhang20244ty}\nRuoqing Zhang, Ziwei Luo, Jens Sjlund, et al. (2024). \\textit{Entropy-regularized Diffusion Policy with Q-Ensembles for Offline Reinforcement Learning}. Neural Information Processing Systems.\n\n\\bibitem{gu2024fu3}\nShangding Gu, Bilgehan Sel, Yuhao Ding, et al. (2024). \\textit{Balance Reward and Safety Optimization for Safe Reinforcement Learning: A Perspective of Gradient Manipulation}. AAAI Conference on Artificial Intelligence.\n\n\\bibitem{ma2024b33}\nRunyu Ma, Jelle Luijkx, Zlatan Ajanovi, et al. (2024). \\textit{ExploRLLM: Guiding Exploration in Reinforcement Learning with Large Language Models}. IEEE International Conference on Robotics and Automation.\n\n\\bibitem{yan2024p3y}\nXue Yan, Yan Song, Xidong Feng, et al. (2024). \\textit{Efficient Reinforcement Learning with Large Language Model Priors}. arXiv.org.\n\n\\bibitem{chen2023ymk}\nYinda Chen, Wei Huang, Shenglong Zhou, et al. (2023). \\textit{Self-Supervised Neuron Segmentation with Multi-Agent Reinforcement Learning}. International Joint Conference on Artificial Intelligence.\n\n\\bibitem{li2024drs}\nKuo Li, Xinze Jin, Qing-Shan Jia, et al. (2024). \\textit{An OCBA-Based Method for Efficient Sample Collection in Reinforcement Learning}. IEEE Transactions on Automation Science and Engineering.\n\n\\bibitem{huang202366f}\nTao Huang, Kai Chen, Bin Li, et al. (2023). \\textit{Demonstration-Guided Reinforcement Learning with Efficient Exploration for Task Automation of Surgical Robot}. IEEE International Conference on Robotics and Automation.\n\n\\bibitem{jin2024035}\nXinze Jin, Kuo Li, and Qing-Shan Jia (2024). \\textit{Constrained reinforcement learning with statewise projection: a control barrier function approach}. Science China Information Sciences.\n\n\\bibitem{ishfaq20235fo}\nHaque Ishfaq, Qingfeng Lan, Pan Xu, et al. (2023). \\textit{Provable and Practical: Efficient Exploration in Reinforcement Learning via Langevin Monte Carlo}. International Conference on Learning Representations.\n\n\\bibitem{guo2024sba}\nSiyuan Guo, Lixin Zou, Hechang Chen, et al. (2024). \\textit{Sample Efficient Offline-to-Online Reinforcement Learning}. IEEE Transactions on Knowledge and Data Engineering.\n\n\\bibitem{surina2025smk}\nAnja Surina, Amin Mansouri, Lars Quaedvlieg, et al. (2025). \\textit{Algorithm Discovery With LLMs: Evolutionary Search Meets Reinforcement Learning}. arXiv.org.\n\n\\bibitem{celik202575j}\nOnur Celik, Zechu Li, Denis Blessing, et al. (2025). \\textit{DIME:Diffusion-Based Maximum Entropy Reinforcement Learning}. arXiv.org.\n\n\\bibitem{hua2023omp}\nHean Hua, and Yongchun Fang (2023). \\textit{A Novel Reinforcement Learning-Based Robust Control Strategy for a Quadrotor}. IEEE transactions on industrial electronics (1982. Print).\n\n\\bibitem{sukhija2024zz8}\nBhavya Sukhija, Stelian Coros, Andreas Krause, et al. (2024). \\textit{MaxInfoRL: Boosting exploration in reinforcement learning through information gain maximization}. International Conference on Learning Representations.\n\n\\bibitem{hsu2024tqd}\nHao-Lun Hsu, Weixin Wang, Miroslav Pajic, et al. (2024). \\textit{Randomized Exploration in Cooperative Multi-Agent Reinforcement Learning}. Neural Information Processing Systems.\n\n\\bibitem{wang20248rm}\nZiyi Wang, Xinran Li, Luoyang Sun, et al. (2024). \\textit{Learning State-Specific Action Masks for Reinforcement Learning}. Algorithms.\n\n\\bibitem{ghamari2024bbm}\nS. Ghamari, Mojtaba Hajihosseini, D. Habibi, et al. (2024). \\textit{Design of an Adaptive Robust PI Controller for DC/DC Boost Converter Using Reinforcement-Learning Technique and Snake Optimization Algorithm}. IEEE Access.\n\n\\bibitem{zhang2024ppn}\nZhengran Zhang, Qi Liu, Yanjie Li, et al. (2024). \\textit{Safe Reinforcement Learning in Autonomous Driving With Epistemic Uncertainty Estimation}. IEEE transactions on intelligent transportation systems (Print).\n\n\\bibitem{dai2024x3l}\nZhenwen Dai, Federico Tomasi, and Sina Ghiassian (2024). \\textit{In-context Exploration-Exploitation for Reinforcement Learning}. International Conference on Learning Representations.\n\n\\bibitem{shuai2025fq3}\nBin Shuai, Min Hua, Yanfei Li, et al. (2025). \\textit{Optimal Energy Management of Plug-In Hybrid Electric Vehicles Through Ensemble Reinforcement Learning With Exploration-to-Exploitation Ratio Control}. IEEE Transactions on Intelligent Vehicles.\n\n\\bibitem{stolz20240y2}\nRoland Stolz, Hanna Krasowski, Jakob Thumm, et al. (2024). \\textit{Excluding the Irrelevant: Focusing Reinforcement Learning through Continuous Action Masking}. Neural Information Processing Systems.\n\n\\bibitem{tappler2024nm1}\nMartin Tappler, Andrea Pferscher, B. Aichernig, et al. (2024). \\textit{Learning and Repair of Deep Reinforcement Learning Policies from Fuzz-Testing Data}. International Conference on Software Engineering.\n\n\\bibitem{rimon20243o6}\nZohar Rimon, Tom Jurgenson, Orr Krupnik, et al. (2024). \\textit{MAMBA: an Effective World Model Approach for Meta-Reinforcement Learning}. International Conference on Learning Representations.\n\n\\bibitem{terven202599m}\nJuan R. Terven (2025). \\textit{Deep Reinforcement Learning: A Chronological Overview and Methods}. Applied Informatics.\n\n\\bibitem{hsiao2024wps}\nHao-Hsiang Hsiao, Yi-Chen Lu, Pruek Vanna-iampikul, et al. (2024). \\textit{FastTuner: Transferable Physical Design Parameter Optimization using Fast Reinforcement Learning}. ACM International Symposium on Physical Design.\n\n\\bibitem{kakooee2024w9m}\nR. Kakooee, and B. Dillenburger (2024). \\textit{Reimagining space layout design through deep reinforcement learning}. Journal of Computational Design and Engineering.\n\n\\bibitem{rafailov2024wtw}\nRafael Rafailov, K. Hatch, Anikait Singh, et al. (2024). \\textit{D5RL: Diverse Datasets for Data-Driven Deep Reinforcement Learning}. RLJ.\n\n\\bibitem{goldie2024cuf}\nA. D. Goldie, Chris Lu, Matthew Jackson, et al. (2024). \\textit{Can Learned Optimization Make Reinforcement Learning Less Difficult?}. Neural Information Processing Systems.\n\n\\bibitem{coelho2024oa6}\nDaniel Coelho, Miguel Oliveira, and Vitor Santos (2024). \\textit{RLfOLD: Reinforcement Learning from Online Demonstrations in Urban Autonomous Driving}. AAAI Conference on Artificial Intelligence.\n\n\\bibitem{huang2024nh4}\nYanjun Huang, Yuxiao Gu, Kang Yuan, et al. (2024). \\textit{Human Knowledge Enhanced Reinforcement Learning for Mandatory Lane-Change of Autonomous Vehicles in Congested Traffic}. IEEE Transactions on Intelligent Vehicles.\n\n\\bibitem{cho2023z4l}\nDaesol Cho, Seungjae Lee, and H. J. Kim (2023). \\textit{Outcome-directed Reinforcement Learning by Uncertainty & Temporal Distance-Aware Curriculum Goal Generation}. International Conference on Learning Representations.\n\n\\bibitem{shi20258tu}\nYucheng Shi, Wenhao Yu, Zaitang Li, et al. (2025). \\textit{MobileGUI-RL: Advancing Mobile GUI Agent through Reinforcement Learning in Online Environment}. arXiv.org.\n\n\\bibitem{li2024ge1}\nGe Li, Hongyi Zhou, Dominik Roth, et al. (2024). \\textit{Open the Black Box: Step-based Policy Updates for Temporally-Correlated Episodic Reinforcement Learning}. International Conference on Learning Representations.\n\n\\bibitem{ghasemi2024j43}\nMajid Ghasemi, Amir Hossein Moosavi, and Dariush Ebrahimi (2024). \\textit{A Comprehensive Survey of Reinforcement Learning: From Algorithms to Practical Challenges}. Unpublished manuscript.\n\n\\bibitem{liu2023729}\nZhen-yu Liu, Ke Wang, Daxin Liu, et al. (2023). \\textit{A Motion Planning Method for Visual Servoing Using Deep Reinforcement Learning in Autonomous Robotic Assembly}. IEEE/ASME transactions on mechatronics.\n\n\\bibitem{shang202305k}\nZhiwei Shang, Renxing Li, Chunhuang Zheng, et al. (2023). \\textit{Relative Entropy Regularized Sample-Efficient Reinforcement Learning With Continuous Actions}. IEEE Transactions on Neural Networks and Learning Systems.\n\n\\bibitem{liu2024xkk}\nZeyang Liu, Lipeng Wan, Xinrui Yang, et al. (2024). \\textit{Imagine, Initialize, and Explore: An Effective Exploration Method in Multi-Agent Reinforcement Learning}. AAAI Conference on Artificial Intelligence.\n\n\\bibitem{kantaros2024sgn}\nY. Kantaros, and Jun Wang (2024). \\textit{Sample-Efficient Reinforcement Learning With Temporal Logic Objectives: Leveraging the Task Specification to Guide Exploration}. IEEE Transactions on Automatic Control.\n\n\\bibitem{li2024zix}\nSiqi Li, Jun Chen, Shanqi Liu, et al. (2024). \\textit{MCMC: Multi-Constrained Model Compression via One-Stage Envelope Reinforcement Learning}. IEEE Transactions on Neural Networks and Learning Systems.\n\n\\bibitem{ang2024t27}\nDony Ang, Cyril Rakovski, and H. Atamian (2024). \\textit{De Novo Drug Design Using Transformer-Based Machine Translation and Reinforcement Learning of an Adaptive Monte Carlo Tree Search}. Pharmaceuticals.\n\n\\bibitem{kim2024qde}\nJangsaeng Kim, Wonjun Shin, Jiyong Yim, et al. (2024). \\textit{Toward Optimized InMemory Reinforcement Learning: Leveraging 1/f Noise of Synaptic Ferroelectric FieldEffectTransistors for Efficient Exploration}. Advanced Intelligent Systems.\n\n\\bibitem{ma2024jej}\nChengzhong Ma, Deyu Yang, Tianyu Wu, et al. (2024). \\textit{Improving Offline Reinforcement Learning With in-Sample Advantage Regularization for Robot Manipulation}. IEEE Transactions on Neural Networks and Learning Systems.\n\n\\bibitem{ge20243g0}\nZhihong Ge, Xingshuo Li, Fei Xu, et al. (2024). \\textit{An Improved Distributed Maximum Power Point Tracking Technique in Photovoltaic Systems Based on Reinforcement Learning Algorithm}. IEEE Journal of Emerging and Selected Topics in Industrial Electronics.\n\n\\bibitem{lu2024ush}\nFeiyu Lu, Mengyu Chen, Hsiang Hsu, et al. (2024). \\textit{Adaptive 3D UI Placement in Mixed Reality Using Deep Reinforcement Learning}. CHI Extended Abstracts.\n\n\\bibitem{yuan2023m1m}\nJianyong Yuan, Peiyu Wang, Junjie Ye, et al. (2023). \\textit{EasySO: Exploration-enhanced Reinforcement Learning for Logic Synthesis Sequence Optimization and a Comprehensive RL Environment}. 2023 IEEE/ACM International Conference on Computer Aided Design (ICCAD).\n\n\\bibitem{zheng2023u9k}\nBowen Zheng, and Ran Cheng (2023). \\textit{Rethinking Population-assisted Off-policy Reinforcement Learning}. Annual Conference on Genetic and Evolutionary Computation.\n\n\\bibitem{wang20241f3}\nTianfu Wang, Qilin Fan, Chao Wang, et al. (2024). \\textit{FlagVNE: A Flexible and Generalizable Reinforcement Learning Framework for Network Resource Allocation}. International Joint Conference on Artificial Intelligence.\n\n\\bibitem{mahankali20248dx}\nSrinath Mahankali, Zhang-Wei Hong, Ayush Sekhari, et al. (2024). \\textit{Random Latent Exploration for Deep Reinforcement Learning}. International Conference on Machine Learning.\n\n\\bibitem{yoon2024lff}\nYoungsik Yoon, Gangbok Lee, Sungsoo Ahn, et al. (2024). \\textit{Breadth-First Exploration on Adaptive Grid for Reinforcement Learning}. International Conference on Machine Learning.\n\n\\bibitem{ishfaq20245to}\nHaque Ishfaq, Yixin Tan, Yu Yang, et al. (2024). \\textit{More Efficient Randomized Exploration for Reinforcement Learning via Approximate Sampling}. RLJ.\n\n\\bibitem{santi2024hct}\nRic De Santi, Manish Prajapat, and Andreas Krause (2024). \\textit{Global Reinforcement Learning: Beyond Linear and Convex Rewards via Submodular Semi-gradient Methods}. International Conference on Machine Learning.\n\n\\bibitem{pham2024j80}\nVan-Hau Pham, Do Thi Thu Hien, Nguyen Phuc Chuong, et al. (2024). \\textit{A Coverage-Guided Fuzzing Method for Automatic Software Vulnerability Detection Using Reinforcement Learning-Enabled Multi-Level Input Mutation}. IEEE Access.\n\n\\bibitem{ding2023whs}\nWei Ding, Siyang Jiang, Hsi-Wen Chen, et al. (2023). \\textit{Incremental Reinforcement Learning with Dual-Adaptive -Greedy Exploration}. AAAI Conference on Artificial Intelligence.\n\n\\bibitem{malaiyappan20245bh}\nJesu Narkarunai Arasu Malaiyappan, Sai Mani Krishna Sistla, and Jawaharbabu Jeyaraman (2024). \\textit{Advancements in Reinforcement Learning Algorithms for Autonomous Systems}. International Journal of Innovative Science and Research Technology.\n\n\\bibitem{gan2023o50}\nXuemei Gan, Ying Zuo, Ansi Zhang, et al. (2023). \\textit{Digital twin-enabled adaptive scheduling strategy based on deep reinforcement learning}. Science China Technological Sciences.\n\n\\bibitem{zhao2023cay}\nKai-Wen Zhao, Yi Ma, Jinyi Liu, et al. (2023). \\textit{Ensemble-based Offline-to-Online Reinforcement Learning: From Pessimistic Learning to Optimistic Exploration}. arXiv.org.\n\n\\bibitem{xu2023m9r}\nKaidi Xu, Shenglong Zhou, and Geoffrey Ye Li (2023). \\textit{Federated Reinforcement Learning for Resource Allocation in V2X Networks}. IEEE Vehicular Technology Conference.\n\n\\bibitem{khlif2023zg3}\nNesrine Khlif, Khraief-Hadded Nahla, and Belghith Safya (2023). \\textit{Reinforcement learning with modified exploration strategy for mobile robot path planning}. Robotica (Cambridge. Print).\n\n\\bibitem{sreedharan2023nae}\nS. Sreedharan, and Michael Katz (2023). \\textit{Optimistic Exploration in Reinforcement Learning Using Symbolic Model Estimates}. Neural Information Processing Systems.\n\n\\bibitem{guo20233sd}\nSiyuan Guo, Yanchao Sun, Jifeng Hu, et al. (2023). \\textit{A Simple Unified Uncertainty-Guided Framework for Offline-to-Online Reinforcement Learning}. arXiv.org.\n\n\\bibitem{zhang2023wqi}\nXiao Zhang, Hai Zhang, Hongtu Zhou, et al. (2023). \\textit{Safe Reinforcement Learning With Dead-Ends Avoidance and Recovery}. IEEE Robotics and Automation Letters.\n\n\\bibitem{beikmohammadi2023v6w}\nAli Beikmohammadi, and S. Magnsson (2023). \\textit{TA-Explore: Teacher-Assisted Exploration for Facilitating Fast Reinforcement Learning}. Adaptive Agents and Multi-Agent Systems.\n\n\\bibitem{yang2023n56}\nQisong Yang, T. D. Simo, N. Jansen, et al. (2023). \\textit{Reinforcement Learning by Guided Safe Exploration}. European Conference on Artificial Intelligence.\n\n\\bibitem{alvarez2023v09}\nJonatan Alvarez, Assia Belbachir, Faiza Belbachir, et al. (2023). \\textit{Forest Fire Localization: From Reinforcement Learning Exploration to a Dynamic Drone Control}. Journal of Intelligent and Robotic Systems.\n\n\\bibitem{li2023kgk}\nTianyi Li, Gen-ke Yang, and Jian Chu (2023). \\textit{Implicit Posteriori Parameter Distribution Optimization in Reinforcement Learning}. IEEE Transactions on Cybernetics.\n\n\\bibitem{zi20238ug}\nYuan Zi, Lei Fan, Xuqing Wu, et al. (2023). \\textit{Active Gamma-Ray Log Pattern Localization With Distributionally Robust Reinforcement Learning}. IEEE Transactions on Geoscience and Remote Sensing.\n\n\\bibitem{yang2023w3h}\nJunjun Yang, Kaige Tan, Lei Feng, et al. (2023). \\textit{Reducing the Learning Time of Reinforcement Learning for the Supervisory Control of Discrete Event Systems}. IEEE Access.\n\n\\bibitem{sun20219kr}\nChuxiong Sun, Rui Wang, Qian Li, et al. (2021). \\textit{Reward Space Noise for Exploration in Deep Reinforcement Learning}. International journal of pattern recognition and artificial intelligence.\n\n\\bibitem{guo2022y6b}\nZijing Guo, Chendie Yao, Yanghe Feng, et al. (2022). \\textit{Survey of Reinforcement Learning based on Human Prior Knowledge}. Journal of Uncertain Systems.\n\n\\bibitem{mazumder2022deb}\nSahisnu Mazumder, Bing Liu, Shuai Wang, et al. (2022). \\textit{Knowledge-Guided Exploration in Deep Reinforcement Learning}. arXiv.org.\n\n\\bibitem{oh2022cei}\nJi-Yun Oh, Joonkee Kim, and Se-Young Yun (2022). \\textit{Risk Perspective Exploration in Distributional Reinforcement Learning}. arXiv.org.\n\n\\bibitem{vidakovi2020q23}\nJ. Vidakovi, B. Jerbi, B. ekoranja, et al. (2020). \\textit{Accelerating Robot Trajectory Learning for Stochastic Tasks}. IEEE Access.\n\n\\bibitem{sun2024kxq}\nXianzhuo Sun, Zhao Xu, Jing Qiu, et al. (2024). \\textit{Optimal Volt/Var Control for Unbalanced Distribution Networks With Human-in-the-Loop Deep Reinforcement Learning}. IEEE Transactions on Smart Grid.\n\n\\bibitem{yu2024x53}\nPeipei Yu, Zhen-yu Wang, Hongcai Zhang, et al. (2024). \\textit{Safe Reinforcement Learning for Power System Control: A Review}. arXiv.org.\n\n\\bibitem{wang2024htz}\nMing Wang, Jie Zhang, Peng Zhang, et al. (2024). \\textit{Cooperative multi-agent reinforcement learning for multi-area integrated scheduling in wafer fabs}. International Journal of Production Research.\n\n\\bibitem{ding20246hx}\nYunlong Ding, Minchi Kuang, Heng Shi, et al. (2024). \\textit{Multi-UAV Cooperative Target Assignment Method Based on Reinforcement Learning}. Drones.\n\n\\bibitem{yang2024yh9}\nJingwen Yang, Ping Wang, and Yongfeng Ju (2024). \\textit{Variable Speed Limit Intelligent Decision-Making Control Strategy Based on Deep Reinforcement Learning under Emergencies}. Sustainability.\n\n\\bibitem{afroosheh2024id4}\nSajjad Afroosheh, Khodakhast Esapour, Reza KhorramNia, et al. (2024). \\textit{Reinforcement learning layoutbased optimal energy management in smart home: AIbased approach}. IET Generation, Transmission &amp; Distribution.\n\n\\bibitem{dong2025887}\nYihong Dong, Xue Jiang, Yongding Tao, et al. (2025). \\textit{RL-PLUS: Countering Capability Boundary Collapse of LLMs in Reinforcement Learning with Hybrid-policy Optimization}. arXiv.org.\n\n\\bibitem{zhu2024sb0}\nQingling Zhu, Xiaoqiang Wu, Qiuzhen Lin, et al. (2024). \\textit{Two-Stage Evolutionary Reinforcement Learning for Enhancing Exploration and Exploitation}. AAAI Conference on Artificial Intelligence.\n\n\\bibitem{xiang2024qhz}\nXuanchen Xiang, Ruisheng Diao, S. Bernadin, et al. (2024). \\textit{An Intelligent Parameter Identification Method of DFIG Systems Using Hybrid Particle Swarm Optimization and Reinforcement Learning}. IEEE Access.\n\n\\bibitem{qi2024hxq}\nJi Qi, Haibo Gao, Huanli Su, et al. (2024). \\textit{Reinforcement Learning and Sim-to-Real Transfer of Reorientation and Landing Control for Quadruped Robots on Asteroids}. IEEE transactions on industrial electronics (1982. Print).\n\n\\bibitem{zhang2024wgo}\nBolei Zhang, Fu Xiao, and Lifa Wu (2024). \\textit{Offline Reinforcement Learning for Asynchronous Task Offloading in Mobile Edge Computing}. IEEE Transactions on Network and Service Management.\n\n\\bibitem{sun2024edc}\nSiqing Sun, Huachao Dong, and Tianbo Li (2024). \\textit{A modified evolutionary reinforcement learning for multi-agent region protection with fewer defenders}. Complex &amp; Intelligent Systems.\n\n\\bibitem{dunsin2024e5w}\nDipo Dunsin, Mohamed Chahine Ghanem, Karim Ouazzane, et al. (2024). \\textit{Reinforcement Learning for an Efficient and Effective Malware Investigation during Cyber Incident Response}. arXiv.org.\n\n\\bibitem{hu2024085}\nHaotian Hu, Yiqin Yang, Jianing Ye, et al. (2024). \\textit{Bayesian Design Principles for Offline-to-Online Reinforcement Learning}. International Conference on Machine Learning.\n\n\\bibitem{ji2024gkw}\nChang-Hoon Ji, Dong-Hee Shin, Young-Han Son, et al. (2024). \\textit{Sparse Graph Representation Learning Based on Reinforcement Learning for Personalized Mild Cognitive Impairment (MCI) Diagnosis}. IEEE journal of biomedical and health informatics.\n\n\\bibitem{parisi2024u3o}\nSimone Parisi, Alireza Kazemipour, and Michael Bowling (2024). \\textit{Beyond Optimism: Exploration With Partially Observable Rewards}. Neural Information Processing Systems.\n\n\\bibitem{wang2024anu}\nYiming Wang, Kaiyan Zhao, Furui Liu, et al. (2024). \\textit{Rethinking Exploration in Reinforcement Learning with Effective Metric-Based Exploration Bonus}. Neural Information Processing Systems.\n\n\\bibitem{wu2024mak}\nFan Wu, Rui Zhang, Qi Yi, et al. (2024). \\textit{OCEAN-MBRL: Offline Conservative Exploration for Model-Based Offline Reinforcement Learning}. AAAI Conference on Artificial Intelligence.\n\n\\bibitem{zhao2024714}\nDongfang Zhao, Huanshi Xu, and Zhang Xun (2024). \\textit{Active Exploration Deep Reinforcement Learning for Continuous Action Space with Forward Prediction}. International Journal of Computational Intelligence Systems.\n\n\\bibitem{hua2025fq5}\nHean Hua, Yaonan Wang, Hang Zhong, et al. (2025). \\textit{A Novel Guided Deep Reinforcement Learning Tracking Control Strategy for Multirotors}. IEEE Transactions on Automation Science and Engineering.\n\n\\bibitem{dai2025h8g}\nRunpeng Dai, Linfeng Song, Haolin Liu, et al. (2025). \\textit{CDE: Curiosity-Driven Exploration for Efficient Reinforcement Learning in Large Language Models}. Unpublished manuscript.\n\n\\bibitem{chang2024u7x}\nXin Chang, Yanbin Li, Guanjie Zhang, et al. (2024). \\textit{An Improved Reinforcement Learning Method Based on Unsupervised Learning}. IEEE Access.\n\n\\bibitem{janjua2024yhk}\nJ. Janjua, Shagufta Kousar, Areeba Khan, et al. (2024). \\textit{Enhancing Scalability in Reinforcement Learning for Open Spaces}. 2024 International Conference on Decision Aid Sciences and Applications (DASA).\n\n\\bibitem{ledesma2024zzm}\nJorge Val Ledesma, Rafa Winiewski, C. Kallese, et al. (2024). \\textit{Water Age Control for Water Distribution Networks via Safe Reinforcement Learning}. IEEE Transactions on Control Systems Technology.\n\n\\bibitem{wu20248f9}\nMingkang Wu, Umer Siddique, Abhinav Sinha, et al. (2024). \\textit{Offline Reinforcement Learning with Failure Under Sparse Reward Environments}. International Conference on Multimodal Interaction.\n\n\\bibitem{honari202473t}\nHomayoun Honari, Amir M. Soufi Enayati, Mehran Ghafarian Tamizi, et al. (2024). \\textit{Meta SAC-Lag: Towards Deployable Safe Reinforcement Learning via MetaGradient-based Hyperparameter Tuning}. IEEE/RJS International Conference on Intelligent RObots and Systems.\n\n\\bibitem{shi2024g6o}\nJiamin Shi, Tangyike Zhang, Ziqi Zong, et al. (2024). \\textit{Task-Driven Autonomous Driving: Balanced Strategies Integrating Curriculum Reinforcement Learning and Residual Policy}. IEEE Robotics and Automation Letters.\n\n\\bibitem{lu2025caz}\nThinh Lu, Divyam Sobti, Deepak Talwar, et al. (2025). \\textit{Reinforcement learning-based dynamic field exploration and reconstruction using multi-robot systems for environmental monitoring}. Frontiers Robotics AI.\n\n\\bibitem{hou20248b2}\nMuhan Hou, Koen V. Hindriks, Guszti Eiben, et al. (2024). \\textit{``Give Me an Example Like This'': Episodic Active Reinforcement Learning from Demonstrations}. International Conference on Human-Agent Interaction.\n\n\\end{thebibliography}\n\n\\end{document}",
  "generation_date": "2025-10-07T12:31:38.574280",
  "processed_papers_data": [
    {
      "success": true,
      "doc_id": "027f9af31d92d5dde654323176c8ac08",
      "summary": "Exploration in environments with sparse rewards has been a persistent problem in reinforcement learning (RL). Many tasks are natural to specify with a sparse reward, and manually shaping a reward function can result in suboptimal performance. However, finding a non-zero reward is exponentially more difficult with increasing task horizon or action dimensionality. This puts many real-world tasks out of practical reach of RL methods. In this work, we use demonstrations to overcome the exploration problem and successfully learn to perform long-horizon, multi-step robotics tasks with continuous control such as stacking blocks with a robot arm. Our method, which builds on top of Deep Deterministic Policy Gradients and Hindsight Experience Replay, provides an order of magnitude of speedup over RL on simulated robotics tasks. It is simple to implement and makes only the additional assumption that we can collect a small set of demonstrations. Furthermore, our method is able to solve tasks not solvable by either RL or behavior cloning alone, and often ends up outperforming the demonstrator policy.",
      "intriguing_abstract": "Exploration in environments with sparse rewards has been a persistent problem in reinforcement learning (RL). Many tasks are natural to specify with a sparse reward, and manually shaping a reward function can result in suboptimal performance. However, finding a non-zero reward is exponentially more difficult with increasing task horizon or action dimensionality. This puts many real-world tasks out of practical reach of RL methods. In this work, we use demonstrations to overcome the exploration problem and successfully learn to perform long-horizon, multi-step robotics tasks with continuous control such as stacking blocks with a robot arm. Our method, which builds on top of Deep Deterministic Policy Gradients and Hindsight Experience Replay, provides an order of magnitude of speedup over RL on simulated robotics tasks. It is simple to implement and makes only the additional assumption that we can collect a small set of demonstrations. Furthermore, our method is able to solve tasks not solvable by either RL or behavior cloning alone, and often ends up outperforming the demonstrator policy.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/c28ec2a40a2c77e20d64cf1c85dc931106df8e83.pdf",
      "citation_key": "nair2017crs",
      "metadata": {
        "title": "Overcoming Exploration in Reinforcement Learning with Demonstrations",
        "authors": [
          "Ashvin Nair",
          "Bob McGrew",
          "Marcin Andrychowicz",
          "Wojciech Zaremba",
          "P. Abbeel"
        ],
        "published_date": "2017",
        "abstract": "Exploration in environments with sparse rewards has been a persistent problem in reinforcement learning (RL). Many tasks are natural to specify with a sparse reward, and manually shaping a reward function can result in suboptimal performance. However, finding a non-zero reward is exponentially more difficult with increasing task horizon or action dimensionality. This puts many real-world tasks out of practical reach of RL methods. In this work, we use demonstrations to overcome the exploration problem and successfully learn to perform long-horizon, multi-step robotics tasks with continuous control such as stacking blocks with a robot arm. Our method, which builds on top of Deep Deterministic Policy Gradients and Hindsight Experience Replay, provides an order of magnitude of speedup over RL on simulated robotics tasks. It is simple to implement and makes only the additional assumption that we can collect a small set of demonstrations. Furthermore, our method is able to solve tasks not solvable by either RL or behavior cloning alone, and often ends up outperforming the demonstrator policy.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/c28ec2a40a2c77e20d64cf1c85dc931106df8e83.pdf",
        "venue": "IEEE International Conference on Robotics and Automation",
        "citationCount": 814,
        "score": 101.75,
        "summary": "Exploration in environments with sparse rewards has been a persistent problem in reinforcement learning (RL). Many tasks are natural to specify with a sparse reward, and manually shaping a reward function can result in suboptimal performance. However, finding a non-zero reward is exponentially more difficult with increasing task horizon or action dimensionality. This puts many real-world tasks out of practical reach of RL methods. In this work, we use demonstrations to overcome the exploration problem and successfully learn to perform long-horizon, multi-step robotics tasks with continuous control such as stacking blocks with a robot arm. Our method, which builds on top of Deep Deterministic Policy Gradients and Hindsight Experience Replay, provides an order of magnitude of speedup over RL on simulated robotics tasks. It is simple to implement and makes only the additional assumption that we can collect a small set of demonstrations. Furthermore, our method is able to solve tasks not solvable by either RL or behavior cloning alone, and often ends up outperforming the demonstrator policy.",
        "keywords": []
      },
      "file_name": "c28ec2a40a2c77e20d64cf1c85dc931106df8e83.pdf"
    },
    {
      "success": true,
      "doc_id": "994037ed87ede9ff73db879622b3a53a",
      "summary": "Count-based exploration algorithms are known to perform near-optimally when used in conjunction with tabular reinforcement learning (RL) methods for solving small discrete Markov decision processes (MDPs). It is generally thought that count-based methods cannot be applied in high-dimensional state spaces, since most states will only occur once. Recent deep RL exploration strategies are able to deal with high-dimensional continuous state spaces through complex heuristics, often relying on optimism in the face of uncertainty or intrinsic motivation. In this work, we describe a surprising finding: a simple generalization of the classic count-based approach can reach near state-of-the-art performance on various high-dimensional and/or continuous deep RL benchmarks. States are mapped to hash codes, which allows to count their occurrences with a hash table. These counts are then used to compute a reward bonus according to the classic count-based exploration theory. We find that simple hash functions can achieve surprisingly good results on many challenging tasks. Furthermore, we show that a domain-dependent learned hash code may further improve these results. Detailed analysis reveals important aspects of a good hash function: 1) having appropriate granularity and 2) encoding information relevant to solving the MDP. This exploration strategy achieves near state-of-the-art performance on both continuous control tasks and Atari 2600 games, hence providing a simple yet powerful baseline for solving MDPs that require considerable exploration.",
      "intriguing_abstract": "Count-based exploration algorithms are known to perform near-optimally when used in conjunction with tabular reinforcement learning (RL) methods for solving small discrete Markov decision processes (MDPs). It is generally thought that count-based methods cannot be applied in high-dimensional state spaces, since most states will only occur once. Recent deep RL exploration strategies are able to deal with high-dimensional continuous state spaces through complex heuristics, often relying on optimism in the face of uncertainty or intrinsic motivation. In this work, we describe a surprising finding: a simple generalization of the classic count-based approach can reach near state-of-the-art performance on various high-dimensional and/or continuous deep RL benchmarks. States are mapped to hash codes, which allows to count their occurrences with a hash table. These counts are then used to compute a reward bonus according to the classic count-based exploration theory. We find that simple hash functions can achieve surprisingly good results on many challenging tasks. Furthermore, we show that a domain-dependent learned hash code may further improve these results. Detailed analysis reveals important aspects of a good hash function: 1) having appropriate granularity and 2) encoding information relevant to solving the MDP. This exploration strategy achieves near state-of-the-art performance on both continuous control tasks and Atari 2600 games, hence providing a simple yet powerful baseline for solving MDPs that require considerable exploration.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/0fcb2034e31a2bc2f12a2b1363d0d77baf445fdf.pdf",
      "citation_key": "tang20166wr",
      "metadata": {
        "title": "#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning",
        "authors": [
          "Haoran Tang",
          "Rein Houthooft",
          "Davis Foote",
          "Adam Stooke",
          "Xi Chen",
          "Yan Duan",
          "John Schulman",
          "F. Turck",
          "P. Abbeel"
        ],
        "published_date": "2016",
        "abstract": "Count-based exploration algorithms are known to perform near-optimally when used in conjunction with tabular reinforcement learning (RL) methods for solving small discrete Markov decision processes (MDPs). It is generally thought that count-based methods cannot be applied in high-dimensional state spaces, since most states will only occur once. Recent deep RL exploration strategies are able to deal with high-dimensional continuous state spaces through complex heuristics, often relying on optimism in the face of uncertainty or intrinsic motivation. In this work, we describe a surprising finding: a simple generalization of the classic count-based approach can reach near state-of-the-art performance on various high-dimensional and/or continuous deep RL benchmarks. States are mapped to hash codes, which allows to count their occurrences with a hash table. These counts are then used to compute a reward bonus according to the classic count-based exploration theory. We find that simple hash functions can achieve surprisingly good results on many challenging tasks. Furthermore, we show that a domain-dependent learned hash code may further improve these results. Detailed analysis reveals important aspects of a good hash function: 1) having appropriate granularity and 2) encoding information relevant to solving the MDP. This exploration strategy achieves near state-of-the-art performance on both continuous control tasks and Atari 2600 games, hence providing a simple yet powerful baseline for solving MDPs that require considerable exploration.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/0fcb2034e31a2bc2f12a2b1363d0d77baf445fdf.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 798,
        "score": 88.66666666666666,
        "summary": "Count-based exploration algorithms are known to perform near-optimally when used in conjunction with tabular reinforcement learning (RL) methods for solving small discrete Markov decision processes (MDPs). It is generally thought that count-based methods cannot be applied in high-dimensional state spaces, since most states will only occur once. Recent deep RL exploration strategies are able to deal with high-dimensional continuous state spaces through complex heuristics, often relying on optimism in the face of uncertainty or intrinsic motivation. In this work, we describe a surprising finding: a simple generalization of the classic count-based approach can reach near state-of-the-art performance on various high-dimensional and/or continuous deep RL benchmarks. States are mapped to hash codes, which allows to count their occurrences with a hash table. These counts are then used to compute a reward bonus according to the classic count-based exploration theory. We find that simple hash functions can achieve surprisingly good results on many challenging tasks. Furthermore, we show that a domain-dependent learned hash code may further improve these results. Detailed analysis reveals important aspects of a good hash function: 1) having appropriate granularity and 2) encoding information relevant to solving the MDP. This exploration strategy achieves near state-of-the-art performance on both continuous control tasks and Atari 2600 games, hence providing a simple yet powerful baseline for solving MDPs that require considerable exploration.",
        "keywords": []
      },
      "file_name": "0fcb2034e31a2bc2f12a2b1363d0d77baf445fdf.pdf"
    },
    {
      "success": true,
      "doc_id": "4c6763c04b95af4f8f5134ef1377c07b",
      "summary": "Conveying complex objectives to reinforcement learning (RL) agents can often be difficult, involving meticulous design of reward functions that are sufficiently informative yet easy enough to provide. Human-in-the-loop RL methods allow practitioners to instead interactively teach agents through tailored feedback; however, such approaches have been challenging to scale since human feedback is very expensive. In this work, we aim to make this process more sample- and feedback-efficient. We present an off-policy, interactive RL algorithm that capitalizes on the strengths of both feedback and off-policy learning. Specifically, we learn a reward model by actively querying a teacher's preferences between two clips of behavior and use it to train an agent. To enable off-policy learning, we relabel all the agent's past experience when its reward model changes. We additionally show that pre-training our agents with unsupervised exploration substantially increases the mileage of its queries. We demonstrate that our approach is capable of learning tasks of higher complexity than previously considered by human-in-the-loop methods, including a variety of locomotion and robotic manipulation skills. We also show that our method is able to utilize real-time human feedback to effectively prevent reward exploitation and learn new behaviors that are difficult to specify with standard reward functions.",
      "intriguing_abstract": "Conveying complex objectives to reinforcement learning (RL) agents can often be difficult, involving meticulous design of reward functions that are sufficiently informative yet easy enough to provide. Human-in-the-loop RL methods allow practitioners to instead interactively teach agents through tailored feedback; however, such approaches have been challenging to scale since human feedback is very expensive. In this work, we aim to make this process more sample- and feedback-efficient. We present an off-policy, interactive RL algorithm that capitalizes on the strengths of both feedback and off-policy learning. Specifically, we learn a reward model by actively querying a teacher's preferences between two clips of behavior and use it to train an agent. To enable off-policy learning, we relabel all the agent's past experience when its reward model changes. We additionally show that pre-training our agents with unsupervised exploration substantially increases the mileage of its queries. We demonstrate that our approach is capable of learning tasks of higher complexity than previously considered by human-in-the-loop methods, including a variety of locomotion and robotic manipulation skills. We also show that our method is able to utilize real-time human feedback to effectively prevent reward exploitation and learn new behaviors that are difficult to specify with standard reward functions.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/45f573f302dc7e77cbc5d1a74ccbac3564bbebc8.pdf",
      "citation_key": "lee2021qzk",
      "metadata": {
        "title": "PEBBLE: Feedback-Efficient Interactive Reinforcement Learning via Relabeling Experience and Unsupervised Pre-training",
        "authors": [
          "Kimin Lee",
          "Laura M. Smith",
          "P. Abbeel"
        ],
        "published_date": "2021",
        "abstract": "Conveying complex objectives to reinforcement learning (RL) agents can often be difficult, involving meticulous design of reward functions that are sufficiently informative yet easy enough to provide. Human-in-the-loop RL methods allow practitioners to instead interactively teach agents through tailored feedback; however, such approaches have been challenging to scale since human feedback is very expensive. In this work, we aim to make this process more sample- and feedback-efficient. We present an off-policy, interactive RL algorithm that capitalizes on the strengths of both feedback and off-policy learning. Specifically, we learn a reward model by actively querying a teacher's preferences between two clips of behavior and use it to train an agent. To enable off-policy learning, we relabel all the agent's past experience when its reward model changes. We additionally show that pre-training our agents with unsupervised exploration substantially increases the mileage of its queries. We demonstrate that our approach is capable of learning tasks of higher complexity than previously considered by human-in-the-loop methods, including a variety of locomotion and robotic manipulation skills. We also show that our method is able to utilize real-time human feedback to effectively prevent reward exploitation and learn new behaviors that are difficult to specify with standard reward functions.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/45f573f302dc7e77cbc5d1a74ccbac3564bbebc8.pdf",
        "venue": "International Conference on Machine Learning",
        "citationCount": 316,
        "score": 79.0,
        "summary": "Conveying complex objectives to reinforcement learning (RL) agents can often be difficult, involving meticulous design of reward functions that are sufficiently informative yet easy enough to provide. Human-in-the-loop RL methods allow practitioners to instead interactively teach agents through tailored feedback; however, such approaches have been challenging to scale since human feedback is very expensive. In this work, we aim to make this process more sample- and feedback-efficient. We present an off-policy, interactive RL algorithm that capitalizes on the strengths of both feedback and off-policy learning. Specifically, we learn a reward model by actively querying a teacher's preferences between two clips of behavior and use it to train an agent. To enable off-policy learning, we relabel all the agent's past experience when its reward model changes. We additionally show that pre-training our agents with unsupervised exploration substantially increases the mileage of its queries. We demonstrate that our approach is capable of learning tasks of higher complexity than previously considered by human-in-the-loop methods, including a variety of locomotion and robotic manipulation skills. We also show that our method is able to utilize real-time human feedback to effectively prevent reward exploitation and learn new behaviors that are difficult to specify with standard reward functions.",
        "keywords": []
      },
      "file_name": "45f573f302dc7e77cbc5d1a74ccbac3564bbebc8.pdf"
    },
    {
      "success": true,
      "doc_id": "a9202c57fa5f4373e67696e52e007ac2",
      "summary": "Autonomous exploration is an important application of multi-vehicle systems, where a team of networked robots are coordinated to explore an unknown environment collaboratively. This technique has earned significant research interest due to its usefulness in search and rescue, fault detection and monitoring, localization and mapping, etc. In this paper, a novel cooperative exploration strategy is proposed for multiple mobile robots, which reduces the overall task completion time and energy costs compared to conventional methods. To efficiently navigate the networked robots during the collaborative tasks, a hierarchical control architecture is designed which contains a high-level decision making layer and a low-level target tracking layer. The proposed cooperative exploration approach is developed using dynamic Voronoi partitions, which minimizes duplicated exploration areas by assigning different target locations to individual robots. To deal with sudden obstacles in the unknown environment, an integrated deep reinforcement learning based collision avoidance algorithm is then proposed, which enables the control policy to learn from human demonstration data and thus improve the learning speed and performance. Finally, simulation and experimental results are provided to demonstrate the effectiveness of the proposed scheme.",
      "intriguing_abstract": "Autonomous exploration is an important application of multi-vehicle systems, where a team of networked robots are coordinated to explore an unknown environment collaboratively. This technique has earned significant research interest due to its usefulness in search and rescue, fault detection and monitoring, localization and mapping, etc. In this paper, a novel cooperative exploration strategy is proposed for multiple mobile robots, which reduces the overall task completion time and energy costs compared to conventional methods. To efficiently navigate the networked robots during the collaborative tasks, a hierarchical control architecture is designed which contains a high-level decision making layer and a low-level target tracking layer. The proposed cooperative exploration approach is developed using dynamic Voronoi partitions, which minimizes duplicated exploration areas by assigning different target locations to individual robots. To deal with sudden obstacles in the unknown environment, an integrated deep reinforcement learning based collision avoidance algorithm is then proposed, which enables the control policy to learn from human demonstration data and thus improve the learning speed and performance. Finally, simulation and experimental results are provided to demonstrate the effectiveness of the proposed scheme.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/f8d8e192979ab5d8d593e76a0c4e2c7581778732.pdf",
      "citation_key": "hu2020qwm",
      "metadata": {
        "title": "Voronoi-Based Multi-Robot Autonomous Exploration in Unknown Environments via Deep Reinforcement Learning",
        "authors": [
          "Junyan Hu",
          "Hanlin Niu",
          "J. Carrasco",
          "B. Lennox",
          "F. Arvin"
        ],
        "published_date": "2020",
        "abstract": "Autonomous exploration is an important application of multi-vehicle systems, where a team of networked robots are coordinated to explore an unknown environment collaboratively. This technique has earned significant research interest due to its usefulness in search and rescue, fault detection and monitoring, localization and mapping, etc. In this paper, a novel cooperative exploration strategy is proposed for multiple mobile robots, which reduces the overall task completion time and energy costs compared to conventional methods. To efficiently navigate the networked robots during the collaborative tasks, a hierarchical control architecture is designed which contains a high-level decision making layer and a low-level target tracking layer. The proposed cooperative exploration approach is developed using dynamic Voronoi partitions, which minimizes duplicated exploration areas by assigning different target locations to individual robots. To deal with sudden obstacles in the unknown environment, an integrated deep reinforcement learning based collision avoidance algorithm is then proposed, which enables the control policy to learn from human demonstration data and thus improve the learning speed and performance. Finally, simulation and experimental results are provided to demonstrate the effectiveness of the proposed scheme.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/f8d8e192979ab5d8d593e76a0c4e2c7581778732.pdf",
        "venue": "IEEE Transactions on Vehicular Technology",
        "citationCount": 291,
        "score": 58.2,
        "summary": "Autonomous exploration is an important application of multi-vehicle systems, where a team of networked robots are coordinated to explore an unknown environment collaboratively. This technique has earned significant research interest due to its usefulness in search and rescue, fault detection and monitoring, localization and mapping, etc. In this paper, a novel cooperative exploration strategy is proposed for multiple mobile robots, which reduces the overall task completion time and energy costs compared to conventional methods. To efficiently navigate the networked robots during the collaborative tasks, a hierarchical control architecture is designed which contains a high-level decision making layer and a low-level target tracking layer. The proposed cooperative exploration approach is developed using dynamic Voronoi partitions, which minimizes duplicated exploration areas by assigning different target locations to individual robots. To deal with sudden obstacles in the unknown environment, an integrated deep reinforcement learning based collision avoidance algorithm is then proposed, which enables the control policy to learn from human demonstration data and thus improve the learning speed and performance. Finally, simulation and experimental results are provided to demonstrate the effectiveness of the proposed scheme.",
        "keywords": []
      },
      "file_name": "f8d8e192979ab5d8d593e76a0c4e2c7581778732.pdf"
    },
    {
      "success": true,
      "doc_id": "65e417399322c15f895db87fec8a2227",
      "summary": "Achieving efficient and scalable exploration in complex domains poses a major challenge in reinforcement learning. While Bayesian and PAC-MDP approaches to the exploration problem offer strong formal guarantees, they are often impractical in higher dimensions due to their reliance on enumerating the state-action space. Hence, exploration in complex domains is often performed with simple epsilon-greedy methods. In this paper, we consider the challenging Atari games domain, which requires processing raw pixel inputs and delayed rewards. We evaluate several more sophisticated exploration strategies, including Thompson sampling and Boltzman exploration, and propose a new exploration method based on assigning exploration bonuses from a concurrently learned model of the system dynamics. By parameterizing our learned model with a neural network, we are able to develop a scalable and efficient approach to exploration bonuses that can be applied to tasks with complex, high-dimensional state spaces. In the Atari domain, our method provides the most consistent improvement across a range of games that pose a major challenge for prior methods. In addition to raw game-scores, we also develop an AUC-100 metric for the Atari Learning domain to evaluate the impact of exploration on this benchmark.",
      "intriguing_abstract": "Achieving efficient and scalable exploration in complex domains poses a major challenge in reinforcement learning. While Bayesian and PAC-MDP approaches to the exploration problem offer strong formal guarantees, they are often impractical in higher dimensions due to their reliance on enumerating the state-action space. Hence, exploration in complex domains is often performed with simple epsilon-greedy methods. In this paper, we consider the challenging Atari games domain, which requires processing raw pixel inputs and delayed rewards. We evaluate several more sophisticated exploration strategies, including Thompson sampling and Boltzman exploration, and propose a new exploration method based on assigning exploration bonuses from a concurrently learned model of the system dynamics. By parameterizing our learned model with a neural network, we are able to develop a scalable and efficient approach to exploration bonuses that can be applied to tasks with complex, high-dimensional state spaces. In the Atari domain, our method provides the most consistent improvement across a range of games that pose a major challenge for prior methods. In addition to raw game-scores, we also develop an AUC-100 metric for the Atari Learning domain to evaluate the impact of exploration on this benchmark.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/2470fcf0f89082de874ac9133ccb3a8667dd89a8.pdf",
      "citation_key": "stadie20158af",
      "metadata": {
        "title": "Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models",
        "authors": [
          "Bradly C. Stadie",
          "S. Levine",
          "P. Abbeel"
        ],
        "published_date": "2015",
        "abstract": "Achieving efficient and scalable exploration in complex domains poses a major challenge in reinforcement learning. While Bayesian and PAC-MDP approaches to the exploration problem offer strong formal guarantees, they are often impractical in higher dimensions due to their reliance on enumerating the state-action space. Hence, exploration in complex domains is often performed with simple epsilon-greedy methods. In this paper, we consider the challenging Atari games domain, which requires processing raw pixel inputs and delayed rewards. We evaluate several more sophisticated exploration strategies, including Thompson sampling and Boltzman exploration, and propose a new exploration method based on assigning exploration bonuses from a concurrently learned model of the system dynamics. By parameterizing our learned model with a neural network, we are able to develop a scalable and efficient approach to exploration bonuses that can be applied to tasks with complex, high-dimensional state spaces. In the Atari domain, our method provides the most consistent improvement across a range of games that pose a major challenge for prior methods. In addition to raw game-scores, we also develop an AUC-100 metric for the Atari Learning domain to evaluate the impact of exploration on this benchmark.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/2470fcf0f89082de874ac9133ccb3a8667dd89a8.pdf",
        "venue": "arXiv.org",
        "citationCount": 512,
        "score": 51.2,
        "summary": "Achieving efficient and scalable exploration in complex domains poses a major challenge in reinforcement learning. While Bayesian and PAC-MDP approaches to the exploration problem offer strong formal guarantees, they are often impractical in higher dimensions due to their reliance on enumerating the state-action space. Hence, exploration in complex domains is often performed with simple epsilon-greedy methods. In this paper, we consider the challenging Atari games domain, which requires processing raw pixel inputs and delayed rewards. We evaluate several more sophisticated exploration strategies, including Thompson sampling and Boltzman exploration, and propose a new exploration method based on assigning exploration bonuses from a concurrently learned model of the system dynamics. By parameterizing our learned model with a neural network, we are able to develop a scalable and efficient approach to exploration bonuses that can be applied to tasks with complex, high-dimensional state spaces. In the Atari domain, our method provides the most consistent improvement across a range of games that pose a major challenge for prior methods. In addition to raw game-scores, we also develop an AUC-100 metric for the Atari Learning domain to evaluate the impact of exploration on this benchmark.",
        "keywords": []
      },
      "file_name": "2470fcf0f89082de874ac9133ccb3a8667dd89a8.pdf"
    },
    {
      "success": true,
      "doc_id": "b58ecf62d547f3d4ab2da7a7fa3cb81d",
      "summary": "Exploration is a fundamental challenge in reinforcement learning (RL). Many of the current exploration methods for deep RL use task-agnostic objectives, such as information gain or bonuses based on state visitation. However, many practical applications of RL involve learning more than a single task, and prior tasks can be used to inform how exploration should be performed in new tasks. In this work, we explore how prior tasks can inform an agent about how to explore effectively in new situations. We introduce a novel gradient-based fast adaptation algorithm -- model agnostic exploration with structured noise (MAESN) -- to learn exploration strategies from prior experience. The prior experience is used both to initialize a policy and to acquire a latent exploration space that can inject structured stochasticity into a policy, producing exploration strategies that are informed by prior knowledge and are more effective than random action-space noise. We show that MAESN is more effective at learning exploration strategies when compared to prior meta-RL methods, RL without learned exploration strategies, and task-agnostic exploration methods. We evaluate our method on a variety of simulated tasks: locomotion with a wheeled robot, locomotion with a quadrupedal walker, and object manipulation.",
      "intriguing_abstract": "Exploration is a fundamental challenge in reinforcement learning (RL). Many of the current exploration methods for deep RL use task-agnostic objectives, such as information gain or bonuses based on state visitation. However, many practical applications of RL involve learning more than a single task, and prior tasks can be used to inform how exploration should be performed in new tasks. In this work, we explore how prior tasks can inform an agent about how to explore effectively in new situations. We introduce a novel gradient-based fast adaptation algorithm -- model agnostic exploration with structured noise (MAESN) -- to learn exploration strategies from prior experience. The prior experience is used both to initialize a policy and to acquire a latent exploration space that can inject structured stochasticity into a policy, producing exploration strategies that are informed by prior knowledge and are more effective than random action-space noise. We show that MAESN is more effective at learning exploration strategies when compared to prior meta-RL methods, RL without learned exploration strategies, and task-agnostic exploration methods. We evaluate our method on a variety of simulated tasks: locomotion with a wheeled robot, locomotion with a quadrupedal walker, and object manipulation.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/68c108795deef06fa929d1f6e96b75dbf7ce8531.pdf",
      "citation_key": "gupta2018rge",
      "metadata": {
        "title": "Meta-Reinforcement Learning of Structured Exploration Strategies",
        "authors": [
          "Abhishek Gupta",
          "Russell Mendonca",
          "Yuxuan Liu",
          "P. Abbeel",
          "S. Levine"
        ],
        "published_date": "2018",
        "abstract": "Exploration is a fundamental challenge in reinforcement learning (RL). Many of the current exploration methods for deep RL use task-agnostic objectives, such as information gain or bonuses based on state visitation. However, many practical applications of RL involve learning more than a single task, and prior tasks can be used to inform how exploration should be performed in new tasks. In this work, we explore how prior tasks can inform an agent about how to explore effectively in new situations. We introduce a novel gradient-based fast adaptation algorithm -- model agnostic exploration with structured noise (MAESN) -- to learn exploration strategies from prior experience. The prior experience is used both to initialize a policy and to acquire a latent exploration space that can inject structured stochasticity into a policy, producing exploration strategies that are informed by prior knowledge and are more effective than random action-space noise. We show that MAESN is more effective at learning exploration strategies when compared to prior meta-RL methods, RL without learned exploration strategies, and task-agnostic exploration methods. We evaluate our method on a variety of simulated tasks: locomotion with a wheeled robot, locomotion with a quadrupedal walker, and object manipulation.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/68c108795deef06fa929d1f6e96b75dbf7ce8531.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 357,
        "score": 51.0,
        "summary": "Exploration is a fundamental challenge in reinforcement learning (RL). Many of the current exploration methods for deep RL use task-agnostic objectives, such as information gain or bonuses based on state visitation. However, many practical applications of RL involve learning more than a single task, and prior tasks can be used to inform how exploration should be performed in new tasks. In this work, we explore how prior tasks can inform an agent about how to explore effectively in new situations. We introduce a novel gradient-based fast adaptation algorithm -- model agnostic exploration with structured noise (MAESN) -- to learn exploration strategies from prior experience. The prior experience is used both to initialize a policy and to acquire a latent exploration space that can inject structured stochasticity into a policy, producing exploration strategies that are informed by prior knowledge and are more effective than random action-space noise. We show that MAESN is more effective at learning exploration strategies when compared to prior meta-RL methods, RL without learned exploration strategies, and task-agnostic exploration methods. We evaluate our method on a variety of simulated tasks: locomotion with a wheeled robot, locomotion with a quadrupedal walker, and object manipulation.",
        "keywords": []
      },
      "file_name": "68c108795deef06fa929d1f6e96b75dbf7ce8531.pdf"
    },
    {
      "success": true,
      "doc_id": "03315d19f2c9b7c27444625721a128c2",
      "summary": "Safety remains a central obstacle preventing widespread use of RL in the real world: learning new tasks in uncertain environments requires extensive exploration, but safety requires limiting exploration. We propose Recovery RL, an algorithm which navigates this tradeoff by (1) leveraging offline data to learn about constraint violating zones before policy learning and (2) separating the goals of improving task performance and constraint satisfaction across two policies: a task policy that only optimizes the task reward and a recovery policy that guides the agent to safety when constraint violation is likely. We evaluate Recovery RL on 6 simulation domains, including two contact-rich manipulation tasks and an image-based navigation task, and an image-based obstacle avoidance task on a physical robot. We compare Recovery RL to 5 prior safe RL methods which jointly optimize for task performance and safety via constrained optimization or reward shaping and find that Recovery RL outperforms the next best prior method across all domains. Results suggest that Recovery RL trades off constraint violations and task successes 220 times more efficiently in simulation domains and 3 times more efficiently in physical experiments. See https://tinyurl.com/rl-recovery for videos and supplementary material.",
      "intriguing_abstract": "Safety remains a central obstacle preventing widespread use of RL in the real world: learning new tasks in uncertain environments requires extensive exploration, but safety requires limiting exploration. We propose Recovery RL, an algorithm which navigates this tradeoff by (1) leveraging offline data to learn about constraint violating zones before policy learning and (2) separating the goals of improving task performance and constraint satisfaction across two policies: a task policy that only optimizes the task reward and a recovery policy that guides the agent to safety when constraint violation is likely. We evaluate Recovery RL on 6 simulation domains, including two contact-rich manipulation tasks and an image-based navigation task, and an image-based obstacle avoidance task on a physical robot. We compare Recovery RL to 5 prior safe RL methods which jointly optimize for task performance and safety via constrained optimization or reward shaping and find that Recovery RL outperforms the next best prior method across all domains. Results suggest that Recovery RL trades off constraint violations and task successes 220 times more efficiently in simulation domains and 3 times more efficiently in physical experiments. See https://tinyurl.com/rl-recovery for videos and supplementary material.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/431dc05ac25510de6264084434254cca877f9ab3.pdf",
      "citation_key": "thananjeyan2020d20",
      "metadata": {
        "title": "Recovery RL: Safe Reinforcement Learning With Learned Recovery Zones",
        "authors": [
          "Brijen Thananjeyan",
          "A. Balakrishna",
          "Suraj Nair",
          "Michael Luo",
          "K. Srinivasan",
          "M. Hwang",
          "Joseph E. Gonzalez",
          "Julian Ibarz",
          "Chelsea Finn",
          "Ken Goldberg"
        ],
        "published_date": "2020",
        "abstract": "Safety remains a central obstacle preventing widespread use of RL in the real world: learning new tasks in uncertain environments requires extensive exploration, but safety requires limiting exploration. We propose Recovery RL, an algorithm which navigates this tradeoff by (1) leveraging offline data to learn about constraint violating zones before policy learning and (2) separating the goals of improving task performance and constraint satisfaction across two policies: a task policy that only optimizes the task reward and a recovery policy that guides the agent to safety when constraint violation is likely. We evaluate Recovery RL on 6 simulation domains, including two contact-rich manipulation tasks and an image-based navigation task, and an image-based obstacle avoidance task on a physical robot. We compare Recovery RL to 5 prior safe RL methods which jointly optimize for task performance and safety via constrained optimization or reward shaping and find that Recovery RL outperforms the next best prior method across all domains. Results suggest that Recovery RL trades off constraint violations and task successes 220 times more efficiently in simulation domains and 3 times more efficiently in physical experiments. See https://tinyurl.com/rl-recovery for videos and supplementary material.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/431dc05ac25510de6264084434254cca877f9ab3.pdf",
        "venue": "IEEE Robotics and Automation Letters",
        "citationCount": 247,
        "score": 49.400000000000006,
        "summary": "Safety remains a central obstacle preventing widespread use of RL in the real world: learning new tasks in uncertain environments requires extensive exploration, but safety requires limiting exploration. We propose Recovery RL, an algorithm which navigates this tradeoff by (1) leveraging offline data to learn about constraint violating zones before policy learning and (2) separating the goals of improving task performance and constraint satisfaction across two policies: a task policy that only optimizes the task reward and a recovery policy that guides the agent to safety when constraint violation is likely. We evaluate Recovery RL on 6 simulation domains, including two contact-rich manipulation tasks and an image-based navigation task, and an image-based obstacle avoidance task on a physical robot. We compare Recovery RL to 5 prior safe RL methods which jointly optimize for task performance and safety via constrained optimization or reward shaping and find that Recovery RL outperforms the next best prior method across all domains. Results suggest that Recovery RL trades off constraint violations and task successes 220 times more efficiently in simulation domains and 3 times more efficiently in physical experiments. See https://tinyurl.com/rl-recovery for videos and supplementary material.",
        "keywords": []
      },
      "file_name": "431dc05ac25510de6264084434254cca877f9ab3.pdf"
    },
    {
      "success": true,
      "doc_id": "d06cbc252a04d9e57fa73dce43e20f25",
      "summary": "Offline Reinforcement Learning promises to learn effective policies from previously-collected, static datasets without the need for exploration. However, existing Q-learning and actor-critic based off-policy RL algorithms fail when bootstrapping from out-of-distribution (OOD) actions or states. We hypothesize that a key missing ingredient from the existing methods is a proper treatment of uncertainty in the offline setting. We propose Uncertainty Weighted Actor-Critic (UWAC), an algorithm that detects OOD state-action pairs and down-weights their contribution in the training objectives accordingly. Implementation-wise, we adopt a practical and effective dropout-based uncertainty estimation method that introduces very little overhead over existing RL algorithms. Empirically, we observe that UWAC substantially improves model stability during training. In addition, UWAC out-performs existing offline RL methods on a variety of competitive tasks, and achieves significant performance gains over the state-of-the-art baseline on datasets with sparse demonstrations collected from human experts.",
      "intriguing_abstract": "Offline Reinforcement Learning promises to learn effective policies from previously-collected, static datasets without the need for exploration. However, existing Q-learning and actor-critic based off-policy RL algorithms fail when bootstrapping from out-of-distribution (OOD) actions or states. We hypothesize that a key missing ingredient from the existing methods is a proper treatment of uncertainty in the offline setting. We propose Uncertainty Weighted Actor-Critic (UWAC), an algorithm that detects OOD state-action pairs and down-weights their contribution in the training objectives accordingly. Implementation-wise, we adopt a practical and effective dropout-based uncertainty estimation method that introduces very little overhead over existing RL algorithms. Empirically, we observe that UWAC substantially improves model stability during training. In addition, UWAC out-performs existing offline RL methods on a variety of competitive tasks, and achieves significant performance gains over the state-of-the-art baseline on datasets with sparse demonstrations collected from human experts.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/2c23085488337c4c1b5673b8d0f4ac95bda73529.pdf",
      "citation_key": "wu2021r67",
      "metadata": {
        "title": "Uncertainty Weighted Actor-Critic for Offline Reinforcement Learning",
        "authors": [
          "Yue Wu",
          "Shuangfei Zhai",
          "Nitish Srivastava",
          "J. Susskind",
          "Jian Zhang",
          "R. Salakhutdinov",
          "Hanlin Goh"
        ],
        "published_date": "2021",
        "abstract": "Offline Reinforcement Learning promises to learn effective policies from previously-collected, static datasets without the need for exploration. However, existing Q-learning and actor-critic based off-policy RL algorithms fail when bootstrapping from out-of-distribution (OOD) actions or states. We hypothesize that a key missing ingredient from the existing methods is a proper treatment of uncertainty in the offline setting. We propose Uncertainty Weighted Actor-Critic (UWAC), an algorithm that detects OOD state-action pairs and down-weights their contribution in the training objectives accordingly. Implementation-wise, we adopt a practical and effective dropout-based uncertainty estimation method that introduces very little overhead over existing RL algorithms. Empirically, we observe that UWAC substantially improves model stability during training. In addition, UWAC out-performs existing offline RL methods on a variety of competitive tasks, and achieves significant performance gains over the state-of-the-art baseline on datasets with sparse demonstrations collected from human experts.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/2c23085488337c4c1b5673b8d0f4ac95bda73529.pdf",
        "venue": "International Conference on Machine Learning",
        "citationCount": 196,
        "score": 49.0,
        "summary": "Offline Reinforcement Learning promises to learn effective policies from previously-collected, static datasets without the need for exploration. However, existing Q-learning and actor-critic based off-policy RL algorithms fail when bootstrapping from out-of-distribution (OOD) actions or states. We hypothesize that a key missing ingredient from the existing methods is a proper treatment of uncertainty in the offline setting. We propose Uncertainty Weighted Actor-Critic (UWAC), an algorithm that detects OOD state-action pairs and down-weights their contribution in the training objectives accordingly. Implementation-wise, we adopt a practical and effective dropout-based uncertainty estimation method that introduces very little overhead over existing RL algorithms. Empirically, we observe that UWAC substantially improves model stability during training. In addition, UWAC out-performs existing offline RL methods on a variety of competitive tasks, and achieves significant performance gains over the state-of-the-art baseline on datasets with sparse demonstrations collected from human experts.",
        "keywords": []
      },
      "file_name": "2c23085488337c4c1b5673b8d0f4ac95bda73529.pdf"
    },
    {
      "success": true,
      "doc_id": "b2dc6fbab492f228f9f82f517dcc0a91",
      "summary": "Evolution strategies (ES) are a family of black-box optimization algorithms able to train deep neural networks roughly as well as Q-learning and policy gradient methods on challenging deep reinforcement learning (RL) problems, but are much faster (e.g. hours vs. days) because they parallelize better. However, many RL problems require directed exploration because they have reward functions that are sparse or deceptive (i.e. contain local optima), and it is unknown how to encourage such exploration with ES. Here we show that algorithms that have been invented to promote directed exploration in small-scale evolved neural networks via populations of exploring agents, specifically novelty search (NS) and quality diversity (QD) algorithms, can be hybridized with ES to improve its performance on sparse or deceptive deep RL tasks, while retaining scalability. Our experiments confirm that the resultant new algorithms, NS-ES and two QD algorithms, NSR-ES and NSRA-ES, avoid local optima encountered by ES to achieve higher performance on Atari and simulated robots learning to walk around a deceptive trap. This paper thus introduces a family of fast, scalable algorithms for reinforcement learning that are capable of directed exploration. It also adds this new family of exploration algorithms to the RL toolbox and raises the interesting possibility that analogous algorithms with multiple simultaneous paths of exploration might also combine well with existing RL algorithms outside ES.",
      "intriguing_abstract": "Evolution strategies (ES) are a family of black-box optimization algorithms able to train deep neural networks roughly as well as Q-learning and policy gradient methods on challenging deep reinforcement learning (RL) problems, but are much faster (e.g. hours vs. days) because they parallelize better. However, many RL problems require directed exploration because they have reward functions that are sparse or deceptive (i.e. contain local optima), and it is unknown how to encourage such exploration with ES. Here we show that algorithms that have been invented to promote directed exploration in small-scale evolved neural networks via populations of exploring agents, specifically novelty search (NS) and quality diversity (QD) algorithms, can be hybridized with ES to improve its performance on sparse or deceptive deep RL tasks, while retaining scalability. Our experiments confirm that the resultant new algorithms, NS-ES and two QD algorithms, NSR-ES and NSRA-ES, avoid local optima encountered by ES to achieve higher performance on Atari and simulated robots learning to walk around a deceptive trap. This paper thus introduces a family of fast, scalable algorithms for reinforcement learning that are capable of directed exploration. It also adds this new family of exploration algorithms to the RL toolbox and raises the interesting possibility that analogous algorithms with multiple simultaneous paths of exploration might also combine well with existing RL algorithms outside ES.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/2064020586d5832b55f80a7dffea1fd90a5d94dd.pdf",
      "citation_key": "conti2017cr2",
      "metadata": {
        "title": "Improving Exploration in Evolution Strategies for Deep Reinforcement Learning via a Population of Novelty-Seeking Agents",
        "authors": [
          "Edoardo Conti",
          "Vashisht Madhavan",
          "F. Such",
          "J. Lehman",
          "Kenneth O. Stanley",
          "J. Clune"
        ],
        "published_date": "2017",
        "abstract": "Evolution strategies (ES) are a family of black-box optimization algorithms able to train deep neural networks roughly as well as Q-learning and policy gradient methods on challenging deep reinforcement learning (RL) problems, but are much faster (e.g. hours vs. days) because they parallelize better. However, many RL problems require directed exploration because they have reward functions that are sparse or deceptive (i.e. contain local optima), and it is unknown how to encourage such exploration with ES. Here we show that algorithms that have been invented to promote directed exploration in small-scale evolved neural networks via populations of exploring agents, specifically novelty search (NS) and quality diversity (QD) algorithms, can be hybridized with ES to improve its performance on sparse or deceptive deep RL tasks, while retaining scalability. Our experiments confirm that the resultant new algorithms, NS-ES and two QD algorithms, NSR-ES and NSRA-ES, avoid local optima encountered by ES to achieve higher performance on Atari and simulated robots learning to walk around a deceptive trap. This paper thus introduces a family of fast, scalable algorithms for reinforcement learning that are capable of directed exploration. It also adds this new family of exploration algorithms to the RL toolbox and raises the interesting possibility that analogous algorithms with multiple simultaneous paths of exploration might also combine well with existing RL algorithms outside ES.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/2064020586d5832b55f80a7dffea1fd90a5d94dd.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 351,
        "score": 43.875,
        "summary": "Evolution strategies (ES) are a family of black-box optimization algorithms able to train deep neural networks roughly as well as Q-learning and policy gradient methods on challenging deep reinforcement learning (RL) problems, but are much faster (e.g. hours vs. days) because they parallelize better. However, many RL problems require directed exploration because they have reward functions that are sparse or deceptive (i.e. contain local optima), and it is unknown how to encourage such exploration with ES. Here we show that algorithms that have been invented to promote directed exploration in small-scale evolved neural networks via populations of exploring agents, specifically novelty search (NS) and quality diversity (QD) algorithms, can be hybridized with ES to improve its performance on sparse or deceptive deep RL tasks, while retaining scalability. Our experiments confirm that the resultant new algorithms, NS-ES and two QD algorithms, NSR-ES and NSRA-ES, avoid local optima encountered by ES to achieve higher performance on Atari and simulated robots learning to walk around a deceptive trap. This paper thus introduces a family of fast, scalable algorithms for reinforcement learning that are capable of directed exploration. It also adds this new family of exploration algorithms to the RL toolbox and raises the interesting possibility that analogous algorithms with multiple simultaneous paths of exploration might also combine well with existing RL algorithms outside ES.",
        "keywords": []
      },
      "file_name": "2064020586d5832b55f80a7dffea1fd90a5d94dd.pdf"
    },
    {
      "success": true,
      "doc_id": "bd897806ed13d3d8199ce0ca8b81e67c",
      "summary": "Recent unsupervised pre-training methods have shown to be effective on language and vision domains by learning useful representations for multiple downstream tasks. In this paper, we investigate if such unsupervised pre-training methods can also be effective for vision-based reinforcement learning (RL). To this end, we introduce a framework that learns representations useful for understanding the dynamics via generative pre-training on videos. Our framework consists of two phases: we pre-train an action-free latent video prediction model, and then utilize the pre-trained representations for efficiently learning action-conditional world models on unseen environments. To incorporate additional action inputs during fine-tuning, we introduce a new architecture that stacks an action-conditional latent prediction model on top of the pre-trained action-free prediction model. Moreover, for better exploration, we propose a video-based intrinsic bonus that leverages pre-trained representations. We demonstrate that our framework significantly improves both final performances and sample-efficiency of vision-based RL in a variety of manipulation and locomotion tasks. Code is available at https://github.com/younggyoseo/apv.",
      "intriguing_abstract": "Recent unsupervised pre-training methods have shown to be effective on language and vision domains by learning useful representations for multiple downstream tasks. In this paper, we investigate if such unsupervised pre-training methods can also be effective for vision-based reinforcement learning (RL). To this end, we introduce a framework that learns representations useful for understanding the dynamics via generative pre-training on videos. Our framework consists of two phases: we pre-train an action-free latent video prediction model, and then utilize the pre-trained representations for efficiently learning action-conditional world models on unseen environments. To incorporate additional action inputs during fine-tuning, we introduce a new architecture that stacks an action-conditional latent prediction model on top of the pre-trained action-free prediction model. Moreover, for better exploration, we propose a video-based intrinsic bonus that leverages pre-trained representations. We demonstrate that our framework significantly improves both final performances and sample-efficiency of vision-based RL in a variety of manipulation and locomotion tasks. Code is available at https://github.com/younggyoseo/apv.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/1c35807e1a4c24e2013fa0a090cee9cc4716a5f5.pdf",
      "citation_key": "seo2022cjf",
      "metadata": {
        "title": "Reinforcement Learning with Action-Free Pre-Training from Videos",
        "authors": [
          "Younggyo Seo",
          "Kimin Lee",
          "Stephen James",
          "P. Abbeel"
        ],
        "published_date": "2022",
        "abstract": "Recent unsupervised pre-training methods have shown to be effective on language and vision domains by learning useful representations for multiple downstream tasks. In this paper, we investigate if such unsupervised pre-training methods can also be effective for vision-based reinforcement learning (RL). To this end, we introduce a framework that learns representations useful for understanding the dynamics via generative pre-training on videos. Our framework consists of two phases: we pre-train an action-free latent video prediction model, and then utilize the pre-trained representations for efficiently learning action-conditional world models on unseen environments. To incorporate additional action inputs during fine-tuning, we introduce a new architecture that stacks an action-conditional latent prediction model on top of the pre-trained action-free prediction model. Moreover, for better exploration, we propose a video-based intrinsic bonus that leverages pre-trained representations. We demonstrate that our framework significantly improves both final performances and sample-efficiency of vision-based RL in a variety of manipulation and locomotion tasks. Code is available at https://github.com/younggyoseo/apv.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/1c35807e1a4c24e2013fa0a090cee9cc4716a5f5.pdf",
        "venue": "International Conference on Machine Learning",
        "citationCount": 129,
        "score": 43.0,
        "summary": "Recent unsupervised pre-training methods have shown to be effective on language and vision domains by learning useful representations for multiple downstream tasks. In this paper, we investigate if such unsupervised pre-training methods can also be effective for vision-based reinforcement learning (RL). To this end, we introduce a framework that learns representations useful for understanding the dynamics via generative pre-training on videos. Our framework consists of two phases: we pre-train an action-free latent video prediction model, and then utilize the pre-trained representations for efficiently learning action-conditional world models on unseen environments. To incorporate additional action inputs during fine-tuning, we introduce a new architecture that stacks an action-conditional latent prediction model on top of the pre-trained action-free prediction model. Moreover, for better exploration, we propose a video-based intrinsic bonus that leverages pre-trained representations. We demonstrate that our framework significantly improves both final performances and sample-efficiency of vision-based RL in a variety of manipulation and locomotion tasks. Code is available at https://github.com/younggyoseo/apv.",
        "keywords": []
      },
      "file_name": "1c35807e1a4c24e2013fa0a090cee9cc4716a5f5.pdf"
    },
    {
      "success": true,
      "doc_id": "34f4608643f7bfe26223b79b01439670",
      "summary": "Reinforcement learning (RL) provides a theoretical framework for continuously improving an agent's behavior via trial and error. However, efficiently learning policies from scratch can be very difficult, particularly for tasks with exploration challenges. In such settings, it might be desirable to initialize RL with an existing policy, offline data, or demonstrations. However, naively performing such initialization in RL often works poorly, especially for value-based methods. In this paper, we present a meta algorithm that can use offline data, demonstrations, or a pre-existing policy to initialize an RL policy, and is compatible with any RL approach. In particular, we propose Jump-Start Reinforcement Learning (JSRL), an algorithm that employs two policies to solve tasks: a guide-policy, and an exploration-policy. By using the guide-policy to form a curriculum of starting states for the exploration-policy, we are able to efficiently improve performance on a set of simulated robotic tasks. We show via experiments that JSRL is able to significantly outperform existing imitation and reinforcement learning algorithms, particularly in the small-data regime. In addition, we provide an upper bound on the sample complexity of JSRL and show that with the help of a guide-policy, one can improve the sample complexity for non-optimism exploration methods from exponential in horizon to polynomial.",
      "intriguing_abstract": "Reinforcement learning (RL) provides a theoretical framework for continuously improving an agent's behavior via trial and error. However, efficiently learning policies from scratch can be very difficult, particularly for tasks with exploration challenges. In such settings, it might be desirable to initialize RL with an existing policy, offline data, or demonstrations. However, naively performing such initialization in RL often works poorly, especially for value-based methods. In this paper, we present a meta algorithm that can use offline data, demonstrations, or a pre-existing policy to initialize an RL policy, and is compatible with any RL approach. In particular, we propose Jump-Start Reinforcement Learning (JSRL), an algorithm that employs two policies to solve tasks: a guide-policy, and an exploration-policy. By using the guide-policy to form a curriculum of starting states for the exploration-policy, we are able to efficiently improve performance on a set of simulated robotic tasks. We show via experiments that JSRL is able to significantly outperform existing imitation and reinforcement learning algorithms, particularly in the small-data regime. In addition, we provide an upper bound on the sample complexity of JSRL and show that with the help of a guide-policy, one can improve the sample complexity for non-optimism exploration methods from exponential in horizon to polynomial.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/f3a63a840185fa8c5e0db4bbe12afa7d3de7d029.pdf",
      "citation_key": "uchendu20221h1",
      "metadata": {
        "title": "Jump-Start Reinforcement Learning",
        "authors": [
          "Ikechukwu Uchendu",
          "Ted Xiao",
          "Yao Lu",
          "Banghua Zhu",
          "Mengyuan Yan",
          "J. Simn",
          "Matthew Bennice",
          "Chuyuan Fu",
          "Cong Ma",
          "Jiantao Jiao",
          "S. Levine",
          "Karol Hausman"
        ],
        "published_date": "2022",
        "abstract": "Reinforcement learning (RL) provides a theoretical framework for continuously improving an agent's behavior via trial and error. However, efficiently learning policies from scratch can be very difficult, particularly for tasks with exploration challenges. In such settings, it might be desirable to initialize RL with an existing policy, offline data, or demonstrations. However, naively performing such initialization in RL often works poorly, especially for value-based methods. In this paper, we present a meta algorithm that can use offline data, demonstrations, or a pre-existing policy to initialize an RL policy, and is compatible with any RL approach. In particular, we propose Jump-Start Reinforcement Learning (JSRL), an algorithm that employs two policies to solve tasks: a guide-policy, and an exploration-policy. By using the guide-policy to form a curriculum of starting states for the exploration-policy, we are able to efficiently improve performance on a set of simulated robotic tasks. We show via experiments that JSRL is able to significantly outperform existing imitation and reinforcement learning algorithms, particularly in the small-data regime. In addition, we provide an upper bound on the sample complexity of JSRL and show that with the help of a guide-policy, one can improve the sample complexity for non-optimism exploration methods from exponential in horizon to polynomial.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/f3a63a840185fa8c5e0db4bbe12afa7d3de7d029.pdf",
        "venue": "International Conference on Machine Learning",
        "citationCount": 127,
        "score": 42.33333333333333,
        "summary": "Reinforcement learning (RL) provides a theoretical framework for continuously improving an agent's behavior via trial and error. However, efficiently learning policies from scratch can be very difficult, particularly for tasks with exploration challenges. In such settings, it might be desirable to initialize RL with an existing policy, offline data, or demonstrations. However, naively performing such initialization in RL often works poorly, especially for value-based methods. In this paper, we present a meta algorithm that can use offline data, demonstrations, or a pre-existing policy to initialize an RL policy, and is compatible with any RL approach. In particular, we propose Jump-Start Reinforcement Learning (JSRL), an algorithm that employs two policies to solve tasks: a guide-policy, and an exploration-policy. By using the guide-policy to form a curriculum of starting states for the exploration-policy, we are able to efficiently improve performance on a set of simulated robotic tasks. We show via experiments that JSRL is able to significantly outperform existing imitation and reinforcement learning algorithms, particularly in the small-data regime. In addition, we provide an upper bound on the sample complexity of JSRL and show that with the help of a guide-policy, one can improve the sample complexity for non-optimism exploration methods from exponential in horizon to polynomial.",
        "keywords": []
      },
      "file_name": "f3a63a840185fa8c5e0db4bbe12afa7d3de7d029.pdf"
    },
    {
      "success": true,
      "doc_id": "144dc3991e7a630f08774f096ad3d630",
      "summary": "This paper investigates the automatic exploration problem under the unknown environment, which is the key point of applying the robotic system to some social tasks. The solution to this problem via stacking decision rules is impossible to cover various environments and sensor properties. Learning-based control methods are adaptive for these scenarios. However, these methods are damaged by low learning efficiency and awkward transferability from simulation to reality. In this paper, we construct a general exploration framework via decomposing the exploration process into the decision, planning, and mapping modules, which increases the modularity of the robotic system. Based on this framework, we propose a deep reinforcement learning-based decision algorithm that uses a deep neural network to learning exploration strategy from the partial map. The results show that this proposed algorithm has better learning efficiency and adaptability for unknown environments. In addition, we conduct the experiments on the physical robot, and the results suggest that the learned policy can be well transferred from simulation to the real robot.",
      "intriguing_abstract": "This paper investigates the automatic exploration problem under the unknown environment, which is the key point of applying the robotic system to some social tasks. The solution to this problem via stacking decision rules is impossible to cover various environments and sensor properties. Learning-based control methods are adaptive for these scenarios. However, these methods are damaged by low learning efficiency and awkward transferability from simulation to reality. In this paper, we construct a general exploration framework via decomposing the exploration process into the decision, planning, and mapping modules, which increases the modularity of the robotic system. Based on this framework, we propose a deep reinforcement learning-based decision algorithm that uses a deep neural network to learning exploration strategy from the partial map. The results show that this proposed algorithm has better learning efficiency and adaptability for unknown environments. In addition, we conduct the experiments on the physical robot, and the results suggest that the learned policy can be well transferred from simulation to the real robot.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/52a6657cf1cb3cc847695a386bd65b5eea34bc13.pdf",
      "citation_key": "li2020r8r",
      "metadata": {
        "title": "Deep Reinforcement Learning-Based Automatic Exploration for Navigation in Unknown Environment",
        "authors": [
          "Haoran Li",
          "Qichao Zhang",
          "Dongbin Zhao"
        ],
        "published_date": "2020",
        "abstract": "This paper investigates the automatic exploration problem under the unknown environment, which is the key point of applying the robotic system to some social tasks. The solution to this problem via stacking decision rules is impossible to cover various environments and sensor properties. Learning-based control methods are adaptive for these scenarios. However, these methods are damaged by low learning efficiency and awkward transferability from simulation to reality. In this paper, we construct a general exploration framework via decomposing the exploration process into the decision, planning, and mapping modules, which increases the modularity of the robotic system. Based on this framework, we propose a deep reinforcement learning-based decision algorithm that uses a deep neural network to learning exploration strategy from the partial map. The results show that this proposed algorithm has better learning efficiency and adaptability for unknown environments. In addition, we conduct the experiments on the physical robot, and the results suggest that the learned policy can be well transferred from simulation to the real robot.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/52a6657cf1cb3cc847695a386bd65b5eea34bc13.pdf",
        "venue": "IEEE Transactions on Neural Networks and Learning Systems",
        "citationCount": 206,
        "score": 41.2,
        "summary": "This paper investigates the automatic exploration problem under the unknown environment, which is the key point of applying the robotic system to some social tasks. The solution to this problem via stacking decision rules is impossible to cover various environments and sensor properties. Learning-based control methods are adaptive for these scenarios. However, these methods are damaged by low learning efficiency and awkward transferability from simulation to reality. In this paper, we construct a general exploration framework via decomposing the exploration process into the decision, planning, and mapping modules, which increases the modularity of the robotic system. Based on this framework, we propose a deep reinforcement learning-based decision algorithm that uses a deep neural network to learning exploration strategy from the partial map. The results show that this proposed algorithm has better learning efficiency and adaptability for unknown environments. In addition, we conduct the experiments on the physical robot, and the results suggest that the learned policy can be well transferred from simulation to the real robot.",
        "keywords": []
      },
      "file_name": "52a6657cf1cb3cc847695a386bd65b5eea34bc13.pdf"
    },
    {
      "success": true,
      "doc_id": "f2e539668ffe2012c981302902d24c6b",
      "summary": "Deep reinforcement learning (DRL) and deep multiagent reinforcement learning (MARL) have achieved significant success across a wide range of domains, including game artificial intelligence (AI), autonomous vehicles, and robotics. However, DRL and deep MARL agents are widely known to be sample inefficient that millions of interactions are usually needed even for relatively simple problem settings, thus preventing the wide application and deployment in real-industry scenarios. One bottleneck challenge behind is the well-known exploration problem, i.e., how efficiently exploring the environment and collecting informative experiences that could benefit policy learning toward the optimal ones. This problem becomes more challenging in complex environments with sparse rewards, noisy distractions, long horizons, and nonstationary co-learners. In this article, we conduct a comprehensive survey on existing exploration methods for both single-agent RL and multiagent RL. We start the survey by identifying several key challenges to efficient exploration. Then, we provide a systematic survey of existing approaches by classifying them into two major categories: uncertainty-oriented exploration and intrinsic motivation-oriented exploration. Beyond the above two main branches, we also include other notable exploration methods with different ideas and techniques. In addition to algorithmic analysis, we provide a comprehensive and unified empirical comparison of different exploration methods for DRL on a set of commonly used benchmarks. According to our algorithmic and empirical investigation, we finally summarize the open problems of exploration in DRL and deep MARL and point out a few future directions.",
      "intriguing_abstract": "Deep reinforcement learning (DRL) and deep multiagent reinforcement learning (MARL) have achieved significant success across a wide range of domains, including game artificial intelligence (AI), autonomous vehicles, and robotics. However, DRL and deep MARL agents are widely known to be sample inefficient that millions of interactions are usually needed even for relatively simple problem settings, thus preventing the wide application and deployment in real-industry scenarios. One bottleneck challenge behind is the well-known exploration problem, i.e., how efficiently exploring the environment and collecting informative experiences that could benefit policy learning toward the optimal ones. This problem becomes more challenging in complex environments with sparse rewards, noisy distractions, long horizons, and nonstationary co-learners. In this article, we conduct a comprehensive survey on existing exploration methods for both single-agent RL and multiagent RL. We start the survey by identifying several key challenges to efficient exploration. Then, we provide a systematic survey of existing approaches by classifying them into two major categories: uncertainty-oriented exploration and intrinsic motivation-oriented exploration. Beyond the above two main branches, we also include other notable exploration methods with different ideas and techniques. In addition to algorithmic analysis, we provide a comprehensive and unified empirical comparison of different exploration methods for DRL on a set of commonly used benchmarks. According to our algorithmic and empirical investigation, we finally summarize the open problems of exploration in DRL and deep MARL and point out a few future directions.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/12075ea34f5fbe32ec5582786761ab34d401209b.pdf",
      "citation_key": "yang2021ngm",
      "metadata": {
        "title": "Exploration in Deep Reinforcement Learning: From Single-Agent to Multiagent Domain",
        "authors": [
          "Tianpei Yang",
          "Hongyao Tang",
          "Chenjia Bai",
          "Jinyi Liu",
          "Jianye Hao",
          "Zhaopeng Meng",
          "Peng Liu",
          "Zhen Wang"
        ],
        "published_date": "2021",
        "abstract": "Deep reinforcement learning (DRL) and deep multiagent reinforcement learning (MARL) have achieved significant success across a wide range of domains, including game artificial intelligence (AI), autonomous vehicles, and robotics. However, DRL and deep MARL agents are widely known to be sample inefficient that millions of interactions are usually needed even for relatively simple problem settings, thus preventing the wide application and deployment in real-industry scenarios. One bottleneck challenge behind is the well-known exploration problem, i.e., how efficiently exploring the environment and collecting informative experiences that could benefit policy learning toward the optimal ones. This problem becomes more challenging in complex environments with sparse rewards, noisy distractions, long horizons, and nonstationary co-learners. In this article, we conduct a comprehensive survey on existing exploration methods for both single-agent RL and multiagent RL. We start the survey by identifying several key challenges to efficient exploration. Then, we provide a systematic survey of existing approaches by classifying them into two major categories: uncertainty-oriented exploration and intrinsic motivation-oriented exploration. Beyond the above two main branches, we also include other notable exploration methods with different ideas and techniques. In addition to algorithmic analysis, we provide a comprehensive and unified empirical comparison of different exploration methods for DRL on a set of commonly used benchmarks. According to our algorithmic and empirical investigation, we finally summarize the open problems of exploration in DRL and deep MARL and point out a few future directions.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/12075ea34f5fbe32ec5582786761ab34d401209b.pdf",
        "venue": "IEEE Transactions on Neural Networks and Learning Systems",
        "citationCount": 120,
        "score": 30.0,
        "summary": "Deep reinforcement learning (DRL) and deep multiagent reinforcement learning (MARL) have achieved significant success across a wide range of domains, including game artificial intelligence (AI), autonomous vehicles, and robotics. However, DRL and deep MARL agents are widely known to be sample inefficient that millions of interactions are usually needed even for relatively simple problem settings, thus preventing the wide application and deployment in real-industry scenarios. One bottleneck challenge behind is the well-known exploration problem, i.e., how efficiently exploring the environment and collecting informative experiences that could benefit policy learning toward the optimal ones. This problem becomes more challenging in complex environments with sparse rewards, noisy distractions, long horizons, and nonstationary co-learners. In this article, we conduct a comprehensive survey on existing exploration methods for both single-agent RL and multiagent RL. We start the survey by identifying several key challenges to efficient exploration. Then, we provide a systematic survey of existing approaches by classifying them into two major categories: uncertainty-oriented exploration and intrinsic motivation-oriented exploration. Beyond the above two main branches, we also include other notable exploration methods with different ideas and techniques. In addition to algorithmic analysis, we provide a comprehensive and unified empirical comparison of different exploration methods for DRL on a set of commonly used benchmarks. According to our algorithmic and empirical investigation, we finally summarize the open problems of exploration in DRL and deep MARL and point out a few future directions.",
        "keywords": []
      },
      "file_name": "12075ea34f5fbe32ec5582786761ab34d401209b.pdf"
    },
    {
      "success": true,
      "doc_id": "d3e38a6603534692181388f14d7c6eb6",
      "summary": "Deep reinforcement learning (RL) agents often fail to generalize to unseen environments (yet semantically similar to trained agents), particularly when they are trained on high-dimensional state spaces, such as images. In this paper, we propose a simple technique to improve a generalization ability of deep RL agents by introducing a randomized (convolutional) neural network that randomly perturbs input observations. It enables trained agents to adapt to new domains by learning robust features invariant across varied and randomized environments. Furthermore, we consider an inference method based on the Monte Carlo approximation to reduce the variance induced by this randomization. We demonstrate the superiority of our method across 2D CoinRun, 3D DeepMind Lab exploration and 3D robotics control tasks: it significantly outperforms various regularization and data augmentation methods for the same purpose.",
      "intriguing_abstract": "Deep reinforcement learning (RL) agents often fail to generalize to unseen environments (yet semantically similar to trained agents), particularly when they are trained on high-dimensional state spaces, such as images. In this paper, we propose a simple technique to improve a generalization ability of deep RL agents by introducing a randomized (convolutional) neural network that randomly perturbs input observations. It enables trained agents to adapt to new domains by learning robust features invariant across varied and randomized environments. Furthermore, we consider an inference method based on the Monte Carlo approximation to reduce the variance induced by this randomization. We demonstrate the superiority of our method across 2D CoinRun, 3D DeepMind Lab exploration and 3D robotics control tasks: it significantly outperforms various regularization and data augmentation methods for the same purpose.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/dc05886db1e6f17f4489d867477b38fe13e31783.pdf",
      "citation_key": "lee2019hnz",
      "metadata": {
        "title": "Network Randomization: A Simple Technique for Generalization in Deep Reinforcement Learning",
        "authors": [
          "Kimin Lee",
          "Kibok Lee",
          "Jinwoo Shin",
          "Honglak Lee"
        ],
        "published_date": "2019",
        "abstract": "Deep reinforcement learning (RL) agents often fail to generalize to unseen environments (yet semantically similar to trained agents), particularly when they are trained on high-dimensional state spaces, such as images. In this paper, we propose a simple technique to improve a generalization ability of deep RL agents by introducing a randomized (convolutional) neural network that randomly perturbs input observations. It enables trained agents to adapt to new domains by learning robust features invariant across varied and randomized environments. Furthermore, we consider an inference method based on the Monte Carlo approximation to reduce the variance induced by this randomization. We demonstrate the superiority of our method across 2D CoinRun, 3D DeepMind Lab exploration and 3D robotics control tasks: it significantly outperforms various regularization and data augmentation methods for the same purpose.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/dc05886db1e6f17f4489d867477b38fe13e31783.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 178,
        "score": 29.666666666666664,
        "summary": "Deep reinforcement learning (RL) agents often fail to generalize to unseen environments (yet semantically similar to trained agents), particularly when they are trained on high-dimensional state spaces, such as images. In this paper, we propose a simple technique to improve a generalization ability of deep RL agents by introducing a randomized (convolutional) neural network that randomly perturbs input observations. It enables trained agents to adapt to new domains by learning robust features invariant across varied and randomized environments. Furthermore, we consider an inference method based on the Monte Carlo approximation to reduce the variance induced by this randomization. We demonstrate the superiority of our method across 2D CoinRun, 3D DeepMind Lab exploration and 3D robotics control tasks: it significantly outperforms various regularization and data augmentation methods for the same purpose.",
        "keywords": []
      },
      "file_name": "dc05886db1e6f17f4489d867477b38fe13e31783.pdf"
    },
    {
      "success": true,
      "doc_id": "b3b8f91034c978f0201ab1f4c137a023",
      "summary": "In recent years, reinforcement learning (RL) systems with general goals beyond a cumulative sum of rewards have gained traction, such as in constrained problems, exploration, and acting upon prior experiences. In this paper, we consider policy optimization in Markov Decision Problems, where the objective is a general concave utility function of the state-action occupancy measure, which subsumes several of the aforementioned examples as special cases. Such generality invalidates the Bellman equation. As this means that dynamic programming no longer works, we focus on direct policy search. Analogously to the Policy Gradient Theorem \\cite{sutton2000policy} available for RL with cumulative rewards, we derive a new Variational Policy Gradient Theorem for RL with general utilities, which establishes that the parametrized policy gradient may be obtained as the solution of a stochastic saddle point problem involving the Fenchel dual of the utility function. We develop a variational Monte Carlo gradient estimation algorithm to compute the policy gradient based on sample paths. We prove that the variational policy gradient scheme converges globally to the optimal policy for the general objective, though the optimization problem is nonconvex. We also establish its rate of convergence of the order $O(1/t)$ by exploiting the hidden convexity of the problem, and proves that it converges exponentially when the problem admits hidden strong convexity. Our analysis applies to the standard RL problem with cumulative rewards as a special case, in which case our result improves the available convergence rate.",
      "intriguing_abstract": "In recent years, reinforcement learning (RL) systems with general goals beyond a cumulative sum of rewards have gained traction, such as in constrained problems, exploration, and acting upon prior experiences. In this paper, we consider policy optimization in Markov Decision Problems, where the objective is a general concave utility function of the state-action occupancy measure, which subsumes several of the aforementioned examples as special cases. Such generality invalidates the Bellman equation. As this means that dynamic programming no longer works, we focus on direct policy search. Analogously to the Policy Gradient Theorem \\cite{sutton2000policy} available for RL with cumulative rewards, we derive a new Variational Policy Gradient Theorem for RL with general utilities, which establishes that the parametrized policy gradient may be obtained as the solution of a stochastic saddle point problem involving the Fenchel dual of the utility function. We develop a variational Monte Carlo gradient estimation algorithm to compute the policy gradient based on sample paths. We prove that the variational policy gradient scheme converges globally to the optimal policy for the general objective, though the optimization problem is nonconvex. We also establish its rate of convergence of the order $O(1/t)$ by exploiting the hidden convexity of the problem, and proves that it converges exponentially when the problem admits hidden strong convexity. Our analysis applies to the standard RL problem with cumulative rewards as a special case, in which case our result improves the available convergence rate.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/6bf9b58b8f418fb2922762550fb78bb22c83c0f8.pdf",
      "citation_key": "zhang2020o5t",
      "metadata": {
        "title": "Variational Policy Gradient Method for Reinforcement Learning with General Utilities",
        "authors": [
          "Junyu Zhang",
          "Alec Koppel",
          "A. S. Bedi",
          "Csaba Szepesvari",
          "Mengdi Wang"
        ],
        "published_date": "2020",
        "abstract": "In recent years, reinforcement learning (RL) systems with general goals beyond a cumulative sum of rewards have gained traction, such as in constrained problems, exploration, and acting upon prior experiences. In this paper, we consider policy optimization in Markov Decision Problems, where the objective is a general concave utility function of the state-action occupancy measure, which subsumes several of the aforementioned examples as special cases. Such generality invalidates the Bellman equation. As this means that dynamic programming no longer works, we focus on direct policy search. Analogously to the Policy Gradient Theorem \\cite{sutton2000policy} available for RL with cumulative rewards, we derive a new Variational Policy Gradient Theorem for RL with general utilities, which establishes that the parametrized policy gradient may be obtained as the solution of a stochastic saddle point problem involving the Fenchel dual of the utility function. We develop a variational Monte Carlo gradient estimation algorithm to compute the policy gradient based on sample paths. We prove that the variational policy gradient scheme converges globally to the optimal policy for the general objective, though the optimization problem is nonconvex. We also establish its rate of convergence of the order $O(1/t)$ by exploiting the hidden convexity of the problem, and proves that it converges exponentially when the problem admits hidden strong convexity. Our analysis applies to the standard RL problem with cumulative rewards as a special case, in which case our result improves the available convergence rate.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/6bf9b58b8f418fb2922762550fb78bb22c83c0f8.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 146,
        "score": 29.200000000000003,
        "summary": "In recent years, reinforcement learning (RL) systems with general goals beyond a cumulative sum of rewards have gained traction, such as in constrained problems, exploration, and acting upon prior experiences. In this paper, we consider policy optimization in Markov Decision Problems, where the objective is a general concave utility function of the state-action occupancy measure, which subsumes several of the aforementioned examples as special cases. Such generality invalidates the Bellman equation. As this means that dynamic programming no longer works, we focus on direct policy search. Analogously to the Policy Gradient Theorem \\cite{sutton2000policy} available for RL with cumulative rewards, we derive a new Variational Policy Gradient Theorem for RL with general utilities, which establishes that the parametrized policy gradient may be obtained as the solution of a stochastic saddle point problem involving the Fenchel dual of the utility function. We develop a variational Monte Carlo gradient estimation algorithm to compute the policy gradient based on sample paths. We prove that the variational policy gradient scheme converges globally to the optimal policy for the general objective, though the optimization problem is nonconvex. We also establish its rate of convergence of the order $O(1/t)$ by exploiting the hidden convexity of the problem, and proves that it converges exponentially when the problem admits hidden strong convexity. Our analysis applies to the standard RL problem with cumulative rewards as a special case, in which case our result improves the available convergence rate.",
        "keywords": []
      },
      "file_name": "6bf9b58b8f418fb2922762550fb78bb22c83c0f8.pdf"
    },
    {
      "success": true,
      "doc_id": "912fb5f96bb966b95342c09481ec4667",
      "summary": "The production process of a smart factory is complex and dynamic. As the core of manufacturing management, the research into the flexible job shop scheduling problem (FJSP) focuses on optimizing scheduling decisions in real time, according to the changes in the production environment. In this paper, deep reinforcement learning (DRL) is proposed to solve the dynamic FJSP (DFJSP) with random job arrival, with the goal of minimizing penalties for earliness and tardiness. A double deep Q-networks (DDQN) architecture is proposed and state features, actions and rewards are designed. A soft -greedy behavior policy is designed according to the scale of the problem. The experimental results show that the proposed DRL is better than other reinforcement learning (RL) algorithms, heuristics and metaheuristics in terms of solution quality and generalization. In addition, the soft -greedy strategy reasonably balances exploration and exploitation, thereby improving the learning efficiency of the scheduling agent. The DRL method is adaptive to the dynamic changes of the production environment in a flexible job shop, which contributes to the establishment of a flexible scheduling system with self-learning, real-time optimization and intelligent decision-making.",
      "intriguing_abstract": "The production process of a smart factory is complex and dynamic. As the core of manufacturing management, the research into the flexible job shop scheduling problem (FJSP) focuses on optimizing scheduling decisions in real time, according to the changes in the production environment. In this paper, deep reinforcement learning (DRL) is proposed to solve the dynamic FJSP (DFJSP) with random job arrival, with the goal of minimizing penalties for earliness and tardiness. A double deep Q-networks (DDQN) architecture is proposed and state features, actions and rewards are designed. A soft -greedy behavior policy is designed according to the scale of the problem. The experimental results show that the proposed DRL is better than other reinforcement learning (RL) algorithms, heuristics and metaheuristics in terms of solution quality and generalization. In addition, the soft -greedy strategy reasonably balances exploration and exploitation, thereby improving the learning efficiency of the scheduling agent. The DRL method is adaptive to the dynamic changes of the production environment in a flexible job shop, which contributes to the establishment of a flexible scheduling system with self-learning, real-time optimization and intelligent decision-making.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/6ce21379ffac786207632d16ea7d6e3eb150f910.pdf",
      "citation_key": "chang20221gc",
      "metadata": {
        "title": "Deep Reinforcement Learning for Dynamic Flexible Job Shop Scheduling with Random Job Arrival",
        "authors": [
          "Jingru Chang",
          "Dong Yu",
          "Y. Hu",
          "Wuwei He",
          "Haoyu Yu"
        ],
        "published_date": "2022",
        "abstract": "The production process of a smart factory is complex and dynamic. As the core of manufacturing management, the research into the flexible job shop scheduling problem (FJSP) focuses on optimizing scheduling decisions in real time, according to the changes in the production environment. In this paper, deep reinforcement learning (DRL) is proposed to solve the dynamic FJSP (DFJSP) with random job arrival, with the goal of minimizing penalties for earliness and tardiness. A double deep Q-networks (DDQN) architecture is proposed and state features, actions and rewards are designed. A soft -greedy behavior policy is designed according to the scale of the problem. The experimental results show that the proposed DRL is better than other reinforcement learning (RL) algorithms, heuristics and metaheuristics in terms of solution quality and generalization. In addition, the soft -greedy strategy reasonably balances exploration and exploitation, thereby improving the learning efficiency of the scheduling agent. The DRL method is adaptive to the dynamic changes of the production environment in a flexible job shop, which contributes to the establishment of a flexible scheduling system with self-learning, real-time optimization and intelligent decision-making.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/6ce21379ffac786207632d16ea7d6e3eb150f910.pdf",
        "venue": "Processes",
        "citationCount": 82,
        "score": 27.333333333333332,
        "summary": "The production process of a smart factory is complex and dynamic. As the core of manufacturing management, the research into the flexible job shop scheduling problem (FJSP) focuses on optimizing scheduling decisions in real time, according to the changes in the production environment. In this paper, deep reinforcement learning (DRL) is proposed to solve the dynamic FJSP (DFJSP) with random job arrival, with the goal of minimizing penalties for earliness and tardiness. A double deep Q-networks (DDQN) architecture is proposed and state features, actions and rewards are designed. A soft -greedy behavior policy is designed according to the scale of the problem. The experimental results show that the proposed DRL is better than other reinforcement learning (RL) algorithms, heuristics and metaheuristics in terms of solution quality and generalization. In addition, the soft -greedy strategy reasonably balances exploration and exploitation, thereby improving the learning efficiency of the scheduling agent. The DRL method is adaptive to the dynamic changes of the production environment in a flexible job shop, which contributes to the establishment of a flexible scheduling system with self-learning, real-time optimization and intelligent decision-making.",
        "keywords": []
      },
      "file_name": "6ce21379ffac786207632d16ea7d6e3eb150f910.pdf"
    },
    {
      "success": true,
      "doc_id": "2e279ca9ac7e40be621ef36788ccc680",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/2e5dc4b028e670d1b9ac0b4e15580cdb7197cb68.pdf",
      "citation_key": "yang2021psl",
      "metadata": {
        "title": "Exploration in Deep Reinforcement Learning: A Comprehensive Survey",
        "authors": [
          "Tianpei Yang",
          "Hongyao Tang",
          "Chenjia Bai",
          "Jinyi Liu",
          "Jianye Hao",
          "Zhaopeng Meng",
          "Peng Liu"
        ],
        "published_date": "2021",
        "abstract": "",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/2e5dc4b028e670d1b9ac0b4e15580cdb7197cb68.pdf",
        "venue": "arXiv.org",
        "citationCount": 103,
        "score": 25.75,
        "summary": "",
        "keywords": []
      },
      "file_name": "2e5dc4b028e670d1b9ac0b4e15580cdb7197cb68.pdf"
    },
    {
      "success": true,
      "doc_id": "77207428ee2735e9d769a53d104cb0e8",
      "summary": "Offline Reinforcement learning (RL) has shown potent in many safe-critical tasks in robotics where exploration is risky and expensive. However, it still struggles to acquire skills in temporally extended tasks. In this paper, we study the problem of offline RL for temporally extended tasks. We propose a hierarchical planning framework, consisting of a low-level goal-conditioned RL policy and a high-level goal planner. The low-level policy is trained via offline RL. We improve the offline training to deal with out-of-distribution goals by a perturbed goal sampling process. The high-level planner selects intermediate sub-goals by taking advantages of model-based planning methods. It plans over future sub-goal sequences based on the learned value function of the low-level policy. We adopt a Conditional Variational Autoencoder to sample meaningful high-dimensional sub-goal candidates and to solve the high-level long-term strategy optimization problem. We evaluate our proposed method in long-horizon driving and robot navigation tasks. Experiments show that our method outperforms baselines with different hierarchical designs and other regular planners without hierarchy in these complex tasks.",
      "intriguing_abstract": "Offline Reinforcement learning (RL) has shown potent in many safe-critical tasks in robotics where exploration is risky and expensive. However, it still struggles to acquire skills in temporally extended tasks. In this paper, we study the problem of offline RL for temporally extended tasks. We propose a hierarchical planning framework, consisting of a low-level goal-conditioned RL policy and a high-level goal planner. The low-level policy is trained via offline RL. We improve the offline training to deal with out-of-distribution goals by a perturbed goal sampling process. The high-level planner selects intermediate sub-goals by taking advantages of model-based planning methods. It plans over future sub-goal sequences based on the learned value function of the low-level policy. We adopt a Conditional Variational Autoencoder to sample meaningful high-dimensional sub-goal candidates and to solve the high-level long-term strategy optimization problem. We evaluate our proposed method in long-horizon driving and robot navigation tasks. Experiments show that our method outperforms baselines with different hierarchical designs and other regular planners without hierarchy in these complex tasks.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/f593dc96b20ce8427182e773e3b2192d707706a8.pdf",
      "citation_key": "li2022ktf",
      "metadata": {
        "title": "Hierarchical Planning Through Goal-Conditioned Offline Reinforcement Learning",
        "authors": [
          "Jinning Li",
          "Chen Tang",
          "M. Tomizuka",
          "Wei Zhan"
        ],
        "published_date": "2022",
        "abstract": "Offline Reinforcement learning (RL) has shown potent in many safe-critical tasks in robotics where exploration is risky and expensive. However, it still struggles to acquire skills in temporally extended tasks. In this paper, we study the problem of offline RL for temporally extended tasks. We propose a hierarchical planning framework, consisting of a low-level goal-conditioned RL policy and a high-level goal planner. The low-level policy is trained via offline RL. We improve the offline training to deal with out-of-distribution goals by a perturbed goal sampling process. The high-level planner selects intermediate sub-goals by taking advantages of model-based planning methods. It plans over future sub-goal sequences based on the learned value function of the low-level policy. We adopt a Conditional Variational Autoencoder to sample meaningful high-dimensional sub-goal candidates and to solve the high-level long-term strategy optimization problem. We evaluate our proposed method in long-horizon driving and robot navigation tasks. Experiments show that our method outperforms baselines with different hierarchical designs and other regular planners without hierarchy in these complex tasks.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/f593dc96b20ce8427182e773e3b2192d707706a8.pdf",
        "venue": "IEEE Robotics and Automation Letters",
        "citationCount": 65,
        "score": 21.666666666666664,
        "summary": "Offline Reinforcement learning (RL) has shown potent in many safe-critical tasks in robotics where exploration is risky and expensive. However, it still struggles to acquire skills in temporally extended tasks. In this paper, we study the problem of offline RL for temporally extended tasks. We propose a hierarchical planning framework, consisting of a low-level goal-conditioned RL policy and a high-level goal planner. The low-level policy is trained via offline RL. We improve the offline training to deal with out-of-distribution goals by a perturbed goal sampling process. The high-level planner selects intermediate sub-goals by taking advantages of model-based planning methods. It plans over future sub-goal sequences based on the learned value function of the low-level policy. We adopt a Conditional Variational Autoencoder to sample meaningful high-dimensional sub-goal candidates and to solve the high-level long-term strategy optimization problem. We evaluate our proposed method in long-horizon driving and robot navigation tasks. Experiments show that our method outperforms baselines with different hierarchical designs and other regular planners without hierarchy in these complex tasks.",
        "keywords": []
      },
      "file_name": "f593dc96b20ce8427182e773e3b2192d707706a8.pdf"
    },
    {
      "success": true,
      "doc_id": "c86920e2f91dea396f1a9ea9f8dc98c3",
      "summary": "Conveying complex objectives to reinforcement learning (RL) agents often requires meticulous reward engineering. Preference-based RL methods are able to learn a more flexible reward model based on human preferences by actively incorporating human feedback, i.e. teacher's preferences between two clips of behaviors. However, poor feedback-efficiency still remains a problem in current preference-based RL algorithms, as tailored human feedback is very expensive. To handle this issue, previous methods have mainly focused on improving query selection and policy initialization. At the same time, recent exploration methods have proven to be a recipe for improving sample-efficiency in RL. We present an exploration method specifically for preference-based RL algorithms. Our main idea is to design an intrinsic reward by measuring the novelty based on learned reward. Specifically, we utilize disagreement across ensemble of learned reward models. Our intuition is that disagreement in learned reward model reflects uncertainty in tailored human feedback and could be useful for exploration. Our experiments show that exploration bonus from uncertainty in learned reward improves both feedback- and sample-efficiency of preference-based RL algorithms on complex robot manipulation tasks from MetaWorld benchmarks, compared with other existing exploration methods that measure the novelty of state visitation.",
      "intriguing_abstract": "Conveying complex objectives to reinforcement learning (RL) agents often requires meticulous reward engineering. Preference-based RL methods are able to learn a more flexible reward model based on human preferences by actively incorporating human feedback, i.e. teacher's preferences between two clips of behaviors. However, poor feedback-efficiency still remains a problem in current preference-based RL algorithms, as tailored human feedback is very expensive. To handle this issue, previous methods have mainly focused on improving query selection and policy initialization. At the same time, recent exploration methods have proven to be a recipe for improving sample-efficiency in RL. We present an exploration method specifically for preference-based RL algorithms. Our main idea is to design an intrinsic reward by measuring the novelty based on learned reward. Specifically, we utilize disagreement across ensemble of learned reward models. Our intuition is that disagreement in learned reward model reflects uncertainty in tailored human feedback and could be useful for exploration. Our experiments show that exploration bonus from uncertainty in learned reward improves both feedback- and sample-efficiency of preference-based RL algorithms on complex robot manipulation tasks from MetaWorld benchmarks, compared with other existing exploration methods that measure the novelty of state visitation.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/cc9f2fd320a279741403c4bfbeb91179803c428c.pdf",
      "citation_key": "liang20226ix",
      "metadata": {
        "title": "Reward Uncertainty for Exploration in Preference-based Reinforcement Learning",
        "authors": [
          "Xi-Xi Liang",
          "Katherine Shu",
          "Kimin Lee",
          "P. Abbeel"
        ],
        "published_date": "2022",
        "abstract": "Conveying complex objectives to reinforcement learning (RL) agents often requires meticulous reward engineering. Preference-based RL methods are able to learn a more flexible reward model based on human preferences by actively incorporating human feedback, i.e. teacher's preferences between two clips of behaviors. However, poor feedback-efficiency still remains a problem in current preference-based RL algorithms, as tailored human feedback is very expensive. To handle this issue, previous methods have mainly focused on improving query selection and policy initialization. At the same time, recent exploration methods have proven to be a recipe for improving sample-efficiency in RL. We present an exploration method specifically for preference-based RL algorithms. Our main idea is to design an intrinsic reward by measuring the novelty based on learned reward. Specifically, we utilize disagreement across ensemble of learned reward models. Our intuition is that disagreement in learned reward model reflects uncertainty in tailored human feedback and could be useful for exploration. Our experiments show that exploration bonus from uncertainty in learned reward improves both feedback- and sample-efficiency of preference-based RL algorithms on complex robot manipulation tasks from MetaWorld benchmarks, compared with other existing exploration methods that measure the novelty of state visitation.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/cc9f2fd320a279741403c4bfbeb91179803c428c.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 59,
        "score": 19.666666666666664,
        "summary": "Conveying complex objectives to reinforcement learning (RL) agents often requires meticulous reward engineering. Preference-based RL methods are able to learn a more flexible reward model based on human preferences by actively incorporating human feedback, i.e. teacher's preferences between two clips of behaviors. However, poor feedback-efficiency still remains a problem in current preference-based RL algorithms, as tailored human feedback is very expensive. To handle this issue, previous methods have mainly focused on improving query selection and policy initialization. At the same time, recent exploration methods have proven to be a recipe for improving sample-efficiency in RL. We present an exploration method specifically for preference-based RL algorithms. Our main idea is to design an intrinsic reward by measuring the novelty based on learned reward. Specifically, we utilize disagreement across ensemble of learned reward models. Our intuition is that disagreement in learned reward model reflects uncertainty in tailored human feedback and could be useful for exploration. Our experiments show that exploration bonus from uncertainty in learned reward improves both feedback- and sample-efficiency of preference-based RL algorithms on complex robot manipulation tasks from MetaWorld benchmarks, compared with other existing exploration methods that measure the novelty of state visitation.",
        "keywords": []
      },
      "file_name": "cc9f2fd320a279741403c4bfbeb91179803c428c.pdf"
    },
    {
      "success": true,
      "doc_id": "e31a16606df19816b140a89594ca3cef",
      "summary": "Efficient exploration remains a challenging research problem in reinforcement learning, especially when an environment contains large state spaces, deceptive local optima, or sparse rewards. To tackle this problem, we present a diversity-driven approach for exploration, which can be easily combined with both off- and on-policy reinforcement learning algorithms. We show that by simply adding a distance measure to the loss function, the proposed methodology significantly enhances an agent's exploratory behaviors, and thus preventing the policy from being trapped in local optima. We further propose an adaptive scaling method for stabilizing the learning process. Our experimental results in Atari 2600 show that our method outperforms baseline approaches in several tasks in terms of mean scores and exploration efficiency.",
      "intriguing_abstract": "Efficient exploration remains a challenging research problem in reinforcement learning, especially when an environment contains large state spaces, deceptive local optima, or sparse rewards. To tackle this problem, we present a diversity-driven approach for exploration, which can be easily combined with both off- and on-policy reinforcement learning algorithms. We show that by simply adding a distance measure to the loss function, the proposed methodology significantly enhances an agent's exploratory behaviors, and thus preventing the policy from being trapped in local optima. We further propose an adaptive scaling method for stabilizing the learning process. Our experimental results in Atari 2600 show that our method outperforms baseline approaches in several tasks in terms of mean scores and exploration efficiency.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/3c3093f5f8e9601637612fcfb8f160f116fa30e4.pdf",
      "citation_key": "hong20182pr",
      "metadata": {
        "title": "Diversity-Driven Exploration Strategy for Deep Reinforcement Learning",
        "authors": [
          "Zhang-Wei Hong",
          "Tzu-Yun Shann",
          "Shih-Yang Su",
          "Yi-Hsiang Chang",
          "Chun-Yi Lee"
        ],
        "published_date": "2018",
        "abstract": "Efficient exploration remains a challenging research problem in reinforcement learning, especially when an environment contains large state spaces, deceptive local optima, or sparse rewards. To tackle this problem, we present a diversity-driven approach for exploration, which can be easily combined with both off- and on-policy reinforcement learning algorithms. We show that by simply adding a distance measure to the loss function, the proposed methodology significantly enhances an agent's exploratory behaviors, and thus preventing the policy from being trapped in local optima. We further propose an adaptive scaling method for stabilizing the learning process. Our experimental results in Atari 2600 show that our method outperforms baseline approaches in several tasks in terms of mean scores and exploration efficiency.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/3c3093f5f8e9601637612fcfb8f160f116fa30e4.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 128,
        "score": 18.285714285714285,
        "summary": "Efficient exploration remains a challenging research problem in reinforcement learning, especially when an environment contains large state spaces, deceptive local optima, or sparse rewards. To tackle this problem, we present a diversity-driven approach for exploration, which can be easily combined with both off- and on-policy reinforcement learning algorithms. We show that by simply adding a distance measure to the loss function, the proposed methodology significantly enhances an agent's exploratory behaviors, and thus preventing the policy from being trapped in local optima. We further propose an adaptive scaling method for stabilizing the learning process. Our experimental results in Atari 2600 show that our method outperforms baseline approaches in several tasks in terms of mean scores and exploration efficiency.",
        "keywords": []
      },
      "file_name": "3c3093f5f8e9601637612fcfb8f160f116fa30e4.pdf"
    },
    {
      "success": true,
      "doc_id": "58be330468d162d9eb34a5e88e7b27f6",
      "summary": "Poor sample efficiency continues to be the primary challenge for deployment of deep Reinforcement Learning (RL) algorithms for real-world applications, and in particular for visuo-motor control. Model-based RL has the potential to be highly sample efficient by concurrently learning a world model and using synthetic rollouts for planning and policy improvement. However, in practice, sample-efficient learning with model-based RL is bottlenecked by the exploration challenge. In this work, we find that leveraging just a handful of demonstrations can dramatically improve the sample-efficiency of model-based RL. Simply appending demonstrations to the interaction dataset, however, does not suffice. We identify key ingredients for leveraging demonstrations in model learning -- policy pretraining, targeted exploration, and oversampling of demonstration data -- which forms the three phases of our model-based RL framework. We empirically study three complex visuo-motor control domains and find that our method is 150%-250% more successful in completing sparse reward tasks compared to prior approaches in the low data regime (100K interaction steps, 5 demonstrations). Code and videos are available at: https://nicklashansen.github.io/modemrl",
      "intriguing_abstract": "Poor sample efficiency continues to be the primary challenge for deployment of deep Reinforcement Learning (RL) algorithms for real-world applications, and in particular for visuo-motor control. Model-based RL has the potential to be highly sample efficient by concurrently learning a world model and using synthetic rollouts for planning and policy improvement. However, in practice, sample-efficient learning with model-based RL is bottlenecked by the exploration challenge. In this work, we find that leveraging just a handful of demonstrations can dramatically improve the sample-efficiency of model-based RL. Simply appending demonstrations to the interaction dataset, however, does not suffice. We identify key ingredients for leveraging demonstrations in model learning -- policy pretraining, targeted exploration, and oversampling of demonstration data -- which forms the three phases of our model-based RL framework. We empirically study three complex visuo-motor control domains and find that our method is 150%-250% more successful in completing sparse reward tasks compared to prior approaches in the low data regime (100K interaction steps, 5 demonstrations). Code and videos are available at: https://nicklashansen.github.io/modemrl",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/b2004f4f19bc6ccae8e4afc554f39142870100f5.pdf",
      "citation_key": "hansen2022jm2",
      "metadata": {
        "title": "MoDem: Accelerating Visual Model-Based Reinforcement Learning with Demonstrations",
        "authors": [
          "Nicklas Hansen",
          "Yixin Lin",
          "H. Su",
          "Xiaolong Wang",
          "Vikash Kumar",
          "A. Rajeswaran"
        ],
        "published_date": "2022",
        "abstract": "Poor sample efficiency continues to be the primary challenge for deployment of deep Reinforcement Learning (RL) algorithms for real-world applications, and in particular for visuo-motor control. Model-based RL has the potential to be highly sample efficient by concurrently learning a world model and using synthetic rollouts for planning and policy improvement. However, in practice, sample-efficient learning with model-based RL is bottlenecked by the exploration challenge. In this work, we find that leveraging just a handful of demonstrations can dramatically improve the sample-efficiency of model-based RL. Simply appending demonstrations to the interaction dataset, however, does not suffice. We identify key ingredients for leveraging demonstrations in model learning -- policy pretraining, targeted exploration, and oversampling of demonstration data -- which forms the three phases of our model-based RL framework. We empirically study three complex visuo-motor control domains and find that our method is 150%-250% more successful in completing sparse reward tasks compared to prior approaches in the low data regime (100K interaction steps, 5 demonstrations). Code and videos are available at: https://nicklashansen.github.io/modemrl",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/b2004f4f19bc6ccae8e4afc554f39142870100f5.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 54,
        "score": 18.0,
        "summary": "Poor sample efficiency continues to be the primary challenge for deployment of deep Reinforcement Learning (RL) algorithms for real-world applications, and in particular for visuo-motor control. Model-based RL has the potential to be highly sample efficient by concurrently learning a world model and using synthetic rollouts for planning and policy improvement. However, in practice, sample-efficient learning with model-based RL is bottlenecked by the exploration challenge. In this work, we find that leveraging just a handful of demonstrations can dramatically improve the sample-efficiency of model-based RL. Simply appending demonstrations to the interaction dataset, however, does not suffice. We identify key ingredients for leveraging demonstrations in model learning -- policy pretraining, targeted exploration, and oversampling of demonstration data -- which forms the three phases of our model-based RL framework. We empirically study three complex visuo-motor control domains and find that our method is 150%-250% more successful in completing sparse reward tasks compared to prior approaches in the low data regime (100K interaction steps, 5 demonstrations). Code and videos are available at: https://nicklashansen.github.io/modemrl",
        "keywords": []
      },
      "file_name": "b2004f4f19bc6ccae8e4afc554f39142870100f5.pdf"
    },
    {
      "success": true,
      "doc_id": "17bf71abe0aa71c79bc1dfacf1310fa5",
      "summary": "Meta-reinforcement learning (RL) methods can meta-train policies that adapt to new tasks with orders of magnitude less data than standard RL, but meta-training itself is costly and time-consuming. If we can meta-train on offline data, then we can reuse the same static dataset, labeled once with rewards for different tasks, to meta-train policies that adapt to a variety of new tasks at meta-test time. Although this capability would make meta-RL a practical tool for real-world use, offline meta-RL presents additional challenges beyond online meta-RL or standard offline RL settings. Meta-RL learns an exploration strategy that collects data for adapting, and also meta-trains a policy that quickly adapts to data from a new task. Since this policy was meta-trained on a fixed, offline dataset, it might behave unpredictably when adapting to data collected by the learned exploration strategy, which differs systematically from the offline data and thus induces distributional shift. We propose a hybrid offline meta-RL algorithm, which uses offline data with rewards to meta-train an adaptive policy, and then collects additional unsupervised online data, without any reward labels to bridge this distribution shift. By not requiring reward labels for online collection, this data can be much cheaper to collect. We compare our method to prior work on offline meta-RL on simulated robot locomotion and manipulation tasks and find that using additional unsupervised online data collection leads to a dramatic improvement in the adaptive capabilities of the meta-trained policies, matching the performance of fully online meta-RL on a range of challenging domains that require generalization to new tasks.",
      "intriguing_abstract": "Meta-reinforcement learning (RL) methods can meta-train policies that adapt to new tasks with orders of magnitude less data than standard RL, but meta-training itself is costly and time-consuming. If we can meta-train on offline data, then we can reuse the same static dataset, labeled once with rewards for different tasks, to meta-train policies that adapt to a variety of new tasks at meta-test time. Although this capability would make meta-RL a practical tool for real-world use, offline meta-RL presents additional challenges beyond online meta-RL or standard offline RL settings. Meta-RL learns an exploration strategy that collects data for adapting, and also meta-trains a policy that quickly adapts to data from a new task. Since this policy was meta-trained on a fixed, offline dataset, it might behave unpredictably when adapting to data collected by the learned exploration strategy, which differs systematically from the offline data and thus induces distributional shift. We propose a hybrid offline meta-RL algorithm, which uses offline data with rewards to meta-train an adaptive policy, and then collects additional unsupervised online data, without any reward labels to bridge this distribution shift. By not requiring reward labels for online collection, this data can be much cheaper to collect. We compare our method to prior work on offline meta-RL on simulated robot locomotion and manipulation tasks and find that using additional unsupervised online data collection leads to a dramatic improvement in the adaptive capabilities of the meta-trained policies, matching the performance of fully online meta-RL on a range of challenging domains that require generalization to new tasks.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/61f371768cdc093828f432660e22f7a17f22e2af.pdf",
      "citation_key": "pong2021i4o",
      "metadata": {
        "title": "Offline Meta-Reinforcement Learning with Online Self-Supervision",
        "authors": [
          "Vitchyr H. Pong",
          "Ashvin Nair",
          "Laura M. Smith",
          "Catherine Huang",
          "S. Levine"
        ],
        "published_date": "2021",
        "abstract": "Meta-reinforcement learning (RL) methods can meta-train policies that adapt to new tasks with orders of magnitude less data than standard RL, but meta-training itself is costly and time-consuming. If we can meta-train on offline data, then we can reuse the same static dataset, labeled once with rewards for different tasks, to meta-train policies that adapt to a variety of new tasks at meta-test time. Although this capability would make meta-RL a practical tool for real-world use, offline meta-RL presents additional challenges beyond online meta-RL or standard offline RL settings. Meta-RL learns an exploration strategy that collects data for adapting, and also meta-trains a policy that quickly adapts to data from a new task. Since this policy was meta-trained on a fixed, offline dataset, it might behave unpredictably when adapting to data collected by the learned exploration strategy, which differs systematically from the offline data and thus induces distributional shift. We propose a hybrid offline meta-RL algorithm, which uses offline data with rewards to meta-train an adaptive policy, and then collects additional unsupervised online data, without any reward labels to bridge this distribution shift. By not requiring reward labels for online collection, this data can be much cheaper to collect. We compare our method to prior work on offline meta-RL on simulated robot locomotion and manipulation tasks and find that using additional unsupervised online data collection leads to a dramatic improvement in the adaptive capabilities of the meta-trained policies, matching the performance of fully online meta-RL on a range of challenging domains that require generalization to new tasks.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/61f371768cdc093828f432660e22f7a17f22e2af.pdf",
        "venue": "International Conference on Machine Learning",
        "citationCount": 70,
        "score": 17.5,
        "summary": "Meta-reinforcement learning (RL) methods can meta-train policies that adapt to new tasks with orders of magnitude less data than standard RL, but meta-training itself is costly and time-consuming. If we can meta-train on offline data, then we can reuse the same static dataset, labeled once with rewards for different tasks, to meta-train policies that adapt to a variety of new tasks at meta-test time. Although this capability would make meta-RL a practical tool for real-world use, offline meta-RL presents additional challenges beyond online meta-RL or standard offline RL settings. Meta-RL learns an exploration strategy that collects data for adapting, and also meta-trains a policy that quickly adapts to data from a new task. Since this policy was meta-trained on a fixed, offline dataset, it might behave unpredictably when adapting to data collected by the learned exploration strategy, which differs systematically from the offline data and thus induces distributional shift. We propose a hybrid offline meta-RL algorithm, which uses offline data with rewards to meta-train an adaptive policy, and then collects additional unsupervised online data, without any reward labels to bridge this distribution shift. By not requiring reward labels for online collection, this data can be much cheaper to collect. We compare our method to prior work on offline meta-RL on simulated robot locomotion and manipulation tasks and find that using additional unsupervised online data collection leads to a dramatic improvement in the adaptive capabilities of the meta-trained policies, matching the performance of fully online meta-RL on a range of challenging domains that require generalization to new tasks.",
        "keywords": []
      },
      "file_name": "61f371768cdc093828f432660e22f7a17f22e2af.pdf"
    },
    {
      "success": true,
      "doc_id": "b6f6e2bc5370966fa6e81b45d1ba52d3",
      "summary": "In efforts to resolve social dilemmas, reinforcement learning is an alternative to imitation and exploration in evolutionary game theory. While imitation and exploration rely on the performance of neighbors, in reinforcement learning individuals alter their strategies based on their own performance in the past. For example, according to the BushMosteller model of reinforcement learning, an individuals strategy choice is driven by whether the received payoff satisfies a preset aspiration or not. Stimuli also play a key role in reinforcement learning in that they can determine whether a strategy should be kept or not. Here we use the Monte Carlo method to study pattern formation and phase transitions towards cooperation in social dilemmas that are driven by reinforcement learning. We distinguish local and global players according to the source of the stimulus they experience. While global players receive their stimuli from the whole neighborhood, local players focus solely on individual performance. We show that global players play a decisive role in ensuring cooperation, while local players fail in this regard, although both types of players show properties of moody cooperators. In particular, global players evoke stronger conditional cooperation in their neighborhoods based on direct reciprocity, which is rooted in the emerging spatial patterns and stronger interfaces around cooperative clusters.",
      "intriguing_abstract": "In efforts to resolve social dilemmas, reinforcement learning is an alternative to imitation and exploration in evolutionary game theory. While imitation and exploration rely on the performance of neighbors, in reinforcement learning individuals alter their strategies based on their own performance in the past. For example, according to the BushMosteller model of reinforcement learning, an individuals strategy choice is driven by whether the received payoff satisfies a preset aspiration or not. Stimuli also play a key role in reinforcement learning in that they can determine whether a strategy should be kept or not. Here we use the Monte Carlo method to study pattern formation and phase transitions towards cooperation in social dilemmas that are driven by reinforcement learning. We distinguish local and global players according to the source of the stimulus they experience. While global players receive their stimuli from the whole neighborhood, local players focus solely on individual performance. We show that global players play a decisive role in ensuring cooperation, while local players fail in this regard, although both types of players show properties of moody cooperators. In particular, global players evoke stronger conditional cooperation in their neighborhoods based on direct reciprocity, which is rooted in the emerging spatial patterns and stronger interfaces around cooperative clusters.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/1d6beec78e720beab5adb0a5fce6fa6b846aed1a.pdf",
      "citation_key": "jia2021kxs",
      "metadata": {
        "title": "Local and global stimuli in reinforcement learning",
        "authors": [
          "Danyang Jia",
          "Hao Guo",
          "Z. Song",
          "Lei Shi",
          "Xinyang Deng",
          "M. Perc",
          "Zhen Wang"
        ],
        "published_date": "2021",
        "abstract": "In efforts to resolve social dilemmas, reinforcement learning is an alternative to imitation and exploration in evolutionary game theory. While imitation and exploration rely on the performance of neighbors, in reinforcement learning individuals alter their strategies based on their own performance in the past. For example, according to the BushMosteller model of reinforcement learning, an individuals strategy choice is driven by whether the received payoff satisfies a preset aspiration or not. Stimuli also play a key role in reinforcement learning in that they can determine whether a strategy should be kept or not. Here we use the Monte Carlo method to study pattern formation and phase transitions towards cooperation in social dilemmas that are driven by reinforcement learning. We distinguish local and global players according to the source of the stimulus they experience. While global players receive their stimuli from the whole neighborhood, local players focus solely on individual performance. We show that global players play a decisive role in ensuring cooperation, while local players fail in this regard, although both types of players show properties of moody cooperators. In particular, global players evoke stronger conditional cooperation in their neighborhoods based on direct reciprocity, which is rooted in the emerging spatial patterns and stronger interfaces around cooperative clusters.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/1d6beec78e720beab5adb0a5fce6fa6b846aed1a.pdf",
        "venue": "New Journal of Physics",
        "citationCount": 70,
        "score": 17.5,
        "summary": "In efforts to resolve social dilemmas, reinforcement learning is an alternative to imitation and exploration in evolutionary game theory. While imitation and exploration rely on the performance of neighbors, in reinforcement learning individuals alter their strategies based on their own performance in the past. For example, according to the BushMosteller model of reinforcement learning, an individuals strategy choice is driven by whether the received payoff satisfies a preset aspiration or not. Stimuli also play a key role in reinforcement learning in that they can determine whether a strategy should be kept or not. Here we use the Monte Carlo method to study pattern formation and phase transitions towards cooperation in social dilemmas that are driven by reinforcement learning. We distinguish local and global players according to the source of the stimulus they experience. While global players receive their stimuli from the whole neighborhood, local players focus solely on individual performance. We show that global players play a decisive role in ensuring cooperation, while local players fail in this regard, although both types of players show properties of moody cooperators. In particular, global players evoke stronger conditional cooperation in their neighborhoods based on direct reciprocity, which is rooted in the emerging spatial patterns and stronger interfaces around cooperative clusters.",
        "keywords": []
      },
      "file_name": "1d6beec78e720beab5adb0a5fce6fa6b846aed1a.pdf"
    },
    {
      "success": true,
      "doc_id": "b6eb7449f394fadbeeafa3aa2719948b",
      "summary": "Guaranteed safety and performance under various circumstances remain technically critical and practically challenging for the wide deployment of autonomous vehicles. Safety-critical systems in general, require safe performance even during the reinforcement learning (RL) period. To address this issue, a Barrier Lyapunov Function-based safe RL (BLF-SRL) algorithm is proposed here for the formulated nonlinear system in strict-feedback form. This approach appropriately arranges and incorporates the BLF items into the optimized backstepping control method to constrain the state-variables in the designed safety region during learning. Wherein, thus, the optimal virtual/actual control in every backstepping subsystem is decomposed with BLF items and also with an adaptive uncertain item to be learned, which achieves safe exploration during the learning process. Then, the principle of Bellman optimality of continuous-time HamiltonJacobiBellman equation in every backstepping subsystem is satisfied with independently approximated actor and critic under the framework of actor-critic through the designed iterative updating. Eventually, the overall system control is optimized with the proposed BLF-SRL method. It is furthermore noteworthy that the variance of the attained control performance under uncertainty is also reduced with the proposed method. The effectiveness of the proposed method is verified with two motion control problems for autonomous vehicles through appropriate comparison simulations.",
      "intriguing_abstract": "Guaranteed safety and performance under various circumstances remain technically critical and practically challenging for the wide deployment of autonomous vehicles. Safety-critical systems in general, require safe performance even during the reinforcement learning (RL) period. To address this issue, a Barrier Lyapunov Function-based safe RL (BLF-SRL) algorithm is proposed here for the formulated nonlinear system in strict-feedback form. This approach appropriately arranges and incorporates the BLF items into the optimized backstepping control method to constrain the state-variables in the designed safety region during learning. Wherein, thus, the optimal virtual/actual control in every backstepping subsystem is decomposed with BLF items and also with an adaptive uncertain item to be learned, which achieves safe exploration during the learning process. Then, the principle of Bellman optimality of continuous-time HamiltonJacobiBellman equation in every backstepping subsystem is satisfied with independently approximated actor and critic under the framework of actor-critic through the designed iterative updating. Eventually, the overall system control is optimized with the proposed BLF-SRL method. It is furthermore noteworthy that the variance of the attained control performance under uncertainty is also reduced with the proposed method. The effectiveness of the proposed method is verified with two motion control problems for autonomous vehicles through appropriate comparison simulations.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/ba44a95f1a8bc5765438d03c01137799e930c88d.pdf",
      "citation_key": "zhang2022dgg",
      "metadata": {
        "title": "Barrier Lyapunov Function-Based Safe Reinforcement Learning for Autonomous Vehicles With Optimized Backstepping",
        "authors": [
          "Yuxiang Zhang",
          "Xiaoling Liang",
          "Dongyu Li",
          "S. Ge",
          "B. Gao",
          "Hong Chen",
          "Tong-heng Lee"
        ],
        "published_date": "2022",
        "abstract": "Guaranteed safety and performance under various circumstances remain technically critical and practically challenging for the wide deployment of autonomous vehicles. Safety-critical systems in general, require safe performance even during the reinforcement learning (RL) period. To address this issue, a Barrier Lyapunov Function-based safe RL (BLF-SRL) algorithm is proposed here for the formulated nonlinear system in strict-feedback form. This approach appropriately arranges and incorporates the BLF items into the optimized backstepping control method to constrain the state-variables in the designed safety region during learning. Wherein, thus, the optimal virtual/actual control in every backstepping subsystem is decomposed with BLF items and also with an adaptive uncertain item to be learned, which achieves safe exploration during the learning process. Then, the principle of Bellman optimality of continuous-time HamiltonJacobiBellman equation in every backstepping subsystem is satisfied with independently approximated actor and critic under the framework of actor-critic through the designed iterative updating. Eventually, the overall system control is optimized with the proposed BLF-SRL method. It is furthermore noteworthy that the variance of the attained control performance under uncertainty is also reduced with the proposed method. The effectiveness of the proposed method is verified with two motion control problems for autonomous vehicles through appropriate comparison simulations.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/ba44a95f1a8bc5765438d03c01137799e930c88d.pdf",
        "venue": "IEEE Transactions on Neural Networks and Learning Systems",
        "citationCount": 51,
        "score": 17.0,
        "summary": "Guaranteed safety and performance under various circumstances remain technically critical and practically challenging for the wide deployment of autonomous vehicles. Safety-critical systems in general, require safe performance even during the reinforcement learning (RL) period. To address this issue, a Barrier Lyapunov Function-based safe RL (BLF-SRL) algorithm is proposed here for the formulated nonlinear system in strict-feedback form. This approach appropriately arranges and incorporates the BLF items into the optimized backstepping control method to constrain the state-variables in the designed safety region during learning. Wherein, thus, the optimal virtual/actual control in every backstepping subsystem is decomposed with BLF items and also with an adaptive uncertain item to be learned, which achieves safe exploration during the learning process. Then, the principle of Bellman optimality of continuous-time HamiltonJacobiBellman equation in every backstepping subsystem is satisfied with independently approximated actor and critic under the framework of actor-critic through the designed iterative updating. Eventually, the overall system control is optimized with the proposed BLF-SRL method. It is furthermore noteworthy that the variance of the attained control performance under uncertainty is also reduced with the proposed method. The effectiveness of the proposed method is verified with two motion control problems for autonomous vehicles through appropriate comparison simulations.",
        "keywords": []
      },
      "file_name": "ba44a95f1a8bc5765438d03c01137799e930c88d.pdf"
    },
    {
      "success": true,
      "doc_id": "14229ff0e1a5f49ed76486abe74381e7",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/d27edce419e1c731c0aeb9e1842d1e022b2cc6ab.pdf",
      "citation_key": "dorfman20216nq",
      "metadata": {
        "title": "Offline Meta Reinforcement Learning - Identifiability Challenges and Effective Data Collection Strategies",
        "authors": [
          "Ron Dorfman",
          "Idan Shenfeld",
          "Aviv Tamar"
        ],
        "published_date": "2021",
        "abstract": "",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/d27edce419e1c731c0aeb9e1842d1e022b2cc6ab.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 64,
        "score": 16.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "d27edce419e1c731c0aeb9e1842d1e022b2cc6ab.pdf"
    },
    {
      "success": true,
      "doc_id": "18aeb91df4a5ff3cf0e124d043444abc",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/116fc10b798e3294b00e92f2b8053d0c89ad9182.pdf",
      "citation_key": "tai2016bp8",
      "metadata": {
        "title": "A robot exploration strategy based on Q-learning network",
        "authors": [
          "L. Tai",
          "Ming Liu"
        ],
        "published_date": "2016",
        "abstract": "",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/116fc10b798e3294b00e92f2b8053d0c89ad9182.pdf",
        "venue": "International Conference on Real-time Computing and Robotics",
        "citationCount": 143,
        "score": 15.888888888888888,
        "summary": "",
        "keywords": []
      },
      "file_name": "116fc10b798e3294b00e92f2b8053d0c89ad9182.pdf"
    },
    {
      "success": true,
      "doc_id": "5745d271d30e2d0fde2d77a055bd169a",
      "summary": "We introduce a new count-based optimistic exploration algorithm for Reinforcement Learning (RL) that is feasible in environments with high-dimensional state-action spaces. The success of RL algorithms in these domains depends crucially on generalisation from limited training experience. Function approximation techniques enable RL agents to generalise in order to estimate the value of unvisited states, but at present few methods enable generalisation regarding uncertainty. This has prevented the combination of scalable RL algorithms with efficient exploration strategies that drive the agent to reduce its uncertainty. We present a new method for computing a generalised state visit-count, which allows the agent to estimate the uncertainty associated with any state. Our \\phi-pseudocount achieves generalisation by exploiting same feature representation of the state space that is used for value function approximation. States that have less frequently observed features are deemed more uncertain. The \\phi-Exploration-Bonus algorithm rewards the agent for exploring in feature space rather than in the untransformed state space. The method is simpler and less computationally expensive than some previous proposals, and achieves near state-of-the-art results on high-dimensional RL benchmarks.",
      "intriguing_abstract": "We introduce a new count-based optimistic exploration algorithm for Reinforcement Learning (RL) that is feasible in environments with high-dimensional state-action spaces. The success of RL algorithms in these domains depends crucially on generalisation from limited training experience. Function approximation techniques enable RL agents to generalise in order to estimate the value of unvisited states, but at present few methods enable generalisation regarding uncertainty. This has prevented the combination of scalable RL algorithms with efficient exploration strategies that drive the agent to reduce its uncertainty. We present a new method for computing a generalised state visit-count, which allows the agent to estimate the uncertainty associated with any state. Our \\phi-pseudocount achieves generalisation by exploiting same feature representation of the state space that is used for value function approximation. States that have less frequently observed features are deemed more uncertain. The \\phi-Exploration-Bonus algorithm rewards the agent for exploring in feature space rather than in the untransformed state space. The method is simpler and less computationally expensive than some previous proposals, and achieves near state-of-the-art results on high-dimensional RL benchmarks.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/0f810eb4777fd05317951ebaa7a3f5835ee84cf4.pdf",
      "citation_key": "martin2017bgt",
      "metadata": {
        "title": "Count-Based Exploration in Feature Space for Reinforcement Learning",
        "authors": [
          "Jarryd Martin",
          "S. N. Sasikumar",
          "Tom Everitt",
          "Marcus Hutter"
        ],
        "published_date": "2017",
        "abstract": "We introduce a new count-based optimistic exploration algorithm for Reinforcement Learning (RL) that is feasible in environments with high-dimensional state-action spaces. The success of RL algorithms in these domains depends crucially on generalisation from limited training experience. Function approximation techniques enable RL agents to generalise in order to estimate the value of unvisited states, but at present few methods enable generalisation regarding uncertainty. This has prevented the combination of scalable RL algorithms with efficient exploration strategies that drive the agent to reduce its uncertainty. We present a new method for computing a generalised state visit-count, which allows the agent to estimate the uncertainty associated with any state. Our \\phi-pseudocount achieves generalisation by exploiting same feature representation of the state space that is used for value function approximation. States that have less frequently observed features are deemed more uncertain. The \\phi-Exploration-Bonus algorithm rewards the agent for exploring in feature space rather than in the untransformed state space. The method is simpler and less computationally expensive than some previous proposals, and achieves near state-of-the-art results on high-dimensional RL benchmarks.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/0f810eb4777fd05317951ebaa7a3f5835ee84cf4.pdf",
        "venue": "International Joint Conference on Artificial Intelligence",
        "citationCount": 126,
        "score": 15.75,
        "summary": "We introduce a new count-based optimistic exploration algorithm for Reinforcement Learning (RL) that is feasible in environments with high-dimensional state-action spaces. The success of RL algorithms in these domains depends crucially on generalisation from limited training experience. Function approximation techniques enable RL agents to generalise in order to estimate the value of unvisited states, but at present few methods enable generalisation regarding uncertainty. This has prevented the combination of scalable RL algorithms with efficient exploration strategies that drive the agent to reduce its uncertainty. We present a new method for computing a generalised state visit-count, which allows the agent to estimate the uncertainty associated with any state. Our \\phi-pseudocount achieves generalisation by exploiting same feature representation of the state space that is used for value function approximation. States that have less frequently observed features are deemed more uncertain. The \\phi-Exploration-Bonus algorithm rewards the agent for exploring in feature space rather than in the untransformed state space. The method is simpler and less computationally expensive than some previous proposals, and achieves near state-of-the-art results on high-dimensional RL benchmarks.",
        "keywords": []
      },
      "file_name": "0f810eb4777fd05317951ebaa7a3f5835ee84cf4.pdf"
    },
    {
      "success": true,
      "doc_id": "35b6bba147ceea110a9550623def2276",
      "summary": "Aerial robots are increasingly being utilized for environmental monitoring and exploration. However, a key challenge is efficiently planning paths to maximize the information value of acquired data as an initially unknown environment is explored. To address this, we propose a new approach for informative path planning based on deep reinforcement learning (RL). Combining recent advances in RL and robotic applications, our method combines tree search with an offline-learned neural network predicting informative sensing actions. We introduce several components making our approach applicable for robotic tasks with high-dimensional state and large action spaces. By deploying the trained network during a mission, our method enables sample-efficient online replanning on platforms with limited computational resources. Simulations show that our approach performs on par with existing methods while reducing runtime by 8-10. We validate its performance using real-world surface temperature data.",
      "intriguing_abstract": "Aerial robots are increasingly being utilized for environmental monitoring and exploration. However, a key challenge is efficiently planning paths to maximize the information value of acquired data as an initially unknown environment is explored. To address this, we propose a new approach for informative path planning based on deep reinforcement learning (RL). Combining recent advances in RL and robotic applications, our method combines tree search with an offline-learned neural network predicting informative sensing actions. We introduce several components making our approach applicable for robotic tasks with high-dimensional state and large action spaces. By deploying the trained network during a mission, our method enables sample-efficient online replanning on platforms with limited computational resources. Simulations show that our approach performs on par with existing methods while reducing runtime by 8-10. We validate its performance using real-world surface temperature data.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/468a3e85a2da0908c6f371ad83f6bcf6b6fdf193.pdf",
      "citation_key": "rckin2021yud",
      "metadata": {
        "title": "Adaptive Informative Path Planning Using Deep Reinforcement Learning for UAV-based Active Sensing",
        "authors": [
          "Julius Rckin",
          "Liren Jin",
          "Marija Popovic"
        ],
        "published_date": "2021",
        "abstract": "Aerial robots are increasingly being utilized for environmental monitoring and exploration. However, a key challenge is efficiently planning paths to maximize the information value of acquired data as an initially unknown environment is explored. To address this, we propose a new approach for informative path planning based on deep reinforcement learning (RL). Combining recent advances in RL and robotic applications, our method combines tree search with an offline-learned neural network predicting informative sensing actions. We introduce several components making our approach applicable for robotic tasks with high-dimensional state and large action spaces. By deploying the trained network during a mission, our method enables sample-efficient online replanning on platforms with limited computational resources. Simulations show that our approach performs on par with existing methods while reducing runtime by 8-10. We validate its performance using real-world surface temperature data.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/468a3e85a2da0908c6f371ad83f6bcf6b6fdf193.pdf",
        "venue": "IEEE International Conference on Robotics and Automation",
        "citationCount": 63,
        "score": 15.75,
        "summary": "Aerial robots are increasingly being utilized for environmental monitoring and exploration. However, a key challenge is efficiently planning paths to maximize the information value of acquired data as an initially unknown environment is explored. To address this, we propose a new approach for informative path planning based on deep reinforcement learning (RL). Combining recent advances in RL and robotic applications, our method combines tree search with an offline-learned neural network predicting informative sensing actions. We introduce several components making our approach applicable for robotic tasks with high-dimensional state and large action spaces. By deploying the trained network during a mission, our method enables sample-efficient online replanning on platforms with limited computational resources. Simulations show that our approach performs on par with existing methods while reducing runtime by 8-10. We validate its performance using real-world surface temperature data.",
        "keywords": []
      },
      "file_name": "468a3e85a2da0908c6f371ad83f6bcf6b6fdf193.pdf"
    },
    {
      "success": true,
      "doc_id": "2fcddb9398c0f1f91d571ed78ce2571e",
      "summary": "This paper investigates exploration strategies of Deep Reinforcement Learning (DRL) methods to learn navigation policies for mobile robots. In particular, we augment the normal external reward for training DRL algorithms with intrinsic reward signals measured by curiosity. We test our approach in a mapless navigation setting, where the autonomous agent is required to navigate without the occupancy map of the environment, to targets whose relative locations can be easily acquired through low-cost solutions (e.g., visible light localization, Wi-Fi signal localization). We validate that the intrinsic motivation is crucial for improving DRL performance in tasks with challenging exploration requirements. Our experimental results show that our proposed method is able to more effectively learn navigation policies, and has better generalization capabilities in previously unseen environments. A video of our experimental results can be found at this https URL",
      "intriguing_abstract": "This paper investigates exploration strategies of Deep Reinforcement Learning (DRL) methods to learn navigation policies for mobile robots. In particular, we augment the normal external reward for training DRL algorithms with intrinsic reward signals measured by curiosity. We test our approach in a mapless navigation setting, where the autonomous agent is required to navigate without the occupancy map of the environment, to targets whose relative locations can be easily acquired through low-cost solutions (e.g., visible light localization, Wi-Fi signal localization). We validate that the intrinsic motivation is crucial for improving DRL performance in tasks with challenging exploration requirements. Our experimental results show that our proposed method is able to more effectively learn navigation policies, and has better generalization capabilities in previously unseen environments. A video of our experimental results can be found at this https URL",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/535d184eadf47fa17ce4073b6e2f180783e85300.pdf",
      "citation_key": "zhelo2018wi8",
      "metadata": {
        "title": "Curiosity-driven Exploration for Mapless Navigation with Deep Reinforcement Learning",
        "authors": [
          "Oleksii Zhelo",
          "Jingwei Zhang",
          "L. Tai",
          "Ming Liu",
          "Wolfram Burgard"
        ],
        "published_date": "2018",
        "abstract": "This paper investigates exploration strategies of Deep Reinforcement Learning (DRL) methods to learn navigation policies for mobile robots. In particular, we augment the normal external reward for training DRL algorithms with intrinsic reward signals measured by curiosity. We test our approach in a mapless navigation setting, where the autonomous agent is required to navigate without the occupancy map of the environment, to targets whose relative locations can be easily acquired through low-cost solutions (e.g., visible light localization, Wi-Fi signal localization). We validate that the intrinsic motivation is crucial for improving DRL performance in tasks with challenging exploration requirements. Our experimental results show that our proposed method is able to more effectively learn navigation policies, and has better generalization capabilities in previously unseen environments. A video of our experimental results can be found at this https URL",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/535d184eadf47fa17ce4073b6e2f180783e85300.pdf",
        "venue": "arXiv.org",
        "citationCount": 108,
        "score": 15.428571428571427,
        "summary": "This paper investigates exploration strategies of Deep Reinforcement Learning (DRL) methods to learn navigation policies for mobile robots. In particular, we augment the normal external reward for training DRL algorithms with intrinsic reward signals measured by curiosity. We test our approach in a mapless navigation setting, where the autonomous agent is required to navigate without the occupancy map of the environment, to targets whose relative locations can be easily acquired through low-cost solutions (e.g., visible light localization, Wi-Fi signal localization). We validate that the intrinsic motivation is crucial for improving DRL performance in tasks with challenging exploration requirements. Our experimental results show that our proposed method is able to more effectively learn navigation policies, and has better generalization capabilities in previously unseen environments. A video of our experimental results can be found at this https URL",
        "keywords": []
      },
      "file_name": "535d184eadf47fa17ce4073b6e2f180783e85300.pdf"
    },
    {
      "success": true,
      "doc_id": "83acf1ce8e348edc459d47f81aa23ab1",
      "summary": "Autonomous underwater vehicle (AUV) plays an increasingly important role in ocean exploration. Existing AUVs are usually not fully autonomous and generally limited to pre-planning or pre-programming tasks. Reinforcement learning (RL) and deep reinforcement learning have been introduced into the AUV design and research to improve its autonomy. However, these methods are still difficult to apply directly to the actual AUV system because of the sparse rewards and low learning efficiency. In this paper, we proposed a deep interactive reinforcement learning method for path following of AUV by combining the advantages of deep reinforcement learning and interactive RL. In addition, since the human trainer cannot provide human rewards for AUV when it is running in the ocean and AUV needs to adapt to a changing environment, we further propose a deep reinforcement learning method that learns from both human rewards and environmental rewards at the same time. We test our methods in two path following tasksstraight line and sinusoids curve following of AUV by simulating in the Gazebo platform. Our experimental results show that with our proposed deep interactive RL method, AUV can converge faster than a DQN learner from only environmental reward. Moreover, AUV learning with our deep RL from both human and environmental rewards can also achieve a similar or even better performance than that with deep interactive RL and can adapt to the actual environment by further learning from environmental rewards.",
      "intriguing_abstract": "Autonomous underwater vehicle (AUV) plays an increasingly important role in ocean exploration. Existing AUVs are usually not fully autonomous and generally limited to pre-planning or pre-programming tasks. Reinforcement learning (RL) and deep reinforcement learning have been introduced into the AUV design and research to improve its autonomy. However, these methods are still difficult to apply directly to the actual AUV system because of the sparse rewards and low learning efficiency. In this paper, we proposed a deep interactive reinforcement learning method for path following of AUV by combining the advantages of deep reinforcement learning and interactive RL. In addition, since the human trainer cannot provide human rewards for AUV when it is running in the ocean and AUV needs to adapt to a changing environment, we further propose a deep reinforcement learning method that learns from both human rewards and environmental rewards at the same time. We test our methods in two path following tasksstraight line and sinusoids curve following of AUV by simulating in the Gazebo platform. Our experimental results show that with our proposed deep interactive RL method, AUV can converge faster than a DQN learner from only environmental reward. Moreover, AUV learning with our deep RL from both human and environmental rewards can also achieve a similar or even better performance than that with deep interactive RL and can adapt to the actual environment by further learning from environmental rewards.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/0d82360a4da311a277607db355dda3f196e8eb3d.pdf",
      "citation_key": "zhang2020bse",
      "metadata": {
        "title": "Deep Interactive Reinforcement Learning for Path Following of Autonomous Underwater Vehicle",
        "authors": [
          "Qilei Zhang",
          "Jinying Lin",
          "Q. Sha",
          "Bo He",
          "Guangliang Li"
        ],
        "published_date": "2020",
        "abstract": "Autonomous underwater vehicle (AUV) plays an increasingly important role in ocean exploration. Existing AUVs are usually not fully autonomous and generally limited to pre-planning or pre-programming tasks. Reinforcement learning (RL) and deep reinforcement learning have been introduced into the AUV design and research to improve its autonomy. However, these methods are still difficult to apply directly to the actual AUV system because of the sparse rewards and low learning efficiency. In this paper, we proposed a deep interactive reinforcement learning method for path following of AUV by combining the advantages of deep reinforcement learning and interactive RL. In addition, since the human trainer cannot provide human rewards for AUV when it is running in the ocean and AUV needs to adapt to a changing environment, we further propose a deep reinforcement learning method that learns from both human rewards and environmental rewards at the same time. We test our methods in two path following tasksstraight line and sinusoids curve following of AUV by simulating in the Gazebo platform. Our experimental results show that with our proposed deep interactive RL method, AUV can converge faster than a DQN learner from only environmental reward. Moreover, AUV learning with our deep RL from both human and environmental rewards can also achieve a similar or even better performance than that with deep interactive RL and can adapt to the actual environment by further learning from environmental rewards.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/0d82360a4da311a277607db355dda3f196e8eb3d.pdf",
        "venue": "IEEE Access",
        "citationCount": 77,
        "score": 15.4,
        "summary": "Autonomous underwater vehicle (AUV) plays an increasingly important role in ocean exploration. Existing AUVs are usually not fully autonomous and generally limited to pre-planning or pre-programming tasks. Reinforcement learning (RL) and deep reinforcement learning have been introduced into the AUV design and research to improve its autonomy. However, these methods are still difficult to apply directly to the actual AUV system because of the sparse rewards and low learning efficiency. In this paper, we proposed a deep interactive reinforcement learning method for path following of AUV by combining the advantages of deep reinforcement learning and interactive RL. In addition, since the human trainer cannot provide human rewards for AUV when it is running in the ocean and AUV needs to adapt to a changing environment, we further propose a deep reinforcement learning method that learns from both human rewards and environmental rewards at the same time. We test our methods in two path following tasksstraight line and sinusoids curve following of AUV by simulating in the Gazebo platform. Our experimental results show that with our proposed deep interactive RL method, AUV can converge faster than a DQN learner from only environmental reward. Moreover, AUV learning with our deep RL from both human and environmental rewards can also achieve a similar or even better performance than that with deep interactive RL and can adapt to the actual environment by further learning from environmental rewards.",
        "keywords": []
      },
      "file_name": "0d82360a4da311a277607db355dda3f196e8eb3d.pdf"
    },
    {
      "success": true,
      "doc_id": "a55279c087c2c66a04b0ef75f7edcd89",
      "summary": "In distributional reinforcement learning (RL), the estimated distribution of value function models both the parametric and intrinsic uncertainties. We propose a novel and efficient exploration method for deep RL that has two components. The first is a decaying schedule to suppress the intrinsic uncertainty. The second is an exploration bonus calculated from the upper quantiles of the learned distribution. In Atari 2600 games, our method outperforms QR-DQN in 12 out of 14 hard games (achieving 483 \\% average gain across 49 games in cumulative rewards over QR-DQN with a big win in Venture). We also compared our algorithm with QR-DQN in a challenging 3D driving simulator (CARLA). Results show that our algorithm achieves near-optimal safety rewards twice faster than QRDQN.",
      "intriguing_abstract": "In distributional reinforcement learning (RL), the estimated distribution of value function models both the parametric and intrinsic uncertainties. We propose a novel and efficient exploration method for deep RL that has two components. The first is a decaying schedule to suppress the intrinsic uncertainty. The second is an exploration bonus calculated from the upper quantiles of the learned distribution. In Atari 2600 games, our method outperforms QR-DQN in 12 out of 14 hard games (achieving 483 \\% average gain across 49 games in cumulative rewards over QR-DQN with a big win in Venture). We also compared our algorithm with QR-DQN in a challenging 3D driving simulator (CARLA). Results show that our algorithm achieves near-optimal safety rewards twice faster than QRDQN.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/f6b218453170edcbb51e49dd44ba2f83af53ef92.pdf",
      "citation_key": "mavrin2019iqm",
      "metadata": {
        "title": "Distributional Reinforcement Learning for Efficient Exploration",
        "authors": [
          "B. Mavrin",
          "Shangtong Zhang",
          "Hengshuai Yao",
          "Linglong Kong",
          "Kaiwen Wu",
          "Yaoliang Yu"
        ],
        "published_date": "2019",
        "abstract": "In distributional reinforcement learning (RL), the estimated distribution of value function models both the parametric and intrinsic uncertainties. We propose a novel and efficient exploration method for deep RL that has two components. The first is a decaying schedule to suppress the intrinsic uncertainty. The second is an exploration bonus calculated from the upper quantiles of the learned distribution. In Atari 2600 games, our method outperforms QR-DQN in 12 out of 14 hard games (achieving 483 \\% average gain across 49 games in cumulative rewards over QR-DQN with a big win in Venture). We also compared our algorithm with QR-DQN in a challenging 3D driving simulator (CARLA). Results show that our algorithm achieves near-optimal safety rewards twice faster than QRDQN.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/f6b218453170edcbb51e49dd44ba2f83af53ef92.pdf",
        "venue": "International Conference on Machine Learning",
        "citationCount": 92,
        "score": 15.333333333333332,
        "summary": "In distributional reinforcement learning (RL), the estimated distribution of value function models both the parametric and intrinsic uncertainties. We propose a novel and efficient exploration method for deep RL that has two components. The first is a decaying schedule to suppress the intrinsic uncertainty. The second is an exploration bonus calculated from the upper quantiles of the learned distribution. In Atari 2600 games, our method outperforms QR-DQN in 12 out of 14 hard games (achieving 483 \\% average gain across 49 games in cumulative rewards over QR-DQN with a big win in Venture). We also compared our algorithm with QR-DQN in a challenging 3D driving simulator (CARLA). Results show that our algorithm achieves near-optimal safety rewards twice faster than QRDQN.",
        "keywords": []
      },
      "file_name": "f6b218453170edcbb51e49dd44ba2f83af53ef92.pdf"
    },
    {
      "success": true,
      "doc_id": "6d91f7526420aae328164a2fa4d4ca05",
      "summary": "\n Achieving sample efficiency in online episodic reinforcement learning (RL) requires optimally balancing exploration and exploitation. When it comes to a finite-horizon episodic Markov decision process with $S$ states, $A$ actions and horizon length $H$, substantial progress has been achieved toward characterizing the minimax-optimal regret, which scales on the order of $\\sqrt{H^2SAT}$ (modulo log factors) with $T$ the total number of samples. While several competing solution paradigms have been proposed to minimize regret, they are either memory-inefficient, or fall short of optimality unless the sample size exceeds an enormous threshold (e.g. $S^6A^4 \\,\\mathrm{poly}(H)$ for existing model-free methods).\n To overcome such a large sample size barrier to efficient RL, we design a novel model-free algorithm, with space complexity $O(SAH)$, that achieves near-optimal regret as soon as the sample size exceeds the order of $SA\\,\\mathrm{poly}(H)$. In terms of this sample size requirement (also referred to the initial burn-in cost), our method improvesby at least a factor of $S^5A^3$upon any prior memory-efficient algorithm that is asymptotically regret-optimal. Leveraging the recently introduced variance reduction strategy (also called reference-advantage decomposition), the proposed algorithm employs an early-settled reference update rule, with the aid of two Q-learning sequences with upper and lower confidence bounds. The design principle of our early-settled variance reduction method might be of independent interest to other RL settings that involve intricate explorationexploitation trade-offs.",
      "intriguing_abstract": "\n Achieving sample efficiency in online episodic reinforcement learning (RL) requires optimally balancing exploration and exploitation. When it comes to a finite-horizon episodic Markov decision process with $S$ states, $A$ actions and horizon length $H$, substantial progress has been achieved toward characterizing the minimax-optimal regret, which scales on the order of $\\sqrt{H^2SAT}$ (modulo log factors) with $T$ the total number of samples. While several competing solution paradigms have been proposed to minimize regret, they are either memory-inefficient, or fall short of optimality unless the sample size exceeds an enormous threshold (e.g. $S^6A^4 \\,\\mathrm{poly}(H)$ for existing model-free methods).\n To overcome such a large sample size barrier to efficient RL, we design a novel model-free algorithm, with space complexity $O(SAH)$, that achieves near-optimal regret as soon as the sample size exceeds the order of $SA\\,\\mathrm{poly}(H)$. In terms of this sample size requirement (also referred to the initial burn-in cost), our method improvesby at least a factor of $S^5A^3$upon any prior memory-efficient algorithm that is asymptotically regret-optimal. Leveraging the recently introduced variance reduction strategy (also called reference-advantage decomposition), the proposed algorithm employs an early-settled reference update rule, with the aid of two Q-learning sequences with upper and lower confidence bounds. The design principle of our early-settled variance reduction method might be of independent interest to other RL settings that involve intricate explorationexploitation trade-offs.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/1f4484086d210a2c44efe5eef0a2b42647822abf.pdf",
      "citation_key": "li2021w3q",
      "metadata": {
        "title": "Breaking the Sample Complexity Barrier to Regret-Optimal Model-Free Reinforcement Learning",
        "authors": [
          "Gen Li",
          "Laixi Shi",
          "Yuxin Chen",
          "Yuantao Gu",
          "Yuejie Chi"
        ],
        "published_date": "2021",
        "abstract": "\n Achieving sample efficiency in online episodic reinforcement learning (RL) requires optimally balancing exploration and exploitation. When it comes to a finite-horizon episodic Markov decision process with $S$ states, $A$ actions and horizon length $H$, substantial progress has been achieved toward characterizing the minimax-optimal regret, which scales on the order of $\\sqrt{H^2SAT}$ (modulo log factors) with $T$ the total number of samples. While several competing solution paradigms have been proposed to minimize regret, they are either memory-inefficient, or fall short of optimality unless the sample size exceeds an enormous threshold (e.g. $S^6A^4 \\,\\mathrm{poly}(H)$ for existing model-free methods).\n To overcome such a large sample size barrier to efficient RL, we design a novel model-free algorithm, with space complexity $O(SAH)$, that achieves near-optimal regret as soon as the sample size exceeds the order of $SA\\,\\mathrm{poly}(H)$. In terms of this sample size requirement (also referred to the initial burn-in cost), our method improvesby at least a factor of $S^5A^3$upon any prior memory-efficient algorithm that is asymptotically regret-optimal. Leveraging the recently introduced variance reduction strategy (also called reference-advantage decomposition), the proposed algorithm employs an early-settled reference update rule, with the aid of two Q-learning sequences with upper and lower confidence bounds. The design principle of our early-settled variance reduction method might be of independent interest to other RL settings that involve intricate explorationexploitation trade-offs.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/1f4484086d210a2c44efe5eef0a2b42647822abf.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 59,
        "score": 14.75,
        "summary": "\n Achieving sample efficiency in online episodic reinforcement learning (RL) requires optimally balancing exploration and exploitation. When it comes to a finite-horizon episodic Markov decision process with $S$ states, $A$ actions and horizon length $H$, substantial progress has been achieved toward characterizing the minimax-optimal regret, which scales on the order of $\\sqrt{H^2SAT}$ (modulo log factors) with $T$ the total number of samples. While several competing solution paradigms have been proposed to minimize regret, they are either memory-inefficient, or fall short of optimality unless the sample size exceeds an enormous threshold (e.g. $S^6A^4 \\,\\mathrm{poly}(H)$ for existing model-free methods).\n To overcome such a large sample size barrier to efficient RL, we design a novel model-free algorithm, with space complexity $O(SAH)$, that achieves near-optimal regret as soon as the sample size exceeds the order of $SA\\,\\mathrm{poly}(H)$. In terms of this sample size requirement (also referred to the initial burn-in cost), our method improvesby at least a factor of $S^5A^3$upon any prior memory-efficient algorithm that is asymptotically regret-optimal. Leveraging the recently introduced variance reduction strategy (also called reference-advantage decomposition), the proposed algorithm employs an early-settled reference update rule, with the aid of two Q-learning sequences with upper and lower confidence bounds. The design principle of our early-settled variance reduction method might be of independent interest to other RL settings that involve intricate explorationexploitation trade-offs.",
        "keywords": []
      },
      "file_name": "1f4484086d210a2c44efe5eef0a2b42647822abf.pdf"
    },
    {
      "success": true,
      "doc_id": "9f258b45970ddef9a663cb4483bfea64",
      "summary": "Muscle-actuated organisms are capable of learning an unparalleled diversity of dexterous movements despite their vast amount of muscles. Reinforcement learning (RL) on large musculoskeletal models, however, has not been able to show similar performance. We conjecture that ineffective exploration in large overactuated action spaces is a key problem. This is supported by the finding that common exploration noise strategies are inadequate in synthetic examples of overactuated systems. We identify differential extrinsic plasticity (DEP), a method from the domain of self-organization, as being able to induce state-space covering exploration within seconds of interaction. By integrating DEP into RL, we achieve fast learning of reaching and locomotion in musculoskeletal systems, outperforming current approaches in all considered tasks in sample efficiency and robustness.",
      "intriguing_abstract": "Muscle-actuated organisms are capable of learning an unparalleled diversity of dexterous movements despite their vast amount of muscles. Reinforcement learning (RL) on large musculoskeletal models, however, has not been able to show similar performance. We conjecture that ineffective exploration in large overactuated action spaces is a key problem. This is supported by the finding that common exploration noise strategies are inadequate in synthetic examples of overactuated systems. We identify differential extrinsic plasticity (DEP), a method from the domain of self-organization, as being able to induce state-space covering exploration within seconds of interaction. By integrating DEP into RL, we achieve fast learning of reaching and locomotion in musculoskeletal systems, outperforming current approaches in all considered tasks in sample efficiency and robustness.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/04615a9955bce148aa7ba29e864389c26e10523a.pdf",
      "citation_key": "schumacher2022x3f",
      "metadata": {
        "title": "DEP-RL: Embodied Exploration for Reinforcement Learning in Overactuated and Musculoskeletal Systems",
        "authors": [
          "Pierre Schumacher",
          "D. Haeufle",
          "Dieter Bchler",
          "S. Schmitt",
          "G. Martius"
        ],
        "published_date": "2022",
        "abstract": "Muscle-actuated organisms are capable of learning an unparalleled diversity of dexterous movements despite their vast amount of muscles. Reinforcement learning (RL) on large musculoskeletal models, however, has not been able to show similar performance. We conjecture that ineffective exploration in large overactuated action spaces is a key problem. This is supported by the finding that common exploration noise strategies are inadequate in synthetic examples of overactuated systems. We identify differential extrinsic plasticity (DEP), a method from the domain of self-organization, as being able to induce state-space covering exploration within seconds of interaction. By integrating DEP into RL, we achieve fast learning of reaching and locomotion in musculoskeletal systems, outperforming current approaches in all considered tasks in sample efficiency and robustness.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/04615a9955bce148aa7ba29e864389c26e10523a.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 44,
        "score": 14.666666666666666,
        "summary": "Muscle-actuated organisms are capable of learning an unparalleled diversity of dexterous movements despite their vast amount of muscles. Reinforcement learning (RL) on large musculoskeletal models, however, has not been able to show similar performance. We conjecture that ineffective exploration in large overactuated action spaces is a key problem. This is supported by the finding that common exploration noise strategies are inadequate in synthetic examples of overactuated systems. We identify differential extrinsic plasticity (DEP), a method from the domain of self-organization, as being able to induce state-space covering exploration within seconds of interaction. By integrating DEP into RL, we achieve fast learning of reaching and locomotion in musculoskeletal systems, outperforming current approaches in all considered tasks in sample efficiency and robustness.",
        "keywords": []
      },
      "file_name": "04615a9955bce148aa7ba29e864389c26e10523a.pdf"
    },
    {
      "success": true,
      "doc_id": "55e0c600089bbbd88a89929956098c7d",
      "summary": "The reinforcement learning (RL) research area is very active, with an important number of new contributions, especially considering the emergent field of deep RL (DRL). However, a number of scientific and technical challenges still need to be resolved, among which we acknowledge the ability to abstract actions or the difficulty to explore the environment in sparse-reward settings which can be addressed by intrinsic motivation (IM). We propose to survey these research works through a new taxonomy based on information theory: we computationally revisit the notions of surprise, novelty, and skill-learning. This allows us to identify advantages and disadvantages of methods and exhibit current outlooks of research. Our analysis suggests that novelty and surprise can assist the building of a hierarchy of transferable skills which abstracts dynamics and makes the exploration process more robust.",
      "intriguing_abstract": "The reinforcement learning (RL) research area is very active, with an important number of new contributions, especially considering the emergent field of deep RL (DRL). However, a number of scientific and technical challenges still need to be resolved, among which we acknowledge the ability to abstract actions or the difficulty to explore the environment in sparse-reward settings which can be addressed by intrinsic motivation (IM). We propose to survey these research works through a new taxonomy based on information theory: we computationally revisit the notions of surprise, novelty, and skill-learning. This allows us to identify advantages and disadvantages of methods and exhibit current outlooks of research. Our analysis suggests that novelty and surprise can assist the building of a hierarchy of transferable skills which abstracts dynamics and makes the exploration process more robust.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/c64a99ec780d5e1c6ba242a82e72e96eb0c3ef8a.pdf",
      "citation_key": "aubret2022inh",
      "metadata": {
        "title": "An Information-Theoretic Perspective on Intrinsic Motivation in Reinforcement Learning: A Survey",
        "authors": [
          "A. Aubret",
          "L. Matignon",
          "S. Hassas"
        ],
        "published_date": "2022",
        "abstract": "The reinforcement learning (RL) research area is very active, with an important number of new contributions, especially considering the emergent field of deep RL (DRL). However, a number of scientific and technical challenges still need to be resolved, among which we acknowledge the ability to abstract actions or the difficulty to explore the environment in sparse-reward settings which can be addressed by intrinsic motivation (IM). We propose to survey these research works through a new taxonomy based on information theory: we computationally revisit the notions of surprise, novelty, and skill-learning. This allows us to identify advantages and disadvantages of methods and exhibit current outlooks of research. Our analysis suggests that novelty and surprise can assist the building of a hierarchy of transferable skills which abstracts dynamics and makes the exploration process more robust.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/c64a99ec780d5e1c6ba242a82e72e96eb0c3ef8a.pdf",
        "venue": "Entropy",
        "citationCount": 44,
        "score": 14.666666666666666,
        "summary": "The reinforcement learning (RL) research area is very active, with an important number of new contributions, especially considering the emergent field of deep RL (DRL). However, a number of scientific and technical challenges still need to be resolved, among which we acknowledge the ability to abstract actions or the difficulty to explore the environment in sparse-reward settings which can be addressed by intrinsic motivation (IM). We propose to survey these research works through a new taxonomy based on information theory: we computationally revisit the notions of surprise, novelty, and skill-learning. This allows us to identify advantages and disadvantages of methods and exhibit current outlooks of research. Our analysis suggests that novelty and surprise can assist the building of a hierarchy of transferable skills which abstracts dynamics and makes the exploration process more robust.",
        "keywords": []
      },
      "file_name": "c64a99ec780d5e1c6ba242a82e72e96eb0c3ef8a.pdf"
    },
    {
      "success": true,
      "doc_id": "66b7929b81d1619d2b69f97e56ba2697",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/7d05987db045c56fa691da40e679cd328f0b68ef.pdf",
      "citation_key": "yuan2020epo",
      "metadata": {
        "title": "Study on the application of reinforcement learning in the operation optimization of HVAC system",
        "authors": [
          "Xiaolei Yuan",
          "Yiqun Pan",
          "Jianrong Yang",
          "Weitong Wang",
          "Zhizhong Huang"
        ],
        "published_date": "2020",
        "abstract": "",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/7d05987db045c56fa691da40e679cd328f0b68ef.pdf",
        "venue": "Building Simulation",
        "citationCount": 72,
        "score": 14.4,
        "summary": "",
        "keywords": []
      },
      "file_name": "7d05987db045c56fa691da40e679cd328f0b68ef.pdf"
    },
    {
      "success": true,
      "doc_id": "7fb4771a2db5adbbe5fe3dcd99ca1de9",
      "summary": "Offline Reinforcement Learning (RL) aims at learning an optimal control from a fixed dataset, without interactions with the system. An agent in this setting should avoid selecting actions whose consequences cannot be predicted from the data. This is the converse of exploration in RL, which favors such actions. We thus take inspiration from the literature on bonus-based exploration to design a new offline RL agent. The core idea is to subtract a prediction-based exploration bonus from the reward, instead of adding it for exploration. This allows the policy to stay close to the support of the dataset and practically extends some previous pessimism-based offline RL methods to a deep learning setting with arbitrary bonuses. We also connect this approach to a more common regularization of the learned policy towards the data. Instantiated with a bonus based on the prediction error of a variational autoencoder, we show that our simple agent is competitive with the state of the art on a set of continuous control locomotion and manipulation tasks.",
      "intriguing_abstract": "Offline Reinforcement Learning (RL) aims at learning an optimal control from a fixed dataset, without interactions with the system. An agent in this setting should avoid selecting actions whose consequences cannot be predicted from the data. This is the converse of exploration in RL, which favors such actions. We thus take inspiration from the literature on bonus-based exploration to design a new offline RL agent. The core idea is to subtract a prediction-based exploration bonus from the reward, instead of adding it for exploration. This allows the policy to stay close to the support of the dataset and practically extends some previous pessimism-based offline RL methods to a deep learning setting with arbitrary bonuses. We also connect this approach to a more common regularization of the learned policy towards the data. Instantiated with a bonus based on the prediction error of a variational autoencoder, we show that our simple agent is competitive with the state of the art on a set of continuous control locomotion and manipulation tasks.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/399806e861a2ef960a81b37b593c2176a728c399.pdf",
      "citation_key": "rezaeifar20211eu",
      "metadata": {
        "title": "Offline Reinforcement Learning as Anti-Exploration",
        "authors": [
          "Shideh Rezaeifar",
          "Robert Dadashi",
          "Nino Vieillard",
          "L'eonard Hussenot",
          "Olivier Bachem",
          "O. Pietquin",
          "M. Geist"
        ],
        "published_date": "2021",
        "abstract": "Offline Reinforcement Learning (RL) aims at learning an optimal control from a fixed dataset, without interactions with the system. An agent in this setting should avoid selecting actions whose consequences cannot be predicted from the data. This is the converse of exploration in RL, which favors such actions. We thus take inspiration from the literature on bonus-based exploration to design a new offline RL agent. The core idea is to subtract a prediction-based exploration bonus from the reward, instead of adding it for exploration. This allows the policy to stay close to the support of the dataset and practically extends some previous pessimism-based offline RL methods to a deep learning setting with arbitrary bonuses. We also connect this approach to a more common regularization of the learned policy towards the data. Instantiated with a bonus based on the prediction error of a variational autoencoder, we show that our simple agent is competitive with the state of the art on a set of continuous control locomotion and manipulation tasks.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/399806e861a2ef960a81b37b593c2176a728c399.pdf",
        "venue": "AAAI Conference on Artificial Intelligence",
        "citationCount": 57,
        "score": 14.25,
        "summary": "Offline Reinforcement Learning (RL) aims at learning an optimal control from a fixed dataset, without interactions with the system. An agent in this setting should avoid selecting actions whose consequences cannot be predicted from the data. This is the converse of exploration in RL, which favors such actions. We thus take inspiration from the literature on bonus-based exploration to design a new offline RL agent. The core idea is to subtract a prediction-based exploration bonus from the reward, instead of adding it for exploration. This allows the policy to stay close to the support of the dataset and practically extends some previous pessimism-based offline RL methods to a deep learning setting with arbitrary bonuses. We also connect this approach to a more common regularization of the learned policy towards the data. Instantiated with a bonus based on the prediction error of a variational autoencoder, we show that our simple agent is competitive with the state of the art on a set of continuous control locomotion and manipulation tasks.",
        "keywords": []
      },
      "file_name": "399806e861a2ef960a81b37b593c2176a728c399.pdf"
    },
    {
      "success": true,
      "doc_id": "0a50c7a4de6e03fda573806f8b7f8657",
      "summary": "Here's a focused summary of the paper for a literature review:\n\n**1. Research Problem & Motivation**\n*   **Problem**: The paper addresses the critical technical problem of achieving effective and real-time resource allocation in Fog computing environments, specifically tailored for latency-sensitive healthcare applications. Existing rule-based treatment recommendation systems in healthcare are often rigid, fail to account for patient-specific comorbidities, and lack robust evidence from randomized controlled trials for many ICU conditions.\n*   **Importance & Challenge**:\n    *   **Healthcare Context**: Critically ill patients require personalized, dynamic treatment plans, which current static guidelines cannot adequately provide. The complex, high-dimensional, and time-sensitive nature of ICU data necessitates advanced decision-making tools.\n    *   **Fog Computing**: Efficient resource management in Fog environments is crucial for low-latency processing of healthcare data, directly impacting Quality of Service (QoS) metrics such as allocation cost, response time, bandwidth efficiency, and energy consumption.\n    *   **Reinforcement Learning (RL)**: While RL offers significant potential for sequential decision-making in healthcare, challenges persist in its architectural design, choice of measurement metrics, model selection, and robust validation in authentic clinical settings.\n    *   **Limitations of Previous RA**: Many existing resource allocation (RA) algorithms are limited by unrealistic dependencies on response time, neglect of task priorities or varying packet sizes, and potential for network bottlenecks.\n\n**2. Related Work & Positioning**\n*   **Relation to Existing Approaches**:\n    *   The work builds upon established Reinforcement Learning (RL) models such as Q-Learning, Fitted-Q-Iteration (FQI), and Deep Q Network (DQN), acknowledging their foundational role while aiming to overcome their inherent limitations.\n    *   It positions itself within the domain of hyperparameter optimization for Deep Reinforcement Learning (DRL), referencing techniques like Population-Based Bandits (PB2), online hyperparameter adaptation (PBT-style), model-based optimization, and Bayesian optimization.\n*   **Limitations of Previous Solutions**:\n    *   **RL Models**: Traditional Q-learning requires extensive exploration and careful learning rate management. DQN faces challenges with sample correlation and non-stationary targets, which can hinder convergence.\n    *   **Hyperparameter Optimization**: Prior hyperparameter optimization techniques (e.g., PBT) often adapt only a single hyperparameter or rely on rudimentary stochastic perturbations, making them inefficient for tracking dynamic optimal configurations or adapting multiple hyperparameters simultaneously.\n    *   **Resource Allocation Algorithms**: Existing RA algorithms frequently:\n        *   Unrealistically depend on response time for task assignment between fog and cloud.\n        *   Fail to consider crucial task requirements like priority and the number of tasks.\n        *   Struggle with accurate capacity calculation, especially with varying packet sizes.\n        *   Are prone to causing network bottlenecks, degrading overall system performance.\n\n**3. Technical Approach & Innovation**\n*   **Core Technical Method**: The paper introduces an **Effective Resource Allocation Strategy (ERAS)** designed for Fog environments, specifically for real-time healthcare applications. ERAS aims to achieve efficient resource management through real-time resource allocation and predictive algorithms.\n*   **Architecture**: ERAS comprises two main modules:\n    *   **Data Preparation Algorithm (DPA)**: Manages the initial processing of incoming patient data. It includes:\n        *   **Sampling Module (SM)**: Uses stratified sampling to categorize data based on location and type (critical, normal, non-critical).\n        *   **Partitioning Module (PM)**: Divides the prepared data into training, testing, and validation sets.\n        *   **Balancing Module (BM)**: Addresses data imbalance using techniques like SMOTE to ensure robust model training.\n    *   **Resource Management Algorithm (RMA)**: This module is responsible for the core resource allocation and load balancing, incorporating an **Optimized Reinforcement Learning Algorithm (ORA)** and a **Resource Allocation Algorithm (RAA)**.\n*   **Innovation**:\n    *   **Optimized Reinforcement Learning**: The primary innovation is the optimization of Deep Q-Network (DQN) hyperparameters using **Particle Swarm Optimization (PSO)**. This novel integration fine-tunes the RL agent for superior performance in resource allocation within a healthcare Fog environment.\n    *   **PSO's Role**: PSO is selected for its global optimization capabilities, its inherent resistance to local minima, and its effective balance of exploitation and exploration, which are critical for identifying optimal RL hyperparameters.\n    *   **Real-time Suitability**: ERAS is engineered for real-time systems in Fog computing, aiming to achieve low latency and enhanced QoS through dynamic load balancing.\n    *   **Integrated Approach**: The strategy offers a comprehensive solution by integrating robust data preparation with an optimized RL-based resource management system.\n\n**4. Key Technical Contributions**\n*   **Novel Algorithms/Methods**:\n    *   Introduction of the **Effective Resource Allocation Strategy (ERAS)**, a holistic framework for resource management in healthcare Fog environments.\n    *   Development of an **Optimized Reinforcement Learning Algorithm (ORA)** that specifically leverages Particle Swarm Optimization (PSO) for hyperparameter tuning of Deep Q-Networks (DQN) in resource allocation.\n    *   A structured **Data Preparation Algorithm (DPA)**, which includes stratified sampling, data partitioning, and imbalance handling (SMOTE), tailored for healthcare data in a Fog context.\n*   **System Design/Architectural Innovations**:\n    *   A two-layer Fog architecture (DPA and RMA) designed for efficient real-time processing and decision-making in critical care scenarios.\n    *   The novel integration of PSO for hyperparameter optimization directly within the RL-based resource allocation pipeline, significantly enhancing the robustness and performance of the RL agent.\n*   **Theoretical Insights/Analysis**:\n    *   The paper implicitly demonstrates the practical advantages of employing a global optimization technique like PSO to overcome the challenges of hyperparameter tuning in complex DRL models, particularly for sequential decision-making problems such as resource allocation.\n    *   It highlights the critical significance of PSO's exploitation, exploration, and local minima avoidance capabilities in achieving superior RL performance compared to other optimization methodologies.\n\n**5. Experimental Validation**\n*   **Experiments Conducted**: The paper conducted experiments comparing ERAS against \"state-of-the-art algorithms\" for resource allocation.\n*   **Key Performance Metrics**:\n    *   **Makespan**: ERAS achieved the *minimum* Makespan, indicating faster task completion.\n    *   **Average Resource Utilization (ARU)**: ERAS *maximized* ARU, demonstrating efficient use of resources.\n    *   **Load Balancing Level (LBL)**: ERAS *maximized* LBL, indicating effective distribution of workload across resources.\n*   **Comparison Results**: ERAS consistently demonstrated superior performance by achieving the lowest Makespan and the highest ARU and LBL when compared to previous resource allocation algorithms, validating its effectiveness.\n\n**6. Limitations & Scope**\n*   **Technical Limitations/Assumptions**:\n    *   The provided content does not fully detail the specific \"state-of-the-art algorithms\" used for comparison, the exact experimental setup, or the precise nature of the healthcare applications (e.g., specific critical care scenarios).\n    *   The explicit details of the RL agent's state representation, action space, and reward function within the ORA for resource allocation are not elaborated in the provided text.\n*   **Scope of Applicability**:\n    *   The strategy is primarily focused on **Fog computing environments** and **healthcare applications**, particularly in critical care settings.\n    *   It is designed for **real-time systems** that demand low latency and high Quality of Service (QoS).\n    *   The proposed approach is generalizable to other resource allocation and load balancing problems requiring dynamic, sequential decision-making where robust hyperparameter optimization is crucial.\n\n**7. Technical Significance**\n*   **Advancement of State-of-the-Art**:\n    *   Advances the application of Reinforcement Learning in resource allocation by effectively addressing the critical challenge of hyperparameter optimization through Particle Swarm Optimization, leading to more robust and efficient RL agents.\n    *   Provides a comprehensive framework (ERAS) that integrates sophisticated data preparation with an optimized RL approach for real-time resource management in Fog environments, marking a significant step towards practical deployment in latency-sensitive domains like healthcare.\n    *   Demonstrates a tangible improvement in efficiency and effectiveness by achieving superior performance in key resource management metrics (Makespan, ARU, LBL) compared to existing approaches.\n*   **Potential Impact on Future Research**:\n    *   Opens new avenues for research into combining meta-heuristic optimization techniques (like PSO) with DRL for fine-tuning complex models in various real-time, dynamic environments beyond healthcare.\n    *   Encourages the development of more adaptive and robust resource allocation strategies in edge/fog computing by providing a validated methodology for optimizing underlying learning algorithms.\n    *   Could inspire the creation of more personalized and adaptive decision support systems in critical care, moving beyond static protocols towards AI-driven, patient-specific interventions.\n    *   Highlights the importance of comprehensive data preparation (sampling, partitioning, balancing) as an integral component for developing effective ML/RL solutions for real-world applications.",
      "intriguing_abstract": "The imperative for real-time, personalized healthcare in critical care settings demands ultra-low latency resource management within Fog computing environments. Current static guidelines and rigid resource allocation strategies often fail to address patient-specific comorbidities and the dynamic, high-dimensional nature of ICU data. This paper introduces the **Effective Resource Allocation Strategy (ERAS)**, a novel framework engineered for efficient, real-time resource allocation in latency-sensitive healthcare applications.\n\nERAS's core innovation lies in its **Optimized Reinforcement Learning Algorithm (ORA)**, which uniquely integrates **Particle Swarm Optimization (PSO)** for dynamic hyperparameter tuning of **Deep Q-Networks (DQN)**. This pioneering fusion addresses critical DRL challenges, such as sample correlation and non-stationary targets, ensuring robust and adaptive decision-making. Coupled with a sophisticated Data Preparation Algorithm (DPA), ERAS achieves superior **Quality of Service (QoS)**. Experimental validation demonstrates ERAS's unprecedented efficiency, achieving minimum **Makespan** and maximum **Average Resource Utilization (ARU)** and **Load Balancing Level (LBL)** compared to existing state-of-the-art methods. This work significantly advances resource management in Fog computing, paving the way for truly personalized and dynamic AI-driven interventions in critical care.",
      "keywords": [
        "Fog computing",
        "real-time resource allocation",
        "latency-sensitive healthcare applications",
        "Reinforcement Learning (RL)",
        "Deep Q-Network (DQN)",
        "hyperparameter optimization",
        "Particle Swarm Optimization (PSO)",
        "Effective Resource Allocation Strategy (ERAS)",
        "Optimized Reinforcement Learning Algorithm (ORA)",
        "Data Preparation Algorithm (DPA)",
        "load balancing",
        "minimum Makespan",
        "maximized Average Resource Utilization",
        "maximized Load Balancing Level"
      ],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/174be0bacee04d9eb13a698d484ab5ae441c1100.pdf",
      "citation_key": "talaat2022ywa",
      "metadata": {
        "title": "Effective deep Q-networks (EDQN) strategy for resource allocation based on optimized reinforcement learning algorithm",
        "authors": [
          "Fatma M. Talaat"
        ],
        "published_date": "2022",
        "abstract": "The healthcare industry has always been an early adopter of new technology and a big benefactor of it. The use of reinforcement learning in the healthcare system has repeatedly resulted in improved outcomes.. Many challenges exist concerning the architecture of the RL method, measurement metrics, and model choice. More significantly, the validation of RL in authentic clinical settings needs further work. This paper presents a new Effective Resource Allocation Strategy (ERAS) for the Fog environment, which is suitable for Healthcare applications. ERAS tries to achieve effective resource management in the Fog environment via real-time resource allocating as well as prediction algorithms. Comparing the ERAS with the state-of-the-art algorithms, ERAS achieved the minimum Makespan as compared to previous resource allocation algorithms, while maximizing the Average Resource Utilization (ARU) and the Load Balancing Level (LBL). For each application, we further compared and contrasted the architecture of the RL models and the assessment metrics. In critical care, RL has tremendous potential to enhance decision-making. This paper presents two main contributions, (i) Optimization of the RL hyperparameters using PSO, and (ii) Using the optimized RL for the resource allocation and load balancing in the fog environment. Because of its exploitation, exploration, and capacity to get rid of local minima, the PSO has a significant significance when compared to other optimization methodologies.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/174be0bacee04d9eb13a698d484ab5ae441c1100.pdf",
        "venue": "Multimedia tools and applications",
        "citationCount": 42,
        "score": 14.0,
        "summary": "Here's a focused summary of the paper for a literature review:\n\n**1. Research Problem & Motivation**\n*   **Problem**: The paper addresses the critical technical problem of achieving effective and real-time resource allocation in Fog computing environments, specifically tailored for latency-sensitive healthcare applications. Existing rule-based treatment recommendation systems in healthcare are often rigid, fail to account for patient-specific comorbidities, and lack robust evidence from randomized controlled trials for many ICU conditions.\n*   **Importance & Challenge**:\n    *   **Healthcare Context**: Critically ill patients require personalized, dynamic treatment plans, which current static guidelines cannot adequately provide. The complex, high-dimensional, and time-sensitive nature of ICU data necessitates advanced decision-making tools.\n    *   **Fog Computing**: Efficient resource management in Fog environments is crucial for low-latency processing of healthcare data, directly impacting Quality of Service (QoS) metrics such as allocation cost, response time, bandwidth efficiency, and energy consumption.\n    *   **Reinforcement Learning (RL)**: While RL offers significant potential for sequential decision-making in healthcare, challenges persist in its architectural design, choice of measurement metrics, model selection, and robust validation in authentic clinical settings.\n    *   **Limitations of Previous RA**: Many existing resource allocation (RA) algorithms are limited by unrealistic dependencies on response time, neglect of task priorities or varying packet sizes, and potential for network bottlenecks.\n\n**2. Related Work & Positioning**\n*   **Relation to Existing Approaches**:\n    *   The work builds upon established Reinforcement Learning (RL) models such as Q-Learning, Fitted-Q-Iteration (FQI), and Deep Q Network (DQN), acknowledging their foundational role while aiming to overcome their inherent limitations.\n    *   It positions itself within the domain of hyperparameter optimization for Deep Reinforcement Learning (DRL), referencing techniques like Population-Based Bandits (PB2), online hyperparameter adaptation (PBT-style), model-based optimization, and Bayesian optimization.\n*   **Limitations of Previous Solutions**:\n    *   **RL Models**: Traditional Q-learning requires extensive exploration and careful learning rate management. DQN faces challenges with sample correlation and non-stationary targets, which can hinder convergence.\n    *   **Hyperparameter Optimization**: Prior hyperparameter optimization techniques (e.g., PBT) often adapt only a single hyperparameter or rely on rudimentary stochastic perturbations, making them inefficient for tracking dynamic optimal configurations or adapting multiple hyperparameters simultaneously.\n    *   **Resource Allocation Algorithms**: Existing RA algorithms frequently:\n        *   Unrealistically depend on response time for task assignment between fog and cloud.\n        *   Fail to consider crucial task requirements like priority and the number of tasks.\n        *   Struggle with accurate capacity calculation, especially with varying packet sizes.\n        *   Are prone to causing network bottlenecks, degrading overall system performance.\n\n**3. Technical Approach & Innovation**\n*   **Core Technical Method**: The paper introduces an **Effective Resource Allocation Strategy (ERAS)** designed for Fog environments, specifically for real-time healthcare applications. ERAS aims to achieve efficient resource management through real-time resource allocation and predictive algorithms.\n*   **Architecture**: ERAS comprises two main modules:\n    *   **Data Preparation Algorithm (DPA)**: Manages the initial processing of incoming patient data. It includes:\n        *   **Sampling Module (SM)**: Uses stratified sampling to categorize data based on location and type (critical, normal, non-critical).\n        *   **Partitioning Module (PM)**: Divides the prepared data into training, testing, and validation sets.\n        *   **Balancing Module (BM)**: Addresses data imbalance using techniques like SMOTE to ensure robust model training.\n    *   **Resource Management Algorithm (RMA)**: This module is responsible for the core resource allocation and load balancing, incorporating an **Optimized Reinforcement Learning Algorithm (ORA)** and a **Resource Allocation Algorithm (RAA)**.\n*   **Innovation**:\n    *   **Optimized Reinforcement Learning**: The primary innovation is the optimization of Deep Q-Network (DQN) hyperparameters using **Particle Swarm Optimization (PSO)**. This novel integration fine-tunes the RL agent for superior performance in resource allocation within a healthcare Fog environment.\n    *   **PSO's Role**: PSO is selected for its global optimization capabilities, its inherent resistance to local minima, and its effective balance of exploitation and exploration, which are critical for identifying optimal RL hyperparameters.\n    *   **Real-time Suitability**: ERAS is engineered for real-time systems in Fog computing, aiming to achieve low latency and enhanced QoS through dynamic load balancing.\n    *   **Integrated Approach**: The strategy offers a comprehensive solution by integrating robust data preparation with an optimized RL-based resource management system.\n\n**4. Key Technical Contributions**\n*   **Novel Algorithms/Methods**:\n    *   Introduction of the **Effective Resource Allocation Strategy (ERAS)**, a holistic framework for resource management in healthcare Fog environments.\n    *   Development of an **Optimized Reinforcement Learning Algorithm (ORA)** that specifically leverages Particle Swarm Optimization (PSO) for hyperparameter tuning of Deep Q-Networks (DQN) in resource allocation.\n    *   A structured **Data Preparation Algorithm (DPA)**, which includes stratified sampling, data partitioning, and imbalance handling (SMOTE), tailored for healthcare data in a Fog context.\n*   **System Design/Architectural Innovations**:\n    *   A two-layer Fog architecture (DPA and RMA) designed for efficient real-time processing and decision-making in critical care scenarios.\n    *   The novel integration of PSO for hyperparameter optimization directly within the RL-based resource allocation pipeline, significantly enhancing the robustness and performance of the RL agent.\n*   **Theoretical Insights/Analysis**:\n    *   The paper implicitly demonstrates the practical advantages of employing a global optimization technique like PSO to overcome the challenges of hyperparameter tuning in complex DRL models, particularly for sequential decision-making problems such as resource allocation.\n    *   It highlights the critical significance of PSO's exploitation, exploration, and local minima avoidance capabilities in achieving superior RL performance compared to other optimization methodologies.\n\n**5. Experimental Validation**\n*   **Experiments Conducted**: The paper conducted experiments comparing ERAS against \"state-of-the-art algorithms\" for resource allocation.\n*   **Key Performance Metrics**:\n    *   **Makespan**: ERAS achieved the *minimum* Makespan, indicating faster task completion.\n    *   **Average Resource Utilization (ARU)**: ERAS *maximized* ARU, demonstrating efficient use of resources.\n    *   **Load Balancing Level (LBL)**: ERAS *maximized* LBL, indicating effective distribution of workload across resources.\n*   **Comparison Results**: ERAS consistently demonstrated superior performance by achieving the lowest Makespan and the highest ARU and LBL when compared to previous resource allocation algorithms, validating its effectiveness.\n\n**6. Limitations & Scope**\n*   **Technical Limitations/Assumptions**:\n    *   The provided content does not fully detail the specific \"state-of-the-art algorithms\" used for comparison, the exact experimental setup, or the precise nature of the healthcare applications (e.g., specific critical care scenarios).\n    *   The explicit details of the RL agent's state representation, action space, and reward function within the ORA for resource allocation are not elaborated in the provided text.\n*   **Scope of Applicability**:\n    *   The strategy is primarily focused on **Fog computing environments** and **healthcare applications**, particularly in critical care settings.\n    *   It is designed for **real-time systems** that demand low latency and high Quality of Service (QoS).\n    *   The proposed approach is generalizable to other resource allocation and load balancing problems requiring dynamic, sequential decision-making where robust hyperparameter optimization is crucial.\n\n**7. Technical Significance**\n*   **Advancement of State-of-the-Art**:\n    *   Advances the application of Reinforcement Learning in resource allocation by effectively addressing the critical challenge of hyperparameter optimization through Particle Swarm Optimization, leading to more robust and efficient RL agents.\n    *   Provides a comprehensive framework (ERAS) that integrates sophisticated data preparation with an optimized RL approach for real-time resource management in Fog environments, marking a significant step towards practical deployment in latency-sensitive domains like healthcare.\n    *   Demonstrates a tangible improvement in efficiency and effectiveness by achieving superior performance in key resource management metrics (Makespan, ARU, LBL) compared to existing approaches.\n*   **Potential Impact on Future Research**:\n    *   Opens new avenues for research into combining meta-heuristic optimization techniques (like PSO) with DRL for fine-tuning complex models in various real-time, dynamic environments beyond healthcare.\n    *   Encourages the development of more adaptive and robust resource allocation strategies in edge/fog computing by providing a validated methodology for optimizing underlying learning algorithms.\n    *   Could inspire the creation of more personalized and adaptive decision support systems in critical care, moving beyond static protocols towards AI-driven, patient-specific interventions.\n    *   Highlights the importance of comprehensive data preparation (sampling, partitioning, balancing) as an integral component for developing effective ML/RL solutions for real-world applications.",
        "keywords": [
          "Fog computing",
          "real-time resource allocation",
          "latency-sensitive healthcare applications",
          "Reinforcement Learning (RL)",
          "Deep Q-Network (DQN)",
          "hyperparameter optimization",
          "Particle Swarm Optimization (PSO)",
          "Effective Resource Allocation Strategy (ERAS)",
          "Optimized Reinforcement Learning Algorithm (ORA)",
          "Data Preparation Algorithm (DPA)",
          "load balancing",
          "minimum Makespan",
          "maximized Average Resource Utilization",
          "maximized Load Balancing Level"
        ],
        "paper_type": "based on the abstract and introduction:\n\n*   the abstract explicitly states: \"this paper presents a new effective resource allocation strategy (eras)\", \"this paper presents two main contributions, (i) optimization of the rl hyperparameters using pso, and (ii) using the optimized rl for the resource allocation and load balancing in the fog environment.\"\n*   it discusses the proposed solution (eras) and its components (real-time resource allocating, prediction algorithms, pso for hyperparameter optimization).\n*   it also mentions comparing eras with \"state-of-the-art algorithms\" and achieving better metrics (minimum makespan, maximizing aru and lbl), which are empirical results used to validate the proposed technical solution.\n*   the introduction sets up a problem (limitations of current healthcare systems, need for new approaches) that the proposed technical solution aims to address.\n\nthis strongly aligns with the \"technical\" classification, as it presents new methods, algorithms, and a system. while it includes empirical evaluation, the core contribution is the development and proposal of the new strategy and optimization technique.\n\n**classification: technical**"
      },
      "file_name": "174be0bacee04d9eb13a698d484ab5ae441c1100.pdf"
    },
    {
      "success": true,
      "doc_id": "5cd186a88fb29c5e23f097840cfacea1",
      "summary": "Modern recommender systems aim to improve user experience. As reinforcement learning (RL) naturally fits this objective---maximizing an user's reward per session---it has become an emerging topic in recommender systems. Developing RL-based recommendation methods, however, is not trivial due to the offline training challenge. Specifically, the keystone of traditional RL is to train an agent with large amounts of online exploration making lots of 'errors' in the process. In the recommendation setting, though, we cannot afford the price of making 'errors' online. As a result, the agent needs to be trained through offline historical implicit feedback, collected under different recommendation policies; traditional RL algorithms may lead to sub-optimal policies under these offline training settings. Here we propose a new learning paradigm---namely Prompt-Based Reinforcement Learning (PRL)---for the offline training of RL-based recommendation agents. While traditional RL algorithms attempt to map state-action input pairs to their expected rewards (e.g., Q-values), PRL directly infers actions (i.e., recommended items) from state-reward inputs. In short, the agents are trained to predict a recommended item given the prior interactions and an observed reward value---with simple supervised learning. At deployment time, this historical (training) data acts as a knowledge base, while the state-reward pairs are used as a prompt. The agents are thus used to answer the question: Which item should be recommended given the prior interactions & the prompted reward value? We implement PRL with four notable recommendation models and conduct experiments on two real-world e-commerce datasets. Experimental results demonstrate the superior performance of our proposed methods.",
      "intriguing_abstract": "Modern recommender systems aim to improve user experience. As reinforcement learning (RL) naturally fits this objective---maximizing an user's reward per session---it has become an emerging topic in recommender systems. Developing RL-based recommendation methods, however, is not trivial due to the offline training challenge. Specifically, the keystone of traditional RL is to train an agent with large amounts of online exploration making lots of 'errors' in the process. In the recommendation setting, though, we cannot afford the price of making 'errors' online. As a result, the agent needs to be trained through offline historical implicit feedback, collected under different recommendation policies; traditional RL algorithms may lead to sub-optimal policies under these offline training settings. Here we propose a new learning paradigm---namely Prompt-Based Reinforcement Learning (PRL)---for the offline training of RL-based recommendation agents. While traditional RL algorithms attempt to map state-action input pairs to their expected rewards (e.g., Q-values), PRL directly infers actions (i.e., recommended items) from state-reward inputs. In short, the agents are trained to predict a recommended item given the prior interactions and an observed reward value---with simple supervised learning. At deployment time, this historical (training) data acts as a knowledge base, while the state-reward pairs are used as a prompt. The agents are thus used to answer the question: Which item should be recommended given the prior interactions & the prompted reward value? We implement PRL with four notable recommendation models and conduct experiments on two real-world e-commerce datasets. Experimental results demonstrate the superior performance of our proposed methods.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/65587d4927fccc30788d3dfc9b639567721ff393.pdf",
      "citation_key": "xin2022qcl",
      "metadata": {
        "title": "Rethinking Reinforcement Learning for Recommendation: A Prompt Perspective",
        "authors": [
          "Xin Xin",
          "Tiago Pimentel",
          "Alexandros Karatzoglou",
          "Pengjie Ren",
          "Konstantina Christakopoulou",
          "Z. Ren"
        ],
        "published_date": "2022",
        "abstract": "Modern recommender systems aim to improve user experience. As reinforcement learning (RL) naturally fits this objective---maximizing an user's reward per session---it has become an emerging topic in recommender systems. Developing RL-based recommendation methods, however, is not trivial due to the offline training challenge. Specifically, the keystone of traditional RL is to train an agent with large amounts of online exploration making lots of 'errors' in the process. In the recommendation setting, though, we cannot afford the price of making 'errors' online. As a result, the agent needs to be trained through offline historical implicit feedback, collected under different recommendation policies; traditional RL algorithms may lead to sub-optimal policies under these offline training settings. Here we propose a new learning paradigm---namely Prompt-Based Reinforcement Learning (PRL)---for the offline training of RL-based recommendation agents. While traditional RL algorithms attempt to map state-action input pairs to their expected rewards (e.g., Q-values), PRL directly infers actions (i.e., recommended items) from state-reward inputs. In short, the agents are trained to predict a recommended item given the prior interactions and an observed reward value---with simple supervised learning. At deployment time, this historical (training) data acts as a knowledge base, while the state-reward pairs are used as a prompt. The agents are thus used to answer the question: Which item should be recommended given the prior interactions & the prompted reward value? We implement PRL with four notable recommendation models and conduct experiments on two real-world e-commerce datasets. Experimental results demonstrate the superior performance of our proposed methods.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/65587d4927fccc30788d3dfc9b639567721ff393.pdf",
        "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
        "citationCount": 41,
        "score": 13.666666666666666,
        "summary": "Modern recommender systems aim to improve user experience. As reinforcement learning (RL) naturally fits this objective---maximizing an user's reward per session---it has become an emerging topic in recommender systems. Developing RL-based recommendation methods, however, is not trivial due to the offline training challenge. Specifically, the keystone of traditional RL is to train an agent with large amounts of online exploration making lots of 'errors' in the process. In the recommendation setting, though, we cannot afford the price of making 'errors' online. As a result, the agent needs to be trained through offline historical implicit feedback, collected under different recommendation policies; traditional RL algorithms may lead to sub-optimal policies under these offline training settings. Here we propose a new learning paradigm---namely Prompt-Based Reinforcement Learning (PRL)---for the offline training of RL-based recommendation agents. While traditional RL algorithms attempt to map state-action input pairs to their expected rewards (e.g., Q-values), PRL directly infers actions (i.e., recommended items) from state-reward inputs. In short, the agents are trained to predict a recommended item given the prior interactions and an observed reward value---with simple supervised learning. At deployment time, this historical (training) data acts as a knowledge base, while the state-reward pairs are used as a prompt. The agents are thus used to answer the question: Which item should be recommended given the prior interactions & the prompted reward value? We implement PRL with four notable recommendation models and conduct experiments on two real-world e-commerce datasets. Experimental results demonstrate the superior performance of our proposed methods.",
        "keywords": []
      },
      "file_name": "65587d4927fccc30788d3dfc9b639567721ff393.pdf"
    },
    {
      "success": true,
      "doc_id": "dd9fc99d3f99f621287f1541428e540a",
      "summary": "Event-triggered model predictive control (eMPC) is a popular optimal control method with an aim to alleviate the computation and/or communication burden of MPC. However, it generally requires a priori knowledge of the closed-loop system behavior along with the communication characteristics for designing the event-trigger policy. This paper attempts to solve this challenge by proposing an efficient eMPC framework and demonstrates successful implementation of this framework on the autonomous vehicle path following. First of all, a model-free reinforcement learning (RL) agent is used to learn the optimal event-trigger policy without the need for a complete dynamical system and communication knowledge in this framework. Furthermore, techniques including prioritized experience replay (PER) buffer and long short-term memory (LSTM) are employed to foster exploration and improve training efficiency. In this paper, we use the proposed framework with three deep RL algorithms, i.e., Double Q-learning (DDQN), Proximal Policy Optimization (PPO), and Soft Actor-Critic (SAC), to solve this problem. Results show that all three deep RL-based eMPC (deep-RL-eMPC) can achieve better evaluation performance than the conventional threshold-based and previous linear Q-based approach in the autonomous path following. In particular, PPO-eMPC with LSTM and DDQN-eMPC with PER and LSTM obtain a superior balance between the closed-loop control performance and event-trigger frequency.",
      "intriguing_abstract": "Event-triggered model predictive control (eMPC) is a popular optimal control method with an aim to alleviate the computation and/or communication burden of MPC. However, it generally requires a priori knowledge of the closed-loop system behavior along with the communication characteristics for designing the event-trigger policy. This paper attempts to solve this challenge by proposing an efficient eMPC framework and demonstrates successful implementation of this framework on the autonomous vehicle path following. First of all, a model-free reinforcement learning (RL) agent is used to learn the optimal event-trigger policy without the need for a complete dynamical system and communication knowledge in this framework. Furthermore, techniques including prioritized experience replay (PER) buffer and long short-term memory (LSTM) are employed to foster exploration and improve training efficiency. In this paper, we use the proposed framework with three deep RL algorithms, i.e., Double Q-learning (DDQN), Proximal Policy Optimization (PPO), and Soft Actor-Critic (SAC), to solve this problem. Results show that all three deep RL-based eMPC (deep-RL-eMPC) can achieve better evaluation performance than the conventional threshold-based and previous linear Q-based approach in the autonomous path following. In particular, PPO-eMPC with LSTM and DDQN-eMPC with PER and LSTM obtain a superior balance between the closed-loop control performance and event-trigger frequency.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/2fd42844445ec644c2c44c093c3522c08b59cb45.pdf",
      "citation_key": "dang2022kwh",
      "metadata": {
        "title": "Event-Triggered Model Predictive Control With Deep Reinforcement Learning for Autonomous Driving",
        "authors": [
          "Fengying Dang",
          "Dong Chen",
          "J. Chen",
          "Zhaojian Li"
        ],
        "published_date": "2022",
        "abstract": "Event-triggered model predictive control (eMPC) is a popular optimal control method with an aim to alleviate the computation and/or communication burden of MPC. However, it generally requires a priori knowledge of the closed-loop system behavior along with the communication characteristics for designing the event-trigger policy. This paper attempts to solve this challenge by proposing an efficient eMPC framework and demonstrates successful implementation of this framework on the autonomous vehicle path following. First of all, a model-free reinforcement learning (RL) agent is used to learn the optimal event-trigger policy without the need for a complete dynamical system and communication knowledge in this framework. Furthermore, techniques including prioritized experience replay (PER) buffer and long short-term memory (LSTM) are employed to foster exploration and improve training efficiency. In this paper, we use the proposed framework with three deep RL algorithms, i.e., Double Q-learning (DDQN), Proximal Policy Optimization (PPO), and Soft Actor-Critic (SAC), to solve this problem. Results show that all three deep RL-based eMPC (deep-RL-eMPC) can achieve better evaluation performance than the conventional threshold-based and previous linear Q-based approach in the autonomous path following. In particular, PPO-eMPC with LSTM and DDQN-eMPC with PER and LSTM obtain a superior balance between the closed-loop control performance and event-trigger frequency.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/2fd42844445ec644c2c44c093c3522c08b59cb45.pdf",
        "venue": "IEEE Transactions on Intelligent Vehicles",
        "citationCount": 39,
        "score": 13.0,
        "summary": "Event-triggered model predictive control (eMPC) is a popular optimal control method with an aim to alleviate the computation and/or communication burden of MPC. However, it generally requires a priori knowledge of the closed-loop system behavior along with the communication characteristics for designing the event-trigger policy. This paper attempts to solve this challenge by proposing an efficient eMPC framework and demonstrates successful implementation of this framework on the autonomous vehicle path following. First of all, a model-free reinforcement learning (RL) agent is used to learn the optimal event-trigger policy without the need for a complete dynamical system and communication knowledge in this framework. Furthermore, techniques including prioritized experience replay (PER) buffer and long short-term memory (LSTM) are employed to foster exploration and improve training efficiency. In this paper, we use the proposed framework with three deep RL algorithms, i.e., Double Q-learning (DDQN), Proximal Policy Optimization (PPO), and Soft Actor-Critic (SAC), to solve this problem. Results show that all three deep RL-based eMPC (deep-RL-eMPC) can achieve better evaluation performance than the conventional threshold-based and previous linear Q-based approach in the autonomous path following. In particular, PPO-eMPC with LSTM and DDQN-eMPC with PER and LSTM obtain a superior balance between the closed-loop control performance and event-trigger frequency.",
        "keywords": []
      },
      "file_name": "2fd42844445ec644c2c44c093c3522c08b59cb45.pdf"
    },
    {
      "success": true,
      "doc_id": "9b1743cd3d6737d993d96f3a34dfdd59",
      "summary": "Reinforcement learning (RL) enables robots to learn skills from interactions with the real world. In practice, the unstructured step-based exploration used in Deep RL -- often very successful in simulation -- leads to jerky motion patterns on real robots. Consequences of the resulting shaky behavior are poor exploration, or even damage to the robot. We address these issues by adapting state-dependent exploration (SDE) to current Deep RL algorithms. To enable this adaptation, we propose two extensions to the original SDE, using more general features and re-sampling the noise periodically, which leads to a new exploration method generalized state-dependent exploration (gSDE). We evaluate gSDE both in simulation, on PyBullet continuous control tasks, and directly on three different real robots: a tendon-driven elastic robot, a quadruped and an RC car. The noise sampling interval of gSDE permits to have a compromise between performance and smoothness, which allows training directly on the real robots without loss of performance. The code is available at https://github.com/DLR-RM/stable-baselines3.",
      "intriguing_abstract": "Reinforcement learning (RL) enables robots to learn skills from interactions with the real world. In practice, the unstructured step-based exploration used in Deep RL -- often very successful in simulation -- leads to jerky motion patterns on real robots. Consequences of the resulting shaky behavior are poor exploration, or even damage to the robot. We address these issues by adapting state-dependent exploration (SDE) to current Deep RL algorithms. To enable this adaptation, we propose two extensions to the original SDE, using more general features and re-sampling the noise periodically, which leads to a new exploration method generalized state-dependent exploration (gSDE). We evaluate gSDE both in simulation, on PyBullet continuous control tasks, and directly on three different real robots: a tendon-driven elastic robot, a quadruped and an RC car. The noise sampling interval of gSDE permits to have a compromise between performance and smoothness, which allows training directly on the real robots without loss of performance. The code is available at https://github.com/DLR-RM/stable-baselines3.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/3e0925355554e3aeb99de8165c268582a82de3bb.pdf",
      "citation_key": "raffin2020o1a",
      "metadata": {
        "title": "Smooth Exploration for Robotic Reinforcement Learning",
        "authors": [
          "A. Raffin",
          "Jens Kober",
          "F. Stulp"
        ],
        "published_date": "2020",
        "abstract": "Reinforcement learning (RL) enables robots to learn skills from interactions with the real world. In practice, the unstructured step-based exploration used in Deep RL -- often very successful in simulation -- leads to jerky motion patterns on real robots. Consequences of the resulting shaky behavior are poor exploration, or even damage to the robot. We address these issues by adapting state-dependent exploration (SDE) to current Deep RL algorithms. To enable this adaptation, we propose two extensions to the original SDE, using more general features and re-sampling the noise periodically, which leads to a new exploration method generalized state-dependent exploration (gSDE). We evaluate gSDE both in simulation, on PyBullet continuous control tasks, and directly on three different real robots: a tendon-driven elastic robot, a quadruped and an RC car. The noise sampling interval of gSDE permits to have a compromise between performance and smoothness, which allows training directly on the real robots without loss of performance. The code is available at https://github.com/DLR-RM/stable-baselines3.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/3e0925355554e3aeb99de8165c268582a82de3bb.pdf",
        "venue": "Conference on Robot Learning",
        "citationCount": 64,
        "score": 12.8,
        "summary": "Reinforcement learning (RL) enables robots to learn skills from interactions with the real world. In practice, the unstructured step-based exploration used in Deep RL -- often very successful in simulation -- leads to jerky motion patterns on real robots. Consequences of the resulting shaky behavior are poor exploration, or even damage to the robot. We address these issues by adapting state-dependent exploration (SDE) to current Deep RL algorithms. To enable this adaptation, we propose two extensions to the original SDE, using more general features and re-sampling the noise periodically, which leads to a new exploration method generalized state-dependent exploration (gSDE). We evaluate gSDE both in simulation, on PyBullet continuous control tasks, and directly on three different real robots: a tendon-driven elastic robot, a quadruped and an RC car. The noise sampling interval of gSDE permits to have a compromise between performance and smoothness, which allows training directly on the real robots without loss of performance. The code is available at https://github.com/DLR-RM/stable-baselines3.",
        "keywords": []
      },
      "file_name": "3e0925355554e3aeb99de8165c268582a82de3bb.pdf"
    },
    {
      "success": true,
      "doc_id": "80dae639395feb8d2cab6c6a123bd873",
      "summary": "Over the last 5years deep learning has progressed tremendously in both image recognition and natural language processing. Now it is increasingly applied to other data rich fields. In drug discovery, recurrent neural networks (RNNs) have been shown to be an effective method to generate novel chemical structures in the form of SMILES. However, ligands generated by current methods have so far provided relatively low diversity and do not fully cover the whole chemical space occupied by known ligands. Here, we propose a new method (DrugEx) to discover de novo drug-like molecules. DrugEx is an RNN model (generator) trained through reinforcement learning which was integrated with a special exploration strategy. As a case study we applied our method to design ligands against the adenosine A2A receptor. From ChEMBL data, a machine learning model (predictor) was created to predict whether generated molecules are active or not. Based on this predictor as the reward function, the generator was trained by reinforcement learning without any further data. We then compared the performance of our method with two previously published methods, REINVENT and ORGANIC. We found that candidate molecules our model designed, and predicted to be active, had a larger chemical diversity and better covered the chemical space of known ligands compared to the state-of-the-art.",
      "intriguing_abstract": "Over the last 5years deep learning has progressed tremendously in both image recognition and natural language processing. Now it is increasingly applied to other data rich fields. In drug discovery, recurrent neural networks (RNNs) have been shown to be an effective method to generate novel chemical structures in the form of SMILES. However, ligands generated by current methods have so far provided relatively low diversity and do not fully cover the whole chemical space occupied by known ligands. Here, we propose a new method (DrugEx) to discover de novo drug-like molecules. DrugEx is an RNN model (generator) trained through reinforcement learning which was integrated with a special exploration strategy. As a case study we applied our method to design ligands against the adenosine A2A receptor. From ChEMBL data, a machine learning model (predictor) was created to predict whether generated molecules are active or not. Based on this predictor as the reward function, the generator was trained by reinforcement learning without any further data. We then compared the performance of our method with two previously published methods, REINVENT and ORGANIC. We found that candidate molecules our model designed, and predicted to be active, had a larger chemical diversity and better covered the chemical space of known ligands compared to the state-of-the-art.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/1fa2a04bffc18cfb46650a11ab0e5696d711f58f.pdf",
      "citation_key": "liu2018jde",
      "metadata": {
        "title": "An exploration strategy improves the diversity of de novo ligands using deep reinforcement learning: a case for the adenosine A2A receptor",
        "authors": [
          "Xuhan Liu",
          "K. Ye",
          "H. V. van Vlijmen",
          "A. IJzerman",
          "G. V. van Westen"
        ],
        "published_date": "2018",
        "abstract": "Over the last 5years deep learning has progressed tremendously in both image recognition and natural language processing. Now it is increasingly applied to other data rich fields. In drug discovery, recurrent neural networks (RNNs) have been shown to be an effective method to generate novel chemical structures in the form of SMILES. However, ligands generated by current methods have so far provided relatively low diversity and do not fully cover the whole chemical space occupied by known ligands. Here, we propose a new method (DrugEx) to discover de novo drug-like molecules. DrugEx is an RNN model (generator) trained through reinforcement learning which was integrated with a special exploration strategy. As a case study we applied our method to design ligands against the adenosine A2A receptor. From ChEMBL data, a machine learning model (predictor) was created to predict whether generated molecules are active or not. Based on this predictor as the reward function, the generator was trained by reinforcement learning without any further data. We then compared the performance of our method with two previously published methods, REINVENT and ORGANIC. We found that candidate molecules our model designed, and predicted to be active, had a larger chemical diversity and better covered the chemical space of known ligands compared to the state-of-the-art.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/1fa2a04bffc18cfb46650a11ab0e5696d711f58f.pdf",
        "venue": "Journal of Cheminformatics",
        "citationCount": 77,
        "score": 11.0,
        "summary": "Over the last 5years deep learning has progressed tremendously in both image recognition and natural language processing. Now it is increasingly applied to other data rich fields. In drug discovery, recurrent neural networks (RNNs) have been shown to be an effective method to generate novel chemical structures in the form of SMILES. However, ligands generated by current methods have so far provided relatively low diversity and do not fully cover the whole chemical space occupied by known ligands. Here, we propose a new method (DrugEx) to discover de novo drug-like molecules. DrugEx is an RNN model (generator) trained through reinforcement learning which was integrated with a special exploration strategy. As a case study we applied our method to design ligands against the adenosine A2A receptor. From ChEMBL data, a machine learning model (predictor) was created to predict whether generated molecules are active or not. Based on this predictor as the reward function, the generator was trained by reinforcement learning without any further data. We then compared the performance of our method with two previously published methods, REINVENT and ORGANIC. We found that candidate molecules our model designed, and predicted to be active, had a larger chemical diversity and better covered the chemical space of known ligands compared to the state-of-the-art.",
        "keywords": []
      },
      "file_name": "1fa2a04bffc18cfb46650a11ab0e5696d711f58f.pdf"
    },
    {
      "success": true,
      "doc_id": "122ead9e13be5a3a03afbc1c1b5036eb",
      "summary": "The ability to perform aggressive movements,which are called aggressive flights, is important for quadrotors during navigation. However, aggressive quadrotor flights are still a great challenge to practical applications. The existing solutions to aggressive flights heavily rely on a predefined trajectory, which is a time-consuming preprocessing step. To avoid such path planning, we propose a curiosity-driven reinforcement learning method for aggressive flight missions and a similarity-based curiosity module is introduced to speed up the training procedure. A branch structure exploration strategy is also applied to guarantee the robustness of the policy and to ensure the policy trained in simulations can be performed in real-world experiments directly. The experimental results in simulations demonstrate that our reinforcement learning algorithm performs well in aggressive flight tasks, speeds up the convergence process and improves the robustness of the policy. Besides, our algorithm shows a satisfactory simulated to real transferability and performs well in real-world experiments.",
      "intriguing_abstract": "The ability to perform aggressive movements,which are called aggressive flights, is important for quadrotors during navigation. However, aggressive quadrotor flights are still a great challenge to practical applications. The existing solutions to aggressive flights heavily rely on a predefined trajectory, which is a time-consuming preprocessing step. To avoid such path planning, we propose a curiosity-driven reinforcement learning method for aggressive flight missions and a similarity-based curiosity module is introduced to speed up the training procedure. A branch structure exploration strategy is also applied to guarantee the robustness of the policy and to ensure the policy trained in simulations can be performed in real-world experiments directly. The experimental results in simulations demonstrate that our reinforcement learning algorithm performs well in aggressive flight tasks, speeds up the convergence process and improves the robustness of the policy. Besides, our algorithm shows a satisfactory simulated to real transferability and performs well in real-world experiments.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/442e9f1e8f6218e68f944fd3028c5385691d4112.pdf",
      "citation_key": "sun2022ul9",
      "metadata": {
        "title": "Aggressive Quadrotor Flight Using Curiosity-Driven Reinforcement Learning",
        "authors": [
          "Qiyu Sun",
          "Jinbao Fang",
          "Weixing Zheng",
          "Yang Tang"
        ],
        "published_date": "2022",
        "abstract": "The ability to perform aggressive movements,which are called aggressive flights, is important for quadrotors during navigation. However, aggressive quadrotor flights are still a great challenge to practical applications. The existing solutions to aggressive flights heavily rely on a predefined trajectory, which is a time-consuming preprocessing step. To avoid such path planning, we propose a curiosity-driven reinforcement learning method for aggressive flight missions and a similarity-based curiosity module is introduced to speed up the training procedure. A branch structure exploration strategy is also applied to guarantee the robustness of the policy and to ensure the policy trained in simulations can be performed in real-world experiments directly. The experimental results in simulations demonstrate that our reinforcement learning algorithm performs well in aggressive flight tasks, speeds up the convergence process and improves the robustness of the policy. Besides, our algorithm shows a satisfactory simulated to real transferability and performs well in real-world experiments.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/442e9f1e8f6218e68f944fd3028c5385691d4112.pdf",
        "venue": "IEEE transactions on industrial electronics (1982. Print)",
        "citationCount": 32,
        "score": 10.666666666666666,
        "summary": "The ability to perform aggressive movements,which are called aggressive flights, is important for quadrotors during navigation. However, aggressive quadrotor flights are still a great challenge to practical applications. The existing solutions to aggressive flights heavily rely on a predefined trajectory, which is a time-consuming preprocessing step. To avoid such path planning, we propose a curiosity-driven reinforcement learning method for aggressive flight missions and a similarity-based curiosity module is introduced to speed up the training procedure. A branch structure exploration strategy is also applied to guarantee the robustness of the policy and to ensure the policy trained in simulations can be performed in real-world experiments directly. The experimental results in simulations demonstrate that our reinforcement learning algorithm performs well in aggressive flight tasks, speeds up the convergence process and improves the robustness of the policy. Besides, our algorithm shows a satisfactory simulated to real transferability and performs well in real-world experiments.",
        "keywords": []
      },
      "file_name": "442e9f1e8f6218e68f944fd3028c5385691d4112.pdf"
    },
    {
      "success": true,
      "doc_id": "d47c30344562d7853a715cb5036d9f95",
      "summary": "Efficient exploration remains a major challenge for reinforcement learning. One reason is that the variability of the returns often depends on the current state and action, and is therefore heteroscedastic. Classical exploration strategies such as upper confidence bound algorithms and Thompson sampling fail to appropriately account for heteroscedasticity, even in the bandit setting. Motivated by recent findings that address this issue in bandits, we propose to use Information-Directed Sampling (IDS) for exploration in reinforcement learning. As our main contribution, we build on recent advances in distributional reinforcement learning and propose a novel, tractable approximation of IDS for deep Q-learning. The resulting exploration strategy explicitly accounts for both parametric uncertainty and heteroscedastic observation noise. We evaluate our method on Atari games and demonstrate a significant improvement over alternative approaches.",
      "intriguing_abstract": "Efficient exploration remains a major challenge for reinforcement learning. One reason is that the variability of the returns often depends on the current state and action, and is therefore heteroscedastic. Classical exploration strategies such as upper confidence bound algorithms and Thompson sampling fail to appropriately account for heteroscedasticity, even in the bandit setting. Motivated by recent findings that address this issue in bandits, we propose to use Information-Directed Sampling (IDS) for exploration in reinforcement learning. As our main contribution, we build on recent advances in distributional reinforcement learning and propose a novel, tractable approximation of IDS for deep Q-learning. The resulting exploration strategy explicitly accounts for both parametric uncertainty and heteroscedastic observation noise. We evaluate our method on Atari games and demonstrate a significant improvement over alternative approaches.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/a25b645c3d24f91164230a0ac5bb2d4ec88c1538.pdf",
      "citation_key": "nikolov20184g9",
      "metadata": {
        "title": "Information-Directed Exploration for Deep Reinforcement Learning",
        "authors": [
          "Nikolay Nikolov",
          "Johannes Kirschner",
          "Felix Berkenkamp",
          "Andreas Krause"
        ],
        "published_date": "2018",
        "abstract": "Efficient exploration remains a major challenge for reinforcement learning. One reason is that the variability of the returns often depends on the current state and action, and is therefore heteroscedastic. Classical exploration strategies such as upper confidence bound algorithms and Thompson sampling fail to appropriately account for heteroscedasticity, even in the bandit setting. Motivated by recent findings that address this issue in bandits, we propose to use Information-Directed Sampling (IDS) for exploration in reinforcement learning. As our main contribution, we build on recent advances in distributional reinforcement learning and propose a novel, tractable approximation of IDS for deep Q-learning. The resulting exploration strategy explicitly accounts for both parametric uncertainty and heteroscedastic observation noise. We evaluate our method on Atari games and demonstrate a significant improvement over alternative approaches.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/a25b645c3d24f91164230a0ac5bb2d4ec88c1538.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 74,
        "score": 10.571428571428571,
        "summary": "Efficient exploration remains a major challenge for reinforcement learning. One reason is that the variability of the returns often depends on the current state and action, and is therefore heteroscedastic. Classical exploration strategies such as upper confidence bound algorithms and Thompson sampling fail to appropriately account for heteroscedasticity, even in the bandit setting. Motivated by recent findings that address this issue in bandits, we propose to use Information-Directed Sampling (IDS) for exploration in reinforcement learning. As our main contribution, we build on recent advances in distributional reinforcement learning and propose a novel, tractable approximation of IDS for deep Q-learning. The resulting exploration strategy explicitly accounts for both parametric uncertainty and heteroscedastic observation noise. We evaluate our method on Atari games and demonstrate a significant improvement over alternative approaches.",
        "keywords": []
      },
      "file_name": "a25b645c3d24f91164230a0ac5bb2d4ec88c1538.pdf"
    },
    {
      "success": true,
      "doc_id": "dc79be4397b9aaa1197c0bb569a63ec8",
      "summary": "We study the problem of reinforcement learning (RL) with low (policy) switching cost - a problem well-motivated by real-life RL applications in which deployments of new policies are costly and the number of policy updates must be low. In this paper, we propose a new algorithm based on stage-wise exploration and adaptive policy elimination that achieves a regret of $\\widetilde{O}(\\sqrt{H^4S^2AT})$ while requiring a switching cost of $O(HSA \\log\\log T)$. This is an exponential improvement over the best-known switching cost $O(H^2SA\\log T)$ among existing methods with $\\widetilde{O}(\\mathrm{poly}(H,S,A)\\sqrt{T})$ regret. In the above, $S,A$ denotes the number of states and actions in an $H$-horizon episodic Markov Decision Process model with unknown transitions, and $T$ is the number of steps. As a byproduct of our new techniques, we also derive a reward-free exploration algorithm with a switching cost of $O(HSA)$. Furthermore, we prove a pair of information-theoretical lower bounds which say that (1) Any no-regret algorithm must have a switching cost of $\\Omega(HSA)$; (2) Any $\\widetilde{O}(\\sqrt{T})$ regret algorithm must incur a switching cost of $\\Omega(HSA\\log\\log T)$. Both our algorithms are thus optimal in their switching costs.",
      "intriguing_abstract": "We study the problem of reinforcement learning (RL) with low (policy) switching cost - a problem well-motivated by real-life RL applications in which deployments of new policies are costly and the number of policy updates must be low. In this paper, we propose a new algorithm based on stage-wise exploration and adaptive policy elimination that achieves a regret of $\\widetilde{O}(\\sqrt{H^4S^2AT})$ while requiring a switching cost of $O(HSA \\log\\log T)$. This is an exponential improvement over the best-known switching cost $O(H^2SA\\log T)$ among existing methods with $\\widetilde{O}(\\mathrm{poly}(H,S,A)\\sqrt{T})$ regret. In the above, $S,A$ denotes the number of states and actions in an $H$-horizon episodic Markov Decision Process model with unknown transitions, and $T$ is the number of steps. As a byproduct of our new techniques, we also derive a reward-free exploration algorithm with a switching cost of $O(HSA)$. Furthermore, we prove a pair of information-theoretical lower bounds which say that (1) Any no-regret algorithm must have a switching cost of $\\Omega(HSA)$; (2) Any $\\widetilde{O}(\\sqrt{T})$ regret algorithm must incur a switching cost of $\\Omega(HSA\\log\\log T)$. Both our algorithms are thus optimal in their switching costs.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/2fc993c78cd84dafe4c36d2ac1cc25a6256aaa9d.pdf",
      "citation_key": "qiao20220gx",
      "metadata": {
        "title": "Sample-Efficient Reinforcement Learning with loglog(T) Switching Cost",
        "authors": [
          "Dan Qiao",
          "Ming Yin",
          "Ming Min",
          "Yu-Xiang Wang"
        ],
        "published_date": "2022",
        "abstract": "We study the problem of reinforcement learning (RL) with low (policy) switching cost - a problem well-motivated by real-life RL applications in which deployments of new policies are costly and the number of policy updates must be low. In this paper, we propose a new algorithm based on stage-wise exploration and adaptive policy elimination that achieves a regret of $\\widetilde{O}(\\sqrt{H^4S^2AT})$ while requiring a switching cost of $O(HSA \\log\\log T)$. This is an exponential improvement over the best-known switching cost $O(H^2SA\\log T)$ among existing methods with $\\widetilde{O}(\\mathrm{poly}(H,S,A)\\sqrt{T})$ regret. In the above, $S,A$ denotes the number of states and actions in an $H$-horizon episodic Markov Decision Process model with unknown transitions, and $T$ is the number of steps. As a byproduct of our new techniques, we also derive a reward-free exploration algorithm with a switching cost of $O(HSA)$. Furthermore, we prove a pair of information-theoretical lower bounds which say that (1) Any no-regret algorithm must have a switching cost of $\\Omega(HSA)$; (2) Any $\\widetilde{O}(\\sqrt{T})$ regret algorithm must incur a switching cost of $\\Omega(HSA\\log\\log T)$. Both our algorithms are thus optimal in their switching costs.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/2fc993c78cd84dafe4c36d2ac1cc25a6256aaa9d.pdf",
        "venue": "International Conference on Machine Learning",
        "citationCount": 31,
        "score": 10.333333333333332,
        "summary": "We study the problem of reinforcement learning (RL) with low (policy) switching cost - a problem well-motivated by real-life RL applications in which deployments of new policies are costly and the number of policy updates must be low. In this paper, we propose a new algorithm based on stage-wise exploration and adaptive policy elimination that achieves a regret of $\\widetilde{O}(\\sqrt{H^4S^2AT})$ while requiring a switching cost of $O(HSA \\log\\log T)$. This is an exponential improvement over the best-known switching cost $O(H^2SA\\log T)$ among existing methods with $\\widetilde{O}(\\mathrm{poly}(H,S,A)\\sqrt{T})$ regret. In the above, $S,A$ denotes the number of states and actions in an $H$-horizon episodic Markov Decision Process model with unknown transitions, and $T$ is the number of steps. As a byproduct of our new techniques, we also derive a reward-free exploration algorithm with a switching cost of $O(HSA)$. Furthermore, we prove a pair of information-theoretical lower bounds which say that (1) Any no-regret algorithm must have a switching cost of $\\Omega(HSA)$; (2) Any $\\widetilde{O}(\\sqrt{T})$ regret algorithm must incur a switching cost of $\\Omega(HSA\\log\\log T)$. Both our algorithms are thus optimal in their switching costs.",
        "keywords": []
      },
      "file_name": "2fc993c78cd84dafe4c36d2ac1cc25a6256aaa9d.pdf"
    },
    {
      "success": true,
      "doc_id": "4b0ab1435c8298da28a0d0aba061f6c7",
      "summary": "We consider the safe reinforcement learning (RL) problem of maximizing utility with extremely low constraint violation rates. Assuming no prior knowledge or pre-training of the environment safety model given a task, an agent has to learn, via exploration, which states and actions are safe. A popular approach in this line of research is to combine a model-free RL algorithm with the Lagrangian method to adjust the weight of the constraint reward relative to the utility reward dynamically. It relies on a single policy to handle the conflict between utility and constraint rewards, which is often challenging. We present SEditor, a two-policy approach that learns a safety editor policy transforming potentially unsafe actions proposed by a utility maximizer policy into safe ones. The safety editor is trained to maximize the constraint reward while minimizing a hinge loss of the utility state-action values before and after an action is edited. SEditor extends existing safety layer designs that assume simplified safety models, to general safe RL scenarios where the safety model can in theory be arbitrarily complex. As a first-order method, it is easy to implement and efficient for both inference and training. On 12 Safety Gym tasks and 2 safe racing tasks, SEditor obtains much a higher overall safety-weighted-utility (SWU) score than the baselines, and demonstrates outstanding utility performance with constraint violation rates as low as once per 2k time steps, even in obstacle-dense environments. On some tasks, this low violation rate is up to 200 times lower than that of an unconstrained RL method with similar utility performance. Code is available at https://github.com/hnyu/seditor.",
      "intriguing_abstract": "We consider the safe reinforcement learning (RL) problem of maximizing utility with extremely low constraint violation rates. Assuming no prior knowledge or pre-training of the environment safety model given a task, an agent has to learn, via exploration, which states and actions are safe. A popular approach in this line of research is to combine a model-free RL algorithm with the Lagrangian method to adjust the weight of the constraint reward relative to the utility reward dynamically. It relies on a single policy to handle the conflict between utility and constraint rewards, which is often challenging. We present SEditor, a two-policy approach that learns a safety editor policy transforming potentially unsafe actions proposed by a utility maximizer policy into safe ones. The safety editor is trained to maximize the constraint reward while minimizing a hinge loss of the utility state-action values before and after an action is edited. SEditor extends existing safety layer designs that assume simplified safety models, to general safe RL scenarios where the safety model can in theory be arbitrarily complex. As a first-order method, it is easy to implement and efficient for both inference and training. On 12 Safety Gym tasks and 2 safe racing tasks, SEditor obtains much a higher overall safety-weighted-utility (SWU) score than the baselines, and demonstrates outstanding utility performance with constraint violation rates as low as once per 2k time steps, even in obstacle-dense environments. On some tasks, this low violation rate is up to 200 times lower than that of an unconstrained RL method with similar utility performance. Code is available at https://github.com/hnyu/seditor.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/0ba7f2e592dda172bc3d07f88cdcf2a57830deec.pdf",
      "citation_key": "yu20222xi",
      "metadata": {
        "title": "Towards Safe Reinforcement Learning with a Safety Editor Policy",
        "authors": [
          "Haonan Yu",
          "Wei Xu",
          "Haichao Zhang"
        ],
        "published_date": "2022",
        "abstract": "We consider the safe reinforcement learning (RL) problem of maximizing utility with extremely low constraint violation rates. Assuming no prior knowledge or pre-training of the environment safety model given a task, an agent has to learn, via exploration, which states and actions are safe. A popular approach in this line of research is to combine a model-free RL algorithm with the Lagrangian method to adjust the weight of the constraint reward relative to the utility reward dynamically. It relies on a single policy to handle the conflict between utility and constraint rewards, which is often challenging. We present SEditor, a two-policy approach that learns a safety editor policy transforming potentially unsafe actions proposed by a utility maximizer policy into safe ones. The safety editor is trained to maximize the constraint reward while minimizing a hinge loss of the utility state-action values before and after an action is edited. SEditor extends existing safety layer designs that assume simplified safety models, to general safe RL scenarios where the safety model can in theory be arbitrarily complex. As a first-order method, it is easy to implement and efficient for both inference and training. On 12 Safety Gym tasks and 2 safe racing tasks, SEditor obtains much a higher overall safety-weighted-utility (SWU) score than the baselines, and demonstrates outstanding utility performance with constraint violation rates as low as once per 2k time steps, even in obstacle-dense environments. On some tasks, this low violation rate is up to 200 times lower than that of an unconstrained RL method with similar utility performance. Code is available at https://github.com/hnyu/seditor.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/0ba7f2e592dda172bc3d07f88cdcf2a57830deec.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 31,
        "score": 10.333333333333332,
        "summary": "We consider the safe reinforcement learning (RL) problem of maximizing utility with extremely low constraint violation rates. Assuming no prior knowledge or pre-training of the environment safety model given a task, an agent has to learn, via exploration, which states and actions are safe. A popular approach in this line of research is to combine a model-free RL algorithm with the Lagrangian method to adjust the weight of the constraint reward relative to the utility reward dynamically. It relies on a single policy to handle the conflict between utility and constraint rewards, which is often challenging. We present SEditor, a two-policy approach that learns a safety editor policy transforming potentially unsafe actions proposed by a utility maximizer policy into safe ones. The safety editor is trained to maximize the constraint reward while minimizing a hinge loss of the utility state-action values before and after an action is edited. SEditor extends existing safety layer designs that assume simplified safety models, to general safe RL scenarios where the safety model can in theory be arbitrarily complex. As a first-order method, it is easy to implement and efficient for both inference and training. On 12 Safety Gym tasks and 2 safe racing tasks, SEditor obtains much a higher overall safety-weighted-utility (SWU) score than the baselines, and demonstrates outstanding utility performance with constraint violation rates as low as once per 2k time steps, even in obstacle-dense environments. On some tasks, this low violation rate is up to 200 times lower than that of an unconstrained RL method with similar utility performance. Code is available at https://github.com/hnyu/seditor.",
        "keywords": []
      },
      "file_name": "0ba7f2e592dda172bc3d07f88cdcf2a57830deec.pdf"
    },
    {
      "success": true,
      "doc_id": "476ae161fb04410cf0e1f8dfe9109380",
      "summary": "Offline Reinforcement Learning (ORL) enablesus to separately study the two interlinked processes of reinforcement learning: collecting informative experience and inferring optimal behaviour. The second step has been widely studied in the offline setting, but just as critical to data-efficient RL is the collection of informative data. The task-agnostic setting for data collection, where the task is not known a priori, is of particular interest due to the possibility of collecting a single dataset and using it to solve several downstream tasks as they arise. We investigate this setting via curiosity-based intrinsic motivation, a family of exploration methods which encourage the agent to explore those states or transitions it has not yet learned to model. With Explore2Offline, we propose to evaluate the quality of collected data by transferring the collected data and inferring policies with reward relabelling and standard offline RL algorithms. We evaluate a wide variety of data collection strategies, including a new exploration agent, Intrinsic Model Predictive Control (IMPC), using this scheme and demonstrate their performance on various tasks. We use this decoupled framework to strengthen intuitions about exploration and the data prerequisites for effective offline RL.",
      "intriguing_abstract": "Offline Reinforcement Learning (ORL) enablesus to separately study the two interlinked processes of reinforcement learning: collecting informative experience and inferring optimal behaviour. The second step has been widely studied in the offline setting, but just as critical to data-efficient RL is the collection of informative data. The task-agnostic setting for data collection, where the task is not known a priori, is of particular interest due to the possibility of collecting a single dataset and using it to solve several downstream tasks as they arise. We investigate this setting via curiosity-based intrinsic motivation, a family of exploration methods which encourage the agent to explore those states or transitions it has not yet learned to model. With Explore2Offline, we propose to evaluate the quality of collected data by transferring the collected data and inferring policies with reward relabelling and standard offline RL algorithms. We evaluate a wide variety of data collection strategies, including a new exploration agent, Intrinsic Model Predictive Control (IMPC), using this scheme and demonstrate their performance on various tasks. We use this decoupled framework to strengthen intuitions about exploration and the data prerequisites for effective offline RL.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/09da56cd3bf72b632c43969be97874fa14a3765c.pdf",
      "citation_key": "lambert202277x",
      "metadata": {
        "title": "The Challenges of Exploration for Offline Reinforcement Learning",
        "authors": [
          "Nathan Lambert",
          "Markus Wulfmeier",
          "William F. Whitney",
          "Arunkumar Byravan",
          "Michael Bloesch",
          "Vibhavari Dasagi",
          "Tim Hertweck",
          "Martin A. Riedmiller"
        ],
        "published_date": "2022",
        "abstract": "Offline Reinforcement Learning (ORL) enablesus to separately study the two interlinked processes of reinforcement learning: collecting informative experience and inferring optimal behaviour. The second step has been widely studied in the offline setting, but just as critical to data-efficient RL is the collection of informative data. The task-agnostic setting for data collection, where the task is not known a priori, is of particular interest due to the possibility of collecting a single dataset and using it to solve several downstream tasks as they arise. We investigate this setting via curiosity-based intrinsic motivation, a family of exploration methods which encourage the agent to explore those states or transitions it has not yet learned to model. With Explore2Offline, we propose to evaluate the quality of collected data by transferring the collected data and inferring policies with reward relabelling and standard offline RL algorithms. We evaluate a wide variety of data collection strategies, including a new exploration agent, Intrinsic Model Predictive Control (IMPC), using this scheme and demonstrate their performance on various tasks. We use this decoupled framework to strengthen intuitions about exploration and the data prerequisites for effective offline RL.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/09da56cd3bf72b632c43969be97874fa14a3765c.pdf",
        "venue": "arXiv.org",
        "citationCount": 30,
        "score": 10.0,
        "summary": "Offline Reinforcement Learning (ORL) enablesus to separately study the two interlinked processes of reinforcement learning: collecting informative experience and inferring optimal behaviour. The second step has been widely studied in the offline setting, but just as critical to data-efficient RL is the collection of informative data. The task-agnostic setting for data collection, where the task is not known a priori, is of particular interest due to the possibility of collecting a single dataset and using it to solve several downstream tasks as they arise. We investigate this setting via curiosity-based intrinsic motivation, a family of exploration methods which encourage the agent to explore those states or transitions it has not yet learned to model. With Explore2Offline, we propose to evaluate the quality of collected data by transferring the collected data and inferring policies with reward relabelling and standard offline RL algorithms. We evaluate a wide variety of data collection strategies, including a new exploration agent, Intrinsic Model Predictive Control (IMPC), using this scheme and demonstrate their performance on various tasks. We use this decoupled framework to strengthen intuitions about exploration and the data prerequisites for effective offline RL.",
        "keywords": []
      },
      "file_name": "09da56cd3bf72b632c43969be97874fa14a3765c.pdf"
    },
    {
      "success": true,
      "doc_id": "e8a395ac3a725e11afece98e4be2863b",
      "summary": "The ability of continual learning systems to transfer knowledge from previously seen tasks in order to maximize performance on new tasks is a significant challenge for the field, limiting the applicability of continual learning solutions to realistic scenarios. Consequently, this study aims to broaden our understanding of transfer and its driving forces in the specific case of continual reinforcement learning. We adopt SAC as the underlying RL algorithm and Continual World as a suite of continuous control tasks. We systematically study how different components of SAC (the actor and the critic, exploration, and data) affect transfer efficacy, and we provide recommendations regarding various modeling options. The best set of choices, dubbed ClonEx-SAC, is evaluated on the recent Continual World benchmark. ClonEx-SAC achieves 87% final success rate compared to 80% of PackNet, the best method in the benchmark. Moreover, the transfer grows from 0.18 to 0.54 according to the metric provided by Continual World.",
      "intriguing_abstract": "The ability of continual learning systems to transfer knowledge from previously seen tasks in order to maximize performance on new tasks is a significant challenge for the field, limiting the applicability of continual learning solutions to realistic scenarios. Consequently, this study aims to broaden our understanding of transfer and its driving forces in the specific case of continual reinforcement learning. We adopt SAC as the underlying RL algorithm and Continual World as a suite of continuous control tasks. We systematically study how different components of SAC (the actor and the critic, exploration, and data) affect transfer efficacy, and we provide recommendations regarding various modeling options. The best set of choices, dubbed ClonEx-SAC, is evaluated on the recent Continual World benchmark. ClonEx-SAC achieves 87% final success rate compared to 80% of PackNet, the best method in the benchmark. Moreover, the transfer grows from 0.18 to 0.54 according to the metric provided by Continual World.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/fc3b5dc5528e24dbbc8f4ec273e622ce40eec855.pdf",
      "citation_key": "woczyk20220mn",
      "metadata": {
        "title": "Disentangling Transfer in Continual Reinforcement Learning",
        "authors": [
          "Maciej Woczyk",
          "Michal Zajkac",
          "Razvan Pascanu",
          "Lukasz Kuci'nski",
          "Piotr Milo's"
        ],
        "published_date": "2022",
        "abstract": "The ability of continual learning systems to transfer knowledge from previously seen tasks in order to maximize performance on new tasks is a significant challenge for the field, limiting the applicability of continual learning solutions to realistic scenarios. Consequently, this study aims to broaden our understanding of transfer and its driving forces in the specific case of continual reinforcement learning. We adopt SAC as the underlying RL algorithm and Continual World as a suite of continuous control tasks. We systematically study how different components of SAC (the actor and the critic, exploration, and data) affect transfer efficacy, and we provide recommendations regarding various modeling options. The best set of choices, dubbed ClonEx-SAC, is evaluated on the recent Continual World benchmark. ClonEx-SAC achieves 87% final success rate compared to 80% of PackNet, the best method in the benchmark. Moreover, the transfer grows from 0.18 to 0.54 according to the metric provided by Continual World.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/fc3b5dc5528e24dbbc8f4ec273e622ce40eec855.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 30,
        "score": 10.0,
        "summary": "The ability of continual learning systems to transfer knowledge from previously seen tasks in order to maximize performance on new tasks is a significant challenge for the field, limiting the applicability of continual learning solutions to realistic scenarios. Consequently, this study aims to broaden our understanding of transfer and its driving forces in the specific case of continual reinforcement learning. We adopt SAC as the underlying RL algorithm and Continual World as a suite of continuous control tasks. We systematically study how different components of SAC (the actor and the critic, exploration, and data) affect transfer efficacy, and we provide recommendations regarding various modeling options. The best set of choices, dubbed ClonEx-SAC, is evaluated on the recent Continual World benchmark. ClonEx-SAC achieves 87% final success rate compared to 80% of PackNet, the best method in the benchmark. Moreover, the transfer grows from 0.18 to 0.54 according to the metric provided by Continual World.",
        "keywords": []
      },
      "file_name": "fc3b5dc5528e24dbbc8f4ec273e622ce40eec855.pdf"
    },
    {
      "success": true,
      "doc_id": "b57666a170d6e4f4abc82af1be44a09e",
      "summary": "The rapid development of the aerospace industry puts forward the urgent need for the evolution of autonomous spacecraft rendezvous technology, which has gained significant attention recently due to increased applications in various space missions. This article studies the relative position tracking problem of the autonomous spacecraft rendezvous under the requirement of collision avoidance. An exploration-adaptive deep deterministic policy gradient (DDPG) algorithm is proposed to train a definite control strategy for this mission. Similar to the DDPG algorithm, four neural networks are used in this method, where two of them are used to generate the deterministic policy, whereas the other two are used to score the obtained policy. Differently, adaptive noise is introduced to reduce the possibility of oscillations and divergences and to cut down the unnecessary computation by weakening the exploration of stabilization problems. In addition, in order to effectively and quickly adapt to some other similar scenarios, a metalearning-based idea is introduced by fine-tuning the prior strategy. Finally, two numerical simulations show that the trained control strategy can effectively avoid the oscillation phenomenon caused by the artificial potential function. Benefiting from this, the trained control strategy based on deep reinforcement learning technology can decrease the energy consumption by 16.44% during the close proximity phase, compared with the traditional artificial potential function method. Besides, after introducing the metalearning-based idea, a strategy available for some other perturbed scenarios can be trained in a relatively short period of time, which illustrates its adaptability.",
      "intriguing_abstract": "The rapid development of the aerospace industry puts forward the urgent need for the evolution of autonomous spacecraft rendezvous technology, which has gained significant attention recently due to increased applications in various space missions. This article studies the relative position tracking problem of the autonomous spacecraft rendezvous under the requirement of collision avoidance. An exploration-adaptive deep deterministic policy gradient (DDPG) algorithm is proposed to train a definite control strategy for this mission. Similar to the DDPG algorithm, four neural networks are used in this method, where two of them are used to generate the deterministic policy, whereas the other two are used to score the obtained policy. Differently, adaptive noise is introduced to reduce the possibility of oscillations and divergences and to cut down the unnecessary computation by weakening the exploration of stabilization problems. In addition, in order to effectively and quickly adapt to some other similar scenarios, a metalearning-based idea is introduced by fine-tuning the prior strategy. Finally, two numerical simulations show that the trained control strategy can effectively avoid the oscillation phenomenon caused by the artificial potential function. Benefiting from this, the trained control strategy based on deep reinforcement learning technology can decrease the energy consumption by 16.44% during the close proximity phase, compared with the traditional artificial potential function method. Besides, after introducing the metalearning-based idea, a strategy available for some other perturbed scenarios can be trained in a relatively short period of time, which illustrates its adaptability.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/46eb68c585bdb8a1051dfda98b4b35610301264f.pdf",
      "citation_key": "qu2022uym",
      "metadata": {
        "title": "Spacecraft Proximity Maneuvering and Rendezvous With Collision Avoidance Based on Reinforcement Learning",
        "authors": [
          "Qingyu Qu",
          "Kexin Liu",
          "Wei Wang",
          "Jinhu Lu"
        ],
        "published_date": "2022",
        "abstract": "The rapid development of the aerospace industry puts forward the urgent need for the evolution of autonomous spacecraft rendezvous technology, which has gained significant attention recently due to increased applications in various space missions. This article studies the relative position tracking problem of the autonomous spacecraft rendezvous under the requirement of collision avoidance. An exploration-adaptive deep deterministic policy gradient (DDPG) algorithm is proposed to train a definite control strategy for this mission. Similar to the DDPG algorithm, four neural networks are used in this method, where two of them are used to generate the deterministic policy, whereas the other two are used to score the obtained policy. Differently, adaptive noise is introduced to reduce the possibility of oscillations and divergences and to cut down the unnecessary computation by weakening the exploration of stabilization problems. In addition, in order to effectively and quickly adapt to some other similar scenarios, a metalearning-based idea is introduced by fine-tuning the prior strategy. Finally, two numerical simulations show that the trained control strategy can effectively avoid the oscillation phenomenon caused by the artificial potential function. Benefiting from this, the trained control strategy based on deep reinforcement learning technology can decrease the energy consumption by 16.44% during the close proximity phase, compared with the traditional artificial potential function method. Besides, after introducing the metalearning-based idea, a strategy available for some other perturbed scenarios can be trained in a relatively short period of time, which illustrates its adaptability.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/46eb68c585bdb8a1051dfda98b4b35610301264f.pdf",
        "venue": "IEEE Transactions on Aerospace and Electronic Systems",
        "citationCount": 29,
        "score": 9.666666666666666,
        "summary": "The rapid development of the aerospace industry puts forward the urgent need for the evolution of autonomous spacecraft rendezvous technology, which has gained significant attention recently due to increased applications in various space missions. This article studies the relative position tracking problem of the autonomous spacecraft rendezvous under the requirement of collision avoidance. An exploration-adaptive deep deterministic policy gradient (DDPG) algorithm is proposed to train a definite control strategy for this mission. Similar to the DDPG algorithm, four neural networks are used in this method, where two of them are used to generate the deterministic policy, whereas the other two are used to score the obtained policy. Differently, adaptive noise is introduced to reduce the possibility of oscillations and divergences and to cut down the unnecessary computation by weakening the exploration of stabilization problems. In addition, in order to effectively and quickly adapt to some other similar scenarios, a metalearning-based idea is introduced by fine-tuning the prior strategy. Finally, two numerical simulations show that the trained control strategy can effectively avoid the oscillation phenomenon caused by the artificial potential function. Benefiting from this, the trained control strategy based on deep reinforcement learning technology can decrease the energy consumption by 16.44% during the close proximity phase, compared with the traditional artificial potential function method. Besides, after introducing the metalearning-based idea, a strategy available for some other perturbed scenarios can be trained in a relatively short period of time, which illustrates its adaptability.",
        "keywords": []
      },
      "file_name": "46eb68c585bdb8a1051dfda98b4b35610301264f.pdf"
    },
    {
      "success": true,
      "doc_id": "b93cf92ee3de8a99e1526e327f527ca8",
      "summary": "A central challenge in generative design is the exploration of vast number of solutions. In this work, we extend two major density-based structural topology optimization (STO) methods based on four classes of exploration algorithms of reinforcement learning (RL) to STO problems, which approaches generative design in a new way. The four methods are: first, using  -greedy policy to disturb the search direction; second, using upper confidence bound (UCB) to add a bonus on sensitivity; last, using Thompson sampling (TS) as well as information-directed sampling (IDS) to direct the search, where the posterior function of reward is fitted by Beta distribution or Gaussian distribution. Those combined methods are evaluated on some structure compliance minimization tasks from 2D to 3D, including the variable thickness design problem of an atmospheric diving suit (ADS). We show that all methods can generate various acceptable design options by varying one or two parameters simply, except that IDS fails to reach the convergence for complex structures due to the limitation of computation ability. We also show that both Beta distribution and Gaussian distribution work well to describe the posterior probability.",
      "intriguing_abstract": "A central challenge in generative design is the exploration of vast number of solutions. In this work, we extend two major density-based structural topology optimization (STO) methods based on four classes of exploration algorithms of reinforcement learning (RL) to STO problems, which approaches generative design in a new way. The four methods are: first, using  -greedy policy to disturb the search direction; second, using upper confidence bound (UCB) to add a bonus on sensitivity; last, using Thompson sampling (TS) as well as information-directed sampling (IDS) to direct the search, where the posterior function of reward is fitted by Beta distribution or Gaussian distribution. Those combined methods are evaluated on some structure compliance minimization tasks from 2D to 3D, including the variable thickness design problem of an atmospheric diving suit (ADS). We show that all methods can generate various acceptable design options by varying one or two parameters simply, except that IDS fails to reach the convergence for complex structures due to the limitation of computation ability. We also show that both Beta distribution and Gaussian distribution work well to describe the posterior probability.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/04efc9768a8e0c5f23b8c8504fb6db8803ffc071.pdf",
      "citation_key": "sun2020zjg",
      "metadata": {
        "title": "Generative Design by Using Exploration Approaches of Reinforcement Learning in Density-Based Structural Topology Optimization",
        "authors": [
          "H. Sun",
          "Ling Ma"
        ],
        "published_date": "2020",
        "abstract": "A central challenge in generative design is the exploration of vast number of solutions. In this work, we extend two major density-based structural topology optimization (STO) methods based on four classes of exploration algorithms of reinforcement learning (RL) to STO problems, which approaches generative design in a new way. The four methods are: first, using  -greedy policy to disturb the search direction; second, using upper confidence bound (UCB) to add a bonus on sensitivity; last, using Thompson sampling (TS) as well as information-directed sampling (IDS) to direct the search, where the posterior function of reward is fitted by Beta distribution or Gaussian distribution. Those combined methods are evaluated on some structure compliance minimization tasks from 2D to 3D, including the variable thickness design problem of an atmospheric diving suit (ADS). We show that all methods can generate various acceptable design options by varying one or two parameters simply, except that IDS fails to reach the convergence for complex structures due to the limitation of computation ability. We also show that both Beta distribution and Gaussian distribution work well to describe the posterior probability.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/04efc9768a8e0c5f23b8c8504fb6db8803ffc071.pdf",
        "venue": "Designs",
        "citationCount": 48,
        "score": 9.600000000000001,
        "summary": "A central challenge in generative design is the exploration of vast number of solutions. In this work, we extend two major density-based structural topology optimization (STO) methods based on four classes of exploration algorithms of reinforcement learning (RL) to STO problems, which approaches generative design in a new way. The four methods are: first, using  -greedy policy to disturb the search direction; second, using upper confidence bound (UCB) to add a bonus on sensitivity; last, using Thompson sampling (TS) as well as information-directed sampling (IDS) to direct the search, where the posterior function of reward is fitted by Beta distribution or Gaussian distribution. Those combined methods are evaluated on some structure compliance minimization tasks from 2D to 3D, including the variable thickness design problem of an atmospheric diving suit (ADS). We show that all methods can generate various acceptable design options by varying one or two parameters simply, except that IDS fails to reach the convergence for complex structures due to the limitation of computation ability. We also show that both Beta distribution and Gaussian distribution work well to describe the posterior probability.",
        "keywords": []
      },
      "file_name": "04efc9768a8e0c5f23b8c8504fb6db8803ffc071.pdf"
    },
    {
      "success": true,
      "doc_id": "55002c2794defa47cc8373448bde2b1a",
      "summary": "The increasing penetration of distributed energy resources and a large volume of unprecedented data from smart metering infrastructure can help consumers transit to an active role in the smart grid. In this article, we propose a human-machine reinforcement learning (RL) framework in the smart grid context to formulate an energy management strategy for electric vehicles and thermostatically controlled loads aggregators. The proposed model-free method accelerates the decision-making speed by substituting the conventional optimization process, and it is more capable of coping with the diverse system environment via online learning. The human intervention is coordinated with machine learning to: 1) prevent the huge loss during the learning process; 2) realize emergency control; and 3) find preferable control policy. The performance of the proposed human-machine RL framework is verified in case studies. It can be concluded that our proposed method performs better than the conventional deep Q-learning and deep deterministic policy gradient in terms of convergence capability and preferable result exploration. Besides, the proposed method can better deal with emergent events, such as a sudden drop of photovoltaic (PV) output. Compared with the conventional model-based method, there are slight deviations between our method and the optimal solution, but the decision-making time is significantly reduced.",
      "intriguing_abstract": "The increasing penetration of distributed energy resources and a large volume of unprecedented data from smart metering infrastructure can help consumers transit to an active role in the smart grid. In this article, we propose a human-machine reinforcement learning (RL) framework in the smart grid context to formulate an energy management strategy for electric vehicles and thermostatically controlled loads aggregators. The proposed model-free method accelerates the decision-making speed by substituting the conventional optimization process, and it is more capable of coping with the diverse system environment via online learning. The human intervention is coordinated with machine learning to: 1) prevent the huge loss during the learning process; 2) realize emergency control; and 3) find preferable control policy. The performance of the proposed human-machine RL framework is verified in case studies. It can be concluded that our proposed method performs better than the conventional deep Q-learning and deep deterministic policy gradient in terms of convergence capability and preferable result exploration. Besides, the proposed method can better deal with emergent events, such as a sudden drop of photovoltaic (PV) output. Compared with the conventional model-based method, there are slight deviations between our method and the optimal solution, but the decision-making time is significantly reduced.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/4df0238d5bfe0ab4cb8eaba67f4388c4e02dc2c8.pdf",
      "citation_key": "tao202294e",
      "metadata": {
        "title": "A Human-Machine Reinforcement Learning Method for Cooperative Energy Management",
        "authors": [
          "Yuechuan Tao",
          "Jing Qiu",
          "Shuying Lai",
          "Xian Zhang",
          "Yunqi Wang",
          "Guibin Wang"
        ],
        "published_date": "2022",
        "abstract": "The increasing penetration of distributed energy resources and a large volume of unprecedented data from smart metering infrastructure can help consumers transit to an active role in the smart grid. In this article, we propose a human-machine reinforcement learning (RL) framework in the smart grid context to formulate an energy management strategy for electric vehicles and thermostatically controlled loads aggregators. The proposed model-free method accelerates the decision-making speed by substituting the conventional optimization process, and it is more capable of coping with the diverse system environment via online learning. The human intervention is coordinated with machine learning to: 1) prevent the huge loss during the learning process; 2) realize emergency control; and 3) find preferable control policy. The performance of the proposed human-machine RL framework is verified in case studies. It can be concluded that our proposed method performs better than the conventional deep Q-learning and deep deterministic policy gradient in terms of convergence capability and preferable result exploration. Besides, the proposed method can better deal with emergent events, such as a sudden drop of photovoltaic (PV) output. Compared with the conventional model-based method, there are slight deviations between our method and the optimal solution, but the decision-making time is significantly reduced.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/4df0238d5bfe0ab4cb8eaba67f4388c4e02dc2c8.pdf",
        "venue": "IEEE Transactions on Industrial Informatics",
        "citationCount": 28,
        "score": 9.333333333333332,
        "summary": "The increasing penetration of distributed energy resources and a large volume of unprecedented data from smart metering infrastructure can help consumers transit to an active role in the smart grid. In this article, we propose a human-machine reinforcement learning (RL) framework in the smart grid context to formulate an energy management strategy for electric vehicles and thermostatically controlled loads aggregators. The proposed model-free method accelerates the decision-making speed by substituting the conventional optimization process, and it is more capable of coping with the diverse system environment via online learning. The human intervention is coordinated with machine learning to: 1) prevent the huge loss during the learning process; 2) realize emergency control; and 3) find preferable control policy. The performance of the proposed human-machine RL framework is verified in case studies. It can be concluded that our proposed method performs better than the conventional deep Q-learning and deep deterministic policy gradient in terms of convergence capability and preferable result exploration. Besides, the proposed method can better deal with emergent events, such as a sudden drop of photovoltaic (PV) output. Compared with the conventional model-based method, there are slight deviations between our method and the optimal solution, but the decision-making time is significantly reduced.",
        "keywords": []
      },
      "file_name": "4df0238d5bfe0ab4cb8eaba67f4388c4e02dc2c8.pdf"
    },
    {
      "success": true,
      "doc_id": "ded80f612662a43cebd0318fa1f500b9",
      "summary": "Scalable and effective exploration remains a key challenge in reinforcement learning (RL). While there are methods with optimality guarantees in the setting of discrete state and action spaces, these methods cannot be applied in high-dimensional deep RL scenarios. As such, most contemporary RL relies on simple heuristics such as epsilon-greedy exploration or adding Gaussian noise to the controls. This paper introduces Variational Information Maximizing Exploration (VIME), an exploration strategy based on maximization of information gain about the agent's belief of environment dynamics. We propose a practical implementation, using variational inference in Bayesian neural networks which efficiently handles continuous state and action spaces. VIME modifies the MDP reward function, and can be applied with several different underlying RL algorithms. We demonstrate that VIME achieves significantly better performance compared to heuristic exploration methods across a variety of continuous control tasks and algorithms, including tasks with very sparse rewards.",
      "intriguing_abstract": "Scalable and effective exploration remains a key challenge in reinforcement learning (RL). While there are methods with optimality guarantees in the setting of discrete state and action spaces, these methods cannot be applied in high-dimensional deep RL scenarios. As such, most contemporary RL relies on simple heuristics such as epsilon-greedy exploration or adding Gaussian noise to the controls. This paper introduces Variational Information Maximizing Exploration (VIME), an exploration strategy based on maximization of information gain about the agent's belief of environment dynamics. We propose a practical implementation, using variational inference in Bayesian neural networks which efficiently handles continuous state and action spaces. VIME modifies the MDP reward function, and can be applied with several different underlying RL algorithms. We demonstrate that VIME achieves significantly better performance compared to heuristic exploration methods across a variety of continuous control tasks and algorithms, including tasks with very sparse rewards.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/fb3c6456708b0e143f545d77dc8ec804eb947395.pdf",
      "citation_key": "houthooft2016yee",
      "metadata": {
        "title": "Curiosity-driven Exploration in Deep Reinforcement Learning via Bayesian Neural Networks",
        "authors": [
          "Rein Houthooft",
          "Xi Chen",
          "Yan Duan",
          "John Schulman",
          "F. Turck",
          "P. Abbeel"
        ],
        "published_date": "2016",
        "abstract": "Scalable and effective exploration remains a key challenge in reinforcement learning (RL). While there are methods with optimality guarantees in the setting of discrete state and action spaces, these methods cannot be applied in high-dimensional deep RL scenarios. As such, most contemporary RL relies on simple heuristics such as epsilon-greedy exploration or adding Gaussian noise to the controls. This paper introduces Variational Information Maximizing Exploration (VIME), an exploration strategy based on maximization of information gain about the agent's belief of environment dynamics. We propose a practical implementation, using variational inference in Bayesian neural networks which efficiently handles continuous state and action spaces. VIME modifies the MDP reward function, and can be applied with several different underlying RL algorithms. We demonstrate that VIME achieves significantly better performance compared to heuristic exploration methods across a variety of continuous control tasks and algorithms, including tasks with very sparse rewards.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/fb3c6456708b0e143f545d77dc8ec804eb947395.pdf",
        "venue": "arXiv.org",
        "citationCount": 79,
        "score": 8.777777777777777,
        "summary": "Scalable and effective exploration remains a key challenge in reinforcement learning (RL). While there are methods with optimality guarantees in the setting of discrete state and action spaces, these methods cannot be applied in high-dimensional deep RL scenarios. As such, most contemporary RL relies on simple heuristics such as epsilon-greedy exploration or adding Gaussian noise to the controls. This paper introduces Variational Information Maximizing Exploration (VIME), an exploration strategy based on maximization of information gain about the agent's belief of environment dynamics. We propose a practical implementation, using variational inference in Bayesian neural networks which efficiently handles continuous state and action spaces. VIME modifies the MDP reward function, and can be applied with several different underlying RL algorithms. We demonstrate that VIME achieves significantly better performance compared to heuristic exploration methods across a variety of continuous control tasks and algorithms, including tasks with very sparse rewards.",
        "keywords": []
      },
      "file_name": "fb3c6456708b0e143f545d77dc8ec804eb947395.pdf"
    },
    {
      "success": true,
      "doc_id": "861ed5c16aff015dabd7e0f739fe5375",
      "summary": "Reinforcement learning (RL) is a powerful data-driven control method that has been largely explored in autonomous driving tasks. However, conventional RL approaches learn control policies through trial-and-error interactions with the environment and therefore may cause disastrous consequences such as collisions when testing in real-world traffic. Offline RL has recently emerged as a promising framework to learn effective policies from previously-collected, static datasets without the requirement of active interactions, making it especially appealing for autonomous driving applications. Despite promising, existing offline RL algorithms such as Batch-Constrained deep Q-learning (BCQ) generally lead to rather conservative policies with limited exploration efficiency. To address such issues, this paper presents an enhanced BCQ algorithm by employing a learnable parameter noise scheme in the perturbation model to increase the diversity of observed actions. In addition, a Lyapunov-based safety enhancement strategy is incorporated to constrain the explorable state space within a safe region. Experimental results in highway and parking traffic scenarios show that our approach outperforms the conventional RL method, as well as state-of-the-art offline RL algorithms.",
      "intriguing_abstract": "Reinforcement learning (RL) is a powerful data-driven control method that has been largely explored in autonomous driving tasks. However, conventional RL approaches learn control policies through trial-and-error interactions with the environment and therefore may cause disastrous consequences such as collisions when testing in real-world traffic. Offline RL has recently emerged as a promising framework to learn effective policies from previously-collected, static datasets without the requirement of active interactions, making it especially appealing for autonomous driving applications. Despite promising, existing offline RL algorithms such as Batch-Constrained deep Q-learning (BCQ) generally lead to rather conservative policies with limited exploration efficiency. To address such issues, this paper presents an enhanced BCQ algorithm by employing a learnable parameter noise scheme in the perturbation model to increase the diversity of observed actions. In addition, a Lyapunov-based safety enhancement strategy is incorporated to constrain the explorable state space within a safe region. Experimental results in highway and parking traffic scenarios show that our approach outperforms the conventional RL method, as well as state-of-the-art offline RL algorithms.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/248a25d697fe0132840e9d03c00aefadf03408d8.pdf",
      "citation_key": "shi20215fg",
      "metadata": {
        "title": "Offline Reinforcement Learning for Autonomous Driving with Safety and Exploration Enhancement",
        "authors": [
          "Tianyu Shi",
          "Dong Chen",
          "Kaian Chen",
          "Zhaojian Li"
        ],
        "published_date": "2021",
        "abstract": "Reinforcement learning (RL) is a powerful data-driven control method that has been largely explored in autonomous driving tasks. However, conventional RL approaches learn control policies through trial-and-error interactions with the environment and therefore may cause disastrous consequences such as collisions when testing in real-world traffic. Offline RL has recently emerged as a promising framework to learn effective policies from previously-collected, static datasets without the requirement of active interactions, making it especially appealing for autonomous driving applications. Despite promising, existing offline RL algorithms such as Batch-Constrained deep Q-learning (BCQ) generally lead to rather conservative policies with limited exploration efficiency. To address such issues, this paper presents an enhanced BCQ algorithm by employing a learnable parameter noise scheme in the perturbation model to increase the diversity of observed actions. In addition, a Lyapunov-based safety enhancement strategy is incorporated to constrain the explorable state space within a safe region. Experimental results in highway and parking traffic scenarios show that our approach outperforms the conventional RL method, as well as state-of-the-art offline RL algorithms.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/248a25d697fe0132840e9d03c00aefadf03408d8.pdf",
        "venue": "arXiv.org",
        "citationCount": 35,
        "score": 8.75,
        "summary": "Reinforcement learning (RL) is a powerful data-driven control method that has been largely explored in autonomous driving tasks. However, conventional RL approaches learn control policies through trial-and-error interactions with the environment and therefore may cause disastrous consequences such as collisions when testing in real-world traffic. Offline RL has recently emerged as a promising framework to learn effective policies from previously-collected, static datasets without the requirement of active interactions, making it especially appealing for autonomous driving applications. Despite promising, existing offline RL algorithms such as Batch-Constrained deep Q-learning (BCQ) generally lead to rather conservative policies with limited exploration efficiency. To address such issues, this paper presents an enhanced BCQ algorithm by employing a learnable parameter noise scheme in the perturbation model to increase the diversity of observed actions. In addition, a Lyapunov-based safety enhancement strategy is incorporated to constrain the explorable state space within a safe region. Experimental results in highway and parking traffic scenarios show that our approach outperforms the conventional RL method, as well as state-of-the-art offline RL algorithms.",
        "keywords": []
      },
      "file_name": "248a25d697fe0132840e9d03c00aefadf03408d8.pdf"
    },
    {
      "success": true,
      "doc_id": "ef515ca2089f8982d35dc9eef6e8ceb2",
      "summary": "The unmanned aerial vehicles (UAVs) networks are very vulnerable to smart jammers that can choose their jamming strategy based on the ongoing channel state accordingly. Although reinforcement learning (RL) algorithms can give UAV networks the ability to make intelligent decisions, the high-dimensional state space makes it difficult for algorithms to converge quickly. This article proposes a knowledge-based RL method, which uses domain knowledge to compress the state space that the agent needs to explore and then improve the algorithm convergence speed. Specifically, we use the inertial law of the aircraft and the law of signal attenuation in free space to guide the highly efficient exploration of the UAVs in the state space. We incorporate the performance indicators of the receiver and the subjective value of the task into the design of the reward function, and build a virtual environment for pretraining to accelerate the convergence of anti-jamming decisions. In addition, the algorithm proposed is completely based on observable data, which is more realistic than those studies that assume the position or the channel strategy of the jammer. The simulation shows that the proposed algorithm can outperform the benchmarks of model-free RL algorithm in terms of converge speed and averaged reward.",
      "intriguing_abstract": "The unmanned aerial vehicles (UAVs) networks are very vulnerable to smart jammers that can choose their jamming strategy based on the ongoing channel state accordingly. Although reinforcement learning (RL) algorithms can give UAV networks the ability to make intelligent decisions, the high-dimensional state space makes it difficult for algorithms to converge quickly. This article proposes a knowledge-based RL method, which uses domain knowledge to compress the state space that the agent needs to explore and then improve the algorithm convergence speed. Specifically, we use the inertial law of the aircraft and the law of signal attenuation in free space to guide the highly efficient exploration of the UAVs in the state space. We incorporate the performance indicators of the receiver and the subjective value of the task into the design of the reward function, and build a virtual environment for pretraining to accelerate the convergence of anti-jamming decisions. In addition, the algorithm proposed is completely based on observable data, which is more realistic than those studies that assume the position or the channel strategy of the jammer. The simulation shows that the proposed algorithm can outperform the benchmarks of model-free RL algorithm in terms of converge speed and averaged reward.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/7d1722451bff3b95e1d082864bb9b8437a0ddf41.pdf",
      "citation_key": "li2021l92",
      "metadata": {
        "title": "UAV Networks Against Multiple Maneuvering Smart Jamming With Knowledge-Based Reinforcement Learning",
        "authors": [
          "Zhiwei Li",
          "Yu Lu",
          "Xi Li",
          "Zeng-Guang Wang",
          "Wenxin Qiao",
          "Yicen Liu"
        ],
        "published_date": "2021",
        "abstract": "The unmanned aerial vehicles (UAVs) networks are very vulnerable to smart jammers that can choose their jamming strategy based on the ongoing channel state accordingly. Although reinforcement learning (RL) algorithms can give UAV networks the ability to make intelligent decisions, the high-dimensional state space makes it difficult for algorithms to converge quickly. This article proposes a knowledge-based RL method, which uses domain knowledge to compress the state space that the agent needs to explore and then improve the algorithm convergence speed. Specifically, we use the inertial law of the aircraft and the law of signal attenuation in free space to guide the highly efficient exploration of the UAVs in the state space. We incorporate the performance indicators of the receiver and the subjective value of the task into the design of the reward function, and build a virtual environment for pretraining to accelerate the convergence of anti-jamming decisions. In addition, the algorithm proposed is completely based on observable data, which is more realistic than those studies that assume the position or the channel strategy of the jammer. The simulation shows that the proposed algorithm can outperform the benchmarks of model-free RL algorithm in terms of converge speed and averaged reward.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/7d1722451bff3b95e1d082864bb9b8437a0ddf41.pdf",
        "venue": "IEEE Internet of Things Journal",
        "citationCount": 35,
        "score": 8.75,
        "summary": "The unmanned aerial vehicles (UAVs) networks are very vulnerable to smart jammers that can choose their jamming strategy based on the ongoing channel state accordingly. Although reinforcement learning (RL) algorithms can give UAV networks the ability to make intelligent decisions, the high-dimensional state space makes it difficult for algorithms to converge quickly. This article proposes a knowledge-based RL method, which uses domain knowledge to compress the state space that the agent needs to explore and then improve the algorithm convergence speed. Specifically, we use the inertial law of the aircraft and the law of signal attenuation in free space to guide the highly efficient exploration of the UAVs in the state space. We incorporate the performance indicators of the receiver and the subjective value of the task into the design of the reward function, and build a virtual environment for pretraining to accelerate the convergence of anti-jamming decisions. In addition, the algorithm proposed is completely based on observable data, which is more realistic than those studies that assume the position or the channel strategy of the jammer. The simulation shows that the proposed algorithm can outperform the benchmarks of model-free RL algorithm in terms of converge speed and averaged reward.",
        "keywords": []
      },
      "file_name": "7d1722451bff3b95e1d082864bb9b8437a0ddf41.pdf"
    },
    {
      "success": true,
      "doc_id": "1f17b20651ecc4da04f071837e9f5b11",
      "summary": "Sufficient good features are indispensable to train well-performed machine learning models. However, it is com-mon that good features are not always enough, where feature augmentation is necessary to enrich high-quality features by joining with other tables. There are two main challenges for the problem. Given a set of tables where we can augment features from, the first challenge is that there are a lot of ways of joining multiple tables and deciding which features (or attributes) to use - selecting the best set of features to augment is hard. Moreover, we may need to materialize the join results for different join options, doing full materialization might be time consuming - efficient but approximate methods are needed. In this paper, we first introduce the design space of the feature augmentation problem. Then, to address the above challenges, we propose a reinforcement learning based framework, namely AutoFeature, to augment the features following an exploration-exploitation strategy. AutoFeature keeps exploring the features in tables that have led to performance improvement. At the same time, AutoFeature also exploits the tables (features) that are rarely selected. In this way, the search space of tables (features) to be augmented can be well explored and a subset of good features can be selected. AutoFeature utilizes sampling techniques to achieve high efficiency. We implement two algorithms, one with multi-arm bandit and the other with branch Deep Q Networks (branch DQN), to realize the framework of AutoFeature. We conducted experiments on three real-world datasets School/XuetangE/Air using 16/23/34 candidate tables with 695/204/338 candidate features. Extensive results show that AutoFeature outperforms other methods by 12.4% and 9.8% on AUC values on two classification datasets (School and XuetangE) and by 0.113 on the MSE value on Air in terms of the model performance.",
      "intriguing_abstract": "Sufficient good features are indispensable to train well-performed machine learning models. However, it is com-mon that good features are not always enough, where feature augmentation is necessary to enrich high-quality features by joining with other tables. There are two main challenges for the problem. Given a set of tables where we can augment features from, the first challenge is that there are a lot of ways of joining multiple tables and deciding which features (or attributes) to use - selecting the best set of features to augment is hard. Moreover, we may need to materialize the join results for different join options, doing full materialization might be time consuming - efficient but approximate methods are needed. In this paper, we first introduce the design space of the feature augmentation problem. Then, to address the above challenges, we propose a reinforcement learning based framework, namely AutoFeature, to augment the features following an exploration-exploitation strategy. AutoFeature keeps exploring the features in tables that have led to performance improvement. At the same time, AutoFeature also exploits the tables (features) that are rarely selected. In this way, the search space of tables (features) to be augmented can be well explored and a subset of good features can be selected. AutoFeature utilizes sampling techniques to achieve high efficiency. We implement two algorithms, one with multi-arm bandit and the other with branch Deep Q Networks (branch DQN), to realize the framework of AutoFeature. We conducted experiments on three real-world datasets School/XuetangE/Air using 16/23/34 candidate tables with 695/204/338 candidate features. Extensive results show that AutoFeature outperforms other methods by 12.4% and 9.8% on AUC values on two classification datasets (School and XuetangE) and by 0.113 on the MSE value on Air in terms of the model performance.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/b0d376434a528ee69d98174d75b4a571c53247ae.pdf",
      "citation_key": "liu20220g4",
      "metadata": {
        "title": "Feature Augmentation with Reinforcement Learning",
        "authors": [
          "Jiabin Liu",
          "Chengliang Chai",
          "Yuyu Luo",
          "Yin Lou",
          "Jianhua Feng",
          "Nan Tang"
        ],
        "published_date": "2022",
        "abstract": "Sufficient good features are indispensable to train well-performed machine learning models. However, it is com-mon that good features are not always enough, where feature augmentation is necessary to enrich high-quality features by joining with other tables. There are two main challenges for the problem. Given a set of tables where we can augment features from, the first challenge is that there are a lot of ways of joining multiple tables and deciding which features (or attributes) to use - selecting the best set of features to augment is hard. Moreover, we may need to materialize the join results for different join options, doing full materialization might be time consuming - efficient but approximate methods are needed. In this paper, we first introduce the design space of the feature augmentation problem. Then, to address the above challenges, we propose a reinforcement learning based framework, namely AutoFeature, to augment the features following an exploration-exploitation strategy. AutoFeature keeps exploring the features in tables that have led to performance improvement. At the same time, AutoFeature also exploits the tables (features) that are rarely selected. In this way, the search space of tables (features) to be augmented can be well explored and a subset of good features can be selected. AutoFeature utilizes sampling techniques to achieve high efficiency. We implement two algorithms, one with multi-arm bandit and the other with branch Deep Q Networks (branch DQN), to realize the framework of AutoFeature. We conducted experiments on three real-world datasets School/XuetangE/Air using 16/23/34 candidate tables with 695/204/338 candidate features. Extensive results show that AutoFeature outperforms other methods by 12.4% and 9.8% on AUC values on two classification datasets (School and XuetangE) and by 0.113 on the MSE value on Air in terms of the model performance.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/b0d376434a528ee69d98174d75b4a571c53247ae.pdf",
        "venue": "IEEE International Conference on Data Engineering",
        "citationCount": 26,
        "score": 8.666666666666666,
        "summary": "Sufficient good features are indispensable to train well-performed machine learning models. However, it is com-mon that good features are not always enough, where feature augmentation is necessary to enrich high-quality features by joining with other tables. There are two main challenges for the problem. Given a set of tables where we can augment features from, the first challenge is that there are a lot of ways of joining multiple tables and deciding which features (or attributes) to use - selecting the best set of features to augment is hard. Moreover, we may need to materialize the join results for different join options, doing full materialization might be time consuming - efficient but approximate methods are needed. In this paper, we first introduce the design space of the feature augmentation problem. Then, to address the above challenges, we propose a reinforcement learning based framework, namely AutoFeature, to augment the features following an exploration-exploitation strategy. AutoFeature keeps exploring the features in tables that have led to performance improvement. At the same time, AutoFeature also exploits the tables (features) that are rarely selected. In this way, the search space of tables (features) to be augmented can be well explored and a subset of good features can be selected. AutoFeature utilizes sampling techniques to achieve high efficiency. We implement two algorithms, one with multi-arm bandit and the other with branch Deep Q Networks (branch DQN), to realize the framework of AutoFeature. We conducted experiments on three real-world datasets School/XuetangE/Air using 16/23/34 candidate tables with 695/204/338 candidate features. Extensive results show that AutoFeature outperforms other methods by 12.4% and 9.8% on AUC values on two classification datasets (School and XuetangE) and by 0.113 on the MSE value on Air in terms of the model performance.",
        "keywords": []
      },
      "file_name": "b0d376434a528ee69d98174d75b4a571c53247ae.pdf"
    },
    {
      "success": true,
      "doc_id": "dbe35b7d71d0fe7cc42cbbe559597814",
      "summary": "This paper studies the plume-tracing strategy for an autonomous underwater vehicle (AUV) in the deep-sea turbulent environment. The tracing problem is modeled as a partially observable Markov decision process with continuous state space and action space due to the spatio-temporal changes of environment. An long short-term memory-based reinforcement learning framework with full use of history information is proposed to generate a smooth strategy while the AUV interacting with the environment. Continuous temporal difference and deterministic policy gradient methods are employed to improve the strategy. To promote the performance of the algorithm, a supervised strategy generated by dynamic programming methods is utilized as transcendental knowledge of the agent. Historical searching trajectorys form and the exploration technology are specially designed to fit the algorithm. Simulation environments are established based on Reynolds-averaged NavierStokes equations and the effectiveness of the learned plume-tracing strategy is validated with simulation experiments.",
      "intriguing_abstract": "This paper studies the plume-tracing strategy for an autonomous underwater vehicle (AUV) in the deep-sea turbulent environment. The tracing problem is modeled as a partially observable Markov decision process with continuous state space and action space due to the spatio-temporal changes of environment. An long short-term memory-based reinforcement learning framework with full use of history information is proposed to generate a smooth strategy while the AUV interacting with the environment. Continuous temporal difference and deterministic policy gradient methods are employed to improve the strategy. To promote the performance of the algorithm, a supervised strategy generated by dynamic programming methods is utilized as transcendental knowledge of the agent. Historical searching trajectorys form and the exploration technology are specially designed to fit the algorithm. Simulation environments are established based on Reynolds-averaged NavierStokes equations and the effectiveness of the learned plume-tracing strategy is validated with simulation experiments.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/2822e79cb293f54e05fd8a7cd2844cfcc683f5d8.pdf",
      "citation_key": "hu20195n2",
      "metadata": {
        "title": "Plume Tracing via Model-Free Reinforcement Learning Method",
        "authors": [
          "Hangkai Hu",
          "Shiji Song",
          "C. L. Phillip Chen"
        ],
        "published_date": "2019",
        "abstract": "This paper studies the plume-tracing strategy for an autonomous underwater vehicle (AUV) in the deep-sea turbulent environment. The tracing problem is modeled as a partially observable Markov decision process with continuous state space and action space due to the spatio-temporal changes of environment. An long short-term memory-based reinforcement learning framework with full use of history information is proposed to generate a smooth strategy while the AUV interacting with the environment. Continuous temporal difference and deterministic policy gradient methods are employed to improve the strategy. To promote the performance of the algorithm, a supervised strategy generated by dynamic programming methods is utilized as transcendental knowledge of the agent. Historical searching trajectorys form and the exploration technology are specially designed to fit the algorithm. Simulation environments are established based on Reynolds-averaged NavierStokes equations and the effectiveness of the learned plume-tracing strategy is validated with simulation experiments.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/2822e79cb293f54e05fd8a7cd2844cfcc683f5d8.pdf",
        "venue": "IEEE Transactions on Neural Networks and Learning Systems",
        "citationCount": 50,
        "score": 8.333333333333332,
        "summary": "This paper studies the plume-tracing strategy for an autonomous underwater vehicle (AUV) in the deep-sea turbulent environment. The tracing problem is modeled as a partially observable Markov decision process with continuous state space and action space due to the spatio-temporal changes of environment. An long short-term memory-based reinforcement learning framework with full use of history information is proposed to generate a smooth strategy while the AUV interacting with the environment. Continuous temporal difference and deterministic policy gradient methods are employed to improve the strategy. To promote the performance of the algorithm, a supervised strategy generated by dynamic programming methods is utilized as transcendental knowledge of the agent. Historical searching trajectorys form and the exploration technology are specially designed to fit the algorithm. Simulation environments are established based on Reynolds-averaged NavierStokes equations and the effectiveness of the learned plume-tracing strategy is validated with simulation experiments.",
        "keywords": []
      },
      "file_name": "2822e79cb293f54e05fd8a7cd2844cfcc683f5d8.pdf"
    },
    {
      "success": true,
      "doc_id": "eaa2b648e653d2365e37821d910b04c9",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/fed0701afdfa6896057f7d04bd30ab1328eff110.pdf",
      "citation_key": "wang2022boj",
      "metadata": {
        "title": "Evolutionary Diversity Optimization with Clustering-based Selection for Reinforcement Learning",
        "authors": [
          "Yutong Wang",
          "Ke Xue",
          "Chaojun Qian"
        ],
        "published_date": "2022",
        "abstract": "",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/fed0701afdfa6896057f7d04bd30ab1328eff110.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 25,
        "score": 8.333333333333332,
        "summary": "",
        "keywords": []
      },
      "file_name": "fed0701afdfa6896057f7d04bd30ab1328eff110.pdf"
    },
    {
      "success": true,
      "doc_id": "900c27ff29649035b88111b3be6556bb",
      "summary": "We tackle the problem of cooperative visual exploration where multiple agents need to jointly explore unseen regions as fast as possible based on visual signals. Classical planning-based methods often suffer from expensive computation overhead at each step and a limited expressiveness of complex cooperation strategy. By contrast, reinforcement learning (RL) has recently become a popular paradigm for tackling this challenge due to its modeling capability of arbitrarily complex strategies and minimal inference overhead. In this paper, we extend the state-of-the-art single-agent visual navigation method, Active Neural SLAM (ANS), to the multi-agent setting by introducing a novel RL-based planning module, Multi-agent Spatial Planner (MSP).MSP leverages a transformer-based architecture, Spatial-TeamFormer, which effectively captures spatial relations and intra-agent interactions via hierarchical spatial self-attentions. In addition, we also implement a few multi-agent enhancements to process local information from each agent for an aligned spatial representation and more precise planning. Finally, we perform policy distillation to extract a meta policy to significantly improve the generalization capability of final policy. We call this overall solution, Multi-Agent Active Neural SLAM (MAANS). MAANS substantially outperforms classical planning-based baselines for the first time in a photo-realistic 3D simulator, Habitat. Code and videos can be found at https://sites.google.com/view/maans.",
      "intriguing_abstract": "We tackle the problem of cooperative visual exploration where multiple agents need to jointly explore unseen regions as fast as possible based on visual signals. Classical planning-based methods often suffer from expensive computation overhead at each step and a limited expressiveness of complex cooperation strategy. By contrast, reinforcement learning (RL) has recently become a popular paradigm for tackling this challenge due to its modeling capability of arbitrarily complex strategies and minimal inference overhead. In this paper, we extend the state-of-the-art single-agent visual navigation method, Active Neural SLAM (ANS), to the multi-agent setting by introducing a novel RL-based planning module, Multi-agent Spatial Planner (MSP).MSP leverages a transformer-based architecture, Spatial-TeamFormer, which effectively captures spatial relations and intra-agent interactions via hierarchical spatial self-attentions. In addition, we also implement a few multi-agent enhancements to process local information from each agent for an aligned spatial representation and more precise planning. Finally, we perform policy distillation to extract a meta policy to significantly improve the generalization capability of final policy. We call this overall solution, Multi-Agent Active Neural SLAM (MAANS). MAANS substantially outperforms classical planning-based baselines for the first time in a photo-realistic 3D simulator, Habitat. Code and videos can be found at https://sites.google.com/view/maans.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/813f6e34feb3dc0346b6392d061af12ff186ba7e.pdf",
      "citation_key": "yu20213c1",
      "metadata": {
        "title": "Learning Efficient Multi-Agent Cooperative Visual Exploration",
        "authors": [
          "Chao Yu",
          "Xinyi Yang",
          "Jiaxuan Gao",
          "Huazhong Yang",
          "Yu Wang",
          "Yi Wu"
        ],
        "published_date": "2021",
        "abstract": "We tackle the problem of cooperative visual exploration where multiple agents need to jointly explore unseen regions as fast as possible based on visual signals. Classical planning-based methods often suffer from expensive computation overhead at each step and a limited expressiveness of complex cooperation strategy. By contrast, reinforcement learning (RL) has recently become a popular paradigm for tackling this challenge due to its modeling capability of arbitrarily complex strategies and minimal inference overhead. In this paper, we extend the state-of-the-art single-agent visual navigation method, Active Neural SLAM (ANS), to the multi-agent setting by introducing a novel RL-based planning module, Multi-agent Spatial Planner (MSP).MSP leverages a transformer-based architecture, Spatial-TeamFormer, which effectively captures spatial relations and intra-agent interactions via hierarchical spatial self-attentions. In addition, we also implement a few multi-agent enhancements to process local information from each agent for an aligned spatial representation and more precise planning. Finally, we perform policy distillation to extract a meta policy to significantly improve the generalization capability of final policy. We call this overall solution, Multi-Agent Active Neural SLAM (MAANS). MAANS substantially outperforms classical planning-based baselines for the first time in a photo-realistic 3D simulator, Habitat. Code and videos can be found at https://sites.google.com/view/maans.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/813f6e34feb3dc0346b6392d061af12ff186ba7e.pdf",
        "venue": "European Conference on Computer Vision",
        "citationCount": 32,
        "score": 8.0,
        "summary": "We tackle the problem of cooperative visual exploration where multiple agents need to jointly explore unseen regions as fast as possible based on visual signals. Classical planning-based methods often suffer from expensive computation overhead at each step and a limited expressiveness of complex cooperation strategy. By contrast, reinforcement learning (RL) has recently become a popular paradigm for tackling this challenge due to its modeling capability of arbitrarily complex strategies and minimal inference overhead. In this paper, we extend the state-of-the-art single-agent visual navigation method, Active Neural SLAM (ANS), to the multi-agent setting by introducing a novel RL-based planning module, Multi-agent Spatial Planner (MSP).MSP leverages a transformer-based architecture, Spatial-TeamFormer, which effectively captures spatial relations and intra-agent interactions via hierarchical spatial self-attentions. In addition, we also implement a few multi-agent enhancements to process local information from each agent for an aligned spatial representation and more precise planning. Finally, we perform policy distillation to extract a meta policy to significantly improve the generalization capability of final policy. We call this overall solution, Multi-Agent Active Neural SLAM (MAANS). MAANS substantially outperforms classical planning-based baselines for the first time in a photo-realistic 3D simulator, Habitat. Code and videos can be found at https://sites.google.com/view/maans.",
        "keywords": []
      },
      "file_name": "813f6e34feb3dc0346b6392d061af12ff186ba7e.pdf"
    },
    {
      "success": true,
      "doc_id": "aa62f9cf60faa6fc25654cfcf40bd482",
      "summary": "Deep reinforcement learning (RL) is capable of identifying and modifying strategies for active flow control. However, the classic active formulation of deep RL requires lengthy active exploration. This paper describes the introduction of expert demonstration into a classic off-policy RL algorithm, the soft actor-critic algorithm, for application to vortex-induced vibration problems. This combined online-learning framework is applied to an oscillator wake environment and a Navier--Stokes environment, with expert demonstration obtained from the pole-placement method and surrogate model optimization. The results show that the soft actor--critic framework combined with expert demonstration enables rapid learning of active flow control strategies through a combination of prior demonstration data and online experience. The present study develops a new data-efficient RL approach for discovering active flow control strategies for vortex-induced vibration, providing a more practical methodology for industrial applications.",
      "intriguing_abstract": "Deep reinforcement learning (RL) is capable of identifying and modifying strategies for active flow control. However, the classic active formulation of deep RL requires lengthy active exploration. This paper describes the introduction of expert demonstration into a classic off-policy RL algorithm, the soft actor-critic algorithm, for application to vortex-induced vibration problems. This combined online-learning framework is applied to an oscillator wake environment and a Navier--Stokes environment, with expert demonstration obtained from the pole-placement method and surrogate model optimization. The results show that the soft actor--critic framework combined with expert demonstration enables rapid learning of active flow control strategies through a combination of prior demonstration data and online experience. The present study develops a new data-efficient RL approach for discovering active flow control strategies for vortex-induced vibration, providing a more practical methodology for industrial applications.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/21828b3f05acffbfdf7de7259ad8b8ebd57fa351.pdf",
      "citation_key": "zheng2022816",
      "metadata": {
        "title": "Data-efficient deep reinforcement learning with expert demonstration for active flow control",
        "authors": [
          "Changdong Zheng",
          "Fangfang Xie",
          "Tingwei Ji",
          "Xinshuai Zhang",
          "Yufeng Lu",
          "Hongjie Zhou",
          "Yao Zheng"
        ],
        "published_date": "2022",
        "abstract": "Deep reinforcement learning (RL) is capable of identifying and modifying strategies for active flow control. However, the classic active formulation of deep RL requires lengthy active exploration. This paper describes the introduction of expert demonstration into a classic off-policy RL algorithm, the soft actor-critic algorithm, for application to vortex-induced vibration problems. This combined online-learning framework is applied to an oscillator wake environment and a Navier--Stokes environment, with expert demonstration obtained from the pole-placement method and surrogate model optimization. The results show that the soft actor--critic framework combined with expert demonstration enables rapid learning of active flow control strategies through a combination of prior demonstration data and online experience. The present study develops a new data-efficient RL approach for discovering active flow control strategies for vortex-induced vibration, providing a more practical methodology for industrial applications.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/21828b3f05acffbfdf7de7259ad8b8ebd57fa351.pdf",
        "venue": "The Physics of Fluids",
        "citationCount": 24,
        "score": 8.0,
        "summary": "Deep reinforcement learning (RL) is capable of identifying and modifying strategies for active flow control. However, the classic active formulation of deep RL requires lengthy active exploration. This paper describes the introduction of expert demonstration into a classic off-policy RL algorithm, the soft actor-critic algorithm, for application to vortex-induced vibration problems. This combined online-learning framework is applied to an oscillator wake environment and a Navier--Stokes environment, with expert demonstration obtained from the pole-placement method and surrogate model optimization. The results show that the soft actor--critic framework combined with expert demonstration enables rapid learning of active flow control strategies through a combination of prior demonstration data and online experience. The present study develops a new data-efficient RL approach for discovering active flow control strategies for vortex-induced vibration, providing a more practical methodology for industrial applications.",
        "keywords": []
      },
      "file_name": "21828b3f05acffbfdf7de7259ad8b8ebd57fa351.pdf"
    },
    {
      "success": true,
      "doc_id": "a7ec397e0099fdaae572455e4ab57c2e",
      "summary": "It is challenging for reinforcement learning (RL) algorithms to succeed in real-world applications. Take financial trading as an example, the market information is noisy yet imperfect and the macroeconomic regulation or other factors may shift between training and evaluation, thus it requires both generalization and high sample efficiency for resolving the task. However, directly applying typical RL algorithms can lead to poor performance in such scenarios. To derive a robust and applicable RL algorithm, in this work, we design a simple but effective method named Ensemble Proximal Policy Optimization (EPPO), which learns ensemble policies in an end-to-end manner. Notably, EPPO combines each policy and the policy ensemble organically and optimizes both simultaneously. In addition, EPPO adopts a diversity enhancement regularization over the policy space which helps to generalize to unseen states and promotes exploration. We theoretically prove that EPPO can increase exploration efficacy, and through comprehensive experimental evaluations on various tasks, we demonstrate that EPPO achieves higher efficiency and is robust for real-world applications compared with vanilla policy optimization algorithms and other ensemble methods. Code and supplemental materials are available at https://seqml.github.io/eppo.",
      "intriguing_abstract": "It is challenging for reinforcement learning (RL) algorithms to succeed in real-world applications. Take financial trading as an example, the market information is noisy yet imperfect and the macroeconomic regulation or other factors may shift between training and evaluation, thus it requires both generalization and high sample efficiency for resolving the task. However, directly applying typical RL algorithms can lead to poor performance in such scenarios. To derive a robust and applicable RL algorithm, in this work, we design a simple but effective method named Ensemble Proximal Policy Optimization (EPPO), which learns ensemble policies in an end-to-end manner. Notably, EPPO combines each policy and the policy ensemble organically and optimizes both simultaneously. In addition, EPPO adopts a diversity enhancement regularization over the policy space which helps to generalize to unseen states and promotes exploration. We theoretically prove that EPPO can increase exploration efficacy, and through comprehensive experimental evaluations on various tasks, we demonstrate that EPPO achieves higher efficiency and is robust for real-world applications compared with vanilla policy optimization algorithms and other ensemble methods. Code and supplemental materials are available at https://seqml.github.io/eppo.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/714b7c61d81687d093fd8b7d6d6737d582a4c9b7.pdf",
      "citation_key": "yang2022mx5",
      "metadata": {
        "title": "Towards Applicable Reinforcement Learning: Improving the Generalization and Sample Efficiency with Policy Ensemble",
        "authors": [
          "Zhengyu Yang",
          "Kan Ren",
          "Xufang Luo",
          "Minghuan Liu",
          "Weiqing Liu",
          "J. Bian",
          "Weinan Zhang",
          "Dongsheng Li"
        ],
        "published_date": "2022",
        "abstract": "It is challenging for reinforcement learning (RL) algorithms to succeed in real-world applications. Take financial trading as an example, the market information is noisy yet imperfect and the macroeconomic regulation or other factors may shift between training and evaluation, thus it requires both generalization and high sample efficiency for resolving the task. However, directly applying typical RL algorithms can lead to poor performance in such scenarios. To derive a robust and applicable RL algorithm, in this work, we design a simple but effective method named Ensemble Proximal Policy Optimization (EPPO), which learns ensemble policies in an end-to-end manner. Notably, EPPO combines each policy and the policy ensemble organically and optimizes both simultaneously. In addition, EPPO adopts a diversity enhancement regularization over the policy space which helps to generalize to unseen states and promotes exploration. We theoretically prove that EPPO can increase exploration efficacy, and through comprehensive experimental evaluations on various tasks, we demonstrate that EPPO achieves higher efficiency and is robust for real-world applications compared with vanilla policy optimization algorithms and other ensemble methods. Code and supplemental materials are available at https://seqml.github.io/eppo.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/714b7c61d81687d093fd8b7d6d6737d582a4c9b7.pdf",
        "venue": "International Joint Conference on Artificial Intelligence",
        "citationCount": 23,
        "score": 7.666666666666666,
        "summary": "It is challenging for reinforcement learning (RL) algorithms to succeed in real-world applications. Take financial trading as an example, the market information is noisy yet imperfect and the macroeconomic regulation or other factors may shift between training and evaluation, thus it requires both generalization and high sample efficiency for resolving the task. However, directly applying typical RL algorithms can lead to poor performance in such scenarios. To derive a robust and applicable RL algorithm, in this work, we design a simple but effective method named Ensemble Proximal Policy Optimization (EPPO), which learns ensemble policies in an end-to-end manner. Notably, EPPO combines each policy and the policy ensemble organically and optimizes both simultaneously. In addition, EPPO adopts a diversity enhancement regularization over the policy space which helps to generalize to unseen states and promotes exploration. We theoretically prove that EPPO can increase exploration efficacy, and through comprehensive experimental evaluations on various tasks, we demonstrate that EPPO achieves higher efficiency and is robust for real-world applications compared with vanilla policy optimization algorithms and other ensemble methods. Code and supplemental materials are available at https://seqml.github.io/eppo.",
        "keywords": []
      },
      "file_name": "714b7c61d81687d093fd8b7d6d6737d582a4c9b7.pdf"
    },
    {
      "success": true,
      "doc_id": "4c5ec89be12435f37ea0547e235d1a2a",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/1e650cb12aaad371a49b0c8c4514e1b988a5178c.pdf",
      "citation_key": "yang2022fou",
      "metadata": {
        "title": "Pareto Policy Pool for Model-based Offline Reinforcement Learning",
        "authors": [
          "Yijun Yang",
          "J. Jiang",
          "Tianyi Zhou",
          "Jie Ma",
          "Yuhui Shi"
        ],
        "published_date": "2022",
        "abstract": "",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/1e650cb12aaad371a49b0c8c4514e1b988a5178c.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 22,
        "score": 7.333333333333333,
        "summary": "",
        "keywords": []
      },
      "file_name": "1e650cb12aaad371a49b0c8c4514e1b988a5178c.pdf"
    },
    {
      "success": true,
      "doc_id": "181eba0a19c2cd3a70377d47a187d36b",
      "summary": "When using deep reinforcement learning algorithms for path planning of a multi-DOF fruit-picking manipulator in unstructured environments, it is much too difficult for the multi-DOF manipulator to obtain high-value samples at the beginning of training, resulting in low learning and convergence efficiency. Aiming to reduce the inefficient exploration in unstructured environments, a reinforcement learning strategy combining expert experience guidance was first proposed in this paper. The ratios of expert experience to newly generated samples and the frequency of return visits to expert experience were studied by the simulation experiments. Some conclusions were that the ratio of expert experience, which declined from 0.45 to 0.35, was more effective in improving learning efficiency of the model than the constant ratio. Compared to an expert experience ratio of 0.35, the success rate increased by 1.26%, and compared to an expert experience ratio of 0.45, the success rate increased by 20.37%. The highest success rate was achieved when the frequency of return visits was 15 in 50 episodes, an improvement of 31.77%. The results showed that the proposed method can effectively improve the model performance and enhance the learning efficiency at the beginning of training in unstructured environments. This training method has implications for the training process of reinforcement learning in other domains.",
      "intriguing_abstract": "When using deep reinforcement learning algorithms for path planning of a multi-DOF fruit-picking manipulator in unstructured environments, it is much too difficult for the multi-DOF manipulator to obtain high-value samples at the beginning of training, resulting in low learning and convergence efficiency. Aiming to reduce the inefficient exploration in unstructured environments, a reinforcement learning strategy combining expert experience guidance was first proposed in this paper. The ratios of expert experience to newly generated samples and the frequency of return visits to expert experience were studied by the simulation experiments. Some conclusions were that the ratio of expert experience, which declined from 0.45 to 0.35, was more effective in improving learning efficiency of the model than the constant ratio. Compared to an expert experience ratio of 0.35, the success rate increased by 1.26%, and compared to an expert experience ratio of 0.45, the success rate increased by 20.37%. The highest success rate was achieved when the frequency of return visits was 15 in 50 episodes, an improvement of 31.77%. The results showed that the proposed method can effectively improve the model performance and enhance the learning efficiency at the beginning of training in unstructured environments. This training method has implications for the training process of reinforcement learning in other domains.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/1d2acae1d631e932c7ebe96fad3c3b04909e5cee.pdf",
      "citation_key": "liu2022uiv",
      "metadata": {
        "title": "A Deep Reinforcement Learning Strategy Combining Expert Experience Guidance for a Fruit-Picking Manipulator",
        "authors": [
          "Yuqi Liu",
          "Po Gao",
          "Change Zheng",
          "Lijing Tian",
          "Ye Tian"
        ],
        "published_date": "2022",
        "abstract": "When using deep reinforcement learning algorithms for path planning of a multi-DOF fruit-picking manipulator in unstructured environments, it is much too difficult for the multi-DOF manipulator to obtain high-value samples at the beginning of training, resulting in low learning and convergence efficiency. Aiming to reduce the inefficient exploration in unstructured environments, a reinforcement learning strategy combining expert experience guidance was first proposed in this paper. The ratios of expert experience to newly generated samples and the frequency of return visits to expert experience were studied by the simulation experiments. Some conclusions were that the ratio of expert experience, which declined from 0.45 to 0.35, was more effective in improving learning efficiency of the model than the constant ratio. Compared to an expert experience ratio of 0.35, the success rate increased by 1.26%, and compared to an expert experience ratio of 0.45, the success rate increased by 20.37%. The highest success rate was achieved when the frequency of return visits was 15 in 50 episodes, an improvement of 31.77%. The results showed that the proposed method can effectively improve the model performance and enhance the learning efficiency at the beginning of training in unstructured environments. This training method has implications for the training process of reinforcement learning in other domains.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/1d2acae1d631e932c7ebe96fad3c3b04909e5cee.pdf",
        "venue": "Electronics",
        "citationCount": 22,
        "score": 7.333333333333333,
        "summary": "When using deep reinforcement learning algorithms for path planning of a multi-DOF fruit-picking manipulator in unstructured environments, it is much too difficult for the multi-DOF manipulator to obtain high-value samples at the beginning of training, resulting in low learning and convergence efficiency. Aiming to reduce the inefficient exploration in unstructured environments, a reinforcement learning strategy combining expert experience guidance was first proposed in this paper. The ratios of expert experience to newly generated samples and the frequency of return visits to expert experience were studied by the simulation experiments. Some conclusions were that the ratio of expert experience, which declined from 0.45 to 0.35, was more effective in improving learning efficiency of the model than the constant ratio. Compared to an expert experience ratio of 0.35, the success rate increased by 1.26%, and compared to an expert experience ratio of 0.45, the success rate increased by 20.37%. The highest success rate was achieved when the frequency of return visits was 15 in 50 episodes, an improvement of 31.77%. The results showed that the proposed method can effectively improve the model performance and enhance the learning efficiency at the beginning of training in unstructured environments. This training method has implications for the training process of reinforcement learning in other domains.",
        "keywords": []
      },
      "file_name": "1d2acae1d631e932c7ebe96fad3c3b04909e5cee.pdf"
    },
    {
      "success": true,
      "doc_id": "ccb66270f8dde97b565ccb4136d75ec5",
      "summary": "Multi-hop reasoning is an effective and ex-plainable approach to predicting missing facts in Knowledge Graphs (KGs). It usually adopts the Reinforcement Learning (RL) framework and searches over the KG to nd an evidential path. However, due to the large exploration space, the RL-based model struggles with the serious sparse reward problem and needs to make a lot of trials. Moreover, its exploration can be biased towards spurious paths that coin-cidentally lead to correct answers. To solve both problems, we propose a simple but effective RL-based method called RARL (Rule-Aware RL). It injects high quality symbolic rules into the models reasoning process and employs partially random beam search, which can not only increase the probability of paths getting rewards, but also alleviate the impact of spurious paths. Experimental results show that it outperforms existing multi-hop methods in terms of Hit@1 and MRR.",
      "intriguing_abstract": "Multi-hop reasoning is an effective and ex-plainable approach to predicting missing facts in Knowledge Graphs (KGs). It usually adopts the Reinforcement Learning (RL) framework and searches over the KG to nd an evidential path. However, due to the large exploration space, the RL-based model struggles with the serious sparse reward problem and needs to make a lot of trials. Moreover, its exploration can be biased towards spurious paths that coin-cidentally lead to correct answers. To solve both problems, we propose a simple but effective RL-based method called RARL (Rule-Aware RL). It injects high quality symbolic rules into the models reasoning process and employs partially random beam search, which can not only increase the probability of paths getting rewards, but also alleviate the impact of spurious paths. Experimental results show that it outperforms existing multi-hop methods in terms of Hit@1 and MRR.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/fe7382db243694c67c667cf2ec80072577d2372b.pdf",
      "citation_key": "hou2021c2r",
      "metadata": {
        "title": "Rule-Aware Reinforcement Learning for Knowledge Graph Reasoning",
        "authors": [
          "Zhongni Hou",
          "Xiaolong Jin",
          "Zixuan Li",
          "Long Bai"
        ],
        "published_date": "2021",
        "abstract": "Multi-hop reasoning is an effective and ex-plainable approach to predicting missing facts in Knowledge Graphs (KGs). It usually adopts the Reinforcement Learning (RL) framework and searches over the KG to nd an evidential path. However, due to the large exploration space, the RL-based model struggles with the serious sparse reward problem and needs to make a lot of trials. Moreover, its exploration can be biased towards spurious paths that coin-cidentally lead to correct answers. To solve both problems, we propose a simple but effective RL-based method called RARL (Rule-Aware RL). It injects high quality symbolic rules into the models reasoning process and employs partially random beam search, which can not only increase the probability of paths getting rewards, but also alleviate the impact of spurious paths. Experimental results show that it outperforms existing multi-hop methods in terms of Hit@1 and MRR.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/fe7382db243694c67c667cf2ec80072577d2372b.pdf",
        "venue": "Findings",
        "citationCount": 29,
        "score": 7.25,
        "summary": "Multi-hop reasoning is an effective and ex-plainable approach to predicting missing facts in Knowledge Graphs (KGs). It usually adopts the Reinforcement Learning (RL) framework and searches over the KG to nd an evidential path. However, due to the large exploration space, the RL-based model struggles with the serious sparse reward problem and needs to make a lot of trials. Moreover, its exploration can be biased towards spurious paths that coin-cidentally lead to correct answers. To solve both problems, we propose a simple but effective RL-based method called RARL (Rule-Aware RL). It injects high quality symbolic rules into the models reasoning process and employs partially random beam search, which can not only increase the probability of paths getting rewards, but also alleviate the impact of spurious paths. Experimental results show that it outperforms existing multi-hop methods in terms of Hit@1 and MRR.",
        "keywords": []
      },
      "file_name": "fe7382db243694c67c667cf2ec80072577d2372b.pdf"
    },
    {
      "success": true,
      "doc_id": "d5cf18b5f54b5fd8a83c8a6d3dabf2fa",
      "summary": "In this work, we study model-based reinforcement learning (RL) in unknown stabilizable linear dynamical systems. When learning a dynamical system, one needs to stabilize the unknown dynamics in order to avoid system blow-ups. We propose an algorithm that certifies fast stabilization of the underlying system by effectively exploring the environment with an improved exploration strategy. We show that the proposed algorithm attains $\\tilde{\\mathcal{O}}(\\sqrt{T})$ regret after $T$ time steps of agent-environment interaction. We also show that the regret of the proposed algorithm has only a polynomial dependence in the problem dimensions, which gives an exponential improvement over the prior methods. Our improved exploration method is simple, yet efficient, and it combines a sophisticated exploration policy in RL with an isotropic exploration strategy to achieve fast stabilization and improved regret. We empirically demonstrate that the proposed algorithm outperforms other popular methods in several adaptive control tasks.",
      "intriguing_abstract": "In this work, we study model-based reinforcement learning (RL) in unknown stabilizable linear dynamical systems. When learning a dynamical system, one needs to stabilize the unknown dynamics in order to avoid system blow-ups. We propose an algorithm that certifies fast stabilization of the underlying system by effectively exploring the environment with an improved exploration strategy. We show that the proposed algorithm attains $\\tilde{\\mathcal{O}}(\\sqrt{T})$ regret after $T$ time steps of agent-environment interaction. We also show that the regret of the proposed algorithm has only a polynomial dependence in the problem dimensions, which gives an exponential improvement over the prior methods. Our improved exploration method is simple, yet efficient, and it combines a sophisticated exploration policy in RL with an isotropic exploration strategy to achieve fast stabilization and improved regret. We empirically demonstrate that the proposed algorithm outperforms other popular methods in several adaptive control tasks.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/ba0e111b711e9b577932a02c9e40bc44b56cb88d.pdf",
      "citation_key": "lale2020xqs",
      "metadata": {
        "title": "Reinforcement Learning with Fast Stabilization in Linear Dynamical Systems",
        "authors": [
          "Sahin Lale",
          "K. Azizzadenesheli",
          "B. Hassibi",
          "Anima Anandkumar"
        ],
        "published_date": "2020",
        "abstract": "In this work, we study model-based reinforcement learning (RL) in unknown stabilizable linear dynamical systems. When learning a dynamical system, one needs to stabilize the unknown dynamics in order to avoid system blow-ups. We propose an algorithm that certifies fast stabilization of the underlying system by effectively exploring the environment with an improved exploration strategy. We show that the proposed algorithm attains $\\tilde{\\mathcal{O}}(\\sqrt{T})$ regret after $T$ time steps of agent-environment interaction. We also show that the regret of the proposed algorithm has only a polynomial dependence in the problem dimensions, which gives an exponential improvement over the prior methods. Our improved exploration method is simple, yet efficient, and it combines a sophisticated exploration policy in RL with an isotropic exploration strategy to achieve fast stabilization and improved regret. We empirically demonstrate that the proposed algorithm outperforms other popular methods in several adaptive control tasks.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/ba0e111b711e9b577932a02c9e40bc44b56cb88d.pdf",
        "venue": "International Conference on Artificial Intelligence and Statistics",
        "citationCount": 36,
        "score": 7.2,
        "summary": "In this work, we study model-based reinforcement learning (RL) in unknown stabilizable linear dynamical systems. When learning a dynamical system, one needs to stabilize the unknown dynamics in order to avoid system blow-ups. We propose an algorithm that certifies fast stabilization of the underlying system by effectively exploring the environment with an improved exploration strategy. We show that the proposed algorithm attains $\\tilde{\\mathcal{O}}(\\sqrt{T})$ regret after $T$ time steps of agent-environment interaction. We also show that the regret of the proposed algorithm has only a polynomial dependence in the problem dimensions, which gives an exponential improvement over the prior methods. Our improved exploration method is simple, yet efficient, and it combines a sophisticated exploration policy in RL with an isotropic exploration strategy to achieve fast stabilization and improved regret. We empirically demonstrate that the proposed algorithm outperforms other popular methods in several adaptive control tasks.",
        "keywords": []
      },
      "file_name": "ba0e111b711e9b577932a02c9e40bc44b56cb88d.pdf"
    },
    {
      "success": true,
      "doc_id": "d6657602c29bb34604ae6ea70fdeb57d",
      "summary": "\\Episode-based reinforcement learning (ERL) algorithms treat reinforcement learning (RL) as a black-box optimization problem where we learn to select a parameter vector of a controller, often represented as a movement primitive, for a given task descriptor called a context. ERL offers several distinct benefits in comparison to step-based RL. It generates smooth control trajectories, can handle non-Markovian reward definitions, and the resulting exploration in parameter space is well suited for solving sparse reward settings. Yet, the high dimensionality of the movement primitive parameters has so far hampered the effective use of deep RL methods. In this paper, we present a new algorithm for deep ERL. It is based on differentiable trust region layers, a successful on-policy deep RL algorithm. These layers allow us to specify trust regions for the policy update that are solved exactly for each state using convex optimization, which enables policies learning with the high precision required for the ERL. We compare our ERL algorithm to state-of-the-art step-based algorithms in many complex simulated robotic control tasks. In doing so, we investigate different reward formulations - dense, sparse, and non-Markovian. While step-based algorithms perform well only on dense rewards, ERL performs favorably on sparse and non-Markovian rewards. Moreover, our results show that the sparse and the non-Markovian rewards are also often better suited to define the desired behavior, allowing us to obtain considerably higher quality policies compared to step-based RL.",
      "intriguing_abstract": "\\Episode-based reinforcement learning (ERL) algorithms treat reinforcement learning (RL) as a black-box optimization problem where we learn to select a parameter vector of a controller, often represented as a movement primitive, for a given task descriptor called a context. ERL offers several distinct benefits in comparison to step-based RL. It generates smooth control trajectories, can handle non-Markovian reward definitions, and the resulting exploration in parameter space is well suited for solving sparse reward settings. Yet, the high dimensionality of the movement primitive parameters has so far hampered the effective use of deep RL methods. In this paper, we present a new algorithm for deep ERL. It is based on differentiable trust region layers, a successful on-policy deep RL algorithm. These layers allow us to specify trust regions for the policy update that are solved exactly for each state using convex optimization, which enables policies learning with the high precision required for the ERL. We compare our ERL algorithm to state-of-the-art step-based algorithms in many complex simulated robotic control tasks. In doing so, we investigate different reward formulations - dense, sparse, and non-Markovian. While step-based algorithms perform well only on dense rewards, ERL performs favorably on sparse and non-Markovian rewards. Moreover, our results show that the sparse and the non-Markovian rewards are also often better suited to define the desired behavior, allowing us to obtain considerably higher quality policies compared to step-based RL.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/cf628a42ee56c8f1b858790822a2bc0a61a49110.pdf",
      "citation_key": "otto2022qef",
      "metadata": {
        "title": "Deep Black-Box Reinforcement Learning with Movement Primitives",
        "authors": [
          "Fabian Otto",
          "Onur elik",
          "Hongyi Zhou",
          "Hanna Ziesche",
          "Ngo Anh Vien",
          "G. Neumann"
        ],
        "published_date": "2022",
        "abstract": "\\Episode-based reinforcement learning (ERL) algorithms treat reinforcement learning (RL) as a black-box optimization problem where we learn to select a parameter vector of a controller, often represented as a movement primitive, for a given task descriptor called a context. ERL offers several distinct benefits in comparison to step-based RL. It generates smooth control trajectories, can handle non-Markovian reward definitions, and the resulting exploration in parameter space is well suited for solving sparse reward settings. Yet, the high dimensionality of the movement primitive parameters has so far hampered the effective use of deep RL methods. In this paper, we present a new algorithm for deep ERL. It is based on differentiable trust region layers, a successful on-policy deep RL algorithm. These layers allow us to specify trust regions for the policy update that are solved exactly for each state using convex optimization, which enables policies learning with the high precision required for the ERL. We compare our ERL algorithm to state-of-the-art step-based algorithms in many complex simulated robotic control tasks. In doing so, we investigate different reward formulations - dense, sparse, and non-Markovian. While step-based algorithms perform well only on dense rewards, ERL performs favorably on sparse and non-Markovian rewards. Moreover, our results show that the sparse and the non-Markovian rewards are also often better suited to define the desired behavior, allowing us to obtain considerably higher quality policies compared to step-based RL.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/cf628a42ee56c8f1b858790822a2bc0a61a49110.pdf",
        "venue": "Conference on Robot Learning",
        "citationCount": 21,
        "score": 7.0,
        "summary": "\\Episode-based reinforcement learning (ERL) algorithms treat reinforcement learning (RL) as a black-box optimization problem where we learn to select a parameter vector of a controller, often represented as a movement primitive, for a given task descriptor called a context. ERL offers several distinct benefits in comparison to step-based RL. It generates smooth control trajectories, can handle non-Markovian reward definitions, and the resulting exploration in parameter space is well suited for solving sparse reward settings. Yet, the high dimensionality of the movement primitive parameters has so far hampered the effective use of deep RL methods. In this paper, we present a new algorithm for deep ERL. It is based on differentiable trust region layers, a successful on-policy deep RL algorithm. These layers allow us to specify trust regions for the policy update that are solved exactly for each state using convex optimization, which enables policies learning with the high precision required for the ERL. We compare our ERL algorithm to state-of-the-art step-based algorithms in many complex simulated robotic control tasks. In doing so, we investigate different reward formulations - dense, sparse, and non-Markovian. While step-based algorithms perform well only on dense rewards, ERL performs favorably on sparse and non-Markovian rewards. Moreover, our results show that the sparse and the non-Markovian rewards are also often better suited to define the desired behavior, allowing us to obtain considerably higher quality policies compared to step-based RL.",
        "keywords": []
      },
      "file_name": "cf628a42ee56c8f1b858790822a2bc0a61a49110.pdf"
    },
    {
      "success": true,
      "doc_id": "7853ae35186c8eb6647e8e0cbd0f620a",
      "summary": "Proportional-Integral-Derivative (PID) control has been the dominant control strategy in the process industry due to its simplicity in design and effectiveness in controlling a wide range of processes. However, most traditional PID tuning methods rely on trial and error for complex processes where insights about the system are limited and may not yield the optimal PID parameters. To address the issue, this work proposes an automatic PID tuning framework based on reinforcement learning (RL), particularly the deterministic policy gradient (DPG) method. Different from existing studies on using RL for PID tuning, in this work, we explicitly consider the closed-loop stability throughout the RL-based tuning process. In particular, we propose a novel episodic tuning framework that allows for an episodic closed-loop operation under selected PID parameters where the actor and critic networks are updated once at the end of each episode. To ensure the closed-loop stability during the tuning, we initialize the training with a conservative but stable baseline PID controller and the resultant reward is used as a benchmark score. A supervisor mechanism is used to monitor the running reward (e.g., tracking error) at each step in the episode. As soon as the running reward exceeds the benchmark score, the underlying controller is replaced by the baseline controller as an early correction to prevent instability. Moreover, we use layer normalization to standardize the input to each layer in actor and critic networks to overcome the issue of policy saturation at action bounds, to ensure the convergence to the optimum. The developed methods are validated through setpoint tracking experiments on a second-order plus dead-time system. Simulation results show that with our scheme, the closed-loop stability can be maintained throughout RL explorations and the explored PID parameters by the RL agent converge quickly to the optimum. Moreover, through simulation verification, the developed RL-based PID tuning method can adapt the PID parameters to changes in the process model automatically without requiring any knowledge about the underlying operating condition, in contrast to other adaptive methods such as the gain scheduling control.",
      "intriguing_abstract": "Proportional-Integral-Derivative (PID) control has been the dominant control strategy in the process industry due to its simplicity in design and effectiveness in controlling a wide range of processes. However, most traditional PID tuning methods rely on trial and error for complex processes where insights about the system are limited and may not yield the optimal PID parameters. To address the issue, this work proposes an automatic PID tuning framework based on reinforcement learning (RL), particularly the deterministic policy gradient (DPG) method. Different from existing studies on using RL for PID tuning, in this work, we explicitly consider the closed-loop stability throughout the RL-based tuning process. In particular, we propose a novel episodic tuning framework that allows for an episodic closed-loop operation under selected PID parameters where the actor and critic networks are updated once at the end of each episode. To ensure the closed-loop stability during the tuning, we initialize the training with a conservative but stable baseline PID controller and the resultant reward is used as a benchmark score. A supervisor mechanism is used to monitor the running reward (e.g., tracking error) at each step in the episode. As soon as the running reward exceeds the benchmark score, the underlying controller is replaced by the baseline controller as an early correction to prevent instability. Moreover, we use layer normalization to standardize the input to each layer in actor and critic networks to overcome the issue of policy saturation at action bounds, to ensure the convergence to the optimum. The developed methods are validated through setpoint tracking experiments on a second-order plus dead-time system. Simulation results show that with our scheme, the closed-loop stability can be maintained throughout RL explorations and the explored PID parameters by the RL agent converge quickly to the optimum. Moreover, through simulation verification, the developed RL-based PID tuning method can adapt the PID parameters to changes in the process model automatically without requiring any knowledge about the underlying operating condition, in contrast to other adaptive methods such as the gain scheduling control.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/fbcace16369032bb0292754bd78d03b68b554a95.pdf",
      "citation_key": "lakhani2021217",
      "metadata": {
        "title": "Stability-Preserving Automatic Tuning of PID Control with Reinforcement Learning",
        "authors": [
          "Ayub I. Lakhani",
          "Myisha A. Chowdhury",
          "Qiugang Lu"
        ],
        "published_date": "2021",
        "abstract": "Proportional-Integral-Derivative (PID) control has been the dominant control strategy in the process industry due to its simplicity in design and effectiveness in controlling a wide range of processes. However, most traditional PID tuning methods rely on trial and error for complex processes where insights about the system are limited and may not yield the optimal PID parameters. To address the issue, this work proposes an automatic PID tuning framework based on reinforcement learning (RL), particularly the deterministic policy gradient (DPG) method. Different from existing studies on using RL for PID tuning, in this work, we explicitly consider the closed-loop stability throughout the RL-based tuning process. In particular, we propose a novel episodic tuning framework that allows for an episodic closed-loop operation under selected PID parameters where the actor and critic networks are updated once at the end of each episode. To ensure the closed-loop stability during the tuning, we initialize the training with a conservative but stable baseline PID controller and the resultant reward is used as a benchmark score. A supervisor mechanism is used to monitor the running reward (e.g., tracking error) at each step in the episode. As soon as the running reward exceeds the benchmark score, the underlying controller is replaced by the baseline controller as an early correction to prevent instability. Moreover, we use layer normalization to standardize the input to each layer in actor and critic networks to overcome the issue of policy saturation at action bounds, to ensure the convergence to the optimum. The developed methods are validated through setpoint tracking experiments on a second-order plus dead-time system. Simulation results show that with our scheme, the closed-loop stability can be maintained throughout RL explorations and the explored PID parameters by the RL agent converge quickly to the optimum. Moreover, through simulation verification, the developed RL-based PID tuning method can adapt the PID parameters to changes in the process model automatically without requiring any knowledge about the underlying operating condition, in contrast to other adaptive methods such as the gain scheduling control.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/fbcace16369032bb0292754bd78d03b68b554a95.pdf",
        "venue": "Complex Engineering Systems",
        "citationCount": 28,
        "score": 7.0,
        "summary": "Proportional-Integral-Derivative (PID) control has been the dominant control strategy in the process industry due to its simplicity in design and effectiveness in controlling a wide range of processes. However, most traditional PID tuning methods rely on trial and error for complex processes where insights about the system are limited and may not yield the optimal PID parameters. To address the issue, this work proposes an automatic PID tuning framework based on reinforcement learning (RL), particularly the deterministic policy gradient (DPG) method. Different from existing studies on using RL for PID tuning, in this work, we explicitly consider the closed-loop stability throughout the RL-based tuning process. In particular, we propose a novel episodic tuning framework that allows for an episodic closed-loop operation under selected PID parameters where the actor and critic networks are updated once at the end of each episode. To ensure the closed-loop stability during the tuning, we initialize the training with a conservative but stable baseline PID controller and the resultant reward is used as a benchmark score. A supervisor mechanism is used to monitor the running reward (e.g., tracking error) at each step in the episode. As soon as the running reward exceeds the benchmark score, the underlying controller is replaced by the baseline controller as an early correction to prevent instability. Moreover, we use layer normalization to standardize the input to each layer in actor and critic networks to overcome the issue of policy saturation at action bounds, to ensure the convergence to the optimum. The developed methods are validated through setpoint tracking experiments on a second-order plus dead-time system. Simulation results show that with our scheme, the closed-loop stability can be maintained throughout RL explorations and the explored PID parameters by the RL agent converge quickly to the optimum. Moreover, through simulation verification, the developed RL-based PID tuning method can adapt the PID parameters to changes in the process model automatically without requiring any knowledge about the underlying operating condition, in contrast to other adaptive methods such as the gain scheduling control.",
        "keywords": []
      },
      "file_name": "fbcace16369032bb0292754bd78d03b68b554a95.pdf"
    },
    {
      "success": true,
      "doc_id": "b6f6cd1783c1196887d5875d6eeb2998",
      "summary": "Deep-space exploration missions are known as particularly challenging with high risk and cost, as they operate in environments with high uncertainty. The fault of exploration robot can even cause the whole mission to failure. One of the solutions is to use swarm robots to operate missions collaboratively. Compared with a single capable robot, a swarm of less sophisticated robots can cooperate on multiple and complex tasks. Reinforcement learning (RL) has made a variety of progress in multi-agent system autonomous cooperative control domains. In this paper, we construct a collaborative exploration scenario, where a multi-robot system explores an unknown Mars surface. Tasks are assigned to robots by human scientists and each robot takes optimal policies autonomously. The method used to train policies is a multi-agent deep deterministic policy gradient algorithm (MADDPG) and we design an experience sample optimizer to improve this algorithm. The results show that, with the increase of robots and targets number, this method is more efficient than traditional deep RL algorithm in a multi-agent collaborative exploration environment.",
      "intriguing_abstract": "Deep-space exploration missions are known as particularly challenging with high risk and cost, as they operate in environments with high uncertainty. The fault of exploration robot can even cause the whole mission to failure. One of the solutions is to use swarm robots to operate missions collaboratively. Compared with a single capable robot, a swarm of less sophisticated robots can cooperate on multiple and complex tasks. Reinforcement learning (RL) has made a variety of progress in multi-agent system autonomous cooperative control domains. In this paper, we construct a collaborative exploration scenario, where a multi-robot system explores an unknown Mars surface. Tasks are assigned to robots by human scientists and each robot takes optimal policies autonomously. The method used to train policies is a multi-agent deep deterministic policy gradient algorithm (MADDPG) and we design an experience sample optimizer to improve this algorithm. The results show that, with the increase of robots and targets number, this method is more efficient than traditional deep RL algorithm in a multi-agent collaborative exploration environment.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/be33087668f98ac746e72999178d7641d27412f9.pdf",
      "citation_key": "huang2020wll",
      "metadata": {
        "title": "A Multi-agent Reinforcement Learning Method for Swarm Robots in Space Collaborative Exploration",
        "authors": [
          "Yixin Huang",
          "Shufan Wu",
          "Z. Mu",
          "Xiangyu Long",
          "Sunhao Chu",
          "G. Zhao"
        ],
        "published_date": "2020",
        "abstract": "Deep-space exploration missions are known as particularly challenging with high risk and cost, as they operate in environments with high uncertainty. The fault of exploration robot can even cause the whole mission to failure. One of the solutions is to use swarm robots to operate missions collaboratively. Compared with a single capable robot, a swarm of less sophisticated robots can cooperate on multiple and complex tasks. Reinforcement learning (RL) has made a variety of progress in multi-agent system autonomous cooperative control domains. In this paper, we construct a collaborative exploration scenario, where a multi-robot system explores an unknown Mars surface. Tasks are assigned to robots by human scientists and each robot takes optimal policies autonomously. The method used to train policies is a multi-agent deep deterministic policy gradient algorithm (MADDPG) and we design an experience sample optimizer to improve this algorithm. The results show that, with the increase of robots and targets number, this method is more efficient than traditional deep RL algorithm in a multi-agent collaborative exploration environment.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/be33087668f98ac746e72999178d7641d27412f9.pdf",
        "venue": "2020 6th International Conference on Control, Automation and Robotics (ICCAR)",
        "citationCount": 34,
        "score": 6.800000000000001,
        "summary": "Deep-space exploration missions are known as particularly challenging with high risk and cost, as they operate in environments with high uncertainty. The fault of exploration robot can even cause the whole mission to failure. One of the solutions is to use swarm robots to operate missions collaboratively. Compared with a single capable robot, a swarm of less sophisticated robots can cooperate on multiple and complex tasks. Reinforcement learning (RL) has made a variety of progress in multi-agent system autonomous cooperative control domains. In this paper, we construct a collaborative exploration scenario, where a multi-robot system explores an unknown Mars surface. Tasks are assigned to robots by human scientists and each robot takes optimal policies autonomously. The method used to train policies is a multi-agent deep deterministic policy gradient algorithm (MADDPG) and we design an experience sample optimizer to improve this algorithm. The results show that, with the increase of robots and targets number, this method is more efficient than traditional deep RL algorithm in a multi-agent collaborative exploration environment.",
        "keywords": []
      },
      "file_name": "be33087668f98ac746e72999178d7641d27412f9.pdf"
    },
    {
      "success": true,
      "doc_id": "3766e679d4b3ed4ac4fd5ab41885c0c3",
      "summary": "With the rapid development of unmanned combat aerial vehicle (UCAV)-related technologies, UCAVs are playing an increasingly important role in military operations. It has become an inevitable trend in the development of future air combat battlefields that UCAVs complete air combat tasks independently to acquire air superiority. In this paper, the UCAV maneuver decision problem in continuous action space is studied based on the deep reinforcement learning strategy optimization method. The UCAV platform model of continuous action space was established. Focusing on the problem of insufficient exploration ability of OrnsteinUhlenbeck (OU) exploration strategy in the deep deterministic policy gradient (DDPG) algorithm, a heuristic DDPG algorithm was proposed by introducing heuristic exploration strategy, and then a UCAV air combat maneuver decision method based on a heuristic DDPG algorithm is proposed. The superior performance of the algorithm is verified by comparison with different algorithms in the test environment, and the effectiveness of the decision method is verified by simulation of air combat tasks with different difficulty and attack modes.",
      "intriguing_abstract": "With the rapid development of unmanned combat aerial vehicle (UCAV)-related technologies, UCAVs are playing an increasingly important role in military operations. It has become an inevitable trend in the development of future air combat battlefields that UCAVs complete air combat tasks independently to acquire air superiority. In this paper, the UCAV maneuver decision problem in continuous action space is studied based on the deep reinforcement learning strategy optimization method. The UCAV platform model of continuous action space was established. Focusing on the problem of insufficient exploration ability of OrnsteinUhlenbeck (OU) exploration strategy in the deep deterministic policy gradient (DDPG) algorithm, a heuristic DDPG algorithm was proposed by introducing heuristic exploration strategy, and then a UCAV air combat maneuver decision method based on a heuristic DDPG algorithm is proposed. The superior performance of the algorithm is verified by comparison with different algorithms in the test environment, and the effectiveness of the decision method is verified by simulation of air combat tasks with different difficulty and attack modes.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/cae05340421a18c64ac0897d57bcdcc9a496a3b8.pdf",
      "citation_key": "yuan2022hto",
      "metadata": {
        "title": "Research on UCAV Maneuvering Decision Method Based on Heuristic Reinforcement Learning",
        "authors": [
          "Wang Yuan",
          "Z. Xiwen",
          "Zhou Rong",
          "Shangqin Tang",
          "Zhou Huan",
          "Dingkai Wei"
        ],
        "published_date": "2022",
        "abstract": "With the rapid development of unmanned combat aerial vehicle (UCAV)-related technologies, UCAVs are playing an increasingly important role in military operations. It has become an inevitable trend in the development of future air combat battlefields that UCAVs complete air combat tasks independently to acquire air superiority. In this paper, the UCAV maneuver decision problem in continuous action space is studied based on the deep reinforcement learning strategy optimization method. The UCAV platform model of continuous action space was established. Focusing on the problem of insufficient exploration ability of OrnsteinUhlenbeck (OU) exploration strategy in the deep deterministic policy gradient (DDPG) algorithm, a heuristic DDPG algorithm was proposed by introducing heuristic exploration strategy, and then a UCAV air combat maneuver decision method based on a heuristic DDPG algorithm is proposed. The superior performance of the algorithm is verified by comparison with different algorithms in the test environment, and the effectiveness of the decision method is verified by simulation of air combat tasks with different difficulty and attack modes.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/cae05340421a18c64ac0897d57bcdcc9a496a3b8.pdf",
        "venue": "Computational Intelligence and Neuroscience",
        "citationCount": 20,
        "score": 6.666666666666666,
        "summary": "With the rapid development of unmanned combat aerial vehicle (UCAV)-related technologies, UCAVs are playing an increasingly important role in military operations. It has become an inevitable trend in the development of future air combat battlefields that UCAVs complete air combat tasks independently to acquire air superiority. In this paper, the UCAV maneuver decision problem in continuous action space is studied based on the deep reinforcement learning strategy optimization method. The UCAV platform model of continuous action space was established. Focusing on the problem of insufficient exploration ability of OrnsteinUhlenbeck (OU) exploration strategy in the deep deterministic policy gradient (DDPG) algorithm, a heuristic DDPG algorithm was proposed by introducing heuristic exploration strategy, and then a UCAV air combat maneuver decision method based on a heuristic DDPG algorithm is proposed. The superior performance of the algorithm is verified by comparison with different algorithms in the test environment, and the effectiveness of the decision method is verified by simulation of air combat tasks with different difficulty and attack modes.",
        "keywords": []
      },
      "file_name": "cae05340421a18c64ac0897d57bcdcc9a496a3b8.pdf"
    },
    {
      "success": true,
      "doc_id": "7b5dcfe47143d5d61cf5528b948adab0",
      "summary": "Vehicle control in autonomous traffic flow is often handled using the best decision-making reinforcement learning methods. However, unexpected critical situations make the collisions more severe and, consequently, the chain collisions. In this work, we first review the leading causes of chain collisions and their subsequent chain events, which might provide an indication of how to prevent and mitigate the crash severity of chain collisions. Then, we consider the problem of chain collision avoidance as a Markov Decision Process problem in order to propose a reinforcement learning-based decision-making strategy and analyse the safety efficiency of existing methods in driving security. To address this, A reward function is being developed to deal with the challenge of multiple vehicle collision avoidance. A perception network structure based on formation and on actor-critic methodologies is employed to enhance the decision-making process. Finally, in the safety efficiency analysis phase, we investigated the safety efficiency performance of the agent vehicle in both single-agent and multi-agent autonomous driving environments. Three state-of-the-art contemporary actor-critic algorithms are used to create an extensive simulation in Unity3D. Moreover, to demonstrate the accuracy of the safety efficiency analysis, multiple training runs of the neural networks in respect of training performance, speed of training, success rate, and stability of rewards with a trade-off between exploitation and exploration during training are presented. Two aspects (single-agent and multi-agent) have assessed the efficiency of algorithms. Every aspect has been analyzed regarding the traffic flows: (1) the controlling efficiency of unexpected traffic situations by the sudden slowdown, (2) abrupt lane change, and (3) smoothly reaching the destination. All the findings of the analysis are intended to shed insight on the benefits of a greater, more reliable autonomous traffic set-up for academics and policymakers, and also to pave the way for the actual carry-out of a driver-less traffic world.",
      "intriguing_abstract": "Vehicle control in autonomous traffic flow is often handled using the best decision-making reinforcement learning methods. However, unexpected critical situations make the collisions more severe and, consequently, the chain collisions. In this work, we first review the leading causes of chain collisions and their subsequent chain events, which might provide an indication of how to prevent and mitigate the crash severity of chain collisions. Then, we consider the problem of chain collision avoidance as a Markov Decision Process problem in order to propose a reinforcement learning-based decision-making strategy and analyse the safety efficiency of existing methods in driving security. To address this, A reward function is being developed to deal with the challenge of multiple vehicle collision avoidance. A perception network structure based on formation and on actor-critic methodologies is employed to enhance the decision-making process. Finally, in the safety efficiency analysis phase, we investigated the safety efficiency performance of the agent vehicle in both single-agent and multi-agent autonomous driving environments. Three state-of-the-art contemporary actor-critic algorithms are used to create an extensive simulation in Unity3D. Moreover, to demonstrate the accuracy of the safety efficiency analysis, multiple training runs of the neural networks in respect of training performance, speed of training, success rate, and stability of rewards with a trade-off between exploitation and exploration during training are presented. Two aspects (single-agent and multi-agent) have assessed the efficiency of algorithms. Every aspect has been analyzed regarding the traffic flows: (1) the controlling efficiency of unexpected traffic situations by the sudden slowdown, (2) abrupt lane change, and (3) smoothly reaching the destination. All the findings of the analysis are intended to shed insight on the benefits of a greater, more reliable autonomous traffic set-up for academics and policymakers, and also to pave the way for the actual carry-out of a driver-less traffic world.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/cac7f83769836707b02adadb0cda8c791ca23c92.pdf",
      "citation_key": "muzahid2022fyb",
      "metadata": {
        "title": "Deep Reinforcement Learning-Based Driving Strategy for Avoidance of Chain Collisions and Its Safety Efficiency Analysis in Autonomous Vehicles",
        "authors": [
          "Abu Jafar Md. Muzahid",
          "Syafiq Fauzi Bin Kamarulzaman",
          "Md. Arafatur Rahman",
          "A. Alenezi"
        ],
        "published_date": "2022",
        "abstract": "Vehicle control in autonomous traffic flow is often handled using the best decision-making reinforcement learning methods. However, unexpected critical situations make the collisions more severe and, consequently, the chain collisions. In this work, we first review the leading causes of chain collisions and their subsequent chain events, which might provide an indication of how to prevent and mitigate the crash severity of chain collisions. Then, we consider the problem of chain collision avoidance as a Markov Decision Process problem in order to propose a reinforcement learning-based decision-making strategy and analyse the safety efficiency of existing methods in driving security. To address this, A reward function is being developed to deal with the challenge of multiple vehicle collision avoidance. A perception network structure based on formation and on actor-critic methodologies is employed to enhance the decision-making process. Finally, in the safety efficiency analysis phase, we investigated the safety efficiency performance of the agent vehicle in both single-agent and multi-agent autonomous driving environments. Three state-of-the-art contemporary actor-critic algorithms are used to create an extensive simulation in Unity3D. Moreover, to demonstrate the accuracy of the safety efficiency analysis, multiple training runs of the neural networks in respect of training performance, speed of training, success rate, and stability of rewards with a trade-off between exploitation and exploration during training are presented. Two aspects (single-agent and multi-agent) have assessed the efficiency of algorithms. Every aspect has been analyzed regarding the traffic flows: (1) the controlling efficiency of unexpected traffic situations by the sudden slowdown, (2) abrupt lane change, and (3) smoothly reaching the destination. All the findings of the analysis are intended to shed insight on the benefits of a greater, more reliable autonomous traffic set-up for academics and policymakers, and also to pave the way for the actual carry-out of a driver-less traffic world.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/cac7f83769836707b02adadb0cda8c791ca23c92.pdf",
        "venue": "IEEE Access",
        "citationCount": 20,
        "score": 6.666666666666666,
        "summary": "Vehicle control in autonomous traffic flow is often handled using the best decision-making reinforcement learning methods. However, unexpected critical situations make the collisions more severe and, consequently, the chain collisions. In this work, we first review the leading causes of chain collisions and their subsequent chain events, which might provide an indication of how to prevent and mitigate the crash severity of chain collisions. Then, we consider the problem of chain collision avoidance as a Markov Decision Process problem in order to propose a reinforcement learning-based decision-making strategy and analyse the safety efficiency of existing methods in driving security. To address this, A reward function is being developed to deal with the challenge of multiple vehicle collision avoidance. A perception network structure based on formation and on actor-critic methodologies is employed to enhance the decision-making process. Finally, in the safety efficiency analysis phase, we investigated the safety efficiency performance of the agent vehicle in both single-agent and multi-agent autonomous driving environments. Three state-of-the-art contemporary actor-critic algorithms are used to create an extensive simulation in Unity3D. Moreover, to demonstrate the accuracy of the safety efficiency analysis, multiple training runs of the neural networks in respect of training performance, speed of training, success rate, and stability of rewards with a trade-off between exploitation and exploration during training are presented. Two aspects (single-agent and multi-agent) have assessed the efficiency of algorithms. Every aspect has been analyzed regarding the traffic flows: (1) the controlling efficiency of unexpected traffic situations by the sudden slowdown, (2) abrupt lane change, and (3) smoothly reaching the destination. All the findings of the analysis are intended to shed insight on the benefits of a greater, more reliable autonomous traffic set-up for academics and policymakers, and also to pave the way for the actual carry-out of a driver-less traffic world.",
        "keywords": []
      },
      "file_name": "cac7f83769836707b02adadb0cda8c791ca23c92.pdf"
    },
    {
      "success": true,
      "doc_id": "c8092cf6b1c01a7a14e98e6e0383bbfd",
      "summary": "In this paper, we study safe building HVAC control via batch reinforcement learning. Random exploration in building HVAC control is infeasible due to safety considerations. However, diverse states are necessary for RL algorithms to learn useful policies. To enable <italic>safety</italic> during exploration, we propose guided exploration by adding a Gaussian noise to a hand-crafted rule-based controller. Adjusting the variance of the noise provides a tradeoff between the <italic>diversity</italic> of the dataset and the <italic>safety</italic>. We apply Conservative Q Learning (CQL) to learn a policy. CQL ensures that the trained policy stays within the policy distribution used to collect the dataset, thereby guarantees safety at deployment. To select the optimal policy during the offline training, we apply model-based performance evaluation. We use the widely adopted CityLearn testbed to evaluate the performance of our proposed method. Compared with a rule-based controller, our approach obtains <inline-formula><tex-math notation=\"LaTeX\">$12\\%\\sim 35\\%$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>12</mml:mn><mml:mo>%</mml:mo><mml:mo></mml:mo><mml:mn>35</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"zhang-ieq1-3164084.gif\"/></alternatives></inline-formula> reduction in ramping, <inline-formula><tex-math notation=\"LaTeX\">$3\\%\\sim 10\\%$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>3</mml:mn><mml:mo>%</mml:mo><mml:mo></mml:mo><mml:mn>10</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"zhang-ieq2-3164084.gif\"/></alternatives></inline-formula> reduction in 1-load factor, <inline-formula><tex-math notation=\"LaTeX\">$3\\%\\sim 8\\%$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>3</mml:mn><mml:mo>%</mml:mo><mml:mo></mml:mo><mml:mn>8</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"zhang-ieq3-3164084.gif\"/></alternatives></inline-formula> reduction in daily peak at deployment with less than <inline-formula><tex-math notation=\"LaTeX\">$10\\%$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>10</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"zhang-ieq4-3164084.gif\"/></alternatives></inline-formula> performance degradation during the exploration. On the contrary, the performance degradation of the state-of-the-art online reinforcement learning algorithm during exploration is around <inline-formula><tex-math notation=\"LaTeX\">$8\\%\\sim 18\\%$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>8</mml:mn><mml:mo>%</mml:mo><mml:mo></mml:mo><mml:mn>18</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"zhang-ieq5-3164084.gif\"/></alternatives></inline-formula>. It also fails to surpass the performance of the rule-based controller at deployment.",
      "intriguing_abstract": "In this paper, we study safe building HVAC control via batch reinforcement learning. Random exploration in building HVAC control is infeasible due to safety considerations. However, diverse states are necessary for RL algorithms to learn useful policies. To enable <italic>safety</italic> during exploration, we propose guided exploration by adding a Gaussian noise to a hand-crafted rule-based controller. Adjusting the variance of the noise provides a tradeoff between the <italic>diversity</italic> of the dataset and the <italic>safety</italic>. We apply Conservative Q Learning (CQL) to learn a policy. CQL ensures that the trained policy stays within the policy distribution used to collect the dataset, thereby guarantees safety at deployment. To select the optimal policy during the offline training, we apply model-based performance evaluation. We use the widely adopted CityLearn testbed to evaluate the performance of our proposed method. Compared with a rule-based controller, our approach obtains <inline-formula><tex-math notation=\"LaTeX\">$12\\%\\sim 35\\%$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>12</mml:mn><mml:mo>%</mml:mo><mml:mo></mml:mo><mml:mn>35</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"zhang-ieq1-3164084.gif\"/></alternatives></inline-formula> reduction in ramping, <inline-formula><tex-math notation=\"LaTeX\">$3\\%\\sim 10\\%$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>3</mml:mn><mml:mo>%</mml:mo><mml:mo></mml:mo><mml:mn>10</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"zhang-ieq2-3164084.gif\"/></alternatives></inline-formula> reduction in 1-load factor, <inline-formula><tex-math notation=\"LaTeX\">$3\\%\\sim 8\\%$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>3</mml:mn><mml:mo>%</mml:mo><mml:mo></mml:mo><mml:mn>8</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"zhang-ieq3-3164084.gif\"/></alternatives></inline-formula> reduction in daily peak at deployment with less than <inline-formula><tex-math notation=\"LaTeX\">$10\\%$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>10</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"zhang-ieq4-3164084.gif\"/></alternatives></inline-formula> performance degradation during the exploration. On the contrary, the performance degradation of the state-of-the-art online reinforcement learning algorithm during exploration is around <inline-formula><tex-math notation=\"LaTeX\">$8\\%\\sim 18\\%$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>8</mml:mn><mml:mo>%</mml:mo><mml:mo></mml:mo><mml:mn>18</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"zhang-ieq5-3164084.gif\"/></alternatives></inline-formula>. It also fails to surpass the performance of the rule-based controller at deployment.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/f4712cdb025ba4ab879bb47d5cf693a4923f1532.pdf",
      "citation_key": "zhang20229rg",
      "metadata": {
        "title": "Safe Building HVAC Control via Batch Reinforcement Learning",
        "authors": [
          "Chi Zhang",
          "S. Kuppannagari",
          "V. Prasanna"
        ],
        "published_date": "2022",
        "abstract": "In this paper, we study safe building HVAC control via batch reinforcement learning. Random exploration in building HVAC control is infeasible due to safety considerations. However, diverse states are necessary for RL algorithms to learn useful policies. To enable <italic>safety</italic> during exploration, we propose guided exploration by adding a Gaussian noise to a hand-crafted rule-based controller. Adjusting the variance of the noise provides a tradeoff between the <italic>diversity</italic> of the dataset and the <italic>safety</italic>. We apply Conservative Q Learning (CQL) to learn a policy. CQL ensures that the trained policy stays within the policy distribution used to collect the dataset, thereby guarantees safety at deployment. To select the optimal policy during the offline training, we apply model-based performance evaluation. We use the widely adopted CityLearn testbed to evaluate the performance of our proposed method. Compared with a rule-based controller, our approach obtains <inline-formula><tex-math notation=\"LaTeX\">$12\\%\\sim 35\\%$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>12</mml:mn><mml:mo>%</mml:mo><mml:mo></mml:mo><mml:mn>35</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"zhang-ieq1-3164084.gif\"/></alternatives></inline-formula> reduction in ramping, <inline-formula><tex-math notation=\"LaTeX\">$3\\%\\sim 10\\%$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>3</mml:mn><mml:mo>%</mml:mo><mml:mo></mml:mo><mml:mn>10</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"zhang-ieq2-3164084.gif\"/></alternatives></inline-formula> reduction in 1-load factor, <inline-formula><tex-math notation=\"LaTeX\">$3\\%\\sim 8\\%$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>3</mml:mn><mml:mo>%</mml:mo><mml:mo></mml:mo><mml:mn>8</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"zhang-ieq3-3164084.gif\"/></alternatives></inline-formula> reduction in daily peak at deployment with less than <inline-formula><tex-math notation=\"LaTeX\">$10\\%$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>10</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"zhang-ieq4-3164084.gif\"/></alternatives></inline-formula> performance degradation during the exploration. On the contrary, the performance degradation of the state-of-the-art online reinforcement learning algorithm during exploration is around <inline-formula><tex-math notation=\"LaTeX\">$8\\%\\sim 18\\%$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>8</mml:mn><mml:mo>%</mml:mo><mml:mo></mml:mo><mml:mn>18</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"zhang-ieq5-3164084.gif\"/></alternatives></inline-formula>. It also fails to surpass the performance of the rule-based controller at deployment.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/f4712cdb025ba4ab879bb47d5cf693a4923f1532.pdf",
        "venue": "IEEE Transactions on Sustainable Computing",
        "citationCount": 20,
        "score": 6.666666666666666,
        "summary": "In this paper, we study safe building HVAC control via batch reinforcement learning. Random exploration in building HVAC control is infeasible due to safety considerations. However, diverse states are necessary for RL algorithms to learn useful policies. To enable <italic>safety</italic> during exploration, we propose guided exploration by adding a Gaussian noise to a hand-crafted rule-based controller. Adjusting the variance of the noise provides a tradeoff between the <italic>diversity</italic> of the dataset and the <italic>safety</italic>. We apply Conservative Q Learning (CQL) to learn a policy. CQL ensures that the trained policy stays within the policy distribution used to collect the dataset, thereby guarantees safety at deployment. To select the optimal policy during the offline training, we apply model-based performance evaluation. We use the widely adopted CityLearn testbed to evaluate the performance of our proposed method. Compared with a rule-based controller, our approach obtains <inline-formula><tex-math notation=\"LaTeX\">$12\\%\\sim 35\\%$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>12</mml:mn><mml:mo>%</mml:mo><mml:mo></mml:mo><mml:mn>35</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"zhang-ieq1-3164084.gif\"/></alternatives></inline-formula> reduction in ramping, <inline-formula><tex-math notation=\"LaTeX\">$3\\%\\sim 10\\%$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>3</mml:mn><mml:mo>%</mml:mo><mml:mo></mml:mo><mml:mn>10</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"zhang-ieq2-3164084.gif\"/></alternatives></inline-formula> reduction in 1-load factor, <inline-formula><tex-math notation=\"LaTeX\">$3\\%\\sim 8\\%$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>3</mml:mn><mml:mo>%</mml:mo><mml:mo></mml:mo><mml:mn>8</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"zhang-ieq3-3164084.gif\"/></alternatives></inline-formula> reduction in daily peak at deployment with less than <inline-formula><tex-math notation=\"LaTeX\">$10\\%$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>10</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"zhang-ieq4-3164084.gif\"/></alternatives></inline-formula> performance degradation during the exploration. On the contrary, the performance degradation of the state-of-the-art online reinforcement learning algorithm during exploration is around <inline-formula><tex-math notation=\"LaTeX\">$8\\%\\sim 18\\%$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>8</mml:mn><mml:mo>%</mml:mo><mml:mo></mml:mo><mml:mn>18</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"zhang-ieq5-3164084.gif\"/></alternatives></inline-formula>. It also fails to surpass the performance of the rule-based controller at deployment.",
        "keywords": []
      },
      "file_name": "f4712cdb025ba4ab879bb47d5cf693a4923f1532.pdf"
    },
    {
      "success": true,
      "doc_id": "7e22f51cc30dd7f90a634164f0d52132",
      "summary": "In this work, a pitch controller of a wind turbine (WT) inspired by reinforcement learning (RL) is designed and implemented. The control system consists of a state estimator, a reward strategy, a policy table, and a policy update algorithm. Novel reward strategies related to the energy deviation from the rated power are defined. They are designed to improve the efficiency of the WT. Two new categories of reward strategies are proposed: only positive (O-P) and positive-negative (P-N) rewards. The relationship of these categories with the exploration-exploitation dilemma, the use of -greedy methods and the learning convergence are also introduced and linked to the WT control problem. In addition, an extensive analysis of the influence of the different rewards in the controller performance and in the learning speed is carried out. The controller is compared with a proportional-integral-derivative (PID) regulator for the same small wind turbine, obtaining better results. The simulations show how the P-N rewards improve the performance of the controller, stabilize the output power around the rated power, and reduce the error over time.",
      "intriguing_abstract": "In this work, a pitch controller of a wind turbine (WT) inspired by reinforcement learning (RL) is designed and implemented. The control system consists of a state estimator, a reward strategy, a policy table, and a policy update algorithm. Novel reward strategies related to the energy deviation from the rated power are defined. They are designed to improve the efficiency of the WT. Two new categories of reward strategies are proposed: only positive (O-P) and positive-negative (P-N) rewards. The relationship of these categories with the exploration-exploitation dilemma, the use of -greedy methods and the learning convergence are also introduced and linked to the WT control problem. In addition, an extensive analysis of the influence of the different rewards in the controller performance and in the learning speed is carried out. The controller is compared with a proportional-integral-derivative (PID) regulator for the same small wind turbine, obtaining better results. The simulations show how the P-N rewards improve the performance of the controller, stabilize the output power around the rated power, and reduce the error over time.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/b1be7ce8c639291adf7663535f0451f9ac03ed55.pdf",
      "citation_key": "sierragarca2020g35",
      "metadata": {
        "title": "Exploring Reward Strategies for Wind Turbine Pitch Control by Reinforcement Learning",
        "authors": [
          "J. E. Sierra-Garca",
          "Matilde Santos"
        ],
        "published_date": "2020",
        "abstract": "In this work, a pitch controller of a wind turbine (WT) inspired by reinforcement learning (RL) is designed and implemented. The control system consists of a state estimator, a reward strategy, a policy table, and a policy update algorithm. Novel reward strategies related to the energy deviation from the rated power are defined. They are designed to improve the efficiency of the WT. Two new categories of reward strategies are proposed: only positive (O-P) and positive-negative (P-N) rewards. The relationship of these categories with the exploration-exploitation dilemma, the use of -greedy methods and the learning convergence are also introduced and linked to the WT control problem. In addition, an extensive analysis of the influence of the different rewards in the controller performance and in the learning speed is carried out. The controller is compared with a proportional-integral-derivative (PID) regulator for the same small wind turbine, obtaining better results. The simulations show how the P-N rewards improve the performance of the controller, stabilize the output power around the rated power, and reduce the error over time.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/b1be7ce8c639291adf7663535f0451f9ac03ed55.pdf",
        "venue": "Applied Sciences",
        "citationCount": 33,
        "score": 6.6000000000000005,
        "summary": "In this work, a pitch controller of a wind turbine (WT) inspired by reinforcement learning (RL) is designed and implemented. The control system consists of a state estimator, a reward strategy, a policy table, and a policy update algorithm. Novel reward strategies related to the energy deviation from the rated power are defined. They are designed to improve the efficiency of the WT. Two new categories of reward strategies are proposed: only positive (O-P) and positive-negative (P-N) rewards. The relationship of these categories with the exploration-exploitation dilemma, the use of -greedy methods and the learning convergence are also introduced and linked to the WT control problem. In addition, an extensive analysis of the influence of the different rewards in the controller performance and in the learning speed is carried out. The controller is compared with a proportional-integral-derivative (PID) regulator for the same small wind turbine, obtaining better results. The simulations show how the P-N rewards improve the performance of the controller, stabilize the output power around the rated power, and reduce the error over time.",
        "keywords": []
      },
      "file_name": "b1be7ce8c639291adf7663535f0451f9ac03ed55.pdf"
    },
    {
      "success": true,
      "doc_id": "9612d12bd3d007f6c2c47d2e832eec66",
      "summary": "The scheduling problems in mass production, manufacturing, assembly, synthesis, and transportation, as well as internet services, can partly be attributed to a hybrid flow-shop scheduling problem (HFSP). To solve the problem, a reinforcement learning (RL) method for HFSP is studied for the first time in this paper. HFSP is described and attributed to the Markov Decision Processes (MDP), for which the special states, actions, and reward function are designed. On this basis, the MDP framework is established. The Boltzmann exploration policy is adopted to trade-off the exploration and exploitation during choosing action in RL. Compared with the first-come-first-serve strategy that is frequently adopted when coding in most of the traditional intelligent algorithms, the rule in the RL method is first-come-first-choice, which is more conducive to achieving the global optimal solution. For validation, the RL method is utilized for scheduling in a metal processing workshop of an automobile engine factory. Then, the method is applied to the sortie scheduling of carrier aircraft in continuous dispatch. The results demonstrate that the machining and support scheduling obtained by this RL method are reasonable in result quality, real-time performance and complexity, indicating that this RL method is practical for HFSP.",
      "intriguing_abstract": "The scheduling problems in mass production, manufacturing, assembly, synthesis, and transportation, as well as internet services, can partly be attributed to a hybrid flow-shop scheduling problem (HFSP). To solve the problem, a reinforcement learning (RL) method for HFSP is studied for the first time in this paper. HFSP is described and attributed to the Markov Decision Processes (MDP), for which the special states, actions, and reward function are designed. On this basis, the MDP framework is established. The Boltzmann exploration policy is adopted to trade-off the exploration and exploitation during choosing action in RL. Compared with the first-come-first-serve strategy that is frequently adopted when coding in most of the traditional intelligent algorithms, the rule in the RL method is first-come-first-choice, which is more conducive to achieving the global optimal solution. For validation, the RL method is utilized for scheduling in a metal processing workshop of an automobile engine factory. Then, the method is applied to the sortie scheduling of carrier aircraft in continuous dispatch. The results demonstrate that the machining and support scheduling obtained by this RL method are reasonable in result quality, real-time performance and complexity, indicating that this RL method is practical for HFSP.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/a064b8183d657178916ae21c43b5099bfef6804d.pdf",
      "citation_key": "han20199g2",
      "metadata": {
        "title": "A Reinforcement Learning Method for a Hybrid Flow-Shop Scheduling Problem",
        "authors": [
          "Wei Han",
          "Fang Guo",
          "Xi-chao Su"
        ],
        "published_date": "2019",
        "abstract": "The scheduling problems in mass production, manufacturing, assembly, synthesis, and transportation, as well as internet services, can partly be attributed to a hybrid flow-shop scheduling problem (HFSP). To solve the problem, a reinforcement learning (RL) method for HFSP is studied for the first time in this paper. HFSP is described and attributed to the Markov Decision Processes (MDP), for which the special states, actions, and reward function are designed. On this basis, the MDP framework is established. The Boltzmann exploration policy is adopted to trade-off the exploration and exploitation during choosing action in RL. Compared with the first-come-first-serve strategy that is frequently adopted when coding in most of the traditional intelligent algorithms, the rule in the RL method is first-come-first-choice, which is more conducive to achieving the global optimal solution. For validation, the RL method is utilized for scheduling in a metal processing workshop of an automobile engine factory. Then, the method is applied to the sortie scheduling of carrier aircraft in continuous dispatch. The results demonstrate that the machining and support scheduling obtained by this RL method are reasonable in result quality, real-time performance and complexity, indicating that this RL method is practical for HFSP.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/a064b8183d657178916ae21c43b5099bfef6804d.pdf",
        "venue": "Algorithms",
        "citationCount": 39,
        "score": 6.5,
        "summary": "The scheduling problems in mass production, manufacturing, assembly, synthesis, and transportation, as well as internet services, can partly be attributed to a hybrid flow-shop scheduling problem (HFSP). To solve the problem, a reinforcement learning (RL) method for HFSP is studied for the first time in this paper. HFSP is described and attributed to the Markov Decision Processes (MDP), for which the special states, actions, and reward function are designed. On this basis, the MDP framework is established. The Boltzmann exploration policy is adopted to trade-off the exploration and exploitation during choosing action in RL. Compared with the first-come-first-serve strategy that is frequently adopted when coding in most of the traditional intelligent algorithms, the rule in the RL method is first-come-first-choice, which is more conducive to achieving the global optimal solution. For validation, the RL method is utilized for scheduling in a metal processing workshop of an automobile engine factory. Then, the method is applied to the sortie scheduling of carrier aircraft in continuous dispatch. The results demonstrate that the machining and support scheduling obtained by this RL method are reasonable in result quality, real-time performance and complexity, indicating that this RL method is practical for HFSP.",
        "keywords": []
      },
      "file_name": "a064b8183d657178916ae21c43b5099bfef6804d.pdf"
    },
    {
      "success": true,
      "doc_id": "647eccf99eec02119eca2feace71dccb",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/bc98c81467ed3a6b21788f39c20cbe659014e551.pdf",
      "citation_key": "wabersich2018t86",
      "metadata": {
        "title": "Safe exploration of nonlinear dynamical systems: A predictive safety filter for reinforcement learning",
        "authors": [
          "K. P. Wabersich",
          "M. Zeilinger"
        ],
        "published_date": "2018",
        "abstract": "",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/bc98c81467ed3a6b21788f39c20cbe659014e551.pdf",
        "venue": "arXiv.org",
        "citationCount": 44,
        "score": 6.285714285714286,
        "summary": "",
        "keywords": []
      },
      "file_name": "bc98c81467ed3a6b21788f39c20cbe659014e551.pdf"
    },
    {
      "success": true,
      "doc_id": "4d66705120910a7aa22655309a0df2ba",
      "summary": "The upper confidence reinforcement learning (UCRL2) strategy introduced in (Jaksch et al., 2010) is a popular method to perform regret minimization in unknown discrete Markov Decision Processes under the average-reward criterion. Despite its nice and generic theoretical regret guarantees, this strategy and its variants have remained until now mostly theoretical as numerical experiments on simple environments exhibit long burn-in phases before the learning takes place. Motivated by practical efficiency, we present UCRL3, following the lines of UCRL2, but with two key modifications: First, it uses state-of-the-art time-uniform concentration inequalities, to compute confidence sets on the reward and transition distributions for each state-action pair. To further tighten exploration, we introduce an adaptive computation of the support of each transition distributions. This enables to revisit the extended value iteration procedure to optimize over distributions with reduced support by disregarding low probability transitions, while still ensuring near-optimism. We demonstrate, through numerical experiments on standard environments, that reducing exploration this way yields a substantial numerical improvement compared to UCRL2 and its variants. On the theoretical side, these key modifications enable to derive a regret bound for UCRL3 improving on UCRL2, that for the first time makes appear a notion of local diameter and effective support, thanks to variance-aware concentration bounds.",
      "intriguing_abstract": "The upper confidence reinforcement learning (UCRL2) strategy introduced in (Jaksch et al., 2010) is a popular method to perform regret minimization in unknown discrete Markov Decision Processes under the average-reward criterion. Despite its nice and generic theoretical regret guarantees, this strategy and its variants have remained until now mostly theoretical as numerical experiments on simple environments exhibit long burn-in phases before the learning takes place. Motivated by practical efficiency, we present UCRL3, following the lines of UCRL2, but with two key modifications: First, it uses state-of-the-art time-uniform concentration inequalities, to compute confidence sets on the reward and transition distributions for each state-action pair. To further tighten exploration, we introduce an adaptive computation of the support of each transition distributions. This enables to revisit the extended value iteration procedure to optimize over distributions with reduced support by disregarding low probability transitions, while still ensuring near-optimism. We demonstrate, through numerical experiments on standard environments, that reducing exploration this way yields a substantial numerical improvement compared to UCRL2 and its variants. On the theoretical side, these key modifications enable to derive a regret bound for UCRL3 improving on UCRL2, that for the first time makes appear a notion of local diameter and effective support, thanks to variance-aware concentration bounds.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/621d57c1243f055bc3850c1f3e38f351f53c947f.pdf",
      "citation_key": "bourel2020tnm",
      "metadata": {
        "title": "Tightening Exploration in Upper Confidence Reinforcement Learning",
        "authors": [
          "Hippolyte Bourel",
          "Odalric-Ambrym Maillard",
          "M. S. Talebi"
        ],
        "published_date": "2020",
        "abstract": "The upper confidence reinforcement learning (UCRL2) strategy introduced in (Jaksch et al., 2010) is a popular method to perform regret minimization in unknown discrete Markov Decision Processes under the average-reward criterion. Despite its nice and generic theoretical regret guarantees, this strategy and its variants have remained until now mostly theoretical as numerical experiments on simple environments exhibit long burn-in phases before the learning takes place. Motivated by practical efficiency, we present UCRL3, following the lines of UCRL2, but with two key modifications: First, it uses state-of-the-art time-uniform concentration inequalities, to compute confidence sets on the reward and transition distributions for each state-action pair. To further tighten exploration, we introduce an adaptive computation of the support of each transition distributions. This enables to revisit the extended value iteration procedure to optimize over distributions with reduced support by disregarding low probability transitions, while still ensuring near-optimism. We demonstrate, through numerical experiments on standard environments, that reducing exploration this way yields a substantial numerical improvement compared to UCRL2 and its variants. On the theoretical side, these key modifications enable to derive a regret bound for UCRL3 improving on UCRL2, that for the first time makes appear a notion of local diameter and effective support, thanks to variance-aware concentration bounds.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/621d57c1243f055bc3850c1f3e38f351f53c947f.pdf",
        "venue": "International Conference on Machine Learning",
        "citationCount": 31,
        "score": 6.2,
        "summary": "The upper confidence reinforcement learning (UCRL2) strategy introduced in (Jaksch et al., 2010) is a popular method to perform regret minimization in unknown discrete Markov Decision Processes under the average-reward criterion. Despite its nice and generic theoretical regret guarantees, this strategy and its variants have remained until now mostly theoretical as numerical experiments on simple environments exhibit long burn-in phases before the learning takes place. Motivated by practical efficiency, we present UCRL3, following the lines of UCRL2, but with two key modifications: First, it uses state-of-the-art time-uniform concentration inequalities, to compute confidence sets on the reward and transition distributions for each state-action pair. To further tighten exploration, we introduce an adaptive computation of the support of each transition distributions. This enables to revisit the extended value iteration procedure to optimize over distributions with reduced support by disregarding low probability transitions, while still ensuring near-optimism. We demonstrate, through numerical experiments on standard environments, that reducing exploration this way yields a substantial numerical improvement compared to UCRL2 and its variants. On the theoretical side, these key modifications enable to derive a regret bound for UCRL3 improving on UCRL2, that for the first time makes appear a notion of local diameter and effective support, thanks to variance-aware concentration bounds.",
        "keywords": []
      },
      "file_name": "621d57c1243f055bc3850c1f3e38f351f53c947f.pdf"
    },
    {
      "success": true,
      "doc_id": "6110f58ca3ca05dcacbe9bf6e06105fb",
      "summary": "Safe reinforcement learning (RL) with assured satisfaction of hard state constraints during training has recently received a lot of attention. Safety filters, e.g., based on control barrier functions (CBFs), provide a promising way for safe RL via modifying the unsafe actions of an RL agent on the fly. Existing safety filter-based approaches typically involve learning of uncertain dynamics and quantifying the learned model error, which leads to conservative filters before a large amount of data is collected to learn a good model, thereby preventing efficient exploration. This paper presents a method for safe and efficient RL using disturbance observers (DOBs) and control barrier functions (CBFs). Unlike most existing safe RL methods that deal with hard state constraints, our method does not involve model learning, and leverages DOBs to accurately estimate the pointwise value of the uncertainty, which is then incorporated into a robust CBF condition to generate safe actions. The DOB-based CBF can be used as a safety filter with model-free RL algorithms by minimally modifying the actions of an RL agent whenever necessary to ensure safety throughout the learning process. Simulation results on a unicycle and a 2D quadrotor demonstrate that the proposed method outperforms a state-of-the-art safe RL algorithm using CBFs and Gaussian processes-based model learning, in terms of safety violation rate, and sample and computational efficiency.",
      "intriguing_abstract": "Safe reinforcement learning (RL) with assured satisfaction of hard state constraints during training has recently received a lot of attention. Safety filters, e.g., based on control barrier functions (CBFs), provide a promising way for safe RL via modifying the unsafe actions of an RL agent on the fly. Existing safety filter-based approaches typically involve learning of uncertain dynamics and quantifying the learned model error, which leads to conservative filters before a large amount of data is collected to learn a good model, thereby preventing efficient exploration. This paper presents a method for safe and efficient RL using disturbance observers (DOBs) and control barrier functions (CBFs). Unlike most existing safe RL methods that deal with hard state constraints, our method does not involve model learning, and leverages DOBs to accurately estimate the pointwise value of the uncertainty, which is then incorporated into a robust CBF condition to generate safe actions. The DOB-based CBF can be used as a safety filter with model-free RL algorithms by minimally modifying the actions of an RL agent whenever necessary to ensure safety throughout the learning process. Simulation results on a unicycle and a 2D quadrotor demonstrate that the proposed method outperforms a state-of-the-art safe RL algorithm using CBFs and Gaussian processes-based model learning, in terms of safety violation rate, and sample and computational efficiency.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/9b5aa6a75d8e9f0e68d12e3b331acad6f32667d4.pdf",
      "citation_key": "cheng20224w2",
      "metadata": {
        "title": "Safe and Efficient Reinforcement Learning using Disturbance-Observer-Based Control Barrier Functions",
        "authors": [
          "Yikun Cheng",
          "Pan Zhao",
          "N. Hovakimyan"
        ],
        "published_date": "2022",
        "abstract": "Safe reinforcement learning (RL) with assured satisfaction of hard state constraints during training has recently received a lot of attention. Safety filters, e.g., based on control barrier functions (CBFs), provide a promising way for safe RL via modifying the unsafe actions of an RL agent on the fly. Existing safety filter-based approaches typically involve learning of uncertain dynamics and quantifying the learned model error, which leads to conservative filters before a large amount of data is collected to learn a good model, thereby preventing efficient exploration. This paper presents a method for safe and efficient RL using disturbance observers (DOBs) and control barrier functions (CBFs). Unlike most existing safe RL methods that deal with hard state constraints, our method does not involve model learning, and leverages DOBs to accurately estimate the pointwise value of the uncertainty, which is then incorporated into a robust CBF condition to generate safe actions. The DOB-based CBF can be used as a safety filter with model-free RL algorithms by minimally modifying the actions of an RL agent whenever necessary to ensure safety throughout the learning process. Simulation results on a unicycle and a 2D quadrotor demonstrate that the proposed method outperforms a state-of-the-art safe RL algorithm using CBFs and Gaussian processes-based model learning, in terms of safety violation rate, and sample and computational efficiency.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/9b5aa6a75d8e9f0e68d12e3b331acad6f32667d4.pdf",
        "venue": "Conference on Learning for Dynamics & Control",
        "citationCount": 18,
        "score": 6.0,
        "summary": "Safe reinforcement learning (RL) with assured satisfaction of hard state constraints during training has recently received a lot of attention. Safety filters, e.g., based on control barrier functions (CBFs), provide a promising way for safe RL via modifying the unsafe actions of an RL agent on the fly. Existing safety filter-based approaches typically involve learning of uncertain dynamics and quantifying the learned model error, which leads to conservative filters before a large amount of data is collected to learn a good model, thereby preventing efficient exploration. This paper presents a method for safe and efficient RL using disturbance observers (DOBs) and control barrier functions (CBFs). Unlike most existing safe RL methods that deal with hard state constraints, our method does not involve model learning, and leverages DOBs to accurately estimate the pointwise value of the uncertainty, which is then incorporated into a robust CBF condition to generate safe actions. The DOB-based CBF can be used as a safety filter with model-free RL algorithms by minimally modifying the actions of an RL agent whenever necessary to ensure safety throughout the learning process. Simulation results on a unicycle and a 2D quadrotor demonstrate that the proposed method outperforms a state-of-the-art safe RL algorithm using CBFs and Gaussian processes-based model learning, in terms of safety violation rate, and sample and computational efficiency.",
        "keywords": []
      },
      "file_name": "9b5aa6a75d8e9f0e68d12e3b331acad6f32667d4.pdf"
    },
    {
      "success": true,
      "doc_id": "4f5be098fcfb6d1ef9f14b519355fe5a",
      "summary": ": Reinforcement learning (RL) is a promising optimal control technique for multi-energy management systems. It does not require a model a priori - reducing the upfront and ongoing project-specic engineering effort and is capable of learning better representations of the underlying system dynamics. However, vanilla RL does not provide constraint satisfaction guarantees - resulting in various unsafe interactions within its safety-critical environment. In this paper, we present two novel safe RL methods, namely SafeFallback and GiveSafe, where the safety constraint formulation is decoupled from the RL formulation and which provides hard-constraint satisfaction guarantees both during training (exploration) and exploitation of the (close-to) optimal policy. In a simulated multi-energy systems case study we have shown that both methods start with a signicantly higher utility (i.e. useful policy) compared to a vanilla RL benchmark (94,6% and 82,8% compared to 35,5%) and that the proposed SafeFallback method even can outperform the vanilla RL benchmark (102,9% to 100%). We conclude that both methods are viably safety constraint handling techniques capable beyond RL, as demonstrated with random agents while still providing hard-constraint guarantees. Finally, we propose fundamental future work to i.a. improve the constraint functions itself as more data becomes available.",
      "intriguing_abstract": ": Reinforcement learning (RL) is a promising optimal control technique for multi-energy management systems. It does not require a model a priori - reducing the upfront and ongoing project-specic engineering effort and is capable of learning better representations of the underlying system dynamics. However, vanilla RL does not provide constraint satisfaction guarantees - resulting in various unsafe interactions within its safety-critical environment. In this paper, we present two novel safe RL methods, namely SafeFallback and GiveSafe, where the safety constraint formulation is decoupled from the RL formulation and which provides hard-constraint satisfaction guarantees both during training (exploration) and exploitation of the (close-to) optimal policy. In a simulated multi-energy systems case study we have shown that both methods start with a signicantly higher utility (i.e. useful policy) compared to a vanilla RL benchmark (94,6% and 82,8% compared to 35,5%) and that the proposed SafeFallback method even can outperform the vanilla RL benchmark (102,9% to 100%). We conclude that both methods are viably safety constraint handling techniques capable beyond RL, as demonstrated with random agents while still providing hard-constraint guarantees. Finally, we propose fundamental future work to i.a. improve the constraint functions itself as more data becomes available.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/6c66fc8000da4d80bb57e60667e35a051016144a.pdf",
      "citation_key": "ceusters2022drp",
      "metadata": {
        "title": "Safe reinforcement learning for multi-energy management systems with known constraint functions",
        "authors": [
          "Glenn Ceusters",
          "L. R. Camargo",
          "R. Franke",
          "Ann Now'e",
          "M. Messagie"
        ],
        "published_date": "2022",
        "abstract": ": Reinforcement learning (RL) is a promising optimal control technique for multi-energy management systems. It does not require a model a priori - reducing the upfront and ongoing project-specic engineering effort and is capable of learning better representations of the underlying system dynamics. However, vanilla RL does not provide constraint satisfaction guarantees - resulting in various unsafe interactions within its safety-critical environment. In this paper, we present two novel safe RL methods, namely SafeFallback and GiveSafe, where the safety constraint formulation is decoupled from the RL formulation and which provides hard-constraint satisfaction guarantees both during training (exploration) and exploitation of the (close-to) optimal policy. In a simulated multi-energy systems case study we have shown that both methods start with a signicantly higher utility (i.e. useful policy) compared to a vanilla RL benchmark (94,6% and 82,8% compared to 35,5%) and that the proposed SafeFallback method even can outperform the vanilla RL benchmark (102,9% to 100%). We conclude that both methods are viably safety constraint handling techniques capable beyond RL, as demonstrated with random agents while still providing hard-constraint guarantees. Finally, we propose fundamental future work to i.a. improve the constraint functions itself as more data becomes available.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/6c66fc8000da4d80bb57e60667e35a051016144a.pdf",
        "venue": "Energy and AI",
        "citationCount": 18,
        "score": 6.0,
        "summary": ": Reinforcement learning (RL) is a promising optimal control technique for multi-energy management systems. It does not require a model a priori - reducing the upfront and ongoing project-specic engineering effort and is capable of learning better representations of the underlying system dynamics. However, vanilla RL does not provide constraint satisfaction guarantees - resulting in various unsafe interactions within its safety-critical environment. In this paper, we present two novel safe RL methods, namely SafeFallback and GiveSafe, where the safety constraint formulation is decoupled from the RL formulation and which provides hard-constraint satisfaction guarantees both during training (exploration) and exploitation of the (close-to) optimal policy. In a simulated multi-energy systems case study we have shown that both methods start with a signicantly higher utility (i.e. useful policy) compared to a vanilla RL benchmark (94,6% and 82,8% compared to 35,5%) and that the proposed SafeFallback method even can outperform the vanilla RL benchmark (102,9% to 100%). We conclude that both methods are viably safety constraint handling techniques capable beyond RL, as demonstrated with random agents while still providing hard-constraint guarantees. Finally, we propose fundamental future work to i.a. improve the constraint functions itself as more data becomes available.",
        "keywords": []
      },
      "file_name": "6c66fc8000da4d80bb57e60667e35a051016144a.pdf"
    },
    {
      "success": true,
      "doc_id": "a3df0157e5529bc8cd63c435e1fdb7ab",
      "summary": "Traditional exploration methods in RL require agents to perform random actions to find rewards. But these approaches struggle on sparse-reward domains like Montezuma's Revenge where the probability that any random action sequence leads to reward is extremely low. Recent algorithms have performed well on such tasks by encouraging agents to visit new states or perform new actions in relation to all prior training episodes (which we call across-training novelty). But such algorithms do not consider whether an agent exhibits intra-life novelty: doing something new within the current episode, regardless of whether those behaviors have been performed in previous episodes. We hypothesize that across-training novelty might discourage agents from revisiting initially non-rewarding states that could become important stepping stones later in training. We introduce Deep Curiosity Search (DeepCS), which encourages intra-life exploration by rewarding agents for visiting as many different states as possible within each episode, and show that DeepCS matches the performance of current state-of-the-art methods on Montezuma's Revenge. We further show that DeepCS improves exploration on Amidar, Freeway, Gravitar, and Tutankham (many of which are hard exploration games). Surprisingly, DeepCS doubles A2C performance on Seaquest, a game we would not have expected to benefit from intra-life exploration because the arena is small and already easily navigated by naive exploration techniques. In one run, DeepCS achieves a maximum training score of 80,000 points on Seaquest, higher than any methods other than Ape-X. The strong performance of DeepCS on these sparse- and dense-reward tasks suggests that encouraging intra-life novelty is an interesting, new approach for improving performance in Deep RL and motivates further research into hybridizing across-training and intra-life exploration methods.",
      "intriguing_abstract": "Traditional exploration methods in RL require agents to perform random actions to find rewards. But these approaches struggle on sparse-reward domains like Montezuma's Revenge where the probability that any random action sequence leads to reward is extremely low. Recent algorithms have performed well on such tasks by encouraging agents to visit new states or perform new actions in relation to all prior training episodes (which we call across-training novelty). But such algorithms do not consider whether an agent exhibits intra-life novelty: doing something new within the current episode, regardless of whether those behaviors have been performed in previous episodes. We hypothesize that across-training novelty might discourage agents from revisiting initially non-rewarding states that could become important stepping stones later in training. We introduce Deep Curiosity Search (DeepCS), which encourages intra-life exploration by rewarding agents for visiting as many different states as possible within each episode, and show that DeepCS matches the performance of current state-of-the-art methods on Montezuma's Revenge. We further show that DeepCS improves exploration on Amidar, Freeway, Gravitar, and Tutankham (many of which are hard exploration games). Surprisingly, DeepCS doubles A2C performance on Seaquest, a game we would not have expected to benefit from intra-life exploration because the arena is small and already easily navigated by naive exploration techniques. In one run, DeepCS achieves a maximum training score of 80,000 points on Seaquest, higher than any methods other than Ape-X. The strong performance of DeepCS on these sparse- and dense-reward tasks suggests that encouraging intra-life novelty is an interesting, new approach for improving performance in Deep RL and motivates further research into hybridizing across-training and intra-life exploration methods.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/5fd3ce235f5fcebd3d2807f710b060add527183b.pdf",
      "citation_key": "stanton20183fs",
      "metadata": {
        "title": "Deep Curiosity Search: Intra-Life Exploration Improves Performance on Challenging Deep Reinforcement Learning Problems",
        "authors": [
          "C. Stanton",
          "J. Clune"
        ],
        "published_date": "2018",
        "abstract": "Traditional exploration methods in RL require agents to perform random actions to find rewards. But these approaches struggle on sparse-reward domains like Montezuma's Revenge where the probability that any random action sequence leads to reward is extremely low. Recent algorithms have performed well on such tasks by encouraging agents to visit new states or perform new actions in relation to all prior training episodes (which we call across-training novelty). But such algorithms do not consider whether an agent exhibits intra-life novelty: doing something new within the current episode, regardless of whether those behaviors have been performed in previous episodes. We hypothesize that across-training novelty might discourage agents from revisiting initially non-rewarding states that could become important stepping stones later in training. We introduce Deep Curiosity Search (DeepCS), which encourages intra-life exploration by rewarding agents for visiting as many different states as possible within each episode, and show that DeepCS matches the performance of current state-of-the-art methods on Montezuma's Revenge. We further show that DeepCS improves exploration on Amidar, Freeway, Gravitar, and Tutankham (many of which are hard exploration games). Surprisingly, DeepCS doubles A2C performance on Seaquest, a game we would not have expected to benefit from intra-life exploration because the arena is small and already easily navigated by naive exploration techniques. In one run, DeepCS achieves a maximum training score of 80,000 points on Seaquest, higher than any methods other than Ape-X. The strong performance of DeepCS on these sparse- and dense-reward tasks suggests that encouraging intra-life novelty is an interesting, new approach for improving performance in Deep RL and motivates further research into hybridizing across-training and intra-life exploration methods.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/5fd3ce235f5fcebd3d2807f710b060add527183b.pdf",
        "venue": "arXiv.org",
        "citationCount": 42,
        "score": 6.0,
        "summary": "Traditional exploration methods in RL require agents to perform random actions to find rewards. But these approaches struggle on sparse-reward domains like Montezuma's Revenge where the probability that any random action sequence leads to reward is extremely low. Recent algorithms have performed well on such tasks by encouraging agents to visit new states or perform new actions in relation to all prior training episodes (which we call across-training novelty). But such algorithms do not consider whether an agent exhibits intra-life novelty: doing something new within the current episode, regardless of whether those behaviors have been performed in previous episodes. We hypothesize that across-training novelty might discourage agents from revisiting initially non-rewarding states that could become important stepping stones later in training. We introduce Deep Curiosity Search (DeepCS), which encourages intra-life exploration by rewarding agents for visiting as many different states as possible within each episode, and show that DeepCS matches the performance of current state-of-the-art methods on Montezuma's Revenge. We further show that DeepCS improves exploration on Amidar, Freeway, Gravitar, and Tutankham (many of which are hard exploration games). Surprisingly, DeepCS doubles A2C performance on Seaquest, a game we would not have expected to benefit from intra-life exploration because the arena is small and already easily navigated by naive exploration techniques. In one run, DeepCS achieves a maximum training score of 80,000 points on Seaquest, higher than any methods other than Ape-X. The strong performance of DeepCS on these sparse- and dense-reward tasks suggests that encouraging intra-life novelty is an interesting, new approach for improving performance in Deep RL and motivates further research into hybridizing across-training and intra-life exploration methods.",
        "keywords": []
      },
      "file_name": "5fd3ce235f5fcebd3d2807f710b060add527183b.pdf"
    },
    {
      "success": true,
      "doc_id": "afd97b480fd53ccacadb3a241972daba",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/a011901fd788fc5ad452dd3d88f9f0970ea547a8.pdf",
      "citation_key": "cideron2020kdj",
      "metadata": {
        "title": "QD-RL: Efficient Mixing of Quality and Diversity in Reinforcement Learning",
        "authors": [
          "Geoffrey Cideron",
          "Thomas Pierrot",
          "Nicolas Perrin",
          "Karim Beguir",
          "Olivier Sigaud"
        ],
        "published_date": "2020",
        "abstract": "",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/a011901fd788fc5ad452dd3d88f9f0970ea547a8.pdf",
        "venue": "arXiv.org",
        "citationCount": 29,
        "score": 5.800000000000001,
        "summary": "",
        "keywords": []
      },
      "file_name": "a011901fd788fc5ad452dd3d88f9f0970ea547a8.pdf"
    },
    {
      "success": true,
      "doc_id": "95ecee8ca0b24ad2426059b6f961d4b1",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/3efc894d0990faeb2f69194195d465ed64694104.pdf",
      "citation_key": "liu2022nhx",
      "metadata": {
        "title": "Feudal Latent Space Exploration for Coordinated Multi-Agent Reinforcement Learning",
        "authors": [
          "Xiangyu Liu",
          "Ying Tan"
        ],
        "published_date": "2022",
        "abstract": "",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/3efc894d0990faeb2f69194195d465ed64694104.pdf",
        "venue": "IEEE Transactions on Neural Networks and Learning Systems",
        "citationCount": 17,
        "score": 5.666666666666666,
        "summary": "",
        "keywords": []
      },
      "file_name": "3efc894d0990faeb2f69194195d465ed64694104.pdf"
    },
    {
      "success": true,
      "doc_id": "2f34df32e6f4208d905c5c8e72f85be2",
      "summary": "Current reinforcement learning (RL) in robotics often experiences difficulty in generalizing to new downstream tasks due to the innate task-specific training paradigm. To alleviate it, unsupervised RL, a framework that pre-trains the agent in a task-agnostic manner without access to the task-specific reward, leverages active exploration for distilling diverse experience into essential skills or reusable knowledge. For exploiting such benefits also in robotic manipulation, we propose an unsupervised method for transferable manipulation skill discovery that ties structured exploration toward interacting behavior and transferable skill learning. It not only enables the agent to learn interaction behavior, the key aspect of the robotic manipulation learning, without access to the environment reward, but also to generalize to arbitrary downstream manipulation tasks with the learned task-agnostic skills. Through comparative experiments, we show that our approach achieves the most diverse interacting behavior and significantly improves sample efficiency in downstream tasks including the extension to multi-object, multitask problems.",
      "intriguing_abstract": "Current reinforcement learning (RL) in robotics often experiences difficulty in generalizing to new downstream tasks due to the innate task-specific training paradigm. To alleviate it, unsupervised RL, a framework that pre-trains the agent in a task-agnostic manner without access to the task-specific reward, leverages active exploration for distilling diverse experience into essential skills or reusable knowledge. For exploiting such benefits also in robotic manipulation, we propose an unsupervised method for transferable manipulation skill discovery that ties structured exploration toward interacting behavior and transferable skill learning. It not only enables the agent to learn interaction behavior, the key aspect of the robotic manipulation learning, without access to the environment reward, but also to generalize to arbitrary downstream manipulation tasks with the learned task-agnostic skills. Through comparative experiments, we show that our approach achieves the most diverse interacting behavior and significantly improves sample efficiency in downstream tasks including the extension to multi-object, multitask problems.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/46bd3c20e6f7a9b6e413ea9f3452965601fd6de9.pdf",
      "citation_key": "cho2022o2c",
      "metadata": {
        "title": "Unsupervised Reinforcement Learning for Transferable Manipulation Skill Discovery",
        "authors": [
          "Daesol Cho",
          "Jigang Kim",
          "H. J. Kim"
        ],
        "published_date": "2022",
        "abstract": "Current reinforcement learning (RL) in robotics often experiences difficulty in generalizing to new downstream tasks due to the innate task-specific training paradigm. To alleviate it, unsupervised RL, a framework that pre-trains the agent in a task-agnostic manner without access to the task-specific reward, leverages active exploration for distilling diverse experience into essential skills or reusable knowledge. For exploiting such benefits also in robotic manipulation, we propose an unsupervised method for transferable manipulation skill discovery that ties structured exploration toward interacting behavior and transferable skill learning. It not only enables the agent to learn interaction behavior, the key aspect of the robotic manipulation learning, without access to the environment reward, but also to generalize to arbitrary downstream manipulation tasks with the learned task-agnostic skills. Through comparative experiments, we show that our approach achieves the most diverse interacting behavior and significantly improves sample efficiency in downstream tasks including the extension to multi-object, multitask problems.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/46bd3c20e6f7a9b6e413ea9f3452965601fd6de9.pdf",
        "venue": "IEEE Robotics and Automation Letters",
        "citationCount": 17,
        "score": 5.666666666666666,
        "summary": "Current reinforcement learning (RL) in robotics often experiences difficulty in generalizing to new downstream tasks due to the innate task-specific training paradigm. To alleviate it, unsupervised RL, a framework that pre-trains the agent in a task-agnostic manner without access to the task-specific reward, leverages active exploration for distilling diverse experience into essential skills or reusable knowledge. For exploiting such benefits also in robotic manipulation, we propose an unsupervised method for transferable manipulation skill discovery that ties structured exploration toward interacting behavior and transferable skill learning. It not only enables the agent to learn interaction behavior, the key aspect of the robotic manipulation learning, without access to the environment reward, but also to generalize to arbitrary downstream manipulation tasks with the learned task-agnostic skills. Through comparative experiments, we show that our approach achieves the most diverse interacting behavior and significantly improves sample efficiency in downstream tasks including the extension to multi-object, multitask problems.",
        "keywords": []
      },
      "file_name": "46bd3c20e6f7a9b6e413ea9f3452965601fd6de9.pdf"
    },
    {
      "success": true,
      "doc_id": "bebd1b13108699f08b4a3465595740b8",
      "summary": "Meta reinforcement learning (meta-RL) extracts knowledge from previous tasks and achieves fast adaptation to new tasks. Despite recent progress, efficient exploration in meta-RL remains a key challenge in sparse-reward tasks, as it requires quickly finding informative task-relevant experiences in both meta-training and adaptation. To address this challenge, we explicitly model an exploration policy learning problem for meta-RL, which is separated from exploitation policy learning, and introduce a novel empowerment-driven exploration objective, which aims to maximize information gain for task identification. We derive a corresponding intrinsic reward and develop a new off-policy meta-RL framework, which efficiently learns separate context-aware exploration and exploitation policies by sharing the knowledge of task inference. Experimental evaluation shows that our meta-RL method significantly outperforms state-of-the-art baselines on various sparse-reward MuJoCo locomotion tasks and more complex sparse-reward Meta-World tasks.",
      "intriguing_abstract": "Meta reinforcement learning (meta-RL) extracts knowledge from previous tasks and achieves fast adaptation to new tasks. Despite recent progress, efficient exploration in meta-RL remains a key challenge in sparse-reward tasks, as it requires quickly finding informative task-relevant experiences in both meta-training and adaptation. To address this challenge, we explicitly model an exploration policy learning problem for meta-RL, which is separated from exploitation policy learning, and introduce a novel empowerment-driven exploration objective, which aims to maximize information gain for task identification. We derive a corresponding intrinsic reward and develop a new off-policy meta-RL framework, which efficiently learns separate context-aware exploration and exploitation policies by sharing the knowledge of task inference. Experimental evaluation shows that our meta-RL method significantly outperforms state-of-the-art baselines on various sparse-reward MuJoCo locomotion tasks and more complex sparse-reward Meta-World tasks.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/1d4288c47a7802575d2a0d231d1283a4f225a85b.pdf",
      "citation_key": "zhang2020xq9",
      "metadata": {
        "title": "MetaCURE: Meta Reinforcement Learning with Empowerment-Driven Exploration",
        "authors": [
          "Jin Zhang",
          "Jianhao Wang",
          "Hao Hu",
          "Tong Chen",
          "Yingfeng Chen",
          "Changjie Fan",
          "Chongjie Zhang"
        ],
        "published_date": "2020",
        "abstract": "Meta reinforcement learning (meta-RL) extracts knowledge from previous tasks and achieves fast adaptation to new tasks. Despite recent progress, efficient exploration in meta-RL remains a key challenge in sparse-reward tasks, as it requires quickly finding informative task-relevant experiences in both meta-training and adaptation. To address this challenge, we explicitly model an exploration policy learning problem for meta-RL, which is separated from exploitation policy learning, and introduce a novel empowerment-driven exploration objective, which aims to maximize information gain for task identification. We derive a corresponding intrinsic reward and develop a new off-policy meta-RL framework, which efficiently learns separate context-aware exploration and exploitation policies by sharing the knowledge of task inference. Experimental evaluation shows that our meta-RL method significantly outperforms state-of-the-art baselines on various sparse-reward MuJoCo locomotion tasks and more complex sparse-reward Meta-World tasks.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/1d4288c47a7802575d2a0d231d1283a4f225a85b.pdf",
        "venue": "International Conference on Machine Learning",
        "citationCount": 28,
        "score": 5.6000000000000005,
        "summary": "Meta reinforcement learning (meta-RL) extracts knowledge from previous tasks and achieves fast adaptation to new tasks. Despite recent progress, efficient exploration in meta-RL remains a key challenge in sparse-reward tasks, as it requires quickly finding informative task-relevant experiences in both meta-training and adaptation. To address this challenge, we explicitly model an exploration policy learning problem for meta-RL, which is separated from exploitation policy learning, and introduce a novel empowerment-driven exploration objective, which aims to maximize information gain for task identification. We derive a corresponding intrinsic reward and develop a new off-policy meta-RL framework, which efficiently learns separate context-aware exploration and exploitation policies by sharing the knowledge of task inference. Experimental evaluation shows that our meta-RL method significantly outperforms state-of-the-art baselines on various sparse-reward MuJoCo locomotion tasks and more complex sparse-reward Meta-World tasks.",
        "keywords": []
      },
      "file_name": "1d4288c47a7802575d2a0d231d1283a4f225a85b.pdf"
    },
    {
      "success": true,
      "doc_id": "e2f25f225dda885d1965d3c514c8a1b7",
      "summary": "Model-based Reinforcement Learning (RL) is a popular learning paradigm due to its potential sample efficiency compared to model-free RL. However, existing empirical model-based RL approaches lack the ability to explore. This work studies a computationally and statistically efficient model-based algorithm for both Kernelized Nonlinear Regulators (KNR) and linear Markov Decision Processes (MDPs). For both models, our algorithm guarantees polynomial sample complexity and only uses access to a planning oracle. Experimentally, we first demonstrate the flexibility and efficacy of our algorithm on a set of exploration challenging control tasks where existing empirical model-based RL approaches completely fail. We then show that our approach retains excellent performance even in common dense reward control benchmarks that do not require heavy exploration. Finally, we demonstrate that our method can also perform reward-free exploration efficiently. Our code can be found at https://github.com/yudasong/PCMLP.",
      "intriguing_abstract": "Model-based Reinforcement Learning (RL) is a popular learning paradigm due to its potential sample efficiency compared to model-free RL. However, existing empirical model-based RL approaches lack the ability to explore. This work studies a computationally and statistically efficient model-based algorithm for both Kernelized Nonlinear Regulators (KNR) and linear Markov Decision Processes (MDPs). For both models, our algorithm guarantees polynomial sample complexity and only uses access to a planning oracle. Experimentally, we first demonstrate the flexibility and efficacy of our algorithm on a set of exploration challenging control tasks where existing empirical model-based RL approaches completely fail. We then show that our approach retains excellent performance even in common dense reward control benchmarks that do not require heavy exploration. Finally, we demonstrate that our method can also perform reward-free exploration efficiently. Our code can be found at https://github.com/yudasong/PCMLP.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/1a39ad2d1553d07084b5e44a28878c8bb5018cef.pdf",
      "citation_key": "song2021elb",
      "metadata": {
        "title": "PC-MLP: Model-based Reinforcement Learning with Policy Cover Guided Exploration",
        "authors": [
          "Yuda Song",
          "Wen Sun"
        ],
        "published_date": "2021",
        "abstract": "Model-based Reinforcement Learning (RL) is a popular learning paradigm due to its potential sample efficiency compared to model-free RL. However, existing empirical model-based RL approaches lack the ability to explore. This work studies a computationally and statistically efficient model-based algorithm for both Kernelized Nonlinear Regulators (KNR) and linear Markov Decision Processes (MDPs). For both models, our algorithm guarantees polynomial sample complexity and only uses access to a planning oracle. Experimentally, we first demonstrate the flexibility and efficacy of our algorithm on a set of exploration challenging control tasks where existing empirical model-based RL approaches completely fail. We then show that our approach retains excellent performance even in common dense reward control benchmarks that do not require heavy exploration. Finally, we demonstrate that our method can also perform reward-free exploration efficiently. Our code can be found at https://github.com/yudasong/PCMLP.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/1a39ad2d1553d07084b5e44a28878c8bb5018cef.pdf",
        "venue": "International Conference on Machine Learning",
        "citationCount": 22,
        "score": 5.5,
        "summary": "Model-based Reinforcement Learning (RL) is a popular learning paradigm due to its potential sample efficiency compared to model-free RL. However, existing empirical model-based RL approaches lack the ability to explore. This work studies a computationally and statistically efficient model-based algorithm for both Kernelized Nonlinear Regulators (KNR) and linear Markov Decision Processes (MDPs). For both models, our algorithm guarantees polynomial sample complexity and only uses access to a planning oracle. Experimentally, we first demonstrate the flexibility and efficacy of our algorithm on a set of exploration challenging control tasks where existing empirical model-based RL approaches completely fail. We then show that our approach retains excellent performance even in common dense reward control benchmarks that do not require heavy exploration. Finally, we demonstrate that our method can also perform reward-free exploration efficiently. Our code can be found at https://github.com/yudasong/PCMLP.",
        "keywords": []
      },
      "file_name": "1a39ad2d1553d07084b5e44a28878c8bb5018cef.pdf"
    },
    {
      "success": true,
      "doc_id": "6f20bab79f10240b22533c8b6ff9e73b",
      "summary": "Task scheduling is a critical problem when one user offloads multiple different tasks to the edge server. When a user has multiple tasks to offload and only one task can be transmitted to server at a time, while server processes tasks according to the transmission order, the problem is NP-hard. However, it is difficult for traditional optimization methods to quickly obtain the optimal solution, while approaches based on reinforcement learning face with the challenge of excessively large action space and slow convergence. In this paper, we propose a Digital Twin (DT)-assisted RL-based task scheduling method in order to improve the performance and convergence of the RL. We use DT to simulate the results of different decisions made by the agent, so that one agent can try multiple actions at a time, or, similarly, multiple agents can interact with environment in parallel in DT. In this way, the exploration efficiency of RL can be significantly improved via DT, and thus RL can converges faster and local optimality is less likely to happen. Particularly, two algorithms are designed to made task scheduling decisions, i.e., DT-assisted asynchronous Q-learning (DTAQL) and DT-assisted exploring Q-learning (DTEQL). Simulation results show that both algorithms significantly improve the convergence speed of Q-learning by increasing the exploration efficiency.",
      "intriguing_abstract": "Task scheduling is a critical problem when one user offloads multiple different tasks to the edge server. When a user has multiple tasks to offload and only one task can be transmitted to server at a time, while server processes tasks according to the transmission order, the problem is NP-hard. However, it is difficult for traditional optimization methods to quickly obtain the optimal solution, while approaches based on reinforcement learning face with the challenge of excessively large action space and slow convergence. In this paper, we propose a Digital Twin (DT)-assisted RL-based task scheduling method in order to improve the performance and convergence of the RL. We use DT to simulate the results of different decisions made by the agent, so that one agent can try multiple actions at a time, or, similarly, multiple agents can interact with environment in parallel in DT. In this way, the exploration efficiency of RL can be significantly improved via DT, and thus RL can converges faster and local optimality is less likely to happen. Particularly, two algorithms are designed to made task scheduling decisions, i.e., DT-assisted asynchronous Q-learning (DTAQL) and DT-assisted exploring Q-learning (DTEQL). Simulation results show that both algorithms significantly improve the convergence speed of Q-learning by increasing the exploration efficiency.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/ecf5dc817fd6326e943b759c889d1285e673b24a.pdf",
      "citation_key": "wang20229ce",
      "metadata": {
        "title": "Digital Twin-Assisted Efficient Reinforcement Learning for Edge Task Scheduling",
        "authors": [
          "Xiucheng Wang",
          "Longfei Ma",
          "Hao Li",
          "Zhisheng Yin",
          "T. Luan",
          "Nan Cheng"
        ],
        "published_date": "2022",
        "abstract": "Task scheduling is a critical problem when one user offloads multiple different tasks to the edge server. When a user has multiple tasks to offload and only one task can be transmitted to server at a time, while server processes tasks according to the transmission order, the problem is NP-hard. However, it is difficult for traditional optimization methods to quickly obtain the optimal solution, while approaches based on reinforcement learning face with the challenge of excessively large action space and slow convergence. In this paper, we propose a Digital Twin (DT)-assisted RL-based task scheduling method in order to improve the performance and convergence of the RL. We use DT to simulate the results of different decisions made by the agent, so that one agent can try multiple actions at a time, or, similarly, multiple agents can interact with environment in parallel in DT. In this way, the exploration efficiency of RL can be significantly improved via DT, and thus RL can converges faster and local optimality is less likely to happen. Particularly, two algorithms are designed to made task scheduling decisions, i.e., DT-assisted asynchronous Q-learning (DTAQL) and DT-assisted exploring Q-learning (DTEQL). Simulation results show that both algorithms significantly improve the convergence speed of Q-learning by increasing the exploration efficiency.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/ecf5dc817fd6326e943b759c889d1285e673b24a.pdf",
        "venue": "IEEE Vehicular Technology Conference",
        "citationCount": 16,
        "score": 5.333333333333333,
        "summary": "Task scheduling is a critical problem when one user offloads multiple different tasks to the edge server. When a user has multiple tasks to offload and only one task can be transmitted to server at a time, while server processes tasks according to the transmission order, the problem is NP-hard. However, it is difficult for traditional optimization methods to quickly obtain the optimal solution, while approaches based on reinforcement learning face with the challenge of excessively large action space and slow convergence. In this paper, we propose a Digital Twin (DT)-assisted RL-based task scheduling method in order to improve the performance and convergence of the RL. We use DT to simulate the results of different decisions made by the agent, so that one agent can try multiple actions at a time, or, similarly, multiple agents can interact with environment in parallel in DT. In this way, the exploration efficiency of RL can be significantly improved via DT, and thus RL can converges faster and local optimality is less likely to happen. Particularly, two algorithms are designed to made task scheduling decisions, i.e., DT-assisted asynchronous Q-learning (DTAQL) and DT-assisted exploring Q-learning (DTEQL). Simulation results show that both algorithms significantly improve the convergence speed of Q-learning by increasing the exploration efficiency.",
        "keywords": []
      },
      "file_name": "ecf5dc817fd6326e943b759c889d1285e673b24a.pdf"
    },
    {
      "success": true,
      "doc_id": "af3f9f4e04d81b196789baab408f9fa5",
      "summary": "Existing offline reinforcement learning (RL) methods face a few major challenges, particularly the distributional shift between the learned policy and the behavior policy. Offline Meta-RL is emerging as a promising approach to address these challenges, aiming to learn an informative meta-policy from a collection of tasks. Nevertheless, as shown in our empirical studies, offline Meta-RL could be outperformed by offline single-task RL methods on tasks with good quality of datasets, indicating that a right balance has to be delicately calibrated between\"exploring\"the out-of-distribution state-actions by following the meta-policy and\"exploiting\"the offline dataset by staying close to the behavior policy. Motivated by such empirical analysis, we explore model-based offline Meta-RL with regularized Policy Optimization (MerPO), which learns a meta-model for efficient task structure inference and an informative meta-policy for safe exploration of out-of-distribution state-actions. In particular, we devise a new meta-Regularized model-based Actor-Critic (RAC) method for within-task policy optimization, as a key building block of MerPO, using conservative policy evaluation and regularized policy improvement; and the intrinsic tradeoff therein is achieved via striking the right balance between two regularizers, one based on the behavior policy and the other on the meta-policy. We theoretically show that the learnt policy offers guaranteed improvement over both the behavior policy and the meta-policy, thus ensuring the performance improvement on new tasks via offline Meta-RL. Experiments corroborate the superior performance of MerPO over existing offline Meta-RL methods.",
      "intriguing_abstract": "Existing offline reinforcement learning (RL) methods face a few major challenges, particularly the distributional shift between the learned policy and the behavior policy. Offline Meta-RL is emerging as a promising approach to address these challenges, aiming to learn an informative meta-policy from a collection of tasks. Nevertheless, as shown in our empirical studies, offline Meta-RL could be outperformed by offline single-task RL methods on tasks with good quality of datasets, indicating that a right balance has to be delicately calibrated between\"exploring\"the out-of-distribution state-actions by following the meta-policy and\"exploiting\"the offline dataset by staying close to the behavior policy. Motivated by such empirical analysis, we explore model-based offline Meta-RL with regularized Policy Optimization (MerPO), which learns a meta-model for efficient task structure inference and an informative meta-policy for safe exploration of out-of-distribution state-actions. In particular, we devise a new meta-Regularized model-based Actor-Critic (RAC) method for within-task policy optimization, as a key building block of MerPO, using conservative policy evaluation and regularized policy improvement; and the intrinsic tradeoff therein is achieved via striking the right balance between two regularizers, one based on the behavior policy and the other on the meta-policy. We theoretically show that the learnt policy offers guaranteed improvement over both the behavior policy and the meta-policy, thus ensuring the performance improvement on new tasks via offline Meta-RL. Experiments corroborate the superior performance of MerPO over existing offline Meta-RL methods.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/02ad21eea9ec32783ba529487e74a76e85499a53.pdf",
      "citation_key": "lin2022vqo",
      "metadata": {
        "title": "Model-Based Offline Meta-Reinforcement Learning with Regularization",
        "authors": [
          "Sen Lin",
          "Jialin Wan",
          "Tengyu Xu",
          "Yingbin Liang",
          "Junshan Zhang"
        ],
        "published_date": "2022",
        "abstract": "Existing offline reinforcement learning (RL) methods face a few major challenges, particularly the distributional shift between the learned policy and the behavior policy. Offline Meta-RL is emerging as a promising approach to address these challenges, aiming to learn an informative meta-policy from a collection of tasks. Nevertheless, as shown in our empirical studies, offline Meta-RL could be outperformed by offline single-task RL methods on tasks with good quality of datasets, indicating that a right balance has to be delicately calibrated between\"exploring\"the out-of-distribution state-actions by following the meta-policy and\"exploiting\"the offline dataset by staying close to the behavior policy. Motivated by such empirical analysis, we explore model-based offline Meta-RL with regularized Policy Optimization (MerPO), which learns a meta-model for efficient task structure inference and an informative meta-policy for safe exploration of out-of-distribution state-actions. In particular, we devise a new meta-Regularized model-based Actor-Critic (RAC) method for within-task policy optimization, as a key building block of MerPO, using conservative policy evaluation and regularized policy improvement; and the intrinsic tradeoff therein is achieved via striking the right balance between two regularizers, one based on the behavior policy and the other on the meta-policy. We theoretically show that the learnt policy offers guaranteed improvement over both the behavior policy and the meta-policy, thus ensuring the performance improvement on new tasks via offline Meta-RL. Experiments corroborate the superior performance of MerPO over existing offline Meta-RL methods.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/02ad21eea9ec32783ba529487e74a76e85499a53.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 16,
        "score": 5.333333333333333,
        "summary": "Existing offline reinforcement learning (RL) methods face a few major challenges, particularly the distributional shift between the learned policy and the behavior policy. Offline Meta-RL is emerging as a promising approach to address these challenges, aiming to learn an informative meta-policy from a collection of tasks. Nevertheless, as shown in our empirical studies, offline Meta-RL could be outperformed by offline single-task RL methods on tasks with good quality of datasets, indicating that a right balance has to be delicately calibrated between\"exploring\"the out-of-distribution state-actions by following the meta-policy and\"exploiting\"the offline dataset by staying close to the behavior policy. Motivated by such empirical analysis, we explore model-based offline Meta-RL with regularized Policy Optimization (MerPO), which learns a meta-model for efficient task structure inference and an informative meta-policy for safe exploration of out-of-distribution state-actions. In particular, we devise a new meta-Regularized model-based Actor-Critic (RAC) method for within-task policy optimization, as a key building block of MerPO, using conservative policy evaluation and regularized policy improvement; and the intrinsic tradeoff therein is achieved via striking the right balance between two regularizers, one based on the behavior policy and the other on the meta-policy. We theoretically show that the learnt policy offers guaranteed improvement over both the behavior policy and the meta-policy, thus ensuring the performance improvement on new tasks via offline Meta-RL. Experiments corroborate the superior performance of MerPO over existing offline Meta-RL methods.",
        "keywords": []
      },
      "file_name": "02ad21eea9ec32783ba529487e74a76e85499a53.pdf"
    },
    {
      "success": true,
      "doc_id": "04a36df55932f0814dfba28024b13272",
      "summary": "GUI testing has been prevailing in software testing. However, existing automated GUI testing tools mostly rely on frameworks of a specific platform. Testers have to fully understand platform features before developing platform-dependent GUI testing tools. Starting from the perspective of testers vision, we observe that GUIs on different platforms share commonalities of widget images and layout designs, which can be leveraged to achieve platform-independent testing. We propose UniRLTest, an automated software testing framework, to achieve platform independence testing. UniRLTest utilizes computer vision techniques to capture all the widgets in the screenshot and constructs a widget tree for each page. A set of all the executable actions in each tree will be generated accordingly. UniRLTest adopts a Deep Q-Network, a reinforcement learning (RL) method, to the exploration process and formalize the Android GUI testing problem to a Marcov Decision Process (MDP), where RL could work. We have conducted evaluation experiments on 25 applications from different platforms. The result shows that UniRLTest outperforms baselines in terms of efficiency and effectiveness.",
      "intriguing_abstract": "GUI testing has been prevailing in software testing. However, existing automated GUI testing tools mostly rely on frameworks of a specific platform. Testers have to fully understand platform features before developing platform-dependent GUI testing tools. Starting from the perspective of testers vision, we observe that GUIs on different platforms share commonalities of widget images and layout designs, which can be leveraged to achieve platform-independent testing. We propose UniRLTest, an automated software testing framework, to achieve platform independence testing. UniRLTest utilizes computer vision techniques to capture all the widgets in the screenshot and constructs a widget tree for each page. A set of all the executable actions in each tree will be generated accordingly. UniRLTest adopts a Deep Q-Network, a reinforcement learning (RL) method, to the exploration process and formalize the Android GUI testing problem to a Marcov Decision Process (MDP), where RL could work. We have conducted evaluation experiments on 25 applications from different platforms. The result shows that UniRLTest outperforms baselines in terms of efficiency and effectiveness.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/a4a509d9019deac486087a0b10158ac115274de6.pdf",
      "citation_key": "zhang2022egf",
      "metadata": {
        "title": "UniRLTest: universal platform-independent testing with reinforcement learning via image understanding",
        "authors": [
          "Ziqian Zhang",
          "Yulei Liu",
          "Shengcheng Yu",
          "Xin Li",
          "Yexiao Yun",
          "Chunrong Fang",
          "Zhenyu Chen"
        ],
        "published_date": "2022",
        "abstract": "GUI testing has been prevailing in software testing. However, existing automated GUI testing tools mostly rely on frameworks of a specific platform. Testers have to fully understand platform features before developing platform-dependent GUI testing tools. Starting from the perspective of testers vision, we observe that GUIs on different platforms share commonalities of widget images and layout designs, which can be leveraged to achieve platform-independent testing. We propose UniRLTest, an automated software testing framework, to achieve platform independence testing. UniRLTest utilizes computer vision techniques to capture all the widgets in the screenshot and constructs a widget tree for each page. A set of all the executable actions in each tree will be generated accordingly. UniRLTest adopts a Deep Q-Network, a reinforcement learning (RL) method, to the exploration process and formalize the Android GUI testing problem to a Marcov Decision Process (MDP), where RL could work. We have conducted evaluation experiments on 25 applications from different platforms. The result shows that UniRLTest outperforms baselines in terms of efficiency and effectiveness.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/a4a509d9019deac486087a0b10158ac115274de6.pdf",
        "venue": "International Symposium on Software Testing and Analysis",
        "citationCount": 16,
        "score": 5.333333333333333,
        "summary": "GUI testing has been prevailing in software testing. However, existing automated GUI testing tools mostly rely on frameworks of a specific platform. Testers have to fully understand platform features before developing platform-dependent GUI testing tools. Starting from the perspective of testers vision, we observe that GUIs on different platforms share commonalities of widget images and layout designs, which can be leveraged to achieve platform-independent testing. We propose UniRLTest, an automated software testing framework, to achieve platform independence testing. UniRLTest utilizes computer vision techniques to capture all the widgets in the screenshot and constructs a widget tree for each page. A set of all the executable actions in each tree will be generated accordingly. UniRLTest adopts a Deep Q-Network, a reinforcement learning (RL) method, to the exploration process and formalize the Android GUI testing problem to a Marcov Decision Process (MDP), where RL could work. We have conducted evaluation experiments on 25 applications from different platforms. The result shows that UniRLTest outperforms baselines in terms of efficiency and effectiveness.",
        "keywords": []
      },
      "file_name": "a4a509d9019deac486087a0b10158ac115274de6.pdf"
    },
    {
      "success": true,
      "doc_id": "a6329196efe29ab24f1d790054ad59fc",
      "summary": "Efficient and effective exploration in continuous space is a central problem in applying reinforcement learning (RL) to autonomous driving. Skills learned from expert demonstrations or designed for specific tasks can benefit the exploration, but they are usually costly-collected, unbalanced/suboptimal, or failing to transfer to diverse tasks. However, human drivers can adapt to varied driving tasks without demonstrations by taking efficient and structural explorations in the entire skill space rather than a limited space with task-specific skills. Inspired by the above fact, we propose an RL algorithm exploring all feasible motion skills instead of a limited set of task-specific and object-centric skills. Without demonstrations, our method can still perform well in diverse tasks. First, we build a task-agnostic and ego-centric (TaEc) motion skill library in a pure motion perspective, which is diverse enough to be reusable in different complex tasks. The motion skills are then encoded into a low-dimension latent skill space, in which RL can do exploration efficiently. Validations in various challenging driving scenarios demonstrate that our proposed method, TaEc-RL, outperforms its counterparts significantly in learning efficiency and task performance.",
      "intriguing_abstract": "Efficient and effective exploration in continuous space is a central problem in applying reinforcement learning (RL) to autonomous driving. Skills learned from expert demonstrations or designed for specific tasks can benefit the exploration, but they are usually costly-collected, unbalanced/suboptimal, or failing to transfer to diverse tasks. However, human drivers can adapt to varied driving tasks without demonstrations by taking efficient and structural explorations in the entire skill space rather than a limited space with task-specific skills. Inspired by the above fact, we propose an RL algorithm exploring all feasible motion skills instead of a limited set of task-specific and object-centric skills. Without demonstrations, our method can still perform well in diverse tasks. First, we build a task-agnostic and ego-centric (TaEc) motion skill library in a pure motion perspective, which is diverse enough to be reusable in different complex tasks. The motion skills are then encoded into a low-dimension latent skill space, in which RL can do exploration efficiently. Validations in various challenging driving scenarios demonstrate that our proposed method, TaEc-RL, outperforms its counterparts significantly in learning efficiency and task performance.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/a17a7256c04afee68f9aa0b7bfdc67fbca998b9c.pdf",
      "citation_key": "zhou2022fny",
      "metadata": {
        "title": "Accelerating Reinforcement Learning for Autonomous Driving Using Task-Agnostic and Ego-Centric Motion Skills",
        "authors": [
          "Tong Zhou",
          "Letian Wang",
          "Ruobing Chen",
          "Wenshuo Wang",
          "Y. Liu"
        ],
        "published_date": "2022",
        "abstract": "Efficient and effective exploration in continuous space is a central problem in applying reinforcement learning (RL) to autonomous driving. Skills learned from expert demonstrations or designed for specific tasks can benefit the exploration, but they are usually costly-collected, unbalanced/suboptimal, or failing to transfer to diverse tasks. However, human drivers can adapt to varied driving tasks without demonstrations by taking efficient and structural explorations in the entire skill space rather than a limited space with task-specific skills. Inspired by the above fact, we propose an RL algorithm exploring all feasible motion skills instead of a limited set of task-specific and object-centric skills. Without demonstrations, our method can still perform well in diverse tasks. First, we build a task-agnostic and ego-centric (TaEc) motion skill library in a pure motion perspective, which is diverse enough to be reusable in different complex tasks. The motion skills are then encoded into a low-dimension latent skill space, in which RL can do exploration efficiently. Validations in various challenging driving scenarios demonstrate that our proposed method, TaEc-RL, outperforms its counterparts significantly in learning efficiency and task performance.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/a17a7256c04afee68f9aa0b7bfdc67fbca998b9c.pdf",
        "venue": "IEEE/RJS International Conference on Intelligent RObots and Systems",
        "citationCount": 16,
        "score": 5.333333333333333,
        "summary": "Efficient and effective exploration in continuous space is a central problem in applying reinforcement learning (RL) to autonomous driving. Skills learned from expert demonstrations or designed for specific tasks can benefit the exploration, but they are usually costly-collected, unbalanced/suboptimal, or failing to transfer to diverse tasks. However, human drivers can adapt to varied driving tasks without demonstrations by taking efficient and structural explorations in the entire skill space rather than a limited space with task-specific skills. Inspired by the above fact, we propose an RL algorithm exploring all feasible motion skills instead of a limited set of task-specific and object-centric skills. Without demonstrations, our method can still perform well in diverse tasks. First, we build a task-agnostic and ego-centric (TaEc) motion skill library in a pure motion perspective, which is diverse enough to be reusable in different complex tasks. The motion skills are then encoded into a low-dimension latent skill space, in which RL can do exploration efficiently. Validations in various challenging driving scenarios demonstrate that our proposed method, TaEc-RL, outperforms its counterparts significantly in learning efficiency and task performance.",
        "keywords": []
      },
      "file_name": "a17a7256c04afee68f9aa0b7bfdc67fbca998b9c.pdf"
    },
    {
      "success": true,
      "doc_id": "363f76ce89fb1c020f4ef3aede55771b",
      "summary": "Safe reinforcement learning (RL) that solves constraint-satisfactory policies provides a promising way to the broader safety-critical applications of RL in real-world problems such as robotics. Among all safe RL approaches, model-based methods reduce training time violations further due to their high sample efficiency. However, lacking safety robustness against the model uncertainties remains an issue in safe model-based RL, especially in training time safety. In this paper, we propose a distributional reachability certificate (DRC) and its Bellman equation to address model uncertainties and characterize robust persistently safe states. Furthermore, we build a safe RL framework to resolve constraints required by the DRC and its corresponding shield policy. We also devise a line search method to maintain safety and reach higher returns simultaneously while leveraging the shield policy. Comprehensive experiments on classical benchmarks such as constrained tracking and navigation indicate that the proposed algorithm achieves comparable returns with much fewer constraint violations during training. Our code is available at https://github.com/ManUtdMoon/Distributional-Reachability-Policy-Optimization. Note to PractitionersAlthough it has been proven that RL can be applied in complex robotics control tasks, the training process of an RL control policy induces frequent failures because the agent needs to learn safety through constraint violations. This issue hinders the promotion of RL because a large amount of failure of robots is too expensive to afford. This paper aims to reduce the training-time violations of RL-based control methods, enabling RL to be leveraged in a broader application area. To achieve the goal, we first introduce a safety quantity describing the distribution of potential constraint violations in the long term. By imposing constraints on the quantile of the safety distribution, we can realize safety robust to the model uncertainty, which is necessary for real-world robot learning with environment uncertainty. Second, we further devise a shield policy aiming to minimize the constraint violation. The policy will intervene when the agent is about to violate state constraints, further enhancing exploration safety. Third, we implement a line search method to find an action pursuing near-optimal performance when fulfilling safety requirements strictly. Our experimental results indicate that the proposed algorithm reduces training-time violations significantly while maintaining competitive task performance. We make a step towards applying RL safely in real-world tasks. Our future work includes conducting physical verification on real robots to evaluate the algorithm and improving safety further by starting from an initially safe control policy that comes from domain knowledge.",
      "intriguing_abstract": "Safe reinforcement learning (RL) that solves constraint-satisfactory policies provides a promising way to the broader safety-critical applications of RL in real-world problems such as robotics. Among all safe RL approaches, model-based methods reduce training time violations further due to their high sample efficiency. However, lacking safety robustness against the model uncertainties remains an issue in safe model-based RL, especially in training time safety. In this paper, we propose a distributional reachability certificate (DRC) and its Bellman equation to address model uncertainties and characterize robust persistently safe states. Furthermore, we build a safe RL framework to resolve constraints required by the DRC and its corresponding shield policy. We also devise a line search method to maintain safety and reach higher returns simultaneously while leveraging the shield policy. Comprehensive experiments on classical benchmarks such as constrained tracking and navigation indicate that the proposed algorithm achieves comparable returns with much fewer constraint violations during training. Our code is available at https://github.com/ManUtdMoon/Distributional-Reachability-Policy-Optimization. Note to PractitionersAlthough it has been proven that RL can be applied in complex robotics control tasks, the training process of an RL control policy induces frequent failures because the agent needs to learn safety through constraint violations. This issue hinders the promotion of RL because a large amount of failure of robots is too expensive to afford. This paper aims to reduce the training-time violations of RL-based control methods, enabling RL to be leveraged in a broader application area. To achieve the goal, we first introduce a safety quantity describing the distribution of potential constraint violations in the long term. By imposing constraints on the quantile of the safety distribution, we can realize safety robust to the model uncertainty, which is necessary for real-world robot learning with environment uncertainty. Second, we further devise a shield policy aiming to minimize the constraint violation. The policy will intervene when the agent is about to violate state constraints, further enhancing exploration safety. Third, we implement a line search method to find an action pursuing near-optimal performance when fulfilling safety requirements strictly. Our experimental results indicate that the proposed algorithm reduces training-time violations significantly while maintaining competitive task performance. We make a step towards applying RL safely in real-world tasks. Our future work includes conducting physical verification on real robots to evaluate the algorithm and improving safety further by starting from an initially safe control policy that comes from domain knowledge.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/69bdc99655204190697067c3da5296e544e6865d.pdf",
      "citation_key": "yu2022bo5",
      "metadata": {
        "title": "Safe Model-Based Reinforcement Learning With an Uncertainty-Aware Reachability Certificate",
        "authors": [
          "Dongjie Yu",
          "Wenjun Zou",
          "Yujie Yang",
          "Haitong Ma",
          "Sheng Li",
          "Yuming Yin",
          "Jianyu Chen",
          "Jingliang Duan"
        ],
        "published_date": "2022",
        "abstract": "Safe reinforcement learning (RL) that solves constraint-satisfactory policies provides a promising way to the broader safety-critical applications of RL in real-world problems such as robotics. Among all safe RL approaches, model-based methods reduce training time violations further due to their high sample efficiency. However, lacking safety robustness against the model uncertainties remains an issue in safe model-based RL, especially in training time safety. In this paper, we propose a distributional reachability certificate (DRC) and its Bellman equation to address model uncertainties and characterize robust persistently safe states. Furthermore, we build a safe RL framework to resolve constraints required by the DRC and its corresponding shield policy. We also devise a line search method to maintain safety and reach higher returns simultaneously while leveraging the shield policy. Comprehensive experiments on classical benchmarks such as constrained tracking and navigation indicate that the proposed algorithm achieves comparable returns with much fewer constraint violations during training. Our code is available at https://github.com/ManUtdMoon/Distributional-Reachability-Policy-Optimization. Note to PractitionersAlthough it has been proven that RL can be applied in complex robotics control tasks, the training process of an RL control policy induces frequent failures because the agent needs to learn safety through constraint violations. This issue hinders the promotion of RL because a large amount of failure of robots is too expensive to afford. This paper aims to reduce the training-time violations of RL-based control methods, enabling RL to be leveraged in a broader application area. To achieve the goal, we first introduce a safety quantity describing the distribution of potential constraint violations in the long term. By imposing constraints on the quantile of the safety distribution, we can realize safety robust to the model uncertainty, which is necessary for real-world robot learning with environment uncertainty. Second, we further devise a shield policy aiming to minimize the constraint violation. The policy will intervene when the agent is about to violate state constraints, further enhancing exploration safety. Third, we implement a line search method to find an action pursuing near-optimal performance when fulfilling safety requirements strictly. Our experimental results indicate that the proposed algorithm reduces training-time violations significantly while maintaining competitive task performance. We make a step towards applying RL safely in real-world tasks. Our future work includes conducting physical verification on real robots to evaluate the algorithm and improving safety further by starting from an initially safe control policy that comes from domain knowledge.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/69bdc99655204190697067c3da5296e544e6865d.pdf",
        "venue": "IEEE Transactions on Automation Science and Engineering",
        "citationCount": 16,
        "score": 5.333333333333333,
        "summary": "Safe reinforcement learning (RL) that solves constraint-satisfactory policies provides a promising way to the broader safety-critical applications of RL in real-world problems such as robotics. Among all safe RL approaches, model-based methods reduce training time violations further due to their high sample efficiency. However, lacking safety robustness against the model uncertainties remains an issue in safe model-based RL, especially in training time safety. In this paper, we propose a distributional reachability certificate (DRC) and its Bellman equation to address model uncertainties and characterize robust persistently safe states. Furthermore, we build a safe RL framework to resolve constraints required by the DRC and its corresponding shield policy. We also devise a line search method to maintain safety and reach higher returns simultaneously while leveraging the shield policy. Comprehensive experiments on classical benchmarks such as constrained tracking and navigation indicate that the proposed algorithm achieves comparable returns with much fewer constraint violations during training. Our code is available at https://github.com/ManUtdMoon/Distributional-Reachability-Policy-Optimization. Note to PractitionersAlthough it has been proven that RL can be applied in complex robotics control tasks, the training process of an RL control policy induces frequent failures because the agent needs to learn safety through constraint violations. This issue hinders the promotion of RL because a large amount of failure of robots is too expensive to afford. This paper aims to reduce the training-time violations of RL-based control methods, enabling RL to be leveraged in a broader application area. To achieve the goal, we first introduce a safety quantity describing the distribution of potential constraint violations in the long term. By imposing constraints on the quantile of the safety distribution, we can realize safety robust to the model uncertainty, which is necessary for real-world robot learning with environment uncertainty. Second, we further devise a shield policy aiming to minimize the constraint violation. The policy will intervene when the agent is about to violate state constraints, further enhancing exploration safety. Third, we implement a line search method to find an action pursuing near-optimal performance when fulfilling safety requirements strictly. Our experimental results indicate that the proposed algorithm reduces training-time violations significantly while maintaining competitive task performance. We make a step towards applying RL safely in real-world tasks. Our future work includes conducting physical verification on real robots to evaluate the algorithm and improving safety further by starting from an initially safe control policy that comes from domain knowledge.",
        "keywords": []
      },
      "file_name": "69bdc99655204190697067c3da5296e544e6865d.pdf"
    },
    {
      "success": true,
      "doc_id": "f0f10c928abc56b8119ff889effeee5a",
      "summary": "In this paper, we present a robotic model-based reinforcement learning method that combines ideas from model identification and model predictive control. We use a feature-based representation of the dynamics that allows the dynamics model to be fitted with a simple least squares procedure, and the features are identified from a high-level specification of the robot's morphology, consisting of the number and connectivity structure of its links. Model predictive control is then used to choose the actions under an optimistic model of the dynamics, which produces an efficient and goal-directed exploration strategy. We present real time experimental results on standard benchmark problems involving the pendulum, cartpole, and double pendulum systems. Experiments indicate that our method is able to learn a range of benchmark tasks substantially faster than the previous best methods. To evaluate our approach on a realistic robotic control task, we also demonstrate real time control of a simulated 7 degree of freedom arm.",
      "intriguing_abstract": "In this paper, we present a robotic model-based reinforcement learning method that combines ideas from model identification and model predictive control. We use a feature-based representation of the dynamics that allows the dynamics model to be fitted with a simple least squares procedure, and the features are identified from a high-level specification of the robot's morphology, consisting of the number and connectivity structure of its links. Model predictive control is then used to choose the actions under an optimistic model of the dynamics, which produces an efficient and goal-directed exploration strategy. We present real time experimental results on standard benchmark problems involving the pendulum, cartpole, and double pendulum systems. Experiments indicate that our method is able to learn a range of benchmark tasks substantially faster than the previous best methods. To evaluate our approach on a realistic robotic control task, we also demonstrate real time control of a simulated 7 degree of freedom arm.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/abaa1a3a8468473fd2827e49623eabc36ffaf8fe.pdf",
      "citation_key": "xie2015vwy",
      "metadata": {
        "title": "Model-based reinforcement learning with parametrized physical models and optimism-driven exploration",
        "authors": [
          "Christopher Xie",
          "S. Patil",
          "T. Moldovan",
          "S. Levine",
          "P. Abbeel"
        ],
        "published_date": "2015",
        "abstract": "In this paper, we present a robotic model-based reinforcement learning method that combines ideas from model identification and model predictive control. We use a feature-based representation of the dynamics that allows the dynamics model to be fitted with a simple least squares procedure, and the features are identified from a high-level specification of the robot's morphology, consisting of the number and connectivity structure of its links. Model predictive control is then used to choose the actions under an optimistic model of the dynamics, which produces an efficient and goal-directed exploration strategy. We present real time experimental results on standard benchmark problems involving the pendulum, cartpole, and double pendulum systems. Experiments indicate that our method is able to learn a range of benchmark tasks substantially faster than the previous best methods. To evaluate our approach on a realistic robotic control task, we also demonstrate real time control of a simulated 7 degree of freedom arm.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/abaa1a3a8468473fd2827e49623eabc36ffaf8fe.pdf",
        "venue": "IEEE International Conference on Robotics and Automation",
        "citationCount": 50,
        "score": 5.0,
        "summary": "In this paper, we present a robotic model-based reinforcement learning method that combines ideas from model identification and model predictive control. We use a feature-based representation of the dynamics that allows the dynamics model to be fitted with a simple least squares procedure, and the features are identified from a high-level specification of the robot's morphology, consisting of the number and connectivity structure of its links. Model predictive control is then used to choose the actions under an optimistic model of the dynamics, which produces an efficient and goal-directed exploration strategy. We present real time experimental results on standard benchmark problems involving the pendulum, cartpole, and double pendulum systems. Experiments indicate that our method is able to learn a range of benchmark tasks substantially faster than the previous best methods. To evaluate our approach on a realistic robotic control task, we also demonstrate real time control of a simulated 7 degree of freedom arm.",
        "keywords": []
      },
      "file_name": "abaa1a3a8468473fd2827e49623eabc36ffaf8fe.pdf"
    },
    {
      "success": true,
      "doc_id": "edf264b59fe8057875070217f6c398ea",
      "summary": "In this paper, a multi-agent deep reinforcement learning method was adopted to realize cooperative spectrum sensing in cognitive radio networks. Each secondary user learns an efficient sensing strategy from the sensing results of some of the selected spectra to avoid interference to the primary users and to coordinate with other secondary users. It is necessary to balance exploration and exploitation in the learning process when using deep reinforcement learning methods, helping explain that upper confidence bound with Hoeffding-style bonus has been adopted in this paper to improve the efficiency of exploration. The simulation results verify that the proposed algorithm, when compared with the conventional reinforcement learning methods with $\\varepsilon $ -greedy exploration, is much easier to achieve faster convergence speed and better reward performance.",
      "intriguing_abstract": "In this paper, a multi-agent deep reinforcement learning method was adopted to realize cooperative spectrum sensing in cognitive radio networks. Each secondary user learns an efficient sensing strategy from the sensing results of some of the selected spectra to avoid interference to the primary users and to coordinate with other secondary users. It is necessary to balance exploration and exploitation in the learning process when using deep reinforcement learning methods, helping explain that upper confidence bound with Hoeffding-style bonus has been adopted in this paper to improve the efficiency of exploration. The simulation results verify that the proposed algorithm, when compared with the conventional reinforcement learning methods with $\\varepsilon $ -greedy exploration, is much easier to achieve faster convergence speed and better reward performance.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/2029ebd195491dd845e14866045225b238f6c392.pdf",
      "citation_key": "zhang2019yjm",
      "metadata": {
        "title": "Multi-Agent Deep Reinforcement Learning-Based Cooperative Spectrum Sensing With Upper Confidence Bound Exploration",
        "authors": [
          "Yu Zhang",
          "Peixiang Cai",
          "Changyong Pan",
          "Subing Zhang"
        ],
        "published_date": "2019",
        "abstract": "In this paper, a multi-agent deep reinforcement learning method was adopted to realize cooperative spectrum sensing in cognitive radio networks. Each secondary user learns an efficient sensing strategy from the sensing results of some of the selected spectra to avoid interference to the primary users and to coordinate with other secondary users. It is necessary to balance exploration and exploitation in the learning process when using deep reinforcement learning methods, helping explain that upper confidence bound with Hoeffding-style bonus has been adopted in this paper to improve the efficiency of exploration. The simulation results verify that the proposed algorithm, when compared with the conventional reinforcement learning methods with $\\varepsilon $ -greedy exploration, is much easier to achieve faster convergence speed and better reward performance.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/2029ebd195491dd845e14866045225b238f6c392.pdf",
        "venue": "IEEE Access",
        "citationCount": 29,
        "score": 4.833333333333333,
        "summary": "In this paper, a multi-agent deep reinforcement learning method was adopted to realize cooperative spectrum sensing in cognitive radio networks. Each secondary user learns an efficient sensing strategy from the sensing results of some of the selected spectra to avoid interference to the primary users and to coordinate with other secondary users. It is necessary to balance exploration and exploitation in the learning process when using deep reinforcement learning methods, helping explain that upper confidence bound with Hoeffding-style bonus has been adopted in this paper to improve the efficiency of exploration. The simulation results verify that the proposed algorithm, when compared with the conventional reinforcement learning methods with $\\varepsilon $ -greedy exploration, is much easier to achieve faster convergence speed and better reward performance.",
        "keywords": []
      },
      "file_name": "2029ebd195491dd845e14866045225b238f6c392.pdf"
    },
    {
      "success": true,
      "doc_id": "b563ea7b2efb087ac0ff2169bd71b945",
      "summary": "Magnetic flux leakage (MFL) inspection is one of the most commonly used nondestructive evaluation (NDT) methods for detecting anomalies of ferromagnetic materials. Sizing of the defect with MFL signal is the key problem of inspection, which is important for evaluating the health condition of the material. However, it is an ill-posed inverse problem that is hard to solve and harder to be accurate. Forward models that give a highly accurate simulated signal with a corresponding depth profile are widely used in solving the inverse problem iteratively. Unfortunately, the policy which determines the iteration process is hard to design. In this article, a reinforcement learning (RL)-based algorithm is proposed to reconstruct the depth of defects with a complex depth profile. Instead of designing the policy, the iterative process of the classic iteration-based method is embedded into the learning process of the RL-based algorithm proposed in this article. The policy is learned from the data generated during the reconstructing iteration. By designing the states, actions, and rewards in the RL structure, the most computationally costly part of calling the forward model during iteration is avoided. An adaptive limited exploration process is given to balance the exploration and exploitation in the inverse problem of MFL inspection in this article. The effectiveness of the proposed algorithm is demonstrated with simulation results under different noise levels. The results demonstrate that the proposed algorithm is robust with good reconstruction accuracy.",
      "intriguing_abstract": "Magnetic flux leakage (MFL) inspection is one of the most commonly used nondestructive evaluation (NDT) methods for detecting anomalies of ferromagnetic materials. Sizing of the defect with MFL signal is the key problem of inspection, which is important for evaluating the health condition of the material. However, it is an ill-posed inverse problem that is hard to solve and harder to be accurate. Forward models that give a highly accurate simulated signal with a corresponding depth profile are widely used in solving the inverse problem iteratively. Unfortunately, the policy which determines the iteration process is hard to design. In this article, a reinforcement learning (RL)-based algorithm is proposed to reconstruct the depth of defects with a complex depth profile. Instead of designing the policy, the iterative process of the classic iteration-based method is embedded into the learning process of the RL-based algorithm proposed in this article. The policy is learned from the data generated during the reconstructing iteration. By designing the states, actions, and rewards in the RL structure, the most computationally costly part of calling the forward model during iteration is avoided. An adaptive limited exploration process is given to balance the exploration and exploitation in the inverse problem of MFL inspection in this article. The effectiveness of the proposed algorithm is demonstrated with simulation results under different noise levels. The results demonstrate that the proposed algorithm is robust with good reconstruction accuracy.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/c447bc7891c2a3a775af4f8fb94e34e7968c1cb7.pdf",
      "citation_key": "wu2021mht",
      "metadata": {
        "title": "A Reinforcement Learning-Based Reconstruction Method for Complex Defect Profiles in MFL Inspection",
        "authors": [
          "Zhenning Wu",
          "Yiming Deng",
          "Jinhai Liu",
          "Lixing Wang"
        ],
        "published_date": "2021",
        "abstract": "Magnetic flux leakage (MFL) inspection is one of the most commonly used nondestructive evaluation (NDT) methods for detecting anomalies of ferromagnetic materials. Sizing of the defect with MFL signal is the key problem of inspection, which is important for evaluating the health condition of the material. However, it is an ill-posed inverse problem that is hard to solve and harder to be accurate. Forward models that give a highly accurate simulated signal with a corresponding depth profile are widely used in solving the inverse problem iteratively. Unfortunately, the policy which determines the iteration process is hard to design. In this article, a reinforcement learning (RL)-based algorithm is proposed to reconstruct the depth of defects with a complex depth profile. Instead of designing the policy, the iterative process of the classic iteration-based method is embedded into the learning process of the RL-based algorithm proposed in this article. The policy is learned from the data generated during the reconstructing iteration. By designing the states, actions, and rewards in the RL structure, the most computationally costly part of calling the forward model during iteration is avoided. An adaptive limited exploration process is given to balance the exploration and exploitation in the inverse problem of MFL inspection in this article. The effectiveness of the proposed algorithm is demonstrated with simulation results under different noise levels. The results demonstrate that the proposed algorithm is robust with good reconstruction accuracy.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/c447bc7891c2a3a775af4f8fb94e34e7968c1cb7.pdf",
        "venue": "IEEE Transactions on Instrumentation and Measurement",
        "citationCount": 19,
        "score": 4.75,
        "summary": "Magnetic flux leakage (MFL) inspection is one of the most commonly used nondestructive evaluation (NDT) methods for detecting anomalies of ferromagnetic materials. Sizing of the defect with MFL signal is the key problem of inspection, which is important for evaluating the health condition of the material. However, it is an ill-posed inverse problem that is hard to solve and harder to be accurate. Forward models that give a highly accurate simulated signal with a corresponding depth profile are widely used in solving the inverse problem iteratively. Unfortunately, the policy which determines the iteration process is hard to design. In this article, a reinforcement learning (RL)-based algorithm is proposed to reconstruct the depth of defects with a complex depth profile. Instead of designing the policy, the iterative process of the classic iteration-based method is embedded into the learning process of the RL-based algorithm proposed in this article. The policy is learned from the data generated during the reconstructing iteration. By designing the states, actions, and rewards in the RL structure, the most computationally costly part of calling the forward model during iteration is avoided. An adaptive limited exploration process is given to balance the exploration and exploitation in the inverse problem of MFL inspection in this article. The effectiveness of the proposed algorithm is demonstrated with simulation results under different noise levels. The results demonstrate that the proposed algorithm is robust with good reconstruction accuracy.",
        "keywords": []
      },
      "file_name": "c447bc7891c2a3a775af4f8fb94e34e7968c1cb7.pdf"
    },
    {
      "success": true,
      "doc_id": "93f6a2cbac8c8438bfcd4898fd590c4e",
      "summary": "We propose a model-based lifelong reinforcement-learning approach that estimates a hierarchical Bayesian posterior distilling the common structure shared across different tasks. The learned posterior combined with a sample-based Bayesian exploration procedure increases the sample efficiency of learning across a family of related tasks. We first derive an analysis of the relationship between the sample complexity and the initialization quality of the posterior in the finite MDP setting. We next scale the approach to continuous-state domains by introducing a Variational Bayesian Lifelong Reinforcement Learning algorithm that can be combined with recent model-based deep RL methods, and that exhibits backward transfer. Experimental results on several challenging domains show that our algorithms achieve both better forward and backward transfer performance than state-of-the-art lifelong RL methods.",
      "intriguing_abstract": "We propose a model-based lifelong reinforcement-learning approach that estimates a hierarchical Bayesian posterior distilling the common structure shared across different tasks. The learned posterior combined with a sample-based Bayesian exploration procedure increases the sample efficiency of learning across a family of related tasks. We first derive an analysis of the relationship between the sample complexity and the initialization quality of the posterior in the finite MDP setting. We next scale the approach to continuous-state domains by introducing a Variational Bayesian Lifelong Reinforcement Learning algorithm that can be combined with recent model-based deep RL methods, and that exhibits backward transfer. Experimental results on several challenging domains show that our algorithms achieve both better forward and backward transfer performance than state-of-the-art lifelong RL methods.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/70e1d6b227fdd605fe61239a953e803df97e521d.pdf",
      "citation_key": "fu20220cl",
      "metadata": {
        "title": "Model-based Lifelong Reinforcement Learning with Bayesian Exploration",
        "authors": [
          "Haotian Fu",
          "Shangqun Yu",
          "Michael S. Littman",
          "G. Konidaris"
        ],
        "published_date": "2022",
        "abstract": "We propose a model-based lifelong reinforcement-learning approach that estimates a hierarchical Bayesian posterior distilling the common structure shared across different tasks. The learned posterior combined with a sample-based Bayesian exploration procedure increases the sample efficiency of learning across a family of related tasks. We first derive an analysis of the relationship between the sample complexity and the initialization quality of the posterior in the finite MDP setting. We next scale the approach to continuous-state domains by introducing a Variational Bayesian Lifelong Reinforcement Learning algorithm that can be combined with recent model-based deep RL methods, and that exhibits backward transfer. Experimental results on several challenging domains show that our algorithms achieve both better forward and backward transfer performance than state-of-the-art lifelong RL methods.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/70e1d6b227fdd605fe61239a953e803df97e521d.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 14,
        "score": 4.666666666666666,
        "summary": "We propose a model-based lifelong reinforcement-learning approach that estimates a hierarchical Bayesian posterior distilling the common structure shared across different tasks. The learned posterior combined with a sample-based Bayesian exploration procedure increases the sample efficiency of learning across a family of related tasks. We first derive an analysis of the relationship between the sample complexity and the initialization quality of the posterior in the finite MDP setting. We next scale the approach to continuous-state domains by introducing a Variational Bayesian Lifelong Reinforcement Learning algorithm that can be combined with recent model-based deep RL methods, and that exhibits backward transfer. Experimental results on several challenging domains show that our algorithms achieve both better forward and backward transfer performance than state-of-the-art lifelong RL methods.",
        "keywords": []
      },
      "file_name": "70e1d6b227fdd605fe61239a953e803df97e521d.pdf"
    },
    {
      "success": true,
      "doc_id": "c07f03a1edd4b651c310abfc3f5f3153",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/a45df3efbd472d4d43ddc4072c61f7f674981ac6.pdf",
      "citation_key": "mndezmolina2022ec5",
      "metadata": {
        "title": "Causal Discovery and Reinforcement Learning: A Synergistic Integration",
        "authors": [
          "Arqumides Mndez-Molina",
          "E. Morales",
          "L. Sucar"
        ],
        "published_date": "2022",
        "abstract": "",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/a45df3efbd472d4d43ddc4072c61f7f674981ac6.pdf",
        "venue": "European Workshop on Probabilistic Graphical Models",
        "citationCount": 14,
        "score": 4.666666666666666,
        "summary": "",
        "keywords": []
      },
      "file_name": "a45df3efbd472d4d43ddc4072c61f7f674981ac6.pdf"
    },
    {
      "success": true,
      "doc_id": "accc562f6a21fdcef4d2b1ec76ebb145",
      "summary": "In lifelong learning, an agent learns throughout its entire life without resets, in a constantly changing environment, as we humans do. Consequently, lifelong learning comes with a plethora of research problems such as continual domain shifts, which result in non-stationary rewards and environment dynamics. These non-stationarities are difficult to detect and cope with due to their continuous nature. Therefore, exploration strategies and learning methods are required that are capable of tracking the steady domain shifts, and adapting to them. We propose Reactive Exploration to track and react to continual domain shifts in lifelong reinforcement learning, and to update the policy correspondingly. To this end, we conduct experiments in order to investigate different exploration strategies. We empirically show that representatives of the policy-gradient family are better suited for lifelong learning, as they adapt more quickly to distribution shifts than Q-learning. Thereby, policy-gradient methods profit the most from Reactive Exploration and show good results in lifelong learning with continual domain shifts. Our code is available at: https://github.com/ml-jku/reactive-exploration.",
      "intriguing_abstract": "In lifelong learning, an agent learns throughout its entire life without resets, in a constantly changing environment, as we humans do. Consequently, lifelong learning comes with a plethora of research problems such as continual domain shifts, which result in non-stationary rewards and environment dynamics. These non-stationarities are difficult to detect and cope with due to their continuous nature. Therefore, exploration strategies and learning methods are required that are capable of tracking the steady domain shifts, and adapting to them. We propose Reactive Exploration to track and react to continual domain shifts in lifelong reinforcement learning, and to update the policy correspondingly. To this end, we conduct experiments in order to investigate different exploration strategies. We empirically show that representatives of the policy-gradient family are better suited for lifelong learning, as they adapt more quickly to distribution shifts than Q-learning. Thereby, policy-gradient methods profit the most from Reactive Exploration and show good results in lifelong learning with continual domain shifts. Our code is available at: https://github.com/ml-jku/reactive-exploration.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/5c0b56d9440ea70c79d1d0032d46c14e06f541f5.pdf",
      "citation_key": "steinparz20220nl",
      "metadata": {
        "title": "Reactive Exploration to Cope with Non-Stationarity in Lifelong Reinforcement Learning",
        "authors": [
          "C. Steinparz",
          "Thomas Schmied",
          "Fabian Paischer",
          "Marius-Constantin Dinu",
          "Vihang Patil",
          "Angela Bitto-Nemling",
          "Hamid Eghbalzadeh",
          "Sepp Hochreiter"
        ],
        "published_date": "2022",
        "abstract": "In lifelong learning, an agent learns throughout its entire life without resets, in a constantly changing environment, as we humans do. Consequently, lifelong learning comes with a plethora of research problems such as continual domain shifts, which result in non-stationary rewards and environment dynamics. These non-stationarities are difficult to detect and cope with due to their continuous nature. Therefore, exploration strategies and learning methods are required that are capable of tracking the steady domain shifts, and adapting to them. We propose Reactive Exploration to track and react to continual domain shifts in lifelong reinforcement learning, and to update the policy correspondingly. To this end, we conduct experiments in order to investigate different exploration strategies. We empirically show that representatives of the policy-gradient family are better suited for lifelong learning, as they adapt more quickly to distribution shifts than Q-learning. Thereby, policy-gradient methods profit the most from Reactive Exploration and show good results in lifelong learning with continual domain shifts. Our code is available at: https://github.com/ml-jku/reactive-exploration.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/5c0b56d9440ea70c79d1d0032d46c14e06f541f5.pdf",
        "venue": "CoLLAs",
        "citationCount": 14,
        "score": 4.666666666666666,
        "summary": "In lifelong learning, an agent learns throughout its entire life without resets, in a constantly changing environment, as we humans do. Consequently, lifelong learning comes with a plethora of research problems such as continual domain shifts, which result in non-stationary rewards and environment dynamics. These non-stationarities are difficult to detect and cope with due to their continuous nature. Therefore, exploration strategies and learning methods are required that are capable of tracking the steady domain shifts, and adapting to them. We propose Reactive Exploration to track and react to continual domain shifts in lifelong reinforcement learning, and to update the policy correspondingly. To this end, we conduct experiments in order to investigate different exploration strategies. We empirically show that representatives of the policy-gradient family are better suited for lifelong learning, as they adapt more quickly to distribution shifts than Q-learning. Thereby, policy-gradient methods profit the most from Reactive Exploration and show good results in lifelong learning with continual domain shifts. Our code is available at: https://github.com/ml-jku/reactive-exploration.",
        "keywords": []
      },
      "file_name": "5c0b56d9440ea70c79d1d0032d46c14e06f541f5.pdf"
    },
    {
      "success": true,
      "doc_id": "f6af0eb9386f87f4c8f512e9add99b69",
      "summary": "The policy gradient method enjoys the simplicity of the objective where the agent optimizes the cumulative reward directly. Moreover, in the continuous action domain, parameterized distribution of action distribution allows easy control of exploration, resulting from the variance of the representing distribution. Entropy can play an essential role in policy optimization by selecting the stochastic policy, which eventually helps better explore the environment in reinforcement learning (RL). However, the stochasticity often reduces as the training progresses; thus, the policy becomes less exploratory. Additionally, certain parametric distributions might only work for some environments and require extensive hyperparameter tuning. This paper aims to mitigate these issues. In particular, we propose an algorithm called Robust Policy Optimization (RPO), which leverages a perturbed distribution. We hypothesize that our method encourages high-entropy actions and provides a way to represent the action space better. We further provide empirical evidence to verify our hypothesis. We evaluated our methods on various continuous control tasks from DeepMind Control, OpenAI Gym, Pybullet, and IsaacGym. We observed that in many settings, RPO increases the policy entropy early in training and then maintains a certain level of entropy throughout the training period. Eventually, our agent RPO shows consistently improved performance compared to PPO and other techniques: entropy regularization, different distributions, and data augmentation. Furthermore, in several settings, our method stays robust in performance, while other baseline mechanisms fail to improve and even worsen the performance.",
      "intriguing_abstract": "The policy gradient method enjoys the simplicity of the objective where the agent optimizes the cumulative reward directly. Moreover, in the continuous action domain, parameterized distribution of action distribution allows easy control of exploration, resulting from the variance of the representing distribution. Entropy can play an essential role in policy optimization by selecting the stochastic policy, which eventually helps better explore the environment in reinforcement learning (RL). However, the stochasticity often reduces as the training progresses; thus, the policy becomes less exploratory. Additionally, certain parametric distributions might only work for some environments and require extensive hyperparameter tuning. This paper aims to mitigate these issues. In particular, we propose an algorithm called Robust Policy Optimization (RPO), which leverages a perturbed distribution. We hypothesize that our method encourages high-entropy actions and provides a way to represent the action space better. We further provide empirical evidence to verify our hypothesis. We evaluated our methods on various continuous control tasks from DeepMind Control, OpenAI Gym, Pybullet, and IsaacGym. We observed that in many settings, RPO increases the policy entropy early in training and then maintains a certain level of entropy throughout the training period. Eventually, our agent RPO shows consistently improved performance compared to PPO and other techniques: entropy regularization, different distributions, and data augmentation. Furthermore, in several settings, our method stays robust in performance, while other baseline mechanisms fail to improve and even worsen the performance.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/0cdbd90989e5f60b6a42dae76b23bd489fcf65cc.pdf",
      "citation_key": "rahman2022p7b",
      "metadata": {
        "title": "Robust Policy Optimization in Deep Reinforcement Learning",
        "authors": [
          "Md Masudur Rahman",
          "Yexiang Xue"
        ],
        "published_date": "2022",
        "abstract": "The policy gradient method enjoys the simplicity of the objective where the agent optimizes the cumulative reward directly. Moreover, in the continuous action domain, parameterized distribution of action distribution allows easy control of exploration, resulting from the variance of the representing distribution. Entropy can play an essential role in policy optimization by selecting the stochastic policy, which eventually helps better explore the environment in reinforcement learning (RL). However, the stochasticity often reduces as the training progresses; thus, the policy becomes less exploratory. Additionally, certain parametric distributions might only work for some environments and require extensive hyperparameter tuning. This paper aims to mitigate these issues. In particular, we propose an algorithm called Robust Policy Optimization (RPO), which leverages a perturbed distribution. We hypothesize that our method encourages high-entropy actions and provides a way to represent the action space better. We further provide empirical evidence to verify our hypothesis. We evaluated our methods on various continuous control tasks from DeepMind Control, OpenAI Gym, Pybullet, and IsaacGym. We observed that in many settings, RPO increases the policy entropy early in training and then maintains a certain level of entropy throughout the training period. Eventually, our agent RPO shows consistently improved performance compared to PPO and other techniques: entropy regularization, different distributions, and data augmentation. Furthermore, in several settings, our method stays robust in performance, while other baseline mechanisms fail to improve and even worsen the performance.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/0cdbd90989e5f60b6a42dae76b23bd489fcf65cc.pdf",
        "venue": "arXiv.org",
        "citationCount": 14,
        "score": 4.666666666666666,
        "summary": "The policy gradient method enjoys the simplicity of the objective where the agent optimizes the cumulative reward directly. Moreover, in the continuous action domain, parameterized distribution of action distribution allows easy control of exploration, resulting from the variance of the representing distribution. Entropy can play an essential role in policy optimization by selecting the stochastic policy, which eventually helps better explore the environment in reinforcement learning (RL). However, the stochasticity often reduces as the training progresses; thus, the policy becomes less exploratory. Additionally, certain parametric distributions might only work for some environments and require extensive hyperparameter tuning. This paper aims to mitigate these issues. In particular, we propose an algorithm called Robust Policy Optimization (RPO), which leverages a perturbed distribution. We hypothesize that our method encourages high-entropy actions and provides a way to represent the action space better. We further provide empirical evidence to verify our hypothesis. We evaluated our methods on various continuous control tasks from DeepMind Control, OpenAI Gym, Pybullet, and IsaacGym. We observed that in many settings, RPO increases the policy entropy early in training and then maintains a certain level of entropy throughout the training period. Eventually, our agent RPO shows consistently improved performance compared to PPO and other techniques: entropy regularization, different distributions, and data augmentation. Furthermore, in several settings, our method stays robust in performance, while other baseline mechanisms fail to improve and even worsen the performance.",
        "keywords": []
      },
      "file_name": "0cdbd90989e5f60b6a42dae76b23bd489fcf65cc.pdf"
    },
    {
      "success": true,
      "doc_id": "b4c6ae9cf04c0665c8a531d41b9ba868",
      "summary": "The deep reinforcement learning-based energy management strategies (EMS) have become a promising solution for hybrid electric vehicles (HEVs). When driving cycles are changed, the neural network will be retrained, which is a time-consuming and laborious task. A more efficient way of choosing EMS is to combine deep reinforcement learning (DRL) with transfer learning, which can transfer knowledge of one domain to the other new domain, making the network of the new domain reach convergence values quickly. Different exploration methods of DRL, including adding action space noise and parameter space noise, are compared against each other in the transfer learning process in this work. Results indicate that the network added parameter space noise is more stable and faster convergent than the others. In conclusion, the best exploration method for transferable EMS is to add noise in the parameter space, while the combination of action space noise and parameter space noise generally performs poorly. Our code is available at https://github.com/BIT-XJY/RL-based-Transferable-EMS.git.",
      "intriguing_abstract": "The deep reinforcement learning-based energy management strategies (EMS) have become a promising solution for hybrid electric vehicles (HEVs). When driving cycles are changed, the neural network will be retrained, which is a time-consuming and laborious task. A more efficient way of choosing EMS is to combine deep reinforcement learning (DRL) with transfer learning, which can transfer knowledge of one domain to the other new domain, making the network of the new domain reach convergence values quickly. Different exploration methods of DRL, including adding action space noise and parameter space noise, are compared against each other in the transfer learning process in this work. Results indicate that the network added parameter space noise is more stable and faster convergent than the others. In conclusion, the best exploration method for transferable EMS is to add noise in the parameter space, while the combination of action space noise and parameter space noise generally performs poorly. Our code is available at https://github.com/BIT-XJY/RL-based-Transferable-EMS.git.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/5f3b337e74618a2364778222162b13bd55a15e27.pdf",
      "citation_key": "xu2022cgd",
      "metadata": {
        "title": "A Comparative Study of Deep Reinforcement Learning-based Transferable Energy Management Strategies for Hybrid Electric Vehicles",
        "authors": [
          "Jingyi Xu",
          "Zirui Li",
          "Li Gao",
          "Junyi Ma",
          "Qi Liu",
          "Yanan Zhao"
        ],
        "published_date": "2022",
        "abstract": "The deep reinforcement learning-based energy management strategies (EMS) have become a promising solution for hybrid electric vehicles (HEVs). When driving cycles are changed, the neural network will be retrained, which is a time-consuming and laborious task. A more efficient way of choosing EMS is to combine deep reinforcement learning (DRL) with transfer learning, which can transfer knowledge of one domain to the other new domain, making the network of the new domain reach convergence values quickly. Different exploration methods of DRL, including adding action space noise and parameter space noise, are compared against each other in the transfer learning process in this work. Results indicate that the network added parameter space noise is more stable and faster convergent than the others. In conclusion, the best exploration method for transferable EMS is to add noise in the parameter space, while the combination of action space noise and parameter space noise generally performs poorly. Our code is available at https://github.com/BIT-XJY/RL-based-Transferable-EMS.git.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/5f3b337e74618a2364778222162b13bd55a15e27.pdf",
        "venue": "2022 IEEE Intelligent Vehicles Symposium (IV)",
        "citationCount": 14,
        "score": 4.666666666666666,
        "summary": "The deep reinforcement learning-based energy management strategies (EMS) have become a promising solution for hybrid electric vehicles (HEVs). When driving cycles are changed, the neural network will be retrained, which is a time-consuming and laborious task. A more efficient way of choosing EMS is to combine deep reinforcement learning (DRL) with transfer learning, which can transfer knowledge of one domain to the other new domain, making the network of the new domain reach convergence values quickly. Different exploration methods of DRL, including adding action space noise and parameter space noise, are compared against each other in the transfer learning process in this work. Results indicate that the network added parameter space noise is more stable and faster convergent than the others. In conclusion, the best exploration method for transferable EMS is to add noise in the parameter space, while the combination of action space noise and parameter space noise generally performs poorly. Our code is available at https://github.com/BIT-XJY/RL-based-Transferable-EMS.git.",
        "keywords": []
      },
      "file_name": "5f3b337e74618a2364778222162b13bd55a15e27.pdf"
    },
    {
      "success": true,
      "doc_id": "49c8d08ccbcaa6b46ebf92e1e14c5de2",
      "summary": "Deep reinforcement learning (DRL) algorithms and evolution strategies (ES) have been applied to various tasks, showing excellent performances. These have the opposite properties, with DRL having good sample efficiency and poor stability, while ES being vice versa. Recently, there have been attempts to combine these algorithms, but these methods fully rely on synchronous update scheme, making it not ideal to maximize the benefits of the parallelism in ES. To solve this challenge, asynchronous update scheme was introduced, which is capable of good time-efficiency and diverse policy exploration. In this paper, we introduce an Asynchronous Evolution Strategy-Reinforcement Learning (AES-RL) that maximizes the parallel efficiency of ES and integrates it with policy gradient methods. Specifically, we propose 1) a novel framework to merge ES and DRL asynchronously and 2) various asynchronous update methods that can take all advantages of asynchronism, ES, and DRL, which are exploration and time efficiency, stability, and sample efficiency, respectively. The proposed framework and update methods are evaluated in continuous control benchmark work, showing superior performance as well as time efficiency compared to the previous methods.",
      "intriguing_abstract": "Deep reinforcement learning (DRL) algorithms and evolution strategies (ES) have been applied to various tasks, showing excellent performances. These have the opposite properties, with DRL having good sample efficiency and poor stability, while ES being vice versa. Recently, there have been attempts to combine these algorithms, but these methods fully rely on synchronous update scheme, making it not ideal to maximize the benefits of the parallelism in ES. To solve this challenge, asynchronous update scheme was introduced, which is capable of good time-efficiency and diverse policy exploration. In this paper, we introduce an Asynchronous Evolution Strategy-Reinforcement Learning (AES-RL) that maximizes the parallel efficiency of ES and integrates it with policy gradient methods. Specifically, we propose 1) a novel framework to merge ES and DRL asynchronously and 2) various asynchronous update methods that can take all advantages of asynchronism, ES, and DRL, which are exploration and time efficiency, stability, and sample efficiency, respectively. The proposed framework and update methods are evaluated in continuous control benchmark work, showing superior performance as well as time efficiency compared to the previous methods.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/b0c40766974df3eae8ff500379e66e5566cd16c9.pdf",
      "citation_key": "lee2020k9k",
      "metadata": {
        "title": "An Efficient Asynchronous Method for Integrating Evolutionary and Gradient-based Policy Search",
        "authors": [
          "Kyunghyun Lee",
          "Byeong-uk Lee",
          "Ukcheol Shin",
          "In-So Kweon"
        ],
        "published_date": "2020",
        "abstract": "Deep reinforcement learning (DRL) algorithms and evolution strategies (ES) have been applied to various tasks, showing excellent performances. These have the opposite properties, with DRL having good sample efficiency and poor stability, while ES being vice versa. Recently, there have been attempts to combine these algorithms, but these methods fully rely on synchronous update scheme, making it not ideal to maximize the benefits of the parallelism in ES. To solve this challenge, asynchronous update scheme was introduced, which is capable of good time-efficiency and diverse policy exploration. In this paper, we introduce an Asynchronous Evolution Strategy-Reinforcement Learning (AES-RL) that maximizes the parallel efficiency of ES and integrates it with policy gradient methods. Specifically, we propose 1) a novel framework to merge ES and DRL asynchronously and 2) various asynchronous update methods that can take all advantages of asynchronism, ES, and DRL, which are exploration and time efficiency, stability, and sample efficiency, respectively. The proposed framework and update methods are evaluated in continuous control benchmark work, showing superior performance as well as time efficiency compared to the previous methods.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/b0c40766974df3eae8ff500379e66e5566cd16c9.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 23,
        "score": 4.6000000000000005,
        "summary": "Deep reinforcement learning (DRL) algorithms and evolution strategies (ES) have been applied to various tasks, showing excellent performances. These have the opposite properties, with DRL having good sample efficiency and poor stability, while ES being vice versa. Recently, there have been attempts to combine these algorithms, but these methods fully rely on synchronous update scheme, making it not ideal to maximize the benefits of the parallelism in ES. To solve this challenge, asynchronous update scheme was introduced, which is capable of good time-efficiency and diverse policy exploration. In this paper, we introduce an Asynchronous Evolution Strategy-Reinforcement Learning (AES-RL) that maximizes the parallel efficiency of ES and integrates it with policy gradient methods. Specifically, we propose 1) a novel framework to merge ES and DRL asynchronously and 2) various asynchronous update methods that can take all advantages of asynchronism, ES, and DRL, which are exploration and time efficiency, stability, and sample efficiency, respectively. The proposed framework and update methods are evaluated in continuous control benchmark work, showing superior performance as well as time efficiency compared to the previous methods.",
        "keywords": []
      },
      "file_name": "b0c40766974df3eae8ff500379e66e5566cd16c9.pdf"
    },
    {
      "success": true,
      "doc_id": "c24e824a87791d1a76dbf03c59ce3eaf",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/4c4d41a977e6d8d82b6cc19ac28e50de3c814fd8.pdf",
      "citation_key": "li2022ec4",
      "metadata": {
        "title": "HyperDQN: A Randomized Exploration Method for Deep Reinforcement Learning",
        "authors": [
          "Ziniu Li",
          "Yingru Li",
          "Yushun Zhang",
          "Tong Zhang",
          "Zhimin Luo"
        ],
        "published_date": "2022",
        "abstract": "",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/4c4d41a977e6d8d82b6cc19ac28e50de3c814fd8.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 13,
        "score": 4.333333333333333,
        "summary": "",
        "keywords": []
      },
      "file_name": "4c4d41a977e6d8d82b6cc19ac28e50de3c814fd8.pdf"
    },
    {
      "success": true,
      "doc_id": "5fbe1843cc486a9c0c342e4bb10f7b4b",
      "summary": "In many real-world applications of reinforcement learning (RL), performing actions requires consuming certain types of resources that are non-replenishable in each episode. Typical applications include robotic control with limited energy and video games with consumable items. In tasks with non-replenishable resources, we observe that popular RL methods such as soft actor critic suffer from poor sample efficiency. The major reason is that, they tend to exhaust resources fast and thus the subsequent exploration is severely restricted due to the absence of resources. To address this challenge, we first formalize the aforementioned problem as a resource-restricted reinforcement learning, and then propose a novel resource-aware exploration bonus (RAEB) to make reasonable usage of resources. An appealing feature of RAEB is that, it can significantly reduce unnecessary resource-consuming trials while effectively encouraging the agent to explore unvisited states. Experiments demonstrate that the proposed RAEB significantly outperforms state-of-the-art exploration strategies in resource-restricted reinforcement learning environments, improving the sample efficiency by up to an order of magnitude.",
      "intriguing_abstract": "In many real-world applications of reinforcement learning (RL), performing actions requires consuming certain types of resources that are non-replenishable in each episode. Typical applications include robotic control with limited energy and video games with consumable items. In tasks with non-replenishable resources, we observe that popular RL methods such as soft actor critic suffer from poor sample efficiency. The major reason is that, they tend to exhaust resources fast and thus the subsequent exploration is severely restricted due to the absence of resources. To address this challenge, we first formalize the aforementioned problem as a resource-restricted reinforcement learning, and then propose a novel resource-aware exploration bonus (RAEB) to make reasonable usage of resources. An appealing feature of RAEB is that, it can significantly reduce unnecessary resource-consuming trials while effectively encouraging the agent to explore unvisited states. Experiments demonstrate that the proposed RAEB significantly outperforms state-of-the-art exploration strategies in resource-restricted reinforcement learning environments, improving the sample efficiency by up to an order of magnitude.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/3d3b0704c61d47c7bafb70ae2670b2786b8e4d81.pdf",
      "citation_key": "wang2022t55",
      "metadata": {
        "title": "Efficient Exploration in Resource-Restricted Reinforcement Learning",
        "authors": [
          "Zhihai Wang",
          "Taoxing Pan",
          "Qi Zhou",
          "Jie Wang"
        ],
        "published_date": "2022",
        "abstract": "In many real-world applications of reinforcement learning (RL), performing actions requires consuming certain types of resources that are non-replenishable in each episode. Typical applications include robotic control with limited energy and video games with consumable items. In tasks with non-replenishable resources, we observe that popular RL methods such as soft actor critic suffer from poor sample efficiency. The major reason is that, they tend to exhaust resources fast and thus the subsequent exploration is severely restricted due to the absence of resources. To address this challenge, we first formalize the aforementioned problem as a resource-restricted reinforcement learning, and then propose a novel resource-aware exploration bonus (RAEB) to make reasonable usage of resources. An appealing feature of RAEB is that, it can significantly reduce unnecessary resource-consuming trials while effectively encouraging the agent to explore unvisited states. Experiments demonstrate that the proposed RAEB significantly outperforms state-of-the-art exploration strategies in resource-restricted reinforcement learning environments, improving the sample efficiency by up to an order of magnitude.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/3d3b0704c61d47c7bafb70ae2670b2786b8e4d81.pdf",
        "venue": "AAAI Conference on Artificial Intelligence",
        "citationCount": 13,
        "score": 4.333333333333333,
        "summary": "In many real-world applications of reinforcement learning (RL), performing actions requires consuming certain types of resources that are non-replenishable in each episode. Typical applications include robotic control with limited energy and video games with consumable items. In tasks with non-replenishable resources, we observe that popular RL methods such as soft actor critic suffer from poor sample efficiency. The major reason is that, they tend to exhaust resources fast and thus the subsequent exploration is severely restricted due to the absence of resources. To address this challenge, we first formalize the aforementioned problem as a resource-restricted reinforcement learning, and then propose a novel resource-aware exploration bonus (RAEB) to make reasonable usage of resources. An appealing feature of RAEB is that, it can significantly reduce unnecessary resource-consuming trials while effectively encouraging the agent to explore unvisited states. Experiments demonstrate that the proposed RAEB significantly outperforms state-of-the-art exploration strategies in resource-restricted reinforcement learning environments, improving the sample efficiency by up to an order of magnitude.",
        "keywords": []
      },
      "file_name": "3d3b0704c61d47c7bafb70ae2670b2786b8e4d81.pdf"
    },
    {
      "success": true,
      "doc_id": "14b1cf7617f5f949aa8950ca25ecbbb7",
      "summary": "Despite the close connection between exploration and sample efficiency, most state of the art reinforcement learning algorithms include no considerations for exploration beyond maximizing the entropy of the policy. In this work we address this seeming missed opportunity. We observe that the most common formulation of directed exploration in deep RL, known as bonus-based exploration (BBE), suffers from bias and slow coverage in the few-sample regime. This causes BBE to be actively detrimental to policy learning in many control tasks. We show that by decoupling the task policy from the exploration policy, directed exploration can be highly effective for sample-efficient continuous control. Our method, Decoupled Exploration and Exploitation Policies (DEEP), can be combined with any off-policy RL algorithm without modification. When used in conjunction with soft actor-critic, DEEP incurs no performance penalty in densely-rewarding environments. On sparse environments, DEEP gives a several-fold improvement in data efficiency due to better exploration.",
      "intriguing_abstract": "Despite the close connection between exploration and sample efficiency, most state of the art reinforcement learning algorithms include no considerations for exploration beyond maximizing the entropy of the policy. In this work we address this seeming missed opportunity. We observe that the most common formulation of directed exploration in deep RL, known as bonus-based exploration (BBE), suffers from bias and slow coverage in the few-sample regime. This causes BBE to be actively detrimental to policy learning in many control tasks. We show that by decoupling the task policy from the exploration policy, directed exploration can be highly effective for sample-efficient continuous control. Our method, Decoupled Exploration and Exploitation Policies (DEEP), can be combined with any off-policy RL algorithm without modification. When used in conjunction with soft actor-critic, DEEP incurs no performance penalty in densely-rewarding environments. On sparse environments, DEEP gives a several-fold improvement in data efficiency due to better exploration.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/9e5fe2ba652774ba3b1127f626c192668a907132.pdf",
      "citation_key": "whitney2021xlu",
      "metadata": {
        "title": "Decoupled Exploration and Exploitation Policies for Sample-Efficient Reinforcement Learning",
        "authors": [
          "William F. Whitney",
          "Michael Bloesch",
          "Jost Tobias Springenberg",
          "A. Abdolmaleki",
          "Kyunghyun Cho",
          "Martin A. Riedmiller"
        ],
        "published_date": "2021",
        "abstract": "Despite the close connection between exploration and sample efficiency, most state of the art reinforcement learning algorithms include no considerations for exploration beyond maximizing the entropy of the policy. In this work we address this seeming missed opportunity. We observe that the most common formulation of directed exploration in deep RL, known as bonus-based exploration (BBE), suffers from bias and slow coverage in the few-sample regime. This causes BBE to be actively detrimental to policy learning in many control tasks. We show that by decoupling the task policy from the exploration policy, directed exploration can be highly effective for sample-efficient continuous control. Our method, Decoupled Exploration and Exploitation Policies (DEEP), can be combined with any off-policy RL algorithm without modification. When used in conjunction with soft actor-critic, DEEP incurs no performance penalty in densely-rewarding environments. On sparse environments, DEEP gives a several-fold improvement in data efficiency due to better exploration.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/9e5fe2ba652774ba3b1127f626c192668a907132.pdf",
        "venue": "",
        "citationCount": 17,
        "score": 4.25,
        "summary": "Despite the close connection between exploration and sample efficiency, most state of the art reinforcement learning algorithms include no considerations for exploration beyond maximizing the entropy of the policy. In this work we address this seeming missed opportunity. We observe that the most common formulation of directed exploration in deep RL, known as bonus-based exploration (BBE), suffers from bias and slow coverage in the few-sample regime. This causes BBE to be actively detrimental to policy learning in many control tasks. We show that by decoupling the task policy from the exploration policy, directed exploration can be highly effective for sample-efficient continuous control. Our method, Decoupled Exploration and Exploitation Policies (DEEP), can be combined with any off-policy RL algorithm without modification. When used in conjunction with soft actor-critic, DEEP incurs no performance penalty in densely-rewarding environments. On sparse environments, DEEP gives a several-fold improvement in data efficiency due to better exploration.",
        "keywords": []
      },
      "file_name": "9e5fe2ba652774ba3b1127f626c192668a907132.pdf"
    },
    {
      "success": true,
      "doc_id": "69d7ccaf524e41296771744d9b5a60d5",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/678cd30fac3fcf215d0e714936f6907ecc6d4361.pdf",
      "citation_key": "suri20226rr",
      "metadata": {
        "title": "Off-Policy Evolutionary Reinforcement Learning with Maximum Mutations",
        "authors": [
          "Karush Suri"
        ],
        "published_date": "2022",
        "abstract": "",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/678cd30fac3fcf215d0e714936f6907ecc6d4361.pdf",
        "venue": "Adaptive Agents and Multi-Agent Systems",
        "citationCount": 12,
        "score": 4.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "678cd30fac3fcf215d0e714936f6907ecc6d4361.pdf"
    },
    {
      "success": true,
      "doc_id": "c2b699470ad8dd4ec501ae31287105fd",
      "summary": "The training process analysis and termination condition of the training process of a Reinforcement Learning (RL) system have always been the key issues to train an RL agent. In this paper, a new approach based on State Entropy and Exploration Entropy is proposed to analyse the training process. The concept of State Entropy is used to denote the uncertainty for an RL agent to select the action at every state that the agent will traverse, while the Exploration Entropy denotes the action selection uncertainty of the whole system. Actually, the action selection uncertainty of a certain state or the whole system reflects the degree of exploration and the stage of the learning process for an agent. The Exploration Entropy is a new criterion to analyse and manage the training process of RL. The theoretical analysis and experiment results illustrate that the curve of Exploration Entropy contains more information than the existing analytical methods.",
      "intriguing_abstract": "The training process analysis and termination condition of the training process of a Reinforcement Learning (RL) system have always been the key issues to train an RL agent. In this paper, a new approach based on State Entropy and Exploration Entropy is proposed to analyse the training process. The concept of State Entropy is used to denote the uncertainty for an RL agent to select the action at every state that the agent will traverse, while the Exploration Entropy denotes the action selection uncertainty of the whole system. Actually, the action selection uncertainty of a certain state or the whole system reflects the degree of exploration and the stage of the learning process for an agent. The Exploration Entropy is a new criterion to analyse and manage the training process of RL. The theoretical analysis and experiment results illustrate that the curve of Exploration Entropy contains more information than the existing analytical methods.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/103f1674121780097f896ffe525bab2c6ae0bcdc.pdf",
      "citation_key": "xin2020y4j",
      "metadata": {
        "title": "Exploration Entropy for Reinforcement Learning",
        "authors": [
          "Bo Xin",
          "Haixu Yu",
          "You Qin",
          "Qing Tang",
          "Zhangqing Zhu"
        ],
        "published_date": "2020",
        "abstract": "The training process analysis and termination condition of the training process of a Reinforcement Learning (RL) system have always been the key issues to train an RL agent. In this paper, a new approach based on State Entropy and Exploration Entropy is proposed to analyse the training process. The concept of State Entropy is used to denote the uncertainty for an RL agent to select the action at every state that the agent will traverse, while the Exploration Entropy denotes the action selection uncertainty of the whole system. Actually, the action selection uncertainty of a certain state or the whole system reflects the degree of exploration and the stage of the learning process for an agent. The Exploration Entropy is a new criterion to analyse and manage the training process of RL. The theoretical analysis and experiment results illustrate that the curve of Exploration Entropy contains more information than the existing analytical methods.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/103f1674121780097f896ffe525bab2c6ae0bcdc.pdf",
        "venue": "",
        "citationCount": 19,
        "score": 3.8000000000000003,
        "summary": "The training process analysis and termination condition of the training process of a Reinforcement Learning (RL) system have always been the key issues to train an RL agent. In this paper, a new approach based on State Entropy and Exploration Entropy is proposed to analyse the training process. The concept of State Entropy is used to denote the uncertainty for an RL agent to select the action at every state that the agent will traverse, while the Exploration Entropy denotes the action selection uncertainty of the whole system. Actually, the action selection uncertainty of a certain state or the whole system reflects the degree of exploration and the stage of the learning process for an agent. The Exploration Entropy is a new criterion to analyse and manage the training process of RL. The theoretical analysis and experiment results illustrate that the curve of Exploration Entropy contains more information than the existing analytical methods.",
        "keywords": []
      },
      "file_name": "103f1674121780097f896ffe525bab2c6ae0bcdc.pdf"
    },
    {
      "success": true,
      "doc_id": "c2b8fc0418cdbcd455588ddff060a59b",
      "summary": "The exploration-exploitation trade-off is at the heart of reinforcement learning (RL). However, most continuous control benchmarks used in recent RL research only require local exploration. This led to the development of algorithms that have basic exploration capabilities, and behave poorly in benchmarks that require more versatile exploration. For instance, as demonstrated in our empirical study, state-of-the-art RL algorithms such as DDPG and TD3 are unable to steer a point mass in even small 2D mazes. In this paper, we propose a new algorithm called \"Plan, Backplay, Chain Skills\" (PBCS) that combines motion planning and reinforcement learning to solve hard exploration environments. In a first phase, a motion planning algorithm is used to find a single good trajectory, then an RL algorithm is trained using a curriculum derived from the trajectory, by combining a variant of the Backplay algorithm and skill chaining. We show that this method outperforms state-of-the-art RL algorithms in 2D maze environments of various sizes, and is able to improve on the trajectory obtained by the motion planning phase.",
      "intriguing_abstract": "The exploration-exploitation trade-off is at the heart of reinforcement learning (RL). However, most continuous control benchmarks used in recent RL research only require local exploration. This led to the development of algorithms that have basic exploration capabilities, and behave poorly in benchmarks that require more versatile exploration. For instance, as demonstrated in our empirical study, state-of-the-art RL algorithms such as DDPG and TD3 are unable to steer a point mass in even small 2D mazes. In this paper, we propose a new algorithm called \"Plan, Backplay, Chain Skills\" (PBCS) that combines motion planning and reinforcement learning to solve hard exploration environments. In a first phase, a motion planning algorithm is used to find a single good trajectory, then an RL algorithm is trained using a curriculum derived from the trajectory, by combining a variant of the Backplay algorithm and skill chaining. We show that this method outperforms state-of-the-art RL algorithms in 2D maze environments of various sizes, and is able to improve on the trajectory obtained by the motion planning phase.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/de93c8aed64229571b03e40b36499d4f07ce875d.pdf",
      "citation_key": "matheron2020zmh",
      "metadata": {
        "title": "PBCS : Efficient Exploration and Exploitation Using a Synergy between Reinforcement Learning and Motion Planning",
        "authors": [
          "Guillaume Matheron",
          "Nicolas Perrin",
          "Olivier Sigaud"
        ],
        "published_date": "2020",
        "abstract": "The exploration-exploitation trade-off is at the heart of reinforcement learning (RL). However, most continuous control benchmarks used in recent RL research only require local exploration. This led to the development of algorithms that have basic exploration capabilities, and behave poorly in benchmarks that require more versatile exploration. For instance, as demonstrated in our empirical study, state-of-the-art RL algorithms such as DDPG and TD3 are unable to steer a point mass in even small 2D mazes. In this paper, we propose a new algorithm called \"Plan, Backplay, Chain Skills\" (PBCS) that combines motion planning and reinforcement learning to solve hard exploration environments. In a first phase, a motion planning algorithm is used to find a single good trajectory, then an RL algorithm is trained using a curriculum derived from the trajectory, by combining a variant of the Backplay algorithm and skill chaining. We show that this method outperforms state-of-the-art RL algorithms in 2D maze environments of various sizes, and is able to improve on the trajectory obtained by the motion planning phase.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/de93c8aed64229571b03e40b36499d4f07ce875d.pdf",
        "venue": "International Conference on Artificial Neural Networks",
        "citationCount": 19,
        "score": 3.8000000000000003,
        "summary": "The exploration-exploitation trade-off is at the heart of reinforcement learning (RL). However, most continuous control benchmarks used in recent RL research only require local exploration. This led to the development of algorithms that have basic exploration capabilities, and behave poorly in benchmarks that require more versatile exploration. For instance, as demonstrated in our empirical study, state-of-the-art RL algorithms such as DDPG and TD3 are unable to steer a point mass in even small 2D mazes. In this paper, we propose a new algorithm called \"Plan, Backplay, Chain Skills\" (PBCS) that combines motion planning and reinforcement learning to solve hard exploration environments. In a first phase, a motion planning algorithm is used to find a single good trajectory, then an RL algorithm is trained using a curriculum derived from the trajectory, by combining a variant of the Backplay algorithm and skill chaining. We show that this method outperforms state-of-the-art RL algorithms in 2D maze environments of various sizes, and is able to improve on the trajectory obtained by the motion planning phase.",
        "keywords": []
      },
      "file_name": "de93c8aed64229571b03e40b36499d4f07ce875d.pdf"
    },
    {
      "success": true,
      "doc_id": "0f7490c2facb94a2b3e02915169b102d",
      "summary": "Offline reinforcement learning (RL) enables the agent to effectively learn from logged data, which significantly extends the applicability of RL algorithms in real-world scenarios where exploration can be expensive or unsafe. Previous works have shown that extracting primitive skills from the recurring and temporally extended structures in the logged data yields better learning. However, these methods suffer greatly when the primitives have limited representation ability to recover the original policy space, especially in offline settings. In this paper, we give a quantitative characterization of the performance of offline hierarchical learning and highlight the importance of learning lossless primitives. To this end, we propose to use a flow-based structure as the representation for low-level policies. This allows us to represent the behaviors in the dataset faithfully while keeping the expression ability to recover the whole policy space. We show that such lossless primitives can drastically improve the performance of hierarchical policies. The experimental results and extensive ablation studies on the standard D4RL benchmark show that our method has a good representation ability for policies and achieves superior performance in most tasks.",
      "intriguing_abstract": "Offline reinforcement learning (RL) enables the agent to effectively learn from logged data, which significantly extends the applicability of RL algorithms in real-world scenarios where exploration can be expensive or unsafe. Previous works have shown that extracting primitive skills from the recurring and temporally extended structures in the logged data yields better learning. However, these methods suffer greatly when the primitives have limited representation ability to recover the original policy space, especially in offline settings. In this paper, we give a quantitative characterization of the performance of offline hierarchical learning and highlight the importance of learning lossless primitives. To this end, we propose to use a flow-based structure as the representation for low-level policies. This allows us to represent the behaviors in the dataset faithfully while keeping the expression ability to recover the whole policy space. We show that such lossless primitives can drastically improve the performance of hierarchical policies. The experimental results and extensive ablation studies on the standard D4RL benchmark show that our method has a good representation ability for policies and achieves superior performance in most tasks.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/9d1445f1845a2880ff9c752845660e9c294aa7b5.pdf",
      "citation_key": "yang2022j0z",
      "metadata": {
        "title": "Flow to Control: Offline Reinforcement Learning with Lossless Primitive Discovery",
        "authors": [
          "Yiqin Yang",
          "Haotian Hu",
          "Wenzhe Li",
          "Siyuan Li",
          "Jun Yang",
          "Qianchuan Zhao",
          "Chongjie Zhang"
        ],
        "published_date": "2022",
        "abstract": "Offline reinforcement learning (RL) enables the agent to effectively learn from logged data, which significantly extends the applicability of RL algorithms in real-world scenarios where exploration can be expensive or unsafe. Previous works have shown that extracting primitive skills from the recurring and temporally extended structures in the logged data yields better learning. However, these methods suffer greatly when the primitives have limited representation ability to recover the original policy space, especially in offline settings. In this paper, we give a quantitative characterization of the performance of offline hierarchical learning and highlight the importance of learning lossless primitives. To this end, we propose to use a flow-based structure as the representation for low-level policies. This allows us to represent the behaviors in the dataset faithfully while keeping the expression ability to recover the whole policy space. We show that such lossless primitives can drastically improve the performance of hierarchical policies. The experimental results and extensive ablation studies on the standard D4RL benchmark show that our method has a good representation ability for policies and achieves superior performance in most tasks.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/9d1445f1845a2880ff9c752845660e9c294aa7b5.pdf",
        "venue": "AAAI Conference on Artificial Intelligence",
        "citationCount": 11,
        "score": 3.6666666666666665,
        "summary": "Offline reinforcement learning (RL) enables the agent to effectively learn from logged data, which significantly extends the applicability of RL algorithms in real-world scenarios where exploration can be expensive or unsafe. Previous works have shown that extracting primitive skills from the recurring and temporally extended structures in the logged data yields better learning. However, these methods suffer greatly when the primitives have limited representation ability to recover the original policy space, especially in offline settings. In this paper, we give a quantitative characterization of the performance of offline hierarchical learning and highlight the importance of learning lossless primitives. To this end, we propose to use a flow-based structure as the representation for low-level policies. This allows us to represent the behaviors in the dataset faithfully while keeping the expression ability to recover the whole policy space. We show that such lossless primitives can drastically improve the performance of hierarchical policies. The experimental results and extensive ablation studies on the standard D4RL benchmark show that our method has a good representation ability for policies and achieves superior performance in most tasks.",
        "keywords": []
      },
      "file_name": "9d1445f1845a2880ff9c752845660e9c294aa7b5.pdf"
    },
    {
      "success": true,
      "doc_id": "9d4b17b945a2f698ad0d19efdc019b1a",
      "summary": "Humans are capable of abstracting various tasks as different combinations of multiple attributes. This perspective of compositionality is vital for human rapid learning and adaption since previous experiences from related tasks can be combined to generalize across novel compositional settings. In this work, we aim to achieve zero-shot policy generalization of Reinforcement Learning (RL) agents by leveraging the task compositionality. Our proposed method is a meta-RL algorithm with disentangled task representation, explicitly encoding different aspects of the tasks. Policy generalization is then performed by inferring unseen compositional task representations via the obtained disentanglement without extra exploration. The evaluation is conducted on three simulated tasks and a challenging real-world robotic insertion task. Experimental results demonstrate that our proposed method achieves policy generalization to unseen compositional tasks in a zero-shot manner.",
      "intriguing_abstract": "Humans are capable of abstracting various tasks as different combinations of multiple attributes. This perspective of compositionality is vital for human rapid learning and adaption since previous experiences from related tasks can be combined to generalize across novel compositional settings. In this work, we aim to achieve zero-shot policy generalization of Reinforcement Learning (RL) agents by leveraging the task compositionality. Our proposed method is a meta-RL algorithm with disentangled task representation, explicitly encoding different aspects of the tasks. Policy generalization is then performed by inferring unseen compositional task representations via the obtained disentanglement without extra exploration. The evaluation is conducted on three simulated tasks and a challenging real-world robotic insertion task. Experimental results demonstrate that our proposed method achieves policy generalization to unseen compositional tasks in a zero-shot manner.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/83f1343500c9f0df62da0d61736738d8d7a9bba0.pdf",
      "citation_key": "wu2022sot",
      "metadata": {
        "title": "Zero-Shot Policy Transfer with Disentangled Task Representation of Meta-Reinforcement Learning",
        "authors": [
          "Zheng Wu",
          "Yichen Xie",
          "Wenzhao Lian",
          "Changhao Wang",
          "Yanjiang Guo",
          "Jianyu Chen",
          "S. Schaal",
          "M. Tomizuka"
        ],
        "published_date": "2022",
        "abstract": "Humans are capable of abstracting various tasks as different combinations of multiple attributes. This perspective of compositionality is vital for human rapid learning and adaption since previous experiences from related tasks can be combined to generalize across novel compositional settings. In this work, we aim to achieve zero-shot policy generalization of Reinforcement Learning (RL) agents by leveraging the task compositionality. Our proposed method is a meta-RL algorithm with disentangled task representation, explicitly encoding different aspects of the tasks. Policy generalization is then performed by inferring unseen compositional task representations via the obtained disentanglement without extra exploration. The evaluation is conducted on three simulated tasks and a challenging real-world robotic insertion task. Experimental results demonstrate that our proposed method achieves policy generalization to unseen compositional tasks in a zero-shot manner.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/83f1343500c9f0df62da0d61736738d8d7a9bba0.pdf",
        "venue": "IEEE International Conference on Robotics and Automation",
        "citationCount": 11,
        "score": 3.6666666666666665,
        "summary": "Humans are capable of abstracting various tasks as different combinations of multiple attributes. This perspective of compositionality is vital for human rapid learning and adaption since previous experiences from related tasks can be combined to generalize across novel compositional settings. In this work, we aim to achieve zero-shot policy generalization of Reinforcement Learning (RL) agents by leveraging the task compositionality. Our proposed method is a meta-RL algorithm with disentangled task representation, explicitly encoding different aspects of the tasks. Policy generalization is then performed by inferring unseen compositional task representations via the obtained disentanglement without extra exploration. The evaluation is conducted on three simulated tasks and a challenging real-world robotic insertion task. Experimental results demonstrate that our proposed method achieves policy generalization to unseen compositional tasks in a zero-shot manner.",
        "keywords": []
      },
      "file_name": "83f1343500c9f0df62da0d61736738d8d7a9bba0.pdf"
    },
    {
      "success": true,
      "doc_id": "6b17adc371b9922bc14058d80ec904b5",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/2e1ca97d8f7604e3b334ff4903bfa67267379317.pdf",
      "citation_key": "kessler202295l",
      "metadata": {
        "title": "The Surprising Effectiveness of Latent World Models for Continual Reinforcement Learning",
        "authors": [
          "Samuel Kessler",
          "Piotr Milo's",
          "Jack Parker-Holder",
          "Stephen J. Roberts"
        ],
        "published_date": "2022",
        "abstract": "",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/2e1ca97d8f7604e3b334ff4903bfa67267379317.pdf",
        "venue": "arXiv.org",
        "citationCount": 11,
        "score": 3.6666666666666665,
        "summary": "",
        "keywords": []
      },
      "file_name": "2e1ca97d8f7604e3b334ff4903bfa67267379317.pdf"
    },
    {
      "success": true,
      "doc_id": "5cb62f1d461201a08b2e6a93a02af39e",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/7551b7f7420087de59ad36e445cb3bdef9c382ee.pdf",
      "citation_key": "raffin2020ka2",
      "metadata": {
        "title": "Generalized State-Dependent Exploration for Deep Reinforcement Learning in Robotics",
        "authors": [
          "A. Raffin",
          "F. Stulp"
        ],
        "published_date": "2020",
        "abstract": "",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/7551b7f7420087de59ad36e445cb3bdef9c382ee.pdf",
        "venue": "arXiv.org",
        "citationCount": 18,
        "score": 3.6,
        "summary": "",
        "keywords": []
      },
      "file_name": "7551b7f7420087de59ad36e445cb3bdef9c382ee.pdf"
    },
    {
      "success": true,
      "doc_id": "d62137ae1c1ed8557b50a03ff9d5d142",
      "summary": "This paper introduces new perspectives on analog design space search. To minimize the time-to-market, this endeavor better cast as constraint satisfaction problem than global optimization defined in prior arts. We incorporate model based agents, contrasted with model-free learning, to implement a trust-region strategy. As such, simple feed-forward networks can be trained with supervised learning, where the convergence is relatively trivial. Experiment results demonstrate orders of magnitude improvement on search iterations. Additionally, the unprecedented consideration of PVT conditions are accommodated. On circuits with TSMC 5/6nm process, our method achieve performance surpassing human designers. Furthermore, this framework is in production in industrial settings.",
      "intriguing_abstract": "This paper introduces new perspectives on analog design space search. To minimize the time-to-market, this endeavor better cast as constraint satisfaction problem than global optimization defined in prior arts. We incorporate model based agents, contrasted with model-free learning, to implement a trust-region strategy. As such, simple feed-forward networks can be trained with supervised learning, where the convergence is relatively trivial. Experiment results demonstrate orders of magnitude improvement on search iterations. Additionally, the unprecedented consideration of PVT conditions are accommodated. On circuits with TSMC 5/6nm process, our method achieve performance surpassing human designers. Furthermore, this framework is in production in industrial settings.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/b6ffc15ab55281dcf08b6a19d6f8bfcb190765b8.pdf",
      "citation_key": "yang20206wi",
      "metadata": {
        "title": "Trust-Region Method with Deep Reinforcement Learning in Analog Design Space Exploration",
        "authors": [
          "Kai-En Yang",
          "Chia-Yu Tsai",
          "Hung-Hao Shen",
          "Chen-Feng Chiang",
          "Feng-Ming Tsai",
          "Chunguang Wang",
          "Yiju Ting",
          "Chia-Shun Yeh",
          "C. Lai"
        ],
        "published_date": "2020",
        "abstract": "This paper introduces new perspectives on analog design space search. To minimize the time-to-market, this endeavor better cast as constraint satisfaction problem than global optimization defined in prior arts. We incorporate model based agents, contrasted with model-free learning, to implement a trust-region strategy. As such, simple feed-forward networks can be trained with supervised learning, where the convergence is relatively trivial. Experiment results demonstrate orders of magnitude improvement on search iterations. Additionally, the unprecedented consideration of PVT conditions are accommodated. On circuits with TSMC 5/6nm process, our method achieve performance surpassing human designers. Furthermore, this framework is in production in industrial settings.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/b6ffc15ab55281dcf08b6a19d6f8bfcb190765b8.pdf",
        "venue": "Design Automation Conference",
        "citationCount": 17,
        "score": 3.4000000000000004,
        "summary": "This paper introduces new perspectives on analog design space search. To minimize the time-to-market, this endeavor better cast as constraint satisfaction problem than global optimization defined in prior arts. We incorporate model based agents, contrasted with model-free learning, to implement a trust-region strategy. As such, simple feed-forward networks can be trained with supervised learning, where the convergence is relatively trivial. Experiment results demonstrate orders of magnitude improvement on search iterations. Additionally, the unprecedented consideration of PVT conditions are accommodated. On circuits with TSMC 5/6nm process, our method achieve performance surpassing human designers. Furthermore, this framework is in production in industrial settings.",
        "keywords": []
      },
      "file_name": "b6ffc15ab55281dcf08b6a19d6f8bfcb190765b8.pdf"
    },
    {
      "success": true,
      "doc_id": "be9def1f7d5e1e48c3e0c3bb7b2ac9fc",
      "summary": "Deep Reinforcement Learning (DRL) methods are inefficient in the initial strategy exploration process due to the huge state space and action space in large-scale complex scenarios. This is becoming one of the bottlenecks in their application to large-scale game adversarial scenarios. This paper proposes a Safe reinforcement learning combined with Imitation learning for Task Assignment (SITA) method for a representative red-blue game confrontation scenario. Aiming at the problem of difficult sampling of Imitation Learning (IL), this paper combines human knowledge with adversarial rules to build a knowledge rule base; We propose the Imitation Learning with the Decoupled Network (ILDN) pre-training method to solve the problem of excessive initial invalid exploration; In order to reduce invalid exploration and improve the stability in the later stages of training, we incorporate Safe Reinforcement Learning (Safe RL) method after pre-training. Finally, we verified in the digital battlefield that the SITA method has higher training efficiency and strong generalization ability in large-scale complex scenarios.",
      "intriguing_abstract": "Deep Reinforcement Learning (DRL) methods are inefficient in the initial strategy exploration process due to the huge state space and action space in large-scale complex scenarios. This is becoming one of the bottlenecks in their application to large-scale game adversarial scenarios. This paper proposes a Safe reinforcement learning combined with Imitation learning for Task Assignment (SITA) method for a representative red-blue game confrontation scenario. Aiming at the problem of difficult sampling of Imitation Learning (IL), this paper combines human knowledge with adversarial rules to build a knowledge rule base; We propose the Imitation Learning with the Decoupled Network (ILDN) pre-training method to solve the problem of excessive initial invalid exploration; In order to reduce invalid exploration and improve the stability in the later stages of training, we incorporate Safe Reinforcement Learning (Safe RL) method after pre-training. Finally, we verified in the digital battlefield that the SITA method has higher training efficiency and strong generalization ability in large-scale complex scenarios.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/06318722ebbac1d9b59e2408ecd3994eadb0ec6b.pdf",
      "citation_key": "liu20228r4",
      "metadata": {
        "title": "Deep Reinforcement Learning Task Assignment Based on Domain Knowledge",
        "authors": [
          "Jiayi Liu",
          "Gang Wang",
          "Xiangke Guo",
          "Siyuan Wang",
          "Qiang Fu"
        ],
        "published_date": "2022",
        "abstract": "Deep Reinforcement Learning (DRL) methods are inefficient in the initial strategy exploration process due to the huge state space and action space in large-scale complex scenarios. This is becoming one of the bottlenecks in their application to large-scale game adversarial scenarios. This paper proposes a Safe reinforcement learning combined with Imitation learning for Task Assignment (SITA) method for a representative red-blue game confrontation scenario. Aiming at the problem of difficult sampling of Imitation Learning (IL), this paper combines human knowledge with adversarial rules to build a knowledge rule base; We propose the Imitation Learning with the Decoupled Network (ILDN) pre-training method to solve the problem of excessive initial invalid exploration; In order to reduce invalid exploration and improve the stability in the later stages of training, we incorporate Safe Reinforcement Learning (Safe RL) method after pre-training. Finally, we verified in the digital battlefield that the SITA method has higher training efficiency and strong generalization ability in large-scale complex scenarios.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/06318722ebbac1d9b59e2408ecd3994eadb0ec6b.pdf",
        "venue": "IEEE Access",
        "citationCount": 10,
        "score": 3.333333333333333,
        "summary": "Deep Reinforcement Learning (DRL) methods are inefficient in the initial strategy exploration process due to the huge state space and action space in large-scale complex scenarios. This is becoming one of the bottlenecks in their application to large-scale game adversarial scenarios. This paper proposes a Safe reinforcement learning combined with Imitation learning for Task Assignment (SITA) method for a representative red-blue game confrontation scenario. Aiming at the problem of difficult sampling of Imitation Learning (IL), this paper combines human knowledge with adversarial rules to build a knowledge rule base; We propose the Imitation Learning with the Decoupled Network (ILDN) pre-training method to solve the problem of excessive initial invalid exploration; In order to reduce invalid exploration and improve the stability in the later stages of training, we incorporate Safe Reinforcement Learning (Safe RL) method after pre-training. Finally, we verified in the digital battlefield that the SITA method has higher training efficiency and strong generalization ability in large-scale complex scenarios.",
        "keywords": []
      },
      "file_name": "06318722ebbac1d9b59e2408ecd3994eadb0ec6b.pdf"
    },
    {
      "success": true,
      "doc_id": "ae825c1d02eb8ffd434323c77b3a488b",
      "summary": "This paper investigates the solution to a mobile-robot exploration problem following autonomous driving principles. The exploration task is formulated in this study as a process of building a map while a robot moves in an indoor environment beginning from full uncertainties. The sequence of robot decisions of how to move defines the strategy of the exploration that this paper aims to investigate, applying one of the Deep Reinforcement Learning methods, known as the Deep Deterministic Policy Gradient (DDPG) algorithm. A custom environment is created representing the mapping process with a map visualization, a robot model, and a reward function. The actor-critic network receives and sends input and output data, respectively, to the custom environment. The input is the data from the laser sensor, which is equipped on the robot. The output is the continuous actions of the robot in terms of linear and angular velocities. The training results of this study show the strengths and weaknesses of the DDPG algorithm for the robotic mapping problem. The implementation was developed in MATLAB platform using its corresponding toolboxes. A comparison with another exploration algorithm is also provided.",
      "intriguing_abstract": "This paper investigates the solution to a mobile-robot exploration problem following autonomous driving principles. The exploration task is formulated in this study as a process of building a map while a robot moves in an indoor environment beginning from full uncertainties. The sequence of robot decisions of how to move defines the strategy of the exploration that this paper aims to investigate, applying one of the Deep Reinforcement Learning methods, known as the Deep Deterministic Policy Gradient (DDPG) algorithm. A custom environment is created representing the mapping process with a map visualization, a robot model, and a reward function. The actor-critic network receives and sends input and output data, respectively, to the custom environment. The input is the data from the laser sensor, which is equipped on the robot. The output is the continuous actions of the robot in terms of linear and angular velocities. The training results of this study show the strengths and weaknesses of the DDPG algorithm for the robotic mapping problem. The implementation was developed in MATLAB platform using its corresponding toolboxes. A comparison with another exploration algorithm is also provided.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/1f577f6f0785c7967bef4ac6d0ab0370c2815b4d.pdf",
      "citation_key": "kamalova2022jpm",
      "metadata": {
        "title": "Occupancy Reward-Driven Exploration with Deep Reinforcement Learning for Mobile Robot System",
        "authors": [
          "A. Kamalova",
          "Suk-Gyu Lee",
          "Soon-H. Kwon"
        ],
        "published_date": "2022",
        "abstract": "This paper investigates the solution to a mobile-robot exploration problem following autonomous driving principles. The exploration task is formulated in this study as a process of building a map while a robot moves in an indoor environment beginning from full uncertainties. The sequence of robot decisions of how to move defines the strategy of the exploration that this paper aims to investigate, applying one of the Deep Reinforcement Learning methods, known as the Deep Deterministic Policy Gradient (DDPG) algorithm. A custom environment is created representing the mapping process with a map visualization, a robot model, and a reward function. The actor-critic network receives and sends input and output data, respectively, to the custom environment. The input is the data from the laser sensor, which is equipped on the robot. The output is the continuous actions of the robot in terms of linear and angular velocities. The training results of this study show the strengths and weaknesses of the DDPG algorithm for the robotic mapping problem. The implementation was developed in MATLAB platform using its corresponding toolboxes. A comparison with another exploration algorithm is also provided.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/1f577f6f0785c7967bef4ac6d0ab0370c2815b4d.pdf",
        "venue": "Applied Sciences",
        "citationCount": 10,
        "score": 3.333333333333333,
        "summary": "This paper investigates the solution to a mobile-robot exploration problem following autonomous driving principles. The exploration task is formulated in this study as a process of building a map while a robot moves in an indoor environment beginning from full uncertainties. The sequence of robot decisions of how to move defines the strategy of the exploration that this paper aims to investigate, applying one of the Deep Reinforcement Learning methods, known as the Deep Deterministic Policy Gradient (DDPG) algorithm. A custom environment is created representing the mapping process with a map visualization, a robot model, and a reward function. The actor-critic network receives and sends input and output data, respectively, to the custom environment. The input is the data from the laser sensor, which is equipped on the robot. The output is the continuous actions of the robot in terms of linear and angular velocities. The training results of this study show the strengths and weaknesses of the DDPG algorithm for the robotic mapping problem. The implementation was developed in MATLAB platform using its corresponding toolboxes. A comparison with another exploration algorithm is also provided.",
        "keywords": []
      },
      "file_name": "1f577f6f0785c7967bef4ac6d0ab0370c2815b4d.pdf"
    },
    {
      "success": true,
      "doc_id": "6b4d488022990a3227b3ac2f3c171051",
      "summary": "Standard model-free reinforcement learning algorithms optimize a policy that generates the action to be taken in the current time step in order to maximize expected future return. While flexible, it faces difficulties arising from the inefficient exploration due to its single step nature. In this work, we present Generative Planning method (GPM), which can generate actions not only for the current step, but also for a number of future steps (thus termed as generative planning). This brings several benefits to GPM. Firstly, since GPM is trained by maximizing value, the plans generated from it can be regarded as intentional action sequences for reaching high value regions. GPM can therefore leverage its generated multi-step plans for temporally coordinated exploration towards high value regions, which is potentially more effective than a sequence of actions generated by perturbing each action at single step level, whose consistent movement decays exponentially with the number of exploration steps. Secondly, starting from a crude initial plan generator, GPM can refine it to be adaptive to the task, which, in return, benefits future explorations. This is potentially more effective than commonly used action-repeat strategy, which is non-adaptive in its form of plans. Additionally, since the multi-step plan can be interpreted as the intent of the agent from now to a span of time period into the future, it offers a more informative and intuitive signal for interpretation. Experiments are conducted on several benchmark environments and the results demonstrated its effectiveness compared with several baseline methods.",
      "intriguing_abstract": "Standard model-free reinforcement learning algorithms optimize a policy that generates the action to be taken in the current time step in order to maximize expected future return. While flexible, it faces difficulties arising from the inefficient exploration due to its single step nature. In this work, we present Generative Planning method (GPM), which can generate actions not only for the current step, but also for a number of future steps (thus termed as generative planning). This brings several benefits to GPM. Firstly, since GPM is trained by maximizing value, the plans generated from it can be regarded as intentional action sequences for reaching high value regions. GPM can therefore leverage its generated multi-step plans for temporally coordinated exploration towards high value regions, which is potentially more effective than a sequence of actions generated by perturbing each action at single step level, whose consistent movement decays exponentially with the number of exploration steps. Secondly, starting from a crude initial plan generator, GPM can refine it to be adaptive to the task, which, in return, benefits future explorations. This is potentially more effective than commonly used action-repeat strategy, which is non-adaptive in its form of plans. Additionally, since the multi-step plan can be interpreted as the intent of the agent from now to a span of time period into the future, it offers a more informative and intuitive signal for interpretation. Experiments are conducted on several benchmark environments and the results demonstrated its effectiveness compared with several baseline methods.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/33e3f13087abd5241d55523140720f5e684b7bee.pdf",
      "citation_key": "zhang2022p0b",
      "metadata": {
        "title": "Generative Planning for Temporally Coordinated Exploration in Reinforcement Learning",
        "authors": [
          "Haichao Zhang",
          "Wei Xu",
          "Haonan Yu"
        ],
        "published_date": "2022",
        "abstract": "Standard model-free reinforcement learning algorithms optimize a policy that generates the action to be taken in the current time step in order to maximize expected future return. While flexible, it faces difficulties arising from the inefficient exploration due to its single step nature. In this work, we present Generative Planning method (GPM), which can generate actions not only for the current step, but also for a number of future steps (thus termed as generative planning). This brings several benefits to GPM. Firstly, since GPM is trained by maximizing value, the plans generated from it can be regarded as intentional action sequences for reaching high value regions. GPM can therefore leverage its generated multi-step plans for temporally coordinated exploration towards high value regions, which is potentially more effective than a sequence of actions generated by perturbing each action at single step level, whose consistent movement decays exponentially with the number of exploration steps. Secondly, starting from a crude initial plan generator, GPM can refine it to be adaptive to the task, which, in return, benefits future explorations. This is potentially more effective than commonly used action-repeat strategy, which is non-adaptive in its form of plans. Additionally, since the multi-step plan can be interpreted as the intent of the agent from now to a span of time period into the future, it offers a more informative and intuitive signal for interpretation. Experiments are conducted on several benchmark environments and the results demonstrated its effectiveness compared with several baseline methods.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/33e3f13087abd5241d55523140720f5e684b7bee.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 10,
        "score": 3.333333333333333,
        "summary": "Standard model-free reinforcement learning algorithms optimize a policy that generates the action to be taken in the current time step in order to maximize expected future return. While flexible, it faces difficulties arising from the inefficient exploration due to its single step nature. In this work, we present Generative Planning method (GPM), which can generate actions not only for the current step, but also for a number of future steps (thus termed as generative planning). This brings several benefits to GPM. Firstly, since GPM is trained by maximizing value, the plans generated from it can be regarded as intentional action sequences for reaching high value regions. GPM can therefore leverage its generated multi-step plans for temporally coordinated exploration towards high value regions, which is potentially more effective than a sequence of actions generated by perturbing each action at single step level, whose consistent movement decays exponentially with the number of exploration steps. Secondly, starting from a crude initial plan generator, GPM can refine it to be adaptive to the task, which, in return, benefits future explorations. This is potentially more effective than commonly used action-repeat strategy, which is non-adaptive in its form of plans. Additionally, since the multi-step plan can be interpreted as the intent of the agent from now to a span of time period into the future, it offers a more informative and intuitive signal for interpretation. Experiments are conducted on several benchmark environments and the results demonstrated its effectiveness compared with several baseline methods.",
        "keywords": []
      },
      "file_name": "33e3f13087abd5241d55523140720f5e684b7bee.pdf"
    },
    {
      "success": true,
      "doc_id": "1bb9ee9111cde9c1f31d23db7aa1550f",
      "summary": "To improve the satisfaction and acceptance of automatic driving, we propose a deep reinforcement learning (DRL)-based autonomous car-following (CF) decision-making strategy using naturalist driving data (NDD). This study examines the traits of CF behavior using 1341 pairs of CF events taken from the Next Generation Simulation (NGSIM) data. Furthermore, in order to improve the random exploration of the agents action, the dynamic characteristics of the speed-acceleration distribution are established in accordance with NDD. The actions varying constraints are achieved via a normal distribution 3 boundary point-to-fit curve. A multiobjective reward function is designed considering safety, efficiency, and comfort, according to the time headway (THW) probability density distribution. The introduction of a penalty reward in mechanical energy allows the agent to internalize negative experiences. Next, a model of agent-environment interaction for CF decision-making control is built using the deep deterministic policy gradient (DDPG) method, which can explore complicated environments. Finally, extensive simulation experiments validate the effectiveness and accuracy of our proposal, and the driving strategy is learned through real-world driving data, which is better than human data.",
      "intriguing_abstract": "To improve the satisfaction and acceptance of automatic driving, we propose a deep reinforcement learning (DRL)-based autonomous car-following (CF) decision-making strategy using naturalist driving data (NDD). This study examines the traits of CF behavior using 1341 pairs of CF events taken from the Next Generation Simulation (NGSIM) data. Furthermore, in order to improve the random exploration of the agents action, the dynamic characteristics of the speed-acceleration distribution are established in accordance with NDD. The actions varying constraints are achieved via a normal distribution 3 boundary point-to-fit curve. A multiobjective reward function is designed considering safety, efficiency, and comfort, according to the time headway (THW) probability density distribution. The introduction of a penalty reward in mechanical energy allows the agent to internalize negative experiences. Next, a model of agent-environment interaction for CF decision-making control is built using the deep deterministic policy gradient (DDPG) method, which can explore complicated environments. Finally, extensive simulation experiments validate the effectiveness and accuracy of our proposal, and the driving strategy is learned through real-world driving data, which is better than human data.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/23e3f39ede3c45cb8cf456de1f56d7a8b8d4bc2e.pdf",
      "citation_key": "li20227ss",
      "metadata": {
        "title": "A Decision-Making Strategy for Car Following Based on Naturalist Driving Data via Deep Reinforcement Learning",
        "authors": [
          "Wenli Li",
          "Yousong Zhang",
          "Xiaohui Shi",
          "Fanke Qiu"
        ],
        "published_date": "2022",
        "abstract": "To improve the satisfaction and acceptance of automatic driving, we propose a deep reinforcement learning (DRL)-based autonomous car-following (CF) decision-making strategy using naturalist driving data (NDD). This study examines the traits of CF behavior using 1341 pairs of CF events taken from the Next Generation Simulation (NGSIM) data. Furthermore, in order to improve the random exploration of the agents action, the dynamic characteristics of the speed-acceleration distribution are established in accordance with NDD. The actions varying constraints are achieved via a normal distribution 3 boundary point-to-fit curve. A multiobjective reward function is designed considering safety, efficiency, and comfort, according to the time headway (THW) probability density distribution. The introduction of a penalty reward in mechanical energy allows the agent to internalize negative experiences. Next, a model of agent-environment interaction for CF decision-making control is built using the deep deterministic policy gradient (DDPG) method, which can explore complicated environments. Finally, extensive simulation experiments validate the effectiveness and accuracy of our proposal, and the driving strategy is learned through real-world driving data, which is better than human data.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/23e3f39ede3c45cb8cf456de1f56d7a8b8d4bc2e.pdf",
        "venue": "Italian National Conference on Sensors",
        "citationCount": 10,
        "score": 3.333333333333333,
        "summary": "To improve the satisfaction and acceptance of automatic driving, we propose a deep reinforcement learning (DRL)-based autonomous car-following (CF) decision-making strategy using naturalist driving data (NDD). This study examines the traits of CF behavior using 1341 pairs of CF events taken from the Next Generation Simulation (NGSIM) data. Furthermore, in order to improve the random exploration of the agents action, the dynamic characteristics of the speed-acceleration distribution are established in accordance with NDD. The actions varying constraints are achieved via a normal distribution 3 boundary point-to-fit curve. A multiobjective reward function is designed considering safety, efficiency, and comfort, according to the time headway (THW) probability density distribution. The introduction of a penalty reward in mechanical energy allows the agent to internalize negative experiences. Next, a model of agent-environment interaction for CF decision-making control is built using the deep deterministic policy gradient (DDPG) method, which can explore complicated environments. Finally, extensive simulation experiments validate the effectiveness and accuracy of our proposal, and the driving strategy is learned through real-world driving data, which is better than human data.",
        "keywords": []
      },
      "file_name": "23e3f39ede3c45cb8cf456de1f56d7a8b8d4bc2e.pdf"
    },
    {
      "success": true,
      "doc_id": "f0635ceadf06bb7cea0868138e5b2c93",
      "summary": "Reinforcement learning (RL) operating on attack graphs leveraging cyber terrain principles are used to develop reward and state associated with determination of surveillance detection routes (SDR). This work extends previous efforts on developing RL methods for path analysis within enterprise networks. This work focuses on building SDR where the routes focus on exploring the network services while trying to evade risk. RL is utilized to support the development of these routes by building a reward mechanism that would help in realization of these paths. The RL algorithm is modified to have a novel warm-up phase which decides in the initial exploration which areas of the network are safe to explore based on the rewards and penalty scale factor.",
      "intriguing_abstract": "Reinforcement learning (RL) operating on attack graphs leveraging cyber terrain principles are used to develop reward and state associated with determination of surveillance detection routes (SDR). This work extends previous efforts on developing RL methods for path analysis within enterprise networks. This work focuses on building SDR where the routes focus on exploring the network services while trying to evade risk. RL is utilized to support the development of these routes by building a reward mechanism that would help in realization of these paths. The RL algorithm is modified to have a novel warm-up phase which decides in the initial exploration which areas of the network are safe to explore based on the rewards and penalty scale factor.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/97fe089acbe9d1f55753cd6b2ad6070e89521f6c.pdf",
      "citation_key": "huang2022or8",
      "metadata": {
        "title": "Exposing Surveillance Detection Routes via Reinforcement Learning, Attack Graphs, and Cyber Terrain",
        "authors": [
          "Lanxiao Huang",
          "Tyler Cody",
          "Christopher Redino",
          "Abdul Rahman",
          "A. Kakkar",
          "Deepak Kushwaha",
          "Cheng Wang",
          "Ryan Clark",
          "Dan Radke",
          "P. Beling",
          "E. Bowen"
        ],
        "published_date": "2022",
        "abstract": "Reinforcement learning (RL) operating on attack graphs leveraging cyber terrain principles are used to develop reward and state associated with determination of surveillance detection routes (SDR). This work extends previous efforts on developing RL methods for path analysis within enterprise networks. This work focuses on building SDR where the routes focus on exploring the network services while trying to evade risk. RL is utilized to support the development of these routes by building a reward mechanism that would help in realization of these paths. The RL algorithm is modified to have a novel warm-up phase which decides in the initial exploration which areas of the network are safe to explore based on the rewards and penalty scale factor.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/97fe089acbe9d1f55753cd6b2ad6070e89521f6c.pdf",
        "venue": "International Conference on Machine Learning and Applications",
        "citationCount": 10,
        "score": 3.333333333333333,
        "summary": "Reinforcement learning (RL) operating on attack graphs leveraging cyber terrain principles are used to develop reward and state associated with determination of surveillance detection routes (SDR). This work extends previous efforts on developing RL methods for path analysis within enterprise networks. This work focuses on building SDR where the routes focus on exploring the network services while trying to evade risk. RL is utilized to support the development of these routes by building a reward mechanism that would help in realization of these paths. The RL algorithm is modified to have a novel warm-up phase which decides in the initial exploration which areas of the network are safe to explore based on the rewards and penalty scale factor.",
        "keywords": []
      },
      "file_name": "97fe089acbe9d1f55753cd6b2ad6070e89521f6c.pdf"
    },
    {
      "success": true,
      "doc_id": "0b400be3e8446747f19fb9f95ebaabd3",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/6d97b81b3473492cb9986a63886cbb128496010c.pdf",
      "citation_key": "modi2019fs3",
      "metadata": {
        "title": "No-regret Exploration in Contextual Reinforcement Learning",
        "authors": [
          "Aditya Modi",
          "Ambuj Tewari"
        ],
        "published_date": "2019",
        "abstract": "",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/6d97b81b3473492cb9986a63886cbb128496010c.pdf",
        "venue": "Conference on Uncertainty in Artificial Intelligence",
        "citationCount": 19,
        "score": 3.1666666666666665,
        "summary": "",
        "keywords": []
      },
      "file_name": "6d97b81b3473492cb9986a63886cbb128496010c.pdf"
    },
    {
      "success": true,
      "doc_id": "b51abd6b920d2bb549f9b281445f3be1",
      "summary": "Human-oriented image captioning with both high diversity and accuracy is a challenging task in vision+language modeling. The reinforcement learning (RL) based frameworks promote the accuracy of image captioning, yet seriously hurt the diversity. In contrast, other methods based on variational auto-encoder (VAE) or generative adversarial network (GAN) can produce diverse yet less accurate captions. In this work, we devote our attention to promote the diversity of RL-based image captioning. To be specific, we devise a partial off-policy learning scheme to balance accuracy and diversity. First, we keep the model exposed to varied candidate captions by sampling from the initial state before RL launched. Second, a novel criterion named max-CIDEr is proposed to serve as the reward for promoting diversity. We combine the above-mentioned offpolicy strategy with the on-policy one to moderate the exploration effect, further balancing the diversity and accuracy for human-like image captioning. Experiments show that our method locates the closest to human performance in the diversity-accuracy space, and achieves the highest Pearson correlation as 0.337 with human performance.",
      "intriguing_abstract": "Human-oriented image captioning with both high diversity and accuracy is a challenging task in vision+language modeling. The reinforcement learning (RL) based frameworks promote the accuracy of image captioning, yet seriously hurt the diversity. In contrast, other methods based on variational auto-encoder (VAE) or generative adversarial network (GAN) can produce diverse yet less accurate captions. In this work, we devote our attention to promote the diversity of RL-based image captioning. To be specific, we devise a partial off-policy learning scheme to balance accuracy and diversity. First, we keep the model exposed to varied candidate captions by sampling from the initial state before RL launched. Second, a novel criterion named max-CIDEr is proposed to serve as the reward for promoting diversity. We combine the above-mentioned offpolicy strategy with the on-policy one to moderate the exploration effect, further balancing the diversity and accuracy for human-like image captioning. Experiments show that our method locates the closest to human performance in the diversity-accuracy space, and achieves the highest Pearson correlation as 0.337 with human performance.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/807f377de905eda62e4cd2f0797153a59296adbb.pdf",
      "citation_key": "shi20215ek",
      "metadata": {
        "title": "Partial Off-policy Learning: Balance Accuracy and Diversity for Human-Oriented Image Captioning",
        "authors": [
          "Jiahe Shi",
          "Yali Li",
          "Shengjin Wang"
        ],
        "published_date": "2021",
        "abstract": "Human-oriented image captioning with both high diversity and accuracy is a challenging task in vision+language modeling. The reinforcement learning (RL) based frameworks promote the accuracy of image captioning, yet seriously hurt the diversity. In contrast, other methods based on variational auto-encoder (VAE) or generative adversarial network (GAN) can produce diverse yet less accurate captions. In this work, we devote our attention to promote the diversity of RL-based image captioning. To be specific, we devise a partial off-policy learning scheme to balance accuracy and diversity. First, we keep the model exposed to varied candidate captions by sampling from the initial state before RL launched. Second, a novel criterion named max-CIDEr is proposed to serve as the reward for promoting diversity. We combine the above-mentioned offpolicy strategy with the on-policy one to moderate the exploration effect, further balancing the diversity and accuracy for human-like image captioning. Experiments show that our method locates the closest to human performance in the diversity-accuracy space, and achieves the highest Pearson correlation as 0.337 with human performance.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/807f377de905eda62e4cd2f0797153a59296adbb.pdf",
        "venue": "IEEE International Conference on Computer Vision",
        "citationCount": 12,
        "score": 3.0,
        "summary": "Human-oriented image captioning with both high diversity and accuracy is a challenging task in vision+language modeling. The reinforcement learning (RL) based frameworks promote the accuracy of image captioning, yet seriously hurt the diversity. In contrast, other methods based on variational auto-encoder (VAE) or generative adversarial network (GAN) can produce diverse yet less accurate captions. In this work, we devote our attention to promote the diversity of RL-based image captioning. To be specific, we devise a partial off-policy learning scheme to balance accuracy and diversity. First, we keep the model exposed to varied candidate captions by sampling from the initial state before RL launched. Second, a novel criterion named max-CIDEr is proposed to serve as the reward for promoting diversity. We combine the above-mentioned offpolicy strategy with the on-policy one to moderate the exploration effect, further balancing the diversity and accuracy for human-like image captioning. Experiments show that our method locates the closest to human performance in the diversity-accuracy space, and achieves the highest Pearson correlation as 0.337 with human performance.",
        "keywords": []
      },
      "file_name": "807f377de905eda62e4cd2f0797153a59296adbb.pdf"
    },
    {
      "success": true,
      "doc_id": "18036d91fbbdab1e87b1b8d9b5feb57b",
      "summary": "Symbolic regression has become a hot topic in recent years due to the surging demand for interpretable machine learning methods. Traditionally, symbolic regression problems are mainly solved by genetic algorithms. Nonetheless, with the development of deep learning, reinforcement learning based symbolic regression methods have received attention gradually. Unfortunately, hardly any of those reinforcement learning based methods have been proven effectively to solve real world regression problems as genetic algorithm based methods. In this paper, we find a general reinforcement learning based symbolic regression method is difficult to solve real world problems since it is hard to balance between exploration and exploitation. To deal with this problem, we propose a hybrid method to use both genetic algorithm and reinforcement learning for solving symbolic regression problems. By doing so, we can combine the advantages of reinforcement learning and genetic algorithm and achieve better performance than using them alone. To validate the effectiveness of the proposed method, we apply the proposed method to ten benchmark datasets. The experimental results show that the proposed method achieves competitive performance compared with several well-known symbolic regression methods on those datasets.",
      "intriguing_abstract": "Symbolic regression has become a hot topic in recent years due to the surging demand for interpretable machine learning methods. Traditionally, symbolic regression problems are mainly solved by genetic algorithms. Nonetheless, with the development of deep learning, reinforcement learning based symbolic regression methods have received attention gradually. Unfortunately, hardly any of those reinforcement learning based methods have been proven effectively to solve real world regression problems as genetic algorithm based methods. In this paper, we find a general reinforcement learning based symbolic regression method is difficult to solve real world problems since it is hard to balance between exploration and exploitation. To deal with this problem, we propose a hybrid method to use both genetic algorithm and reinforcement learning for solving symbolic regression problems. By doing so, we can combine the advantages of reinforcement learning and genetic algorithm and achieve better performance than using them alone. To validate the effectiveness of the proposed method, we apply the proposed method to ten benchmark datasets. The experimental results show that the proposed method achieves competitive performance compared with several well-known symbolic regression methods on those datasets.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/f14645d3a0740504ee632ab06f045cceaa5297bc.pdf",
      "citation_key": "zhang2021qq6",
      "metadata": {
        "title": "RL-GEP: Symbolic Regression via Gene Expression Programming and Reinforcement Learning",
        "authors": [
          "Hengzhe Zhang",
          "Aimin Zhou"
        ],
        "published_date": "2021",
        "abstract": "Symbolic regression has become a hot topic in recent years due to the surging demand for interpretable machine learning methods. Traditionally, symbolic regression problems are mainly solved by genetic algorithms. Nonetheless, with the development of deep learning, reinforcement learning based symbolic regression methods have received attention gradually. Unfortunately, hardly any of those reinforcement learning based methods have been proven effectively to solve real world regression problems as genetic algorithm based methods. In this paper, we find a general reinforcement learning based symbolic regression method is difficult to solve real world problems since it is hard to balance between exploration and exploitation. To deal with this problem, we propose a hybrid method to use both genetic algorithm and reinforcement learning for solving symbolic regression problems. By doing so, we can combine the advantages of reinforcement learning and genetic algorithm and achieve better performance than using them alone. To validate the effectiveness of the proposed method, we apply the proposed method to ten benchmark datasets. The experimental results show that the proposed method achieves competitive performance compared with several well-known symbolic regression methods on those datasets.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/f14645d3a0740504ee632ab06f045cceaa5297bc.pdf",
        "venue": "IEEE International Joint Conference on Neural Network",
        "citationCount": 12,
        "score": 3.0,
        "summary": "Symbolic regression has become a hot topic in recent years due to the surging demand for interpretable machine learning methods. Traditionally, symbolic regression problems are mainly solved by genetic algorithms. Nonetheless, with the development of deep learning, reinforcement learning based symbolic regression methods have received attention gradually. Unfortunately, hardly any of those reinforcement learning based methods have been proven effectively to solve real world regression problems as genetic algorithm based methods. In this paper, we find a general reinforcement learning based symbolic regression method is difficult to solve real world problems since it is hard to balance between exploration and exploitation. To deal with this problem, we propose a hybrid method to use both genetic algorithm and reinforcement learning for solving symbolic regression problems. By doing so, we can combine the advantages of reinforcement learning and genetic algorithm and achieve better performance than using them alone. To validate the effectiveness of the proposed method, we apply the proposed method to ten benchmark datasets. The experimental results show that the proposed method achieves competitive performance compared with several well-known symbolic regression methods on those datasets.",
        "keywords": []
      },
      "file_name": "f14645d3a0740504ee632ab06f045cceaa5297bc.pdf"
    },
    {
      "success": true,
      "doc_id": "9ae86deb8588fd045ae825d6809dc4d8",
      "summary": "Reinforcement Learning (RL), especially Deep Reinforcement Learning (DRL), has made great progress in many areas, such as robots, video games and driving. However, sample inefficiency is a big obstacle to the widespread practical application of DRL. Inspired by the decision making in human brain, this problem can be solved by incorporating instance based learning, i.e. episodic memory. Many episodic memory based RL algorithms have emerged recently. However, these algorithms either only replace parametric DRL algorithm with episodic control or incorporate episodic memory in a single component of DRL. In contrast to preview works, this paper proposes a new sample-efficient reinforcement learning architecture which introduces a new episodic memory module and incorporates episodic thought into some key components of DRL: exploration, experience replay and loss function. Taking Deep Q-Network (DQN) algorithm for example, when combined with DQN, our algorithm is called High Efficient Episodic Memory DQN (HE-EMDQN). In HE-EMDQN, a new non-parametric episodic memory module is introduced to help calculate the loss and modify the predicted value for exploration. For the sake of accelerating the sample learning in experience replay, an auxiliary small buffer called percentile best episode replay memory is designed to compose a mixed mini-batch. We show across the testing environments that our algorithm is significantly more powerful and sample-efficient than DQN and the recent episodic memory deep q-network (EMDQN). This work provides a new perspective for other RL algorithms to improve sample efficiency by utilising episodic memory efficiently.",
      "intriguing_abstract": "Reinforcement Learning (RL), especially Deep Reinforcement Learning (DRL), has made great progress in many areas, such as robots, video games and driving. However, sample inefficiency is a big obstacle to the widespread practical application of DRL. Inspired by the decision making in human brain, this problem can be solved by incorporating instance based learning, i.e. episodic memory. Many episodic memory based RL algorithms have emerged recently. However, these algorithms either only replace parametric DRL algorithm with episodic control or incorporate episodic memory in a single component of DRL. In contrast to preview works, this paper proposes a new sample-efficient reinforcement learning architecture which introduces a new episodic memory module and incorporates episodic thought into some key components of DRL: exploration, experience replay and loss function. Taking Deep Q-Network (DQN) algorithm for example, when combined with DQN, our algorithm is called High Efficient Episodic Memory DQN (HE-EMDQN). In HE-EMDQN, a new non-parametric episodic memory module is introduced to help calculate the loss and modify the predicted value for exploration. For the sake of accelerating the sample learning in experience replay, an auxiliary small buffer called percentile best episode replay memory is designed to compose a mixed mini-batch. We show across the testing environments that our algorithm is significantly more powerful and sample-efficient than DQN and the recent episodic memory deep q-network (EMDQN). This work provides a new perspective for other RL algorithms to improve sample efficiency by utilising episodic memory efficiently.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/f715558b65fd4f3c6966505c237d9a622947010b.pdf",
      "citation_key": "yang2020dxb",
      "metadata": {
        "title": "Sample Efficient Reinforcement Learning Method via High Efficient Episodic Memory",
        "authors": [
          "Dujia Yang",
          "Xiaowei Qin",
          "Xiaodong Xu",
          "Chensheng Li",
          "Guo Wei"
        ],
        "published_date": "2020",
        "abstract": "Reinforcement Learning (RL), especially Deep Reinforcement Learning (DRL), has made great progress in many areas, such as robots, video games and driving. However, sample inefficiency is a big obstacle to the widespread practical application of DRL. Inspired by the decision making in human brain, this problem can be solved by incorporating instance based learning, i.e. episodic memory. Many episodic memory based RL algorithms have emerged recently. However, these algorithms either only replace parametric DRL algorithm with episodic control or incorporate episodic memory in a single component of DRL. In contrast to preview works, this paper proposes a new sample-efficient reinforcement learning architecture which introduces a new episodic memory module and incorporates episodic thought into some key components of DRL: exploration, experience replay and loss function. Taking Deep Q-Network (DQN) algorithm for example, when combined with DQN, our algorithm is called High Efficient Episodic Memory DQN (HE-EMDQN). In HE-EMDQN, a new non-parametric episodic memory module is introduced to help calculate the loss and modify the predicted value for exploration. For the sake of accelerating the sample learning in experience replay, an auxiliary small buffer called percentile best episode replay memory is designed to compose a mixed mini-batch. We show across the testing environments that our algorithm is significantly more powerful and sample-efficient than DQN and the recent episodic memory deep q-network (EMDQN). This work provides a new perspective for other RL algorithms to improve sample efficiency by utilising episodic memory efficiently.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/f715558b65fd4f3c6966505c237d9a622947010b.pdf",
        "venue": "IEEE Access",
        "citationCount": 15,
        "score": 3.0,
        "summary": "Reinforcement Learning (RL), especially Deep Reinforcement Learning (DRL), has made great progress in many areas, such as robots, video games and driving. However, sample inefficiency is a big obstacle to the widespread practical application of DRL. Inspired by the decision making in human brain, this problem can be solved by incorporating instance based learning, i.e. episodic memory. Many episodic memory based RL algorithms have emerged recently. However, these algorithms either only replace parametric DRL algorithm with episodic control or incorporate episodic memory in a single component of DRL. In contrast to preview works, this paper proposes a new sample-efficient reinforcement learning architecture which introduces a new episodic memory module and incorporates episodic thought into some key components of DRL: exploration, experience replay and loss function. Taking Deep Q-Network (DQN) algorithm for example, when combined with DQN, our algorithm is called High Efficient Episodic Memory DQN (HE-EMDQN). In HE-EMDQN, a new non-parametric episodic memory module is introduced to help calculate the loss and modify the predicted value for exploration. For the sake of accelerating the sample learning in experience replay, an auxiliary small buffer called percentile best episode replay memory is designed to compose a mixed mini-batch. We show across the testing environments that our algorithm is significantly more powerful and sample-efficient than DQN and the recent episodic memory deep q-network (EMDQN). This work provides a new perspective for other RL algorithms to improve sample efficiency by utilising episodic memory efficiently.",
        "keywords": []
      },
      "file_name": "f715558b65fd4f3c6966505c237d9a622947010b.pdf"
    },
    {
      "success": true,
      "doc_id": "791cae618ba44d98a18fef77569ea783",
      "summary": "Similar to their counterparts in nature, the flexible bodies of snake-like robots enhance their movement capability and adaptability in diverse environments. However, this flexibility corresponds to a complex control task involving highly redundant degrees of freedom, where traditional model-based methods usually fail to propel the robots energy-efficiently. In this work, we present a novel approach for designing an energy-efficient slithering gait for a snake-like robot using a model-free reinforcement learning (RL) algorithm. Specifically, we present an RL-based controller for generating locomotion gaits at a wide range of velocities, which is trained using the proximal policy optimization (PPO) algorithm. Meanwhile, a traditional parameterized gait controller is presented and the parameter sets are optimized using the grid search and Bayesian optimization algorithms for the purposes of reasonable comparisons. Based on the analysis of the simulation results, we demonstrate that this RL-based controller exhibits very natural and adaptive movements, which are also substantially more energy-efficient than the gaits generated by the parameterized controller. Videos are shown athttps://videoviewsite.wixsite.com/rlsnake.",
      "intriguing_abstract": "Similar to their counterparts in nature, the flexible bodies of snake-like robots enhance their movement capability and adaptability in diverse environments. However, this flexibility corresponds to a complex control task involving highly redundant degrees of freedom, where traditional model-based methods usually fail to propel the robots energy-efficiently. In this work, we present a novel approach for designing an energy-efficient slithering gait for a snake-like robot using a model-free reinforcement learning (RL) algorithm. Specifically, we present an RL-based controller for generating locomotion gaits at a wide range of velocities, which is trained using the proximal policy optimization (PPO) algorithm. Meanwhile, a traditional parameterized gait controller is presented and the parameter sets are optimized using the grid search and Bayesian optimization algorithms for the purposes of reasonable comparisons. Based on the analysis of the simulation results, we demonstrate that this RL-based controller exhibits very natural and adaptive movements, which are also substantially more energy-efficient than the gaits generated by the parameterized controller. Videos are shown athttps://videoviewsite.wixsite.com/rlsnake.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/e0d4af45fa3073dbfa9b6e464fa88d52e6f06928.pdf",
      "citation_key": "bing2019py7",
      "metadata": {
        "title": "Energy-Efficient Slithering Gait Exploration for a Snake-like Robot based on Reinforcement Learning",
        "authors": [
          "Zhenshan Bing",
          "Christian Lemke",
          "Zhuangyi Jiang",
          "Kai Huang",
          "A. Knoll"
        ],
        "published_date": "2019",
        "abstract": "Similar to their counterparts in nature, the flexible bodies of snake-like robots enhance their movement capability and adaptability in diverse environments. However, this flexibility corresponds to a complex control task involving highly redundant degrees of freedom, where traditional model-based methods usually fail to propel the robots energy-efficiently. In this work, we present a novel approach for designing an energy-efficient slithering gait for a snake-like robot using a model-free reinforcement learning (RL) algorithm. Specifically, we present an RL-based controller for generating locomotion gaits at a wide range of velocities, which is trained using the proximal policy optimization (PPO) algorithm. Meanwhile, a traditional parameterized gait controller is presented and the parameter sets are optimized using the grid search and Bayesian optimization algorithms for the purposes of reasonable comparisons. Based on the analysis of the simulation results, we demonstrate that this RL-based controller exhibits very natural and adaptive movements, which are also substantially more energy-efficient than the gaits generated by the parameterized controller. Videos are shown athttps://videoviewsite.wixsite.com/rlsnake.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/e0d4af45fa3073dbfa9b6e464fa88d52e6f06928.pdf",
        "venue": "International Joint Conference on Artificial Intelligence",
        "citationCount": 17,
        "score": 2.833333333333333,
        "summary": "Similar to their counterparts in nature, the flexible bodies of snake-like robots enhance their movement capability and adaptability in diverse environments. However, this flexibility corresponds to a complex control task involving highly redundant degrees of freedom, where traditional model-based methods usually fail to propel the robots energy-efficiently. In this work, we present a novel approach for designing an energy-efficient slithering gait for a snake-like robot using a model-free reinforcement learning (RL) algorithm. Specifically, we present an RL-based controller for generating locomotion gaits at a wide range of velocities, which is trained using the proximal policy optimization (PPO) algorithm. Meanwhile, a traditional parameterized gait controller is presented and the parameter sets are optimized using the grid search and Bayesian optimization algorithms for the purposes of reasonable comparisons. Based on the analysis of the simulation results, we demonstrate that this RL-based controller exhibits very natural and adaptive movements, which are also substantially more energy-efficient than the gaits generated by the parameterized controller. Videos are shown athttps://videoviewsite.wixsite.com/rlsnake.",
        "keywords": []
      },
      "file_name": "e0d4af45fa3073dbfa9b6e464fa88d52e6f06928.pdf"
    },
    {
      "success": true,
      "doc_id": "4ea41b1882ccb5251d7ccc6b7062e24b",
      "summary": "Deep reinforcement learning (DRL) techniques have been used to solve a discretionary lane change decision-making problem and are showing promising results. However, since the input information for the discretionary lane change problem is continuous and can be in high dimension, it is an open challenge for DRL to optimize the exploration-exploitation trade-off. Conventional model-less exploration methods lack a systematic way to incorporate additional engineering or model-based knowledge of our application into consideration and as a result, the training can be inefficient and may dwell on a policy, e.g. lane change strategy that is impractical. In previous related work, many used the rule-based safety check policy to guide the exploration and collect input information data. However, it is not guaranteed to get the optimal policy and the performance is dependent on the safety check policy selected. In this paper, we developed an explicit statistical aggregated environment model using a conditional variational auto-encoder and a model-based exploration strategy leveraging it. The agent is guided to explore with surprise-based intrinsic reward derived from the environment model. The result is compared with annealing epsilon-greedy exploration and with rule-based safety check exploration. We demonstrate that the performance of the developed model-based exploration method is comparable with the best rule-based safety check exploration and much better than the epsilon-greedy exploration.",
      "intriguing_abstract": "Deep reinforcement learning (DRL) techniques have been used to solve a discretionary lane change decision-making problem and are showing promising results. However, since the input information for the discretionary lane change problem is continuous and can be in high dimension, it is an open challenge for DRL to optimize the exploration-exploitation trade-off. Conventional model-less exploration methods lack a systematic way to incorporate additional engineering or model-based knowledge of our application into consideration and as a result, the training can be inefficient and may dwell on a policy, e.g. lane change strategy that is impractical. In previous related work, many used the rule-based safety check policy to guide the exploration and collect input information data. However, it is not guaranteed to get the optimal policy and the performance is dependent on the safety check policy selected. In this paper, we developed an explicit statistical aggregated environment model using a conditional variational auto-encoder and a model-based exploration strategy leveraging it. The agent is guided to explore with surprise-based intrinsic reward derived from the environment model. The result is compared with annealing epsilon-greedy exploration and with rule-based safety check exploration. We demonstrate that the performance of the developed model-based exploration method is comparable with the best rule-based safety check exploration and much better than the epsilon-greedy exploration.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/f80432fa03c91283cadbf6c262a7bc6e45f4edd3.pdf",
      "citation_key": "zhang20192ef",
      "metadata": {
        "title": "Discretionary Lane Change Decision Making using Reinforcement Learning with Model-Based Exploration",
        "authors": [
          "Songan Zhang",
          "H. Peng",
          "S. Nageshrao",
          "H. E. Tseng"
        ],
        "published_date": "2019",
        "abstract": "Deep reinforcement learning (DRL) techniques have been used to solve a discretionary lane change decision-making problem and are showing promising results. However, since the input information for the discretionary lane change problem is continuous and can be in high dimension, it is an open challenge for DRL to optimize the exploration-exploitation trade-off. Conventional model-less exploration methods lack a systematic way to incorporate additional engineering or model-based knowledge of our application into consideration and as a result, the training can be inefficient and may dwell on a policy, e.g. lane change strategy that is impractical. In previous related work, many used the rule-based safety check policy to guide the exploration and collect input information data. However, it is not guaranteed to get the optimal policy and the performance is dependent on the safety check policy selected. In this paper, we developed an explicit statistical aggregated environment model using a conditional variational auto-encoder and a model-based exploration strategy leveraging it. The agent is guided to explore with surprise-based intrinsic reward derived from the environment model. The result is compared with annealing epsilon-greedy exploration and with rule-based safety check exploration. We demonstrate that the performance of the developed model-based exploration method is comparable with the best rule-based safety check exploration and much better than the epsilon-greedy exploration.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/f80432fa03c91283cadbf6c262a7bc6e45f4edd3.pdf",
        "venue": "International Conference on Machine Learning and Applications",
        "citationCount": 17,
        "score": 2.833333333333333,
        "summary": "Deep reinforcement learning (DRL) techniques have been used to solve a discretionary lane change decision-making problem and are showing promising results. However, since the input information for the discretionary lane change problem is continuous and can be in high dimension, it is an open challenge for DRL to optimize the exploration-exploitation trade-off. Conventional model-less exploration methods lack a systematic way to incorporate additional engineering or model-based knowledge of our application into consideration and as a result, the training can be inefficient and may dwell on a policy, e.g. lane change strategy that is impractical. In previous related work, many used the rule-based safety check policy to guide the exploration and collect input information data. However, it is not guaranteed to get the optimal policy and the performance is dependent on the safety check policy selected. In this paper, we developed an explicit statistical aggregated environment model using a conditional variational auto-encoder and a model-based exploration strategy leveraging it. The agent is guided to explore with surprise-based intrinsic reward derived from the environment model. The result is compared with annealing epsilon-greedy exploration and with rule-based safety check exploration. We demonstrate that the performance of the developed model-based exploration method is comparable with the best rule-based safety check exploration and much better than the epsilon-greedy exploration.",
        "keywords": []
      },
      "file_name": "f80432fa03c91283cadbf6c262a7bc6e45f4edd3.pdf"
    },
    {
      "success": true,
      "doc_id": "d477f1ec0a1dc170dea4d66e0d3a1910",
      "summary": "Reinforcement Learning (RL) agents often encounter the bottleneck of the performance when the dilemma of exploration and exploitation arises. In this study, an adaptive exploration strategy with multi-attribute decision-making is proposed to address the trade-off problem between exploration and exploitation. Firstly, the proposed method decomposes a complex task into several sub-tasks and trains each sub-task using the same training method individually. Then, the proposed method uses a multi-attribute decision-making method to develop an action policy integrating the training results of these trained sub-tasks. There are practical advantages to improve learning performance by allowing multiple learners to learn in parallel. An adaptive exploration strategy determines the probability of exploration depending on the information entropy instead of the suffocating work of empirical tuning. Finally, transfer learning extends the applicability of the proposed method. The experiment of the robot migration, the robot confrontation, and the real wheeled mobile robot are used to demonstrate the availability and practicability of the proposed method.",
      "intriguing_abstract": "Reinforcement Learning (RL) agents often encounter the bottleneck of the performance when the dilemma of exploration and exploitation arises. In this study, an adaptive exploration strategy with multi-attribute decision-making is proposed to address the trade-off problem between exploration and exploitation. Firstly, the proposed method decomposes a complex task into several sub-tasks and trains each sub-task using the same training method individually. Then, the proposed method uses a multi-attribute decision-making method to develop an action policy integrating the training results of these trained sub-tasks. There are practical advantages to improve learning performance by allowing multiple learners to learn in parallel. An adaptive exploration strategy determines the probability of exploration depending on the information entropy instead of the suffocating work of empirical tuning. Finally, transfer learning extends the applicability of the proposed method. The experiment of the robot migration, the robot confrontation, and the real wheeled mobile robot are used to demonstrate the availability and practicability of the proposed method.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/117cd9e1dafc577e53e2d46897a784ed1e65996f.pdf",
      "citation_key": "hu2020yhq",
      "metadata": {
        "title": "Adaptive Exploration Strategy With Multi-Attribute Decision-Making for Reinforcement Learning",
        "authors": [
          "Chunyang Hu",
          "Meng Xu"
        ],
        "published_date": "2020",
        "abstract": "Reinforcement Learning (RL) agents often encounter the bottleneck of the performance when the dilemma of exploration and exploitation arises. In this study, an adaptive exploration strategy with multi-attribute decision-making is proposed to address the trade-off problem between exploration and exploitation. Firstly, the proposed method decomposes a complex task into several sub-tasks and trains each sub-task using the same training method individually. Then, the proposed method uses a multi-attribute decision-making method to develop an action policy integrating the training results of these trained sub-tasks. There are practical advantages to improve learning performance by allowing multiple learners to learn in parallel. An adaptive exploration strategy determines the probability of exploration depending on the information entropy instead of the suffocating work of empirical tuning. Finally, transfer learning extends the applicability of the proposed method. The experiment of the robot migration, the robot confrontation, and the real wheeled mobile robot are used to demonstrate the availability and practicability of the proposed method.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/117cd9e1dafc577e53e2d46897a784ed1e65996f.pdf",
        "venue": "IEEE Access",
        "citationCount": 14,
        "score": 2.8000000000000003,
        "summary": "Reinforcement Learning (RL) agents often encounter the bottleneck of the performance when the dilemma of exploration and exploitation arises. In this study, an adaptive exploration strategy with multi-attribute decision-making is proposed to address the trade-off problem between exploration and exploitation. Firstly, the proposed method decomposes a complex task into several sub-tasks and trains each sub-task using the same training method individually. Then, the proposed method uses a multi-attribute decision-making method to develop an action policy integrating the training results of these trained sub-tasks. There are practical advantages to improve learning performance by allowing multiple learners to learn in parallel. An adaptive exploration strategy determines the probability of exploration depending on the information entropy instead of the suffocating work of empirical tuning. Finally, transfer learning extends the applicability of the proposed method. The experiment of the robot migration, the robot confrontation, and the real wheeled mobile robot are used to demonstrate the availability and practicability of the proposed method.",
        "keywords": []
      },
      "file_name": "117cd9e1dafc577e53e2d46897a784ed1e65996f.pdf"
    },
    {
      "success": true,
      "doc_id": "f11943db8792e6941a1daf201376d33e",
      "summary": "We introduce a novel method to teach a robotic agent to interactively explore cluttered yet structured scenes, such as kitchen pantries and grocery shelves, by leveraging the physical plausibility of the scene. We propose a novel learning framework to train an effective scene exploration policy to discover hidden objects with minimal interactions. First, we define a novel scene grammar to represent structured clutter. Then we train a Graph Neural Network (GNN) based Scene Generation agent using deep reinforcement learning (deep RL), to manipulate this Scene Grammar to create a diverse set of stable scenes, each containing multiple hidden objects. Given such cluttered scenes, we then train a Scene Exploration agent, using deep RL, to uncover hidden objects by interactively rearranging the scene. We show that our learned agents hide and discover significantly more objects than the baselines. We present quantitative results that prove the generalization capabilities of our agents. We also demonstrate sim-to-real transfer by successfully deploying the learned policy on a real UR10 robot to explore real-world cluttered scenes. The supplemental video can be found at: https://www.youtube.com/watch?v=T2Jo7wwaXss.",
      "intriguing_abstract": "We introduce a novel method to teach a robotic agent to interactively explore cluttered yet structured scenes, such as kitchen pantries and grocery shelves, by leveraging the physical plausibility of the scene. We propose a novel learning framework to train an effective scene exploration policy to discover hidden objects with minimal interactions. First, we define a novel scene grammar to represent structured clutter. Then we train a Graph Neural Network (GNN) based Scene Generation agent using deep reinforcement learning (deep RL), to manipulate this Scene Grammar to create a diverse set of stable scenes, each containing multiple hidden objects. Given such cluttered scenes, we then train a Scene Exploration agent, using deep RL, to uncover hidden objects by interactively rearranging the scene. We show that our learned agents hide and discover significantly more objects than the baselines. We present quantitative results that prove the generalization capabilities of our agents. We also demonstrate sim-to-real transfer by successfully deploying the learned policy on a real UR10 robot to explore real-world cluttered scenes. The supplemental video can be found at: https://www.youtube.com/watch?v=T2Jo7wwaXss.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/46cbd8acfacca5fc74b8d05bb06264aff0d82a4e.pdf",
      "citation_key": "kumar20216sy",
      "metadata": {
        "title": "Graph-based Cluttered Scene Generation and Interactive Exploration using Deep Reinforcement Learning",
        "authors": [
          "K. N. Kumar",
          "Irfan Essa",
          "Sehoon Ha"
        ],
        "published_date": "2021",
        "abstract": "We introduce a novel method to teach a robotic agent to interactively explore cluttered yet structured scenes, such as kitchen pantries and grocery shelves, by leveraging the physical plausibility of the scene. We propose a novel learning framework to train an effective scene exploration policy to discover hidden objects with minimal interactions. First, we define a novel scene grammar to represent structured clutter. Then we train a Graph Neural Network (GNN) based Scene Generation agent using deep reinforcement learning (deep RL), to manipulate this Scene Grammar to create a diverse set of stable scenes, each containing multiple hidden objects. Given such cluttered scenes, we then train a Scene Exploration agent, using deep RL, to uncover hidden objects by interactively rearranging the scene. We show that our learned agents hide and discover significantly more objects than the baselines. We present quantitative results that prove the generalization capabilities of our agents. We also demonstrate sim-to-real transfer by successfully deploying the learned policy on a real UR10 robot to explore real-world cluttered scenes. The supplemental video can be found at: https://www.youtube.com/watch?v=T2Jo7wwaXss.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/46cbd8acfacca5fc74b8d05bb06264aff0d82a4e.pdf",
        "venue": "IEEE International Conference on Robotics and Automation",
        "citationCount": 11,
        "score": 2.75,
        "summary": "We introduce a novel method to teach a robotic agent to interactively explore cluttered yet structured scenes, such as kitchen pantries and grocery shelves, by leveraging the physical plausibility of the scene. We propose a novel learning framework to train an effective scene exploration policy to discover hidden objects with minimal interactions. First, we define a novel scene grammar to represent structured clutter. Then we train a Graph Neural Network (GNN) based Scene Generation agent using deep reinforcement learning (deep RL), to manipulate this Scene Grammar to create a diverse set of stable scenes, each containing multiple hidden objects. Given such cluttered scenes, we then train a Scene Exploration agent, using deep RL, to uncover hidden objects by interactively rearranging the scene. We show that our learned agents hide and discover significantly more objects than the baselines. We present quantitative results that prove the generalization capabilities of our agents. We also demonstrate sim-to-real transfer by successfully deploying the learned policy on a real UR10 robot to explore real-world cluttered scenes. The supplemental video can be found at: https://www.youtube.com/watch?v=T2Jo7wwaXss.",
        "keywords": []
      },
      "file_name": "46cbd8acfacca5fc74b8d05bb06264aff0d82a4e.pdf"
    },
    {
      "success": true,
      "doc_id": "261668b99a3c2c0f8cd6bbbec672d6ad",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/48de0dab9255ededce3cdaeac84f039d726f7f3e.pdf",
      "citation_key": "asiain2018wxr",
      "metadata": {
        "title": "Controller exploitation-exploration reinforcement learning architecture for computing near-optimal policies",
        "authors": [
          "Erick Asiain",
          "J. Clempner",
          "A. Poznyak"
        ],
        "published_date": "2018",
        "abstract": "",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/48de0dab9255ededce3cdaeac84f039d726f7f3e.pdf",
        "venue": "Soft Computing - A Fusion of Foundations, Methodologies and Applications",
        "citationCount": 19,
        "score": 2.714285714285714,
        "summary": "",
        "keywords": []
      },
      "file_name": "48de0dab9255ededce3cdaeac84f039d726f7f3e.pdf"
    },
    {
      "success": true,
      "doc_id": "8bb0977b33f607fcf2554721591b6590",
      "summary": "Deep reinforcement learning (DRL) has achieved remarkable results in many high-dimensional continuous control tasks. However, the RL agent still explores the environment randomly, resulting in low exploration efficiency and learning performance, especially in robotic manipulation tasks with sparse rewards. To address this problem, in this paper, we intro-duce a simplified Intrinsic Curiosity Module (S-ICM) into the off-policy RL methods to encourage the agent to pursue novel and surprising states for improving the exploration competence. This method can be combined with an arbitrary off-policy RL algorithm. We evaluate our approach on three challenging robotic manipulation tasks provided by OpenAI Gym. In our experiments, we combined our method with Deep Deterministic Policy Gradient (DDPG) with and without Hindsight Experience Replay (HER). The empirical results show that our proposed method significantly outperforms vanilla RL algorithms both in sample-efficiency and learning performance.",
      "intriguing_abstract": "Deep reinforcement learning (DRL) has achieved remarkable results in many high-dimensional continuous control tasks. However, the RL agent still explores the environment randomly, resulting in low exploration efficiency and learning performance, especially in robotic manipulation tasks with sparse rewards. To address this problem, in this paper, we intro-duce a simplified Intrinsic Curiosity Module (S-ICM) into the off-policy RL methods to encourage the agent to pursue novel and surprising states for improving the exploration competence. This method can be combined with an arbitrary off-policy RL algorithm. We evaluate our approach on three challenging robotic manipulation tasks provided by OpenAI Gym. In our experiments, we combined our method with Deep Deterministic Policy Gradient (DDPG) with and without Hindsight Experience Replay (HER). The empirical results show that our proposed method significantly outperforms vanilla RL algorithms both in sample-efficiency and learning performance.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/071de005741bd666c7a9ccf40d5ed1d502f5282b.pdf",
      "citation_key": "li2019tj1",
      "metadata": {
        "title": "Curiosity-Driven Exploration for Off-Policy Reinforcement Learning Methods*",
        "authors": [
          "Boyao Li",
          "Tao Lu",
          "Jiayi Li",
          "N. Lu",
          "Yinghao Cai",
          "Shuo Wang"
        ],
        "published_date": "2019",
        "abstract": "Deep reinforcement learning (DRL) has achieved remarkable results in many high-dimensional continuous control tasks. However, the RL agent still explores the environment randomly, resulting in low exploration efficiency and learning performance, especially in robotic manipulation tasks with sparse rewards. To address this problem, in this paper, we intro-duce a simplified Intrinsic Curiosity Module (S-ICM) into the off-policy RL methods to encourage the agent to pursue novel and surprising states for improving the exploration competence. This method can be combined with an arbitrary off-policy RL algorithm. We evaluate our approach on three challenging robotic manipulation tasks provided by OpenAI Gym. In our experiments, we combined our method with Deep Deterministic Policy Gradient (DDPG) with and without Hindsight Experience Replay (HER). The empirical results show that our proposed method significantly outperforms vanilla RL algorithms both in sample-efficiency and learning performance.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/071de005741bd666c7a9ccf40d5ed1d502f5282b.pdf",
        "venue": "IEEE International Conference on Robotics and Biomimetics",
        "citationCount": 14,
        "score": 2.333333333333333,
        "summary": "Deep reinforcement learning (DRL) has achieved remarkable results in many high-dimensional continuous control tasks. However, the RL agent still explores the environment randomly, resulting in low exploration efficiency and learning performance, especially in robotic manipulation tasks with sparse rewards. To address this problem, in this paper, we intro-duce a simplified Intrinsic Curiosity Module (S-ICM) into the off-policy RL methods to encourage the agent to pursue novel and surprising states for improving the exploration competence. This method can be combined with an arbitrary off-policy RL algorithm. We evaluate our approach on three challenging robotic manipulation tasks provided by OpenAI Gym. In our experiments, we combined our method with Deep Deterministic Policy Gradient (DDPG) with and without Hindsight Experience Replay (HER). The empirical results show that our proposed method significantly outperforms vanilla RL algorithms both in sample-efficiency and learning performance.",
        "keywords": []
      },
      "file_name": "071de005741bd666c7a9ccf40d5ed1d502f5282b.pdf"
    },
    {
      "success": true,
      "doc_id": "dc7e1f5db47c03484584f9e6fcd2cf74",
      "summary": "Despite the remarkable progress made by the policy gradient algorithms in reinforcement learning (RL), sub-optimal policies usually result from the local exploration property of the policy gradient update. In this work, we propose a method referred to as Zeroth-Order Supervised Policy Improvement (ZOSPI) that exploits the estimated value function Q globally while preserves the local exploitation of the policy gradient methods. We prove that with a good function structure, the zeroth-order optimization strategy combining both local and global samplings can find the global minima within a polynomial number of samples. To improve the exploration efficiency in unknown environments, ZOSPI is further combined with bootstrapped Q networks. Different from the standard policy gradient methods, the policy learning of ZOSPI is conducted in a self-supervision manner so that the policy can be implemented with gradient-free non-parametric models besides the neural network approximator. Experiments show that ZOSPI achieves competitive results on MuJoCo locomotion tasks with a remarkable sample efficiency.",
      "intriguing_abstract": "Despite the remarkable progress made by the policy gradient algorithms in reinforcement learning (RL), sub-optimal policies usually result from the local exploration property of the policy gradient update. In this work, we propose a method referred to as Zeroth-Order Supervised Policy Improvement (ZOSPI) that exploits the estimated value function Q globally while preserves the local exploitation of the policy gradient methods. We prove that with a good function structure, the zeroth-order optimization strategy combining both local and global samplings can find the global minima within a polynomial number of samples. To improve the exploration efficiency in unknown environments, ZOSPI is further combined with bootstrapped Q networks. Different from the standard policy gradient methods, the policy learning of ZOSPI is conducted in a self-supervision manner so that the policy can be implemented with gradient-free non-parametric models besides the neural network approximator. Experiments show that ZOSPI achieves competitive results on MuJoCo locomotion tasks with a remarkable sample efficiency.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/3f673101c2cac3b47639056e2988e018546c3c90.pdf",
      "citation_key": "sun2020c1p",
      "metadata": {
        "title": "Zeroth-Order Supervised Policy Improvement",
        "authors": [
          "Hao Sun",
          "Ziping Xu",
          "Yuhang Song",
          "Meng Fang",
          "Jiechao Xiong",
          "Bo Dai",
          "Zhengyou Zhang",
          "Bolei Zhou"
        ],
        "published_date": "2020",
        "abstract": "Despite the remarkable progress made by the policy gradient algorithms in reinforcement learning (RL), sub-optimal policies usually result from the local exploration property of the policy gradient update. In this work, we propose a method referred to as Zeroth-Order Supervised Policy Improvement (ZOSPI) that exploits the estimated value function Q globally while preserves the local exploitation of the policy gradient methods. We prove that with a good function structure, the zeroth-order optimization strategy combining both local and global samplings can find the global minima within a polynomial number of samples. To improve the exploration efficiency in unknown environments, ZOSPI is further combined with bootstrapped Q networks. Different from the standard policy gradient methods, the policy learning of ZOSPI is conducted in a self-supervision manner so that the policy can be implemented with gradient-free non-parametric models besides the neural network approximator. Experiments show that ZOSPI achieves competitive results on MuJoCo locomotion tasks with a remarkable sample efficiency.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/3f673101c2cac3b47639056e2988e018546c3c90.pdf",
        "venue": "arXiv.org",
        "citationCount": 10,
        "score": 2.0,
        "summary": "Despite the remarkable progress made by the policy gradient algorithms in reinforcement learning (RL), sub-optimal policies usually result from the local exploration property of the policy gradient update. In this work, we propose a method referred to as Zeroth-Order Supervised Policy Improvement (ZOSPI) that exploits the estimated value function Q globally while preserves the local exploitation of the policy gradient methods. We prove that with a good function structure, the zeroth-order optimization strategy combining both local and global samplings can find the global minima within a polynomial number of samples. To improve the exploration efficiency in unknown environments, ZOSPI is further combined with bootstrapped Q networks. Different from the standard policy gradient methods, the policy learning of ZOSPI is conducted in a self-supervision manner so that the policy can be implemented with gradient-free non-parametric models besides the neural network approximator. Experiments show that ZOSPI achieves competitive results on MuJoCo locomotion tasks with a remarkable sample efficiency.",
        "keywords": []
      },
      "file_name": "3f673101c2cac3b47639056e2988e018546c3c90.pdf"
    },
    {
      "success": true,
      "doc_id": "33600511082a4d6587bba850e66b911c",
      "summary": "Stylistic response generation is crucial for building an engaging dialogue system for industrial use. While it has attracted much research interest, existing methods often generate stylistic responses at the cost of the content quality (relevance and fluency). To enable better balance between the content quality and the style, we introduce a new training strategy, know as Information-Guided Reinforcement Learning (IG-RL). In IG-RL, a training model is encouraged to explore stylistic expressions while being constrained to maintain its content quality. This is achieved by adopting reinforcement learning strategy with statistical style information guidance for quality-preserving explorations. Experiments on two datasets show that the proposed approach outperforms several strong baselines in terms of the overall response performance.",
      "intriguing_abstract": "Stylistic response generation is crucial for building an engaging dialogue system for industrial use. While it has attracted much research interest, existing methods often generate stylistic responses at the cost of the content quality (relevance and fluency). To enable better balance between the content quality and the style, we introduce a new training strategy, know as Information-Guided Reinforcement Learning (IG-RL). In IG-RL, a training model is encouraged to explore stylistic expressions while being constrained to maintain its content quality. This is achieved by adopting reinforcement learning strategy with statistical style information guidance for quality-preserving explorations. Experiments on two datasets show that the proposed approach outperforms several strong baselines in terms of the overall response performance.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/24107405a96a53d4c292b08608300a6c7e457ffe.pdf",
      "citation_key": "su2020k2m",
      "metadata": {
        "title": "Stylistic Dialogue Generation via Information-Guided Reinforcement Learning Strategy",
        "authors": [
          "Yixuan Su",
          "Deng Cai",
          "Yan Wang",
          "Simon Baker",
          "A. Korhonen",
          "Nigel Collier",
          "Xiaojiang Liu"
        ],
        "published_date": "2020",
        "abstract": "Stylistic response generation is crucial for building an engaging dialogue system for industrial use. While it has attracted much research interest, existing methods often generate stylistic responses at the cost of the content quality (relevance and fluency). To enable better balance between the content quality and the style, we introduce a new training strategy, know as Information-Guided Reinforcement Learning (IG-RL). In IG-RL, a training model is encouraged to explore stylistic expressions while being constrained to maintain its content quality. This is achieved by adopting reinforcement learning strategy with statistical style information guidance for quality-preserving explorations. Experiments on two datasets show that the proposed approach outperforms several strong baselines in terms of the overall response performance.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/24107405a96a53d4c292b08608300a6c7e457ffe.pdf",
        "venue": "arXiv.org",
        "citationCount": 10,
        "score": 2.0,
        "summary": "Stylistic response generation is crucial for building an engaging dialogue system for industrial use. While it has attracted much research interest, existing methods often generate stylistic responses at the cost of the content quality (relevance and fluency). To enable better balance between the content quality and the style, we introduce a new training strategy, know as Information-Guided Reinforcement Learning (IG-RL). In IG-RL, a training model is encouraged to explore stylistic expressions while being constrained to maintain its content quality. This is achieved by adopting reinforcement learning strategy with statistical style information guidance for quality-preserving explorations. Experiments on two datasets show that the proposed approach outperforms several strong baselines in terms of the overall response performance.",
        "keywords": []
      },
      "file_name": "24107405a96a53d4c292b08608300a6c7e457ffe.pdf"
    },
    {
      "success": true,
      "doc_id": "09afd36152ecc83582a2150bf2353e9c",
      "summary": "Multi-agent reinforcement learning (MARL) for cooperative tasks has been extensively studied in recent years. The balance of exploration and exploitation is crucial to MARL algorithms performance in terms of the learning speed and the quality of the obtained strategy. To this end, we propose an algorithm known as the weighted relative frequency of obtaining the maximal reward (WRFMR), which uses a weight parameter and the action probability to balance exploration and exploitation and accelerate convergence to the optimal joint action. For the WRFMR algorithm, each agent needs to share the state and the immediate reward and does not need to observe the actions of the other agents. Theoretical analysis on the model of WRFMR in cooperative repeated games shows that each optimal joint action is an asymptotically stable critical point if the component action of every optimal joint action is unique. The box-pushing task, the distributed sensor network (DSN) task, and a strategy game known as blood battlefield are used for empirical studies. Both the DSN task and the box-pushing task involve full cooperation, while blood battle comprises both cooperation and competition. The simulation results show that the WRFMR algorithm outperforms the other algorithms regarding the success rate and the learning speed.",
      "intriguing_abstract": "Multi-agent reinforcement learning (MARL) for cooperative tasks has been extensively studied in recent years. The balance of exploration and exploitation is crucial to MARL algorithms performance in terms of the learning speed and the quality of the obtained strategy. To this end, we propose an algorithm known as the weighted relative frequency of obtaining the maximal reward (WRFMR), which uses a weight parameter and the action probability to balance exploration and exploitation and accelerate convergence to the optimal joint action. For the WRFMR algorithm, each agent needs to share the state and the immediate reward and does not need to observe the actions of the other agents. Theoretical analysis on the model of WRFMR in cooperative repeated games shows that each optimal joint action is an asymptotically stable critical point if the component action of every optimal joint action is unique. The box-pushing task, the distributed sensor network (DSN) task, and a strategy game known as blood battlefield are used for empirical studies. Both the DSN task and the box-pushing task involve full cooperation, while blood battle comprises both cooperation and competition. The simulation results show that the WRFMR algorithm outperforms the other algorithms regarding the success rate and the learning speed.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/57f2811d8d8da81f2b4ed80097f5e47a49d76d19.pdf",
      "citation_key": "liu2020o0c",
      "metadata": {
        "title": "WRFMR: A Multi-Agent Reinforcement Learning Method for Cooperative Tasks",
        "authors": [
          "Hui Liu",
          "Zhen Zhang",
          "Dongqing Wang"
        ],
        "published_date": "2020",
        "abstract": "Multi-agent reinforcement learning (MARL) for cooperative tasks has been extensively studied in recent years. The balance of exploration and exploitation is crucial to MARL algorithms performance in terms of the learning speed and the quality of the obtained strategy. To this end, we propose an algorithm known as the weighted relative frequency of obtaining the maximal reward (WRFMR), which uses a weight parameter and the action probability to balance exploration and exploitation and accelerate convergence to the optimal joint action. For the WRFMR algorithm, each agent needs to share the state and the immediate reward and does not need to observe the actions of the other agents. Theoretical analysis on the model of WRFMR in cooperative repeated games shows that each optimal joint action is an asymptotically stable critical point if the component action of every optimal joint action is unique. The box-pushing task, the distributed sensor network (DSN) task, and a strategy game known as blood battlefield are used for empirical studies. Both the DSN task and the box-pushing task involve full cooperation, while blood battle comprises both cooperation and competition. The simulation results show that the WRFMR algorithm outperforms the other algorithms regarding the success rate and the learning speed.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/57f2811d8d8da81f2b4ed80097f5e47a49d76d19.pdf",
        "venue": "IEEE Access",
        "citationCount": 10,
        "score": 2.0,
        "summary": "Multi-agent reinforcement learning (MARL) for cooperative tasks has been extensively studied in recent years. The balance of exploration and exploitation is crucial to MARL algorithms performance in terms of the learning speed and the quality of the obtained strategy. To this end, we propose an algorithm known as the weighted relative frequency of obtaining the maximal reward (WRFMR), which uses a weight parameter and the action probability to balance exploration and exploitation and accelerate convergence to the optimal joint action. For the WRFMR algorithm, each agent needs to share the state and the immediate reward and does not need to observe the actions of the other agents. Theoretical analysis on the model of WRFMR in cooperative repeated games shows that each optimal joint action is an asymptotically stable critical point if the component action of every optimal joint action is unique. The box-pushing task, the distributed sensor network (DSN) task, and a strategy game known as blood battlefield are used for empirical studies. Both the DSN task and the box-pushing task involve full cooperation, while blood battle comprises both cooperation and competition. The simulation results show that the WRFMR algorithm outperforms the other algorithms regarding the success rate and the learning speed.",
        "keywords": []
      },
      "file_name": "57f2811d8d8da81f2b4ed80097f5e47a49d76d19.pdf"
    },
    {
      "success": true,
      "doc_id": "d4c20051b42273bc86b88d67950d631b",
      "summary": "Sample efficiency and exploration remain major challenges in online reinforcement learning (RL). A powerful approach that can be applied to address these issues is the inclusion of offline data, such as prior trajectories from a human expert or a sub-optimal exploration policy. Previous methods have relied on extensive modifications and additional complexity to ensure the effective use of this data. Instead, we ask: can we simply apply existing off-policy methods to leverage offline data when learning online? In this work, we demonstrate that the answer is yes; however, a set of minimal but important changes to existing off-policy RL algorithms are required to achieve reliable performance. We extensively ablate these design choices, demonstrating the key factors that most affect performance, and arrive at a set of recommendations that practitioners can readily apply, whether their data comprise a small number of expert demonstrations or large volumes of sub-optimal trajectories. We see that correct application of these simple recommendations can provide a $\\mathbf{2.5\\times}$ improvement over existing approaches across a diverse set of competitive benchmarks, with no additional computational overhead. We have released our code at https://github.com/ikostrikov/rlpd.",
      "intriguing_abstract": "Sample efficiency and exploration remain major challenges in online reinforcement learning (RL). A powerful approach that can be applied to address these issues is the inclusion of offline data, such as prior trajectories from a human expert or a sub-optimal exploration policy. Previous methods have relied on extensive modifications and additional complexity to ensure the effective use of this data. Instead, we ask: can we simply apply existing off-policy methods to leverage offline data when learning online? In this work, we demonstrate that the answer is yes; however, a set of minimal but important changes to existing off-policy RL algorithms are required to achieve reliable performance. We extensively ablate these design choices, demonstrating the key factors that most affect performance, and arrive at a set of recommendations that practitioners can readily apply, whether their data comprise a small number of expert demonstrations or large volumes of sub-optimal trajectories. We see that correct application of these simple recommendations can provide a $\\mathbf{2.5\\times}$ improvement over existing approaches across a diverse set of competitive benchmarks, with no additional computational overhead. We have released our code at https://github.com/ikostrikov/rlpd.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/bd38cbbb346a347cb5b60ac4a133b3d73cb44e07.pdf",
      "citation_key": "ball20235zm",
      "metadata": {
        "title": "Efficient Online Reinforcement Learning with Offline Data",
        "authors": [
          "Philip J. Ball",
          "Laura M. Smith",
          "Ilya Kostrikov",
          "S. Levine"
        ],
        "published_date": "2023",
        "abstract": "Sample efficiency and exploration remain major challenges in online reinforcement learning (RL). A powerful approach that can be applied to address these issues is the inclusion of offline data, such as prior trajectories from a human expert or a sub-optimal exploration policy. Previous methods have relied on extensive modifications and additional complexity to ensure the effective use of this data. Instead, we ask: can we simply apply existing off-policy methods to leverage offline data when learning online? In this work, we demonstrate that the answer is yes; however, a set of minimal but important changes to existing off-policy RL algorithms are required to achieve reliable performance. We extensively ablate these design choices, demonstrating the key factors that most affect performance, and arrive at a set of recommendations that practitioners can readily apply, whether their data comprise a small number of expert demonstrations or large volumes of sub-optimal trajectories. We see that correct application of these simple recommendations can provide a $\\mathbf{2.5\\times}$ improvement over existing approaches across a diverse set of competitive benchmarks, with no additional computational overhead. We have released our code at https://github.com/ikostrikov/rlpd.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/bd38cbbb346a347cb5b60ac4a133b3d73cb44e07.pdf",
        "venue": "International Conference on Machine Learning",
        "citationCount": 220,
        "score": 110.0,
        "summary": "Sample efficiency and exploration remain major challenges in online reinforcement learning (RL). A powerful approach that can be applied to address these issues is the inclusion of offline data, such as prior trajectories from a human expert or a sub-optimal exploration policy. Previous methods have relied on extensive modifications and additional complexity to ensure the effective use of this data. Instead, we ask: can we simply apply existing off-policy methods to leverage offline data when learning online? In this work, we demonstrate that the answer is yes; however, a set of minimal but important changes to existing off-policy RL algorithms are required to achieve reliable performance. We extensively ablate these design choices, demonstrating the key factors that most affect performance, and arrive at a set of recommendations that practitioners can readily apply, whether their data comprise a small number of expert demonstrations or large volumes of sub-optimal trajectories. We see that correct application of these simple recommendations can provide a $\\mathbf{2.5\\times}$ improvement over existing approaches across a diverse set of competitive benchmarks, with no additional computational overhead. We have released our code at https://github.com/ikostrikov/rlpd.",
        "keywords": []
      },
      "file_name": "bd38cbbb346a347cb5b60ac4a133b3d73cb44e07.pdf"
    },
    {
      "success": true,
      "doc_id": "ae99c8a4e57cd58e5d83af2a4885f2ff",
      "summary": "To address the issue of significant unpredictability and intermittent nature of renewable energy sources, particularly wind and solar power, this paper introduces a novel optimization model based on online reinforcement learning. Initially, an energy management optimization model is designed to achieve plan adherence and minimize energy storage (ES) operation costs, taking into account the inherent challenges of wind power-photovoltaic energy storage systems (WPESS). An online reinforcement learning framework is employed, which defines various state variables, action variables, and reward functions for the energy management optimization model. The state-action-reward-state-action (SARSA) algorithm is applied to learn the joint scheduling strategy for the microgrid system, utilizing its iterative exploration mechanisms and interaction with the environment. This strategy aims to accomplish the goals of effective power tracking and reduction of storage charging and discharging. The proposed method's effectiveness is validated using a residential community with electric vehicle (EV) charging loads as a test case. Numerical analyses demonstrate that the approach is not reliant on traditional mathematical models and adapts well to the uncertainties and complex constraints of the WPESS, maintaining a low-cost requirement while achieving computational efficiency significantly higher than that of the model predictive control (MPC) and deep Q-network (DQN) algorithm.",
      "intriguing_abstract": "To address the issue of significant unpredictability and intermittent nature of renewable energy sources, particularly wind and solar power, this paper introduces a novel optimization model based on online reinforcement learning. Initially, an energy management optimization model is designed to achieve plan adherence and minimize energy storage (ES) operation costs, taking into account the inherent challenges of wind power-photovoltaic energy storage systems (WPESS). An online reinforcement learning framework is employed, which defines various state variables, action variables, and reward functions for the energy management optimization model. The state-action-reward-state-action (SARSA) algorithm is applied to learn the joint scheduling strategy for the microgrid system, utilizing its iterative exploration mechanisms and interaction with the environment. This strategy aims to accomplish the goals of effective power tracking and reduction of storage charging and discharging. The proposed method's effectiveness is validated using a residential community with electric vehicle (EV) charging loads as a test case. Numerical analyses demonstrate that the approach is not reliant on traditional mathematical models and adapts well to the uncertainties and complex constraints of the WPESS, maintaining a low-cost requirement while achieving computational efficiency significantly higher than that of the model predictive control (MPC) and deep Q-network (DQN) algorithm.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/1b1efa2f9731ab3801c46bfc877695d41e437406.pdf",
      "citation_key": "meng2025l1q",
      "metadata": {
        "title": "An Online Reinforcement Learning-Based Energy Management Strategy for Microgrids With Centralized Control",
        "authors": [
          "Qing-ran Meng",
          "Sheharyar Hussain",
          "Fengzhang Luo",
          "Zhongguan Wang",
          "Xiaolong Jin"
        ],
        "published_date": "2025",
        "abstract": "To address the issue of significant unpredictability and intermittent nature of renewable energy sources, particularly wind and solar power, this paper introduces a novel optimization model based on online reinforcement learning. Initially, an energy management optimization model is designed to achieve plan adherence and minimize energy storage (ES) operation costs, taking into account the inherent challenges of wind power-photovoltaic energy storage systems (WPESS). An online reinforcement learning framework is employed, which defines various state variables, action variables, and reward functions for the energy management optimization model. The state-action-reward-state-action (SARSA) algorithm is applied to learn the joint scheduling strategy for the microgrid system, utilizing its iterative exploration mechanisms and interaction with the environment. This strategy aims to accomplish the goals of effective power tracking and reduction of storage charging and discharging. The proposed method's effectiveness is validated using a residential community with electric vehicle (EV) charging loads as a test case. Numerical analyses demonstrate that the approach is not reliant on traditional mathematical models and adapts well to the uncertainties and complex constraints of the WPESS, maintaining a low-cost requirement while achieving computational efficiency significantly higher than that of the model predictive control (MPC) and deep Q-network (DQN) algorithm.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/1b1efa2f9731ab3801c46bfc877695d41e437406.pdf",
        "venue": "IEEE transactions on industry applications",
        "citationCount": 62,
        "score": 62.0,
        "summary": "To address the issue of significant unpredictability and intermittent nature of renewable energy sources, particularly wind and solar power, this paper introduces a novel optimization model based on online reinforcement learning. Initially, an energy management optimization model is designed to achieve plan adherence and minimize energy storage (ES) operation costs, taking into account the inherent challenges of wind power-photovoltaic energy storage systems (WPESS). An online reinforcement learning framework is employed, which defines various state variables, action variables, and reward functions for the energy management optimization model. The state-action-reward-state-action (SARSA) algorithm is applied to learn the joint scheduling strategy for the microgrid system, utilizing its iterative exploration mechanisms and interaction with the environment. This strategy aims to accomplish the goals of effective power tracking and reduction of storage charging and discharging. The proposed method's effectiveness is validated using a residential community with electric vehicle (EV) charging loads as a test case. Numerical analyses demonstrate that the approach is not reliant on traditional mathematical models and adapts well to the uncertainties and complex constraints of the WPESS, maintaining a low-cost requirement while achieving computational efficiency significantly higher than that of the model predictive control (MPC) and deep Q-network (DQN) algorithm.",
        "keywords": []
      },
      "file_name": "1b1efa2f9731ab3801c46bfc877695d41e437406.pdf"
    },
    {
      "success": true,
      "doc_id": "0682090167765ea5c3cd3aca99483d84",
      "summary": "The advancement of large language models (LLMs) has significantly propelled the field of code generation. Previous work integrated reinforcement learning (RL) with compiler feedback for exploring the output space of LLMs to enhance code generation quality. However, the lengthy code generated by LLMs in response to complex human requirements makes RL exploration a challenge. Also, since the unit tests may not cover the complicated code, optimizing LLMs by using these unexecuted code snippets is ineffective. To tackle these challenges, we introduce StepCoder, a novel RL framework for code generation, consisting of two main components: CCCS addresses the exploration challenge by breaking the long sequences code generation task into a Curriculum of Code Completion Subtasks, while FGO only optimizes the model by masking the unexecuted code segments to provide Fine-Grained Optimization. In addition, we furthermore construct the APPS+ dataset for RL training, which is manually verified to ensure the correctness of unit tests. Experimental results show that our method improves the ability to explore the output space and outperforms state-of-the-art approaches in corresponding benchmarks. Our dataset APPS+ and StepCoder are available online.",
      "intriguing_abstract": "The advancement of large language models (LLMs) has significantly propelled the field of code generation. Previous work integrated reinforcement learning (RL) with compiler feedback for exploring the output space of LLMs to enhance code generation quality. However, the lengthy code generated by LLMs in response to complex human requirements makes RL exploration a challenge. Also, since the unit tests may not cover the complicated code, optimizing LLMs by using these unexecuted code snippets is ineffective. To tackle these challenges, we introduce StepCoder, a novel RL framework for code generation, consisting of two main components: CCCS addresses the exploration challenge by breaking the long sequences code generation task into a Curriculum of Code Completion Subtasks, while FGO only optimizes the model by masking the unexecuted code segments to provide Fine-Grained Optimization. In addition, we furthermore construct the APPS+ dataset for RL training, which is manually verified to ensure the correctness of unit tests. Experimental results show that our method improves the ability to explore the output space and outperforms state-of-the-art approaches in corresponding benchmarks. Our dataset APPS+ and StepCoder are available online.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/08e84c939b88fc50aaa74ef76e202e61a1ad940b.pdf",
      "citation_key": "dou2024kjg",
      "metadata": {
        "title": "StepCoder: Improve Code Generation with Reinforcement Learning from Compiler Feedback",
        "authors": [
          "Shihan Dou",
          "Yan Liu",
          "Haoxiang Jia",
          "Limao Xiong",
          "Enyu Zhou",
          "Junjie Shan",
          "Caishuang Huang",
          "Wei Shen",
          "Xiaoran Fan",
          "Zhiheng Xi",
          "Yuhao Zhou",
          "Tao Ji",
          "Rui Zheng",
          "Qi Zhang",
          "Xuanjing Huang",
          "Tao Gui"
        ],
        "published_date": "2024",
        "abstract": "The advancement of large language models (LLMs) has significantly propelled the field of code generation. Previous work integrated reinforcement learning (RL) with compiler feedback for exploring the output space of LLMs to enhance code generation quality. However, the lengthy code generated by LLMs in response to complex human requirements makes RL exploration a challenge. Also, since the unit tests may not cover the complicated code, optimizing LLMs by using these unexecuted code snippets is ineffective. To tackle these challenges, we introduce StepCoder, a novel RL framework for code generation, consisting of two main components: CCCS addresses the exploration challenge by breaking the long sequences code generation task into a Curriculum of Code Completion Subtasks, while FGO only optimizes the model by masking the unexecuted code segments to provide Fine-Grained Optimization. In addition, we furthermore construct the APPS+ dataset for RL training, which is manually verified to ensure the correctness of unit tests. Experimental results show that our method improves the ability to explore the output space and outperforms state-of-the-art approaches in corresponding benchmarks. Our dataset APPS+ and StepCoder are available online.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/08e84c939b88fc50aaa74ef76e202e61a1ad940b.pdf",
        "venue": "arXiv.org",
        "citationCount": 61,
        "score": 61.0,
        "summary": "The advancement of large language models (LLMs) has significantly propelled the field of code generation. Previous work integrated reinforcement learning (RL) with compiler feedback for exploring the output space of LLMs to enhance code generation quality. However, the lengthy code generated by LLMs in response to complex human requirements makes RL exploration a challenge. Also, since the unit tests may not cover the complicated code, optimizing LLMs by using these unexecuted code snippets is ineffective. To tackle these challenges, we introduce StepCoder, a novel RL framework for code generation, consisting of two main components: CCCS addresses the exploration challenge by breaking the long sequences code generation task into a Curriculum of Code Completion Subtasks, while FGO only optimizes the model by masking the unexecuted code segments to provide Fine-Grained Optimization. In addition, we furthermore construct the APPS+ dataset for RL training, which is manually verified to ensure the correctness of unit tests. Experimental results show that our method improves the ability to explore the output space and outperforms state-of-the-art approaches in corresponding benchmarks. Our dataset APPS+ and StepCoder are available online.",
        "keywords": []
      },
      "file_name": "08e84c939b88fc50aaa74ef76e202e61a1ad940b.pdf"
    },
    {
      "success": true,
      "doc_id": "4ba1c1d50829c33ccb0089271856109a",
      "summary": "Large transformer models trained on diverse datasets have shown a remarkable ability to learn in-context, achieving high few-shot performance on tasks they were not explicitly trained to solve. In this paper, we study the in-context learning capabilities of transformers in decision-making problems, i.e., reinforcement learning (RL) for bandits and Markov decision processes. To do so, we introduce and study Decision-Pretrained Transformer (DPT), a supervised pretraining method where the transformer predicts an optimal action given a query state and an in-context dataset of interactions, across a diverse set of tasks. This procedure, while simple, produces a model with several surprising capabilities. We find that the pretrained transformer can be used to solve a range of RL problems in-context, exhibiting both exploration online and conservatism offline, despite not being explicitly trained to do so. The model also generalizes beyond the pretraining distribution to new tasks and automatically adapts its decision-making strategies to unknown structure. Theoretically, we show DPT can be viewed as an efficient implementation of Bayesian posterior sampling, a provably sample-efficient RL algorithm. We further leverage this connection to provide guarantees on the regret of the in-context algorithm yielded by DPT, and prove that it can learn faster than algorithms used to generate the pretraining data. These results suggest a promising yet simple path towards instilling strong in-context decision-making abilities in transformers.",
      "intriguing_abstract": "Large transformer models trained on diverse datasets have shown a remarkable ability to learn in-context, achieving high few-shot performance on tasks they were not explicitly trained to solve. In this paper, we study the in-context learning capabilities of transformers in decision-making problems, i.e., reinforcement learning (RL) for bandits and Markov decision processes. To do so, we introduce and study Decision-Pretrained Transformer (DPT), a supervised pretraining method where the transformer predicts an optimal action given a query state and an in-context dataset of interactions, across a diverse set of tasks. This procedure, while simple, produces a model with several surprising capabilities. We find that the pretrained transformer can be used to solve a range of RL problems in-context, exhibiting both exploration online and conservatism offline, despite not being explicitly trained to do so. The model also generalizes beyond the pretraining distribution to new tasks and automatically adapts its decision-making strategies to unknown structure. Theoretically, we show DPT can be viewed as an efficient implementation of Bayesian posterior sampling, a provably sample-efficient RL algorithm. We further leverage this connection to provide guarantees on the regret of the in-context algorithm yielded by DPT, and prove that it can learn faster than algorithms used to generate the pretraining data. These results suggest a promising yet simple path towards instilling strong in-context decision-making abilities in transformers.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/5bac7d00035bc1e246a34f9ee3152b290f97bb92.pdf",
      "citation_key": "lee202337c",
      "metadata": {
        "title": "Supervised Pretraining Can Learn In-Context Reinforcement Learning",
        "authors": [
          "Jonathan Lee",
          "Annie Xie",
          "Aldo Pacchiano",
          "Yash Chandak",
          "Chelsea Finn",
          "Ofir Nachum",
          "E. Brunskill"
        ],
        "published_date": "2023",
        "abstract": "Large transformer models trained on diverse datasets have shown a remarkable ability to learn in-context, achieving high few-shot performance on tasks they were not explicitly trained to solve. In this paper, we study the in-context learning capabilities of transformers in decision-making problems, i.e., reinforcement learning (RL) for bandits and Markov decision processes. To do so, we introduce and study Decision-Pretrained Transformer (DPT), a supervised pretraining method where the transformer predicts an optimal action given a query state and an in-context dataset of interactions, across a diverse set of tasks. This procedure, while simple, produces a model with several surprising capabilities. We find that the pretrained transformer can be used to solve a range of RL problems in-context, exhibiting both exploration online and conservatism offline, despite not being explicitly trained to do so. The model also generalizes beyond the pretraining distribution to new tasks and automatically adapts its decision-making strategies to unknown structure. Theoretically, we show DPT can be viewed as an efficient implementation of Bayesian posterior sampling, a provably sample-efficient RL algorithm. We further leverage this connection to provide guarantees on the regret of the in-context algorithm yielded by DPT, and prove that it can learn faster than algorithms used to generate the pretraining data. These results suggest a promising yet simple path towards instilling strong in-context decision-making abilities in transformers.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/5bac7d00035bc1e246a34f9ee3152b290f97bb92.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 110,
        "score": 55.0,
        "summary": "Large transformer models trained on diverse datasets have shown a remarkable ability to learn in-context, achieving high few-shot performance on tasks they were not explicitly trained to solve. In this paper, we study the in-context learning capabilities of transformers in decision-making problems, i.e., reinforcement learning (RL) for bandits and Markov decision processes. To do so, we introduce and study Decision-Pretrained Transformer (DPT), a supervised pretraining method where the transformer predicts an optimal action given a query state and an in-context dataset of interactions, across a diverse set of tasks. This procedure, while simple, produces a model with several surprising capabilities. We find that the pretrained transformer can be used to solve a range of RL problems in-context, exhibiting both exploration online and conservatism offline, despite not being explicitly trained to do so. The model also generalizes beyond the pretraining distribution to new tasks and automatically adapts its decision-making strategies to unknown structure. Theoretically, we show DPT can be viewed as an efficient implementation of Bayesian posterior sampling, a provably sample-efficient RL algorithm. We further leverage this connection to provide guarantees on the regret of the in-context algorithm yielded by DPT, and prove that it can learn faster than algorithms used to generate the pretraining data. These results suggest a promising yet simple path towards instilling strong in-context decision-making abilities in transformers.",
        "keywords": []
      },
      "file_name": "5bac7d00035bc1e246a34f9ee3152b290f97bb92.pdf"
    },
    {
      "success": true,
      "doc_id": "95460228e607dcc30c0b53198ee29586",
      "summary": "Autonomous underwater vehicles (AUVs) have become important tools in the ocean exploration and have drawn considerable attention. Precise control for AUVs is the prerequisite to effectively execute underwater tasks. However, the classical control methods such as model predictive control (MPC) rely heavily on the dynamics model of the controlled system which is difficult to obtain for AUVs. To address this issue, a new reinforcement learning (RL) framework for AUV path-following control is proposed in this article. Specifically, we propose a novel actor-model-critic (AMC) architecture integrating a neural network model with the traditional actor-critic architecture. The neural network model is designed to learn the state transition function to explore the spatio-temporal change patterns of the AUV as well as the surrounding environment. Based on the AMC architecture, a RL-based controller agent named ModelPPO is constructed to control the AUV. With the required sailing speed achieved by a traditional proportional-integral (PI) controller, ModelPPO can control the rudder and elevator fins so that the AUV follows the desired path. Finally, a simulation platform is built to evaluate the performance of the proposed method that is compared with MPC and other RL-based methods. The obtained results demonstrate that the proposed method can achieve better performance than other methods, which demonstrate the great potential of the advanced artificial intelligence methods in solving the traditional motion control problems for intelligent vehicles.",
      "intriguing_abstract": "Autonomous underwater vehicles (AUVs) have become important tools in the ocean exploration and have drawn considerable attention. Precise control for AUVs is the prerequisite to effectively execute underwater tasks. However, the classical control methods such as model predictive control (MPC) rely heavily on the dynamics model of the controlled system which is difficult to obtain for AUVs. To address this issue, a new reinforcement learning (RL) framework for AUV path-following control is proposed in this article. Specifically, we propose a novel actor-model-critic (AMC) architecture integrating a neural network model with the traditional actor-critic architecture. The neural network model is designed to learn the state transition function to explore the spatio-temporal change patterns of the AUV as well as the surrounding environment. Based on the AMC architecture, a RL-based controller agent named ModelPPO is constructed to control the AUV. With the required sailing speed achieved by a traditional proportional-integral (PI) controller, ModelPPO can control the rudder and elevator fins so that the AUV follows the desired path. Finally, a simulation platform is built to evaluate the performance of the proposed method that is compared with MPC and other RL-based methods. The obtained results demonstrate that the proposed method can achieve better performance than other methods, which demonstrate the great potential of the advanced artificial intelligence methods in solving the traditional motion control problems for intelligent vehicles.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/043582a1ed2d2b7d08a804bafe9db188e9a65d96.pdf",
      "citation_key": "ma2024r2p",
      "metadata": {
        "title": "Neural Network Model-Based Reinforcement Learning Control for AUV 3-D Path Following",
        "authors": [
          "D. Ma",
          "Xi Chen",
          "Weihao Ma",
          "Huarong Zheng",
          "Fengzhong Qu"
        ],
        "published_date": "2024",
        "abstract": "Autonomous underwater vehicles (AUVs) have become important tools in the ocean exploration and have drawn considerable attention. Precise control for AUVs is the prerequisite to effectively execute underwater tasks. However, the classical control methods such as model predictive control (MPC) rely heavily on the dynamics model of the controlled system which is difficult to obtain for AUVs. To address this issue, a new reinforcement learning (RL) framework for AUV path-following control is proposed in this article. Specifically, we propose a novel actor-model-critic (AMC) architecture integrating a neural network model with the traditional actor-critic architecture. The neural network model is designed to learn the state transition function to explore the spatio-temporal change patterns of the AUV as well as the surrounding environment. Based on the AMC architecture, a RL-based controller agent named ModelPPO is constructed to control the AUV. With the required sailing speed achieved by a traditional proportional-integral (PI) controller, ModelPPO can control the rudder and elevator fins so that the AUV follows the desired path. Finally, a simulation platform is built to evaluate the performance of the proposed method that is compared with MPC and other RL-based methods. The obtained results demonstrate that the proposed method can achieve better performance than other methods, which demonstrate the great potential of the advanced artificial intelligence methods in solving the traditional motion control problems for intelligent vehicles.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/043582a1ed2d2b7d08a804bafe9db188e9a65d96.pdf",
        "venue": "IEEE Transactions on Intelligent Vehicles",
        "citationCount": 51,
        "score": 51.0,
        "summary": "Autonomous underwater vehicles (AUVs) have become important tools in the ocean exploration and have drawn considerable attention. Precise control for AUVs is the prerequisite to effectively execute underwater tasks. However, the classical control methods such as model predictive control (MPC) rely heavily on the dynamics model of the controlled system which is difficult to obtain for AUVs. To address this issue, a new reinforcement learning (RL) framework for AUV path-following control is proposed in this article. Specifically, we propose a novel actor-model-critic (AMC) architecture integrating a neural network model with the traditional actor-critic architecture. The neural network model is designed to learn the state transition function to explore the spatio-temporal change patterns of the AUV as well as the surrounding environment. Based on the AMC architecture, a RL-based controller agent named ModelPPO is constructed to control the AUV. With the required sailing speed achieved by a traditional proportional-integral (PI) controller, ModelPPO can control the rudder and elevator fins so that the AUV follows the desired path. Finally, a simulation platform is built to evaluate the performance of the proposed method that is compared with MPC and other RL-based methods. The obtained results demonstrate that the proposed method can achieve better performance than other methods, which demonstrate the great potential of the advanced artificial intelligence methods in solving the traditional motion control problems for intelligent vehicles.",
        "keywords": []
      },
      "file_name": "043582a1ed2d2b7d08a804bafe9db188e9a65d96.pdf"
    },
    {
      "success": true,
      "doc_id": "b072930ac48736b9471664c8e168d8c3",
      "summary": "Benchmarks play a crucial role in the development and analysis of reinforcement learning (RL) algorithms. We identify that existing benchmarks used for research into open-ended learning fall into one of two categories. Either they are too slow for meaningful research to be performed without enormous computational resources, like Crafter, NetHack and Minecraft, or they are not complex enough to pose a significant challenge, like Minigrid and Procgen. To remedy this, we first present Craftax-Classic: a ground-up rewrite of Crafter in JAX that runs up to 250x faster than the Python-native original. A run of PPO using 1 billion environment interactions finishes in under an hour using only a single GPU and averages 90% of the optimal reward. To provide a more compelling challenge we present the main Craftax benchmark, a significant extension of the Crafter mechanics with elements inspired from NetHack. Solving Craftax requires deep exploration, long term planning and memory, as well as continual adaptation to novel situations as more of the world is discovered. We show that existing methods including global and episodic exploration, as well as unsupervised environment design fail to make material progress on the benchmark. We believe that Craftax can for the first time allow researchers to experiment in a complex, open-ended environment with limited computational resources.",
      "intriguing_abstract": "Benchmarks play a crucial role in the development and analysis of reinforcement learning (RL) algorithms. We identify that existing benchmarks used for research into open-ended learning fall into one of two categories. Either they are too slow for meaningful research to be performed without enormous computational resources, like Crafter, NetHack and Minecraft, or they are not complex enough to pose a significant challenge, like Minigrid and Procgen. To remedy this, we first present Craftax-Classic: a ground-up rewrite of Crafter in JAX that runs up to 250x faster than the Python-native original. A run of PPO using 1 billion environment interactions finishes in under an hour using only a single GPU and averages 90% of the optimal reward. To provide a more compelling challenge we present the main Craftax benchmark, a significant extension of the Crafter mechanics with elements inspired from NetHack. Solving Craftax requires deep exploration, long term planning and memory, as well as continual adaptation to novel situations as more of the world is discovered. We show that existing methods including global and episodic exploration, as well as unsupervised environment design fail to make material progress on the benchmark. We believe that Craftax can for the first time allow researchers to experiment in a complex, open-ended environment with limited computational resources.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/139a84c6fc4887ce2374489d79af0df9e1e7e4d6.pdf",
      "citation_key": "matthews20241yx",
      "metadata": {
        "title": "Craftax: A Lightning-Fast Benchmark for Open-Ended Reinforcement Learning",
        "authors": [
          "Michael Matthews",
          "Michael Beukman",
          "Benjamin Ellis",
          "Mikayel Samvelyan",
          "Matthew Jackson",
          "Samuel Coward",
          "Jakob Foerster"
        ],
        "published_date": "2024",
        "abstract": "Benchmarks play a crucial role in the development and analysis of reinforcement learning (RL) algorithms. We identify that existing benchmarks used for research into open-ended learning fall into one of two categories. Either they are too slow for meaningful research to be performed without enormous computational resources, like Crafter, NetHack and Minecraft, or they are not complex enough to pose a significant challenge, like Minigrid and Procgen. To remedy this, we first present Craftax-Classic: a ground-up rewrite of Crafter in JAX that runs up to 250x faster than the Python-native original. A run of PPO using 1 billion environment interactions finishes in under an hour using only a single GPU and averages 90% of the optimal reward. To provide a more compelling challenge we present the main Craftax benchmark, a significant extension of the Crafter mechanics with elements inspired from NetHack. Solving Craftax requires deep exploration, long term planning and memory, as well as continual adaptation to novel situations as more of the world is discovered. We show that existing methods including global and episodic exploration, as well as unsupervised environment design fail to make material progress on the benchmark. We believe that Craftax can for the first time allow researchers to experiment in a complex, open-ended environment with limited computational resources.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/139a84c6fc4887ce2374489d79af0df9e1e7e4d6.pdf",
        "venue": "International Conference on Machine Learning",
        "citationCount": 47,
        "score": 47.0,
        "summary": "Benchmarks play a crucial role in the development and analysis of reinforcement learning (RL) algorithms. We identify that existing benchmarks used for research into open-ended learning fall into one of two categories. Either they are too slow for meaningful research to be performed without enormous computational resources, like Crafter, NetHack and Minecraft, or they are not complex enough to pose a significant challenge, like Minigrid and Procgen. To remedy this, we first present Craftax-Classic: a ground-up rewrite of Crafter in JAX that runs up to 250x faster than the Python-native original. A run of PPO using 1 billion environment interactions finishes in under an hour using only a single GPU and averages 90% of the optimal reward. To provide a more compelling challenge we present the main Craftax benchmark, a significant extension of the Crafter mechanics with elements inspired from NetHack. Solving Craftax requires deep exploration, long term planning and memory, as well as continual adaptation to novel situations as more of the world is discovered. We show that existing methods including global and episodic exploration, as well as unsupervised environment design fail to make material progress on the benchmark. We believe that Craftax can for the first time allow researchers to experiment in a complex, open-ended environment with limited computational resources.",
        "keywords": []
      },
      "file_name": "139a84c6fc4887ce2374489d79af0df9e1e7e4d6.pdf"
    },
    {
      "success": true,
      "doc_id": "9e5a1f1fa1eb651d7f5c4281b5eaf461",
      "summary": "The unmanned aerial vehicles (UAVs) are competent to perform a variety of applications, possessing great potential and promise. The deep neural networks (DNN) technology has enabled the UAV-assisted paradigm, accelerated the construction of smart cities, and propelled the development of the Internet of Things (IoT). UAVs play an increasingly important role in various applications, such as surveillance, environmental monitoring, emergency rescue, and supplies delivery, for which a robust path-planning technique is the foundation and prerequisite. However, existing methods lack comprehensive consideration of the complicated urban environment and do not provide an overall assessment of the robustness and generalization. Meanwhile, due to the resource constraints and hardware limitations of UAVs, the complexity of deploying the network needs to be reduced. This article proposes a lightweight, reinforcement-learning-based (RL-based) real-time path-planning method for UAVs, adaptive SAC (ASAC) algorithm, which optimizing the training process, network architecture, and algorithmic models. First of all, we establish a framework of global training and local adaptation, where the structured environment model is constructed for interaction, and local dynamically varying information aids in improving generalization. Second, ASAC introduces a cross-layer connection approach that passes the original state information into the higher layers to avoid feature loss and improve learning efficiency. Finally, we propose an adaptive temperature coefficient, which flexibly adjusts the exploration probability of UAVs with the training phase and experience data accumulation. In addition, a series of comparison experiments have been conducted in conjunction with practical application requirements, and the results have fully proved the favorable superiority of ASAC.",
      "intriguing_abstract": "The unmanned aerial vehicles (UAVs) are competent to perform a variety of applications, possessing great potential and promise. The deep neural networks (DNN) technology has enabled the UAV-assisted paradigm, accelerated the construction of smart cities, and propelled the development of the Internet of Things (IoT). UAVs play an increasingly important role in various applications, such as surveillance, environmental monitoring, emergency rescue, and supplies delivery, for which a robust path-planning technique is the foundation and prerequisite. However, existing methods lack comprehensive consideration of the complicated urban environment and do not provide an overall assessment of the robustness and generalization. Meanwhile, due to the resource constraints and hardware limitations of UAVs, the complexity of deploying the network needs to be reduced. This article proposes a lightweight, reinforcement-learning-based (RL-based) real-time path-planning method for UAVs, adaptive SAC (ASAC) algorithm, which optimizing the training process, network architecture, and algorithmic models. First of all, we establish a framework of global training and local adaptation, where the structured environment model is constructed for interaction, and local dynamically varying information aids in improving generalization. Second, ASAC introduces a cross-layer connection approach that passes the original state information into the higher layers to avoid feature loss and improve learning efficiency. Finally, we propose an adaptive temperature coefficient, which flexibly adjusts the exploration probability of UAVs with the training phase and experience data accumulation. In addition, a series of comparison experiments have been conducted in conjunction with practical application requirements, and the results have fully proved the favorable superiority of ASAC.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/26662adf92cacf0810a14faa514360f270e97b53.pdf",
      "citation_key": "xi2024e2i",
      "metadata": {
        "title": "A Lightweight Reinforcement-Learning-Based Real-Time Path-Planning Method for Unmanned Aerial Vehicles",
        "authors": [
          "Meng Xi",
          "Huiao Dai",
          "Jingyi He",
          "Wenjie Li",
          "Jiabao Wen",
          "Shuai Xiao",
          "Jiachen Yang"
        ],
        "published_date": "2024",
        "abstract": "The unmanned aerial vehicles (UAVs) are competent to perform a variety of applications, possessing great potential and promise. The deep neural networks (DNN) technology has enabled the UAV-assisted paradigm, accelerated the construction of smart cities, and propelled the development of the Internet of Things (IoT). UAVs play an increasingly important role in various applications, such as surveillance, environmental monitoring, emergency rescue, and supplies delivery, for which a robust path-planning technique is the foundation and prerequisite. However, existing methods lack comprehensive consideration of the complicated urban environment and do not provide an overall assessment of the robustness and generalization. Meanwhile, due to the resource constraints and hardware limitations of UAVs, the complexity of deploying the network needs to be reduced. This article proposes a lightweight, reinforcement-learning-based (RL-based) real-time path-planning method for UAVs, adaptive SAC (ASAC) algorithm, which optimizing the training process, network architecture, and algorithmic models. First of all, we establish a framework of global training and local adaptation, where the structured environment model is constructed for interaction, and local dynamically varying information aids in improving generalization. Second, ASAC introduces a cross-layer connection approach that passes the original state information into the higher layers to avoid feature loss and improve learning efficiency. Finally, we propose an adaptive temperature coefficient, which flexibly adjusts the exploration probability of UAVs with the training phase and experience data accumulation. In addition, a series of comparison experiments have been conducted in conjunction with practical application requirements, and the results have fully proved the favorable superiority of ASAC.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/26662adf92cacf0810a14faa514360f270e97b53.pdf",
        "venue": "IEEE Internet of Things Journal",
        "citationCount": 44,
        "score": 44.0,
        "summary": "The unmanned aerial vehicles (UAVs) are competent to perform a variety of applications, possessing great potential and promise. The deep neural networks (DNN) technology has enabled the UAV-assisted paradigm, accelerated the construction of smart cities, and propelled the development of the Internet of Things (IoT). UAVs play an increasingly important role in various applications, such as surveillance, environmental monitoring, emergency rescue, and supplies delivery, for which a robust path-planning technique is the foundation and prerequisite. However, existing methods lack comprehensive consideration of the complicated urban environment and do not provide an overall assessment of the robustness and generalization. Meanwhile, due to the resource constraints and hardware limitations of UAVs, the complexity of deploying the network needs to be reduced. This article proposes a lightweight, reinforcement-learning-based (RL-based) real-time path-planning method for UAVs, adaptive SAC (ASAC) algorithm, which optimizing the training process, network architecture, and algorithmic models. First of all, we establish a framework of global training and local adaptation, where the structured environment model is constructed for interaction, and local dynamically varying information aids in improving generalization. Second, ASAC introduces a cross-layer connection approach that passes the original state information into the higher layers to avoid feature loss and improve learning efficiency. Finally, we propose an adaptive temperature coefficient, which flexibly adjusts the exploration probability of UAVs with the training phase and experience data accumulation. In addition, a series of comparison experiments have been conducted in conjunction with practical application requirements, and the results have fully proved the favorable superiority of ASAC.",
        "keywords": []
      },
      "file_name": "26662adf92cacf0810a14faa514360f270e97b53.pdf"
    },
    {
      "success": true,
      "doc_id": "4dfa6d7e515ae313054fc19726a3fbd5",
      "summary": "A safe and reliable task planning method is a prerequisite for the collaborative execution of ocean observation data collection tasks by multiple unmanned surface vessels (multi-USVs). Deep reinforcement learning (DRL) combines the powerful nonlinear function-fitting capabilities of deep neural networks with the decision-making and control abilities of reinforcement learning, providing a novel approach to solving the multi-USV task planning problem. However, when applied to the field of multi-USV task planning, it faces challenges, such as a vast exploration space, extended training times, and unstable training process. To this end, this article proposes a multi-USV task planning method based on improved DRL. The proposed method draws on the idea of a value decomposition network, breaking down the multi-USV task planning problem into two subproblems: 1) task allocation and 2) autonomous collision avoidance. Different state spaces, action spaces, and reward functions are designed for the various subproblems. Based on this, a deep neural network is used to map the state space of each subproblem to the action space of each USV, and the generated strategy of the deep neural network is assessed based on the corresponding reward function. This successfully integrates task allocation and path planning into a comprehensive task planning framework. Deep neural networks consist of the Actor networks and the Critic networks. During the training phase of the Critic network, different methods are used to train different Critic networks to improve the convergence speed of the algorithm. An improved temporal difference error method is specifically applied to train the Critic network for evaluating autonomous collision avoidance strategies, resulting in improving the autonomous collision avoidance ability of USVs. At the same time, to improve the efficiency of task allocation, hierarchical mechanisms and regional division mechanisms are introduced to construct subsystem task planning models, which further decompose the task planning problem. A combination of successor features and an improved temporal difference error method is specifically applied to train another Critic network for evaluating the subsystems task allocation schemes and collaborative motion trajectories, aiming to enhance the allocation efficiency of the subsystems. Furthermore, transfer learning is employed to merge the subsystem task planning, using it as a constraint to direct the exploration and assessment of both the cluster task allocation schemes and the cluster collaborative motion trajectories. This enables rapid and accurate learning for task allocation within the multi-USV cluster. During the training phase of the Actor network, the introduction of the experience replay method and target network technique is employed to enhance the proximal policy optimization algorithm. This facilitates distributed joint training of the Actor network, thereby improving the accuracy of the algorithm. Simulation results validate the effectiveness and superiority of this method.",
      "intriguing_abstract": "A safe and reliable task planning method is a prerequisite for the collaborative execution of ocean observation data collection tasks by multiple unmanned surface vessels (multi-USVs). Deep reinforcement learning (DRL) combines the powerful nonlinear function-fitting capabilities of deep neural networks with the decision-making and control abilities of reinforcement learning, providing a novel approach to solving the multi-USV task planning problem. However, when applied to the field of multi-USV task planning, it faces challenges, such as a vast exploration space, extended training times, and unstable training process. To this end, this article proposes a multi-USV task planning method based on improved DRL. The proposed method draws on the idea of a value decomposition network, breaking down the multi-USV task planning problem into two subproblems: 1) task allocation and 2) autonomous collision avoidance. Different state spaces, action spaces, and reward functions are designed for the various subproblems. Based on this, a deep neural network is used to map the state space of each subproblem to the action space of each USV, and the generated strategy of the deep neural network is assessed based on the corresponding reward function. This successfully integrates task allocation and path planning into a comprehensive task planning framework. Deep neural networks consist of the Actor networks and the Critic networks. During the training phase of the Critic network, different methods are used to train different Critic networks to improve the convergence speed of the algorithm. An improved temporal difference error method is specifically applied to train the Critic network for evaluating autonomous collision avoidance strategies, resulting in improving the autonomous collision avoidance ability of USVs. At the same time, to improve the efficiency of task allocation, hierarchical mechanisms and regional division mechanisms are introduced to construct subsystem task planning models, which further decompose the task planning problem. A combination of successor features and an improved temporal difference error method is specifically applied to train another Critic network for evaluating the subsystems task allocation schemes and collaborative motion trajectories, aiming to enhance the allocation efficiency of the subsystems. Furthermore, transfer learning is employed to merge the subsystem task planning, using it as a constraint to direct the exploration and assessment of both the cluster task allocation schemes and the cluster collaborative motion trajectories. This enables rapid and accurate learning for task allocation within the multi-USV cluster. During the training phase of the Actor network, the introduction of the experience replay method and target network technique is employed to enhance the proximal policy optimization algorithm. This facilitates distributed joint training of the Actor network, thereby improving the accuracy of the algorithm. Simulation results validate the effectiveness and superiority of this method.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/914eaadede7a95116362cd6982321f93044b3b19.pdf",
      "citation_key": "zhang20242te",
      "metadata": {
        "title": "Multi-USV Task Planning Method Based on Improved Deep Reinforcement Learning",
        "authors": [
          "Jing Zhang",
          "Jian-Lin Ren",
          "Yani Cui",
          "Delong Fu",
          "Jingyu Cong"
        ],
        "published_date": "2024",
        "abstract": "A safe and reliable task planning method is a prerequisite for the collaborative execution of ocean observation data collection tasks by multiple unmanned surface vessels (multi-USVs). Deep reinforcement learning (DRL) combines the powerful nonlinear function-fitting capabilities of deep neural networks with the decision-making and control abilities of reinforcement learning, providing a novel approach to solving the multi-USV task planning problem. However, when applied to the field of multi-USV task planning, it faces challenges, such as a vast exploration space, extended training times, and unstable training process. To this end, this article proposes a multi-USV task planning method based on improved DRL. The proposed method draws on the idea of a value decomposition network, breaking down the multi-USV task planning problem into two subproblems: 1) task allocation and 2) autonomous collision avoidance. Different state spaces, action spaces, and reward functions are designed for the various subproblems. Based on this, a deep neural network is used to map the state space of each subproblem to the action space of each USV, and the generated strategy of the deep neural network is assessed based on the corresponding reward function. This successfully integrates task allocation and path planning into a comprehensive task planning framework. Deep neural networks consist of the Actor networks and the Critic networks. During the training phase of the Critic network, different methods are used to train different Critic networks to improve the convergence speed of the algorithm. An improved temporal difference error method is specifically applied to train the Critic network for evaluating autonomous collision avoidance strategies, resulting in improving the autonomous collision avoidance ability of USVs. At the same time, to improve the efficiency of task allocation, hierarchical mechanisms and regional division mechanisms are introduced to construct subsystem task planning models, which further decompose the task planning problem. A combination of successor features and an improved temporal difference error method is specifically applied to train another Critic network for evaluating the subsystems task allocation schemes and collaborative motion trajectories, aiming to enhance the allocation efficiency of the subsystems. Furthermore, transfer learning is employed to merge the subsystem task planning, using it as a constraint to direct the exploration and assessment of both the cluster task allocation schemes and the cluster collaborative motion trajectories. This enables rapid and accurate learning for task allocation within the multi-USV cluster. During the training phase of the Actor network, the introduction of the experience replay method and target network technique is employed to enhance the proximal policy optimization algorithm. This facilitates distributed joint training of the Actor network, thereby improving the accuracy of the algorithm. Simulation results validate the effectiveness and superiority of this method.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/914eaadede7a95116362cd6982321f93044b3b19.pdf",
        "venue": "IEEE Internet of Things Journal",
        "citationCount": 41,
        "score": 41.0,
        "summary": "A safe and reliable task planning method is a prerequisite for the collaborative execution of ocean observation data collection tasks by multiple unmanned surface vessels (multi-USVs). Deep reinforcement learning (DRL) combines the powerful nonlinear function-fitting capabilities of deep neural networks with the decision-making and control abilities of reinforcement learning, providing a novel approach to solving the multi-USV task planning problem. However, when applied to the field of multi-USV task planning, it faces challenges, such as a vast exploration space, extended training times, and unstable training process. To this end, this article proposes a multi-USV task planning method based on improved DRL. The proposed method draws on the idea of a value decomposition network, breaking down the multi-USV task planning problem into two subproblems: 1) task allocation and 2) autonomous collision avoidance. Different state spaces, action spaces, and reward functions are designed for the various subproblems. Based on this, a deep neural network is used to map the state space of each subproblem to the action space of each USV, and the generated strategy of the deep neural network is assessed based on the corresponding reward function. This successfully integrates task allocation and path planning into a comprehensive task planning framework. Deep neural networks consist of the Actor networks and the Critic networks. During the training phase of the Critic network, different methods are used to train different Critic networks to improve the convergence speed of the algorithm. An improved temporal difference error method is specifically applied to train the Critic network for evaluating autonomous collision avoidance strategies, resulting in improving the autonomous collision avoidance ability of USVs. At the same time, to improve the efficiency of task allocation, hierarchical mechanisms and regional division mechanisms are introduced to construct subsystem task planning models, which further decompose the task planning problem. A combination of successor features and an improved temporal difference error method is specifically applied to train another Critic network for evaluating the subsystems task allocation schemes and collaborative motion trajectories, aiming to enhance the allocation efficiency of the subsystems. Furthermore, transfer learning is employed to merge the subsystem task planning, using it as a constraint to direct the exploration and assessment of both the cluster task allocation schemes and the cluster collaborative motion trajectories. This enables rapid and accurate learning for task allocation within the multi-USV cluster. During the training phase of the Actor network, the introduction of the experience replay method and target network technique is employed to enhance the proximal policy optimization algorithm. This facilitates distributed joint training of the Actor network, thereby improving the accuracy of the algorithm. Simulation results validate the effectiveness and superiority of this method.",
        "keywords": []
      },
      "file_name": "914eaadede7a95116362cd6982321f93044b3b19.pdf"
    },
    {
      "success": true,
      "doc_id": "5bfa378565cc5dce0c3f08413decd04a",
      "summary": "In this paper, we propose R$^3$: Learning Reasoning through Reverse Curriculum Reinforcement Learning (RL), a novel method that employs only outcome supervision to achieve the benefits of process supervision for large language models. The core challenge in applying RL to complex reasoning is to identify a sequence of actions that result in positive rewards and provide appropriate supervision for optimization. Outcome supervision provides sparse rewards for final results without identifying error locations, whereas process supervision offers step-wise rewards but requires extensive manual annotation. R$^3$ overcomes these limitations by learning from correct demonstrations. Specifically, R$^3$ progressively slides the start state of reasoning from a demonstration's end to its beginning, facilitating easier model exploration at all stages. Thus, R$^3$ establishes a step-wise curriculum, allowing outcome supervision to offer step-level signals and precisely pinpoint errors. Using Llama2-7B, our method surpasses RL baseline on eight reasoning tasks by $4.1$ points on average. Notebaly, in program-based reasoning on GSM8K, it exceeds the baseline by $4.2$ points across three backbone models, and without any extra data, Codellama-7B + R$^3$ performs comparable to larger models or closed-source models.",
      "intriguing_abstract": "In this paper, we propose R$^3$: Learning Reasoning through Reverse Curriculum Reinforcement Learning (RL), a novel method that employs only outcome supervision to achieve the benefits of process supervision for large language models. The core challenge in applying RL to complex reasoning is to identify a sequence of actions that result in positive rewards and provide appropriate supervision for optimization. Outcome supervision provides sparse rewards for final results without identifying error locations, whereas process supervision offers step-wise rewards but requires extensive manual annotation. R$^3$ overcomes these limitations by learning from correct demonstrations. Specifically, R$^3$ progressively slides the start state of reasoning from a demonstration's end to its beginning, facilitating easier model exploration at all stages. Thus, R$^3$ establishes a step-wise curriculum, allowing outcome supervision to offer step-level signals and precisely pinpoint errors. Using Llama2-7B, our method surpasses RL baseline on eight reasoning tasks by $4.1$ points on average. Notebaly, in program-based reasoning on GSM8K, it exceeds the baseline by $4.2$ points across three backbone models, and without any extra data, Codellama-7B + R$^3$ performs comparable to larger models or closed-source models.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/7d6168fbd3ed72f9098573007f4b8c2ec9e576b9.pdf",
      "citation_key": "xi2024tj9",
      "metadata": {
        "title": "Training Large Language Models for Reasoning through Reverse Curriculum Reinforcement Learning",
        "authors": [
          "Zhiheng Xi",
          "Wenxiang Chen",
          "Boyang Hong",
          "Senjie Jin",
          "Rui Zheng",
          "Wei He",
          "Yiwen Ding",
          "Shichun Liu",
          "Xin Guo",
          "Junzhe Wang",
          "Honglin Guo",
          "Wei Shen",
          "Xiaoran Fan",
          "Yuhao Zhou",
          "Shihan Dou",
          "Xiao Wang",
          "Xinbo Zhang",
          "Peng Sun",
          "Tao Gui",
          "Qi Zhang",
          "Xuanjing Huang"
        ],
        "published_date": "2024",
        "abstract": "In this paper, we propose R$^3$: Learning Reasoning through Reverse Curriculum Reinforcement Learning (RL), a novel method that employs only outcome supervision to achieve the benefits of process supervision for large language models. The core challenge in applying RL to complex reasoning is to identify a sequence of actions that result in positive rewards and provide appropriate supervision for optimization. Outcome supervision provides sparse rewards for final results without identifying error locations, whereas process supervision offers step-wise rewards but requires extensive manual annotation. R$^3$ overcomes these limitations by learning from correct demonstrations. Specifically, R$^3$ progressively slides the start state of reasoning from a demonstration's end to its beginning, facilitating easier model exploration at all stages. Thus, R$^3$ establishes a step-wise curriculum, allowing outcome supervision to offer step-level signals and precisely pinpoint errors. Using Llama2-7B, our method surpasses RL baseline on eight reasoning tasks by $4.1$ points on average. Notebaly, in program-based reasoning on GSM8K, it exceeds the baseline by $4.2$ points across three backbone models, and without any extra data, Codellama-7B + R$^3$ performs comparable to larger models or closed-source models.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/7d6168fbd3ed72f9098573007f4b8c2ec9e576b9.pdf",
        "venue": "International Conference on Machine Learning",
        "citationCount": 39,
        "score": 39.0,
        "summary": "In this paper, we propose R$^3$: Learning Reasoning through Reverse Curriculum Reinforcement Learning (RL), a novel method that employs only outcome supervision to achieve the benefits of process supervision for large language models. The core challenge in applying RL to complex reasoning is to identify a sequence of actions that result in positive rewards and provide appropriate supervision for optimization. Outcome supervision provides sparse rewards for final results without identifying error locations, whereas process supervision offers step-wise rewards but requires extensive manual annotation. R$^3$ overcomes these limitations by learning from correct demonstrations. Specifically, R$^3$ progressively slides the start state of reasoning from a demonstration's end to its beginning, facilitating easier model exploration at all stages. Thus, R$^3$ establishes a step-wise curriculum, allowing outcome supervision to offer step-level signals and precisely pinpoint errors. Using Llama2-7B, our method surpasses RL baseline on eight reasoning tasks by $4.1$ points on average. Notebaly, in program-based reasoning on GSM8K, it exceeds the baseline by $4.2$ points across three backbone models, and without any extra data, Codellama-7B + R$^3$ performs comparable to larger models or closed-source models.",
        "keywords": []
      },
      "file_name": "7d6168fbd3ed72f9098573007f4b8c2ec9e576b9.pdf"
    },
    {
      "success": true,
      "doc_id": "e6aed8534d6565f535a8928e1b83fa03",
      "summary": "This article presents a digital twin (DT)-enhanced reinforcement learning (RL) framework aimed at optimizing performance and reliability in network resource management, since the traditional RL methods face several unified challenges when applied to physical networks, including limited exploration efficiency, slow convergence, poor long-term performance, and safety concerns during the exploration phase. To deal with the above challenges, a comprehensive DT-based framework is proposed to enhance the convergence speed and performance for unified RL-based resource management. The proposed framework provides safe action exploration, more accurate estimates of long-term returns, faster training convergence, higher convergence performance, and real-time adaptation to varying network conditions. Then, two case studies on ultra-reliable and low-latency communication (URLLC) services and multiple unmanned aerial vehicles (UAV) network are presented, demonstrating improvements of the proposed framework in performance, convergence speed, and training cost reduction both on traditional RL and neural network based Deep RL (DRL). Finally, the article identifies and explores some of the research challenges and open issues in this rapidly evolving field.",
      "intriguing_abstract": "This article presents a digital twin (DT)-enhanced reinforcement learning (RL) framework aimed at optimizing performance and reliability in network resource management, since the traditional RL methods face several unified challenges when applied to physical networks, including limited exploration efficiency, slow convergence, poor long-term performance, and safety concerns during the exploration phase. To deal with the above challenges, a comprehensive DT-based framework is proposed to enhance the convergence speed and performance for unified RL-based resource management. The proposed framework provides safe action exploration, more accurate estimates of long-term returns, faster training convergence, higher convergence performance, and real-time adaptation to varying network conditions. Then, two case studies on ultra-reliable and low-latency communication (URLLC) services and multiple unmanned aerial vehicles (UAV) network are presented, demonstrating improvements of the proposed framework in performance, convergence speed, and training cost reduction both on traditional RL and neural network based Deep RL (DRL). Finally, the article identifies and explores some of the research challenges and open issues in this rapidly evolving field.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/a694a3fb49f3c6e63e2fa5ddb72fe3aa490592f1.pdf",
      "citation_key": "cheng2024vjq",
      "metadata": {
        "title": "Toward Enhanced Reinforcement Learning-Based Resource Management via Digital Twin: Opportunities, Applications, and Challenges",
        "authors": [
          "Nan Cheng",
          "Xiucheng Wang",
          "Zan Li",
          "Zhisheng Yin",
          "Tom H. Luan",
          "Xuemin Shen"
        ],
        "published_date": "2024",
        "abstract": "This article presents a digital twin (DT)-enhanced reinforcement learning (RL) framework aimed at optimizing performance and reliability in network resource management, since the traditional RL methods face several unified challenges when applied to physical networks, including limited exploration efficiency, slow convergence, poor long-term performance, and safety concerns during the exploration phase. To deal with the above challenges, a comprehensive DT-based framework is proposed to enhance the convergence speed and performance for unified RL-based resource management. The proposed framework provides safe action exploration, more accurate estimates of long-term returns, faster training convergence, higher convergence performance, and real-time adaptation to varying network conditions. Then, two case studies on ultra-reliable and low-latency communication (URLLC) services and multiple unmanned aerial vehicles (UAV) network are presented, demonstrating improvements of the proposed framework in performance, convergence speed, and training cost reduction both on traditional RL and neural network based Deep RL (DRL). Finally, the article identifies and explores some of the research challenges and open issues in this rapidly evolving field.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/a694a3fb49f3c6e63e2fa5ddb72fe3aa490592f1.pdf",
        "venue": "IEEE Network",
        "citationCount": 21,
        "score": 21.0,
        "summary": "This article presents a digital twin (DT)-enhanced reinforcement learning (RL) framework aimed at optimizing performance and reliability in network resource management, since the traditional RL methods face several unified challenges when applied to physical networks, including limited exploration efficiency, slow convergence, poor long-term performance, and safety concerns during the exploration phase. To deal with the above challenges, a comprehensive DT-based framework is proposed to enhance the convergence speed and performance for unified RL-based resource management. The proposed framework provides safe action exploration, more accurate estimates of long-term returns, faster training convergence, higher convergence performance, and real-time adaptation to varying network conditions. Then, two case studies on ultra-reliable and low-latency communication (URLLC) services and multiple unmanned aerial vehicles (UAV) network are presented, demonstrating improvements of the proposed framework in performance, convergence speed, and training cost reduction both on traditional RL and neural network based Deep RL (DRL). Finally, the article identifies and explores some of the research challenges and open issues in this rapidly evolving field.",
        "keywords": []
      },
      "file_name": "a694a3fb49f3c6e63e2fa5ddb72fe3aa490592f1.pdf"
    },
    {
      "success": true,
      "doc_id": "7de4c46aad9cfb462e0cb93a79994ffc",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/25db1b77bc330476c3cf6ce43236404c578b4372.pdf",
      "citation_key": "sun20238u5",
      "metadata": {
        "title": "Model-Bellman Inconsistency for Model-based Offline Reinforcement Learning",
        "authors": [
          "Yihao Sun",
          "Jiajin Zhang",
          "Chengxing Jia",
          "Hao-Chu Lin",
          "Junyin Ye",
          "Yangze Yu"
        ],
        "published_date": "2023",
        "abstract": "",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/25db1b77bc330476c3cf6ce43236404c578b4372.pdf",
        "venue": "International Conference on Machine Learning",
        "citationCount": 42,
        "score": 21.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "25db1b77bc330476c3cf6ce43236404c578b4372.pdf"
    },
    {
      "success": true,
      "doc_id": "71c15431cb6c6432d675e215ad71ccd9",
      "summary": "Visual reinforcement learning (RL) has shown promise in continuous control tasks. Despite its progress, current algorithms are still unsatisfactory in virtually every aspect of the performance such as sample efficiency, asymptotic performance, and their robustness to the choice of random seeds. In this paper, we identify a major shortcoming in existing visual RL methods that is the agents often exhibit sustained inactivity during early training, thereby limiting their ability to explore effectively. Expanding upon this crucial observation, we additionally unveil a significant correlation between the agents' inclination towards motorically inactive exploration and the absence of neuronal activity within their policy networks. To quantify this inactivity, we adopt dormant ratio as a metric to measure inactivity in the RL agent's network. Empirically, we also recognize that the dormant ratio can act as a standalone indicator of an agent's activity level, regardless of the received reward signals. Leveraging the aforementioned insights, we introduce DrM, a method that uses three core mechanisms to guide agents' exploration-exploitation trade-offs by actively minimizing the dormant ratio. Experiments demonstrate that DrM achieves significant improvements in sample efficiency and asymptotic performance with no broken seeds (76 seeds in total) across three continuous control benchmark environments, including DeepMind Control Suite, MetaWorld, and Adroit. Most importantly, DrM is the first model-free algorithm that consistently solves tasks in both the Dog and Manipulator domains from the DeepMind Control Suite as well as three dexterous hand manipulation tasks without demonstrations in Adroit, all based on pixel observations.",
      "intriguing_abstract": "Visual reinforcement learning (RL) has shown promise in continuous control tasks. Despite its progress, current algorithms are still unsatisfactory in virtually every aspect of the performance such as sample efficiency, asymptotic performance, and their robustness to the choice of random seeds. In this paper, we identify a major shortcoming in existing visual RL methods that is the agents often exhibit sustained inactivity during early training, thereby limiting their ability to explore effectively. Expanding upon this crucial observation, we additionally unveil a significant correlation between the agents' inclination towards motorically inactive exploration and the absence of neuronal activity within their policy networks. To quantify this inactivity, we adopt dormant ratio as a metric to measure inactivity in the RL agent's network. Empirically, we also recognize that the dormant ratio can act as a standalone indicator of an agent's activity level, regardless of the received reward signals. Leveraging the aforementioned insights, we introduce DrM, a method that uses three core mechanisms to guide agents' exploration-exploitation trade-offs by actively minimizing the dormant ratio. Experiments demonstrate that DrM achieves significant improvements in sample efficiency and asymptotic performance with no broken seeds (76 seeds in total) across three continuous control benchmark environments, including DeepMind Control Suite, MetaWorld, and Adroit. Most importantly, DrM is the first model-free algorithm that consistently solves tasks in both the Dog and Manipulator domains from the DeepMind Control Suite as well as three dexterous hand manipulation tasks without demonstrations in Adroit, all based on pixel observations.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/4e98282f5f3f1a388b8d95380473d4ef4878266e.pdf",
      "citation_key": "xu2023t6r",
      "metadata": {
        "title": "DrM: Mastering Visual Reinforcement Learning through Dormant Ratio Minimization",
        "authors": [
          "Guowei Xu",
          "Ruijie Zheng",
          "Yongyuan Liang",
          "Xiyao Wang",
          "Zhecheng Yuan",
          "Tianying Ji",
          "Yu Luo",
          "Xiaoyu Liu",
          "Jiaxin Yuan",
          "Pu Hua",
          "Shuzhen Li",
          "Yanjie Ze",
          "Hal Daum'e",
          "Furong Huang",
          "Huazhe Xu"
        ],
        "published_date": "2023",
        "abstract": "Visual reinforcement learning (RL) has shown promise in continuous control tasks. Despite its progress, current algorithms are still unsatisfactory in virtually every aspect of the performance such as sample efficiency, asymptotic performance, and their robustness to the choice of random seeds. In this paper, we identify a major shortcoming in existing visual RL methods that is the agents often exhibit sustained inactivity during early training, thereby limiting their ability to explore effectively. Expanding upon this crucial observation, we additionally unveil a significant correlation between the agents' inclination towards motorically inactive exploration and the absence of neuronal activity within their policy networks. To quantify this inactivity, we adopt dormant ratio as a metric to measure inactivity in the RL agent's network. Empirically, we also recognize that the dormant ratio can act as a standalone indicator of an agent's activity level, regardless of the received reward signals. Leveraging the aforementioned insights, we introduce DrM, a method that uses three core mechanisms to guide agents' exploration-exploitation trade-offs by actively minimizing the dormant ratio. Experiments demonstrate that DrM achieves significant improvements in sample efficiency and asymptotic performance with no broken seeds (76 seeds in total) across three continuous control benchmark environments, including DeepMind Control Suite, MetaWorld, and Adroit. Most importantly, DrM is the first model-free algorithm that consistently solves tasks in both the Dog and Manipulator domains from the DeepMind Control Suite as well as three dexterous hand manipulation tasks without demonstrations in Adroit, all based on pixel observations.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/4e98282f5f3f1a388b8d95380473d4ef4878266e.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 39,
        "score": 19.5,
        "summary": "Visual reinforcement learning (RL) has shown promise in continuous control tasks. Despite its progress, current algorithms are still unsatisfactory in virtually every aspect of the performance such as sample efficiency, asymptotic performance, and their robustness to the choice of random seeds. In this paper, we identify a major shortcoming in existing visual RL methods that is the agents often exhibit sustained inactivity during early training, thereby limiting their ability to explore effectively. Expanding upon this crucial observation, we additionally unveil a significant correlation between the agents' inclination towards motorically inactive exploration and the absence of neuronal activity within their policy networks. To quantify this inactivity, we adopt dormant ratio as a metric to measure inactivity in the RL agent's network. Empirically, we also recognize that the dormant ratio can act as a standalone indicator of an agent's activity level, regardless of the received reward signals. Leveraging the aforementioned insights, we introduce DrM, a method that uses three core mechanisms to guide agents' exploration-exploitation trade-offs by actively minimizing the dormant ratio. Experiments demonstrate that DrM achieves significant improvements in sample efficiency and asymptotic performance with no broken seeds (76 seeds in total) across three continuous control benchmark environments, including DeepMind Control Suite, MetaWorld, and Adroit. Most importantly, DrM is the first model-free algorithm that consistently solves tasks in both the Dog and Manipulator domains from the DeepMind Control Suite as well as three dexterous hand manipulation tasks without demonstrations in Adroit, all based on pixel observations.",
        "keywords": []
      },
      "file_name": "4e98282f5f3f1a388b8d95380473d4ef4878266e.pdf"
    },
    {
      "success": true,
      "doc_id": "159012d6cf34ac1ebbf23c6b1f4a59d4",
      "summary": "Multimodal Reward Models (MRMs) play a crucial role in enhancing the performance of Multimodal Large Language Models (MLLMs). While recent advancements have primarily focused on improving the model structure and training data of MRMs, there has been limited exploration into the effectiveness of long-term reasoning capabilities for reward modeling and how to activate these capabilities in MRMs. In this paper, we explore how Reinforcement Learning (RL) can be used to improve reward modeling. Specifically, we reformulate the reward modeling problem as a rule-based RL task. However, we observe that directly applying existing RL algorithms, such as Reinforce++, to reward modeling often leads to training instability or even collapse due to the inherent limitations of these algorithms. To address this issue, we propose the StableReinforce algorithm, which refines the training loss, advantage estimation strategy, and reward design of existing RL methods. These refinements result in more stable training dynamics and superior performance. To facilitate MRM training, we collect 200K preference data from diverse datasets. Our reward model, R1-Reward, trained using the StableReinforce algorithm on this dataset, significantly improves performance on multimodal reward modeling benchmarks. Compared to previous SOTA models, R1-Reward achieves a $8.4\\%$ improvement on the VL Reward-Bench and a $14.3\\%$ improvement on the Multimodal Reward Bench. Moreover, with more inference compute, R1-Reward's performance is further enhanced, highlighting the potential of RL algorithms in optimizing MRMs.",
      "intriguing_abstract": "Multimodal Reward Models (MRMs) play a crucial role in enhancing the performance of Multimodal Large Language Models (MLLMs). While recent advancements have primarily focused on improving the model structure and training data of MRMs, there has been limited exploration into the effectiveness of long-term reasoning capabilities for reward modeling and how to activate these capabilities in MRMs. In this paper, we explore how Reinforcement Learning (RL) can be used to improve reward modeling. Specifically, we reformulate the reward modeling problem as a rule-based RL task. However, we observe that directly applying existing RL algorithms, such as Reinforce++, to reward modeling often leads to training instability or even collapse due to the inherent limitations of these algorithms. To address this issue, we propose the StableReinforce algorithm, which refines the training loss, advantage estimation strategy, and reward design of existing RL methods. These refinements result in more stable training dynamics and superior performance. To facilitate MRM training, we collect 200K preference data from diverse datasets. Our reward model, R1-Reward, trained using the StableReinforce algorithm on this dataset, significantly improves performance on multimodal reward modeling benchmarks. Compared to previous SOTA models, R1-Reward achieves a $8.4\\%$ improvement on the VL Reward-Bench and a $14.3\\%$ improvement on the Multimodal Reward Bench. Moreover, with more inference compute, R1-Reward's performance is further enhanced, highlighting the potential of RL algorithms in optimizing MRMs.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/abeb46288d537f98f76b979040a547ee81216377.pdf",
      "citation_key": "zhang2025wku",
      "metadata": {
        "title": "R1-Reward: Training Multimodal Reward Model Through Stable Reinforcement Learning",
        "authors": [
          "Yi-Fan Zhang",
          "Xingyu Lu",
          "Xiao Hu",
          "Chaoyou Fu",
          "Bin Wen",
          "Tianke Zhang",
          "Changyi Liu",
          "Kaiyu Jiang",
          "Kaibing Chen",
          "Kaiyu Tang",
          "Haojie Ding",
          "Jiankang Chen",
          "Fan Yang",
          "Zhang Zhang",
          "Tingting Gao",
          "Liang Wang"
        ],
        "published_date": "2025",
        "abstract": "Multimodal Reward Models (MRMs) play a crucial role in enhancing the performance of Multimodal Large Language Models (MLLMs). While recent advancements have primarily focused on improving the model structure and training data of MRMs, there has been limited exploration into the effectiveness of long-term reasoning capabilities for reward modeling and how to activate these capabilities in MRMs. In this paper, we explore how Reinforcement Learning (RL) can be used to improve reward modeling. Specifically, we reformulate the reward modeling problem as a rule-based RL task. However, we observe that directly applying existing RL algorithms, such as Reinforce++, to reward modeling often leads to training instability or even collapse due to the inherent limitations of these algorithms. To address this issue, we propose the StableReinforce algorithm, which refines the training loss, advantage estimation strategy, and reward design of existing RL methods. These refinements result in more stable training dynamics and superior performance. To facilitate MRM training, we collect 200K preference data from diverse datasets. Our reward model, R1-Reward, trained using the StableReinforce algorithm on this dataset, significantly improves performance on multimodal reward modeling benchmarks. Compared to previous SOTA models, R1-Reward achieves a $8.4\\%$ improvement on the VL Reward-Bench and a $14.3\\%$ improvement on the Multimodal Reward Bench. Moreover, with more inference compute, R1-Reward's performance is further enhanced, highlighting the potential of RL algorithms in optimizing MRMs.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/abeb46288d537f98f76b979040a547ee81216377.pdf",
        "venue": "arXiv.org",
        "citationCount": 19,
        "score": 19.0,
        "summary": "Multimodal Reward Models (MRMs) play a crucial role in enhancing the performance of Multimodal Large Language Models (MLLMs). While recent advancements have primarily focused on improving the model structure and training data of MRMs, there has been limited exploration into the effectiveness of long-term reasoning capabilities for reward modeling and how to activate these capabilities in MRMs. In this paper, we explore how Reinforcement Learning (RL) can be used to improve reward modeling. Specifically, we reformulate the reward modeling problem as a rule-based RL task. However, we observe that directly applying existing RL algorithms, such as Reinforce++, to reward modeling often leads to training instability or even collapse due to the inherent limitations of these algorithms. To address this issue, we propose the StableReinforce algorithm, which refines the training loss, advantage estimation strategy, and reward design of existing RL methods. These refinements result in more stable training dynamics and superior performance. To facilitate MRM training, we collect 200K preference data from diverse datasets. Our reward model, R1-Reward, trained using the StableReinforce algorithm on this dataset, significantly improves performance on multimodal reward modeling benchmarks. Compared to previous SOTA models, R1-Reward achieves a $8.4\\%$ improvement on the VL Reward-Bench and a $14.3\\%$ improvement on the Multimodal Reward Bench. Moreover, with more inference compute, R1-Reward's performance is further enhanced, highlighting the potential of RL algorithms in optimizing MRMs.",
        "keywords": []
      },
      "file_name": "abeb46288d537f98f76b979040a547ee81216377.pdf"
    },
    {
      "success": true,
      "doc_id": "999fb6e72dea89cc79210d2ceacaf283",
      "summary": "Recent high-capacity vision-language-action (VLA) models have demonstrated impressive performance on a range of robotic manipulation tasks by imitating human demonstrations. However, exploiting offline data with limited visited states will cause execution failure in out-of-distribution scenarios. Intuitively, an exploration-based method that improves on online collected data at test time could address this limitation. We present VLA-RL, an algorithmic and systematic framework that leverages online reinforcement learning (RL) to improve pretrained auto-regressive VLAs in downstream tasks. Within a unified perspective, we first introduce a trajectory-level RL formulation for auto-regressive VLA training, which models general robotic manipulation trajectory as multi-modal multi-turn conversation. To address the challenge of sparse rewards, we fine-tune a pretrained vision-language model as a robotic process reward model, which is trained on pseudo reward labels annotated on automatically extracted task segments. To scale up, we identify several implementation findings that improve the stability and efficiency including curriculum selection strategy, GPU-balanced vectorized environments, batch decoding, and critic warmup. VLA-RL enables OpenVLA-7B to surpass the strongest finetuned baseline by 4.5% on 40 challenging robotic manipulation tasks in LIBERO, and even matches the performance of advanced commercial models such as $\\pi_0$-FAST. Notably, we observe that VLA-RL benefits from increased test-time optimization, indicating an early spark of inference scaling laws in robotics.",
      "intriguing_abstract": "Recent high-capacity vision-language-action (VLA) models have demonstrated impressive performance on a range of robotic manipulation tasks by imitating human demonstrations. However, exploiting offline data with limited visited states will cause execution failure in out-of-distribution scenarios. Intuitively, an exploration-based method that improves on online collected data at test time could address this limitation. We present VLA-RL, an algorithmic and systematic framework that leverages online reinforcement learning (RL) to improve pretrained auto-regressive VLAs in downstream tasks. Within a unified perspective, we first introduce a trajectory-level RL formulation for auto-regressive VLA training, which models general robotic manipulation trajectory as multi-modal multi-turn conversation. To address the challenge of sparse rewards, we fine-tune a pretrained vision-language model as a robotic process reward model, which is trained on pseudo reward labels annotated on automatically extracted task segments. To scale up, we identify several implementation findings that improve the stability and efficiency including curriculum selection strategy, GPU-balanced vectorized environments, batch decoding, and critic warmup. VLA-RL enables OpenVLA-7B to surpass the strongest finetuned baseline by 4.5% on 40 challenging robotic manipulation tasks in LIBERO, and even matches the performance of advanced commercial models such as $\\pi_0$-FAST. Notably, we observe that VLA-RL benefits from increased test-time optimization, indicating an early spark of inference scaling laws in robotics.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/4a3e88d203564e547f5fb3f3d816a0b381492eae.pdf",
      "citation_key": "lu2025j7f",
      "metadata": {
        "title": "VLA-RL: Towards Masterful and General Robotic Manipulation with Scalable Reinforcement Learning",
        "authors": [
          "Guanxing Lu",
          "Wenkai Guo",
          "Chubin Zhang",
          "Yuheng Zhou",
          "Hao Jiang",
          "Zifeng Gao",
          "Yansong Tang",
          "Ziwei Wang"
        ],
        "published_date": "2025",
        "abstract": "Recent high-capacity vision-language-action (VLA) models have demonstrated impressive performance on a range of robotic manipulation tasks by imitating human demonstrations. However, exploiting offline data with limited visited states will cause execution failure in out-of-distribution scenarios. Intuitively, an exploration-based method that improves on online collected data at test time could address this limitation. We present VLA-RL, an algorithmic and systematic framework that leverages online reinforcement learning (RL) to improve pretrained auto-regressive VLAs in downstream tasks. Within a unified perspective, we first introduce a trajectory-level RL formulation for auto-regressive VLA training, which models general robotic manipulation trajectory as multi-modal multi-turn conversation. To address the challenge of sparse rewards, we fine-tune a pretrained vision-language model as a robotic process reward model, which is trained on pseudo reward labels annotated on automatically extracted task segments. To scale up, we identify several implementation findings that improve the stability and efficiency including curriculum selection strategy, GPU-balanced vectorized environments, batch decoding, and critic warmup. VLA-RL enables OpenVLA-7B to surpass the strongest finetuned baseline by 4.5% on 40 challenging robotic manipulation tasks in LIBERO, and even matches the performance of advanced commercial models such as $\\pi_0$-FAST. Notably, we observe that VLA-RL benefits from increased test-time optimization, indicating an early spark of inference scaling laws in robotics.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/4a3e88d203564e547f5fb3f3d816a0b381492eae.pdf",
        "venue": "arXiv.org",
        "citationCount": 18,
        "score": 18.0,
        "summary": "Recent high-capacity vision-language-action (VLA) models have demonstrated impressive performance on a range of robotic manipulation tasks by imitating human demonstrations. However, exploiting offline data with limited visited states will cause execution failure in out-of-distribution scenarios. Intuitively, an exploration-based method that improves on online collected data at test time could address this limitation. We present VLA-RL, an algorithmic and systematic framework that leverages online reinforcement learning (RL) to improve pretrained auto-regressive VLAs in downstream tasks. Within a unified perspective, we first introduce a trajectory-level RL formulation for auto-regressive VLA training, which models general robotic manipulation trajectory as multi-modal multi-turn conversation. To address the challenge of sparse rewards, we fine-tune a pretrained vision-language model as a robotic process reward model, which is trained on pseudo reward labels annotated on automatically extracted task segments. To scale up, we identify several implementation findings that improve the stability and efficiency including curriculum selection strategy, GPU-balanced vectorized environments, batch decoding, and critic warmup. VLA-RL enables OpenVLA-7B to surpass the strongest finetuned baseline by 4.5% on 40 challenging robotic manipulation tasks in LIBERO, and even matches the performance of advanced commercial models such as $\\pi_0$-FAST. Notably, we observe that VLA-RL benefits from increased test-time optimization, indicating an early spark of inference scaling laws in robotics.",
        "keywords": []
      },
      "file_name": "4a3e88d203564e547f5fb3f3d816a0b381492eae.pdf"
    },
    {
      "success": true,
      "doc_id": "b312f4470afac29aadce8ba32b1e0803",
      "summary": "Existing approaches for improving generalization in deep reinforcement learning (RL) have mostly focused on representation learning, neglecting RL-specific aspects such as exploration. We hypothesize that the agent's exploration strategy plays a key role in its ability to generalize to new environments. Through a series of experiments in a tabular contextual MDP, we show that exploration is helpful not only for efficiently finding the optimal policy for the training environments but also for acquiring knowledge that helps decision making in unseen environments. Based on these observations, we propose EDE: Exploration via Distributional Ensemble, a method that encourages exploration of states with high epistemic uncertainty through an ensemble of Q-value distributions. Our algorithm is the first value-based approach to achieve state-of-the-art on both Procgen and Crafter, two benchmarks for generalization in RL with high-dimensional observations. The open-sourced implementation can be found at https://github.com/facebookresearch/ede .",
      "intriguing_abstract": "Existing approaches for improving generalization in deep reinforcement learning (RL) have mostly focused on representation learning, neglecting RL-specific aspects such as exploration. We hypothesize that the agent's exploration strategy plays a key role in its ability to generalize to new environments. Through a series of experiments in a tabular contextual MDP, we show that exploration is helpful not only for efficiently finding the optimal policy for the training environments but also for acquiring knowledge that helps decision making in unseen environments. Based on these observations, we propose EDE: Exploration via Distributional Ensemble, a method that encourages exploration of states with high epistemic uncertainty through an ensemble of Q-value distributions. Our algorithm is the first value-based approach to achieve state-of-the-art on both Procgen and Crafter, two benchmarks for generalization in RL with high-dimensional observations. The open-sourced implementation can be found at https://github.com/facebookresearch/ede .",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/1a73038804052a40c12aae696848ece2168f6da7.pdf",
      "citation_key": "jiang2023qmw",
      "metadata": {
        "title": "On the Importance of Exploration for Generalization in Reinforcement Learning",
        "authors": [
          "Yiding Jiang",
          "J. Z. Kolter",
          "R. Raileanu"
        ],
        "published_date": "2023",
        "abstract": "Existing approaches for improving generalization in deep reinforcement learning (RL) have mostly focused on representation learning, neglecting RL-specific aspects such as exploration. We hypothesize that the agent's exploration strategy plays a key role in its ability to generalize to new environments. Through a series of experiments in a tabular contextual MDP, we show that exploration is helpful not only for efficiently finding the optimal policy for the training environments but also for acquiring knowledge that helps decision making in unseen environments. Based on these observations, we propose EDE: Exploration via Distributional Ensemble, a method that encourages exploration of states with high epistemic uncertainty through an ensemble of Q-value distributions. Our algorithm is the first value-based approach to achieve state-of-the-art on both Procgen and Crafter, two benchmarks for generalization in RL with high-dimensional observations. The open-sourced implementation can be found at https://github.com/facebookresearch/ede .",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/1a73038804052a40c12aae696848ece2168f6da7.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 32,
        "score": 16.0,
        "summary": "Existing approaches for improving generalization in deep reinforcement learning (RL) have mostly focused on representation learning, neglecting RL-specific aspects such as exploration. We hypothesize that the agent's exploration strategy plays a key role in its ability to generalize to new environments. Through a series of experiments in a tabular contextual MDP, we show that exploration is helpful not only for efficiently finding the optimal policy for the training environments but also for acquiring knowledge that helps decision making in unseen environments. Based on these observations, we propose EDE: Exploration via Distributional Ensemble, a method that encourages exploration of states with high epistemic uncertainty through an ensemble of Q-value distributions. Our algorithm is the first value-based approach to achieve state-of-the-art on both Procgen and Crafter, two benchmarks for generalization in RL with high-dimensional observations. The open-sourced implementation can be found at https://github.com/facebookresearch/ede .",
        "keywords": []
      },
      "file_name": "1a73038804052a40c12aae696848ece2168f6da7.pdf"
    },
    {
      "success": true,
      "doc_id": "39b565b62b74b78699e0dc6e65be1e5d",
      "summary": "This paper presents advanced techniques of training diffusion policies for offline reinforcement learning (RL). At the core is a mean-reverting stochastic differential equation (SDE) that transfers a complex action distribution into a standard Gaussian and then samples actions conditioned on the environment state with a corresponding reverse-time SDE, like a typical diffusion policy. We show that such an SDE has a solution that we can use to calculate the log probability of the policy, yielding an entropy regularizer that improves the exploration of offline datasets. To mitigate the impact of inaccurate value functions from out-of-distribution data points, we further propose to learn the lower confidence bound of Q-ensembles for more robust policy improvement. By combining the entropy-regularized diffusion policy with Q-ensembles in offline RL, our method achieves state-of-the-art performance on most tasks in D4RL benchmarks. Code is available at https://github.com/ruoqizzz/Entropy-Regularized-Diffusion-Policy-with-QEnsemble.",
      "intriguing_abstract": "This paper presents advanced techniques of training diffusion policies for offline reinforcement learning (RL). At the core is a mean-reverting stochastic differential equation (SDE) that transfers a complex action distribution into a standard Gaussian and then samples actions conditioned on the environment state with a corresponding reverse-time SDE, like a typical diffusion policy. We show that such an SDE has a solution that we can use to calculate the log probability of the policy, yielding an entropy regularizer that improves the exploration of offline datasets. To mitigate the impact of inaccurate value functions from out-of-distribution data points, we further propose to learn the lower confidence bound of Q-ensembles for more robust policy improvement. By combining the entropy-regularized diffusion policy with Q-ensembles in offline RL, our method achieves state-of-the-art performance on most tasks in D4RL benchmarks. Code is available at https://github.com/ruoqizzz/Entropy-Regularized-Diffusion-Policy-with-QEnsemble.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/79f923d6575bd8253e2f0b70813caa61a870ccee.pdf",
      "citation_key": "zhang20244ty",
      "metadata": {
        "title": "Entropy-regularized Diffusion Policy with Q-Ensembles for Offline Reinforcement Learning",
        "authors": [
          "Ruoqing Zhang",
          "Ziwei Luo",
          "Jens Sjlund",
          "Thomas B. Schn",
          "Per Mattsson"
        ],
        "published_date": "2024",
        "abstract": "This paper presents advanced techniques of training diffusion policies for offline reinforcement learning (RL). At the core is a mean-reverting stochastic differential equation (SDE) that transfers a complex action distribution into a standard Gaussian and then samples actions conditioned on the environment state with a corresponding reverse-time SDE, like a typical diffusion policy. We show that such an SDE has a solution that we can use to calculate the log probability of the policy, yielding an entropy regularizer that improves the exploration of offline datasets. To mitigate the impact of inaccurate value functions from out-of-distribution data points, we further propose to learn the lower confidence bound of Q-ensembles for more robust policy improvement. By combining the entropy-regularized diffusion policy with Q-ensembles in offline RL, our method achieves state-of-the-art performance on most tasks in D4RL benchmarks. Code is available at https://github.com/ruoqizzz/Entropy-Regularized-Diffusion-Policy-with-QEnsemble.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/79f923d6575bd8253e2f0b70813caa61a870ccee.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 16,
        "score": 16.0,
        "summary": "This paper presents advanced techniques of training diffusion policies for offline reinforcement learning (RL). At the core is a mean-reverting stochastic differential equation (SDE) that transfers a complex action distribution into a standard Gaussian and then samples actions conditioned on the environment state with a corresponding reverse-time SDE, like a typical diffusion policy. We show that such an SDE has a solution that we can use to calculate the log probability of the policy, yielding an entropy regularizer that improves the exploration of offline datasets. To mitigate the impact of inaccurate value functions from out-of-distribution data points, we further propose to learn the lower confidence bound of Q-ensembles for more robust policy improvement. By combining the entropy-regularized diffusion policy with Q-ensembles in offline RL, our method achieves state-of-the-art performance on most tasks in D4RL benchmarks. Code is available at https://github.com/ruoqizzz/Entropy-Regularized-Diffusion-Policy-with-QEnsemble.",
        "keywords": []
      },
      "file_name": "79f923d6575bd8253e2f0b70813caa61a870ccee.pdf"
    },
    {
      "success": true,
      "doc_id": "fd9e9638630405ae7bb2fe0b3448bd44",
      "summary": "Ensuring the safety of Reinforcement Learning (RL) is crucial for its deployment in real-world applications. Nevertheless, managing the trade-off between reward and safety during exploration presents a significant challenge. Improving reward performance through policy adjustments may adversely affect safety performance. In this study, we aim to address this conflicting relation by leveraging the theory of gradient manipulation. Initially, we analyze the conflict between reward and safety gradients. Subsequently, we tackle the balance between reward and safety optimization by proposing a soft switching policy optimization method, for which we provide convergence analysis. Based on our theoretical examination, we provide a safe RL framework to overcome the aforementioned challenge, and we develop a Safety-MuJoCo Benchmark to assess the performance of safe RL algorithms. Finally, we evaluate the effectiveness of our method on the Safety-MuJoCo Benchmark and a popular safe benchmark, Omnisafe. Experimental results demonstrate that our algorithms outperform several state-of-the-art baselines in terms of balancing reward and safety optimization.",
      "intriguing_abstract": "Ensuring the safety of Reinforcement Learning (RL) is crucial for its deployment in real-world applications. Nevertheless, managing the trade-off between reward and safety during exploration presents a significant challenge. Improving reward performance through policy adjustments may adversely affect safety performance. In this study, we aim to address this conflicting relation by leveraging the theory of gradient manipulation. Initially, we analyze the conflict between reward and safety gradients. Subsequently, we tackle the balance between reward and safety optimization by proposing a soft switching policy optimization method, for which we provide convergence analysis. Based on our theoretical examination, we provide a safe RL framework to overcome the aforementioned challenge, and we develop a Safety-MuJoCo Benchmark to assess the performance of safe RL algorithms. Finally, we evaluate the effectiveness of our method on the Safety-MuJoCo Benchmark and a popular safe benchmark, Omnisafe. Experimental results demonstrate that our algorithms outperform several state-of-the-art baselines in terms of balancing reward and safety optimization.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/ff87e30cb969d40b1d5c8ddc8932427793467f29.pdf",
      "citation_key": "gu2024fu3",
      "metadata": {
        "title": "Balance Reward and Safety Optimization for Safe Reinforcement Learning: A Perspective of Gradient Manipulation",
        "authors": [
          "Shangding Gu",
          "Bilgehan Sel",
          "Yuhao Ding",
          "Lu Wang",
          "Qingwei Lin",
          "Ming Jin",
          "Alois Knoll"
        ],
        "published_date": "2024",
        "abstract": "Ensuring the safety of Reinforcement Learning (RL) is crucial for its deployment in real-world applications. Nevertheless, managing the trade-off between reward and safety during exploration presents a significant challenge. Improving reward performance through policy adjustments may adversely affect safety performance. In this study, we aim to address this conflicting relation by leveraging the theory of gradient manipulation. Initially, we analyze the conflict between reward and safety gradients. Subsequently, we tackle the balance between reward and safety optimization by proposing a soft switching policy optimization method, for which we provide convergence analysis. Based on our theoretical examination, we provide a safe RL framework to overcome the aforementioned challenge, and we develop a Safety-MuJoCo Benchmark to assess the performance of safe RL algorithms. Finally, we evaluate the effectiveness of our method on the Safety-MuJoCo Benchmark and a popular safe benchmark, Omnisafe. Experimental results demonstrate that our algorithms outperform several state-of-the-art baselines in terms of balancing reward and safety optimization.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/ff87e30cb969d40b1d5c8ddc8932427793467f29.pdf",
        "venue": "AAAI Conference on Artificial Intelligence",
        "citationCount": 16,
        "score": 16.0,
        "summary": "Ensuring the safety of Reinforcement Learning (RL) is crucial for its deployment in real-world applications. Nevertheless, managing the trade-off between reward and safety during exploration presents a significant challenge. Improving reward performance through policy adjustments may adversely affect safety performance. In this study, we aim to address this conflicting relation by leveraging the theory of gradient manipulation. Initially, we analyze the conflict between reward and safety gradients. Subsequently, we tackle the balance between reward and safety optimization by proposing a soft switching policy optimization method, for which we provide convergence analysis. Based on our theoretical examination, we provide a safe RL framework to overcome the aforementioned challenge, and we develop a Safety-MuJoCo Benchmark to assess the performance of safe RL algorithms. Finally, we evaluate the effectiveness of our method on the Safety-MuJoCo Benchmark and a popular safe benchmark, Omnisafe. Experimental results demonstrate that our algorithms outperform several state-of-the-art baselines in terms of balancing reward and safety optimization.",
        "keywords": []
      },
      "file_name": "ff87e30cb969d40b1d5c8ddc8932427793467f29.pdf"
    },
    {
      "success": true,
      "doc_id": "d96c299958acfe8cf0006e6dec558f8e",
      "summary": "In robot manipulation, Reinforcement Learning (RL) often suffers from low sample efficiency and uncertain convergence, especially in large observation and action spaces. Foundation Models (FMs) offer an alternative, demonstrating promise in zero-shot and few-shot settings. However, they can be unreliable due to limited physical and spatial understanding. We introduce ExploRLLM, a method that combines the strengths of both paradigms. In our approach, FMs improve RL convergence by generating policy code and efficient representations, while a residual RL agent compensates for the FMs' limited physical understanding. We show that Explorllm outperforms both policies derived from FMs and RL baselines in table-top manipulation tasks. Additionally, real-world experiments show that the policies exhibit promising zero-shot sim-to-real transfer. Supplementary material is available at https://explorllm.github.io.",
      "intriguing_abstract": "In robot manipulation, Reinforcement Learning (RL) often suffers from low sample efficiency and uncertain convergence, especially in large observation and action spaces. Foundation Models (FMs) offer an alternative, demonstrating promise in zero-shot and few-shot settings. However, they can be unreliable due to limited physical and spatial understanding. We introduce ExploRLLM, a method that combines the strengths of both paradigms. In our approach, FMs improve RL convergence by generating policy code and efficient representations, while a residual RL agent compensates for the FMs' limited physical understanding. We show that Explorllm outperforms both policies derived from FMs and RL baselines in table-top manipulation tasks. Additionally, real-world experiments show that the policies exhibit promising zero-shot sim-to-real transfer. Supplementary material is available at https://explorllm.github.io.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/aedd5c4bc11d8faf01e8456c91a183f2f76e5778.pdf",
      "citation_key": "ma2024b33",
      "metadata": {
        "title": "ExploRLLM: Guiding Exploration in Reinforcement Learning with Large Language Models",
        "authors": [
          "Runyu Ma",
          "Jelle Luijkx",
          "Zlatan Ajanovi",
          "Jens Kober"
        ],
        "published_date": "2024",
        "abstract": "In robot manipulation, Reinforcement Learning (RL) often suffers from low sample efficiency and uncertain convergence, especially in large observation and action spaces. Foundation Models (FMs) offer an alternative, demonstrating promise in zero-shot and few-shot settings. However, they can be unreliable due to limited physical and spatial understanding. We introduce ExploRLLM, a method that combines the strengths of both paradigms. In our approach, FMs improve RL convergence by generating policy code and efficient representations, while a residual RL agent compensates for the FMs' limited physical understanding. We show that Explorllm outperforms both policies derived from FMs and RL baselines in table-top manipulation tasks. Additionally, real-world experiments show that the policies exhibit promising zero-shot sim-to-real transfer. Supplementary material is available at https://explorllm.github.io.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/aedd5c4bc11d8faf01e8456c91a183f2f76e5778.pdf",
        "venue": "IEEE International Conference on Robotics and Automation",
        "citationCount": 15,
        "score": 15.0,
        "summary": "In robot manipulation, Reinforcement Learning (RL) often suffers from low sample efficiency and uncertain convergence, especially in large observation and action spaces. Foundation Models (FMs) offer an alternative, demonstrating promise in zero-shot and few-shot settings. However, they can be unreliable due to limited physical and spatial understanding. We introduce ExploRLLM, a method that combines the strengths of both paradigms. In our approach, FMs improve RL convergence by generating policy code and efficient representations, while a residual RL agent compensates for the FMs' limited physical understanding. We show that Explorllm outperforms both policies derived from FMs and RL baselines in table-top manipulation tasks. Additionally, real-world experiments show that the policies exhibit promising zero-shot sim-to-real transfer. Supplementary material is available at https://explorllm.github.io.",
        "keywords": []
      },
      "file_name": "aedd5c4bc11d8faf01e8456c91a183f2f76e5778.pdf"
    },
    {
      "success": true,
      "doc_id": "84195b70e1fd83e957bd443242cd8173",
      "summary": "In sequential decision-making (SDM) tasks, methods like reinforcement learning (RL) and heuristic search have made notable advances in specific cases. However, they often require extensive exploration and face challenges in generalizing across diverse environments due to their limited grasp of the underlying decision dynamics. In contrast, large language models (LLMs) have recently emerged as powerful general-purpose tools, due to their capacity to maintain vast amounts of domain-specific knowledge. To harness this rich prior knowledge for efficiently solving complex SDM tasks, we propose treating LLMs as prior action distributions and integrating them into RL frameworks through Bayesian inference methods, making use of variational inference and direct posterior sampling. The proposed approaches facilitate the seamless incorporation of fixed LLM priors into both policy-based and value-based RL frameworks. Our experiments show that incorporating LLM-based action priors significantly reduces exploration and optimization complexity, substantially improving sample efficiency compared to traditional RL techniques, e.g., using LLM priors decreases the number of required samples by over 90% in offline learning scenarios.",
      "intriguing_abstract": "In sequential decision-making (SDM) tasks, methods like reinforcement learning (RL) and heuristic search have made notable advances in specific cases. However, they often require extensive exploration and face challenges in generalizing across diverse environments due to their limited grasp of the underlying decision dynamics. In contrast, large language models (LLMs) have recently emerged as powerful general-purpose tools, due to their capacity to maintain vast amounts of domain-specific knowledge. To harness this rich prior knowledge for efficiently solving complex SDM tasks, we propose treating LLMs as prior action distributions and integrating them into RL frameworks through Bayesian inference methods, making use of variational inference and direct posterior sampling. The proposed approaches facilitate the seamless incorporation of fixed LLM priors into both policy-based and value-based RL frameworks. Our experiments show that incorporating LLM-based action priors significantly reduces exploration and optimization complexity, substantially improving sample efficiency compared to traditional RL techniques, e.g., using LLM priors decreases the number of required samples by over 90% in offline learning scenarios.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/e0bf76edd8e6b793b81c16a915ede48e377ebab6.pdf",
      "citation_key": "yan2024p3y",
      "metadata": {
        "title": "Efficient Reinforcement Learning with Large Language Model Priors",
        "authors": [
          "Xue Yan",
          "Yan Song",
          "Xidong Feng",
          "Mengyue Yang",
          "Haifeng Zhang",
          "H. Ammar",
          "Jun Wang"
        ],
        "published_date": "2024",
        "abstract": "In sequential decision-making (SDM) tasks, methods like reinforcement learning (RL) and heuristic search have made notable advances in specific cases. However, they often require extensive exploration and face challenges in generalizing across diverse environments due to their limited grasp of the underlying decision dynamics. In contrast, large language models (LLMs) have recently emerged as powerful general-purpose tools, due to their capacity to maintain vast amounts of domain-specific knowledge. To harness this rich prior knowledge for efficiently solving complex SDM tasks, we propose treating LLMs as prior action distributions and integrating them into RL frameworks through Bayesian inference methods, making use of variational inference and direct posterior sampling. The proposed approaches facilitate the seamless incorporation of fixed LLM priors into both policy-based and value-based RL frameworks. Our experiments show that incorporating LLM-based action priors significantly reduces exploration and optimization complexity, substantially improving sample efficiency compared to traditional RL techniques, e.g., using LLM priors decreases the number of required samples by over 90% in offline learning scenarios.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/e0bf76edd8e6b793b81c16a915ede48e377ebab6.pdf",
        "venue": "arXiv.org",
        "citationCount": 15,
        "score": 15.0,
        "summary": "In sequential decision-making (SDM) tasks, methods like reinforcement learning (RL) and heuristic search have made notable advances in specific cases. However, they often require extensive exploration and face challenges in generalizing across diverse environments due to their limited grasp of the underlying decision dynamics. In contrast, large language models (LLMs) have recently emerged as powerful general-purpose tools, due to their capacity to maintain vast amounts of domain-specific knowledge. To harness this rich prior knowledge for efficiently solving complex SDM tasks, we propose treating LLMs as prior action distributions and integrating them into RL frameworks through Bayesian inference methods, making use of variational inference and direct posterior sampling. The proposed approaches facilitate the seamless incorporation of fixed LLM priors into both policy-based and value-based RL frameworks. Our experiments show that incorporating LLM-based action priors significantly reduces exploration and optimization complexity, substantially improving sample efficiency compared to traditional RL techniques, e.g., using LLM priors decreases the number of required samples by over 90% in offline learning scenarios.",
        "keywords": []
      },
      "file_name": "e0bf76edd8e6b793b81c16a915ede48e377ebab6.pdf"
    },
    {
      "success": true,
      "doc_id": "74c40e13c2766f1d74bb7c8e7a37425c",
      "summary": "The performance of existing supervised neuron segmentation methods is highly dependent on the number of accurate annotations, especially when applied to large scale electron microscopy (EM) data. By extracting semantic information from unlabeled data, self-supervised methods can improve the performance of downstream tasks, among which the mask image model (MIM) has been widely used due to its simplicity and effectiveness in recovering original information from masked images. However, due to the high degree of structural locality in EM images, as well as the existence of considerable noise, many voxels contain little discriminative information, making MIM pretraining inefficient on the neuron segmentation task. To overcome this challenge, we propose a decision-based MIM that utilizes reinforcement learning (RL) to automatically search for optimal image masking ratio and masking strategy. Due to the vast exploration space, using single-agent RL for voxel prediction is impractical. Therefore, we treat each input patch as an agent with a shared behavior policy, allowing for multi-agent collaboration. Furthermore, this multi-agent model can capture dependencies between voxels, which is beneficial for the downstream segmentation task. Experiments conducted on representative EM datasets demonstrate that our approach has a significant advantage over alternative self-supervised methods on the task of neuron segmentation. Code is available at https://github.com/ydchen0806/dbMiM.",
      "intriguing_abstract": "The performance of existing supervised neuron segmentation methods is highly dependent on the number of accurate annotations, especially when applied to large scale electron microscopy (EM) data. By extracting semantic information from unlabeled data, self-supervised methods can improve the performance of downstream tasks, among which the mask image model (MIM) has been widely used due to its simplicity and effectiveness in recovering original information from masked images. However, due to the high degree of structural locality in EM images, as well as the existence of considerable noise, many voxels contain little discriminative information, making MIM pretraining inefficient on the neuron segmentation task. To overcome this challenge, we propose a decision-based MIM that utilizes reinforcement learning (RL) to automatically search for optimal image masking ratio and masking strategy. Due to the vast exploration space, using single-agent RL for voxel prediction is impractical. Therefore, we treat each input patch as an agent with a shared behavior policy, allowing for multi-agent collaboration. Furthermore, this multi-agent model can capture dependencies between voxels, which is beneficial for the downstream segmentation task. Experiments conducted on representative EM datasets demonstrate that our approach has a significant advantage over alternative self-supervised methods on the task of neuron segmentation. Code is available at https://github.com/ydchen0806/dbMiM.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/5f5e9ec8bcc5e83eefecc0565bdc004f929f4723.pdf",
      "citation_key": "chen2023ymk",
      "metadata": {
        "title": "Self-Supervised Neuron Segmentation with Multi-Agent Reinforcement Learning",
        "authors": [
          "Yinda Chen",
          "Wei Huang",
          "Shenglong Zhou",
          "Qi Chen",
          "Zhiwei Xiong"
        ],
        "published_date": "2023",
        "abstract": "The performance of existing supervised neuron segmentation methods is highly dependent on the number of accurate annotations, especially when applied to large scale electron microscopy (EM) data. By extracting semantic information from unlabeled data, self-supervised methods can improve the performance of downstream tasks, among which the mask image model (MIM) has been widely used due to its simplicity and effectiveness in recovering original information from masked images. However, due to the high degree of structural locality in EM images, as well as the existence of considerable noise, many voxels contain little discriminative information, making MIM pretraining inefficient on the neuron segmentation task. To overcome this challenge, we propose a decision-based MIM that utilizes reinforcement learning (RL) to automatically search for optimal image masking ratio and masking strategy. Due to the vast exploration space, using single-agent RL for voxel prediction is impractical. Therefore, we treat each input patch as an agent with a shared behavior policy, allowing for multi-agent collaboration. Furthermore, this multi-agent model can capture dependencies between voxels, which is beneficial for the downstream segmentation task. Experiments conducted on representative EM datasets demonstrate that our approach has a significant advantage over alternative self-supervised methods on the task of neuron segmentation. Code is available at https://github.com/ydchen0806/dbMiM.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/5f5e9ec8bcc5e83eefecc0565bdc004f929f4723.pdf",
        "venue": "International Joint Conference on Artificial Intelligence",
        "citationCount": 28,
        "score": 14.0,
        "summary": "The performance of existing supervised neuron segmentation methods is highly dependent on the number of accurate annotations, especially when applied to large scale electron microscopy (EM) data. By extracting semantic information from unlabeled data, self-supervised methods can improve the performance of downstream tasks, among which the mask image model (MIM) has been widely used due to its simplicity and effectiveness in recovering original information from masked images. However, due to the high degree of structural locality in EM images, as well as the existence of considerable noise, many voxels contain little discriminative information, making MIM pretraining inefficient on the neuron segmentation task. To overcome this challenge, we propose a decision-based MIM that utilizes reinforcement learning (RL) to automatically search for optimal image masking ratio and masking strategy. Due to the vast exploration space, using single-agent RL for voxel prediction is impractical. Therefore, we treat each input patch as an agent with a shared behavior policy, allowing for multi-agent collaboration. Furthermore, this multi-agent model can capture dependencies between voxels, which is beneficial for the downstream segmentation task. Experiments conducted on representative EM datasets demonstrate that our approach has a significant advantage over alternative self-supervised methods on the task of neuron segmentation. Code is available at https://github.com/ydchen0806/dbMiM.",
        "keywords": []
      },
      "file_name": "5f5e9ec8bcc5e83eefecc0565bdc004f929f4723.pdf"
    },
    {
      "success": true,
      "doc_id": "c0f8024e112ba2d0fb2cf5bd7a882e58",
      "summary": "This work focuses on the sample collection in reinforcement learning (RL), where the interaction with the environment is typically time-consuming and extravagantly expensive. In order to collect samples in a more valuable way, we propose a confidence-based sampling strategy based on the optimal computing budget allocation algorithm (OCBA), which actively allocates the computing efforts to actions with different predictive uncertainties. We estimate the uncertainty with ensembles and generalize them from tabular representations to function approximations. The OCBA-based sampling strategy could be easily integrated into various off-policy RL algorithms, where we take Q-learning, DQN, and SAC as examples to show the incorporation. Besides, we provide the theoretical analysis towards convergence and evaluate the algorithms experimentally. According to the experiments, the incorporated algorithms obtain remarkable gains compared with modern ensemble-based RL algorithms. Note to PractitionersReinforcement learning is a powerful tool for handling sequential decision-making problems, e.g., autonomous driving and robotics control, where the behaviors typically have a long-term effect on future events. However, although RL achieves human-level control in some tasks, it severely suffers from low sample efficiency. Therefore, implementing RL in some practical areas, e.g., healthcare and rescue, is extremely hard due to the requirement of massive samples. This work aims to enhance the exploration of RL by incorporating OCBA, which provides an asymptotically optimal data-collection strategy for simulation-based optimization. Based on ensemble-based uncertainty estimation and OCBA-based action selection, the incorporated RL algorithms show competitive performance on many benchmarks and significantly reduce the sampling efforts during iterations.",
      "intriguing_abstract": "This work focuses on the sample collection in reinforcement learning (RL), where the interaction with the environment is typically time-consuming and extravagantly expensive. In order to collect samples in a more valuable way, we propose a confidence-based sampling strategy based on the optimal computing budget allocation algorithm (OCBA), which actively allocates the computing efforts to actions with different predictive uncertainties. We estimate the uncertainty with ensembles and generalize them from tabular representations to function approximations. The OCBA-based sampling strategy could be easily integrated into various off-policy RL algorithms, where we take Q-learning, DQN, and SAC as examples to show the incorporation. Besides, we provide the theoretical analysis towards convergence and evaluate the algorithms experimentally. According to the experiments, the incorporated algorithms obtain remarkable gains compared with modern ensemble-based RL algorithms. Note to PractitionersReinforcement learning is a powerful tool for handling sequential decision-making problems, e.g., autonomous driving and robotics control, where the behaviors typically have a long-term effect on future events. However, although RL achieves human-level control in some tasks, it severely suffers from low sample efficiency. Therefore, implementing RL in some practical areas, e.g., healthcare and rescue, is extremely hard due to the requirement of massive samples. This work aims to enhance the exploration of RL by incorporating OCBA, which provides an asymptotically optimal data-collection strategy for simulation-based optimization. Based on ensemble-based uncertainty estimation and OCBA-based action selection, the incorporated RL algorithms show competitive performance on many benchmarks and significantly reduce the sampling efforts during iterations.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/d60df0754df6ccb14c563f07f865f391da3cba2d.pdf",
      "citation_key": "li2024drs",
      "metadata": {
        "title": "An OCBA-Based Method for Efficient Sample Collection in Reinforcement Learning",
        "authors": [
          "Kuo Li",
          "Xinze Jin",
          "Qing-Shan Jia",
          "Dongchun Ren",
          "Huaxia Xia"
        ],
        "published_date": "2024",
        "abstract": "This work focuses on the sample collection in reinforcement learning (RL), where the interaction with the environment is typically time-consuming and extravagantly expensive. In order to collect samples in a more valuable way, we propose a confidence-based sampling strategy based on the optimal computing budget allocation algorithm (OCBA), which actively allocates the computing efforts to actions with different predictive uncertainties. We estimate the uncertainty with ensembles and generalize them from tabular representations to function approximations. The OCBA-based sampling strategy could be easily integrated into various off-policy RL algorithms, where we take Q-learning, DQN, and SAC as examples to show the incorporation. Besides, we provide the theoretical analysis towards convergence and evaluate the algorithms experimentally. According to the experiments, the incorporated algorithms obtain remarkable gains compared with modern ensemble-based RL algorithms. Note to PractitionersReinforcement learning is a powerful tool for handling sequential decision-making problems, e.g., autonomous driving and robotics control, where the behaviors typically have a long-term effect on future events. However, although RL achieves human-level control in some tasks, it severely suffers from low sample efficiency. Therefore, implementing RL in some practical areas, e.g., healthcare and rescue, is extremely hard due to the requirement of massive samples. This work aims to enhance the exploration of RL by incorporating OCBA, which provides an asymptotically optimal data-collection strategy for simulation-based optimization. Based on ensemble-based uncertainty estimation and OCBA-based action selection, the incorporated RL algorithms show competitive performance on many benchmarks and significantly reduce the sampling efforts during iterations.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/d60df0754df6ccb14c563f07f865f391da3cba2d.pdf",
        "venue": "IEEE Transactions on Automation Science and Engineering",
        "citationCount": 13,
        "score": 13.0,
        "summary": "This work focuses on the sample collection in reinforcement learning (RL), where the interaction with the environment is typically time-consuming and extravagantly expensive. In order to collect samples in a more valuable way, we propose a confidence-based sampling strategy based on the optimal computing budget allocation algorithm (OCBA), which actively allocates the computing efforts to actions with different predictive uncertainties. We estimate the uncertainty with ensembles and generalize them from tabular representations to function approximations. The OCBA-based sampling strategy could be easily integrated into various off-policy RL algorithms, where we take Q-learning, DQN, and SAC as examples to show the incorporation. Besides, we provide the theoretical analysis towards convergence and evaluate the algorithms experimentally. According to the experiments, the incorporated algorithms obtain remarkable gains compared with modern ensemble-based RL algorithms. Note to PractitionersReinforcement learning is a powerful tool for handling sequential decision-making problems, e.g., autonomous driving and robotics control, where the behaviors typically have a long-term effect on future events. However, although RL achieves human-level control in some tasks, it severely suffers from low sample efficiency. Therefore, implementing RL in some practical areas, e.g., healthcare and rescue, is extremely hard due to the requirement of massive samples. This work aims to enhance the exploration of RL by incorporating OCBA, which provides an asymptotically optimal data-collection strategy for simulation-based optimization. Based on ensemble-based uncertainty estimation and OCBA-based action selection, the incorporated RL algorithms show competitive performance on many benchmarks and significantly reduce the sampling efforts during iterations.",
        "keywords": []
      },
      "file_name": "d60df0754df6ccb14c563f07f865f391da3cba2d.pdf"
    },
    {
      "success": true,
      "doc_id": "5df8664a27689923b654586a30127db7",
      "summary": "Task automation of surgical robot has the potentials to improve surgical efficiency. Recent reinforcement learning (RL) based approaches provide scalable solutions to surgical automation, but typically require extensive data collection to solve a task if no prior knowledge is given. This issue is known as the exploration challenge, which can be alleviated by providing expert demonstrations to an RL agent. Yet, how to make effective use of demonstration data to improve exploration efficiency still remains an open challenge. In this work, we introduce Demonstration-guided EXploration (DEX), an efficient reinforcement learning algorithm that aims to overcome the exploration problem with expert demonstrations for surgical automation. To effectively exploit demonstrations, our method estimates expert-like behaviors with higher values to facilitate productive interactions, and adopts non-parametric regression to enable such guidance at states unobserved in demonstration data. Extensive experiments on 10 surgical manipulation tasks from SurRoL, a comprehensive surgical simulation platform, demonstrate significant improvements in the exploration efficiency and task success rates of our method. Moreover, we also deploy the learned policies to the da Vinci Research Kit (dVRK) platform to show the effectiveness on the real robot. Code is available at https://github.com/med-air/DEX.",
      "intriguing_abstract": "Task automation of surgical robot has the potentials to improve surgical efficiency. Recent reinforcement learning (RL) based approaches provide scalable solutions to surgical automation, but typically require extensive data collection to solve a task if no prior knowledge is given. This issue is known as the exploration challenge, which can be alleviated by providing expert demonstrations to an RL agent. Yet, how to make effective use of demonstration data to improve exploration efficiency still remains an open challenge. In this work, we introduce Demonstration-guided EXploration (DEX), an efficient reinforcement learning algorithm that aims to overcome the exploration problem with expert demonstrations for surgical automation. To effectively exploit demonstrations, our method estimates expert-like behaviors with higher values to facilitate productive interactions, and adopts non-parametric regression to enable such guidance at states unobserved in demonstration data. Extensive experiments on 10 surgical manipulation tasks from SurRoL, a comprehensive surgical simulation platform, demonstrate significant improvements in the exploration efficiency and task success rates of our method. Moreover, we also deploy the learned policies to the da Vinci Research Kit (dVRK) platform to show the effectiveness on the real robot. Code is available at https://github.com/med-air/DEX.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/03c7f9a6b743eb1c9127b8d4552147b2b1a0c1e1.pdf",
      "citation_key": "huang202366f",
      "metadata": {
        "title": "Demonstration-Guided Reinforcement Learning with Efficient Exploration for Task Automation of Surgical Robot",
        "authors": [
          "Tao Huang",
          "Kai Chen",
          "Bin Li",
          "Yunhui Liu",
          "Qingxu Dou"
        ],
        "published_date": "2023",
        "abstract": "Task automation of surgical robot has the potentials to improve surgical efficiency. Recent reinforcement learning (RL) based approaches provide scalable solutions to surgical automation, but typically require extensive data collection to solve a task if no prior knowledge is given. This issue is known as the exploration challenge, which can be alleviated by providing expert demonstrations to an RL agent. Yet, how to make effective use of demonstration data to improve exploration efficiency still remains an open challenge. In this work, we introduce Demonstration-guided EXploration (DEX), an efficient reinforcement learning algorithm that aims to overcome the exploration problem with expert demonstrations for surgical automation. To effectively exploit demonstrations, our method estimates expert-like behaviors with higher values to facilitate productive interactions, and adopts non-parametric regression to enable such guidance at states unobserved in demonstration data. Extensive experiments on 10 surgical manipulation tasks from SurRoL, a comprehensive surgical simulation platform, demonstrate significant improvements in the exploration efficiency and task success rates of our method. Moreover, we also deploy the learned policies to the da Vinci Research Kit (dVRK) platform to show the effectiveness on the real robot. Code is available at https://github.com/med-air/DEX.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/03c7f9a6b743eb1c9127b8d4552147b2b1a0c1e1.pdf",
        "venue": "IEEE International Conference on Robotics and Automation",
        "citationCount": 26,
        "score": 13.0,
        "summary": "Task automation of surgical robot has the potentials to improve surgical efficiency. Recent reinforcement learning (RL) based approaches provide scalable solutions to surgical automation, but typically require extensive data collection to solve a task if no prior knowledge is given. This issue is known as the exploration challenge, which can be alleviated by providing expert demonstrations to an RL agent. Yet, how to make effective use of demonstration data to improve exploration efficiency still remains an open challenge. In this work, we introduce Demonstration-guided EXploration (DEX), an efficient reinforcement learning algorithm that aims to overcome the exploration problem with expert demonstrations for surgical automation. To effectively exploit demonstrations, our method estimates expert-like behaviors with higher values to facilitate productive interactions, and adopts non-parametric regression to enable such guidance at states unobserved in demonstration data. Extensive experiments on 10 surgical manipulation tasks from SurRoL, a comprehensive surgical simulation platform, demonstrate significant improvements in the exploration efficiency and task success rates of our method. Moreover, we also deploy the learned policies to the da Vinci Research Kit (dVRK) platform to show the effectiveness on the real robot. Code is available at https://github.com/med-air/DEX.",
        "keywords": []
      },
      "file_name": "03c7f9a6b743eb1c9127b8d4552147b2b1a0c1e1.pdf"
    },
    {
      "success": true,
      "doc_id": "8e6fa1e2b1edd41b74f8b93ea72be558",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/7bd4edf878976d329f326f3a12675a66cbc075e9.pdf",
      "citation_key": "jin2024035",
      "metadata": {
        "title": "Constrained reinforcement learning with statewise projection: a control barrier function approach",
        "authors": [
          "Xinze Jin",
          "Kuo Li",
          "Qing-Shan Jia"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/7bd4edf878976d329f326f3a12675a66cbc075e9.pdf",
        "venue": "Science China Information Sciences",
        "citationCount": 13,
        "score": 13.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "7bd4edf878976d329f326f3a12675a66cbc075e9.pdf"
    },
    {
      "success": true,
      "doc_id": "abce21c5fdfd57765f87a82653d6186c",
      "summary": "We present a scalable and effective exploration strategy based on Thompson sampling for reinforcement learning (RL). One of the key shortcomings of existing Thompson sampling algorithms is the need to perform a Gaussian approximation of the posterior distribution, which is not a good surrogate in most practical settings. We instead directly sample the Q function from its posterior distribution, by using Langevin Monte Carlo, an efficient type of Markov Chain Monte Carlo (MCMC) method. Our method only needs to perform noisy gradient descent updates to learn the exact posterior distribution of the Q function, which makes our approach easy to deploy in deep RL. We provide a rigorous theoretical analysis for the proposed method and demonstrate that, in the linear Markov decision process (linear MDP) setting, it has a regret bound of $\\tilde{O}(d^{3/2}H^{3/2}\\sqrt{T})$, where $d$ is the dimension of the feature mapping, $H$ is the planning horizon, and $T$ is the total number of steps. We apply this approach to deep RL, by using Adam optimizer to perform gradient updates. Our approach achieves better or similar results compared with state-of-the-art deep RL algorithms on several challenging exploration tasks from the Atari57 suite.",
      "intriguing_abstract": "We present a scalable and effective exploration strategy based on Thompson sampling for reinforcement learning (RL). One of the key shortcomings of existing Thompson sampling algorithms is the need to perform a Gaussian approximation of the posterior distribution, which is not a good surrogate in most practical settings. We instead directly sample the Q function from its posterior distribution, by using Langevin Monte Carlo, an efficient type of Markov Chain Monte Carlo (MCMC) method. Our method only needs to perform noisy gradient descent updates to learn the exact posterior distribution of the Q function, which makes our approach easy to deploy in deep RL. We provide a rigorous theoretical analysis for the proposed method and demonstrate that, in the linear Markov decision process (linear MDP) setting, it has a regret bound of $\\tilde{O}(d^{3/2}H^{3/2}\\sqrt{T})$, where $d$ is the dimension of the feature mapping, $H$ is the planning horizon, and $T$ is the total number of steps. We apply this approach to deep RL, by using Adam optimizer to perform gradient updates. Our approach achieves better or similar results compared with state-of-the-art deep RL algorithms on several challenging exploration tasks from the Atari57 suite.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/c90aa0f206c6fd41c490c142f63f7ba046cae6b7.pdf",
      "citation_key": "ishfaq20235fo",
      "metadata": {
        "title": "Provable and Practical: Efficient Exploration in Reinforcement Learning via Langevin Monte Carlo",
        "authors": [
          "Haque Ishfaq",
          "Qingfeng Lan",
          "Pan Xu",
          "A. Mahmood",
          "Doina Precup",
          "Anima Anandkumar",
          "K. Azizzadenesheli"
        ],
        "published_date": "2023",
        "abstract": "We present a scalable and effective exploration strategy based on Thompson sampling for reinforcement learning (RL). One of the key shortcomings of existing Thompson sampling algorithms is the need to perform a Gaussian approximation of the posterior distribution, which is not a good surrogate in most practical settings. We instead directly sample the Q function from its posterior distribution, by using Langevin Monte Carlo, an efficient type of Markov Chain Monte Carlo (MCMC) method. Our method only needs to perform noisy gradient descent updates to learn the exact posterior distribution of the Q function, which makes our approach easy to deploy in deep RL. We provide a rigorous theoretical analysis for the proposed method and demonstrate that, in the linear Markov decision process (linear MDP) setting, it has a regret bound of $\\tilde{O}(d^{3/2}H^{3/2}\\sqrt{T})$, where $d$ is the dimension of the feature mapping, $H$ is the planning horizon, and $T$ is the total number of steps. We apply this approach to deep RL, by using Adam optimizer to perform gradient updates. Our approach achieves better or similar results compared with state-of-the-art deep RL algorithms on several challenging exploration tasks from the Atari57 suite.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/c90aa0f206c6fd41c490c142f63f7ba046cae6b7.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 24,
        "score": 12.0,
        "summary": "We present a scalable and effective exploration strategy based on Thompson sampling for reinforcement learning (RL). One of the key shortcomings of existing Thompson sampling algorithms is the need to perform a Gaussian approximation of the posterior distribution, which is not a good surrogate in most practical settings. We instead directly sample the Q function from its posterior distribution, by using Langevin Monte Carlo, an efficient type of Markov Chain Monte Carlo (MCMC) method. Our method only needs to perform noisy gradient descent updates to learn the exact posterior distribution of the Q function, which makes our approach easy to deploy in deep RL. We provide a rigorous theoretical analysis for the proposed method and demonstrate that, in the linear Markov decision process (linear MDP) setting, it has a regret bound of $\\tilde{O}(d^{3/2}H^{3/2}\\sqrt{T})$, where $d$ is the dimension of the feature mapping, $H$ is the planning horizon, and $T$ is the total number of steps. We apply this approach to deep RL, by using Adam optimizer to perform gradient updates. Our approach achieves better or similar results compared with state-of-the-art deep RL algorithms on several challenging exploration tasks from the Atari57 suite.",
        "keywords": []
      },
      "file_name": "c90aa0f206c6fd41c490c142f63f7ba046cae6b7.pdf"
    },
    {
      "success": true,
      "doc_id": "26a448ce7b253af0c0028c4dda054060",
      "summary": "Offline reinforcement learning (RL) makes it possible to train the agents entirely from a previously collected dataset. However, constrained by the quality of the offline dataset, offline RL agents typically have limited performance and cannot be directly deployed. Thus, it is desirable to further finetune the pretrained offline RL agents via online interactions with the environment. Existing offline-to-online RL algorithms suffer from the low sample efficiency issue, due to two inherent challenges, i.e., exploration limitation and distribution shift. To this end, we propose a sample-efficient offline-to-online RL algorithm via Optimistic Exploration and Meta Adaptation (OEMA). Specifically, we first propose an optimistic exploration strategy according to the principle of optimism in the face of uncertainty. This allows agents to sufficiently explore the environment in a stable manner. Moreover, we propose a meta learning based adaptation method, which can reduce the distribution shift and accelerate the offline-to-online adaptation process. We empirically demonstrate that OEMA improves the sample efficiency on D4RL benchmark. Besides, we provide in-depth analyses to verify the effectiveness of both optimistic exploration and meta adaptation.",
      "intriguing_abstract": "Offline reinforcement learning (RL) makes it possible to train the agents entirely from a previously collected dataset. However, constrained by the quality of the offline dataset, offline RL agents typically have limited performance and cannot be directly deployed. Thus, it is desirable to further finetune the pretrained offline RL agents via online interactions with the environment. Existing offline-to-online RL algorithms suffer from the low sample efficiency issue, due to two inherent challenges, i.e., exploration limitation and distribution shift. To this end, we propose a sample-efficient offline-to-online RL algorithm via Optimistic Exploration and Meta Adaptation (OEMA). Specifically, we first propose an optimistic exploration strategy according to the principle of optimism in the face of uncertainty. This allows agents to sufficiently explore the environment in a stable manner. Moreover, we propose a meta learning based adaptation method, which can reduce the distribution shift and accelerate the offline-to-online adaptation process. We empirically demonstrate that OEMA improves the sample efficiency on D4RL benchmark. Besides, we provide in-depth analyses to verify the effectiveness of both optimistic exploration and meta adaptation.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/53db22a9d4ae77dd8218ba867184898adc84d1d1.pdf",
      "citation_key": "guo2024sba",
      "metadata": {
        "title": "Sample Efficient Offline-to-Online Reinforcement Learning",
        "authors": [
          "Siyuan Guo",
          "Lixin Zou",
          "Hechang Chen",
          "B. Qu",
          "Haotian Chi",
          "Philip S. Yu",
          "Yi Chang"
        ],
        "published_date": "2024",
        "abstract": "Offline reinforcement learning (RL) makes it possible to train the agents entirely from a previously collected dataset. However, constrained by the quality of the offline dataset, offline RL agents typically have limited performance and cannot be directly deployed. Thus, it is desirable to further finetune the pretrained offline RL agents via online interactions with the environment. Existing offline-to-online RL algorithms suffer from the low sample efficiency issue, due to two inherent challenges, i.e., exploration limitation and distribution shift. To this end, we propose a sample-efficient offline-to-online RL algorithm via Optimistic Exploration and Meta Adaptation (OEMA). Specifically, we first propose an optimistic exploration strategy according to the principle of optimism in the face of uncertainty. This allows agents to sufficiently explore the environment in a stable manner. Moreover, we propose a meta learning based adaptation method, which can reduce the distribution shift and accelerate the offline-to-online adaptation process. We empirically demonstrate that OEMA improves the sample efficiency on D4RL benchmark. Besides, we provide in-depth analyses to verify the effectiveness of both optimistic exploration and meta adaptation.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/53db22a9d4ae77dd8218ba867184898adc84d1d1.pdf",
        "venue": "IEEE Transactions on Knowledge and Data Engineering",
        "citationCount": 12,
        "score": 12.0,
        "summary": "Offline reinforcement learning (RL) makes it possible to train the agents entirely from a previously collected dataset. However, constrained by the quality of the offline dataset, offline RL agents typically have limited performance and cannot be directly deployed. Thus, it is desirable to further finetune the pretrained offline RL agents via online interactions with the environment. Existing offline-to-online RL algorithms suffer from the low sample efficiency issue, due to two inherent challenges, i.e., exploration limitation and distribution shift. To this end, we propose a sample-efficient offline-to-online RL algorithm via Optimistic Exploration and Meta Adaptation (OEMA). Specifically, we first propose an optimistic exploration strategy according to the principle of optimism in the face of uncertainty. This allows agents to sufficiently explore the environment in a stable manner. Moreover, we propose a meta learning based adaptation method, which can reduce the distribution shift and accelerate the offline-to-online adaptation process. We empirically demonstrate that OEMA improves the sample efficiency on D4RL benchmark. Besides, we provide in-depth analyses to verify the effectiveness of both optimistic exploration and meta adaptation.",
        "keywords": []
      },
      "file_name": "53db22a9d4ae77dd8218ba867184898adc84d1d1.pdf"
    },
    {
      "success": true,
      "doc_id": "ec7bdcc6f93321fb9462d2915e9b070e",
      "summary": "Discovering efficient algorithms for solving complex problems has been an outstanding challenge in mathematics and computer science, requiring substantial human expertise over the years. Recent advancements in evolutionary search with large language models (LLMs) have shown promise in accelerating the discovery of algorithms across various domains, particularly in mathematics and optimization. However, existing approaches treat the LLM as a static generator, missing the opportunity to update the model with the signal obtained from evolutionary exploration. In this work, we propose to augment LLM-based evolutionary search by continuously refining the search operator - the LLM - through reinforcement learning (RL) fine-tuning. Our method leverages evolutionary search as an exploration strategy to discover improved algorithms, while RL optimizes the LLM policy based on these discoveries. Our experiments on combinatorial optimization tasks demonstrate that integrating RL with evolutionary search accelerates the discovery of superior algorithms, showcasing the potential of RL-enhanced evolutionary strategies for algorithm design.",
      "intriguing_abstract": "Discovering efficient algorithms for solving complex problems has been an outstanding challenge in mathematics and computer science, requiring substantial human expertise over the years. Recent advancements in evolutionary search with large language models (LLMs) have shown promise in accelerating the discovery of algorithms across various domains, particularly in mathematics and optimization. However, existing approaches treat the LLM as a static generator, missing the opportunity to update the model with the signal obtained from evolutionary exploration. In this work, we propose to augment LLM-based evolutionary search by continuously refining the search operator - the LLM - through reinforcement learning (RL) fine-tuning. Our method leverages evolutionary search as an exploration strategy to discover improved algorithms, while RL optimizes the LLM policy based on these discoveries. Our experiments on combinatorial optimization tasks demonstrate that integrating RL with evolutionary search accelerates the discovery of superior algorithms, showcasing the potential of RL-enhanced evolutionary strategies for algorithm design.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/fbc0b5e1b822796d7ae97268def2e0993b5da644.pdf",
      "citation_key": "surina2025smk",
      "metadata": {
        "title": "Algorithm Discovery With LLMs: Evolutionary Search Meets Reinforcement Learning",
        "authors": [
          "Anja Surina",
          "Amin Mansouri",
          "Lars Quaedvlieg",
          "Amal Seddas",
          "Maryna Viazovska",
          "Emmanuel Abbe",
          "Caglar Gulcehre"
        ],
        "published_date": "2025",
        "abstract": "Discovering efficient algorithms for solving complex problems has been an outstanding challenge in mathematics and computer science, requiring substantial human expertise over the years. Recent advancements in evolutionary search with large language models (LLMs) have shown promise in accelerating the discovery of algorithms across various domains, particularly in mathematics and optimization. However, existing approaches treat the LLM as a static generator, missing the opportunity to update the model with the signal obtained from evolutionary exploration. In this work, we propose to augment LLM-based evolutionary search by continuously refining the search operator - the LLM - through reinforcement learning (RL) fine-tuning. Our method leverages evolutionary search as an exploration strategy to discover improved algorithms, while RL optimizes the LLM policy based on these discoveries. Our experiments on combinatorial optimization tasks demonstrate that integrating RL with evolutionary search accelerates the discovery of superior algorithms, showcasing the potential of RL-enhanced evolutionary strategies for algorithm design.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/fbc0b5e1b822796d7ae97268def2e0993b5da644.pdf",
        "venue": "arXiv.org",
        "citationCount": 12,
        "score": 12.0,
        "summary": "Discovering efficient algorithms for solving complex problems has been an outstanding challenge in mathematics and computer science, requiring substantial human expertise over the years. Recent advancements in evolutionary search with large language models (LLMs) have shown promise in accelerating the discovery of algorithms across various domains, particularly in mathematics and optimization. However, existing approaches treat the LLM as a static generator, missing the opportunity to update the model with the signal obtained from evolutionary exploration. In this work, we propose to augment LLM-based evolutionary search by continuously refining the search operator - the LLM - through reinforcement learning (RL) fine-tuning. Our method leverages evolutionary search as an exploration strategy to discover improved algorithms, while RL optimizes the LLM policy based on these discoveries. Our experiments on combinatorial optimization tasks demonstrate that integrating RL with evolutionary search accelerates the discovery of superior algorithms, showcasing the potential of RL-enhanced evolutionary strategies for algorithm design.",
        "keywords": []
      },
      "file_name": "fbc0b5e1b822796d7ae97268def2e0993b5da644.pdf"
    },
    {
      "success": true,
      "doc_id": "b6cac38273ded9ff9ab1c8deb1459c37",
      "summary": "Maximum entropy reinforcement learning (MaxEnt-RL) has become the standard approach to RL due to its beneficial exploration properties. Traditionally, policies are parameterized using Gaussian distributions, which significantly limits their representational capacity. Diffusion-based policies offer a more expressive alternative, yet integrating them into MaxEnt-RL poses challenges-primarily due to the intractability of computing their marginal entropy. To overcome this, we propose Diffusion-Based Maximum Entropy RL (DIME). \\emph{DIME} leverages recent advances in approximate inference with diffusion models to derive a lower bound on the maximum entropy objective. Additionally, we propose a policy iteration scheme that provably converges to the optimal diffusion policy. Our method enables the use of expressive diffusion-based policies while retaining the principled exploration benefits of MaxEnt-RL, significantly outperforming other diffusion-based methods on challenging high-dimensional control benchmarks. It is also competitive with state-of-the-art non-diffusion based RL methods while requiring fewer algorithmic design choices and smaller update-to-data ratios, reducing computational complexity.",
      "intriguing_abstract": "Maximum entropy reinforcement learning (MaxEnt-RL) has become the standard approach to RL due to its beneficial exploration properties. Traditionally, policies are parameterized using Gaussian distributions, which significantly limits their representational capacity. Diffusion-based policies offer a more expressive alternative, yet integrating them into MaxEnt-RL poses challenges-primarily due to the intractability of computing their marginal entropy. To overcome this, we propose Diffusion-Based Maximum Entropy RL (DIME). \\emph{DIME} leverages recent advances in approximate inference with diffusion models to derive a lower bound on the maximum entropy objective. Additionally, we propose a policy iteration scheme that provably converges to the optimal diffusion policy. Our method enables the use of expressive diffusion-based policies while retaining the principled exploration benefits of MaxEnt-RL, significantly outperforming other diffusion-based methods on challenging high-dimensional control benchmarks. It is also competitive with state-of-the-art non-diffusion based RL methods while requiring fewer algorithmic design choices and smaller update-to-data ratios, reducing computational complexity.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/97c7066b53b208b24001ad0dcc49a851fcf13bfd.pdf",
      "citation_key": "celik202575j",
      "metadata": {
        "title": "DIME:Diffusion-Based Maximum Entropy Reinforcement Learning",
        "authors": [
          "Onur Celik",
          "Zechu Li",
          "Denis Blessing",
          "Ge Li",
          "Daniel Palanicek",
          "Jan Peters",
          "G. Chalvatzaki",
          "Gerhard Neumann"
        ],
        "published_date": "2025",
        "abstract": "Maximum entropy reinforcement learning (MaxEnt-RL) has become the standard approach to RL due to its beneficial exploration properties. Traditionally, policies are parameterized using Gaussian distributions, which significantly limits their representational capacity. Diffusion-based policies offer a more expressive alternative, yet integrating them into MaxEnt-RL poses challenges-primarily due to the intractability of computing their marginal entropy. To overcome this, we propose Diffusion-Based Maximum Entropy RL (DIME). \\emph{DIME} leverages recent advances in approximate inference with diffusion models to derive a lower bound on the maximum entropy objective. Additionally, we propose a policy iteration scheme that provably converges to the optimal diffusion policy. Our method enables the use of expressive diffusion-based policies while retaining the principled exploration benefits of MaxEnt-RL, significantly outperforming other diffusion-based methods on challenging high-dimensional control benchmarks. It is also competitive with state-of-the-art non-diffusion based RL methods while requiring fewer algorithmic design choices and smaller update-to-data ratios, reducing computational complexity.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/97c7066b53b208b24001ad0dcc49a851fcf13bfd.pdf",
        "venue": "arXiv.org",
        "citationCount": 12,
        "score": 12.0,
        "summary": "Maximum entropy reinforcement learning (MaxEnt-RL) has become the standard approach to RL due to its beneficial exploration properties. Traditionally, policies are parameterized using Gaussian distributions, which significantly limits their representational capacity. Diffusion-based policies offer a more expressive alternative, yet integrating them into MaxEnt-RL poses challenges-primarily due to the intractability of computing their marginal entropy. To overcome this, we propose Diffusion-Based Maximum Entropy RL (DIME). \\emph{DIME} leverages recent advances in approximate inference with diffusion models to derive a lower bound on the maximum entropy objective. Additionally, we propose a policy iteration scheme that provably converges to the optimal diffusion policy. Our method enables the use of expressive diffusion-based policies while retaining the principled exploration benefits of MaxEnt-RL, significantly outperforming other diffusion-based methods on challenging high-dimensional control benchmarks. It is also competitive with state-of-the-art non-diffusion based RL methods while requiring fewer algorithmic design choices and smaller update-to-data ratios, reducing computational complexity.",
        "keywords": []
      },
      "file_name": "97c7066b53b208b24001ad0dcc49a851fcf13bfd.pdf"
    },
    {
      "success": true,
      "doc_id": "c90fbf68e2018e7a82b212868f528c04",
      "summary": "In this article, a novel reinforcement learning (RL)-based robust control approach is proposed for quadrotors, which guarantees efficient learning and satisfactory tracking performance by simultaneously evaluating the RL and the baseline method in training. Different from existing works, the key novelty is to design a practice-reliable RL control framework for quadrotors in a two-part cooperative manner. In the first part, based on the hierarchical property, a new robust integral of the signum of the error (RISE) design is proposed to ensure asymptotic convergence, which includes the nonlinear and the disturbance rejection terms. In the second part, a one-actor-dual-critic (OADC) learning framework is proposed, where the designed switching logic in the first part works as a benchmark to guide the learning. Specifically, the two critics independently evaluate the RL policy and the switching logic simultaneously, which are utilized for policy update, only when both are positive, corresponding to the remarkable actor-better exploration actions. The asymptotic RISE controller, together with the two critics in OADC learning framework, guarantees accurate judgment on every exploration. On this basis, the satisfactory performance of the RL policy is guaranteed by the actor-better exploration based learning while the chattering problem arisen from the switching logic is addressed completely. Plenty of comparative experimental tests are presented to illustrate the superior performance of the proposed RL controller in terms of tracking accuracy and robustness.",
      "intriguing_abstract": "In this article, a novel reinforcement learning (RL)-based robust control approach is proposed for quadrotors, which guarantees efficient learning and satisfactory tracking performance by simultaneously evaluating the RL and the baseline method in training. Different from existing works, the key novelty is to design a practice-reliable RL control framework for quadrotors in a two-part cooperative manner. In the first part, based on the hierarchical property, a new robust integral of the signum of the error (RISE) design is proposed to ensure asymptotic convergence, which includes the nonlinear and the disturbance rejection terms. In the second part, a one-actor-dual-critic (OADC) learning framework is proposed, where the designed switching logic in the first part works as a benchmark to guide the learning. Specifically, the two critics independently evaluate the RL policy and the switching logic simultaneously, which are utilized for policy update, only when both are positive, corresponding to the remarkable actor-better exploration actions. The asymptotic RISE controller, together with the two critics in OADC learning framework, guarantees accurate judgment on every exploration. On this basis, the satisfactory performance of the RL policy is guaranteed by the actor-better exploration based learning while the chattering problem arisen from the switching logic is addressed completely. Plenty of comparative experimental tests are presented to illustrate the superior performance of the proposed RL controller in terms of tracking accuracy and robustness.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/81fdf2b3db2b3e7be90b51866a31b73587eecd30.pdf",
      "citation_key": "hua2023omp",
      "metadata": {
        "title": "A Novel Reinforcement Learning-Based Robust Control Strategy for a Quadrotor",
        "authors": [
          "Hean Hua",
          "Yongchun Fang"
        ],
        "published_date": "2023",
        "abstract": "In this article, a novel reinforcement learning (RL)-based robust control approach is proposed for quadrotors, which guarantees efficient learning and satisfactory tracking performance by simultaneously evaluating the RL and the baseline method in training. Different from existing works, the key novelty is to design a practice-reliable RL control framework for quadrotors in a two-part cooperative manner. In the first part, based on the hierarchical property, a new robust integral of the signum of the error (RISE) design is proposed to ensure asymptotic convergence, which includes the nonlinear and the disturbance rejection terms. In the second part, a one-actor-dual-critic (OADC) learning framework is proposed, where the designed switching logic in the first part works as a benchmark to guide the learning. Specifically, the two critics independently evaluate the RL policy and the switching logic simultaneously, which are utilized for policy update, only when both are positive, corresponding to the remarkable actor-better exploration actions. The asymptotic RISE controller, together with the two critics in OADC learning framework, guarantees accurate judgment on every exploration. On this basis, the satisfactory performance of the RL policy is guaranteed by the actor-better exploration based learning while the chattering problem arisen from the switching logic is addressed completely. Plenty of comparative experimental tests are presented to illustrate the superior performance of the proposed RL controller in terms of tracking accuracy and robustness.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/81fdf2b3db2b3e7be90b51866a31b73587eecd30.pdf",
        "venue": "IEEE transactions on industrial electronics (1982. Print)",
        "citationCount": 23,
        "score": 11.5,
        "summary": "In this article, a novel reinforcement learning (RL)-based robust control approach is proposed for quadrotors, which guarantees efficient learning and satisfactory tracking performance by simultaneously evaluating the RL and the baseline method in training. Different from existing works, the key novelty is to design a practice-reliable RL control framework for quadrotors in a two-part cooperative manner. In the first part, based on the hierarchical property, a new robust integral of the signum of the error (RISE) design is proposed to ensure asymptotic convergence, which includes the nonlinear and the disturbance rejection terms. In the second part, a one-actor-dual-critic (OADC) learning framework is proposed, where the designed switching logic in the first part works as a benchmark to guide the learning. Specifically, the two critics independently evaluate the RL policy and the switching logic simultaneously, which are utilized for policy update, only when both are positive, corresponding to the remarkable actor-better exploration actions. The asymptotic RISE controller, together with the two critics in OADC learning framework, guarantees accurate judgment on every exploration. On this basis, the satisfactory performance of the RL policy is guaranteed by the actor-better exploration based learning while the chattering problem arisen from the switching logic is addressed completely. Plenty of comparative experimental tests are presented to illustrate the superior performance of the proposed RL controller in terms of tracking accuracy and robustness.",
        "keywords": []
      },
      "file_name": "81fdf2b3db2b3e7be90b51866a31b73587eecd30.pdf"
    },
    {
      "success": true,
      "doc_id": "74552fc7fbf7e4b6221b2ff106816593",
      "summary": "Reinforcement learning (RL) algorithms aim to balance exploiting the current best strategy with exploring new options that could lead to higher rewards. Most common RL algorithms use undirected exploration, i.e., select random sequences of actions. Exploration can also be directed using intrinsic rewards, such as curiosity or model epistemic uncertainty. However, effectively balancing task and intrinsic rewards is challenging and often task-dependent. In this work, we introduce a framework, MaxInfoRL, for balancing intrinsic and extrinsic exploration. MaxInfoRL steers exploration towards informative transitions, by maximizing intrinsic rewards such as the information gain about the underlying task. When combined with Boltzmann exploration, this approach naturally trades off maximization of the value function with that of the entropy over states, rewards, and actions. We show that our approach achieves sublinear regret in the simplified setting of multi-armed bandits. We then apply this general formulation to a variety of off-policy model-free RL methods for continuous state-action spaces, yielding novel algorithms that achieve superior performance across hard exploration problems and complex scenarios such as visual control tasks.",
      "intriguing_abstract": "Reinforcement learning (RL) algorithms aim to balance exploiting the current best strategy with exploring new options that could lead to higher rewards. Most common RL algorithms use undirected exploration, i.e., select random sequences of actions. Exploration can also be directed using intrinsic rewards, such as curiosity or model epistemic uncertainty. However, effectively balancing task and intrinsic rewards is challenging and often task-dependent. In this work, we introduce a framework, MaxInfoRL, for balancing intrinsic and extrinsic exploration. MaxInfoRL steers exploration towards informative transitions, by maximizing intrinsic rewards such as the information gain about the underlying task. When combined with Boltzmann exploration, this approach naturally trades off maximization of the value function with that of the entropy over states, rewards, and actions. We show that our approach achieves sublinear regret in the simplified setting of multi-armed bandits. We then apply this general formulation to a variety of off-policy model-free RL methods for continuous state-action spaces, yielding novel algorithms that achieve superior performance across hard exploration problems and complex scenarios such as visual control tasks.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/e4fef8d5864c5468100ca167639ef3fa374c0442.pdf",
      "citation_key": "sukhija2024zz8",
      "metadata": {
        "title": "MaxInfoRL: Boosting exploration in reinforcement learning through information gain maximization",
        "authors": [
          "Bhavya Sukhija",
          "Stelian Coros",
          "Andreas Krause",
          "Pieter Abbeel",
          "Carmelo Sferrazza"
        ],
        "published_date": "2024",
        "abstract": "Reinforcement learning (RL) algorithms aim to balance exploiting the current best strategy with exploring new options that could lead to higher rewards. Most common RL algorithms use undirected exploration, i.e., select random sequences of actions. Exploration can also be directed using intrinsic rewards, such as curiosity or model epistemic uncertainty. However, effectively balancing task and intrinsic rewards is challenging and often task-dependent. In this work, we introduce a framework, MaxInfoRL, for balancing intrinsic and extrinsic exploration. MaxInfoRL steers exploration towards informative transitions, by maximizing intrinsic rewards such as the information gain about the underlying task. When combined with Boltzmann exploration, this approach naturally trades off maximization of the value function with that of the entropy over states, rewards, and actions. We show that our approach achieves sublinear regret in the simplified setting of multi-armed bandits. We then apply this general formulation to a variety of off-policy model-free RL methods for continuous state-action spaces, yielding novel algorithms that achieve superior performance across hard exploration problems and complex scenarios such as visual control tasks.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/e4fef8d5864c5468100ca167639ef3fa374c0442.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 11,
        "score": 11.0,
        "summary": "Reinforcement learning (RL) algorithms aim to balance exploiting the current best strategy with exploring new options that could lead to higher rewards. Most common RL algorithms use undirected exploration, i.e., select random sequences of actions. Exploration can also be directed using intrinsic rewards, such as curiosity or model epistemic uncertainty. However, effectively balancing task and intrinsic rewards is challenging and often task-dependent. In this work, we introduce a framework, MaxInfoRL, for balancing intrinsic and extrinsic exploration. MaxInfoRL steers exploration towards informative transitions, by maximizing intrinsic rewards such as the information gain about the underlying task. When combined with Boltzmann exploration, this approach naturally trades off maximization of the value function with that of the entropy over states, rewards, and actions. We show that our approach achieves sublinear regret in the simplified setting of multi-armed bandits. We then apply this general formulation to a variety of off-policy model-free RL methods for continuous state-action spaces, yielding novel algorithms that achieve superior performance across hard exploration problems and complex scenarios such as visual control tasks.",
        "keywords": []
      },
      "file_name": "e4fef8d5864c5468100ca167639ef3fa374c0442.pdf"
    },
    {
      "success": true,
      "doc_id": "df740e683a288c4f07acf50d8fbd41b0",
      "summary": "We present the first study on provably efficient randomized exploration in cooperative multi-agent reinforcement learning (MARL). We propose a unified algorithm framework for randomized exploration in parallel Markov Decision Processes (MDPs), and two Thompson Sampling (TS)-type algorithms, CoopTS-PHE and CoopTS-LMC, incorporating the perturbed-history exploration (PHE) strategy and the Langevin Monte Carlo exploration (LMC) strategy, respectively, which are flexible in design and easy to implement in practice. For a special class of parallel MDPs where the transition is (approximately) linear, we theoretically prove that both CoopTS-PHE and CoopTS-LMC achieve a $\\widetilde{\\mathcal{O}}(d^{3/2}H^2\\sqrt{MK})$ regret bound with communication complexity $\\widetilde{\\mathcal{O}}(dHM^2)$, where $d$ is the feature dimension, $H$ is the horizon length, $M$ is the number of agents, and $K$ is the number of episodes. This is the first theoretical result for randomized exploration in cooperative MARL. We evaluate our proposed method on multiple parallel RL environments, including a deep exploration problem (i.e., $N$-chain), a video game, and a real-world problem in energy systems. Our experimental results support that our framework can achieve better performance, even under conditions of misspecified transition models. Additionally, we establish a connection between our unified framework and the practical application of federated learning.",
      "intriguing_abstract": "We present the first study on provably efficient randomized exploration in cooperative multi-agent reinforcement learning (MARL). We propose a unified algorithm framework for randomized exploration in parallel Markov Decision Processes (MDPs), and two Thompson Sampling (TS)-type algorithms, CoopTS-PHE and CoopTS-LMC, incorporating the perturbed-history exploration (PHE) strategy and the Langevin Monte Carlo exploration (LMC) strategy, respectively, which are flexible in design and easy to implement in practice. For a special class of parallel MDPs where the transition is (approximately) linear, we theoretically prove that both CoopTS-PHE and CoopTS-LMC achieve a $\\widetilde{\\mathcal{O}}(d^{3/2}H^2\\sqrt{MK})$ regret bound with communication complexity $\\widetilde{\\mathcal{O}}(dHM^2)$, where $d$ is the feature dimension, $H$ is the horizon length, $M$ is the number of agents, and $K$ is the number of episodes. This is the first theoretical result for randomized exploration in cooperative MARL. We evaluate our proposed method on multiple parallel RL environments, including a deep exploration problem (i.e., $N$-chain), a video game, and a real-world problem in energy systems. Our experimental results support that our framework can achieve better performance, even under conditions of misspecified transition models. Additionally, we establish a connection between our unified framework and the practical application of federated learning.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/7da3eebf5c2f481bc38ae6115cec3fc71dc87f2b.pdf",
      "citation_key": "hsu2024tqd",
      "metadata": {
        "title": "Randomized Exploration in Cooperative Multi-Agent Reinforcement Learning",
        "authors": [
          "Hao-Lun Hsu",
          "Weixin Wang",
          "Miroslav Pajic",
          "Pan Xu"
        ],
        "published_date": "2024",
        "abstract": "We present the first study on provably efficient randomized exploration in cooperative multi-agent reinforcement learning (MARL). We propose a unified algorithm framework for randomized exploration in parallel Markov Decision Processes (MDPs), and two Thompson Sampling (TS)-type algorithms, CoopTS-PHE and CoopTS-LMC, incorporating the perturbed-history exploration (PHE) strategy and the Langevin Monte Carlo exploration (LMC) strategy, respectively, which are flexible in design and easy to implement in practice. For a special class of parallel MDPs where the transition is (approximately) linear, we theoretically prove that both CoopTS-PHE and CoopTS-LMC achieve a $\\widetilde{\\mathcal{O}}(d^{3/2}H^2\\sqrt{MK})$ regret bound with communication complexity $\\widetilde{\\mathcal{O}}(dHM^2)$, where $d$ is the feature dimension, $H$ is the horizon length, $M$ is the number of agents, and $K$ is the number of episodes. This is the first theoretical result for randomized exploration in cooperative MARL. We evaluate our proposed method on multiple parallel RL environments, including a deep exploration problem (i.e., $N$-chain), a video game, and a real-world problem in energy systems. Our experimental results support that our framework can achieve better performance, even under conditions of misspecified transition models. Additionally, we establish a connection between our unified framework and the practical application of federated learning.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/7da3eebf5c2f481bc38ae6115cec3fc71dc87f2b.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 11,
        "score": 11.0,
        "summary": "We present the first study on provably efficient randomized exploration in cooperative multi-agent reinforcement learning (MARL). We propose a unified algorithm framework for randomized exploration in parallel Markov Decision Processes (MDPs), and two Thompson Sampling (TS)-type algorithms, CoopTS-PHE and CoopTS-LMC, incorporating the perturbed-history exploration (PHE) strategy and the Langevin Monte Carlo exploration (LMC) strategy, respectively, which are flexible in design and easy to implement in practice. For a special class of parallel MDPs where the transition is (approximately) linear, we theoretically prove that both CoopTS-PHE and CoopTS-LMC achieve a $\\widetilde{\\mathcal{O}}(d^{3/2}H^2\\sqrt{MK})$ regret bound with communication complexity $\\widetilde{\\mathcal{O}}(dHM^2)$, where $d$ is the feature dimension, $H$ is the horizon length, $M$ is the number of agents, and $K$ is the number of episodes. This is the first theoretical result for randomized exploration in cooperative MARL. We evaluate our proposed method on multiple parallel RL environments, including a deep exploration problem (i.e., $N$-chain), a video game, and a real-world problem in energy systems. Our experimental results support that our framework can achieve better performance, even under conditions of misspecified transition models. Additionally, we establish a connection between our unified framework and the practical application of federated learning.",
        "keywords": []
      },
      "file_name": "7da3eebf5c2f481bc38ae6115cec3fc71dc87f2b.pdf"
    },
    {
      "success": true,
      "doc_id": "6acf1062b074d91201419201afe4759a",
      "summary": "Efficient yet sufficient exploration remains a critical challenge in reinforcement learning (RL), especially for Markov Decision Processes (MDPs) with vast action spaces. Previous approaches have commonly involved projecting the original action space into a latent space or employing environmental action masks to reduce the action possibilities. Nevertheless, these methods often lack interpretability or rely on expert knowledge. In this study, we introduce a novel method for automatically reducing the action space in environments with discrete action spaces while preserving interpretability. The proposed approach learns state-specific masks with a dual purpose: (1) eliminating actions with minimal influence on the MDP and (2) aggregating actions with identical behavioral consequences within the MDP. Specifically, we introduce a novel concept called Bisimulation Metrics on Actions by States (BMAS) to quantify the behavioral consequences of actions within the MDP and design a dedicated mask model to ensure their binary nature. Crucially, we present a practical learning procedure for training the mask model, leveraging transition data collected by any RL policy. Our method is designed to be plug-and-play and adaptable to all RL policies, and to validate its effectiveness, an integration into two prominent RL algorithms, DQN and PPO, is performed. Experimental results obtained from Maze, Atari, and RTS2 reveal a substantial acceleration in the RL learning process and noteworthy performance improvements facilitated by the introduced approach.",
      "intriguing_abstract": "Efficient yet sufficient exploration remains a critical challenge in reinforcement learning (RL), especially for Markov Decision Processes (MDPs) with vast action spaces. Previous approaches have commonly involved projecting the original action space into a latent space or employing environmental action masks to reduce the action possibilities. Nevertheless, these methods often lack interpretability or rely on expert knowledge. In this study, we introduce a novel method for automatically reducing the action space in environments with discrete action spaces while preserving interpretability. The proposed approach learns state-specific masks with a dual purpose: (1) eliminating actions with minimal influence on the MDP and (2) aggregating actions with identical behavioral consequences within the MDP. Specifically, we introduce a novel concept called Bisimulation Metrics on Actions by States (BMAS) to quantify the behavioral consequences of actions within the MDP and design a dedicated mask model to ensure their binary nature. Crucially, we present a practical learning procedure for training the mask model, leveraging transition data collected by any RL policy. Our method is designed to be plug-and-play and adaptable to all RL policies, and to validate its effectiveness, an integration into two prominent RL algorithms, DQN and PPO, is performed. Experimental results obtained from Maze, Atari, and RTS2 reveal a substantial acceleration in the RL learning process and noteworthy performance improvements facilitated by the introduced approach.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/2e6534fbeb932fc058b9f865c0b25a3f201a0704.pdf",
      "citation_key": "wang20248rm",
      "metadata": {
        "title": "Learning State-Specific Action Masks for Reinforcement Learning",
        "authors": [
          "Ziyi Wang",
          "Xinran Li",
          "Luoyang Sun",
          "Haifeng Zhang",
          "Hualin Liu",
          "Jun Wang"
        ],
        "published_date": "2024",
        "abstract": "Efficient yet sufficient exploration remains a critical challenge in reinforcement learning (RL), especially for Markov Decision Processes (MDPs) with vast action spaces. Previous approaches have commonly involved projecting the original action space into a latent space or employing environmental action masks to reduce the action possibilities. Nevertheless, these methods often lack interpretability or rely on expert knowledge. In this study, we introduce a novel method for automatically reducing the action space in environments with discrete action spaces while preserving interpretability. The proposed approach learns state-specific masks with a dual purpose: (1) eliminating actions with minimal influence on the MDP and (2) aggregating actions with identical behavioral consequences within the MDP. Specifically, we introduce a novel concept called Bisimulation Metrics on Actions by States (BMAS) to quantify the behavioral consequences of actions within the MDP and design a dedicated mask model to ensure their binary nature. Crucially, we present a practical learning procedure for training the mask model, leveraging transition data collected by any RL policy. Our method is designed to be plug-and-play and adaptable to all RL policies, and to validate its effectiveness, an integration into two prominent RL algorithms, DQN and PPO, is performed. Experimental results obtained from Maze, Atari, and RTS2 reveal a substantial acceleration in the RL learning process and noteworthy performance improvements facilitated by the introduced approach.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/2e6534fbeb932fc058b9f865c0b25a3f201a0704.pdf",
        "venue": "Algorithms",
        "citationCount": 11,
        "score": 11.0,
        "summary": "Efficient yet sufficient exploration remains a critical challenge in reinforcement learning (RL), especially for Markov Decision Processes (MDPs) with vast action spaces. Previous approaches have commonly involved projecting the original action space into a latent space or employing environmental action masks to reduce the action possibilities. Nevertheless, these methods often lack interpretability or rely on expert knowledge. In this study, we introduce a novel method for automatically reducing the action space in environments with discrete action spaces while preserving interpretability. The proposed approach learns state-specific masks with a dual purpose: (1) eliminating actions with minimal influence on the MDP and (2) aggregating actions with identical behavioral consequences within the MDP. Specifically, we introduce a novel concept called Bisimulation Metrics on Actions by States (BMAS) to quantify the behavioral consequences of actions within the MDP and design a dedicated mask model to ensure their binary nature. Crucially, we present a practical learning procedure for training the mask model, leveraging transition data collected by any RL policy. Our method is designed to be plug-and-play and adaptable to all RL policies, and to validate its effectiveness, an integration into two prominent RL algorithms, DQN and PPO, is performed. Experimental results obtained from Maze, Atari, and RTS2 reveal a substantial acceleration in the RL learning process and noteworthy performance improvements facilitated by the introduced approach.",
        "keywords": []
      },
      "file_name": "2e6534fbeb932fc058b9f865c0b25a3f201a0704.pdf"
    },
    {
      "success": true,
      "doc_id": "f263b3ddcf43bfe7b5e8f5e1db9c8973",
      "summary": "The DC/DC Boost converter exhibits a non-minimum phase system with a right half-plane zero structure, posing significant challenges for the design of effective control approaches. This article presents the design of a robust Proportional-Integral (PI) controller for this converter with an online adaptive mechanism based on the Reinforcement-Learning (RL) strategy. Classical PI controllers are simple and easy to build, but they need to be more robust against a wide range of disturbances and more adaptable to operational parameters. To address these issues, the RL adaptive strategy is used to optimize the performance of the PI controller. Some of the main advantages of the RL are lower sensitivity to error, more reliable results through the collection of data from the environment, an ideal model behavior within a specific context, and better frequency matching in real-time applications. Random exploration, nevertheless, can result in disastrous outcomes and surprising performance in real-world settings. Therefore, we opt for the Deterministic Policy Gradient (DPG) technique, which employs a deterministic action function as opposed to a stochastic one. DPG combines the benefits of actor-critics, deep Q-networks, and the deterministic policy gradient method. In addition, this method adopts the Snake Optimization (SO) algorithm to optimize the initial condition of gains, yielding more reliable results with faster dynamics. The SO method is known for its disciplined and nature-inspired approach, which results in faster decision-making and greater accuracy compared to other optimization algorithms. A structure using a hardware setup with CONTROLLINO MAXI Automation is built, which offers a more cost-effective and precise measurement method. Finally, the results achieved by simulations and experiments demonstrate the robustness of this approach.",
      "intriguing_abstract": "The DC/DC Boost converter exhibits a non-minimum phase system with a right half-plane zero structure, posing significant challenges for the design of effective control approaches. This article presents the design of a robust Proportional-Integral (PI) controller for this converter with an online adaptive mechanism based on the Reinforcement-Learning (RL) strategy. Classical PI controllers are simple and easy to build, but they need to be more robust against a wide range of disturbances and more adaptable to operational parameters. To address these issues, the RL adaptive strategy is used to optimize the performance of the PI controller. Some of the main advantages of the RL are lower sensitivity to error, more reliable results through the collection of data from the environment, an ideal model behavior within a specific context, and better frequency matching in real-time applications. Random exploration, nevertheless, can result in disastrous outcomes and surprising performance in real-world settings. Therefore, we opt for the Deterministic Policy Gradient (DPG) technique, which employs a deterministic action function as opposed to a stochastic one. DPG combines the benefits of actor-critics, deep Q-networks, and the deterministic policy gradient method. In addition, this method adopts the Snake Optimization (SO) algorithm to optimize the initial condition of gains, yielding more reliable results with faster dynamics. The SO method is known for its disciplined and nature-inspired approach, which results in faster decision-making and greater accuracy compared to other optimization algorithms. A structure using a hardware setup with CONTROLLINO MAXI Automation is built, which offers a more cost-effective and precise measurement method. Finally, the results achieved by simulations and experiments demonstrate the robustness of this approach.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/01936f6df3c760d23df237d8d15cb7faadce9520.pdf",
      "citation_key": "ghamari2024bbm",
      "metadata": {
        "title": "Design of an Adaptive Robust PI Controller for DC/DC Boost Converter Using Reinforcement-Learning Technique and Snake Optimization Algorithm",
        "authors": [
          "S. Ghamari",
          "Mojtaba Hajihosseini",
          "D. Habibi",
          "Asma Aziz"
        ],
        "published_date": "2024",
        "abstract": "The DC/DC Boost converter exhibits a non-minimum phase system with a right half-plane zero structure, posing significant challenges for the design of effective control approaches. This article presents the design of a robust Proportional-Integral (PI) controller for this converter with an online adaptive mechanism based on the Reinforcement-Learning (RL) strategy. Classical PI controllers are simple and easy to build, but they need to be more robust against a wide range of disturbances and more adaptable to operational parameters. To address these issues, the RL adaptive strategy is used to optimize the performance of the PI controller. Some of the main advantages of the RL are lower sensitivity to error, more reliable results through the collection of data from the environment, an ideal model behavior within a specific context, and better frequency matching in real-time applications. Random exploration, nevertheless, can result in disastrous outcomes and surprising performance in real-world settings. Therefore, we opt for the Deterministic Policy Gradient (DPG) technique, which employs a deterministic action function as opposed to a stochastic one. DPG combines the benefits of actor-critics, deep Q-networks, and the deterministic policy gradient method. In addition, this method adopts the Snake Optimization (SO) algorithm to optimize the initial condition of gains, yielding more reliable results with faster dynamics. The SO method is known for its disciplined and nature-inspired approach, which results in faster decision-making and greater accuracy compared to other optimization algorithms. A structure using a hardware setup with CONTROLLINO MAXI Automation is built, which offers a more cost-effective and precise measurement method. Finally, the results achieved by simulations and experiments demonstrate the robustness of this approach.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/01936f6df3c760d23df237d8d15cb7faadce9520.pdf",
        "venue": "IEEE Access",
        "citationCount": 11,
        "score": 11.0,
        "summary": "The DC/DC Boost converter exhibits a non-minimum phase system with a right half-plane zero structure, posing significant challenges for the design of effective control approaches. This article presents the design of a robust Proportional-Integral (PI) controller for this converter with an online adaptive mechanism based on the Reinforcement-Learning (RL) strategy. Classical PI controllers are simple and easy to build, but they need to be more robust against a wide range of disturbances and more adaptable to operational parameters. To address these issues, the RL adaptive strategy is used to optimize the performance of the PI controller. Some of the main advantages of the RL are lower sensitivity to error, more reliable results through the collection of data from the environment, an ideal model behavior within a specific context, and better frequency matching in real-time applications. Random exploration, nevertheless, can result in disastrous outcomes and surprising performance in real-world settings. Therefore, we opt for the Deterministic Policy Gradient (DPG) technique, which employs a deterministic action function as opposed to a stochastic one. DPG combines the benefits of actor-critics, deep Q-networks, and the deterministic policy gradient method. In addition, this method adopts the Snake Optimization (SO) algorithm to optimize the initial condition of gains, yielding more reliable results with faster dynamics. The SO method is known for its disciplined and nature-inspired approach, which results in faster decision-making and greater accuracy compared to other optimization algorithms. A structure using a hardware setup with CONTROLLINO MAXI Automation is built, which offers a more cost-effective and precise measurement method. Finally, the results achieved by simulations and experiments demonstrate the robustness of this approach.",
        "keywords": []
      },
      "file_name": "01936f6df3c760d23df237d8d15cb7faadce9520.pdf"
    },
    {
      "success": true,
      "doc_id": "df014b8c273332789c8ca2f10b2f77fa",
      "summary": "Safety is one of the critical challenges in the autonomous driving task. Recent works address the safety by implementing a safe reinforcement learning (safe RL) mechanism. However, most approaches make conservative decisions without knowing the confidence of the actions, which ultimately causes traffic congestion and low travel efficiency. This paper proposes an uncertainty-augmented Lagrangian safe reinforcement algorithm (Lag-U) to improve exploration and safety performance for autonomous driving. First, epistemic uncertainty is introduced into safe RL by using deep ensemble. We use the estimated epistemic uncertainty to encourage exploration and to learn a risk-sensitive policy by adaptively modifying safety constraints. Second, we facilitate an intervention assurance to choose safer actions based on the quantified epistemic uncertainty during deployment. Experimental results prove that the proposed method outperforms other safe RL baselines. The trained vehicle can make a decent trade-off between high efficiency and avoiding risks, thus preventing ultra-conservative policy.",
      "intriguing_abstract": "Safety is one of the critical challenges in the autonomous driving task. Recent works address the safety by implementing a safe reinforcement learning (safe RL) mechanism. However, most approaches make conservative decisions without knowing the confidence of the actions, which ultimately causes traffic congestion and low travel efficiency. This paper proposes an uncertainty-augmented Lagrangian safe reinforcement algorithm (Lag-U) to improve exploration and safety performance for autonomous driving. First, epistemic uncertainty is introduced into safe RL by using deep ensemble. We use the estimated epistemic uncertainty to encourage exploration and to learn a risk-sensitive policy by adaptively modifying safety constraints. Second, we facilitate an intervention assurance to choose safer actions based on the quantified epistemic uncertainty during deployment. Experimental results prove that the proposed method outperforms other safe RL baselines. The trained vehicle can make a decent trade-off between high efficiency and avoiding risks, thus preventing ultra-conservative policy.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/c225c6e24526c160bbceaaa36b447df9d8e95f13.pdf",
      "citation_key": "zhang2024ppn",
      "metadata": {
        "title": "Safe Reinforcement Learning in Autonomous Driving With Epistemic Uncertainty Estimation",
        "authors": [
          "Zhengran Zhang",
          "Qi Liu",
          "Yanjie Li",
          "Ke Lin",
          "Linyu Li"
        ],
        "published_date": "2024",
        "abstract": "Safety is one of the critical challenges in the autonomous driving task. Recent works address the safety by implementing a safe reinforcement learning (safe RL) mechanism. However, most approaches make conservative decisions without knowing the confidence of the actions, which ultimately causes traffic congestion and low travel efficiency. This paper proposes an uncertainty-augmented Lagrangian safe reinforcement algorithm (Lag-U) to improve exploration and safety performance for autonomous driving. First, epistemic uncertainty is introduced into safe RL by using deep ensemble. We use the estimated epistemic uncertainty to encourage exploration and to learn a risk-sensitive policy by adaptively modifying safety constraints. Second, we facilitate an intervention assurance to choose safer actions based on the quantified epistemic uncertainty during deployment. Experimental results prove that the proposed method outperforms other safe RL baselines. The trained vehicle can make a decent trade-off between high efficiency and avoiding risks, thus preventing ultra-conservative policy.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/c225c6e24526c160bbceaaa36b447df9d8e95f13.pdf",
        "venue": "IEEE transactions on intelligent transportation systems (Print)",
        "citationCount": 11,
        "score": 11.0,
        "summary": "Safety is one of the critical challenges in the autonomous driving task. Recent works address the safety by implementing a safe reinforcement learning (safe RL) mechanism. However, most approaches make conservative decisions without knowing the confidence of the actions, which ultimately causes traffic congestion and low travel efficiency. This paper proposes an uncertainty-augmented Lagrangian safe reinforcement algorithm (Lag-U) to improve exploration and safety performance for autonomous driving. First, epistemic uncertainty is introduced into safe RL by using deep ensemble. We use the estimated epistemic uncertainty to encourage exploration and to learn a risk-sensitive policy by adaptively modifying safety constraints. Second, we facilitate an intervention assurance to choose safer actions based on the quantified epistemic uncertainty during deployment. Experimental results prove that the proposed method outperforms other safe RL baselines. The trained vehicle can make a decent trade-off between high efficiency and avoiding risks, thus preventing ultra-conservative policy.",
        "keywords": []
      },
      "file_name": "c225c6e24526c160bbceaaa36b447df9d8e95f13.pdf"
    },
    {
      "success": true,
      "doc_id": "37b6c924fd3a226e06e066bb4c452dd5",
      "summary": "In-context learning is a promising approach for online policy learning of offline reinforcement learning (RL) methods, which can be achieved at inference time without gradient optimization. However, this method is hindered by significant computational costs resulting from the gathering of large training trajectory sets and the need to train large Transformer models. We address this challenge by introducing an In-context Exploration-Exploitation (ICEE) algorithm, designed to optimize the efficiency of in-context policy learning. Unlike existing models, ICEE performs an exploration-exploitation trade-off at inference time within a Transformer model, without the need for explicit Bayesian inference. Consequently, ICEE can solve Bayesian optimization problems as efficiently as Gaussian process biased methods do, but in significantly less time. Through experiments in grid world environments, we demonstrate that ICEE can learn to solve new RL tasks using only tens of episodes, marking a substantial improvement over the hundreds of episodes needed by the previous in-context learning method.",
      "intriguing_abstract": "In-context learning is a promising approach for online policy learning of offline reinforcement learning (RL) methods, which can be achieved at inference time without gradient optimization. However, this method is hindered by significant computational costs resulting from the gathering of large training trajectory sets and the need to train large Transformer models. We address this challenge by introducing an In-context Exploration-Exploitation (ICEE) algorithm, designed to optimize the efficiency of in-context policy learning. Unlike existing models, ICEE performs an exploration-exploitation trade-off at inference time within a Transformer model, without the need for explicit Bayesian inference. Consequently, ICEE can solve Bayesian optimization problems as efficiently as Gaussian process biased methods do, but in significantly less time. Through experiments in grid world environments, we demonstrate that ICEE can learn to solve new RL tasks using only tens of episodes, marking a substantial improvement over the hundreds of episodes needed by the previous in-context learning method.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/c734971c6000e3f2769ab5165d00816af80dd76f.pdf",
      "citation_key": "dai2024x3l",
      "metadata": {
        "title": "In-context Exploration-Exploitation for Reinforcement Learning",
        "authors": [
          "Zhenwen Dai",
          "Federico Tomasi",
          "Sina Ghiassian"
        ],
        "published_date": "2024",
        "abstract": "In-context learning is a promising approach for online policy learning of offline reinforcement learning (RL) methods, which can be achieved at inference time without gradient optimization. However, this method is hindered by significant computational costs resulting from the gathering of large training trajectory sets and the need to train large Transformer models. We address this challenge by introducing an In-context Exploration-Exploitation (ICEE) algorithm, designed to optimize the efficiency of in-context policy learning. Unlike existing models, ICEE performs an exploration-exploitation trade-off at inference time within a Transformer model, without the need for explicit Bayesian inference. Consequently, ICEE can solve Bayesian optimization problems as efficiently as Gaussian process biased methods do, but in significantly less time. Through experiments in grid world environments, we demonstrate that ICEE can learn to solve new RL tasks using only tens of episodes, marking a substantial improvement over the hundreds of episodes needed by the previous in-context learning method.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/c734971c6000e3f2769ab5165d00816af80dd76f.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 10,
        "score": 10.0,
        "summary": "In-context learning is a promising approach for online policy learning of offline reinforcement learning (RL) methods, which can be achieved at inference time without gradient optimization. However, this method is hindered by significant computational costs resulting from the gathering of large training trajectory sets and the need to train large Transformer models. We address this challenge by introducing an In-context Exploration-Exploitation (ICEE) algorithm, designed to optimize the efficiency of in-context policy learning. Unlike existing models, ICEE performs an exploration-exploitation trade-off at inference time within a Transformer model, without the need for explicit Bayesian inference. Consequently, ICEE can solve Bayesian optimization problems as efficiently as Gaussian process biased methods do, but in significantly less time. Through experiments in grid world environments, we demonstrate that ICEE can learn to solve new RL tasks using only tens of episodes, marking a substantial improvement over the hundreds of episodes needed by the previous in-context learning method.",
        "keywords": []
      },
      "file_name": "c734971c6000e3f2769ab5165d00816af80dd76f.pdf"
    },
    {
      "success": true,
      "doc_id": "990a14f49ca7451f2e0f18d44aa15bf3",
      "summary": "Reinforcement learning (RL) has demonstrated its advantages in the intelligent control of many vehicle systems. However, controlling the exploration-to-exploitation (E2E) ratio of RL for the best performance in real-world operations is a great challenge. To obtain the optimal E2E ratio for managing the energy flow of a plug-in hybrid electric vehicle (PHEV) in real-world driving, this paper proposes an ensemble learning scheme based on two independent Q-learning agents working back-to-back competitively. At each sampling time, these agents generate two candidate control actions based on state observation and their control policies. Three decay functions, including the widely-used exponential decay and two new decay methods i.e., reciprocal function-base decay and step-based decay, are introduced to formulate one-dimensional look-up tables for E2E control. Then the PHEV control action will be selected from the candidate actions by a learning automata module(LAM) developed in this paper. The combinations of the three decay methods and three ensemble strategies with maximum-based, random-based, and weighted-based methods are investigated with the considerations of their energy efficiency, real-time performance, and robustness. Experiments are carried out on software-in-loop and hardware-in-the-loop platforms to demonstrate the energy-saving potentials. By implementing the ensemble learning scheme based on the weighted-based ensemble method in the control of the studied PHEV, up to 1.04% of energy can be saved under the predefined real-world driving cycles compared to the conventional Q-learning scheme.",
      "intriguing_abstract": "Reinforcement learning (RL) has demonstrated its advantages in the intelligent control of many vehicle systems. However, controlling the exploration-to-exploitation (E2E) ratio of RL for the best performance in real-world operations is a great challenge. To obtain the optimal E2E ratio for managing the energy flow of a plug-in hybrid electric vehicle (PHEV) in real-world driving, this paper proposes an ensemble learning scheme based on two independent Q-learning agents working back-to-back competitively. At each sampling time, these agents generate two candidate control actions based on state observation and their control policies. Three decay functions, including the widely-used exponential decay and two new decay methods i.e., reciprocal function-base decay and step-based decay, are introduced to formulate one-dimensional look-up tables for E2E control. Then the PHEV control action will be selected from the candidate actions by a learning automata module(LAM) developed in this paper. The combinations of the three decay methods and three ensemble strategies with maximum-based, random-based, and weighted-based methods are investigated with the considerations of their energy efficiency, real-time performance, and robustness. Experiments are carried out on software-in-loop and hardware-in-the-loop platforms to demonstrate the energy-saving potentials. By implementing the ensemble learning scheme based on the weighted-based ensemble method in the control of the studied PHEV, up to 1.04% of energy can be saved under the predefined real-world driving cycles compared to the conventional Q-learning scheme.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/ed8ea3d06c173849f02ee8afcf8db07df0f31261.pdf",
      "citation_key": "shuai2025fq3",
      "metadata": {
        "title": "Optimal Energy Management of Plug-In Hybrid Electric Vehicles Through Ensemble Reinforcement Learning With Exploration-to-Exploitation Ratio Control",
        "authors": [
          "Bin Shuai",
          "Min Hua",
          "Yanfei Li",
          "Shijin Shuai",
          "Hongming Xu",
          "Quan Zhou"
        ],
        "published_date": "2025",
        "abstract": "Reinforcement learning (RL) has demonstrated its advantages in the intelligent control of many vehicle systems. However, controlling the exploration-to-exploitation (E2E) ratio of RL for the best performance in real-world operations is a great challenge. To obtain the optimal E2E ratio for managing the energy flow of a plug-in hybrid electric vehicle (PHEV) in real-world driving, this paper proposes an ensemble learning scheme based on two independent Q-learning agents working back-to-back competitively. At each sampling time, these agents generate two candidate control actions based on state observation and their control policies. Three decay functions, including the widely-used exponential decay and two new decay methods i.e., reciprocal function-base decay and step-based decay, are introduced to formulate one-dimensional look-up tables for E2E control. Then the PHEV control action will be selected from the candidate actions by a learning automata module(LAM) developed in this paper. The combinations of the three decay methods and three ensemble strategies with maximum-based, random-based, and weighted-based methods are investigated with the considerations of their energy efficiency, real-time performance, and robustness. Experiments are carried out on software-in-loop and hardware-in-the-loop platforms to demonstrate the energy-saving potentials. By implementing the ensemble learning scheme based on the weighted-based ensemble method in the control of the studied PHEV, up to 1.04% of energy can be saved under the predefined real-world driving cycles compared to the conventional Q-learning scheme.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/ed8ea3d06c173849f02ee8afcf8db07df0f31261.pdf",
        "venue": "IEEE Transactions on Intelligent Vehicles",
        "citationCount": 10,
        "score": 10.0,
        "summary": "Reinforcement learning (RL) has demonstrated its advantages in the intelligent control of many vehicle systems. However, controlling the exploration-to-exploitation (E2E) ratio of RL for the best performance in real-world operations is a great challenge. To obtain the optimal E2E ratio for managing the energy flow of a plug-in hybrid electric vehicle (PHEV) in real-world driving, this paper proposes an ensemble learning scheme based on two independent Q-learning agents working back-to-back competitively. At each sampling time, these agents generate two candidate control actions based on state observation and their control policies. Three decay functions, including the widely-used exponential decay and two new decay methods i.e., reciprocal function-base decay and step-based decay, are introduced to formulate one-dimensional look-up tables for E2E control. Then the PHEV control action will be selected from the candidate actions by a learning automata module(LAM) developed in this paper. The combinations of the three decay methods and three ensemble strategies with maximum-based, random-based, and weighted-based methods are investigated with the considerations of their energy efficiency, real-time performance, and robustness. Experiments are carried out on software-in-loop and hardware-in-the-loop platforms to demonstrate the energy-saving potentials. By implementing the ensemble learning scheme based on the weighted-based ensemble method in the control of the studied PHEV, up to 1.04% of energy can be saved under the predefined real-world driving cycles compared to the conventional Q-learning scheme.",
        "keywords": []
      },
      "file_name": "ed8ea3d06c173849f02ee8afcf8db07df0f31261.pdf"
    },
    {
      "success": true,
      "doc_id": "b77cb6f60003bb46d0a911195aa21f0f",
      "summary": "Continuous action spaces in reinforcement learning (RL) are commonly defined as multidimensional intervals. While intervals usually reflect the action boundaries for tasks well, they can be challenging for learning because the typically large global action space leads to frequent exploration of irrelevant actions. Yet, little task knowledge can be sufficient to identify significantly smaller state-specific sets of relevant actions. Focusing learning on these relevant actions can significantly improve training efficiency and effectiveness. In this paper, we propose to focus learning on the set of relevant actions and introduce three continuous action masking methods for exactly mapping the action space to the state-dependent set of relevant actions. Thus, our methods ensure that only relevant actions are executed, enhancing the predictability of the RL agent and enabling its use in safety-critical applications. We further derive the implications of the proposed methods on the policy gradient. Using proximal policy optimization (PPO), we evaluate our methods on four control tasks, where the relevant action set is computed based on the system dynamics and a relevant state set. Our experiments show that the three action masking methods achieve higher final rewards and converge faster than the baseline without action masking.",
      "intriguing_abstract": "Continuous action spaces in reinforcement learning (RL) are commonly defined as multidimensional intervals. While intervals usually reflect the action boundaries for tasks well, they can be challenging for learning because the typically large global action space leads to frequent exploration of irrelevant actions. Yet, little task knowledge can be sufficient to identify significantly smaller state-specific sets of relevant actions. Focusing learning on these relevant actions can significantly improve training efficiency and effectiveness. In this paper, we propose to focus learning on the set of relevant actions and introduce three continuous action masking methods for exactly mapping the action space to the state-dependent set of relevant actions. Thus, our methods ensure that only relevant actions are executed, enhancing the predictability of the RL agent and enabling its use in safety-critical applications. We further derive the implications of the proposed methods on the policy gradient. Using proximal policy optimization (PPO), we evaluate our methods on four control tasks, where the relevant action set is computed based on the system dynamics and a relevant state set. Our experiments show that the three action masking methods achieve higher final rewards and converge faster than the baseline without action masking.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/dcbafb5ec43572d6ca170d86569d09a5dd40a8f8.pdf",
      "citation_key": "stolz20240y2",
      "metadata": {
        "title": "Excluding the Irrelevant: Focusing Reinforcement Learning through Continuous Action Masking",
        "authors": [
          "Roland Stolz",
          "Hanna Krasowski",
          "Jakob Thumm",
          "Michael Eichelbeck",
          "Philipp Gassert",
          "Matthias Althoff"
        ],
        "published_date": "2024",
        "abstract": "Continuous action spaces in reinforcement learning (RL) are commonly defined as multidimensional intervals. While intervals usually reflect the action boundaries for tasks well, they can be challenging for learning because the typically large global action space leads to frequent exploration of irrelevant actions. Yet, little task knowledge can be sufficient to identify significantly smaller state-specific sets of relevant actions. Focusing learning on these relevant actions can significantly improve training efficiency and effectiveness. In this paper, we propose to focus learning on the set of relevant actions and introduce three continuous action masking methods for exactly mapping the action space to the state-dependent set of relevant actions. Thus, our methods ensure that only relevant actions are executed, enhancing the predictability of the RL agent and enabling its use in safety-critical applications. We further derive the implications of the proposed methods on the policy gradient. Using proximal policy optimization (PPO), we evaluate our methods on four control tasks, where the relevant action set is computed based on the system dynamics and a relevant state set. Our experiments show that the three action masking methods achieve higher final rewards and converge faster than the baseline without action masking.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/dcbafb5ec43572d6ca170d86569d09a5dd40a8f8.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 10,
        "score": 10.0,
        "summary": "Continuous action spaces in reinforcement learning (RL) are commonly defined as multidimensional intervals. While intervals usually reflect the action boundaries for tasks well, they can be challenging for learning because the typically large global action space leads to frequent exploration of irrelevant actions. Yet, little task knowledge can be sufficient to identify significantly smaller state-specific sets of relevant actions. Focusing learning on these relevant actions can significantly improve training efficiency and effectiveness. In this paper, we propose to focus learning on the set of relevant actions and introduce three continuous action masking methods for exactly mapping the action space to the state-dependent set of relevant actions. Thus, our methods ensure that only relevant actions are executed, enhancing the predictability of the RL agent and enabling its use in safety-critical applications. We further derive the implications of the proposed methods on the policy gradient. Using proximal policy optimization (PPO), we evaluate our methods on four control tasks, where the relevant action set is computed based on the system dynamics and a relevant state set. Our experiments show that the three action masking methods achieve higher final rewards and converge faster than the baseline without action masking.",
        "keywords": []
      },
      "file_name": "dcbafb5ec43572d6ca170d86569d09a5dd40a8f8.pdf"
    },
    {
      "success": true,
      "doc_id": "cf9e6a6c8badc7b9b252daf229dd128d",
      "summary": "Reinforcement learning from demonstrations (RLfD) is a promising approach to improve the exploration efficiency of reinforcement learning (RL) by learning from expert demonstrations in addition to interactions with the environment. In this paper, we propose a framework that combines techniques from search-based testing with RLfD with the goal to raise the level of dependability of RL policies and to reduce human engineering effort. Within our framework, we provide methods for efficiently training, evaluating, and repairing RL policies. Instead of relying on the costly collection of demonstrations from (human) experts, we automatically compute a diverse set of demonstrations via search-based fuzzing methods and use the fuzz demonstrations for RLfD. To evaluate the safety and robustness of the trained RL agent, we search for safety-critical scenarios in the black-box environment. Finally, when unsafe behavior is detected, we compute demonstrations through fuzz testing that represent safe behavior and use them to repair the policy. Our experiments show that our framework is able to efficiently learn high-performing and safe policies without requiring any expert knowledge.",
      "intriguing_abstract": "Reinforcement learning from demonstrations (RLfD) is a promising approach to improve the exploration efficiency of reinforcement learning (RL) by learning from expert demonstrations in addition to interactions with the environment. In this paper, we propose a framework that combines techniques from search-based testing with RLfD with the goal to raise the level of dependability of RL policies and to reduce human engineering effort. Within our framework, we provide methods for efficiently training, evaluating, and repairing RL policies. Instead of relying on the costly collection of demonstrations from (human) experts, we automatically compute a diverse set of demonstrations via search-based fuzzing methods and use the fuzz demonstrations for RLfD. To evaluate the safety and robustness of the trained RL agent, we search for safety-critical scenarios in the black-box environment. Finally, when unsafe behavior is detected, we compute demonstrations through fuzz testing that represent safe behavior and use them to repair the policy. Our experiments show that our framework is able to efficiently learn high-performing and safe policies without requiring any expert knowledge.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/492f441bc6fdbb5f4b9273197ae563126439abeb.pdf",
      "citation_key": "tappler2024nm1",
      "metadata": {
        "title": "Learning and Repair of Deep Reinforcement Learning Policies from Fuzz-Testing Data",
        "authors": [
          "Martin Tappler",
          "Andrea Pferscher",
          "B. Aichernig",
          "Bettina Knighofer"
        ],
        "published_date": "2024",
        "abstract": "Reinforcement learning from demonstrations (RLfD) is a promising approach to improve the exploration efficiency of reinforcement learning (RL) by learning from expert demonstrations in addition to interactions with the environment. In this paper, we propose a framework that combines techniques from search-based testing with RLfD with the goal to raise the level of dependability of RL policies and to reduce human engineering effort. Within our framework, we provide methods for efficiently training, evaluating, and repairing RL policies. Instead of relying on the costly collection of demonstrations from (human) experts, we automatically compute a diverse set of demonstrations via search-based fuzzing methods and use the fuzz demonstrations for RLfD. To evaluate the safety and robustness of the trained RL agent, we search for safety-critical scenarios in the black-box environment. Finally, when unsafe behavior is detected, we compute demonstrations through fuzz testing that represent safe behavior and use them to repair the policy. Our experiments show that our framework is able to efficiently learn high-performing and safe policies without requiring any expert knowledge.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/492f441bc6fdbb5f4b9273197ae563126439abeb.pdf",
        "venue": "International Conference on Software Engineering",
        "citationCount": 10,
        "score": 10.0,
        "summary": "Reinforcement learning from demonstrations (RLfD) is a promising approach to improve the exploration efficiency of reinforcement learning (RL) by learning from expert demonstrations in addition to interactions with the environment. In this paper, we propose a framework that combines techniques from search-based testing with RLfD with the goal to raise the level of dependability of RL policies and to reduce human engineering effort. Within our framework, we provide methods for efficiently training, evaluating, and repairing RL policies. Instead of relying on the costly collection of demonstrations from (human) experts, we automatically compute a diverse set of demonstrations via search-based fuzzing methods and use the fuzz demonstrations for RLfD. To evaluate the safety and robustness of the trained RL agent, we search for safety-critical scenarios in the black-box environment. Finally, when unsafe behavior is detected, we compute demonstrations through fuzz testing that represent safe behavior and use them to repair the policy. Our experiments show that our framework is able to efficiently learn high-performing and safe policies without requiring any expert knowledge.",
        "keywords": []
      },
      "file_name": "492f441bc6fdbb5f4b9273197ae563126439abeb.pdf"
    },
    {
      "success": true,
      "doc_id": "b8672ec0ff6699c918e92090b1bb38a1",
      "summary": "Meta-reinforcement learning (meta-RL) is a promising framework for tackling challenging domains requiring efficient exploration. Existing meta-RL algorithms are characterized by low sample efficiency, and mostly focus on low-dimensional task distributions. In parallel, model-based RL methods have been successful in solving partially observable MDPs, of which meta-RL is a special case. In this work, we leverage this success and propose a new model-based approach to meta-RL, based on elements from existing state-of-the-art model-based and meta-RL methods. We demonstrate the effectiveness of our approach on common meta-RL benchmark domains, attaining greater return with better sample efficiency (up to $15\\times$) while requiring very little hyperparameter tuning. In addition, we validate our approach on a slate of more challenging, higher-dimensional domains, taking a step towards real-world generalizing agents.",
      "intriguing_abstract": "Meta-reinforcement learning (meta-RL) is a promising framework for tackling challenging domains requiring efficient exploration. Existing meta-RL algorithms are characterized by low sample efficiency, and mostly focus on low-dimensional task distributions. In parallel, model-based RL methods have been successful in solving partially observable MDPs, of which meta-RL is a special case. In this work, we leverage this success and propose a new model-based approach to meta-RL, based on elements from existing state-of-the-art model-based and meta-RL methods. We demonstrate the effectiveness of our approach on common meta-RL benchmark domains, attaining greater return with better sample efficiency (up to $15\\times$) while requiring very little hyperparameter tuning. In addition, we validate our approach on a slate of more challenging, higher-dimensional domains, taking a step towards real-world generalizing agents.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/4d3ffd8feca81d750aa30122b49cc1e874e70c1a.pdf",
      "citation_key": "rimon20243o6",
      "metadata": {
        "title": "MAMBA: an Effective World Model Approach for Meta-Reinforcement Learning",
        "authors": [
          "Zohar Rimon",
          "Tom Jurgenson",
          "Orr Krupnik",
          "Gilad Adler",
          "Aviv Tamar"
        ],
        "published_date": "2024",
        "abstract": "Meta-reinforcement learning (meta-RL) is a promising framework for tackling challenging domains requiring efficient exploration. Existing meta-RL algorithms are characterized by low sample efficiency, and mostly focus on low-dimensional task distributions. In parallel, model-based RL methods have been successful in solving partially observable MDPs, of which meta-RL is a special case. In this work, we leverage this success and propose a new model-based approach to meta-RL, based on elements from existing state-of-the-art model-based and meta-RL methods. We demonstrate the effectiveness of our approach on common meta-RL benchmark domains, attaining greater return with better sample efficiency (up to $15\\times$) while requiring very little hyperparameter tuning. In addition, we validate our approach on a slate of more challenging, higher-dimensional domains, taking a step towards real-world generalizing agents.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/4d3ffd8feca81d750aa30122b49cc1e874e70c1a.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 10,
        "score": 10.0,
        "summary": "Meta-reinforcement learning (meta-RL) is a promising framework for tackling challenging domains requiring efficient exploration. Existing meta-RL algorithms are characterized by low sample efficiency, and mostly focus on low-dimensional task distributions. In parallel, model-based RL methods have been successful in solving partially observable MDPs, of which meta-RL is a special case. In this work, we leverage this success and propose a new model-based approach to meta-RL, based on elements from existing state-of-the-art model-based and meta-RL methods. We demonstrate the effectiveness of our approach on common meta-RL benchmark domains, attaining greater return with better sample efficiency (up to $15\\times$) while requiring very little hyperparameter tuning. In addition, we validate our approach on a slate of more challenging, higher-dimensional domains, taking a step towards real-world generalizing agents.",
        "keywords": []
      },
      "file_name": "4d3ffd8feca81d750aa30122b49cc1e874e70c1a.pdf"
    },
    {
      "success": true,
      "doc_id": "06aa887acf464eb8808dcb11447bc3c6",
      "summary": "Introduction: Deep reinforcement learning (deep RL) integrates the principles of reinforcement learning with deep neural networks, enabling agents to excel in diverse tasks ranging from playing board games such as Go and Chess to controlling robotic systems and autonomous vehicles. By leveraging foundational concepts of value functions, policy optimization, and temporal difference methods, deep RL has rapidly evolved and found applications in areas such as gaming, robotics, finance, and healthcare. Objective: This paper seeks to provide a comprehensive yet accessible overview of the evolution of deep RL and its leading algorithms. It aims to serve both as an introduction for newcomers to the field and as a practical guide for those seeking to select the most appropriate methods for specific problem domains. Methods: We begin by outlining fundamental reinforcement learning principles, followed by an exploration of early tabular Q-learning methods. We then trace the historical development of deep RL, highlighting key milestones such as the advent of deep Q-networks (DQN). The survey extends to policy gradient methods, actorcritic architectures, and state-of-the-art algorithms such as proximal policy optimization, soft actorcritic, and emerging model-based approaches. Throughout, we discuss the current challenges facing deep RL, including issues of sample efficiency, interpretability, and safety, as well as open research questions involving large-scale training, hierarchical architectures, and multi-task learning. Results: Our analysis demonstrates how critical breakthroughs have driven deep RL into increasingly complex application domains. We highlight existing limitations and ongoing bottlenecks, such as high data requirements and the need for more transparent, ethically aligned systems. Finally, we survey potential future directions, highlighting the importance of reliability and ethical considerations for real-world deployments.",
      "intriguing_abstract": "Introduction: Deep reinforcement learning (deep RL) integrates the principles of reinforcement learning with deep neural networks, enabling agents to excel in diverse tasks ranging from playing board games such as Go and Chess to controlling robotic systems and autonomous vehicles. By leveraging foundational concepts of value functions, policy optimization, and temporal difference methods, deep RL has rapidly evolved and found applications in areas such as gaming, robotics, finance, and healthcare. Objective: This paper seeks to provide a comprehensive yet accessible overview of the evolution of deep RL and its leading algorithms. It aims to serve both as an introduction for newcomers to the field and as a practical guide for those seeking to select the most appropriate methods for specific problem domains. Methods: We begin by outlining fundamental reinforcement learning principles, followed by an exploration of early tabular Q-learning methods. We then trace the historical development of deep RL, highlighting key milestones such as the advent of deep Q-networks (DQN). The survey extends to policy gradient methods, actorcritic architectures, and state-of-the-art algorithms such as proximal policy optimization, soft actorcritic, and emerging model-based approaches. Throughout, we discuss the current challenges facing deep RL, including issues of sample efficiency, interpretability, and safety, as well as open research questions involving large-scale training, hierarchical architectures, and multi-task learning. Results: Our analysis demonstrates how critical breakthroughs have driven deep RL into increasingly complex application domains. We highlight existing limitations and ongoing bottlenecks, such as high data requirements and the need for more transparent, ethically aligned systems. Finally, we survey potential future directions, highlighting the importance of reliability and ethical considerations for real-world deployments.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/40d1e0a1e8a861305f9354be747620782fc203ce.pdf",
      "citation_key": "terven202599m",
      "metadata": {
        "title": "Deep Reinforcement Learning: A Chronological Overview and Methods",
        "authors": [
          "Juan R. Terven"
        ],
        "published_date": "2025",
        "abstract": "Introduction: Deep reinforcement learning (deep RL) integrates the principles of reinforcement learning with deep neural networks, enabling agents to excel in diverse tasks ranging from playing board games such as Go and Chess to controlling robotic systems and autonomous vehicles. By leveraging foundational concepts of value functions, policy optimization, and temporal difference methods, deep RL has rapidly evolved and found applications in areas such as gaming, robotics, finance, and healthcare. Objective: This paper seeks to provide a comprehensive yet accessible overview of the evolution of deep RL and its leading algorithms. It aims to serve both as an introduction for newcomers to the field and as a practical guide for those seeking to select the most appropriate methods for specific problem domains. Methods: We begin by outlining fundamental reinforcement learning principles, followed by an exploration of early tabular Q-learning methods. We then trace the historical development of deep RL, highlighting key milestones such as the advent of deep Q-networks (DQN). The survey extends to policy gradient methods, actorcritic architectures, and state-of-the-art algorithms such as proximal policy optimization, soft actorcritic, and emerging model-based approaches. Throughout, we discuss the current challenges facing deep RL, including issues of sample efficiency, interpretability, and safety, as well as open research questions involving large-scale training, hierarchical architectures, and multi-task learning. Results: Our analysis demonstrates how critical breakthroughs have driven deep RL into increasingly complex application domains. We highlight existing limitations and ongoing bottlenecks, such as high data requirements and the need for more transparent, ethically aligned systems. Finally, we survey potential future directions, highlighting the importance of reliability and ethical considerations for real-world deployments.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/40d1e0a1e8a861305f9354be747620782fc203ce.pdf",
        "venue": "Applied Informatics",
        "citationCount": 9,
        "score": 9.0,
        "summary": "Introduction: Deep reinforcement learning (deep RL) integrates the principles of reinforcement learning with deep neural networks, enabling agents to excel in diverse tasks ranging from playing board games such as Go and Chess to controlling robotic systems and autonomous vehicles. By leveraging foundational concepts of value functions, policy optimization, and temporal difference methods, deep RL has rapidly evolved and found applications in areas such as gaming, robotics, finance, and healthcare. Objective: This paper seeks to provide a comprehensive yet accessible overview of the evolution of deep RL and its leading algorithms. It aims to serve both as an introduction for newcomers to the field and as a practical guide for those seeking to select the most appropriate methods for specific problem domains. Methods: We begin by outlining fundamental reinforcement learning principles, followed by an exploration of early tabular Q-learning methods. We then trace the historical development of deep RL, highlighting key milestones such as the advent of deep Q-networks (DQN). The survey extends to policy gradient methods, actorcritic architectures, and state-of-the-art algorithms such as proximal policy optimization, soft actorcritic, and emerging model-based approaches. Throughout, we discuss the current challenges facing deep RL, including issues of sample efficiency, interpretability, and safety, as well as open research questions involving large-scale training, hierarchical architectures, and multi-task learning. Results: Our analysis demonstrates how critical breakthroughs have driven deep RL into increasingly complex application domains. We highlight existing limitations and ongoing bottlenecks, such as high data requirements and the need for more transparent, ethically aligned systems. Finally, we survey potential future directions, highlighting the importance of reliability and ethical considerations for real-world deployments.",
        "keywords": []
      },
      "file_name": "40d1e0a1e8a861305f9354be747620782fc203ce.pdf"
    },
    {
      "success": true,
      "doc_id": "7fa008561865b8d5d582d30d3ca60cc8",
      "summary": "Current state-of-the-art Design Space Exploration (DSE) methods in Physical Design (PD), including Bayesian optimization (BO) and Ant Colony Optimization (ACO), mainly rely on black-boxed rather than parametric (e.g., neural networks) approaches to improve end-of-flow Power, Performance, and Area (PPA) metrics, which often fail to generalize across unseen designs as netlist features are not properly leveraged. To overcome this issue, in this paper, we develop a Reinforcement Learning (RL) agent that leverages Graph Neural Networks (GNNs) and Transformers to perform \"fast\" DSE on unseen designs by sequentially encoding netlist features across different PD stages. Particularly, an attention-based encoder-decoder framework is devised for \"conditional\" parameter tuning, and a PPA estimator is introduced to predict end-of-flow PPA metrics for RL reward estimation. Extensive studies across 7 industrial designs under the TSMC 28nm technology node demonstrate that the proposed framework FastTuner, significantly outperforms existing state-of-the-art DSE techniques in both optimization quality and runtime. where we observe improvements up to 79.38% in Total Negative Slack (TNS), 12.22% in total power, and 50x in runtime.",
      "intriguing_abstract": "Current state-of-the-art Design Space Exploration (DSE) methods in Physical Design (PD), including Bayesian optimization (BO) and Ant Colony Optimization (ACO), mainly rely on black-boxed rather than parametric (e.g., neural networks) approaches to improve end-of-flow Power, Performance, and Area (PPA) metrics, which often fail to generalize across unseen designs as netlist features are not properly leveraged. To overcome this issue, in this paper, we develop a Reinforcement Learning (RL) agent that leverages Graph Neural Networks (GNNs) and Transformers to perform \"fast\" DSE on unseen designs by sequentially encoding netlist features across different PD stages. Particularly, an attention-based encoder-decoder framework is devised for \"conditional\" parameter tuning, and a PPA estimator is introduced to predict end-of-flow PPA metrics for RL reward estimation. Extensive studies across 7 industrial designs under the TSMC 28nm technology node demonstrate that the proposed framework FastTuner, significantly outperforms existing state-of-the-art DSE techniques in both optimization quality and runtime. where we observe improvements up to 79.38% in Total Negative Slack (TNS), 12.22% in total power, and 50x in runtime.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/736c35ed3e8e86ce99e02ce117dba7ee77e60dda.pdf",
      "citation_key": "hsiao2024wps",
      "metadata": {
        "title": "FastTuner: Transferable Physical Design Parameter Optimization using Fast Reinforcement Learning",
        "authors": [
          "Hao-Hsiang Hsiao",
          "Yi-Chen Lu",
          "Pruek Vanna-iampikul",
          "S. Lim"
        ],
        "published_date": "2024",
        "abstract": "Current state-of-the-art Design Space Exploration (DSE) methods in Physical Design (PD), including Bayesian optimization (BO) and Ant Colony Optimization (ACO), mainly rely on black-boxed rather than parametric (e.g., neural networks) approaches to improve end-of-flow Power, Performance, and Area (PPA) metrics, which often fail to generalize across unseen designs as netlist features are not properly leveraged. To overcome this issue, in this paper, we develop a Reinforcement Learning (RL) agent that leverages Graph Neural Networks (GNNs) and Transformers to perform \"fast\" DSE on unseen designs by sequentially encoding netlist features across different PD stages. Particularly, an attention-based encoder-decoder framework is devised for \"conditional\" parameter tuning, and a PPA estimator is introduced to predict end-of-flow PPA metrics for RL reward estimation. Extensive studies across 7 industrial designs under the TSMC 28nm technology node demonstrate that the proposed framework FastTuner, significantly outperforms existing state-of-the-art DSE techniques in both optimization quality and runtime. where we observe improvements up to 79.38% in Total Negative Slack (TNS), 12.22% in total power, and 50x in runtime.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/736c35ed3e8e86ce99e02ce117dba7ee77e60dda.pdf",
        "venue": "ACM International Symposium on Physical Design",
        "citationCount": 9,
        "score": 9.0,
        "summary": "Current state-of-the-art Design Space Exploration (DSE) methods in Physical Design (PD), including Bayesian optimization (BO) and Ant Colony Optimization (ACO), mainly rely on black-boxed rather than parametric (e.g., neural networks) approaches to improve end-of-flow Power, Performance, and Area (PPA) metrics, which often fail to generalize across unseen designs as netlist features are not properly leveraged. To overcome this issue, in this paper, we develop a Reinforcement Learning (RL) agent that leverages Graph Neural Networks (GNNs) and Transformers to perform \"fast\" DSE on unseen designs by sequentially encoding netlist features across different PD stages. Particularly, an attention-based encoder-decoder framework is devised for \"conditional\" parameter tuning, and a PPA estimator is introduced to predict end-of-flow PPA metrics for RL reward estimation. Extensive studies across 7 industrial designs under the TSMC 28nm technology node demonstrate that the proposed framework FastTuner, significantly outperforms existing state-of-the-art DSE techniques in both optimization quality and runtime. where we observe improvements up to 79.38% in Total Negative Slack (TNS), 12.22% in total power, and 50x in runtime.",
        "keywords": []
      },
      "file_name": "736c35ed3e8e86ce99e02ce117dba7ee77e60dda.pdf"
    },
    {
      "success": true,
      "doc_id": "f3b741f43fb09c9363c7b564895c2cae",
      "summary": "\n Space layout design is a critical aspect of architectural design, influencing functionality and aesthetics. The inherent combinatorial nature of layout design poses challenges for traditional planning approaches; thus, it demands the exploration of novel methods. This paper presents a novel framework that leverages the potential of deep reinforcement learning (RL) algorithms to optimize space layouts. RL has demonstrated remarkable success in addressing complex decision-making problems, yet its application in the design process remains relatively unexplored. We argue that RL is particularly well-suited for the design process due to its ability to accommodate offline tasks and seamless integration with existing CAD software, effectively acting as a simulator for design exploration. Framing space layout design as an RL problem and employing RL methods allows for the automated exploration of the expansive design space, thereby enhancing the discovery of innovative solutions. This paper also elucidates the synergy between the design process and the RL problem, which opens new avenues for exploring the potential of RL algorithms in design. We aim to foster experimentation and collaboration within the RL and architecture communities. To facilitate our research, we have developed SpaceLayoutGym, an environment specifically designed for space layout design tasks. SpaceLayoutGym serves as a customizable environment that encapsulates the essential elements of the layout design process within an RL framework. To showcase the effectiveness of SpaceLayoutGym and the capabilities of RL as an artificial space layout designer, we employ the PPO algorithm to train the RL agent in selected design scenarios with both geometrical constraints and topological objectives. The study further extends to contrast the effectiveness of PPO agents with that of genetic algorithms, and also includes a comparative analysis with existing layouts. Our results demonstrate the potential of RL to optimize space layouts, offering a promising direction for the future of AI-aided design.",
      "intriguing_abstract": "\n Space layout design is a critical aspect of architectural design, influencing functionality and aesthetics. The inherent combinatorial nature of layout design poses challenges for traditional planning approaches; thus, it demands the exploration of novel methods. This paper presents a novel framework that leverages the potential of deep reinforcement learning (RL) algorithms to optimize space layouts. RL has demonstrated remarkable success in addressing complex decision-making problems, yet its application in the design process remains relatively unexplored. We argue that RL is particularly well-suited for the design process due to its ability to accommodate offline tasks and seamless integration with existing CAD software, effectively acting as a simulator for design exploration. Framing space layout design as an RL problem and employing RL methods allows for the automated exploration of the expansive design space, thereby enhancing the discovery of innovative solutions. This paper also elucidates the synergy between the design process and the RL problem, which opens new avenues for exploring the potential of RL algorithms in design. We aim to foster experimentation and collaboration within the RL and architecture communities. To facilitate our research, we have developed SpaceLayoutGym, an environment specifically designed for space layout design tasks. SpaceLayoutGym serves as a customizable environment that encapsulates the essential elements of the layout design process within an RL framework. To showcase the effectiveness of SpaceLayoutGym and the capabilities of RL as an artificial space layout designer, we employ the PPO algorithm to train the RL agent in selected design scenarios with both geometrical constraints and topological objectives. The study further extends to contrast the effectiveness of PPO agents with that of genetic algorithms, and also includes a comparative analysis with existing layouts. Our results demonstrate the potential of RL to optimize space layouts, offering a promising direction for the future of AI-aided design.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/a69adfee23dbc90b2292499ffb1bf9fbd7d656c5.pdf",
      "citation_key": "kakooee2024w9m",
      "metadata": {
        "title": "Reimagining space layout design through deep reinforcement learning",
        "authors": [
          "R. Kakooee",
          "B. Dillenburger"
        ],
        "published_date": "2024",
        "abstract": "\n Space layout design is a critical aspect of architectural design, influencing functionality and aesthetics. The inherent combinatorial nature of layout design poses challenges for traditional planning approaches; thus, it demands the exploration of novel methods. This paper presents a novel framework that leverages the potential of deep reinforcement learning (RL) algorithms to optimize space layouts. RL has demonstrated remarkable success in addressing complex decision-making problems, yet its application in the design process remains relatively unexplored. We argue that RL is particularly well-suited for the design process due to its ability to accommodate offline tasks and seamless integration with existing CAD software, effectively acting as a simulator for design exploration. Framing space layout design as an RL problem and employing RL methods allows for the automated exploration of the expansive design space, thereby enhancing the discovery of innovative solutions. This paper also elucidates the synergy between the design process and the RL problem, which opens new avenues for exploring the potential of RL algorithms in design. We aim to foster experimentation and collaboration within the RL and architecture communities. To facilitate our research, we have developed SpaceLayoutGym, an environment specifically designed for space layout design tasks. SpaceLayoutGym serves as a customizable environment that encapsulates the essential elements of the layout design process within an RL framework. To showcase the effectiveness of SpaceLayoutGym and the capabilities of RL as an artificial space layout designer, we employ the PPO algorithm to train the RL agent in selected design scenarios with both geometrical constraints and topological objectives. The study further extends to contrast the effectiveness of PPO agents with that of genetic algorithms, and also includes a comparative analysis with existing layouts. Our results demonstrate the potential of RL to optimize space layouts, offering a promising direction for the future of AI-aided design.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/a69adfee23dbc90b2292499ffb1bf9fbd7d656c5.pdf",
        "venue": "Journal of Computational Design and Engineering",
        "citationCount": 9,
        "score": 9.0,
        "summary": "\n Space layout design is a critical aspect of architectural design, influencing functionality and aesthetics. The inherent combinatorial nature of layout design poses challenges for traditional planning approaches; thus, it demands the exploration of novel methods. This paper presents a novel framework that leverages the potential of deep reinforcement learning (RL) algorithms to optimize space layouts. RL has demonstrated remarkable success in addressing complex decision-making problems, yet its application in the design process remains relatively unexplored. We argue that RL is particularly well-suited for the design process due to its ability to accommodate offline tasks and seamless integration with existing CAD software, effectively acting as a simulator for design exploration. Framing space layout design as an RL problem and employing RL methods allows for the automated exploration of the expansive design space, thereby enhancing the discovery of innovative solutions. This paper also elucidates the synergy between the design process and the RL problem, which opens new avenues for exploring the potential of RL algorithms in design. We aim to foster experimentation and collaboration within the RL and architecture communities. To facilitate our research, we have developed SpaceLayoutGym, an environment specifically designed for space layout design tasks. SpaceLayoutGym serves as a customizable environment that encapsulates the essential elements of the layout design process within an RL framework. To showcase the effectiveness of SpaceLayoutGym and the capabilities of RL as an artificial space layout designer, we employ the PPO algorithm to train the RL agent in selected design scenarios with both geometrical constraints and topological objectives. The study further extends to contrast the effectiveness of PPO agents with that of genetic algorithms, and also includes a comparative analysis with existing layouts. Our results demonstrate the potential of RL to optimize space layouts, offering a promising direction for the future of AI-aided design.",
        "keywords": []
      },
      "file_name": "a69adfee23dbc90b2292499ffb1bf9fbd7d656c5.pdf"
    },
    {
      "success": true,
      "doc_id": "5e7f3d0316b0d0447e99de0a540f9338",
      "summary": "Offline reinforcement learning algorithms hold the promise of enabling data-driven RL methods that do not require costly or dangerous real-world exploration and benefit from large pre-collected datasets. This in turn can facilitate real-world applications, as well as a more standardized approach to RL research. Furthermore, offline RL methods can provide effective initializations for online finetuning to overcome challenges with exploration. However, evaluating progress on offline RL algorithms requires effective and challenging benchmarks that capture properties of real-world tasks, provide a range of task difficulties, and cover a range of challenges both in terms of the parameters of the domain (e.g., length of the horizon, sparsity of rewards) and the parameters of the data (e.g., narrow demonstration data or broad exploratory data). While considerable progress in offline RL in recent years has been enabled by simpler benchmark tasks, the most widely used datasets are increasingly saturating in performance and may fail to reflect properties of realistic tasks. We propose a new benchmark for offline RL that focuses on realistic simulations of robotic manipulation and locomotion environments, based on models of real-world robotic systems, and comprising a variety of data sources, including scripted data, play-style data collected by human teleoperators, and other data sources. Our proposed benchmark covers state-based and image-based domains, and supports both offline RL and online fine-tuning evaluation, with some of the tasks specifically designed to require both pre-training and fine-tuning. We hope that our proposed benchmark will facilitate further progress on both offline RL and fine-tuning algorithms. Website with code, examples, tasks, and data is available at \\url{https://sites.google.com/view/d5rl/}",
      "intriguing_abstract": "Offline reinforcement learning algorithms hold the promise of enabling data-driven RL methods that do not require costly or dangerous real-world exploration and benefit from large pre-collected datasets. This in turn can facilitate real-world applications, as well as a more standardized approach to RL research. Furthermore, offline RL methods can provide effective initializations for online finetuning to overcome challenges with exploration. However, evaluating progress on offline RL algorithms requires effective and challenging benchmarks that capture properties of real-world tasks, provide a range of task difficulties, and cover a range of challenges both in terms of the parameters of the domain (e.g., length of the horizon, sparsity of rewards) and the parameters of the data (e.g., narrow demonstration data or broad exploratory data). While considerable progress in offline RL in recent years has been enabled by simpler benchmark tasks, the most widely used datasets are increasingly saturating in performance and may fail to reflect properties of realistic tasks. We propose a new benchmark for offline RL that focuses on realistic simulations of robotic manipulation and locomotion environments, based on models of real-world robotic systems, and comprising a variety of data sources, including scripted data, play-style data collected by human teleoperators, and other data sources. Our proposed benchmark covers state-based and image-based domains, and supports both offline RL and online fine-tuning evaluation, with some of the tasks specifically designed to require both pre-training and fine-tuning. We hope that our proposed benchmark will facilitate further progress on both offline RL and fine-tuning algorithms. Website with code, examples, tasks, and data is available at \\url{https://sites.google.com/view/d5rl/}",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/e5d6cca71ea0fb216a25f86e96d3480886fdba27.pdf",
      "citation_key": "rafailov2024wtw",
      "metadata": {
        "title": "D5RL: Diverse Datasets for Data-Driven Deep Reinforcement Learning",
        "authors": [
          "Rafael Rafailov",
          "K. Hatch",
          "Anikait Singh",
          "Laura Smith",
          "Aviral Kumar",
          "Ilya Kostrikov",
          "Philippe Hansen-Estruch",
          "Victor Kolev",
          "Philip Ball",
          "Jiajun Wu",
          "Chelsea Finn",
          "Sergey Levine"
        ],
        "published_date": "2024",
        "abstract": "Offline reinforcement learning algorithms hold the promise of enabling data-driven RL methods that do not require costly or dangerous real-world exploration and benefit from large pre-collected datasets. This in turn can facilitate real-world applications, as well as a more standardized approach to RL research. Furthermore, offline RL methods can provide effective initializations for online finetuning to overcome challenges with exploration. However, evaluating progress on offline RL algorithms requires effective and challenging benchmarks that capture properties of real-world tasks, provide a range of task difficulties, and cover a range of challenges both in terms of the parameters of the domain (e.g., length of the horizon, sparsity of rewards) and the parameters of the data (e.g., narrow demonstration data or broad exploratory data). While considerable progress in offline RL in recent years has been enabled by simpler benchmark tasks, the most widely used datasets are increasingly saturating in performance and may fail to reflect properties of realistic tasks. We propose a new benchmark for offline RL that focuses on realistic simulations of robotic manipulation and locomotion environments, based on models of real-world robotic systems, and comprising a variety of data sources, including scripted data, play-style data collected by human teleoperators, and other data sources. Our proposed benchmark covers state-based and image-based domains, and supports both offline RL and online fine-tuning evaluation, with some of the tasks specifically designed to require both pre-training and fine-tuning. We hope that our proposed benchmark will facilitate further progress on both offline RL and fine-tuning algorithms. Website with code, examples, tasks, and data is available at \\url{https://sites.google.com/view/d5rl/}",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/e5d6cca71ea0fb216a25f86e96d3480886fdba27.pdf",
        "venue": "RLJ",
        "citationCount": 9,
        "score": 9.0,
        "summary": "Offline reinforcement learning algorithms hold the promise of enabling data-driven RL methods that do not require costly or dangerous real-world exploration and benefit from large pre-collected datasets. This in turn can facilitate real-world applications, as well as a more standardized approach to RL research. Furthermore, offline RL methods can provide effective initializations for online finetuning to overcome challenges with exploration. However, evaluating progress on offline RL algorithms requires effective and challenging benchmarks that capture properties of real-world tasks, provide a range of task difficulties, and cover a range of challenges both in terms of the parameters of the domain (e.g., length of the horizon, sparsity of rewards) and the parameters of the data (e.g., narrow demonstration data or broad exploratory data). While considerable progress in offline RL in recent years has been enabled by simpler benchmark tasks, the most widely used datasets are increasingly saturating in performance and may fail to reflect properties of realistic tasks. We propose a new benchmark for offline RL that focuses on realistic simulations of robotic manipulation and locomotion environments, based on models of real-world robotic systems, and comprising a variety of data sources, including scripted data, play-style data collected by human teleoperators, and other data sources. Our proposed benchmark covers state-based and image-based domains, and supports both offline RL and online fine-tuning evaluation, with some of the tasks specifically designed to require both pre-training and fine-tuning. We hope that our proposed benchmark will facilitate further progress on both offline RL and fine-tuning algorithms. Website with code, examples, tasks, and data is available at \\url{https://sites.google.com/view/d5rl/}",
        "keywords": []
      },
      "file_name": "e5d6cca71ea0fb216a25f86e96d3480886fdba27.pdf"
    },
    {
      "success": true,
      "doc_id": "9e73e3c658b7368657914dbfa4bd72fd",
      "summary": "While reinforcement learning (RL) holds great potential for decision making in the real world, it suffers from a number of unique difficulties which often need specific consideration. In particular: it is highly non-stationary; suffers from high degrees of plasticity loss; and requires exploration to prevent premature convergence to local optima and maximize return. In this paper, we consider whether learned optimization can help overcome these problems. Our method, Learned Optimization for Plasticity, Exploration and Non-stationarity (OPEN), meta-learns an update rule whose input features and output structure are informed by previously proposed solutions to these difficulties. We show that our parameterization is flexible enough to enable meta-learning in diverse learning contexts, including the ability to use stochasticity for exploration. Our experiments demonstrate that when meta-trained on single and small sets of environments, OPEN outperforms or equals traditionally used optimizers. Furthermore, OPEN shows strong generalization characteristics across a range of environments and agent architectures.",
      "intriguing_abstract": "While reinforcement learning (RL) holds great potential for decision making in the real world, it suffers from a number of unique difficulties which often need specific consideration. In particular: it is highly non-stationary; suffers from high degrees of plasticity loss; and requires exploration to prevent premature convergence to local optima and maximize return. In this paper, we consider whether learned optimization can help overcome these problems. Our method, Learned Optimization for Plasticity, Exploration and Non-stationarity (OPEN), meta-learns an update rule whose input features and output structure are informed by previously proposed solutions to these difficulties. We show that our parameterization is flexible enough to enable meta-learning in diverse learning contexts, including the ability to use stochasticity for exploration. Our experiments demonstrate that when meta-trained on single and small sets of environments, OPEN outperforms or equals traditionally used optimizers. Furthermore, OPEN shows strong generalization characteristics across a range of environments and agent architectures.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/5ecf53ab083f72f10421225a7dde25eb51cb6b22.pdf",
      "citation_key": "goldie2024cuf",
      "metadata": {
        "title": "Can Learned Optimization Make Reinforcement Learning Less Difficult?",
        "authors": [
          "A. D. Goldie",
          "Chris Lu",
          "Matthew Jackson",
          "S. Whiteson",
          "J. Foerster"
        ],
        "published_date": "2024",
        "abstract": "While reinforcement learning (RL) holds great potential for decision making in the real world, it suffers from a number of unique difficulties which often need specific consideration. In particular: it is highly non-stationary; suffers from high degrees of plasticity loss; and requires exploration to prevent premature convergence to local optima and maximize return. In this paper, we consider whether learned optimization can help overcome these problems. Our method, Learned Optimization for Plasticity, Exploration and Non-stationarity (OPEN), meta-learns an update rule whose input features and output structure are informed by previously proposed solutions to these difficulties. We show that our parameterization is flexible enough to enable meta-learning in diverse learning contexts, including the ability to use stochasticity for exploration. Our experiments demonstrate that when meta-trained on single and small sets of environments, OPEN outperforms or equals traditionally used optimizers. Furthermore, OPEN shows strong generalization characteristics across a range of environments and agent architectures.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/5ecf53ab083f72f10421225a7dde25eb51cb6b22.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 9,
        "score": 9.0,
        "summary": "While reinforcement learning (RL) holds great potential for decision making in the real world, it suffers from a number of unique difficulties which often need specific consideration. In particular: it is highly non-stationary; suffers from high degrees of plasticity loss; and requires exploration to prevent premature convergence to local optima and maximize return. In this paper, we consider whether learned optimization can help overcome these problems. Our method, Learned Optimization for Plasticity, Exploration and Non-stationarity (OPEN), meta-learns an update rule whose input features and output structure are informed by previously proposed solutions to these difficulties. We show that our parameterization is flexible enough to enable meta-learning in diverse learning contexts, including the ability to use stochasticity for exploration. Our experiments demonstrate that when meta-trained on single and small sets of environments, OPEN outperforms or equals traditionally used optimizers. Furthermore, OPEN shows strong generalization characteristics across a range of environments and agent architectures.",
        "keywords": []
      },
      "file_name": "5ecf53ab083f72f10421225a7dde25eb51cb6b22.pdf"
    },
    {
      "success": true,
      "doc_id": "46ed0664571cf4242b94cc3e96087eec",
      "summary": "Here's a focused summary of the paper \"RLfOLD: Reinforcement Learning from Online Demonstrations in Urban Autonomous Driving\" for a literature review:\n\n*   **1. Research Problem & Motivation**\n    *   **Specific Technical Problem:** Traditional Reinforcement Learning from Demonstrations (RLfD) relies on *offline* expert demonstrations, which creates a \"distribution gap\" between the static demonstration data and the dynamic training environment. This limits the agent's ability to generalize and perform effectively, especially in complex, real-time domains like urban autonomous driving (AD) \\cite{coelho2024oa6}. Offline dataset collection is also laborious and challenging to ensure diversity \\cite{coelho2024oa6}.\n    *   **Importance and Challenge:** Urban AD is a highly intricate task requiring continuous real-time decision-making, adherence to regulations, and interaction with dynamic agents. While Imitation Learning (IL) is sample-efficient, it struggles with generalization. Reinforcement Learning (RL) handles unknown situations but is sample-inefficient. RLfD aims to combine their strengths, but the offline demonstration limitation hinders its full potential \\cite{coelho2024oa6}.\n\n*   **2. Related Work & Positioning**\n    *   **Relation to Existing Approaches:** RLfOLD builds upon the principles of RLfD, which integrates expert demonstrations into RL training to boost sample efficiency \\cite{coelho2024oa6}. It acknowledges the successes of IL methods (e.g., CIL, Transfuser, LAV) in CARLA for rapid learning and RL methods (e.g., RLAD, IAs, CADRE) for handling unknown situations \\cite{coelho2024oa6}.\n    *   **Limitations of Previous Solutions:** Existing IL methods suffer from a significant distribution gap, leading to poor generalization \\cite{coelho2024oa6}. RL methods are often sample-inefficient and can face challenges like catastrophic self-overfitting in vision-based tasks \\cite{coelho2024oa6}. Crucially, prior RLfD approaches (e.g., CIRL, BC-SAC, GRIAD) are limited by their reliance on *offline* demonstrations, which introduces the aforementioned distribution gap and can be time-consuming to collect \\cite{coelho2024oa6}.\n\n*   **3. Technical Approach & Innovation**\n    *   **Core Technical Method:** RLfOLD proposes a novel approach that leverages *online demonstrations* collected using privileged information from the simulator during the agent's exploration \\cite{coelho2024oa6}. These demonstrations are seamlessly integrated into a single replay buffer alongside the agent's experiences \\cite{coelho2024oa6}. The core algorithm is a modified Soft Actor-Critic (SAC) that incorporates an Imitation Learning (IL) loss \\cite{coelho2024oa6}. The system uses a compact encoder (from RLAD) with Adaptive Local Signal Mixing (A-LIX) layers and image augmentations to mitigate self-overfitting \\cite{coelho2024oa6}. An online expert, based on simple heuristics, guides exploration and provides actions for the IL loss \\cite{coelho2024oa6}.\n    *   **Novelty/Difference:**\n        *   **Online Demonstrations:** The primary innovation is the shift from static *offline* datasets to dynamic *online* demonstrations, which directly addresses and bridges the distribution gap by ensuring the agent learns from relevant and up-to-date scenarios \\cite{coelho2024oa6}.\n        *   **Dual Standard Deviation Policy Network:** Unlike conventional actor-critic policies, RLfOLD's policy network outputs *two distinct standard deviations*: `_RL` for exploration in the RL component and `_IL` for the IL training. This allows the algorithm to adapt to varying levels of uncertainty inherent in both RL and IL, balancing exploration and exploitation \\cite{coelho2024oa6}.\n        *   **Uncertainty-Based Expert Guidance:** The exploration process is enhanced by an online expert that is selectively invoked when the agent faces high uncertainty, improving decision-making and learning efficiency \\cite{coelho2024oa6}.\n\n*   **4. Key Technical Contributions**\n    *   **Novel Algorithms/Methods:** Introduction of RLfOLD, a novel RLfD framework that integrates IL and RL through online demonstrations, effectively bridging the distribution gap \\cite{coelho2024oa6}.\n    *   **Novel Algorithms/Methods:** A policy network design that outputs two standard deviations (`_RL` and `_IL`) for adaptive control in exploration and IL training, considering uncertainty in both domains \\cite{coelho2024oa6}.\n    *   **Novel Algorithms/Methods:** Incorporation of an uncertainty-based technique guided by an online expert to enhance the exploration process \\cite{coelho2024oa6}.\n    *   **System Design/Architectural Innovations:** A streamlined replay buffer design that integrates both agent and online expert experiences into a single buffer, simplifying the learning framework \\cite{coelho2024oa6}.\n    *   **System Design/Architectural Innovations:** Utilization of a significantly smaller and robust encoder (approx. 0.65M parameters) with A-LIX layers and image augmentations, demonstrating high performance with reduced computational resources \\cite{coelho2024oa6}.\n\n*   **5. Experimental Validation**\n    *   **Experiments Conducted:** Extensive experiments were conducted on the CARLA NoCrash benchmark, a standard evaluation platform for urban autonomous driving \\cite{coelho2024oa6}.\n    *   **Key Performance Metrics and Comparison Results:** RLfOLD demonstrated superior effectiveness and efficiency, surpassing state-of-the-art methods in the NoCrash benchmark \\cite{coelho2024oa6}. This achievement was particularly notable given that RLfOLD used significantly fewer resources, specifically a smaller encoder and a single-camera setup \\cite{coelho2024oa6}.\n\n*   **6. Limitations & Scope**\n    *   **Technical Limitations/Assumptions:** The current online expert is based on simple heuristics, with future plans to transition to a neural network-based approach \\cite{coelho2024oa6}. The method relies on access to \"privileged information\" from the simulator for online demonstration generation, which might require careful consideration for real-world deployment \\cite{coelho2024oa6}.\n    *   **Scope of Applicability:** The work is primarily focused on and validated within the domain of urban autonomous driving using the CARLA simulator \\cite{coelho2024oa6}. While the authors suggest broader applicability, empirical evidence is currently limited to this specific context \\cite{coelho2024oa6}.\n\n*   **7. Technical Significance**\n    *   **Advance State-of-the-Art:** RLfOLD significantly advances the technical state-of-the-art in RLfD by introducing the concept of online demonstrations, effectively resolving the long-standing distribution gap issue \\cite{coelho2024oa6}. Its ability to outperform state-of-the-art methods with substantially fewer resources (smaller encoder, single camera) highlights a crucial step towards more practical and deployable AD solutions \\cite{coelho2024oa6}. The dual standard deviation policy network offers a novel and effective mechanism for balancing exploration and imitation \\cite{coelho2024oa6}.\n    *   **Potential Impact on Future Research:** This work opens new research avenues for dynamic, interactive RLfD systems, moving beyond static datasets \\cite{coelho2024oa6}. The demonstration of high performance with limited resources could inspire future research into resource-efficient deep reinforcement learning for complex real-world applications, particularly in robotics and autonomous systems \\cite{coelho2024oa6}. The uncertainty-based expert guidance mechanism also provides a foundation for more intelligent and adaptive human-in-the-loop or expert-guided learning systems \\cite{coelho2024oa6}.",
      "intriguing_abstract": "Traditional Reinforcement Learning from Demonstrations (RLfD) faces a critical \"distribution gap\" in dynamic environments like urban autonomous driving, where static offline datasets fail to capture real-time complexities. We introduce **RLfOLD**, a novel framework that revolutionizes RLfD by leveraging *online demonstrations* collected directly during agent exploration. This paradigm shift fundamentally bridges the distribution gap, enabling robust and generalizable learning for complex autonomous tasks.\n\nRLfOLD integrates these dynamic demonstrations into a modified **Soft Actor-Critic (SAC)** algorithm, featuring a groundbreaking **policy network** that outputs *dual standard deviations* ($\\sigma_{RL}$ and $\\sigma_{IL}$). This innovative design allows for adaptive control, precisely balancing exploration and imitation based on inherent uncertainties. Further enhancing learning efficiency, an **uncertainty-based online expert** selectively guides the agent. Validated on the challenging **CARLA NoCrash benchmark**, RLfOLD significantly outperforms state-of-the-art methods, remarkably achieving superior results with a compact encoder and single-camera setup, demonstrating unprecedented **resource efficiency**. This work paves the way for practical, high-performance **deep reinforcement learning** in complex real-world autonomous systems.",
      "keywords": [
        "RLfOLD",
        "Reinforcement Learning from Demonstrations (RLfD)",
        "online demonstrations",
        "urban autonomous driving",
        "distribution gap",
        "Soft Actor-Critic (SAC)",
        "dual standard deviation policy network",
        "uncertainty-based expert guidance",
        "CARLA NoCrash benchmark",
        "resource-efficient deep learning",
        "Imitation Learning",
        "generalization"
      ],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/4186f74da6d793c91c5ca4d8a10cf2f8638eade6.pdf",
      "citation_key": "coelho2024oa6",
      "metadata": {
        "title": "RLfOLD: Reinforcement Learning from Online Demonstrations in Urban Autonomous Driving",
        "authors": [
          "Daniel Coelho",
          "Miguel Oliveira",
          "Vitor Santos"
        ],
        "published_date": "2024",
        "abstract": "Reinforcement Learning from Demonstrations (RLfD) has emerged as an effective method by fusing expert demonstrations into Reinforcement Learning (RL) training, harnessing the strengths of both Imitation Learning (IL) and RL. However, existing algorithms rely on offline demonstrations, which can introduce a distribution gap between the demonstrations and the actual training environment, limiting their performance. In this paper, we propose a novel approach, Reinforcement Learning from Online Demonstrations (RLfOLD), that leverages online demonstrations to address this limitation, ensuring the agent learns from relevant and up-to-date scenarios, thus effectively bridging the distribution gap. Unlike conventional policy networks used in typical actor-critic algorithms, RLfOLD introduces a policy network that outputs two standard deviations: one for exploration and the other for IL training. This novel design allows the agent to adapt to varying levels of uncertainty inherent in both RL and IL. Furthermore, we introduce an exploration process guided by an online expert, incorporating an uncertainty-based technique. Our experiments on the CARLA NoCrash benchmark demonstrate the effectiveness and efficiency of RLfOLD. Notably, even with a significantly smaller encoder and a single camera setup, RLfOLD surpasses state-of-the-art methods in this evaluation. These results, achieved with limited resources, highlight RLfOLD as a highly promising solution for real-world applications.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/4186f74da6d793c91c5ca4d8a10cf2f8638eade6.pdf",
        "venue": "AAAI Conference on Artificial Intelligence",
        "citationCount": 9,
        "score": 9.0,
        "summary": "Here's a focused summary of the paper \"RLfOLD: Reinforcement Learning from Online Demonstrations in Urban Autonomous Driving\" for a literature review:\n\n*   **1. Research Problem & Motivation**\n    *   **Specific Technical Problem:** Traditional Reinforcement Learning from Demonstrations (RLfD) relies on *offline* expert demonstrations, which creates a \"distribution gap\" between the static demonstration data and the dynamic training environment. This limits the agent's ability to generalize and perform effectively, especially in complex, real-time domains like urban autonomous driving (AD) \\cite{coelho2024oa6}. Offline dataset collection is also laborious and challenging to ensure diversity \\cite{coelho2024oa6}.\n    *   **Importance and Challenge:** Urban AD is a highly intricate task requiring continuous real-time decision-making, adherence to regulations, and interaction with dynamic agents. While Imitation Learning (IL) is sample-efficient, it struggles with generalization. Reinforcement Learning (RL) handles unknown situations but is sample-inefficient. RLfD aims to combine their strengths, but the offline demonstration limitation hinders its full potential \\cite{coelho2024oa6}.\n\n*   **2. Related Work & Positioning**\n    *   **Relation to Existing Approaches:** RLfOLD builds upon the principles of RLfD, which integrates expert demonstrations into RL training to boost sample efficiency \\cite{coelho2024oa6}. It acknowledges the successes of IL methods (e.g., CIL, Transfuser, LAV) in CARLA for rapid learning and RL methods (e.g., RLAD, IAs, CADRE) for handling unknown situations \\cite{coelho2024oa6}.\n    *   **Limitations of Previous Solutions:** Existing IL methods suffer from a significant distribution gap, leading to poor generalization \\cite{coelho2024oa6}. RL methods are often sample-inefficient and can face challenges like catastrophic self-overfitting in vision-based tasks \\cite{coelho2024oa6}. Crucially, prior RLfD approaches (e.g., CIRL, BC-SAC, GRIAD) are limited by their reliance on *offline* demonstrations, which introduces the aforementioned distribution gap and can be time-consuming to collect \\cite{coelho2024oa6}.\n\n*   **3. Technical Approach & Innovation**\n    *   **Core Technical Method:** RLfOLD proposes a novel approach that leverages *online demonstrations* collected using privileged information from the simulator during the agent's exploration \\cite{coelho2024oa6}. These demonstrations are seamlessly integrated into a single replay buffer alongside the agent's experiences \\cite{coelho2024oa6}. The core algorithm is a modified Soft Actor-Critic (SAC) that incorporates an Imitation Learning (IL) loss \\cite{coelho2024oa6}. The system uses a compact encoder (from RLAD) with Adaptive Local Signal Mixing (A-LIX) layers and image augmentations to mitigate self-overfitting \\cite{coelho2024oa6}. An online expert, based on simple heuristics, guides exploration and provides actions for the IL loss \\cite{coelho2024oa6}.\n    *   **Novelty/Difference:**\n        *   **Online Demonstrations:** The primary innovation is the shift from static *offline* datasets to dynamic *online* demonstrations, which directly addresses and bridges the distribution gap by ensuring the agent learns from relevant and up-to-date scenarios \\cite{coelho2024oa6}.\n        *   **Dual Standard Deviation Policy Network:** Unlike conventional actor-critic policies, RLfOLD's policy network outputs *two distinct standard deviations*: `_RL` for exploration in the RL component and `_IL` for the IL training. This allows the algorithm to adapt to varying levels of uncertainty inherent in both RL and IL, balancing exploration and exploitation \\cite{coelho2024oa6}.\n        *   **Uncertainty-Based Expert Guidance:** The exploration process is enhanced by an online expert that is selectively invoked when the agent faces high uncertainty, improving decision-making and learning efficiency \\cite{coelho2024oa6}.\n\n*   **4. Key Technical Contributions**\n    *   **Novel Algorithms/Methods:** Introduction of RLfOLD, a novel RLfD framework that integrates IL and RL through online demonstrations, effectively bridging the distribution gap \\cite{coelho2024oa6}.\n    *   **Novel Algorithms/Methods:** A policy network design that outputs two standard deviations (`_RL` and `_IL`) for adaptive control in exploration and IL training, considering uncertainty in both domains \\cite{coelho2024oa6}.\n    *   **Novel Algorithms/Methods:** Incorporation of an uncertainty-based technique guided by an online expert to enhance the exploration process \\cite{coelho2024oa6}.\n    *   **System Design/Architectural Innovations:** A streamlined replay buffer design that integrates both agent and online expert experiences into a single buffer, simplifying the learning framework \\cite{coelho2024oa6}.\n    *   **System Design/Architectural Innovations:** Utilization of a significantly smaller and robust encoder (approx. 0.65M parameters) with A-LIX layers and image augmentations, demonstrating high performance with reduced computational resources \\cite{coelho2024oa6}.\n\n*   **5. Experimental Validation**\n    *   **Experiments Conducted:** Extensive experiments were conducted on the CARLA NoCrash benchmark, a standard evaluation platform for urban autonomous driving \\cite{coelho2024oa6}.\n    *   **Key Performance Metrics and Comparison Results:** RLfOLD demonstrated superior effectiveness and efficiency, surpassing state-of-the-art methods in the NoCrash benchmark \\cite{coelho2024oa6}. This achievement was particularly notable given that RLfOLD used significantly fewer resources, specifically a smaller encoder and a single-camera setup \\cite{coelho2024oa6}.\n\n*   **6. Limitations & Scope**\n    *   **Technical Limitations/Assumptions:** The current online expert is based on simple heuristics, with future plans to transition to a neural network-based approach \\cite{coelho2024oa6}. The method relies on access to \"privileged information\" from the simulator for online demonstration generation, which might require careful consideration for real-world deployment \\cite{coelho2024oa6}.\n    *   **Scope of Applicability:** The work is primarily focused on and validated within the domain of urban autonomous driving using the CARLA simulator \\cite{coelho2024oa6}. While the authors suggest broader applicability, empirical evidence is currently limited to this specific context \\cite{coelho2024oa6}.\n\n*   **7. Technical Significance**\n    *   **Advance State-of-the-Art:** RLfOLD significantly advances the technical state-of-the-art in RLfD by introducing the concept of online demonstrations, effectively resolving the long-standing distribution gap issue \\cite{coelho2024oa6}. Its ability to outperform state-of-the-art methods with substantially fewer resources (smaller encoder, single camera) highlights a crucial step towards more practical and deployable AD solutions \\cite{coelho2024oa6}. The dual standard deviation policy network offers a novel and effective mechanism for balancing exploration and imitation \\cite{coelho2024oa6}.\n    *   **Potential Impact on Future Research:** This work opens new research avenues for dynamic, interactive RLfD systems, moving beyond static datasets \\cite{coelho2024oa6}. The demonstration of high performance with limited resources could inspire future research into resource-efficient deep reinforcement learning for complex real-world applications, particularly in robotics and autonomous systems \\cite{coelho2024oa6}. The uncertainty-based expert guidance mechanism also provides a foundation for more intelligent and adaptive human-in-the-loop or expert-guided learning systems \\cite{coelho2024oa6}.",
        "keywords": [
          "RLfOLD",
          "Reinforcement Learning from Demonstrations (RLfD)",
          "online demonstrations",
          "urban autonomous driving",
          "distribution gap",
          "Soft Actor-Critic (SAC)",
          "dual standard deviation policy network",
          "uncertainty-based expert guidance",
          "CARLA NoCrash benchmark",
          "resource-efficient deep learning",
          "Imitation Learning",
          "generalization"
        ],
        "paper_type": "based on the abstract and introduction:\n\n*   the abstract explicitly states: \"we propose a novel approach, reinforcement learning from online demonstrations (rl-fold)\".\n*   it then details specific technical innovations: \"rlfold introduces a policy network that outputs two standard deviations\", and \"we introduce an exploration process guided by an online expert\".\n*   the introduction sets up the problem and discusses existing methods (il, rl) and their limitations, which the proposed method aims to address.\n*   while the paper includes \"experiments on the carla nocrash benchmark\" to \"demonstrate the effectiveness and efficiency,\" this empirical evaluation serves to validate the *new method* being presented.\n\nthese points strongly align with the criteria for a **technical** paper, which presents new methods, algorithms, or systems.\n\n**classification: technical**"
      },
      "file_name": "4186f74da6d793c91c5ca4d8a10cf2f8638eade6.pdf"
    },
    {
      "success": true,
      "doc_id": "fbe5e55792b08182100c06e52a51e54f",
      "summary": "Mandatory lane-change scenarios are often challenging for autonomous vehicles in complex environments. In this paper, a human-knowledge-enhanced reinforcement learning (RL) method for lane-change decision making is proposed, where the human intelligence is integrated with RL algorithm in a multiple manner. First, this paper constructs a complex ramp-off scenario with congested traffic flow to help agents master lane-change skills. On the basis of the Twin Delayed Deep Deterministic policy gradient (TD3) algorithm, the human prior experience is encoded into reward function and safety constraints offline, and the online guidance of experts is also introduced into the framework, which can limit the unsafe exploration during the training process and provide demonstration in complex scenarios. The experimental results indicate that our method can effectively improve the training efficiency and outperform typical RL method and expert drivers, without specific requirements on the expertise. The proposed method can enhance the learning ability of RL based driving strategies.",
      "intriguing_abstract": "Mandatory lane-change scenarios are often challenging for autonomous vehicles in complex environments. In this paper, a human-knowledge-enhanced reinforcement learning (RL) method for lane-change decision making is proposed, where the human intelligence is integrated with RL algorithm in a multiple manner. First, this paper constructs a complex ramp-off scenario with congested traffic flow to help agents master lane-change skills. On the basis of the Twin Delayed Deep Deterministic policy gradient (TD3) algorithm, the human prior experience is encoded into reward function and safety constraints offline, and the online guidance of experts is also introduced into the framework, which can limit the unsafe exploration during the training process and provide demonstration in complex scenarios. The experimental results indicate that our method can effectively improve the training efficiency and outperform typical RL method and expert drivers, without specific requirements on the expertise. The proposed method can enhance the learning ability of RL based driving strategies.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/eb6b79b00d43fa03945ae2477c60e79edaf72d28.pdf",
      "citation_key": "huang2024nh4",
      "metadata": {
        "title": "Human Knowledge Enhanced Reinforcement Learning for Mandatory Lane-Change of Autonomous Vehicles in Congested Traffic",
        "authors": [
          "Yanjun Huang",
          "Yuxiao Gu",
          "Kang Yuan",
          "Shuo Yang",
          "Tao Liu",
          "Hong Chen"
        ],
        "published_date": "2024",
        "abstract": "Mandatory lane-change scenarios are often challenging for autonomous vehicles in complex environments. In this paper, a human-knowledge-enhanced reinforcement learning (RL) method for lane-change decision making is proposed, where the human intelligence is integrated with RL algorithm in a multiple manner. First, this paper constructs a complex ramp-off scenario with congested traffic flow to help agents master lane-change skills. On the basis of the Twin Delayed Deep Deterministic policy gradient (TD3) algorithm, the human prior experience is encoded into reward function and safety constraints offline, and the online guidance of experts is also introduced into the framework, which can limit the unsafe exploration during the training process and provide demonstration in complex scenarios. The experimental results indicate that our method can effectively improve the training efficiency and outperform typical RL method and expert drivers, without specific requirements on the expertise. The proposed method can enhance the learning ability of RL based driving strategies.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/eb6b79b00d43fa03945ae2477c60e79edaf72d28.pdf",
        "venue": "IEEE Transactions on Intelligent Vehicles",
        "citationCount": 9,
        "score": 9.0,
        "summary": "Mandatory lane-change scenarios are often challenging for autonomous vehicles in complex environments. In this paper, a human-knowledge-enhanced reinforcement learning (RL) method for lane-change decision making is proposed, where the human intelligence is integrated with RL algorithm in a multiple manner. First, this paper constructs a complex ramp-off scenario with congested traffic flow to help agents master lane-change skills. On the basis of the Twin Delayed Deep Deterministic policy gradient (TD3) algorithm, the human prior experience is encoded into reward function and safety constraints offline, and the online guidance of experts is also introduced into the framework, which can limit the unsafe exploration during the training process and provide demonstration in complex scenarios. The experimental results indicate that our method can effectively improve the training efficiency and outperform typical RL method and expert drivers, without specific requirements on the expertise. The proposed method can enhance the learning ability of RL based driving strategies.",
        "keywords": []
      },
      "file_name": "eb6b79b00d43fa03945ae2477c60e79edaf72d28.pdf"
    },
    {
      "success": true,
      "doc_id": "f5d8aba4ef54d388e87f15d8ffb2a477",
      "summary": "Current reinforcement learning (RL) often suffers when solving a challenging exploration problem where the desired outcomes or high rewards are rarely observed. Even though curriculum RL, a framework that solves complex tasks by proposing a sequence of surrogate tasks, shows reasonable results, most of the previous works still have difficulty in proposing curriculum due to the absence of a mechanism for obtaining calibrated guidance to the desired outcome state without any prior domain knowledge. To alleviate it, we propose an uncertainty&temporal distance-aware curriculum goal generation method for the outcome-directed RL via solving a bipartite matching problem. It could not only provide precisely calibrated guidance of the curriculum to the desired outcome states but also bring much better sample efficiency and geometry-agnostic curriculum goal proposal capability compared to previous curriculum RL methods. We demonstrate that our algorithm significantly outperforms these prior methods in a variety of challenging navigation tasks and robotic manipulation tasks in a quantitative and qualitative way.",
      "intriguing_abstract": "Current reinforcement learning (RL) often suffers when solving a challenging exploration problem where the desired outcomes or high rewards are rarely observed. Even though curriculum RL, a framework that solves complex tasks by proposing a sequence of surrogate tasks, shows reasonable results, most of the previous works still have difficulty in proposing curriculum due to the absence of a mechanism for obtaining calibrated guidance to the desired outcome state without any prior domain knowledge. To alleviate it, we propose an uncertainty&temporal distance-aware curriculum goal generation method for the outcome-directed RL via solving a bipartite matching problem. It could not only provide precisely calibrated guidance of the curriculum to the desired outcome states but also bring much better sample efficiency and geometry-agnostic curriculum goal proposal capability compared to previous curriculum RL methods. We demonstrate that our algorithm significantly outperforms these prior methods in a variety of challenging navigation tasks and robotic manipulation tasks in a quantitative and qualitative way.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/b093a3fa79512c48524f81c754bddec7b16afb17.pdf",
      "citation_key": "cho2023z4l",
      "metadata": {
        "title": "Outcome-directed Reinforcement Learning by Uncertainty & Temporal Distance-Aware Curriculum Goal Generation",
        "authors": [
          "Daesol Cho",
          "Seungjae Lee",
          "H. J. Kim"
        ],
        "published_date": "2023",
        "abstract": "Current reinforcement learning (RL) often suffers when solving a challenging exploration problem where the desired outcomes or high rewards are rarely observed. Even though curriculum RL, a framework that solves complex tasks by proposing a sequence of surrogate tasks, shows reasonable results, most of the previous works still have difficulty in proposing curriculum due to the absence of a mechanism for obtaining calibrated guidance to the desired outcome state without any prior domain knowledge. To alleviate it, we propose an uncertainty&temporal distance-aware curriculum goal generation method for the outcome-directed RL via solving a bipartite matching problem. It could not only provide precisely calibrated guidance of the curriculum to the desired outcome states but also bring much better sample efficiency and geometry-agnostic curriculum goal proposal capability compared to previous curriculum RL methods. We demonstrate that our algorithm significantly outperforms these prior methods in a variety of challenging navigation tasks and robotic manipulation tasks in a quantitative and qualitative way.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/b093a3fa79512c48524f81c754bddec7b16afb17.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 16,
        "score": 8.0,
        "summary": "Current reinforcement learning (RL) often suffers when solving a challenging exploration problem where the desired outcomes or high rewards are rarely observed. Even though curriculum RL, a framework that solves complex tasks by proposing a sequence of surrogate tasks, shows reasonable results, most of the previous works still have difficulty in proposing curriculum due to the absence of a mechanism for obtaining calibrated guidance to the desired outcome state without any prior domain knowledge. To alleviate it, we propose an uncertainty&temporal distance-aware curriculum goal generation method for the outcome-directed RL via solving a bipartite matching problem. It could not only provide precisely calibrated guidance of the curriculum to the desired outcome states but also bring much better sample efficiency and geometry-agnostic curriculum goal proposal capability compared to previous curriculum RL methods. We demonstrate that our algorithm significantly outperforms these prior methods in a variety of challenging navigation tasks and robotic manipulation tasks in a quantitative and qualitative way.",
        "keywords": []
      },
      "file_name": "b093a3fa79512c48524f81c754bddec7b16afb17.pdf"
    },
    {
      "success": true,
      "doc_id": "25ce5587c00f4a0aac4ca6004c0d3e9f",
      "summary": "Recently, there has been a surge of vision-based GUI agents designed to automate everyday mobile and web tasks. These agents interpret raw GUI screenshots and autonomously decide where to click, scroll, or type, which bypasses handcrafted rules and app-specific APIs. However, most existing methods trained GUI agent in the offline environment using pre-collected trajectories. This approach limits scalability, causes overfitting to specific UI templates, and leads to brittle policies when faced with unseen environment. We present MobileGUI-RL, a scalable framework that trains GUI agent in online environment. MobileGUI-RL contains two key components. It (i) synthesizes a curriculum of learnable tasks through self-exploration and filtering, and (ii) adapts GRPO to GUI navigation with trajectory-aware advantages and composite rewards that balance task success and execution efficiency. Experiments on three online mobile-agent benchmarks show consistent gains, validating the effectiveness of our approach.",
      "intriguing_abstract": "Recently, there has been a surge of vision-based GUI agents designed to automate everyday mobile and web tasks. These agents interpret raw GUI screenshots and autonomously decide where to click, scroll, or type, which bypasses handcrafted rules and app-specific APIs. However, most existing methods trained GUI agent in the offline environment using pre-collected trajectories. This approach limits scalability, causes overfitting to specific UI templates, and leads to brittle policies when faced with unseen environment. We present MobileGUI-RL, a scalable framework that trains GUI agent in online environment. MobileGUI-RL contains two key components. It (i) synthesizes a curriculum of learnable tasks through self-exploration and filtering, and (ii) adapts GRPO to GUI navigation with trajectory-aware advantages and composite rewards that balance task success and execution efficiency. Experiments on three online mobile-agent benchmarks show consistent gains, validating the effectiveness of our approach.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/9a67ff1d46d691f7741822d7a13587a517b1be14.pdf",
      "citation_key": "shi20258tu",
      "metadata": {
        "title": "MobileGUI-RL: Advancing Mobile GUI Agent through Reinforcement Learning in Online Environment",
        "authors": [
          "Yucheng Shi",
          "Wenhao Yu",
          "Zaitang Li",
          "Yonglin Wang",
          "Hongming Zhang",
          "Ninghao Liu",
          "Haitao Mi",
          "Dong Yu"
        ],
        "published_date": "2025",
        "abstract": "Recently, there has been a surge of vision-based GUI agents designed to automate everyday mobile and web tasks. These agents interpret raw GUI screenshots and autonomously decide where to click, scroll, or type, which bypasses handcrafted rules and app-specific APIs. However, most existing methods trained GUI agent in the offline environment using pre-collected trajectories. This approach limits scalability, causes overfitting to specific UI templates, and leads to brittle policies when faced with unseen environment. We present MobileGUI-RL, a scalable framework that trains GUI agent in online environment. MobileGUI-RL contains two key components. It (i) synthesizes a curriculum of learnable tasks through self-exploration and filtering, and (ii) adapts GRPO to GUI navigation with trajectory-aware advantages and composite rewards that balance task success and execution efficiency. Experiments on three online mobile-agent benchmarks show consistent gains, validating the effectiveness of our approach.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/9a67ff1d46d691f7741822d7a13587a517b1be14.pdf",
        "venue": "arXiv.org",
        "citationCount": 8,
        "score": 8.0,
        "summary": "Recently, there has been a surge of vision-based GUI agents designed to automate everyday mobile and web tasks. These agents interpret raw GUI screenshots and autonomously decide where to click, scroll, or type, which bypasses handcrafted rules and app-specific APIs. However, most existing methods trained GUI agent in the offline environment using pre-collected trajectories. This approach limits scalability, causes overfitting to specific UI templates, and leads to brittle policies when faced with unseen environment. We present MobileGUI-RL, a scalable framework that trains GUI agent in online environment. MobileGUI-RL contains two key components. It (i) synthesizes a curriculum of learnable tasks through self-exploration and filtering, and (ii) adapts GRPO to GUI navigation with trajectory-aware advantages and composite rewards that balance task success and execution efficiency. Experiments on three online mobile-agent benchmarks show consistent gains, validating the effectiveness of our approach.",
        "keywords": []
      },
      "file_name": "9a67ff1d46d691f7741822d7a13587a517b1be14.pdf"
    },
    {
      "success": true,
      "doc_id": "8a501c39b890062b2d0d5b102e8875f8",
      "summary": "Current advancements in reinforcement learning (RL) have predominantly focused on learning step-based policies that generate actions for each perceived state. While these methods efficiently leverage step information from environmental interaction, they often ignore the temporal correlation between actions, resulting in inefficient exploration and unsmooth trajectories that are challenging to implement on real hardware. Episodic RL (ERL) seeks to overcome these challenges by exploring in parameters space that capture the correlation of actions. However, these approaches typically compromise data efficiency, as they treat trajectories as opaque \\emph{black boxes}. In this work, we introduce a novel ERL algorithm, Temporally-Correlated Episodic RL (TCE), which effectively utilizes step information in episodic policy updates, opening the 'black box' in existing ERL methods while retaining the smooth and consistent exploration in parameter space. TCE synergistically combines the advantages of step-based and episodic RL, achieving comparable performance to recent ERL methods while maintaining data efficiency akin to state-of-the-art (SoTA) step-based RL.",
      "intriguing_abstract": "Current advancements in reinforcement learning (RL) have predominantly focused on learning step-based policies that generate actions for each perceived state. While these methods efficiently leverage step information from environmental interaction, they often ignore the temporal correlation between actions, resulting in inefficient exploration and unsmooth trajectories that are challenging to implement on real hardware. Episodic RL (ERL) seeks to overcome these challenges by exploring in parameters space that capture the correlation of actions. However, these approaches typically compromise data efficiency, as they treat trajectories as opaque \\emph{black boxes}. In this work, we introduce a novel ERL algorithm, Temporally-Correlated Episodic RL (TCE), which effectively utilizes step information in episodic policy updates, opening the 'black box' in existing ERL methods while retaining the smooth and consistent exploration in parameter space. TCE synergistically combines the advantages of step-based and episodic RL, achieving comparable performance to recent ERL methods while maintaining data efficiency akin to state-of-the-art (SoTA) step-based RL.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/134acf45a016fced57cc8f8e171eeb6e0015b0b8.pdf",
      "citation_key": "li2024ge1",
      "metadata": {
        "title": "Open the Black Box: Step-based Policy Updates for Temporally-Correlated Episodic Reinforcement Learning",
        "authors": [
          "Ge Li",
          "Hongyi Zhou",
          "Dominik Roth",
          "Serge Thilges",
          "Fabian Otto",
          "Rudolf Lioutikov",
          "Gerhard Neumann"
        ],
        "published_date": "2024",
        "abstract": "Current advancements in reinforcement learning (RL) have predominantly focused on learning step-based policies that generate actions for each perceived state. While these methods efficiently leverage step information from environmental interaction, they often ignore the temporal correlation between actions, resulting in inefficient exploration and unsmooth trajectories that are challenging to implement on real hardware. Episodic RL (ERL) seeks to overcome these challenges by exploring in parameters space that capture the correlation of actions. However, these approaches typically compromise data efficiency, as they treat trajectories as opaque \\emph{black boxes}. In this work, we introduce a novel ERL algorithm, Temporally-Correlated Episodic RL (TCE), which effectively utilizes step information in episodic policy updates, opening the 'black box' in existing ERL methods while retaining the smooth and consistent exploration in parameter space. TCE synergistically combines the advantages of step-based and episodic RL, achieving comparable performance to recent ERL methods while maintaining data efficiency akin to state-of-the-art (SoTA) step-based RL.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/134acf45a016fced57cc8f8e171eeb6e0015b0b8.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 8,
        "score": 8.0,
        "summary": "Current advancements in reinforcement learning (RL) have predominantly focused on learning step-based policies that generate actions for each perceived state. While these methods efficiently leverage step information from environmental interaction, they often ignore the temporal correlation between actions, resulting in inefficient exploration and unsmooth trajectories that are challenging to implement on real hardware. Episodic RL (ERL) seeks to overcome these challenges by exploring in parameters space that capture the correlation of actions. However, these approaches typically compromise data efficiency, as they treat trajectories as opaque \\emph{black boxes}. In this work, we introduce a novel ERL algorithm, Temporally-Correlated Episodic RL (TCE), which effectively utilizes step information in episodic policy updates, opening the 'black box' in existing ERL methods while retaining the smooth and consistent exploration in parameter space. TCE synergistically combines the advantages of step-based and episodic RL, achieving comparable performance to recent ERL methods while maintaining data efficiency akin to state-of-the-art (SoTA) step-based RL.",
        "keywords": []
      },
      "file_name": "134acf45a016fced57cc8f8e171eeb6e0015b0b8.pdf"
    },
    {
      "success": true,
      "doc_id": "2dd256452e222ea09ce42575cd882a02",
      "summary": "Reinforcement Learning (RL) has emerged as a powerful paradigm in Artificial Intelligence (AI), enabling agents to learn optimal behaviors through interactions with their environments. Drawing from the foundations of trial and error, RL equips agents to make informed decisions through feedback in the form of rewards or penalties. This paper presents a comprehensive survey of RL, meticulously analyzing a wide range of algorithms, from foundational tabular methods to advanced Deep Reinforcement Learning (DRL) techniques. We categorize and evaluate these algorithms based on key criteria such as scalability, sample efficiency, and suitability. We compare the methods in the form of their strengths and weaknesses in diverse settings. Additionally, we offer practical insights into the selection and implementation of RL algorithms, addressing common challenges like convergence, stability, and the exploration-exploitation dilemma. This paper serves as a comprehensive reference for researchers and practitioners aiming to harness the full potential of RL in solving complex, real-world problems.",
      "intriguing_abstract": "Reinforcement Learning (RL) has emerged as a powerful paradigm in Artificial Intelligence (AI), enabling agents to learn optimal behaviors through interactions with their environments. Drawing from the foundations of trial and error, RL equips agents to make informed decisions through feedback in the form of rewards or penalties. This paper presents a comprehensive survey of RL, meticulously analyzing a wide range of algorithms, from foundational tabular methods to advanced Deep Reinforcement Learning (DRL) techniques. We categorize and evaluate these algorithms based on key criteria such as scalability, sample efficiency, and suitability. We compare the methods in the form of their strengths and weaknesses in diverse settings. Additionally, we offer practical insights into the selection and implementation of RL algorithms, addressing common challenges like convergence, stability, and the exploration-exploitation dilemma. This paper serves as a comprehensive reference for researchers and practitioners aiming to harness the full potential of RL in solving complex, real-world problems.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/a9c896060fa85f01f289baaad346e98e94dbed4c.pdf",
      "citation_key": "ghasemi2024j43",
      "metadata": {
        "title": "A Comprehensive Survey of Reinforcement Learning: From Algorithms to Practical Challenges",
        "authors": [
          "Majid Ghasemi",
          "Amir Hossein Moosavi",
          "Dariush Ebrahimi"
        ],
        "published_date": "2024",
        "abstract": "Reinforcement Learning (RL) has emerged as a powerful paradigm in Artificial Intelligence (AI), enabling agents to learn optimal behaviors through interactions with their environments. Drawing from the foundations of trial and error, RL equips agents to make informed decisions through feedback in the form of rewards or penalties. This paper presents a comprehensive survey of RL, meticulously analyzing a wide range of algorithms, from foundational tabular methods to advanced Deep Reinforcement Learning (DRL) techniques. We categorize and evaluate these algorithms based on key criteria such as scalability, sample efficiency, and suitability. We compare the methods in the form of their strengths and weaknesses in diverse settings. Additionally, we offer practical insights into the selection and implementation of RL algorithms, addressing common challenges like convergence, stability, and the exploration-exploitation dilemma. This paper serves as a comprehensive reference for researchers and practitioners aiming to harness the full potential of RL in solving complex, real-world problems.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/a9c896060fa85f01f289baaad346e98e94dbed4c.pdf",
        "venue": "",
        "citationCount": 8,
        "score": 8.0,
        "summary": "Reinforcement Learning (RL) has emerged as a powerful paradigm in Artificial Intelligence (AI), enabling agents to learn optimal behaviors through interactions with their environments. Drawing from the foundations of trial and error, RL equips agents to make informed decisions through feedback in the form of rewards or penalties. This paper presents a comprehensive survey of RL, meticulously analyzing a wide range of algorithms, from foundational tabular methods to advanced Deep Reinforcement Learning (DRL) techniques. We categorize and evaluate these algorithms based on key criteria such as scalability, sample efficiency, and suitability. We compare the methods in the form of their strengths and weaknesses in diverse settings. Additionally, we offer practical insights into the selection and implementation of RL algorithms, addressing common challenges like convergence, stability, and the exploration-exploitation dilemma. This paper serves as a comprehensive reference for researchers and practitioners aiming to harness the full potential of RL in solving complex, real-world problems.",
        "keywords": []
      },
      "file_name": "a9c896060fa85f01f289baaad346e98e94dbed4c.pdf"
    },
    {
      "success": true,
      "doc_id": "2071e691a01c4e039f8b7bdedc07f5d4",
      "summary": "Assembly positioning by visual servoing (VS) is a basis for autonomous robotic assembly. In practice, VS control suffers potential stability and convergence problems due to image and physical constraints, e.g., field of view constraints, image local minima, obstacle collisions, and occlusion. Therefore, this article proposes a novel deep reinforcement learning-based hybrid visual servoing (DRL-HVS) controller for motion planning of VS tasks. DRL-HVS controller takes current observed image features and camera pose as inputs, and the core parameters of hybrid VS are dynamically optimized using a deep deterministic policy gradient (DDPG) algorithm to obtain an optimal motion scheme, considering image/physical constraints and robot motion performance. In addition, an adaptive exploration strategy is proposed to further improve the training efficiency by adaptively tuning the exploration noise parameters. In this way, the offline pretrained DRL-HVS controller in the virtual environment, where the DDPG actorcritic network is continuously optimized, can be quickly deployed to a real robot system for real-time control. Experiments based on an eye-in-hand VS system are conducted with a calibrated HIKVISION RGB camera mounted on the end-effector of a GSK-RB03A1 six degree-of-freedom (6-DoF) robot. Basic VS task experiments show that the proposed controller achieves better performance than the existing methods: the servoing time is 24% smaller than that of the five-dimensional VS method, a 100% success rate with the perturbed ranges of the initial position within 25 mm for translation and 20 for rotation, and a 48% efficiency improvement. Moreover, a planetary gear component assembly process case study, where the robot aims to automatically put the gears on the gear shafts, is conducted to demonstrate the applicability of the proposed method in practice.",
      "intriguing_abstract": "Assembly positioning by visual servoing (VS) is a basis for autonomous robotic assembly. In practice, VS control suffers potential stability and convergence problems due to image and physical constraints, e.g., field of view constraints, image local minima, obstacle collisions, and occlusion. Therefore, this article proposes a novel deep reinforcement learning-based hybrid visual servoing (DRL-HVS) controller for motion planning of VS tasks. DRL-HVS controller takes current observed image features and camera pose as inputs, and the core parameters of hybrid VS are dynamically optimized using a deep deterministic policy gradient (DDPG) algorithm to obtain an optimal motion scheme, considering image/physical constraints and robot motion performance. In addition, an adaptive exploration strategy is proposed to further improve the training efficiency by adaptively tuning the exploration noise parameters. In this way, the offline pretrained DRL-HVS controller in the virtual environment, where the DDPG actorcritic network is continuously optimized, can be quickly deployed to a real robot system for real-time control. Experiments based on an eye-in-hand VS system are conducted with a calibrated HIKVISION RGB camera mounted on the end-effector of a GSK-RB03A1 six degree-of-freedom (6-DoF) robot. Basic VS task experiments show that the proposed controller achieves better performance than the existing methods: the servoing time is 24% smaller than that of the five-dimensional VS method, a 100% success rate with the perturbed ranges of the initial position within 25 mm for translation and 20 for rotation, and a 48% efficiency improvement. Moreover, a planetary gear component assembly process case study, where the robot aims to automatically put the gears on the gear shafts, is conducted to demonstrate the applicability of the proposed method in practice.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/68de2fdc9b516eba96b9726dfa0e77ee66336605.pdf",
      "citation_key": "liu2023729",
      "metadata": {
        "title": "A Motion Planning Method for Visual Servoing Using Deep Reinforcement Learning in Autonomous Robotic Assembly",
        "authors": [
          "Zhen-yu Liu",
          "Ke Wang",
          "Daxin Liu",
          "Qide Wang",
          "Jianrong Tan"
        ],
        "published_date": "2023",
        "abstract": "Assembly positioning by visual servoing (VS) is a basis for autonomous robotic assembly. In practice, VS control suffers potential stability and convergence problems due to image and physical constraints, e.g., field of view constraints, image local minima, obstacle collisions, and occlusion. Therefore, this article proposes a novel deep reinforcement learning-based hybrid visual servoing (DRL-HVS) controller for motion planning of VS tasks. DRL-HVS controller takes current observed image features and camera pose as inputs, and the core parameters of hybrid VS are dynamically optimized using a deep deterministic policy gradient (DDPG) algorithm to obtain an optimal motion scheme, considering image/physical constraints and robot motion performance. In addition, an adaptive exploration strategy is proposed to further improve the training efficiency by adaptively tuning the exploration noise parameters. In this way, the offline pretrained DRL-HVS controller in the virtual environment, where the DDPG actorcritic network is continuously optimized, can be quickly deployed to a real robot system for real-time control. Experiments based on an eye-in-hand VS system are conducted with a calibrated HIKVISION RGB camera mounted on the end-effector of a GSK-RB03A1 six degree-of-freedom (6-DoF) robot. Basic VS task experiments show that the proposed controller achieves better performance than the existing methods: the servoing time is 24% smaller than that of the five-dimensional VS method, a 100% success rate with the perturbed ranges of the initial position within 25 mm for translation and 20 for rotation, and a 48% efficiency improvement. Moreover, a planetary gear component assembly process case study, where the robot aims to automatically put the gears on the gear shafts, is conducted to demonstrate the applicability of the proposed method in practice.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/68de2fdc9b516eba96b9726dfa0e77ee66336605.pdf",
        "venue": "IEEE/ASME transactions on mechatronics",
        "citationCount": 15,
        "score": 7.5,
        "summary": "Assembly positioning by visual servoing (VS) is a basis for autonomous robotic assembly. In practice, VS control suffers potential stability and convergence problems due to image and physical constraints, e.g., field of view constraints, image local minima, obstacle collisions, and occlusion. Therefore, this article proposes a novel deep reinforcement learning-based hybrid visual servoing (DRL-HVS) controller for motion planning of VS tasks. DRL-HVS controller takes current observed image features and camera pose as inputs, and the core parameters of hybrid VS are dynamically optimized using a deep deterministic policy gradient (DDPG) algorithm to obtain an optimal motion scheme, considering image/physical constraints and robot motion performance. In addition, an adaptive exploration strategy is proposed to further improve the training efficiency by adaptively tuning the exploration noise parameters. In this way, the offline pretrained DRL-HVS controller in the virtual environment, where the DDPG actorcritic network is continuously optimized, can be quickly deployed to a real robot system for real-time control. Experiments based on an eye-in-hand VS system are conducted with a calibrated HIKVISION RGB camera mounted on the end-effector of a GSK-RB03A1 six degree-of-freedom (6-DoF) robot. Basic VS task experiments show that the proposed controller achieves better performance than the existing methods: the servoing time is 24% smaller than that of the five-dimensional VS method, a 100% success rate with the perturbed ranges of the initial position within 25 mm for translation and 20 for rotation, and a 48% efficiency improvement. Moreover, a planetary gear component assembly process case study, where the robot aims to automatically put the gears on the gear shafts, is conducted to demonstrate the applicability of the proposed method in practice.",
        "keywords": []
      },
      "file_name": "68de2fdc9b516eba96b9726dfa0e77ee66336605.pdf"
    },
    {
      "success": true,
      "doc_id": "1852147937732df5c0207edf8cc39b47",
      "summary": "In this article, a novel reinforcement learning (RL) approach, continuous dynamic policy programming (CDPP), is proposed to tackle the issues of both learning stability and sample efficiency in the current RL methods with continuous actions. The proposed method naturally extends the relative entropy regularization from the value function-based framework to the actorcritic (AC) framework of deep deterministic policy gradient (DDPG) to stabilize the learning process in continuous action space. It tackles the intractable softmax operation over continuous actions in the critic by Monte Carlo estimation and explores the practical advantages of the Mellowmax operator. A Boltzmann sampling policy is proposed to guide the exploration of actor following the relative entropy regularized critic for superior learning capability, exploration efficiency, and robustness. Evaluated by several benchmark and real-robot-based simulation tasks, the proposed method illustrates the positive impact of the relative entropy regularization including efficient exploration behavior and stable policy update in RL with continuous action space and successfully outperforms the related baseline approaches in both sample efficiency and learning stability.",
      "intriguing_abstract": "In this article, a novel reinforcement learning (RL) approach, continuous dynamic policy programming (CDPP), is proposed to tackle the issues of both learning stability and sample efficiency in the current RL methods with continuous actions. The proposed method naturally extends the relative entropy regularization from the value function-based framework to the actorcritic (AC) framework of deep deterministic policy gradient (DDPG) to stabilize the learning process in continuous action space. It tackles the intractable softmax operation over continuous actions in the critic by Monte Carlo estimation and explores the practical advantages of the Mellowmax operator. A Boltzmann sampling policy is proposed to guide the exploration of actor following the relative entropy regularized critic for superior learning capability, exploration efficiency, and robustness. Evaluated by several benchmark and real-robot-based simulation tasks, the proposed method illustrates the positive impact of the relative entropy regularization including efficient exploration behavior and stable policy update in RL with continuous action space and successfully outperforms the related baseline approaches in both sample efficiency and learning stability.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/01f35fa70fc881ab80206121738380c57f8d2074.pdf",
      "citation_key": "shang202305k",
      "metadata": {
        "title": "Relative Entropy Regularized Sample-Efficient Reinforcement Learning With Continuous Actions",
        "authors": [
          "Zhiwei Shang",
          "Renxing Li",
          "Chunhuang Zheng",
          "Huiyun Li",
          "Yunduan Cui"
        ],
        "published_date": "2023",
        "abstract": "In this article, a novel reinforcement learning (RL) approach, continuous dynamic policy programming (CDPP), is proposed to tackle the issues of both learning stability and sample efficiency in the current RL methods with continuous actions. The proposed method naturally extends the relative entropy regularization from the value function-based framework to the actorcritic (AC) framework of deep deterministic policy gradient (DDPG) to stabilize the learning process in continuous action space. It tackles the intractable softmax operation over continuous actions in the critic by Monte Carlo estimation and explores the practical advantages of the Mellowmax operator. A Boltzmann sampling policy is proposed to guide the exploration of actor following the relative entropy regularized critic for superior learning capability, exploration efficiency, and robustness. Evaluated by several benchmark and real-robot-based simulation tasks, the proposed method illustrates the positive impact of the relative entropy regularization including efficient exploration behavior and stable policy update in RL with continuous action space and successfully outperforms the related baseline approaches in both sample efficiency and learning stability.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/01f35fa70fc881ab80206121738380c57f8d2074.pdf",
        "venue": "IEEE Transactions on Neural Networks and Learning Systems",
        "citationCount": 15,
        "score": 7.5,
        "summary": "In this article, a novel reinforcement learning (RL) approach, continuous dynamic policy programming (CDPP), is proposed to tackle the issues of both learning stability and sample efficiency in the current RL methods with continuous actions. The proposed method naturally extends the relative entropy regularization from the value function-based framework to the actorcritic (AC) framework of deep deterministic policy gradient (DDPG) to stabilize the learning process in continuous action space. It tackles the intractable softmax operation over continuous actions in the critic by Monte Carlo estimation and explores the practical advantages of the Mellowmax operator. A Boltzmann sampling policy is proposed to guide the exploration of actor following the relative entropy regularized critic for superior learning capability, exploration efficiency, and robustness. Evaluated by several benchmark and real-robot-based simulation tasks, the proposed method illustrates the positive impact of the relative entropy regularization including efficient exploration behavior and stable policy update in RL with continuous action space and successfully outperforms the related baseline approaches in both sample efficiency and learning stability.",
        "keywords": []
      },
      "file_name": "01f35fa70fc881ab80206121738380c57f8d2074.pdf"
    },
    {
      "success": true,
      "doc_id": "fa98566a5def32e8eda9ac8d49d19637",
      "summary": "Effective exploration is crucial to discovering optimal strategies for multi-agent reinforcement learning (MARL) in complex coordination tasks. Existing methods mainly utilize intrinsic rewards to enable committed exploration or use role-based learning for decomposing joint action spaces instead of directly conducting a collective search in the entire action-observation space. However, they often face challenges obtaining specific joint action sequences to reach successful states in long-horizon tasks. To address this limitation, we propose Imagine, Initialize, and Explore (IIE), a novel method that offers a promising solution for efficient multi-agent exploration in complex scenarios. IIE employs a transformer model to imagine how the agents reach a critical state that can influence each other's transition functions. Then, we initialize the environment at this state using a simulator before the exploration phase. We formulate the imagination as a sequence modeling problem, where the states, observations, prompts, actions, and rewards are predicted autoregressively. The prompt consists of timestep-to-go, return-to-go, influence value, and one-shot demonstration, specifying the desired state and trajectory as well as guiding the action generation. By initializing agents at the critical states, IIE significantly increases the likelihood of discovering potentially important under-explored regions. Despite its simplicity, empirical results demonstrate that our method outperforms multi-agent exploration baselines on the StarCraft Multi-Agent Challenge (SMAC) and SMACv2 environments. Particularly, IIE shows improved performance in the sparse-reward SMAC tasks and produces more effective curricula over the initialized states than other generative methods, such as CVAE-GAN and diffusion models.",
      "intriguing_abstract": "Effective exploration is crucial to discovering optimal strategies for multi-agent reinforcement learning (MARL) in complex coordination tasks. Existing methods mainly utilize intrinsic rewards to enable committed exploration or use role-based learning for decomposing joint action spaces instead of directly conducting a collective search in the entire action-observation space. However, they often face challenges obtaining specific joint action sequences to reach successful states in long-horizon tasks. To address this limitation, we propose Imagine, Initialize, and Explore (IIE), a novel method that offers a promising solution for efficient multi-agent exploration in complex scenarios. IIE employs a transformer model to imagine how the agents reach a critical state that can influence each other's transition functions. Then, we initialize the environment at this state using a simulator before the exploration phase. We formulate the imagination as a sequence modeling problem, where the states, observations, prompts, actions, and rewards are predicted autoregressively. The prompt consists of timestep-to-go, return-to-go, influence value, and one-shot demonstration, specifying the desired state and trajectory as well as guiding the action generation. By initializing agents at the critical states, IIE significantly increases the likelihood of discovering potentially important under-explored regions. Despite its simplicity, empirical results demonstrate that our method outperforms multi-agent exploration baselines on the StarCraft Multi-Agent Challenge (SMAC) and SMACv2 environments. Particularly, IIE shows improved performance in the sparse-reward SMAC tasks and produces more effective curricula over the initialized states than other generative methods, such as CVAE-GAN and diffusion models.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/5dce3f2cfe018ada4bebeaa15aac6fe1d01b18ba.pdf",
      "citation_key": "liu2024xkk",
      "metadata": {
        "title": "Imagine, Initialize, and Explore: An Effective Exploration Method in Multi-Agent Reinforcement Learning",
        "authors": [
          "Zeyang Liu",
          "Lipeng Wan",
          "Xinrui Yang",
          "Zhuoran Chen",
          "Xingyu Chen",
          "Xuguang Lan"
        ],
        "published_date": "2024",
        "abstract": "Effective exploration is crucial to discovering optimal strategies for multi-agent reinforcement learning (MARL) in complex coordination tasks. Existing methods mainly utilize intrinsic rewards to enable committed exploration or use role-based learning for decomposing joint action spaces instead of directly conducting a collective search in the entire action-observation space. However, they often face challenges obtaining specific joint action sequences to reach successful states in long-horizon tasks. To address this limitation, we propose Imagine, Initialize, and Explore (IIE), a novel method that offers a promising solution for efficient multi-agent exploration in complex scenarios. IIE employs a transformer model to imagine how the agents reach a critical state that can influence each other's transition functions. Then, we initialize the environment at this state using a simulator before the exploration phase. We formulate the imagination as a sequence modeling problem, where the states, observations, prompts, actions, and rewards are predicted autoregressively. The prompt consists of timestep-to-go, return-to-go, influence value, and one-shot demonstration, specifying the desired state and trajectory as well as guiding the action generation. By initializing agents at the critical states, IIE significantly increases the likelihood of discovering potentially important under-explored regions. Despite its simplicity, empirical results demonstrate that our method outperforms multi-agent exploration baselines on the StarCraft Multi-Agent Challenge (SMAC) and SMACv2 environments. Particularly, IIE shows improved performance in the sparse-reward SMAC tasks and produces more effective curricula over the initialized states than other generative methods, such as CVAE-GAN and diffusion models.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/5dce3f2cfe018ada4bebeaa15aac6fe1d01b18ba.pdf",
        "venue": "AAAI Conference on Artificial Intelligence",
        "citationCount": 7,
        "score": 7.0,
        "summary": "Effective exploration is crucial to discovering optimal strategies for multi-agent reinforcement learning (MARL) in complex coordination tasks. Existing methods mainly utilize intrinsic rewards to enable committed exploration or use role-based learning for decomposing joint action spaces instead of directly conducting a collective search in the entire action-observation space. However, they often face challenges obtaining specific joint action sequences to reach successful states in long-horizon tasks. To address this limitation, we propose Imagine, Initialize, and Explore (IIE), a novel method that offers a promising solution for efficient multi-agent exploration in complex scenarios. IIE employs a transformer model to imagine how the agents reach a critical state that can influence each other's transition functions. Then, we initialize the environment at this state using a simulator before the exploration phase. We formulate the imagination as a sequence modeling problem, where the states, observations, prompts, actions, and rewards are predicted autoregressively. The prompt consists of timestep-to-go, return-to-go, influence value, and one-shot demonstration, specifying the desired state and trajectory as well as guiding the action generation. By initializing agents at the critical states, IIE significantly increases the likelihood of discovering potentially important under-explored regions. Despite its simplicity, empirical results demonstrate that our method outperforms multi-agent exploration baselines on the StarCraft Multi-Agent Challenge (SMAC) and SMACv2 environments. Particularly, IIE shows improved performance in the sparse-reward SMAC tasks and produces more effective curricula over the initialized states than other generative methods, such as CVAE-GAN and diffusion models.",
        "keywords": []
      },
      "file_name": "5dce3f2cfe018ada4bebeaa15aac6fe1d01b18ba.pdf"
    },
    {
      "success": true,
      "doc_id": "4405f2e608e554b5e9b7af1c4abb9bd0",
      "summary": "In this article, we address the problem of learning optimal control policies for systems with uncertain dynamics and high-level control objectives specified as linear temporal logic (LTL) formulas. Uncertainty is considered in the workspace structure and the outcomes of control decisions giving rise to an unknown Markov decision process (MDP). Existing reinforcement learning (RL) algorithms for LTL tasks typically rely on exploring a product MDP state-space uniformly (using e.g., an $\\epsilon$-greedy policy) compromising sample-efficiency. This issue becomes more pronounced as the rewards get sparser and the MDP size or the task complexity increase. In this article, we propose an accelerated RL algorithm that can learn control policies significantly faster than competitive approaches. Its sample-efficiency relies on a novel task-driven exploration strategy that biases exploration toward directions that may contribute to task satisfaction. We provide theoretical analysis and extensive comparative experiments demonstrating the sample-efficiency of the proposed method. The benefit of our method becomes more evident as the task complexity or the MDP size increases.",
      "intriguing_abstract": "In this article, we address the problem of learning optimal control policies for systems with uncertain dynamics and high-level control objectives specified as linear temporal logic (LTL) formulas. Uncertainty is considered in the workspace structure and the outcomes of control decisions giving rise to an unknown Markov decision process (MDP). Existing reinforcement learning (RL) algorithms for LTL tasks typically rely on exploring a product MDP state-space uniformly (using e.g., an $\\epsilon$-greedy policy) compromising sample-efficiency. This issue becomes more pronounced as the rewards get sparser and the MDP size or the task complexity increase. In this article, we propose an accelerated RL algorithm that can learn control policies significantly faster than competitive approaches. Its sample-efficiency relies on a novel task-driven exploration strategy that biases exploration toward directions that may contribute to task satisfaction. We provide theoretical analysis and extensive comparative experiments demonstrating the sample-efficiency of the proposed method. The benefit of our method becomes more evident as the task complexity or the MDP size increases.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/390ba2d438a33bf0add55e0e7b2831fc034db41f.pdf",
      "citation_key": "kantaros2024sgn",
      "metadata": {
        "title": "Sample-Efficient Reinforcement Learning With Temporal Logic Objectives: Leveraging the Task Specification to Guide Exploration",
        "authors": [
          "Y. Kantaros",
          "Jun Wang"
        ],
        "published_date": "2024",
        "abstract": "In this article, we address the problem of learning optimal control policies for systems with uncertain dynamics and high-level control objectives specified as linear temporal logic (LTL) formulas. Uncertainty is considered in the workspace structure and the outcomes of control decisions giving rise to an unknown Markov decision process (MDP). Existing reinforcement learning (RL) algorithms for LTL tasks typically rely on exploring a product MDP state-space uniformly (using e.g., an $\\epsilon$-greedy policy) compromising sample-efficiency. This issue becomes more pronounced as the rewards get sparser and the MDP size or the task complexity increase. In this article, we propose an accelerated RL algorithm that can learn control policies significantly faster than competitive approaches. Its sample-efficiency relies on a novel task-driven exploration strategy that biases exploration toward directions that may contribute to task satisfaction. We provide theoretical analysis and extensive comparative experiments demonstrating the sample-efficiency of the proposed method. The benefit of our method becomes more evident as the task complexity or the MDP size increases.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/390ba2d438a33bf0add55e0e7b2831fc034db41f.pdf",
        "venue": "IEEE Transactions on Automatic Control",
        "citationCount": 7,
        "score": 7.0,
        "summary": "In this article, we address the problem of learning optimal control policies for systems with uncertain dynamics and high-level control objectives specified as linear temporal logic (LTL) formulas. Uncertainty is considered in the workspace structure and the outcomes of control decisions giving rise to an unknown Markov decision process (MDP). Existing reinforcement learning (RL) algorithms for LTL tasks typically rely on exploring a product MDP state-space uniformly (using e.g., an $\\epsilon$-greedy policy) compromising sample-efficiency. This issue becomes more pronounced as the rewards get sparser and the MDP size or the task complexity increase. In this article, we propose an accelerated RL algorithm that can learn control policies significantly faster than competitive approaches. Its sample-efficiency relies on a novel task-driven exploration strategy that biases exploration toward directions that may contribute to task satisfaction. We provide theoretical analysis and extensive comparative experiments demonstrating the sample-efficiency of the proposed method. The benefit of our method becomes more evident as the task complexity or the MDP size increases.",
        "keywords": []
      },
      "file_name": "390ba2d438a33bf0add55e0e7b2831fc034db41f.pdf"
    },
    {
      "success": true,
      "doc_id": "99fb12403259fa94a5356d42878d04f6",
      "summary": "Model compression methods are being developed to bridge the gap between the massive scale of neural networks and the limited hardware resources on edge devices. Since most real-world applications deployed on resource-limited hardware platforms typically have multiple hardware constraints simultaneously, most existing model compression approaches that only consider optimizing one single hardware objective are ineffective. In this article, we propose an automated pruning method called multi-constrained model compression (MCMC) that allows for the optimization of multiple hardware targets, such as latency, floating point operations (FLOPs), and memory usage, while minimizing the impact on accuracy. Specifically, we propose an improved multi-objective reinforcement learning (MORL) algorithm, the one-stage envelope deep deterministic policy gradient (DDPG) algorithm, to determine the pruning strategy for neural networks. Our improved one-stage envelope DDPG algorithm reduces exploration time and offers greater flexibility in adjusting target priorities, enhancing its suitability for pruning tasks. For instance, on the visual geometry group (VGG)-16 network, our method achieved an 80% reduction in FLOPs, a <inline-formula> <tex-math notation=\"LaTeX\">$2.31\\times $ </tex-math></inline-formula> reduction in memory usage, and a <inline-formula> <tex-math notation=\"LaTeX\">$1.92\\times $ </tex-math></inline-formula> acceleration, with an accuracy improvement of 0.09% compared with the baseline. For larger datasets, such as ImageNet, we reduced FLOPs by 50% for MobileNet-V1, resulting in a <inline-formula> <tex-math notation=\"LaTeX\">$4.7\\times $ </tex-math></inline-formula> faster speed and <inline-formula> <tex-math notation=\"LaTeX\">$1.48\\times $ </tex-math></inline-formula> memory compression, while maintaining the same accuracy. When applied to edge devices, such as JETSON XAVIER NX, our method resulted in a 71% reduction in FLOPs for MobileNet-V1, leading to a <inline-formula> <tex-math notation=\"LaTeX\">$1.63\\times $ </tex-math></inline-formula> faster speed, <inline-formula> <tex-math notation=\"LaTeX\">$1.64\\times $ </tex-math></inline-formula> memory compression, and an accuracy improvement.",
      "intriguing_abstract": "Model compression methods are being developed to bridge the gap between the massive scale of neural networks and the limited hardware resources on edge devices. Since most real-world applications deployed on resource-limited hardware platforms typically have multiple hardware constraints simultaneously, most existing model compression approaches that only consider optimizing one single hardware objective are ineffective. In this article, we propose an automated pruning method called multi-constrained model compression (MCMC) that allows for the optimization of multiple hardware targets, such as latency, floating point operations (FLOPs), and memory usage, while minimizing the impact on accuracy. Specifically, we propose an improved multi-objective reinforcement learning (MORL) algorithm, the one-stage envelope deep deterministic policy gradient (DDPG) algorithm, to determine the pruning strategy for neural networks. Our improved one-stage envelope DDPG algorithm reduces exploration time and offers greater flexibility in adjusting target priorities, enhancing its suitability for pruning tasks. For instance, on the visual geometry group (VGG)-16 network, our method achieved an 80% reduction in FLOPs, a <inline-formula> <tex-math notation=\"LaTeX\">$2.31\\times $ </tex-math></inline-formula> reduction in memory usage, and a <inline-formula> <tex-math notation=\"LaTeX\">$1.92\\times $ </tex-math></inline-formula> acceleration, with an accuracy improvement of 0.09% compared with the baseline. For larger datasets, such as ImageNet, we reduced FLOPs by 50% for MobileNet-V1, resulting in a <inline-formula> <tex-math notation=\"LaTeX\">$4.7\\times $ </tex-math></inline-formula> faster speed and <inline-formula> <tex-math notation=\"LaTeX\">$1.48\\times $ </tex-math></inline-formula> memory compression, while maintaining the same accuracy. When applied to edge devices, such as JETSON XAVIER NX, our method resulted in a 71% reduction in FLOPs for MobileNet-V1, leading to a <inline-formula> <tex-math notation=\"LaTeX\">$1.63\\times $ </tex-math></inline-formula> faster speed, <inline-formula> <tex-math notation=\"LaTeX\">$1.64\\times $ </tex-math></inline-formula> memory compression, and an accuracy improvement.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/085dddfa3105967d4b9f09b1cd0fa7725779faf1.pdf",
      "citation_key": "li2024zix",
      "metadata": {
        "title": "MCMC: Multi-Constrained Model Compression via One-Stage Envelope Reinforcement Learning",
        "authors": [
          "Siqi Li",
          "Jun Chen",
          "Shanqi Liu",
          "Chengrui Zhu",
          "Guanzhong Tian",
          "Yong Liu"
        ],
        "published_date": "2024",
        "abstract": "Model compression methods are being developed to bridge the gap between the massive scale of neural networks and the limited hardware resources on edge devices. Since most real-world applications deployed on resource-limited hardware platforms typically have multiple hardware constraints simultaneously, most existing model compression approaches that only consider optimizing one single hardware objective are ineffective. In this article, we propose an automated pruning method called multi-constrained model compression (MCMC) that allows for the optimization of multiple hardware targets, such as latency, floating point operations (FLOPs), and memory usage, while minimizing the impact on accuracy. Specifically, we propose an improved multi-objective reinforcement learning (MORL) algorithm, the one-stage envelope deep deterministic policy gradient (DDPG) algorithm, to determine the pruning strategy for neural networks. Our improved one-stage envelope DDPG algorithm reduces exploration time and offers greater flexibility in adjusting target priorities, enhancing its suitability for pruning tasks. For instance, on the visual geometry group (VGG)-16 network, our method achieved an 80% reduction in FLOPs, a <inline-formula> <tex-math notation=\"LaTeX\">$2.31\\times $ </tex-math></inline-formula> reduction in memory usage, and a <inline-formula> <tex-math notation=\"LaTeX\">$1.92\\times $ </tex-math></inline-formula> acceleration, with an accuracy improvement of 0.09% compared with the baseline. For larger datasets, such as ImageNet, we reduced FLOPs by 50% for MobileNet-V1, resulting in a <inline-formula> <tex-math notation=\"LaTeX\">$4.7\\times $ </tex-math></inline-formula> faster speed and <inline-formula> <tex-math notation=\"LaTeX\">$1.48\\times $ </tex-math></inline-formula> memory compression, while maintaining the same accuracy. When applied to edge devices, such as JETSON XAVIER NX, our method resulted in a 71% reduction in FLOPs for MobileNet-V1, leading to a <inline-formula> <tex-math notation=\"LaTeX\">$1.63\\times $ </tex-math></inline-formula> faster speed, <inline-formula> <tex-math notation=\"LaTeX\">$1.64\\times $ </tex-math></inline-formula> memory compression, and an accuracy improvement.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/085dddfa3105967d4b9f09b1cd0fa7725779faf1.pdf",
        "venue": "IEEE Transactions on Neural Networks and Learning Systems",
        "citationCount": 7,
        "score": 7.0,
        "summary": "Model compression methods are being developed to bridge the gap between the massive scale of neural networks and the limited hardware resources on edge devices. Since most real-world applications deployed on resource-limited hardware platforms typically have multiple hardware constraints simultaneously, most existing model compression approaches that only consider optimizing one single hardware objective are ineffective. In this article, we propose an automated pruning method called multi-constrained model compression (MCMC) that allows for the optimization of multiple hardware targets, such as latency, floating point operations (FLOPs), and memory usage, while minimizing the impact on accuracy. Specifically, we propose an improved multi-objective reinforcement learning (MORL) algorithm, the one-stage envelope deep deterministic policy gradient (DDPG) algorithm, to determine the pruning strategy for neural networks. Our improved one-stage envelope DDPG algorithm reduces exploration time and offers greater flexibility in adjusting target priorities, enhancing its suitability for pruning tasks. For instance, on the visual geometry group (VGG)-16 network, our method achieved an 80% reduction in FLOPs, a <inline-formula> <tex-math notation=\"LaTeX\">$2.31\\times $ </tex-math></inline-formula> reduction in memory usage, and a <inline-formula> <tex-math notation=\"LaTeX\">$1.92\\times $ </tex-math></inline-formula> acceleration, with an accuracy improvement of 0.09% compared with the baseline. For larger datasets, such as ImageNet, we reduced FLOPs by 50% for MobileNet-V1, resulting in a <inline-formula> <tex-math notation=\"LaTeX\">$4.7\\times $ </tex-math></inline-formula> faster speed and <inline-formula> <tex-math notation=\"LaTeX\">$1.48\\times $ </tex-math></inline-formula> memory compression, while maintaining the same accuracy. When applied to edge devices, such as JETSON XAVIER NX, our method resulted in a 71% reduction in FLOPs for MobileNet-V1, leading to a <inline-formula> <tex-math notation=\"LaTeX\">$1.63\\times $ </tex-math></inline-formula> faster speed, <inline-formula> <tex-math notation=\"LaTeX\">$1.64\\times $ </tex-math></inline-formula> memory compression, and an accuracy improvement.",
        "keywords": []
      },
      "file_name": "085dddfa3105967d4b9f09b1cd0fa7725779faf1.pdf"
    },
    {
      "success": true,
      "doc_id": "876ddac66c2a0a6048c966e49eb3917c",
      "summary": "The discovery of novel therapeutic compounds through de novo drug design represents a critical challenge in the field of pharmaceutical research. Traditional drug discovery approaches are often resource intensive and time consuming, leading researchers to explore innovative methods that harness the power of deep learning and reinforcement learning techniques. Here, we introduce a novel drug design approach called drugAI that leverages the EncoderDecoder Transformer architecture in tandem with Reinforcement Learning via a Monte Carlo Tree Search (RL-MCTS) to expedite the process of drug discovery while ensuring the production of valid small molecules with drug-like characteristics and strong binding affinities towards their targets. We successfully integrated the EncoderDecoder Transformer architecture, which generates molecular structures (drugs) from scratch with the RL-MCTS, serving as a reinforcement learning framework. The RL-MCTS combines the exploitation and exploration capabilities of a Monte Carlo Tree Search with the machine translation of a transformer-based EncoderDecoder model. This dynamic approach allows the model to iteratively refine its drug candidate generation process, ensuring that the generated molecules adhere to essential physicochemical and biological constraints and effectively bind to their targets. The results from drugAI showcase the effectiveness of the proposed approach across various benchmark datasets, demonstrating a significant improvement in both the validity and drug-likeness of the generated compounds, compared to two existing benchmark methods. Moreover, drugAI ensures that the generated molecules exhibit strong binding affinities to their respective targets. In summary, this research highlights the real-world applications of drugAI in drug discovery pipelines, potentially accelerating the identification of promising drug candidates for a wide range of diseases.",
      "intriguing_abstract": "The discovery of novel therapeutic compounds through de novo drug design represents a critical challenge in the field of pharmaceutical research. Traditional drug discovery approaches are often resource intensive and time consuming, leading researchers to explore innovative methods that harness the power of deep learning and reinforcement learning techniques. Here, we introduce a novel drug design approach called drugAI that leverages the EncoderDecoder Transformer architecture in tandem with Reinforcement Learning via a Monte Carlo Tree Search (RL-MCTS) to expedite the process of drug discovery while ensuring the production of valid small molecules with drug-like characteristics and strong binding affinities towards their targets. We successfully integrated the EncoderDecoder Transformer architecture, which generates molecular structures (drugs) from scratch with the RL-MCTS, serving as a reinforcement learning framework. The RL-MCTS combines the exploitation and exploration capabilities of a Monte Carlo Tree Search with the machine translation of a transformer-based EncoderDecoder model. This dynamic approach allows the model to iteratively refine its drug candidate generation process, ensuring that the generated molecules adhere to essential physicochemical and biological constraints and effectively bind to their targets. The results from drugAI showcase the effectiveness of the proposed approach across various benchmark datasets, demonstrating a significant improvement in both the validity and drug-likeness of the generated compounds, compared to two existing benchmark methods. Moreover, drugAI ensures that the generated molecules exhibit strong binding affinities to their respective targets. In summary, this research highlights the real-world applications of drugAI in drug discovery pipelines, potentially accelerating the identification of promising drug candidates for a wide range of diseases.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/99d0760520eaa1dc81b5531cd45101e00474bbd1.pdf",
      "citation_key": "ang2024t27",
      "metadata": {
        "title": "De Novo Drug Design Using Transformer-Based Machine Translation and Reinforcement Learning of an Adaptive Monte Carlo Tree Search",
        "authors": [
          "Dony Ang",
          "Cyril Rakovski",
          "H. Atamian"
        ],
        "published_date": "2024",
        "abstract": "The discovery of novel therapeutic compounds through de novo drug design represents a critical challenge in the field of pharmaceutical research. Traditional drug discovery approaches are often resource intensive and time consuming, leading researchers to explore innovative methods that harness the power of deep learning and reinforcement learning techniques. Here, we introduce a novel drug design approach called drugAI that leverages the EncoderDecoder Transformer architecture in tandem with Reinforcement Learning via a Monte Carlo Tree Search (RL-MCTS) to expedite the process of drug discovery while ensuring the production of valid small molecules with drug-like characteristics and strong binding affinities towards their targets. We successfully integrated the EncoderDecoder Transformer architecture, which generates molecular structures (drugs) from scratch with the RL-MCTS, serving as a reinforcement learning framework. The RL-MCTS combines the exploitation and exploration capabilities of a Monte Carlo Tree Search with the machine translation of a transformer-based EncoderDecoder model. This dynamic approach allows the model to iteratively refine its drug candidate generation process, ensuring that the generated molecules adhere to essential physicochemical and biological constraints and effectively bind to their targets. The results from drugAI showcase the effectiveness of the proposed approach across various benchmark datasets, demonstrating a significant improvement in both the validity and drug-likeness of the generated compounds, compared to two existing benchmark methods. Moreover, drugAI ensures that the generated molecules exhibit strong binding affinities to their respective targets. In summary, this research highlights the real-world applications of drugAI in drug discovery pipelines, potentially accelerating the identification of promising drug candidates for a wide range of diseases.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/99d0760520eaa1dc81b5531cd45101e00474bbd1.pdf",
        "venue": "Pharmaceuticals",
        "citationCount": 7,
        "score": 7.0,
        "summary": "The discovery of novel therapeutic compounds through de novo drug design represents a critical challenge in the field of pharmaceutical research. Traditional drug discovery approaches are often resource intensive and time consuming, leading researchers to explore innovative methods that harness the power of deep learning and reinforcement learning techniques. Here, we introduce a novel drug design approach called drugAI that leverages the EncoderDecoder Transformer architecture in tandem with Reinforcement Learning via a Monte Carlo Tree Search (RL-MCTS) to expedite the process of drug discovery while ensuring the production of valid small molecules with drug-like characteristics and strong binding affinities towards their targets. We successfully integrated the EncoderDecoder Transformer architecture, which generates molecular structures (drugs) from scratch with the RL-MCTS, serving as a reinforcement learning framework. The RL-MCTS combines the exploitation and exploration capabilities of a Monte Carlo Tree Search with the machine translation of a transformer-based EncoderDecoder model. This dynamic approach allows the model to iteratively refine its drug candidate generation process, ensuring that the generated molecules adhere to essential physicochemical and biological constraints and effectively bind to their targets. The results from drugAI showcase the effectiveness of the proposed approach across various benchmark datasets, demonstrating a significant improvement in both the validity and drug-likeness of the generated compounds, compared to two existing benchmark methods. Moreover, drugAI ensures that the generated molecules exhibit strong binding affinities to their respective targets. In summary, this research highlights the real-world applications of drugAI in drug discovery pipelines, potentially accelerating the identification of promising drug candidates for a wide range of diseases.",
        "keywords": []
      },
      "file_name": "99d0760520eaa1dc81b5531cd45101e00474bbd1.pdf"
    },
    {
      "success": true,
      "doc_id": "d5c0c8e9523082f67bf8329a4b2e4466",
      "summary": "Reinforcement learning (RL), exhibiting outstanding performance in various fields, requires large amounts of data for high performance. While exploration techniques address this requirement, conventional exploration methods have limitations: complexity of hardware implementation and significant hardware burden. Herein, inmemory RL systems leveraging intrinsic 1/f noise of synaptic ferroelectric fieldeffecttransistors (FeFETs) for efficient exploration are proposed. The electrical characteristics of fabricated FeFETs with lowpower operation capability verify their suitability for neuromorphic systems. The proposed system achieves comparable performance to the conventional exploration method without additional circuits. The intrinsic 1/f noise of the FeFETs facilitates efficient exploration and offers significant advantages: efficiency in hardware implementation and simplicity in adjusting the 1/f noise level for optimal performance. This approach effectively addresses the challenges of conventional exploration methods. The operation mechanism of the exploration method utilizing the 1/f noise is systematically analyzed. The proposed inmemory RL system demonstrates robustness and reliability to the devicetodevice variation and the initial conductance distribution. This work provides further insights into the exploration methods of RL, paving the way for advanced inmemory RL systems.",
      "intriguing_abstract": "Reinforcement learning (RL), exhibiting outstanding performance in various fields, requires large amounts of data for high performance. While exploration techniques address this requirement, conventional exploration methods have limitations: complexity of hardware implementation and significant hardware burden. Herein, inmemory RL systems leveraging intrinsic 1/f noise of synaptic ferroelectric fieldeffecttransistors (FeFETs) for efficient exploration are proposed. The electrical characteristics of fabricated FeFETs with lowpower operation capability verify their suitability for neuromorphic systems. The proposed system achieves comparable performance to the conventional exploration method without additional circuits. The intrinsic 1/f noise of the FeFETs facilitates efficient exploration and offers significant advantages: efficiency in hardware implementation and simplicity in adjusting the 1/f noise level for optimal performance. This approach effectively addresses the challenges of conventional exploration methods. The operation mechanism of the exploration method utilizing the 1/f noise is systematically analyzed. The proposed inmemory RL system demonstrates robustness and reliability to the devicetodevice variation and the initial conductance distribution. This work provides further insights into the exploration methods of RL, paving the way for advanced inmemory RL systems.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/3da8d2e6a0f8d5d62cbaae18235235483144c6a0.pdf",
      "citation_key": "kim2024qde",
      "metadata": {
        "title": "Toward Optimized InMemory Reinforcement Learning: Leveraging 1/f Noise of Synaptic Ferroelectric FieldEffectTransistors for Efficient Exploration",
        "authors": [
          "Jangsaeng Kim",
          "Wonjun Shin",
          "Jiyong Yim",
          "Dongseok Kwon",
          "Dae-Hun Kwon",
          "JongHo Lee"
        ],
        "published_date": "2024",
        "abstract": "Reinforcement learning (RL), exhibiting outstanding performance in various fields, requires large amounts of data for high performance. While exploration techniques address this requirement, conventional exploration methods have limitations: complexity of hardware implementation and significant hardware burden. Herein, inmemory RL systems leveraging intrinsic 1/f noise of synaptic ferroelectric fieldeffecttransistors (FeFETs) for efficient exploration are proposed. The electrical characteristics of fabricated FeFETs with lowpower operation capability verify their suitability for neuromorphic systems. The proposed system achieves comparable performance to the conventional exploration method without additional circuits. The intrinsic 1/f noise of the FeFETs facilitates efficient exploration and offers significant advantages: efficiency in hardware implementation and simplicity in adjusting the 1/f noise level for optimal performance. This approach effectively addresses the challenges of conventional exploration methods. The operation mechanism of the exploration method utilizing the 1/f noise is systematically analyzed. The proposed inmemory RL system demonstrates robustness and reliability to the devicetodevice variation and the initial conductance distribution. This work provides further insights into the exploration methods of RL, paving the way for advanced inmemory RL systems.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/3da8d2e6a0f8d5d62cbaae18235235483144c6a0.pdf",
        "venue": "Advanced Intelligent Systems",
        "citationCount": 6,
        "score": 6.0,
        "summary": "Reinforcement learning (RL), exhibiting outstanding performance in various fields, requires large amounts of data for high performance. While exploration techniques address this requirement, conventional exploration methods have limitations: complexity of hardware implementation and significant hardware burden. Herein, inmemory RL systems leveraging intrinsic 1/f noise of synaptic ferroelectric fieldeffecttransistors (FeFETs) for efficient exploration are proposed. The electrical characteristics of fabricated FeFETs with lowpower operation capability verify their suitability for neuromorphic systems. The proposed system achieves comparable performance to the conventional exploration method without additional circuits. The intrinsic 1/f noise of the FeFETs facilitates efficient exploration and offers significant advantages: efficiency in hardware implementation and simplicity in adjusting the 1/f noise level for optimal performance. This approach effectively addresses the challenges of conventional exploration methods. The operation mechanism of the exploration method utilizing the 1/f noise is systematically analyzed. The proposed inmemory RL system demonstrates robustness and reliability to the devicetodevice variation and the initial conductance distribution. This work provides further insights into the exploration methods of RL, paving the way for advanced inmemory RL systems.",
        "keywords": []
      },
      "file_name": "3da8d2e6a0f8d5d62cbaae18235235483144c6a0.pdf"
    },
    {
      "success": true,
      "doc_id": "c546f6e09ea0b548153819fc0cfdbf09",
      "summary": "Offline reinforcement learning (RL) aims to learn the possible policy from a fixed dataset without real-time interactions with the environment. By avoiding the risky exploration of the robot, this approach is expected to significantly improve the robots learning efficiency and safety. However, due to errors in value estimation from out-of-distribution actions, most offline RL algorithms constrain or regularize the policy to the actions contained within the dataset. The cost of such methods is the introduction of new hyperparameters and additional complexity. In this article, we aim to adapt offline RL to robotic manipulation with minimal changes and to avoid evaluating out-of-distribution actions as much as possible. Therefore, we improve offline RL with in-sample advantage regularization (ISAR). To mitigate the impact of unseen actions, the ISAR learns the state-value function only with the dataset sample to regress the optimal action-value function. Our method calculates the advantage function of action-state pairs based on in-sample value estimation and adds a behavior cloning (BC) regularization term in the policy update. This improves sample efficiency with minimal changes, resulting in a simple and easy-to-implement method. The experiments of the D4RL robot benchmark and multigoal sparse rewards robotic tasks show that the ISAR achieves excellent performance comparable to current state-of-the-art algorithms without the need for complex parameter tuning and too much training time. In addition, we demonstrate the effectiveness of our method on a real-world robot platform.",
      "intriguing_abstract": "Offline reinforcement learning (RL) aims to learn the possible policy from a fixed dataset without real-time interactions with the environment. By avoiding the risky exploration of the robot, this approach is expected to significantly improve the robots learning efficiency and safety. However, due to errors in value estimation from out-of-distribution actions, most offline RL algorithms constrain or regularize the policy to the actions contained within the dataset. The cost of such methods is the introduction of new hyperparameters and additional complexity. In this article, we aim to adapt offline RL to robotic manipulation with minimal changes and to avoid evaluating out-of-distribution actions as much as possible. Therefore, we improve offline RL with in-sample advantage regularization (ISAR). To mitigate the impact of unseen actions, the ISAR learns the state-value function only with the dataset sample to regress the optimal action-value function. Our method calculates the advantage function of action-state pairs based on in-sample value estimation and adds a behavior cloning (BC) regularization term in the policy update. This improves sample efficiency with minimal changes, resulting in a simple and easy-to-implement method. The experiments of the D4RL robot benchmark and multigoal sparse rewards robotic tasks show that the ISAR achieves excellent performance comparable to current state-of-the-art algorithms without the need for complex parameter tuning and too much training time. In addition, we demonstrate the effectiveness of our method on a real-world robot platform.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/086a4d45a9deef5a10ee4febcd4c92c95a6305de.pdf",
      "citation_key": "ma2024jej",
      "metadata": {
        "title": "Improving Offline Reinforcement Learning With in-Sample Advantage Regularization for Robot Manipulation",
        "authors": [
          "Chengzhong Ma",
          "Deyu Yang",
          "Tianyu Wu",
          "Zeyang Liu",
          "Houxue Yang",
          "Xingyu Chen",
          "Xuguang Lan",
          "Nanning Zheng"
        ],
        "published_date": "2024",
        "abstract": "Offline reinforcement learning (RL) aims to learn the possible policy from a fixed dataset without real-time interactions with the environment. By avoiding the risky exploration of the robot, this approach is expected to significantly improve the robots learning efficiency and safety. However, due to errors in value estimation from out-of-distribution actions, most offline RL algorithms constrain or regularize the policy to the actions contained within the dataset. The cost of such methods is the introduction of new hyperparameters and additional complexity. In this article, we aim to adapt offline RL to robotic manipulation with minimal changes and to avoid evaluating out-of-distribution actions as much as possible. Therefore, we improve offline RL with in-sample advantage regularization (ISAR). To mitigate the impact of unseen actions, the ISAR learns the state-value function only with the dataset sample to regress the optimal action-value function. Our method calculates the advantage function of action-state pairs based on in-sample value estimation and adds a behavior cloning (BC) regularization term in the policy update. This improves sample efficiency with minimal changes, resulting in a simple and easy-to-implement method. The experiments of the D4RL robot benchmark and multigoal sparse rewards robotic tasks show that the ISAR achieves excellent performance comparable to current state-of-the-art algorithms without the need for complex parameter tuning and too much training time. In addition, we demonstrate the effectiveness of our method on a real-world robot platform.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/086a4d45a9deef5a10ee4febcd4c92c95a6305de.pdf",
        "venue": "IEEE Transactions on Neural Networks and Learning Systems",
        "citationCount": 6,
        "score": 6.0,
        "summary": "Offline reinforcement learning (RL) aims to learn the possible policy from a fixed dataset without real-time interactions with the environment. By avoiding the risky exploration of the robot, this approach is expected to significantly improve the robots learning efficiency and safety. However, due to errors in value estimation from out-of-distribution actions, most offline RL algorithms constrain or regularize the policy to the actions contained within the dataset. The cost of such methods is the introduction of new hyperparameters and additional complexity. In this article, we aim to adapt offline RL to robotic manipulation with minimal changes and to avoid evaluating out-of-distribution actions as much as possible. Therefore, we improve offline RL with in-sample advantage regularization (ISAR). To mitigate the impact of unseen actions, the ISAR learns the state-value function only with the dataset sample to regress the optimal action-value function. Our method calculates the advantage function of action-state pairs based on in-sample value estimation and adds a behavior cloning (BC) regularization term in the policy update. This improves sample efficiency with minimal changes, resulting in a simple and easy-to-implement method. The experiments of the D4RL robot benchmark and multigoal sparse rewards robotic tasks show that the ISAR achieves excellent performance comparable to current state-of-the-art algorithms without the need for complex parameter tuning and too much training time. In addition, we demonstrate the effectiveness of our method on a real-world robot platform.",
        "keywords": []
      },
      "file_name": "086a4d45a9deef5a10ee4febcd4c92c95a6305de.pdf"
    },
    {
      "success": true,
      "doc_id": "79f0577d321aa1fb71cef7d1e3024da0",
      "summary": "The mismatch problem is commonly happened in photovoltaic systems due to partial shading conditions. Distributed maximum power point tracking architectures can be used to solve such problem. Reinforcement learning (RL) method, which is one of the advanced artificial intelligence methods is proposed to improve the tracking speed. However, the drawbacks, such as the lack of limited adaptability and explorationexploitation tradeoff theory, make the RL method low in efficiency. Therefore, this article combines the Beta method and $\\varepsilon$greedy algorithm with the RL method to address this problem. The simulation and experimental tests have been carried out and the result shows the efficiency of the proposed RL method is up to 96.85%, which verifies the superiority of the proposed scheme.",
      "intriguing_abstract": "The mismatch problem is commonly happened in photovoltaic systems due to partial shading conditions. Distributed maximum power point tracking architectures can be used to solve such problem. Reinforcement learning (RL) method, which is one of the advanced artificial intelligence methods is proposed to improve the tracking speed. However, the drawbacks, such as the lack of limited adaptability and explorationexploitation tradeoff theory, make the RL method low in efficiency. Therefore, this article combines the Beta method and $\\varepsilon$greedy algorithm with the RL method to address this problem. The simulation and experimental tests have been carried out and the result shows the efficiency of the proposed RL method is up to 96.85%, which verifies the superiority of the proposed scheme.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/3dee83a4b0fadde414e00ff350940303eb859be1.pdf",
      "citation_key": "ge20243g0",
      "metadata": {
        "title": "An Improved Distributed Maximum Power Point Tracking Technique in Photovoltaic Systems Based on Reinforcement Learning Algorithm",
        "authors": [
          "Zhihong Ge",
          "Xingshuo Li",
          "Fei Xu",
          "Haimeng Wu",
          "Ruichi Wang",
          "Shuye Ding"
        ],
        "published_date": "2024",
        "abstract": "The mismatch problem is commonly happened in photovoltaic systems due to partial shading conditions. Distributed maximum power point tracking architectures can be used to solve such problem. Reinforcement learning (RL) method, which is one of the advanced artificial intelligence methods is proposed to improve the tracking speed. However, the drawbacks, such as the lack of limited adaptability and explorationexploitation tradeoff theory, make the RL method low in efficiency. Therefore, this article combines the Beta method and $\\varepsilon$greedy algorithm with the RL method to address this problem. The simulation and experimental tests have been carried out and the result shows the efficiency of the proposed RL method is up to 96.85%, which verifies the superiority of the proposed scheme.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/3dee83a4b0fadde414e00ff350940303eb859be1.pdf",
        "venue": "IEEE Journal of Emerging and Selected Topics in Industrial Electronics",
        "citationCount": 6,
        "score": 6.0,
        "summary": "The mismatch problem is commonly happened in photovoltaic systems due to partial shading conditions. Distributed maximum power point tracking architectures can be used to solve such problem. Reinforcement learning (RL) method, which is one of the advanced artificial intelligence methods is proposed to improve the tracking speed. However, the drawbacks, such as the lack of limited adaptability and explorationexploitation tradeoff theory, make the RL method low in efficiency. Therefore, this article combines the Beta method and $\\varepsilon$greedy algorithm with the RL method to address this problem. The simulation and experimental tests have been carried out and the result shows the efficiency of the proposed RL method is up to 96.85%, which verifies the superiority of the proposed scheme.",
        "keywords": []
      },
      "file_name": "3dee83a4b0fadde414e00ff350940303eb859be1.pdf"
    },
    {
      "success": true,
      "doc_id": "ea531e256a478bdba0950e893c5f71c7",
      "summary": "Mixed Reality (MR) could assist users tasks by continuously integrating virtual content with their view of the physical environment. However, where and how to place these content to best support the users has been a challenging problem due to the dynamic nature of MR experiences. In contrast to prior work that investigates optimization-based methods, we are exploring how reinforcement learning (RL) could assist with continuous 3D content placement that is aware of users poses and their surrounding environments. Through an initial exploration and preliminary evaluation, our results demonstrate the potential of RL to position content that maximizes the reward for users on the go. We further identify future directions for research that could harness the power of RL for personalized and optimized UI and content placement in MR.",
      "intriguing_abstract": "Mixed Reality (MR) could assist users tasks by continuously integrating virtual content with their view of the physical environment. However, where and how to place these content to best support the users has been a challenging problem due to the dynamic nature of MR experiences. In contrast to prior work that investigates optimization-based methods, we are exploring how reinforcement learning (RL) could assist with continuous 3D content placement that is aware of users poses and their surrounding environments. Through an initial exploration and preliminary evaluation, our results demonstrate the potential of RL to position content that maximizes the reward for users on the go. We further identify future directions for research that could harness the power of RL for personalized and optimized UI and content placement in MR.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/22c1ec46a81e9db6194b8784f4fe431f71953757.pdf",
      "citation_key": "lu2024ush",
      "metadata": {
        "title": "Adaptive 3D UI Placement in Mixed Reality Using Deep Reinforcement Learning",
        "authors": [
          "Feiyu Lu",
          "Mengyu Chen",
          "Hsiang Hsu",
          "Pranav Deshpande",
          "Cheng Yao Wang",
          "Blair MacIntyre"
        ],
        "published_date": "2024",
        "abstract": "Mixed Reality (MR) could assist users tasks by continuously integrating virtual content with their view of the physical environment. However, where and how to place these content to best support the users has been a challenging problem due to the dynamic nature of MR experiences. In contrast to prior work that investigates optimization-based methods, we are exploring how reinforcement learning (RL) could assist with continuous 3D content placement that is aware of users poses and their surrounding environments. Through an initial exploration and preliminary evaluation, our results demonstrate the potential of RL to position content that maximizes the reward for users on the go. We further identify future directions for research that could harness the power of RL for personalized and optimized UI and content placement in MR.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/22c1ec46a81e9db6194b8784f4fe431f71953757.pdf",
        "venue": "CHI Extended Abstracts",
        "citationCount": 6,
        "score": 6.0,
        "summary": "Mixed Reality (MR) could assist users tasks by continuously integrating virtual content with their view of the physical environment. However, where and how to place these content to best support the users has been a challenging problem due to the dynamic nature of MR experiences. In contrast to prior work that investigates optimization-based methods, we are exploring how reinforcement learning (RL) could assist with continuous 3D content placement that is aware of users poses and their surrounding environments. Through an initial exploration and preliminary evaluation, our results demonstrate the potential of RL to position content that maximizes the reward for users on the go. We further identify future directions for research that could harness the power of RL for personalized and optimized UI and content placement in MR.",
        "keywords": []
      },
      "file_name": "22c1ec46a81e9db6194b8784f4fe431f71953757.pdf"
    },
    {
      "success": true,
      "doc_id": "3325a69db8bf364339473b86d1038339",
      "summary": "Optimizing the quality of results (QoR) of a circuit during the logic synthesis (LS) phase in chip design is critical yet challenging. While most existing methods often mitigate the computational hardness by restricting the action space to a small set of operators and fixing the operator's parameters, they are susceptible to local minima and may not meet the high demand from industrial cases. In this paper, we develop a more comprehensive optimization approach via sample-efficient reinforcement learning (RL). Specifically, we first build a complete logic synthesis-RL environment, where the action space consists of three types of operators: logic optimization, technology mapping, and post-mapping, along with their associated continuouslbinary parameters for optimization as well. Based on this environment, we devise a hybrid proximal policy optimization (PPO) model to handle both discrete operators and parameters and design a distributed architecture to improve sample collection efficiency. Furthermore, we devise a dynamic exploration module to improve the exploration efficiency under the constraint of limited samples. We term our method as Exploration-enhanced RL for Logic Synthesis Sequence Optimization(EasySO). Results on the EPFL benchmark show that our method significantly outperforms current state-of-the-art models based on Bayesian optimization (BO) and the previous RL-based methods. Compared to resyn2, our EasySO achieves an average of 25.4% LUT-6 count optimization without sacrificing level values. Moreover, as of the time for this submission, we rank 26 first places among 40 optimization targets in the EPFL competition.",
      "intriguing_abstract": "Optimizing the quality of results (QoR) of a circuit during the logic synthesis (LS) phase in chip design is critical yet challenging. While most existing methods often mitigate the computational hardness by restricting the action space to a small set of operators and fixing the operator's parameters, they are susceptible to local minima and may not meet the high demand from industrial cases. In this paper, we develop a more comprehensive optimization approach via sample-efficient reinforcement learning (RL). Specifically, we first build a complete logic synthesis-RL environment, where the action space consists of three types of operators: logic optimization, technology mapping, and post-mapping, along with their associated continuouslbinary parameters for optimization as well. Based on this environment, we devise a hybrid proximal policy optimization (PPO) model to handle both discrete operators and parameters and design a distributed architecture to improve sample collection efficiency. Furthermore, we devise a dynamic exploration module to improve the exploration efficiency under the constraint of limited samples. We term our method as Exploration-enhanced RL for Logic Synthesis Sequence Optimization(EasySO). Results on the EPFL benchmark show that our method significantly outperforms current state-of-the-art models based on Bayesian optimization (BO) and the previous RL-based methods. Compared to resyn2, our EasySO achieves an average of 25.4% LUT-6 count optimization without sacrificing level values. Moreover, as of the time for this submission, we rank 26 first places among 40 optimization targets in the EPFL competition.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/9c7209f3d6b9bf362ea4d34730f34cf8511967f4.pdf",
      "citation_key": "yuan2023m1m",
      "metadata": {
        "title": "EasySO: Exploration-enhanced Reinforcement Learning for Logic Synthesis Sequence Optimization and a Comprehensive RL Environment",
        "authors": [
          "Jianyong Yuan",
          "Peiyu Wang",
          "Junjie Ye",
          "Mingxuan Yuan",
          "Jianye Hao",
          "Junchi Yan"
        ],
        "published_date": "2023",
        "abstract": "Optimizing the quality of results (QoR) of a circuit during the logic synthesis (LS) phase in chip design is critical yet challenging. While most existing methods often mitigate the computational hardness by restricting the action space to a small set of operators and fixing the operator's parameters, they are susceptible to local minima and may not meet the high demand from industrial cases. In this paper, we develop a more comprehensive optimization approach via sample-efficient reinforcement learning (RL). Specifically, we first build a complete logic synthesis-RL environment, where the action space consists of three types of operators: logic optimization, technology mapping, and post-mapping, along with their associated continuouslbinary parameters for optimization as well. Based on this environment, we devise a hybrid proximal policy optimization (PPO) model to handle both discrete operators and parameters and design a distributed architecture to improve sample collection efficiency. Furthermore, we devise a dynamic exploration module to improve the exploration efficiency under the constraint of limited samples. We term our method as Exploration-enhanced RL for Logic Synthesis Sequence Optimization(EasySO). Results on the EPFL benchmark show that our method significantly outperforms current state-of-the-art models based on Bayesian optimization (BO) and the previous RL-based methods. Compared to resyn2, our EasySO achieves an average of 25.4% LUT-6 count optimization without sacrificing level values. Moreover, as of the time for this submission, we rank 26 first places among 40 optimization targets in the EPFL competition.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/9c7209f3d6b9bf362ea4d34730f34cf8511967f4.pdf",
        "venue": "2023 IEEE/ACM International Conference on Computer Aided Design (ICCAD)",
        "citationCount": 11,
        "score": 5.5,
        "summary": "Optimizing the quality of results (QoR) of a circuit during the logic synthesis (LS) phase in chip design is critical yet challenging. While most existing methods often mitigate the computational hardness by restricting the action space to a small set of operators and fixing the operator's parameters, they are susceptible to local minima and may not meet the high demand from industrial cases. In this paper, we develop a more comprehensive optimization approach via sample-efficient reinforcement learning (RL). Specifically, we first build a complete logic synthesis-RL environment, where the action space consists of three types of operators: logic optimization, technology mapping, and post-mapping, along with their associated continuouslbinary parameters for optimization as well. Based on this environment, we devise a hybrid proximal policy optimization (PPO) model to handle both discrete operators and parameters and design a distributed architecture to improve sample collection efficiency. Furthermore, we devise a dynamic exploration module to improve the exploration efficiency under the constraint of limited samples. We term our method as Exploration-enhanced RL for Logic Synthesis Sequence Optimization(EasySO). Results on the EPFL benchmark show that our method significantly outperforms current state-of-the-art models based on Bayesian optimization (BO) and the previous RL-based methods. Compared to resyn2, our EasySO achieves an average of 25.4% LUT-6 count optimization without sacrificing level values. Moreover, as of the time for this submission, we rank 26 first places among 40 optimization targets in the EPFL competition.",
        "keywords": []
      },
      "file_name": "9c7209f3d6b9bf362ea4d34730f34cf8511967f4.pdf"
    },
    {
      "success": true,
      "doc_id": "ccffb1e1fb2874808f07395fe74cf038",
      "summary": "While off-policy reinforcement learning (RL) algorithms are sample efficient due to gradient-based updates and data reuse in the replay buffer, they struggle with convergence to local optima due to limited exploration. On the other hand, population-based algorithms offer a natural exploration strategy, but their heuristic black-box operators are inefficient. Recent algorithms have integrated these two methods, connecting them through a shared replay buffer. However, the effect of using diverse data from population optimization iterations on off-policy RL algorithms has not been thoroughly investigated. In this paper, we first analyze the use of off-policy RL algorithms in combination with population-based algorithms, showing that the use of population data could introduce an overlooked error and harm performance. To test this, we propose a uniform and scalable training design and conduct experiments on our tailored framework in robot locomotion tasks from the OpenAI gym. Our results substantiate that using population data in off-policy RL can cause instability during training and even degrade performance. To remedy this issue, we further propose a double replay buffer design that provides more on-policy data and show its effectiveness through experiments. Our results offer practical insights for training these hybrid methods.",
      "intriguing_abstract": "While off-policy reinforcement learning (RL) algorithms are sample efficient due to gradient-based updates and data reuse in the replay buffer, they struggle with convergence to local optima due to limited exploration. On the other hand, population-based algorithms offer a natural exploration strategy, but their heuristic black-box operators are inefficient. Recent algorithms have integrated these two methods, connecting them through a shared replay buffer. However, the effect of using diverse data from population optimization iterations on off-policy RL algorithms has not been thoroughly investigated. In this paper, we first analyze the use of off-policy RL algorithms in combination with population-based algorithms, showing that the use of population data could introduce an overlooked error and harm performance. To test this, we propose a uniform and scalable training design and conduct experiments on our tailored framework in robot locomotion tasks from the OpenAI gym. Our results substantiate that using population data in off-policy RL can cause instability during training and even degrade performance. To remedy this issue, we further propose a double replay buffer design that provides more on-policy data and show its effectiveness through experiments. Our results offer practical insights for training these hybrid methods.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/200726cba07dec06a56ff46aa38836e9730a23a2.pdf",
      "citation_key": "zheng2023u9k",
      "metadata": {
        "title": "Rethinking Population-assisted Off-policy Reinforcement Learning",
        "authors": [
          "Bowen Zheng",
          "Ran Cheng"
        ],
        "published_date": "2023",
        "abstract": "While off-policy reinforcement learning (RL) algorithms are sample efficient due to gradient-based updates and data reuse in the replay buffer, they struggle with convergence to local optima due to limited exploration. On the other hand, population-based algorithms offer a natural exploration strategy, but their heuristic black-box operators are inefficient. Recent algorithms have integrated these two methods, connecting them through a shared replay buffer. However, the effect of using diverse data from population optimization iterations on off-policy RL algorithms has not been thoroughly investigated. In this paper, we first analyze the use of off-policy RL algorithms in combination with population-based algorithms, showing that the use of population data could introduce an overlooked error and harm performance. To test this, we propose a uniform and scalable training design and conduct experiments on our tailored framework in robot locomotion tasks from the OpenAI gym. Our results substantiate that using population data in off-policy RL can cause instability during training and even degrade performance. To remedy this issue, we further propose a double replay buffer design that provides more on-policy data and show its effectiveness through experiments. Our results offer practical insights for training these hybrid methods.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/200726cba07dec06a56ff46aa38836e9730a23a2.pdf",
        "venue": "Annual Conference on Genetic and Evolutionary Computation",
        "citationCount": 11,
        "score": 5.5,
        "summary": "While off-policy reinforcement learning (RL) algorithms are sample efficient due to gradient-based updates and data reuse in the replay buffer, they struggle with convergence to local optima due to limited exploration. On the other hand, population-based algorithms offer a natural exploration strategy, but their heuristic black-box operators are inefficient. Recent algorithms have integrated these two methods, connecting them through a shared replay buffer. However, the effect of using diverse data from population optimization iterations on off-policy RL algorithms has not been thoroughly investigated. In this paper, we first analyze the use of off-policy RL algorithms in combination with population-based algorithms, showing that the use of population data could introduce an overlooked error and harm performance. To test this, we propose a uniform and scalable training design and conduct experiments on our tailored framework in robot locomotion tasks from the OpenAI gym. Our results substantiate that using population data in off-policy RL can cause instability during training and even degrade performance. To remedy this issue, we further propose a double replay buffer design that provides more on-policy data and show its effectiveness through experiments. Our results offer practical insights for training these hybrid methods.",
        "keywords": []
      },
      "file_name": "200726cba07dec06a56ff46aa38836e9730a23a2.pdf"
    },
    {
      "success": true,
      "doc_id": "3e55119cfa037961651545b89675fa68",
      "summary": "Virtual network embedding (VNE) is an essential resource allocation task in network virtualization, aiming to map virtual network requests (VNRs) onto physical infrastructure. Reinforcement learning (RL) has recently emerged as a promising solution to this problem. However, existing RL-based VNE methods are limited by the unidirectional action design and one-size-fits-all training strategy, resulting in restricted searchability and generalizability. In this paper, we propose a flexible and generalizable RL framework for VNE, named FlagVNE. Specifically, we design a bidirectional action-based Markov decision process model that enables the joint selection of virtual and physical nodes, thus improving the exploration flexibility of solution space. To tackle the expansive and dynamic action space, we design a hierarchical decoder to generate adaptive action probability distributions and ensure high training efficiency. Furthermore, to overcome the generalization issue for varying VNR sizes, we propose a meta-RL-based training method with a curriculum scheduling strategy, facilitating specialized policy training for each VNR size. Finally, extensive experimental results show the effectiveness of FlagVNE across multiple key metrics. Our code is available at https://github.com/GeminiLight/flag-vne.",
      "intriguing_abstract": "Virtual network embedding (VNE) is an essential resource allocation task in network virtualization, aiming to map virtual network requests (VNRs) onto physical infrastructure. Reinforcement learning (RL) has recently emerged as a promising solution to this problem. However, existing RL-based VNE methods are limited by the unidirectional action design and one-size-fits-all training strategy, resulting in restricted searchability and generalizability. In this paper, we propose a flexible and generalizable RL framework for VNE, named FlagVNE. Specifically, we design a bidirectional action-based Markov decision process model that enables the joint selection of virtual and physical nodes, thus improving the exploration flexibility of solution space. To tackle the expansive and dynamic action space, we design a hierarchical decoder to generate adaptive action probability distributions and ensure high training efficiency. Furthermore, to overcome the generalization issue for varying VNR sizes, we propose a meta-RL-based training method with a curriculum scheduling strategy, facilitating specialized policy training for each VNR size. Finally, extensive experimental results show the effectiveness of FlagVNE across multiple key metrics. Our code is available at https://github.com/GeminiLight/flag-vne.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/3c483c11f5fd9234576a93aea71a7ec1435b8514.pdf",
      "citation_key": "wang20241f3",
      "metadata": {
        "title": "FlagVNE: A Flexible and Generalizable Reinforcement Learning Framework for Network Resource Allocation",
        "authors": [
          "Tianfu Wang",
          "Qilin Fan",
          "Chao Wang",
          "Long Yang",
          "Leilei Ding",
          "Nicholas Jing Yuan",
          "Hui Xiong"
        ],
        "published_date": "2024",
        "abstract": "Virtual network embedding (VNE) is an essential resource allocation task in network virtualization, aiming to map virtual network requests (VNRs) onto physical infrastructure. Reinforcement learning (RL) has recently emerged as a promising solution to this problem. However, existing RL-based VNE methods are limited by the unidirectional action design and one-size-fits-all training strategy, resulting in restricted searchability and generalizability. In this paper, we propose a flexible and generalizable RL framework for VNE, named FlagVNE. Specifically, we design a bidirectional action-based Markov decision process model that enables the joint selection of virtual and physical nodes, thus improving the exploration flexibility of solution space. To tackle the expansive and dynamic action space, we design a hierarchical decoder to generate adaptive action probability distributions and ensure high training efficiency. Furthermore, to overcome the generalization issue for varying VNR sizes, we propose a meta-RL-based training method with a curriculum scheduling strategy, facilitating specialized policy training for each VNR size. Finally, extensive experimental results show the effectiveness of FlagVNE across multiple key metrics. Our code is available at https://github.com/GeminiLight/flag-vne.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/3c483c11f5fd9234576a93aea71a7ec1435b8514.pdf",
        "venue": "International Joint Conference on Artificial Intelligence",
        "citationCount": 5,
        "score": 5.0,
        "summary": "Virtual network embedding (VNE) is an essential resource allocation task in network virtualization, aiming to map virtual network requests (VNRs) onto physical infrastructure. Reinforcement learning (RL) has recently emerged as a promising solution to this problem. However, existing RL-based VNE methods are limited by the unidirectional action design and one-size-fits-all training strategy, resulting in restricted searchability and generalizability. In this paper, we propose a flexible and generalizable RL framework for VNE, named FlagVNE. Specifically, we design a bidirectional action-based Markov decision process model that enables the joint selection of virtual and physical nodes, thus improving the exploration flexibility of solution space. To tackle the expansive and dynamic action space, we design a hierarchical decoder to generate adaptive action probability distributions and ensure high training efficiency. Furthermore, to overcome the generalization issue for varying VNR sizes, we propose a meta-RL-based training method with a curriculum scheduling strategy, facilitating specialized policy training for each VNR size. Finally, extensive experimental results show the effectiveness of FlagVNE across multiple key metrics. Our code is available at https://github.com/GeminiLight/flag-vne.",
        "keywords": []
      },
      "file_name": "3c483c11f5fd9234576a93aea71a7ec1435b8514.pdf"
    },
    {
      "success": true,
      "doc_id": "e155b327d5988beaa2f5452cc77e9d30",
      "summary": "We introduce Random Latent Exploration (RLE), a simple yet effective exploration strategy in reinforcement learning (RL). On average, RLE outperforms noise-based methods, which perturb the agent's actions, and bonus-based exploration, which rewards the agent for attempting novel behaviors. The core idea of RLE is to encourage the agent to explore different parts of the environment by pursuing randomly sampled goals in a latent space. RLE is as simple as noise-based methods, as it avoids complex bonus calculations but retains the deep exploration benefits of bonus-based methods. Our experiments show that RLE improves performance on average in both discrete (e.g., Atari) and continuous control tasks (e.g., Isaac Gym), enhancing exploration while remaining a simple and general plug-in for existing RL algorithms. Project website and code: https://srinathm1359.github.io/random-latent-exploration",
      "intriguing_abstract": "We introduce Random Latent Exploration (RLE), a simple yet effective exploration strategy in reinforcement learning (RL). On average, RLE outperforms noise-based methods, which perturb the agent's actions, and bonus-based exploration, which rewards the agent for attempting novel behaviors. The core idea of RLE is to encourage the agent to explore different parts of the environment by pursuing randomly sampled goals in a latent space. RLE is as simple as noise-based methods, as it avoids complex bonus calculations but retains the deep exploration benefits of bonus-based methods. Our experiments show that RLE improves performance on average in both discrete (e.g., Atari) and continuous control tasks (e.g., Isaac Gym), enhancing exploration while remaining a simple and general plug-in for existing RL algorithms. Project website and code: https://srinathm1359.github.io/random-latent-exploration",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/d06737f9395e592f35ef251e09bea1c18037b096.pdf",
      "citation_key": "mahankali20248dx",
      "metadata": {
        "title": "Random Latent Exploration for Deep Reinforcement Learning",
        "authors": [
          "Srinath Mahankali",
          "Zhang-Wei Hong",
          "Ayush Sekhari",
          "Alexander Rakhlin",
          "Pulkit Agrawal"
        ],
        "published_date": "2024",
        "abstract": "We introduce Random Latent Exploration (RLE), a simple yet effective exploration strategy in reinforcement learning (RL). On average, RLE outperforms noise-based methods, which perturb the agent's actions, and bonus-based exploration, which rewards the agent for attempting novel behaviors. The core idea of RLE is to encourage the agent to explore different parts of the environment by pursuing randomly sampled goals in a latent space. RLE is as simple as noise-based methods, as it avoids complex bonus calculations but retains the deep exploration benefits of bonus-based methods. Our experiments show that RLE improves performance on average in both discrete (e.g., Atari) and continuous control tasks (e.g., Isaac Gym), enhancing exploration while remaining a simple and general plug-in for existing RL algorithms. Project website and code: https://srinathm1359.github.io/random-latent-exploration",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/d06737f9395e592f35ef251e09bea1c18037b096.pdf",
        "venue": "International Conference on Machine Learning",
        "citationCount": 5,
        "score": 5.0,
        "summary": "We introduce Random Latent Exploration (RLE), a simple yet effective exploration strategy in reinforcement learning (RL). On average, RLE outperforms noise-based methods, which perturb the agent's actions, and bonus-based exploration, which rewards the agent for attempting novel behaviors. The core idea of RLE is to encourage the agent to explore different parts of the environment by pursuing randomly sampled goals in a latent space. RLE is as simple as noise-based methods, as it avoids complex bonus calculations but retains the deep exploration benefits of bonus-based methods. Our experiments show that RLE improves performance on average in both discrete (e.g., Atari) and continuous control tasks (e.g., Isaac Gym), enhancing exploration while remaining a simple and general plug-in for existing RL algorithms. Project website and code: https://srinathm1359.github.io/random-latent-exploration",
        "keywords": []
      },
      "file_name": "d06737f9395e592f35ef251e09bea1c18037b096.pdf"
    },
    {
      "success": true,
      "doc_id": "6cb1064365d63acee9f2fe34d31982e6",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/3ef01975a45bcf78120d76c1bc73e8ec12e213bf.pdf",
      "citation_key": "yoon2024lff",
      "metadata": {
        "title": "Breadth-First Exploration on Adaptive Grid for Reinforcement Learning",
        "authors": [
          "Youngsik Yoon",
          "Gangbok Lee",
          "Sungsoo Ahn",
          "Jungseul Ok"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/3ef01975a45bcf78120d76c1bc73e8ec12e213bf.pdf",
        "venue": "International Conference on Machine Learning",
        "citationCount": 5,
        "score": 5.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "3ef01975a45bcf78120d76c1bc73e8ec12e213bf.pdf"
    },
    {
      "success": true,
      "doc_id": "3fca6b0eb43086d82576eff1e86c6c1d",
      "summary": "Thompson sampling (TS) is one of the most popular exploration techniques in reinforcement learning (RL). However, most TS algorithms with theoretical guarantees are difficult to implement and not generalizable to Deep RL. While the emerging approximate sampling-based exploration schemes are promising, most existing algorithms are specific to linear Markov Decision Processes (MDP) with suboptimal regret bounds, or only use the most basic samplers such as Langevin Monte Carlo. In this work, we propose an algorithmic framework that incorporates different approximate sampling methods with the recently proposed Feel-Good Thompson Sampling (FGTS) approach (Zhang, 2022; Dann et al., 2021), which was previously known to be computationally intractable in general. When applied to linear MDPs, our regret analysis yields the best known dependency of regret on dimensionality, surpassing existing randomized algorithms. Additionally, we provide explicit sampling complexity for each employed sampler. Empirically, we show that in tasks where deep exploration is necessary, our proposed algorithms that combine FGTS and approximate sampling perform significantly better compared to other strong baselines. On several challenging games from the Atari 57 suite, our algorithms achieve performance that is either better than or on par with other strong baselines from the deep RL literature.",
      "intriguing_abstract": "Thompson sampling (TS) is one of the most popular exploration techniques in reinforcement learning (RL). However, most TS algorithms with theoretical guarantees are difficult to implement and not generalizable to Deep RL. While the emerging approximate sampling-based exploration schemes are promising, most existing algorithms are specific to linear Markov Decision Processes (MDP) with suboptimal regret bounds, or only use the most basic samplers such as Langevin Monte Carlo. In this work, we propose an algorithmic framework that incorporates different approximate sampling methods with the recently proposed Feel-Good Thompson Sampling (FGTS) approach (Zhang, 2022; Dann et al., 2021), which was previously known to be computationally intractable in general. When applied to linear MDPs, our regret analysis yields the best known dependency of regret on dimensionality, surpassing existing randomized algorithms. Additionally, we provide explicit sampling complexity for each employed sampler. Empirically, we show that in tasks where deep exploration is necessary, our proposed algorithms that combine FGTS and approximate sampling perform significantly better compared to other strong baselines. On several challenging games from the Atari 57 suite, our algorithms achieve performance that is either better than or on par with other strong baselines from the deep RL literature.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/39d2839aa4c3d8c0e64553891fe98ba261703154.pdf",
      "citation_key": "ishfaq20245to",
      "metadata": {
        "title": "More Efficient Randomized Exploration for Reinforcement Learning via Approximate Sampling",
        "authors": [
          "Haque Ishfaq",
          "Yixin Tan",
          "Yu Yang",
          "Qingfeng Lan",
          "Jianfeng Lu",
          "A. R. Mahmood",
          "D. Precup",
          "Pan Xu"
        ],
        "published_date": "2024",
        "abstract": "Thompson sampling (TS) is one of the most popular exploration techniques in reinforcement learning (RL). However, most TS algorithms with theoretical guarantees are difficult to implement and not generalizable to Deep RL. While the emerging approximate sampling-based exploration schemes are promising, most existing algorithms are specific to linear Markov Decision Processes (MDP) with suboptimal regret bounds, or only use the most basic samplers such as Langevin Monte Carlo. In this work, we propose an algorithmic framework that incorporates different approximate sampling methods with the recently proposed Feel-Good Thompson Sampling (FGTS) approach (Zhang, 2022; Dann et al., 2021), which was previously known to be computationally intractable in general. When applied to linear MDPs, our regret analysis yields the best known dependency of regret on dimensionality, surpassing existing randomized algorithms. Additionally, we provide explicit sampling complexity for each employed sampler. Empirically, we show that in tasks where deep exploration is necessary, our proposed algorithms that combine FGTS and approximate sampling perform significantly better compared to other strong baselines. On several challenging games from the Atari 57 suite, our algorithms achieve performance that is either better than or on par with other strong baselines from the deep RL literature.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/39d2839aa4c3d8c0e64553891fe98ba261703154.pdf",
        "venue": "RLJ",
        "citationCount": 5,
        "score": 5.0,
        "summary": "Thompson sampling (TS) is one of the most popular exploration techniques in reinforcement learning (RL). However, most TS algorithms with theoretical guarantees are difficult to implement and not generalizable to Deep RL. While the emerging approximate sampling-based exploration schemes are promising, most existing algorithms are specific to linear Markov Decision Processes (MDP) with suboptimal regret bounds, or only use the most basic samplers such as Langevin Monte Carlo. In this work, we propose an algorithmic framework that incorporates different approximate sampling methods with the recently proposed Feel-Good Thompson Sampling (FGTS) approach (Zhang, 2022; Dann et al., 2021), which was previously known to be computationally intractable in general. When applied to linear MDPs, our regret analysis yields the best known dependency of regret on dimensionality, surpassing existing randomized algorithms. Additionally, we provide explicit sampling complexity for each employed sampler. Empirically, we show that in tasks where deep exploration is necessary, our proposed algorithms that combine FGTS and approximate sampling perform significantly better compared to other strong baselines. On several challenging games from the Atari 57 suite, our algorithms achieve performance that is either better than or on par with other strong baselines from the deep RL literature.",
        "keywords": []
      },
      "file_name": "39d2839aa4c3d8c0e64553891fe98ba261703154.pdf"
    },
    {
      "success": true,
      "doc_id": "916c1e22715b2fa9e2f60f535c832de3",
      "summary": "In classic Reinforcement Learning (RL), the agent maximizes an additive objective of the visited states, e.g., a value function. Unfortunately, objectives of this type cannot model many real-world applications such as experiment design, exploration, imitation learning, and risk-averse RL to name a few. This is due to the fact that additive objectives disregard interactions between states that are crucial for certain tasks. To tackle this problem, we introduce Global RL (GRL), where rewards are globally defined over trajectories instead of locally over states. Global rewards can capture negative interactions among states, e.g., in exploration, via submodularity, positive interactions, e.g., synergetic effects, via supermodularity, while mixed interactions via combinations of them. By exploiting ideas from submodular optimization, we propose a novel algorithmic scheme that converts any GRL problem to a sequence of classic RL problems and solves it efficiently with curvature-dependent approximation guarantees. We also provide hardness of approximation results and empirically demonstrate the effectiveness of our method on several GRL instances.",
      "intriguing_abstract": "In classic Reinforcement Learning (RL), the agent maximizes an additive objective of the visited states, e.g., a value function. Unfortunately, objectives of this type cannot model many real-world applications such as experiment design, exploration, imitation learning, and risk-averse RL to name a few. This is due to the fact that additive objectives disregard interactions between states that are crucial for certain tasks. To tackle this problem, we introduce Global RL (GRL), where rewards are globally defined over trajectories instead of locally over states. Global rewards can capture negative interactions among states, e.g., in exploration, via submodularity, positive interactions, e.g., synergetic effects, via supermodularity, while mixed interactions via combinations of them. By exploiting ideas from submodular optimization, we propose a novel algorithmic scheme that converts any GRL problem to a sequence of classic RL problems and solves it efficiently with curvature-dependent approximation guarantees. We also provide hardness of approximation results and empirically demonstrate the effectiveness of our method on several GRL instances.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/cb7ea0176eec1f809f3bc3a34aeb279d44dda612.pdf",
      "citation_key": "santi2024hct",
      "metadata": {
        "title": "Global Reinforcement Learning: Beyond Linear and Convex Rewards via Submodular Semi-gradient Methods",
        "authors": [
          "Ric De Santi",
          "Manish Prajapat",
          "Andreas Krause"
        ],
        "published_date": "2024",
        "abstract": "In classic Reinforcement Learning (RL), the agent maximizes an additive objective of the visited states, e.g., a value function. Unfortunately, objectives of this type cannot model many real-world applications such as experiment design, exploration, imitation learning, and risk-averse RL to name a few. This is due to the fact that additive objectives disregard interactions between states that are crucial for certain tasks. To tackle this problem, we introduce Global RL (GRL), where rewards are globally defined over trajectories instead of locally over states. Global rewards can capture negative interactions among states, e.g., in exploration, via submodularity, positive interactions, e.g., synergetic effects, via supermodularity, while mixed interactions via combinations of them. By exploiting ideas from submodular optimization, we propose a novel algorithmic scheme that converts any GRL problem to a sequence of classic RL problems and solves it efficiently with curvature-dependent approximation guarantees. We also provide hardness of approximation results and empirically demonstrate the effectiveness of our method on several GRL instances.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/cb7ea0176eec1f809f3bc3a34aeb279d44dda612.pdf",
        "venue": "International Conference on Machine Learning",
        "citationCount": 5,
        "score": 5.0,
        "summary": "In classic Reinforcement Learning (RL), the agent maximizes an additive objective of the visited states, e.g., a value function. Unfortunately, objectives of this type cannot model many real-world applications such as experiment design, exploration, imitation learning, and risk-averse RL to name a few. This is due to the fact that additive objectives disregard interactions between states that are crucial for certain tasks. To tackle this problem, we introduce Global RL (GRL), where rewards are globally defined over trajectories instead of locally over states. Global rewards can capture negative interactions among states, e.g., in exploration, via submodularity, positive interactions, e.g., synergetic effects, via supermodularity, while mixed interactions via combinations of them. By exploiting ideas from submodular optimization, we propose a novel algorithmic scheme that converts any GRL problem to a sequence of classic RL problems and solves it efficiently with curvature-dependent approximation guarantees. We also provide hardness of approximation results and empirically demonstrate the effectiveness of our method on several GRL instances.",
        "keywords": []
      },
      "file_name": "cb7ea0176eec1f809f3bc3a34aeb279d44dda612.pdf"
    },
    {
      "success": true,
      "doc_id": "5d16b40c6486db09cfbc52de74fb6353",
      "summary": "Fuzzing is a popular and effective software testing technique that automatically generates or modifies inputs to test the stability and vulnerabilities of a software system, which has been widely applied and improved by security researchers and experts. The goal of fuzzing is to uncover potential weaknesses in software by providing unexpected and invalid inputs to the target program to monitor its behavior and identify errors or unintended outcomes. Recently, researchers have also integrated promising machine learning algorithms, such as reinforcement learning, to enhance the fuzzing process. Reinforcement learning (RL) has been proven to be able to improve the effectiveness of fuzzing by selecting and prioritizing transformation actions with higher coverage, which reduces the required effort to uncover vulnerabilities. However, RL-based fuzzing models also encounter certain limitations, including an imbalance between exploitation and exploration. In this study, we propose a coverage-guided RL-based fuzzing model that enhances grey-box fuzzing, in which we leverage deep Q-learning to predict and select input variations to maximize code coverage and use code coverage as a reward signal. This model is complemented by simple input selection and scheduling algorithms that promote a more balanced approach to exploiting and exploring software. Furthermore, we introduce a multi-level input mutation model combined with RL to create a sequence of actions for comprehensive input variation. The proposed model is compared to other fuzzing tools in testing various real-world programs, where the results indicate a notable enhancement in terms of code coverage, discovered paths, and execution speed of our solution.",
      "intriguing_abstract": "Fuzzing is a popular and effective software testing technique that automatically generates or modifies inputs to test the stability and vulnerabilities of a software system, which has been widely applied and improved by security researchers and experts. The goal of fuzzing is to uncover potential weaknesses in software by providing unexpected and invalid inputs to the target program to monitor its behavior and identify errors or unintended outcomes. Recently, researchers have also integrated promising machine learning algorithms, such as reinforcement learning, to enhance the fuzzing process. Reinforcement learning (RL) has been proven to be able to improve the effectiveness of fuzzing by selecting and prioritizing transformation actions with higher coverage, which reduces the required effort to uncover vulnerabilities. However, RL-based fuzzing models also encounter certain limitations, including an imbalance between exploitation and exploration. In this study, we propose a coverage-guided RL-based fuzzing model that enhances grey-box fuzzing, in which we leverage deep Q-learning to predict and select input variations to maximize code coverage and use code coverage as a reward signal. This model is complemented by simple input selection and scheduling algorithms that promote a more balanced approach to exploiting and exploring software. Furthermore, we introduce a multi-level input mutation model combined with RL to create a sequence of actions for comprehensive input variation. The proposed model is compared to other fuzzing tools in testing various real-world programs, where the results indicate a notable enhancement in terms of code coverage, discovered paths, and execution speed of our solution.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/b31c76815615c16cc8505dbb38d2921f921c029d.pdf",
      "citation_key": "pham2024j80",
      "metadata": {
        "title": "A Coverage-Guided Fuzzing Method for Automatic Software Vulnerability Detection Using Reinforcement Learning-Enabled Multi-Level Input Mutation",
        "authors": [
          "Van-Hau Pham",
          "Do Thi Thu Hien",
          "Nguyen Phuc Chuong",
          "Pham Thanh Thai",
          "Phan The Duy"
        ],
        "published_date": "2024",
        "abstract": "Fuzzing is a popular and effective software testing technique that automatically generates or modifies inputs to test the stability and vulnerabilities of a software system, which has been widely applied and improved by security researchers and experts. The goal of fuzzing is to uncover potential weaknesses in software by providing unexpected and invalid inputs to the target program to monitor its behavior and identify errors or unintended outcomes. Recently, researchers have also integrated promising machine learning algorithms, such as reinforcement learning, to enhance the fuzzing process. Reinforcement learning (RL) has been proven to be able to improve the effectiveness of fuzzing by selecting and prioritizing transformation actions with higher coverage, which reduces the required effort to uncover vulnerabilities. However, RL-based fuzzing models also encounter certain limitations, including an imbalance between exploitation and exploration. In this study, we propose a coverage-guided RL-based fuzzing model that enhances grey-box fuzzing, in which we leverage deep Q-learning to predict and select input variations to maximize code coverage and use code coverage as a reward signal. This model is complemented by simple input selection and scheduling algorithms that promote a more balanced approach to exploiting and exploring software. Furthermore, we introduce a multi-level input mutation model combined with RL to create a sequence of actions for comprehensive input variation. The proposed model is compared to other fuzzing tools in testing various real-world programs, where the results indicate a notable enhancement in terms of code coverage, discovered paths, and execution speed of our solution.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/b31c76815615c16cc8505dbb38d2921f921c029d.pdf",
        "venue": "IEEE Access",
        "citationCount": 5,
        "score": 5.0,
        "summary": "Fuzzing is a popular and effective software testing technique that automatically generates or modifies inputs to test the stability and vulnerabilities of a software system, which has been widely applied and improved by security researchers and experts. The goal of fuzzing is to uncover potential weaknesses in software by providing unexpected and invalid inputs to the target program to monitor its behavior and identify errors or unintended outcomes. Recently, researchers have also integrated promising machine learning algorithms, such as reinforcement learning, to enhance the fuzzing process. Reinforcement learning (RL) has been proven to be able to improve the effectiveness of fuzzing by selecting and prioritizing transformation actions with higher coverage, which reduces the required effort to uncover vulnerabilities. However, RL-based fuzzing models also encounter certain limitations, including an imbalance between exploitation and exploration. In this study, we propose a coverage-guided RL-based fuzzing model that enhances grey-box fuzzing, in which we leverage deep Q-learning to predict and select input variations to maximize code coverage and use code coverage as a reward signal. This model is complemented by simple input selection and scheduling algorithms that promote a more balanced approach to exploiting and exploring software. Furthermore, we introduce a multi-level input mutation model combined with RL to create a sequence of actions for comprehensive input variation. The proposed model is compared to other fuzzing tools in testing various real-world programs, where the results indicate a notable enhancement in terms of code coverage, discovered paths, and execution speed of our solution.",
        "keywords": []
      },
      "file_name": "b31c76815615c16cc8505dbb38d2921f921c029d.pdf"
    },
    {
      "success": true,
      "doc_id": "fdaae7c1f3b6a37475a65ff2e51acaa7",
      "summary": "Here's a focused summary of the paper \\cite{ding2023whs} for a literature review:\n\n### Technical Paper Analysis: Incremental Reinforcement Learning with Dual-Adaptive -Greedy Exploration \\cite{ding2023whs}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses \"Incremental Reinforcement Learning (Incremental RL),\" a new challenge where the search space of the Markov Decision Process (MDP) continually expands, meaning state and action spaces are progressively enlarged.\n    *   **Importance and Challenge**:\n        *   **Real-world Relevance**: Most RL frameworks assume fixed environments, which is unrealistic as real-world applications (e.g., autonomous vehicles) frequently update, introducing new states and actions.\n        *   **Computational Overhead**: Retraining RL agents from scratch for every environment update is computationally prohibitive and time-consuming.\n        *   **Exploration Inefficiency**: Existing exploration methods (like `-greedy`) are inefficient in exploring unseen transitions in an exponentially growing search space, especially when an agent has a strong inductive bias from prior experience.\n        *   **Preserving Knowledge**: The challenge lies in efficiently adapting to new state/action spaces while preserving previously learned behavior and avoiding redundant computation.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: This work builds upon conventional RL, particularly Deep Q-Learning, and addresses challenges in sample efficiency, exploration strategies, and value estimation.\n    *   **Limitations of Previous Solutions**:\n        *   **Fixed Environments**: Most prior RL works assume static environments with fixed-yet-known state and action sets, making them difficult to generalize to evolving scenarios.\n        *   **Lifelong RL Distinction**: While related to lifelong reinforcement learning (which trains agents for sequences of similar tasks), Incremental RL specifically deals with *expanding* state and action spaces, a challenge not explicitly addressed by lifelong learning's fixed-set assumption.\n        *   **Inefficient Retraining**: The straightforward approach of retraining agents when environments vary is undesirable due to high computational costs.\n        *   **Sub-optimal Exploration**: Standard `-greedy` exploration is inefficient for increasing search spaces, leading to over-exploration of well-known states and under-exploration of new ones.\n        *   **Inductive Bias**: Previous training can create a strong inductive bias, making it difficult for agents to explore newly added actions or states effectively.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes Dual-Adaptive ``-greedy Exploration (DAE) to address Incremental RL. DAE combines two main strategies: a Meta Policy () and an Explorer ().\n    *   **Novelty/Difference**:\n        *   **Formalizing Incremental RL**: This is the first work to formally model and formulate the challenge of Incremental RL.\n        *   **Dual-Adaptive Exploration**: DAE introduces a novel mechanism for adaptive exploration:\n            *   **Meta Policy ()**: This component adaptively determines a state-dependent `_t` (exploration probability) by assessing the exploration convergence of the current state. It's trained as a binary classifier using a pseudo ground truth based on the TD-Error rate, allowing more exploration for states with high uncertainty.\n            *   **Explorer ()**: This component estimates the relative frequency of actions given a state, guiding the agent to explore the \"least-tried\" actions rather than uniformly random ones. It's implemented as a deep neural network trained by gradient ascent to increase the relative frequency of taken actions.\n        *   **Synergistic Design**: DAE uses the Meta Policy to decide *when* to explore (state-dependent ``) and the Explorer to decide *what* to explore (least-tried actions), making exploration highly targeted and efficient in expanding environments.\n        *   **Handling Expansion**: DAE specifically addresses new states (higher `` from Meta Policy) and new actions (Explorer encourages sampling of initially low Q-value actions) to overcome prior biases.\n\n4.  **Key Technical Contributions**\n    *   **Novel Problem Formulation**: Formal modeling and definition of Incremental Reinforcement Learning (Incremental RL), where MDP state and action spaces continually expand.\n    *   **Novel Exploration Algorithm (DAE)**: Introduction of Dual-Adaptive ``-greedy Exploration, which features:\n        *   A **Meta Policy ()** for adaptive exploitation-exploration trade-off, dynamically adjusting `` based on state-specific value estimation convergence (TD-Error rate).\n        *   An **Explorer ()** for adaptive action exploration, guiding the agent to prioritize least-tried actions by estimating their relative frequencies.\n    *   **System Design for Incremental Learning**: Strategies for reusing trained policies, initializing new input/output neurons in deep Q-networks, and handling Q-values of newly added actions to facilitate incremental adaptation.\n    *   **New Testbed**: Release of a new testbed based on a synthetic \"Expanding World\" environment and an adapted Atari benchmark to evaluate Incremental RL algorithms.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: Experiments were primarily conducted on a custom-designed synthetic environment called \"Expanding World\" (with continually increasing state and action dimensions) and an adapted Atari benchmark (though details for Atari are not in the provided text).\n    *   **Key Performance Metrics**: The primary metric was \"training overhead,\" measured as the number of training steps required to attain near-optimal policies as the state and action spaces increase.\n    *   **Comparison Results**:\n        *   DAE was compared against eight baselines, including various `-greedy` variants (e.g., fixed ``, decaying ``, `-greedy (Retrain)`).\n        *   Results on \"Expanding World\" demonstrated that DAE significantly reduced training overhead, efficiently adapting to new environments.\n        *   DAE achieved an average performance improvement of over 80% compared to the examined baselines, showcasing its ability to efficiently learn unseen transitions and adapt to expanding search spaces.\n\n6.  **Limitations & Scope**\n    *   **Technical Assumptions**: The study assumes that existing transitions (state-action pairs) remain unchanged when the environment expands, focusing the challenge on learning new transitions.\n    *   **Initialization Strategy**: The Q-values of newly-added actions are initialized with small values, and new input/output neurons are added to the deep Q-network, which might influence initial exploration behavior.\n    *   **Scope of Applicability**: The proposed DAE framework is primarily designed for value-based reinforcement learning (Deep Q-Learning) and addresses exploration in environments with expanding state and action spaces.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: \\cite{ding2023whs} significantly advances the technical state-of-the-art by formally defining and providing an effective solution for Incremental RL, a more realistic and challenging setting than traditionally studied fixed-environment RL.\n    *   **Improved Generalization and Efficiency**: DAE's dual-adaptive exploration mechanism allows RL agents to adapt to evolving environments without costly retraining, leading to improved sample efficiency and generalization capabilities.\n    *   **Potential Impact on Future Research**: The introduction of Incremental RL as a formal problem and the release of a dedicated testbed will likely stimulate further research into adaptive RL agents, lifelong learning in dynamic environments, and more sophisticated exploration strategies for continually expanding search spaces. This work paves the way for more practical and robust RL applications in real-world scenarios.",
      "intriguing_abstract": "Traditional Reinforcement Learning (RL) often assumes static environments, a stark contrast to the dynamic, ever-evolving real world where new states and actions frequently emerge. This fundamental mismatch leads to prohibitive computational overhead from retraining and inefficient exploration in exponentially expanding search spaces. We introduce **Incremental Reinforcement Learning (Incremental RL)**, a novel problem formulation addressing environments where the Markov Decision Process (MDP) continually expands.\n\nTo tackle this critical challenge, we propose **Dual-Adaptive ``-greedy Exploration (DAE)**, a pioneering algorithm designed for Deep Q-Learning. DAE features a synergistic dual mechanism: a **Meta Policy ()** that adaptively determines state-dependent exploration probabilities based on value estimation convergence (TD-Error rate), and an **Explorer ()** that guides the agent to prioritize least-tried actions by estimating their relative frequencies. This unique combination allows DAE to efficiently adapt to new state and action spaces, preserving learned knowledge while overcoming prior inductive biases. Our experiments on a novel \"Expanding World\" testbed demonstrate DAE's remarkable ability to reduce training overhead by over 80% compared to existing baselines, significantly improving sample efficiency. This work not only formally defines Incremental RL but also provides a robust solution, paving the way for truly adaptive and practical RL agents in dynamic real-world applications.",
      "keywords": [
        "Incremental Reinforcement Learning (Incremental RL)",
        "Expanding State and Action Spaces",
        "Dual-Adaptive -greedy Exploration (DAE)",
        "Meta Policy ()",
        "Explorer ()",
        "Adaptive Exploration",
        "Novel Problem Formulation",
        "Computational Overhead Reduction",
        "Training Overhead",
        "Deep Q-Learning",
        "TD-Error Rate",
        "Expanding World Testbed",
        "Preserving Learned Behavior",
        "Real-world Applications"
      ],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/c1844cda42b3732a5576d05bb6e007eb1db00919.pdf",
      "citation_key": "ding2023whs",
      "metadata": {
        "title": "Incremental Reinforcement Learning with Dual-Adaptive -Greedy Exploration",
        "authors": [
          "Wei Ding",
          "Siyang Jiang",
          "Hsi-Wen Chen",
          "Ming-Syan Chen"
        ],
        "published_date": "2023",
        "abstract": "Reinforcement learning (RL) has achieved impressive performance in various domains. However, most RL frameworks oversimplify the problem by assuming a fixed-yet-known environment and often have difficulty being generalized to real-world scenarios. In this paper, we address a new challenge with a more realistic setting, Incremental Reinforcement Learning, where the search space of the Markov Decision Process continually expands. While previous methods usually suffer from the lack of efficiency in exploring the unseen transitions, especially with increasing search space, we present a new exploration framework named Dual-Adaptive -greedy Exploration (DAE) to address the challenge of Incremental RL. Specifically, DAE employs a Meta Policy and an Explorer to avoid redundant computation on those sufficiently\nlearned samples. Furthermore, we release a testbed based on a synthetic environment and the Atari benchmark to validate the effectiveness of any exploration algorithms under Incremental RL. Experimental results demonstrate that the proposed framework can efficiently learn the unseen transitions in new environments, leading to notable performance improvement, i.e., an average of more than 80%, over eight baselines examined.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/c1844cda42b3732a5576d05bb6e007eb1db00919.pdf",
        "venue": "AAAI Conference on Artificial Intelligence",
        "citationCount": 10,
        "score": 5.0,
        "summary": "Here's a focused summary of the paper \\cite{ding2023whs} for a literature review:\n\n### Technical Paper Analysis: Incremental Reinforcement Learning with Dual-Adaptive -Greedy Exploration \\cite{ding2023whs}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses \"Incremental Reinforcement Learning (Incremental RL),\" a new challenge where the search space of the Markov Decision Process (MDP) continually expands, meaning state and action spaces are progressively enlarged.\n    *   **Importance and Challenge**:\n        *   **Real-world Relevance**: Most RL frameworks assume fixed environments, which is unrealistic as real-world applications (e.g., autonomous vehicles) frequently update, introducing new states and actions.\n        *   **Computational Overhead**: Retraining RL agents from scratch for every environment update is computationally prohibitive and time-consuming.\n        *   **Exploration Inefficiency**: Existing exploration methods (like `-greedy`) are inefficient in exploring unseen transitions in an exponentially growing search space, especially when an agent has a strong inductive bias from prior experience.\n        *   **Preserving Knowledge**: The challenge lies in efficiently adapting to new state/action spaces while preserving previously learned behavior and avoiding redundant computation.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: This work builds upon conventional RL, particularly Deep Q-Learning, and addresses challenges in sample efficiency, exploration strategies, and value estimation.\n    *   **Limitations of Previous Solutions**:\n        *   **Fixed Environments**: Most prior RL works assume static environments with fixed-yet-known state and action sets, making them difficult to generalize to evolving scenarios.\n        *   **Lifelong RL Distinction**: While related to lifelong reinforcement learning (which trains agents for sequences of similar tasks), Incremental RL specifically deals with *expanding* state and action spaces, a challenge not explicitly addressed by lifelong learning's fixed-set assumption.\n        *   **Inefficient Retraining**: The straightforward approach of retraining agents when environments vary is undesirable due to high computational costs.\n        *   **Sub-optimal Exploration**: Standard `-greedy` exploration is inefficient for increasing search spaces, leading to over-exploration of well-known states and under-exploration of new ones.\n        *   **Inductive Bias**: Previous training can create a strong inductive bias, making it difficult for agents to explore newly added actions or states effectively.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes Dual-Adaptive ``-greedy Exploration (DAE) to address Incremental RL. DAE combines two main strategies: a Meta Policy () and an Explorer ().\n    *   **Novelty/Difference**:\n        *   **Formalizing Incremental RL**: This is the first work to formally model and formulate the challenge of Incremental RL.\n        *   **Dual-Adaptive Exploration**: DAE introduces a novel mechanism for adaptive exploration:\n            *   **Meta Policy ()**: This component adaptively determines a state-dependent `_t` (exploration probability) by assessing the exploration convergence of the current state. It's trained as a binary classifier using a pseudo ground truth based on the TD-Error rate, allowing more exploration for states with high uncertainty.\n            *   **Explorer ()**: This component estimates the relative frequency of actions given a state, guiding the agent to explore the \"least-tried\" actions rather than uniformly random ones. It's implemented as a deep neural network trained by gradient ascent to increase the relative frequency of taken actions.\n        *   **Synergistic Design**: DAE uses the Meta Policy to decide *when* to explore (state-dependent ``) and the Explorer to decide *what* to explore (least-tried actions), making exploration highly targeted and efficient in expanding environments.\n        *   **Handling Expansion**: DAE specifically addresses new states (higher `` from Meta Policy) and new actions (Explorer encourages sampling of initially low Q-value actions) to overcome prior biases.\n\n4.  **Key Technical Contributions**\n    *   **Novel Problem Formulation**: Formal modeling and definition of Incremental Reinforcement Learning (Incremental RL), where MDP state and action spaces continually expand.\n    *   **Novel Exploration Algorithm (DAE)**: Introduction of Dual-Adaptive ``-greedy Exploration, which features:\n        *   A **Meta Policy ()** for adaptive exploitation-exploration trade-off, dynamically adjusting `` based on state-specific value estimation convergence (TD-Error rate).\n        *   An **Explorer ()** for adaptive action exploration, guiding the agent to prioritize least-tried actions by estimating their relative frequencies.\n    *   **System Design for Incremental Learning**: Strategies for reusing trained policies, initializing new input/output neurons in deep Q-networks, and handling Q-values of newly added actions to facilitate incremental adaptation.\n    *   **New Testbed**: Release of a new testbed based on a synthetic \"Expanding World\" environment and an adapted Atari benchmark to evaluate Incremental RL algorithms.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: Experiments were primarily conducted on a custom-designed synthetic environment called \"Expanding World\" (with continually increasing state and action dimensions) and an adapted Atari benchmark (though details for Atari are not in the provided text).\n    *   **Key Performance Metrics**: The primary metric was \"training overhead,\" measured as the number of training steps required to attain near-optimal policies as the state and action spaces increase.\n    *   **Comparison Results**:\n        *   DAE was compared against eight baselines, including various `-greedy` variants (e.g., fixed ``, decaying ``, `-greedy (Retrain)`).\n        *   Results on \"Expanding World\" demonstrated that DAE significantly reduced training overhead, efficiently adapting to new environments.\n        *   DAE achieved an average performance improvement of over 80% compared to the examined baselines, showcasing its ability to efficiently learn unseen transitions and adapt to expanding search spaces.\n\n6.  **Limitations & Scope**\n    *   **Technical Assumptions**: The study assumes that existing transitions (state-action pairs) remain unchanged when the environment expands, focusing the challenge on learning new transitions.\n    *   **Initialization Strategy**: The Q-values of newly-added actions are initialized with small values, and new input/output neurons are added to the deep Q-network, which might influence initial exploration behavior.\n    *   **Scope of Applicability**: The proposed DAE framework is primarily designed for value-based reinforcement learning (Deep Q-Learning) and addresses exploration in environments with expanding state and action spaces.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: \\cite{ding2023whs} significantly advances the technical state-of-the-art by formally defining and providing an effective solution for Incremental RL, a more realistic and challenging setting than traditionally studied fixed-environment RL.\n    *   **Improved Generalization and Efficiency**: DAE's dual-adaptive exploration mechanism allows RL agents to adapt to evolving environments without costly retraining, leading to improved sample efficiency and generalization capabilities.\n    *   **Potential Impact on Future Research**: The introduction of Incremental RL as a formal problem and the release of a dedicated testbed will likely stimulate further research into adaptive RL agents, lifelong learning in dynamic environments, and more sophisticated exploration strategies for continually expanding search spaces. This work paves the way for more practical and robust RL applications in real-world scenarios.",
        "keywords": [
          "Incremental Reinforcement Learning (Incremental RL)",
          "Expanding State and Action Spaces",
          "Dual-Adaptive -greedy Exploration (DAE)",
          "Meta Policy ()",
          "Explorer ()",
          "Adaptive Exploration",
          "Novel Problem Formulation",
          "Computational Overhead Reduction",
          "Training Overhead",
          "Deep Q-Learning",
          "TD-Error Rate",
          "Expanding World Testbed",
          "Preserving Learned Behavior",
          "Real-world Applications"
        ],
        "paper_type": "the paper should be classified as **technical**.\n\nhere's why:\n\n*   **abstract:** explicitly states \"we present a new exploration framework named dual-adaptive -greedy exploration (dae)\" and describes its components (\"employs a meta policy and an explorer\"). it addresses a \"new challenge\" (incremental reinforcement learning) and proposes a solution.\n*   **introduction:** sets up a technical problem (limitations of conventional rl in dynamic environments, leading to the challenge of incremental rl).\n*   **criteria match:** this aligns perfectly with the \"technical\" classification criteria: \"presents new methods, algorithms, or systems\" and \"abstract mentions: 'propose', 'develop', 'present', 'algorithm', 'method'\". while it also includes empirical validation, the core contribution is the *development and presentation of a new framework/method*."
      },
      "file_name": "c1844cda42b3732a5576d05bb6e007eb1db00919.pdf"
    },
    {
      "success": true,
      "doc_id": "362f70972c636168d121b264d4fe4cb0",
      "summary": "Reinforcement learning, often known as RL, has developed as a strong paradigm to teach autonomous software agents to make choices in contexts that are both complicated and dynamic. This abstract investigates recent developments and uses of RL in a variety of fields, showing both its transformational potential and the constraints that it faces at present. Recent developments in reinforcement learning (RL) algorithms, in particular deep reinforcement learning (DRL), have made it possible to make major advancements in autonomous decision- making tasks. DRL algorithms can learn complicated representations of state-action spaces by using deep neural networks. This allows for more efficient exploration and exploitation methods to be implemented. Additionally, advancements in algorithmic enhancements, such as prioritized experience replay and distributional reinforcement learning, have improved the stability and sample efficiency of reinforcement learning algorithms, which has made it possible for these algorithms to be used in real-world applications. Robotics, autonomous cars, game playing, finance, and healthcare are just a few of the many fields that may benefit from the use of RL. In the field of robotics, reinforcement learning (RL) makes it possible for autonomous agents to learn how to navigate, manipulate, and move about in environments that are both complicated and unstructured. To improve both safety and efficiency on the road, autonomous cars make use of reinforcement learning (RL) to make decisions in dynamic traffic situations. In finance, RL algorithms are used for portfolio optimization, algorithmic trading, and risk management. These applications serve to improve investment techniques and decision-making procedures. Furthermore, in the field of healthcare, RL supports individualized treatment planning, clinical decision support, and medical image analysis, which enables physicians to provide patients with care that is specifically suited to their needs. Despite the promising improvements and applications, RL is still confronted with several difficulties that restrict its capacity to be widely adopted and scaled. Among these problems are the inefficiency of the sample, the trade-offs between exploration and exploitation, concerns about safety and dependability, and the need for explainability and interpretability in decision-making processes. To effectively address these difficulties, it is necessary to engage in collaborative efforts across several disciplines, conduct research on algorithmic developments, and establish extensive assessment frameworks (Anon, 2022).",
      "intriguing_abstract": "Reinforcement learning, often known as RL, has developed as a strong paradigm to teach autonomous software agents to make choices in contexts that are both complicated and dynamic. This abstract investigates recent developments and uses of RL in a variety of fields, showing both its transformational potential and the constraints that it faces at present. Recent developments in reinforcement learning (RL) algorithms, in particular deep reinforcement learning (DRL), have made it possible to make major advancements in autonomous decision- making tasks. DRL algorithms can learn complicated representations of state-action spaces by using deep neural networks. This allows for more efficient exploration and exploitation methods to be implemented. Additionally, advancements in algorithmic enhancements, such as prioritized experience replay and distributional reinforcement learning, have improved the stability and sample efficiency of reinforcement learning algorithms, which has made it possible for these algorithms to be used in real-world applications. Robotics, autonomous cars, game playing, finance, and healthcare are just a few of the many fields that may benefit from the use of RL. In the field of robotics, reinforcement learning (RL) makes it possible for autonomous agents to learn how to navigate, manipulate, and move about in environments that are both complicated and unstructured. To improve both safety and efficiency on the road, autonomous cars make use of reinforcement learning (RL) to make decisions in dynamic traffic situations. In finance, RL algorithms are used for portfolio optimization, algorithmic trading, and risk management. These applications serve to improve investment techniques and decision-making procedures. Furthermore, in the field of healthcare, RL supports individualized treatment planning, clinical decision support, and medical image analysis, which enables physicians to provide patients with care that is specifically suited to their needs. Despite the promising improvements and applications, RL is still confronted with several difficulties that restrict its capacity to be widely adopted and scaled. Among these problems are the inefficiency of the sample, the trade-offs between exploration and exploitation, concerns about safety and dependability, and the need for explainability and interpretability in decision-making processes. To effectively address these difficulties, it is necessary to engage in collaborative efforts across several disciplines, conduct research on algorithmic developments, and establish extensive assessment frameworks (Anon, 2022).",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/8af5e79310ec1d8529eba38705e5f29dce789b00.pdf",
      "citation_key": "malaiyappan20245bh",
      "metadata": {
        "title": "Advancements in Reinforcement Learning Algorithms for Autonomous Systems",
        "authors": [
          "Jesu Narkarunai Arasu Malaiyappan",
          "Sai Mani Krishna Sistla",
          "Jawaharbabu Jeyaraman"
        ],
        "published_date": "2024",
        "abstract": "Reinforcement learning, often known as RL, has developed as a strong paradigm to teach autonomous software agents to make choices in contexts that are both complicated and dynamic. This abstract investigates recent developments and uses of RL in a variety of fields, showing both its transformational potential and the constraints that it faces at present. Recent developments in reinforcement learning (RL) algorithms, in particular deep reinforcement learning (DRL), have made it possible to make major advancements in autonomous decision- making tasks. DRL algorithms can learn complicated representations of state-action spaces by using deep neural networks. This allows for more efficient exploration and exploitation methods to be implemented. Additionally, advancements in algorithmic enhancements, such as prioritized experience replay and distributional reinforcement learning, have improved the stability and sample efficiency of reinforcement learning algorithms, which has made it possible for these algorithms to be used in real-world applications. Robotics, autonomous cars, game playing, finance, and healthcare are just a few of the many fields that may benefit from the use of RL. In the field of robotics, reinforcement learning (RL) makes it possible for autonomous agents to learn how to navigate, manipulate, and move about in environments that are both complicated and unstructured. To improve both safety and efficiency on the road, autonomous cars make use of reinforcement learning (RL) to make decisions in dynamic traffic situations. In finance, RL algorithms are used for portfolio optimization, algorithmic trading, and risk management. These applications serve to improve investment techniques and decision-making procedures. Furthermore, in the field of healthcare, RL supports individualized treatment planning, clinical decision support, and medical image analysis, which enables physicians to provide patients with care that is specifically suited to their needs. Despite the promising improvements and applications, RL is still confronted with several difficulties that restrict its capacity to be widely adopted and scaled. Among these problems are the inefficiency of the sample, the trade-offs between exploration and exploitation, concerns about safety and dependability, and the need for explainability and interpretability in decision-making processes. To effectively address these difficulties, it is necessary to engage in collaborative efforts across several disciplines, conduct research on algorithmic developments, and establish extensive assessment frameworks (Anon, 2022).",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/8af5e79310ec1d8529eba38705e5f29dce789b00.pdf",
        "venue": "International Journal of Innovative Science and Research Technology",
        "citationCount": 5,
        "score": 5.0,
        "summary": "Reinforcement learning, often known as RL, has developed as a strong paradigm to teach autonomous software agents to make choices in contexts that are both complicated and dynamic. This abstract investigates recent developments and uses of RL in a variety of fields, showing both its transformational potential and the constraints that it faces at present. Recent developments in reinforcement learning (RL) algorithms, in particular deep reinforcement learning (DRL), have made it possible to make major advancements in autonomous decision- making tasks. DRL algorithms can learn complicated representations of state-action spaces by using deep neural networks. This allows for more efficient exploration and exploitation methods to be implemented. Additionally, advancements in algorithmic enhancements, such as prioritized experience replay and distributional reinforcement learning, have improved the stability and sample efficiency of reinforcement learning algorithms, which has made it possible for these algorithms to be used in real-world applications. Robotics, autonomous cars, game playing, finance, and healthcare are just a few of the many fields that may benefit from the use of RL. In the field of robotics, reinforcement learning (RL) makes it possible for autonomous agents to learn how to navigate, manipulate, and move about in environments that are both complicated and unstructured. To improve both safety and efficiency on the road, autonomous cars make use of reinforcement learning (RL) to make decisions in dynamic traffic situations. In finance, RL algorithms are used for portfolio optimization, algorithmic trading, and risk management. These applications serve to improve investment techniques and decision-making procedures. Furthermore, in the field of healthcare, RL supports individualized treatment planning, clinical decision support, and medical image analysis, which enables physicians to provide patients with care that is specifically suited to their needs. Despite the promising improvements and applications, RL is still confronted with several difficulties that restrict its capacity to be widely adopted and scaled. Among these problems are the inefficiency of the sample, the trade-offs between exploration and exploitation, concerns about safety and dependability, and the need for explainability and interpretability in decision-making processes. To effectively address these difficulties, it is necessary to engage in collaborative efforts across several disciplines, conduct research on algorithmic developments, and establish extensive assessment frameworks (Anon, 2022).",
        "keywords": []
      },
      "file_name": "8af5e79310ec1d8529eba38705e5f29dce789b00.pdf"
    },
    {
      "success": true,
      "doc_id": "a6eb5137bf7ac122b529132cce1d6afb",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/5221ba291d5901f950220f50d289d5e01d81b0c4.pdf",
      "citation_key": "gan2023o50",
      "metadata": {
        "title": "Digital twin-enabled adaptive scheduling strategy based on deep reinforcement learning",
        "authors": [
          "Xuemei Gan",
          "Ying Zuo",
          "Ansi Zhang",
          "Shaobo Li",
          "Fei Tao"
        ],
        "published_date": "2023",
        "abstract": "",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/5221ba291d5901f950220f50d289d5e01d81b0c4.pdf",
        "venue": "Science China Technological Sciences",
        "citationCount": 8,
        "score": 4.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "5221ba291d5901f950220f50d289d5e01d81b0c4.pdf"
    },
    {
      "success": true,
      "doc_id": "8f9ae7230ee5a9d388e8e780598ab11f",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/8ae6c7cff8bb2d766f5f9d3585cd262032378b33.pdf",
      "citation_key": "zhao2023cay",
      "metadata": {
        "title": "Ensemble-based Offline-to-Online Reinforcement Learning: From Pessimistic Learning to Optimistic Exploration",
        "authors": [
          "Kai-Wen Zhao",
          "Yi Ma",
          "Jinyi Liu",
          "Yan Zheng",
          "Zhaopeng Meng"
        ],
        "published_date": "2023",
        "abstract": "",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/8ae6c7cff8bb2d766f5f9d3585cd262032378b33.pdf",
        "venue": "arXiv.org",
        "citationCount": 8,
        "score": 4.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "8ae6c7cff8bb2d766f5f9d3585cd262032378b33.pdf"
    },
    {
      "success": true,
      "doc_id": "6853221c44deb84fd79a0f0a281ea47d",
      "summary": "Resource allocation significantly impacts the performance of vehicle-to-everything (V2X) networks. Most existing algorithms for resource allocation are based on optimization or machine learning (e.g., reinforcement learning). In this paper, we explore resource allocation in a V2X network under the framework of federated reinforcement learning (FRL). On one hand, the usage of RL overcomes many challenges from the model-based optimization schemes. On the other hand, federated learning (FL) enables agents to deal with a number of practical issues, such as privacy, communication overhead, and exploration efficiency. The framework of FRL is then implemented by the in-exact alternative direction method of multipliers (ADMM), where subproblems are solved approximately using policy gradients and their second moments. The developed algorithm, FRLPGiA, has a nice numerical performance compared with some baseline methods for solving the resource allocation problem in a V2X network.",
      "intriguing_abstract": "Resource allocation significantly impacts the performance of vehicle-to-everything (V2X) networks. Most existing algorithms for resource allocation are based on optimization or machine learning (e.g., reinforcement learning). In this paper, we explore resource allocation in a V2X network under the framework of federated reinforcement learning (FRL). On one hand, the usage of RL overcomes many challenges from the model-based optimization schemes. On the other hand, federated learning (FL) enables agents to deal with a number of practical issues, such as privacy, communication overhead, and exploration efficiency. The framework of FRL is then implemented by the in-exact alternative direction method of multipliers (ADMM), where subproblems are solved approximately using policy gradients and their second moments. The developed algorithm, FRLPGiA, has a nice numerical performance compared with some baseline methods for solving the resource allocation problem in a V2X network.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/dad0fcbc6f2c9b7c4d7b7b1d4513d94d57ea63c6.pdf",
      "citation_key": "xu2023m9r",
      "metadata": {
        "title": "Federated Reinforcement Learning for Resource Allocation in V2X Networks",
        "authors": [
          "Kaidi Xu",
          "Shenglong Zhou",
          "Geoffrey Ye Li"
        ],
        "published_date": "2023",
        "abstract": "Resource allocation significantly impacts the performance of vehicle-to-everything (V2X) networks. Most existing algorithms for resource allocation are based on optimization or machine learning (e.g., reinforcement learning). In this paper, we explore resource allocation in a V2X network under the framework of federated reinforcement learning (FRL). On one hand, the usage of RL overcomes many challenges from the model-based optimization schemes. On the other hand, federated learning (FL) enables agents to deal with a number of practical issues, such as privacy, communication overhead, and exploration efficiency. The framework of FRL is then implemented by the in-exact alternative direction method of multipliers (ADMM), where subproblems are solved approximately using policy gradients and their second moments. The developed algorithm, FRLPGiA, has a nice numerical performance compared with some baseline methods for solving the resource allocation problem in a V2X network.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/dad0fcbc6f2c9b7c4d7b7b1d4513d94d57ea63c6.pdf",
        "venue": "IEEE Vehicular Technology Conference",
        "citationCount": 8,
        "score": 4.0,
        "summary": "Resource allocation significantly impacts the performance of vehicle-to-everything (V2X) networks. Most existing algorithms for resource allocation are based on optimization or machine learning (e.g., reinforcement learning). In this paper, we explore resource allocation in a V2X network under the framework of federated reinforcement learning (FRL). On one hand, the usage of RL overcomes many challenges from the model-based optimization schemes. On the other hand, federated learning (FL) enables agents to deal with a number of practical issues, such as privacy, communication overhead, and exploration efficiency. The framework of FRL is then implemented by the in-exact alternative direction method of multipliers (ADMM), where subproblems are solved approximately using policy gradients and their second moments. The developed algorithm, FRLPGiA, has a nice numerical performance compared with some baseline methods for solving the resource allocation problem in a V2X network.",
        "keywords": []
      },
      "file_name": "dad0fcbc6f2c9b7c4d7b7b1d4513d94d57ea63c6.pdf"
    },
    {
      "success": true,
      "doc_id": "7e519e131bd66d81371934097d4f1e12",
      "summary": "Abstract Driven by the remarkable developments we have observed in recent years, path planning for mobile robots is a difficult part of robot navigation. Artificial intelligence applied to mobile robotics is also a distinct challenge; reinforcement learning (RL) is one of the most used algorithms in robotics. The exploration-exploitation dilemma is a motivating challenge for the performance of RL algorithms. The problem is balancing exploitation and exploration, as too much exploration leads to a decrease in cumulative reward, while too much exploitation locks the agent in a local optimum. This paper proposes a new path planning method for mobile robot based on Q-learning with an improved exploration strategy. In addition, a comparative study of Boltzmann distribution and \n$\\epsilon$\n -greedy politics is presented. Through simulations, the better performance of the proposed method in terms of execution time, path length, and cost function is confirmed.",
      "intriguing_abstract": "Abstract Driven by the remarkable developments we have observed in recent years, path planning for mobile robots is a difficult part of robot navigation. Artificial intelligence applied to mobile robotics is also a distinct challenge; reinforcement learning (RL) is one of the most used algorithms in robotics. The exploration-exploitation dilemma is a motivating challenge for the performance of RL algorithms. The problem is balancing exploitation and exploration, as too much exploration leads to a decrease in cumulative reward, while too much exploitation locks the agent in a local optimum. This paper proposes a new path planning method for mobile robot based on Q-learning with an improved exploration strategy. In addition, a comparative study of Boltzmann distribution and \n$\\epsilon$\n -greedy politics is presented. Through simulations, the better performance of the proposed method in terms of execution time, path length, and cost function is confirmed.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/ae47e4e2a4b749543c4de8624049c1d368cbc859.pdf",
      "citation_key": "khlif2023zg3",
      "metadata": {
        "title": "Reinforcement learning with modified exploration strategy for mobile robot path planning",
        "authors": [
          "Nesrine Khlif",
          "Khraief-Hadded Nahla",
          "Belghith Safya"
        ],
        "published_date": "2023",
        "abstract": "Abstract Driven by the remarkable developments we have observed in recent years, path planning for mobile robots is a difficult part of robot navigation. Artificial intelligence applied to mobile robotics is also a distinct challenge; reinforcement learning (RL) is one of the most used algorithms in robotics. The exploration-exploitation dilemma is a motivating challenge for the performance of RL algorithms. The problem is balancing exploitation and exploration, as too much exploration leads to a decrease in cumulative reward, while too much exploitation locks the agent in a local optimum. This paper proposes a new path planning method for mobile robot based on Q-learning with an improved exploration strategy. In addition, a comparative study of Boltzmann distribution and \n$\\epsilon$\n -greedy politics is presented. Through simulations, the better performance of the proposed method in terms of execution time, path length, and cost function is confirmed.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/ae47e4e2a4b749543c4de8624049c1d368cbc859.pdf",
        "venue": "Robotica (Cambridge. Print)",
        "citationCount": 7,
        "score": 3.5,
        "summary": "Abstract Driven by the remarkable developments we have observed in recent years, path planning for mobile robots is a difficult part of robot navigation. Artificial intelligence applied to mobile robotics is also a distinct challenge; reinforcement learning (RL) is one of the most used algorithms in robotics. The exploration-exploitation dilemma is a motivating challenge for the performance of RL algorithms. The problem is balancing exploitation and exploration, as too much exploration leads to a decrease in cumulative reward, while too much exploitation locks the agent in a local optimum. This paper proposes a new path planning method for mobile robot based on Q-learning with an improved exploration strategy. In addition, a comparative study of Boltzmann distribution and \n$\\epsilon$\n -greedy politics is presented. Through simulations, the better performance of the proposed method in terms of execution time, path length, and cost function is confirmed.",
        "keywords": []
      },
      "file_name": "ae47e4e2a4b749543c4de8624049c1d368cbc859.pdf"
    },
    {
      "success": true,
      "doc_id": "e35ccd0d71974deccd9c371741bae3ed",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/c6ed1f7f478f9d004d5f6b9783df79f82d0d1464.pdf",
      "citation_key": "sreedharan2023nae",
      "metadata": {
        "title": "Optimistic Exploration in Reinforcement Learning Using Symbolic Model Estimates",
        "authors": [
          "S. Sreedharan",
          "Michael Katz"
        ],
        "published_date": "2023",
        "abstract": "",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/c6ed1f7f478f9d004d5f6b9783df79f82d0d1464.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 7,
        "score": 3.5,
        "summary": "",
        "keywords": []
      },
      "file_name": "c6ed1f7f478f9d004d5f6b9783df79f82d0d1464.pdf"
    },
    {
      "success": true,
      "doc_id": "ee1dc6d09d02e8fa1805ba6dc999e054",
      "summary": "Offline reinforcement learning (RL) provides a promising solution to learning an agent fully relying on a data-driven paradigm. However, constrained by the limited quality of the offline dataset, its performance is often sub-optimal. Therefore, it is desired to further finetune the agent via extra online interactions before deployment. Unfortunately, offline-to-online RL can be challenging due to two main challenges: constrained exploratory behavior and state-action distribution shift. To this end, we propose a Simple Unified uNcertainty-Guided (SUNG) framework, which naturally unifies the solution to both challenges with the tool of uncertainty. Specifically, SUNG quantifies uncertainty via a VAE-based state-action visitation density estimator. To facilitate efficient exploration, SUNG presents a practical optimistic exploration strategy to select informative actions with both high value and high uncertainty. Moreover, SUNG develops an adaptive exploitation method by applying conservative offline RL objectives to high-uncertainty samples and standard online RL objectives to low-uncertainty samples to smoothly bridge offline and online stages. SUNG achieves state-of-the-art online finetuning performance when combined with different offline RL methods, across various environments and datasets in D4RL benchmark.",
      "intriguing_abstract": "Offline reinforcement learning (RL) provides a promising solution to learning an agent fully relying on a data-driven paradigm. However, constrained by the limited quality of the offline dataset, its performance is often sub-optimal. Therefore, it is desired to further finetune the agent via extra online interactions before deployment. Unfortunately, offline-to-online RL can be challenging due to two main challenges: constrained exploratory behavior and state-action distribution shift. To this end, we propose a Simple Unified uNcertainty-Guided (SUNG) framework, which naturally unifies the solution to both challenges with the tool of uncertainty. Specifically, SUNG quantifies uncertainty via a VAE-based state-action visitation density estimator. To facilitate efficient exploration, SUNG presents a practical optimistic exploration strategy to select informative actions with both high value and high uncertainty. Moreover, SUNG develops an adaptive exploitation method by applying conservative offline RL objectives to high-uncertainty samples and standard online RL objectives to low-uncertainty samples to smoothly bridge offline and online stages. SUNG achieves state-of-the-art online finetuning performance when combined with different offline RL methods, across various environments and datasets in D4RL benchmark.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/2807f9c666335946113fb11dccadf36f8d78b772.pdf",
      "citation_key": "guo20233sd",
      "metadata": {
        "title": "A Simple Unified Uncertainty-Guided Framework for Offline-to-Online Reinforcement Learning",
        "authors": [
          "Siyuan Guo",
          "Yanchao Sun",
          "Jifeng Hu",
          "Sili Huang",
          "Hechang Chen",
          "Haiyin Piao",
          "Lichao Sun",
          "Yi Chang"
        ],
        "published_date": "2023",
        "abstract": "Offline reinforcement learning (RL) provides a promising solution to learning an agent fully relying on a data-driven paradigm. However, constrained by the limited quality of the offline dataset, its performance is often sub-optimal. Therefore, it is desired to further finetune the agent via extra online interactions before deployment. Unfortunately, offline-to-online RL can be challenging due to two main challenges: constrained exploratory behavior and state-action distribution shift. To this end, we propose a Simple Unified uNcertainty-Guided (SUNG) framework, which naturally unifies the solution to both challenges with the tool of uncertainty. Specifically, SUNG quantifies uncertainty via a VAE-based state-action visitation density estimator. To facilitate efficient exploration, SUNG presents a practical optimistic exploration strategy to select informative actions with both high value and high uncertainty. Moreover, SUNG develops an adaptive exploitation method by applying conservative offline RL objectives to high-uncertainty samples and standard online RL objectives to low-uncertainty samples to smoothly bridge offline and online stages. SUNG achieves state-of-the-art online finetuning performance when combined with different offline RL methods, across various environments and datasets in D4RL benchmark.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/2807f9c666335946113fb11dccadf36f8d78b772.pdf",
        "venue": "arXiv.org",
        "citationCount": 7,
        "score": 3.5,
        "summary": "Offline reinforcement learning (RL) provides a promising solution to learning an agent fully relying on a data-driven paradigm. However, constrained by the limited quality of the offline dataset, its performance is often sub-optimal. Therefore, it is desired to further finetune the agent via extra online interactions before deployment. Unfortunately, offline-to-online RL can be challenging due to two main challenges: constrained exploratory behavior and state-action distribution shift. To this end, we propose a Simple Unified uNcertainty-Guided (SUNG) framework, which naturally unifies the solution to both challenges with the tool of uncertainty. Specifically, SUNG quantifies uncertainty via a VAE-based state-action visitation density estimator. To facilitate efficient exploration, SUNG presents a practical optimistic exploration strategy to select informative actions with both high value and high uncertainty. Moreover, SUNG develops an adaptive exploitation method by applying conservative offline RL objectives to high-uncertainty samples and standard online RL objectives to low-uncertainty samples to smoothly bridge offline and online stages. SUNG achieves state-of-the-art online finetuning performance when combined with different offline RL methods, across various environments and datasets in D4RL benchmark.",
        "keywords": []
      },
      "file_name": "2807f9c666335946113fb11dccadf36f8d78b772.pdf"
    },
    {
      "success": true,
      "doc_id": "9d44a84b7806baa1ab7a89bf4008864d",
      "summary": "Safety is one of the main challenges in applying reinforcement learning to tasks in realistic environments. To ensure safety during and after the training process, existing methods tend to adopt overly conservative policies to avoid unsafe situations. However, an overly conservative policy severely hinders exploration. In this letter, we propose a method to construct a boundary that discriminates between safe and unsafe states. The boundary we construct is equivalent to distinguishing dead-end states, indicating the maximum extent to which safe exploration is guaranteed, and thus has a minimum limitation on exploration. Similar to Recovery Reinforcement Learning, we utilize a decoupled RL framework to learn two policies, (1) a task policy that only considers improving the task performance, and (2) a recovery policy that maximizes safety. The recovery policy and a corresponding safety critic are pre-trained on an offline dataset, in which the safety critic evaluates the upper bound of safety in each state as awareness of environmental safety for the agent. During online training, a behavior correction mechanism is adopted, ensuring the agent interacts with the environment using safe actions only. Finally, experiments of continuous control tasks demonstrate that our approach has better task performance with fewer safety violations than state-of-the-art algorithms.",
      "intriguing_abstract": "Safety is one of the main challenges in applying reinforcement learning to tasks in realistic environments. To ensure safety during and after the training process, existing methods tend to adopt overly conservative policies to avoid unsafe situations. However, an overly conservative policy severely hinders exploration. In this letter, we propose a method to construct a boundary that discriminates between safe and unsafe states. The boundary we construct is equivalent to distinguishing dead-end states, indicating the maximum extent to which safe exploration is guaranteed, and thus has a minimum limitation on exploration. Similar to Recovery Reinforcement Learning, we utilize a decoupled RL framework to learn two policies, (1) a task policy that only considers improving the task performance, and (2) a recovery policy that maximizes safety. The recovery policy and a corresponding safety critic are pre-trained on an offline dataset, in which the safety critic evaluates the upper bound of safety in each state as awareness of environmental safety for the agent. During online training, a behavior correction mechanism is adopted, ensuring the agent interacts with the environment using safe actions only. Finally, experiments of continuous control tasks demonstrate that our approach has better task performance with fewer safety violations than state-of-the-art algorithms.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/9eed36fb9b9bbb97579953d5e303dc0cf6c70a58.pdf",
      "citation_key": "zhang2023wqi",
      "metadata": {
        "title": "Safe Reinforcement Learning With Dead-Ends Avoidance and Recovery",
        "authors": [
          "Xiao Zhang",
          "Hai Zhang",
          "Hongtu Zhou",
          "Chang Huang",
          "Di Zhang",
          "Chen Ye",
          "Junqiao Zhao"
        ],
        "published_date": "2023",
        "abstract": "Safety is one of the main challenges in applying reinforcement learning to tasks in realistic environments. To ensure safety during and after the training process, existing methods tend to adopt overly conservative policies to avoid unsafe situations. However, an overly conservative policy severely hinders exploration. In this letter, we propose a method to construct a boundary that discriminates between safe and unsafe states. The boundary we construct is equivalent to distinguishing dead-end states, indicating the maximum extent to which safe exploration is guaranteed, and thus has a minimum limitation on exploration. Similar to Recovery Reinforcement Learning, we utilize a decoupled RL framework to learn two policies, (1) a task policy that only considers improving the task performance, and (2) a recovery policy that maximizes safety. The recovery policy and a corresponding safety critic are pre-trained on an offline dataset, in which the safety critic evaluates the upper bound of safety in each state as awareness of environmental safety for the agent. During online training, a behavior correction mechanism is adopted, ensuring the agent interacts with the environment using safe actions only. Finally, experiments of continuous control tasks demonstrate that our approach has better task performance with fewer safety violations than state-of-the-art algorithms.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/9eed36fb9b9bbb97579953d5e303dc0cf6c70a58.pdf",
        "venue": "IEEE Robotics and Automation Letters",
        "citationCount": 7,
        "score": 3.5,
        "summary": "Safety is one of the main challenges in applying reinforcement learning to tasks in realistic environments. To ensure safety during and after the training process, existing methods tend to adopt overly conservative policies to avoid unsafe situations. However, an overly conservative policy severely hinders exploration. In this letter, we propose a method to construct a boundary that discriminates between safe and unsafe states. The boundary we construct is equivalent to distinguishing dead-end states, indicating the maximum extent to which safe exploration is guaranteed, and thus has a minimum limitation on exploration. Similar to Recovery Reinforcement Learning, we utilize a decoupled RL framework to learn two policies, (1) a task policy that only considers improving the task performance, and (2) a recovery policy that maximizes safety. The recovery policy and a corresponding safety critic are pre-trained on an offline dataset, in which the safety critic evaluates the upper bound of safety in each state as awareness of environmental safety for the agent. During online training, a behavior correction mechanism is adopted, ensuring the agent interacts with the environment using safe actions only. Finally, experiments of continuous control tasks demonstrate that our approach has better task performance with fewer safety violations than state-of-the-art algorithms.",
        "keywords": []
      },
      "file_name": "9eed36fb9b9bbb97579953d5e303dc0cf6c70a58.pdf"
    },
    {
      "success": true,
      "doc_id": "9ed50577cec11029f5ae00b3869530d0",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/9c9a527ad885e664a31f0b70ec5f838d0c6ebfa6.pdf",
      "citation_key": "beikmohammadi2023v6w",
      "metadata": {
        "title": "TA-Explore: Teacher-Assisted Exploration for Facilitating Fast Reinforcement Learning",
        "authors": [
          "Ali Beikmohammadi",
          "S. Magnsson"
        ],
        "published_date": "2023",
        "abstract": "",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/9c9a527ad885e664a31f0b70ec5f838d0c6ebfa6.pdf",
        "venue": "Adaptive Agents and Multi-Agent Systems",
        "citationCount": 6,
        "score": 3.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "9c9a527ad885e664a31f0b70ec5f838d0c6ebfa6.pdf"
    },
    {
      "success": true,
      "doc_id": "5e77a91943bb7806cecc1ea0cc0ff517",
      "summary": "Safety is critical to broadening the application of reinforcement learning (RL). Often, we train RL agents in a controlled environment, such as a laboratory, before deploying them in the real world. However, the real-world target task might be unknown prior to deployment. Reward-free RL trains an agent without the reward to adapt quickly once the reward is revealed. We consider the constrained reward-free setting, where an agent (the guide) learns to explore safely without the reward signal. This agent is trained in a controlled environment, which allows unsafe interactions and still provides the safety signal. After the target task is revealed, safety violations are not allowed anymore. Thus, the guide is leveraged to compose a safe behaviour policy. Drawing from transfer learning, we also regularize a target policy (the student) towards the guide while the student is unreliable and gradually eliminate the influence of the guide as training progresses. The empirical analysis shows that this method can achieve safe transfer learning and helps the student solve the target task faster.",
      "intriguing_abstract": "Safety is critical to broadening the application of reinforcement learning (RL). Often, we train RL agents in a controlled environment, such as a laboratory, before deploying them in the real world. However, the real-world target task might be unknown prior to deployment. Reward-free RL trains an agent without the reward to adapt quickly once the reward is revealed. We consider the constrained reward-free setting, where an agent (the guide) learns to explore safely without the reward signal. This agent is trained in a controlled environment, which allows unsafe interactions and still provides the safety signal. After the target task is revealed, safety violations are not allowed anymore. Thus, the guide is leveraged to compose a safe behaviour policy. Drawing from transfer learning, we also regularize a target policy (the student) towards the guide while the student is unreliable and gradually eliminate the influence of the guide as training progresses. The empirical analysis shows that this method can achieve safe transfer learning and helps the student solve the target task faster.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/8ca9a74503c240b2746e351995ee0415657f1cd0.pdf",
      "citation_key": "yang2023n56",
      "metadata": {
        "title": "Reinforcement Learning by Guided Safe Exploration",
        "authors": [
          "Qisong Yang",
          "T. D. Simo",
          "N. Jansen",
          "Simon Tindemans",
          "M. Spaan"
        ],
        "published_date": "2023",
        "abstract": "Safety is critical to broadening the application of reinforcement learning (RL). Often, we train RL agents in a controlled environment, such as a laboratory, before deploying them in the real world. However, the real-world target task might be unknown prior to deployment. Reward-free RL trains an agent without the reward to adapt quickly once the reward is revealed. We consider the constrained reward-free setting, where an agent (the guide) learns to explore safely without the reward signal. This agent is trained in a controlled environment, which allows unsafe interactions and still provides the safety signal. After the target task is revealed, safety violations are not allowed anymore. Thus, the guide is leveraged to compose a safe behaviour policy. Drawing from transfer learning, we also regularize a target policy (the student) towards the guide while the student is unreliable and gradually eliminate the influence of the guide as training progresses. The empirical analysis shows that this method can achieve safe transfer learning and helps the student solve the target task faster.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/8ca9a74503c240b2746e351995ee0415657f1cd0.pdf",
        "venue": "European Conference on Artificial Intelligence",
        "citationCount": 6,
        "score": 3.0,
        "summary": "Safety is critical to broadening the application of reinforcement learning (RL). Often, we train RL agents in a controlled environment, such as a laboratory, before deploying them in the real world. However, the real-world target task might be unknown prior to deployment. Reward-free RL trains an agent without the reward to adapt quickly once the reward is revealed. We consider the constrained reward-free setting, where an agent (the guide) learns to explore safely without the reward signal. This agent is trained in a controlled environment, which allows unsafe interactions and still provides the safety signal. After the target task is revealed, safety violations are not allowed anymore. Thus, the guide is leveraged to compose a safe behaviour policy. Drawing from transfer learning, we also regularize a target policy (the student) towards the guide while the student is unreliable and gradually eliminate the influence of the guide as training progresses. The empirical analysis shows that this method can achieve safe transfer learning and helps the student solve the target task faster.",
        "keywords": []
      },
      "file_name": "8ca9a74503c240b2746e351995ee0415657f1cd0.pdf"
    },
    {
      "success": true,
      "doc_id": "bd748d7b862011c9f10d1353a0f987ee",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/06ffbf40c29daa4f6b4c5025d96ab1c27c0ea117.pdf",
      "citation_key": "alvarez2023v09",
      "metadata": {
        "title": "Forest Fire Localization: From Reinforcement Learning Exploration to a Dynamic Drone Control",
        "authors": [
          "Jonatan Alvarez",
          "Assia Belbachir",
          "Faiza Belbachir",
          "Jamy Chahal",
          "Abdelhak Goudjil",
          "Johvany Gustave",
          "Aybke ztrk Suri"
        ],
        "published_date": "2023",
        "abstract": "",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/06ffbf40c29daa4f6b4c5025d96ab1c27c0ea117.pdf",
        "venue": "Journal of Intelligent and Robotic Systems",
        "citationCount": 6,
        "score": 3.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "06ffbf40c29daa4f6b4c5025d96ab1c27c0ea117.pdf"
    },
    {
      "success": true,
      "doc_id": "3cbebf8bf7f675326f0dcefdcbd7d5ba",
      "summary": "Efficient and intelligent exploration remains a major challenge in the field of deep reinforcement learning (DRL). Bayesian inference with a distributional representation is usually an effective way to improve the exploration ability of the RL agent. However, when optimizing Bayesian neural networks (BNNs), most algorithms need to specify an explicit parameter distribution such as a multivariate Gaussian distribution. This may reduce the flexibility of model representation and affect the algorithm performance. Therefore, to improve sample efficiency and exploration based on Bayesian methods, we propose a novel implicit posteriori parameter distribution optimization (IPPDO) algorithm. First, we adopt a distributional perspective on the parameter and model it with an implicit distribution, which is approximated by generative models. Each model corresponds to a learned latent space, providing structured stochasticity for each layer in the network. Next, to make it possible to optimize an implicit posteriori parameter distribution, we build an energy-based model (EBM) with value function to represent the implicit distribution which is not constrained by any analytic density function. Then, we design a training algorithm based on amortized Stein variational gradient descent (SVGD) to improve the model learning efficiency. We compare IPPDO with other prevailing DRL algorithms on the OpenAI Gym, MuJoCo, and Box2D platforms. Experiments on various tasks demonstrate that the proposed algorithm can represent the parameter uncertainty implicitly for a learned policy and can consistently outperform competing approaches.",
      "intriguing_abstract": "Efficient and intelligent exploration remains a major challenge in the field of deep reinforcement learning (DRL). Bayesian inference with a distributional representation is usually an effective way to improve the exploration ability of the RL agent. However, when optimizing Bayesian neural networks (BNNs), most algorithms need to specify an explicit parameter distribution such as a multivariate Gaussian distribution. This may reduce the flexibility of model representation and affect the algorithm performance. Therefore, to improve sample efficiency and exploration based on Bayesian methods, we propose a novel implicit posteriori parameter distribution optimization (IPPDO) algorithm. First, we adopt a distributional perspective on the parameter and model it with an implicit distribution, which is approximated by generative models. Each model corresponds to a learned latent space, providing structured stochasticity for each layer in the network. Next, to make it possible to optimize an implicit posteriori parameter distribution, we build an energy-based model (EBM) with value function to represent the implicit distribution which is not constrained by any analytic density function. Then, we design a training algorithm based on amortized Stein variational gradient descent (SVGD) to improve the model learning efficiency. We compare IPPDO with other prevailing DRL algorithms on the OpenAI Gym, MuJoCo, and Box2D platforms. Experiments on various tasks demonstrate that the proposed algorithm can represent the parameter uncertainty implicitly for a learned policy and can consistently outperform competing approaches.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/7f437f4af59ff994d97482ee1c12aaeb4b310e85.pdf",
      "citation_key": "li2023kgk",
      "metadata": {
        "title": "Implicit Posteriori Parameter Distribution Optimization in Reinforcement Learning",
        "authors": [
          "Tianyi Li",
          "Gen-ke Yang",
          "Jian Chu"
        ],
        "published_date": "2023",
        "abstract": "Efficient and intelligent exploration remains a major challenge in the field of deep reinforcement learning (DRL). Bayesian inference with a distributional representation is usually an effective way to improve the exploration ability of the RL agent. However, when optimizing Bayesian neural networks (BNNs), most algorithms need to specify an explicit parameter distribution such as a multivariate Gaussian distribution. This may reduce the flexibility of model representation and affect the algorithm performance. Therefore, to improve sample efficiency and exploration based on Bayesian methods, we propose a novel implicit posteriori parameter distribution optimization (IPPDO) algorithm. First, we adopt a distributional perspective on the parameter and model it with an implicit distribution, which is approximated by generative models. Each model corresponds to a learned latent space, providing structured stochasticity for each layer in the network. Next, to make it possible to optimize an implicit posteriori parameter distribution, we build an energy-based model (EBM) with value function to represent the implicit distribution which is not constrained by any analytic density function. Then, we design a training algorithm based on amortized Stein variational gradient descent (SVGD) to improve the model learning efficiency. We compare IPPDO with other prevailing DRL algorithms on the OpenAI Gym, MuJoCo, and Box2D platforms. Experiments on various tasks demonstrate that the proposed algorithm can represent the parameter uncertainty implicitly for a learned policy and can consistently outperform competing approaches.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/7f437f4af59ff994d97482ee1c12aaeb4b310e85.pdf",
        "venue": "IEEE Transactions on Cybernetics",
        "citationCount": 6,
        "score": 3.0,
        "summary": "Efficient and intelligent exploration remains a major challenge in the field of deep reinforcement learning (DRL). Bayesian inference with a distributional representation is usually an effective way to improve the exploration ability of the RL agent. However, when optimizing Bayesian neural networks (BNNs), most algorithms need to specify an explicit parameter distribution such as a multivariate Gaussian distribution. This may reduce the flexibility of model representation and affect the algorithm performance. Therefore, to improve sample efficiency and exploration based on Bayesian methods, we propose a novel implicit posteriori parameter distribution optimization (IPPDO) algorithm. First, we adopt a distributional perspective on the parameter and model it with an implicit distribution, which is approximated by generative models. Each model corresponds to a learned latent space, providing structured stochasticity for each layer in the network. Next, to make it possible to optimize an implicit posteriori parameter distribution, we build an energy-based model (EBM) with value function to represent the implicit distribution which is not constrained by any analytic density function. Then, we design a training algorithm based on amortized Stein variational gradient descent (SVGD) to improve the model learning efficiency. We compare IPPDO with other prevailing DRL algorithms on the OpenAI Gym, MuJoCo, and Box2D platforms. Experiments on various tasks demonstrate that the proposed algorithm can represent the parameter uncertainty implicitly for a learned policy and can consistently outperform competing approaches.",
        "keywords": []
      },
      "file_name": "7f437f4af59ff994d97482ee1c12aaeb4b310e85.pdf"
    },
    {
      "success": true,
      "doc_id": "340ed8e98569eee4d274b1010c7a74b0",
      "summary": "Accurately localizing 1-D signal patterns, such as Gamma-ray well-log depth matching, is crucial in the oilfield service industry as it directly affects the quality of oil and gas exploration. However, traditional methods such as well-log curve analysis and pattern hand-picking matching are labor-intensive and heavily rely on human expertise, leading to inconsistent results. Although attempts have been made to automate this process, challenges such as low computational performance, nonrobustness, and nongeneralization remain unsolved. To address these challenges, we have developed a data-driven AI system that learns an active signal pattern localization strategy inspired by human attention. Our artificial intelligence system uses an offline reinforcement learning (RL) framework as its central component, which solves a highly abstracted Markov decision process (MDP) problem via offline training on human-labeled historical data. The RL agent uses top-down reasoning to determine the location of target signal fragments by deforming a bounding window using simple transformation actions. To overcome distribution shifts between logged data and real and ensure generalization, we propose a discrete distributionally robust soft actor-critic (SAC) RL framework (DRSAC-Discrete) to solve the MDP problem under uncertainty. By exploring unfamiliar environments in a restrictive manner, the DRSAC-Discrete algorithm provides a safe solution that can be used when data is limited during the early stage of this industrial application. We evaluated the RL-based localization system on augmented field Gamma-ray well-log datasets, and the results showed promising localization capability. Furthermore, the DRSAC-Discrete algorithm demonstrated relatively robust performance guarantees when facing data shortage.",
      "intriguing_abstract": "Accurately localizing 1-D signal patterns, such as Gamma-ray well-log depth matching, is crucial in the oilfield service industry as it directly affects the quality of oil and gas exploration. However, traditional methods such as well-log curve analysis and pattern hand-picking matching are labor-intensive and heavily rely on human expertise, leading to inconsistent results. Although attempts have been made to automate this process, challenges such as low computational performance, nonrobustness, and nongeneralization remain unsolved. To address these challenges, we have developed a data-driven AI system that learns an active signal pattern localization strategy inspired by human attention. Our artificial intelligence system uses an offline reinforcement learning (RL) framework as its central component, which solves a highly abstracted Markov decision process (MDP) problem via offline training on human-labeled historical data. The RL agent uses top-down reasoning to determine the location of target signal fragments by deforming a bounding window using simple transformation actions. To overcome distribution shifts between logged data and real and ensure generalization, we propose a discrete distributionally robust soft actor-critic (SAC) RL framework (DRSAC-Discrete) to solve the MDP problem under uncertainty. By exploring unfamiliar environments in a restrictive manner, the DRSAC-Discrete algorithm provides a safe solution that can be used when data is limited during the early stage of this industrial application. We evaluated the RL-based localization system on augmented field Gamma-ray well-log datasets, and the results showed promising localization capability. Furthermore, the DRSAC-Discrete algorithm demonstrated relatively robust performance guarantees when facing data shortage.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/839395c4823ac8fff990485e7ce54e53c94bae6b.pdf",
      "citation_key": "zi20238ug",
      "metadata": {
        "title": "Active Gamma-Ray Log Pattern Localization With Distributionally Robust Reinforcement Learning",
        "authors": [
          "Yuan Zi",
          "Lei Fan",
          "Xuqing Wu",
          "Jiefu Chen",
          "Shirui Wang",
          "Zhu Han"
        ],
        "published_date": "2023",
        "abstract": "Accurately localizing 1-D signal patterns, such as Gamma-ray well-log depth matching, is crucial in the oilfield service industry as it directly affects the quality of oil and gas exploration. However, traditional methods such as well-log curve analysis and pattern hand-picking matching are labor-intensive and heavily rely on human expertise, leading to inconsistent results. Although attempts have been made to automate this process, challenges such as low computational performance, nonrobustness, and nongeneralization remain unsolved. To address these challenges, we have developed a data-driven AI system that learns an active signal pattern localization strategy inspired by human attention. Our artificial intelligence system uses an offline reinforcement learning (RL) framework as its central component, which solves a highly abstracted Markov decision process (MDP) problem via offline training on human-labeled historical data. The RL agent uses top-down reasoning to determine the location of target signal fragments by deforming a bounding window using simple transformation actions. To overcome distribution shifts between logged data and real and ensure generalization, we propose a discrete distributionally robust soft actor-critic (SAC) RL framework (DRSAC-Discrete) to solve the MDP problem under uncertainty. By exploring unfamiliar environments in a restrictive manner, the DRSAC-Discrete algorithm provides a safe solution that can be used when data is limited during the early stage of this industrial application. We evaluated the RL-based localization system on augmented field Gamma-ray well-log datasets, and the results showed promising localization capability. Furthermore, the DRSAC-Discrete algorithm demonstrated relatively robust performance guarantees when facing data shortage.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/839395c4823ac8fff990485e7ce54e53c94bae6b.pdf",
        "venue": "IEEE Transactions on Geoscience and Remote Sensing",
        "citationCount": 5,
        "score": 2.5,
        "summary": "Accurately localizing 1-D signal patterns, such as Gamma-ray well-log depth matching, is crucial in the oilfield service industry as it directly affects the quality of oil and gas exploration. However, traditional methods such as well-log curve analysis and pattern hand-picking matching are labor-intensive and heavily rely on human expertise, leading to inconsistent results. Although attempts have been made to automate this process, challenges such as low computational performance, nonrobustness, and nongeneralization remain unsolved. To address these challenges, we have developed a data-driven AI system that learns an active signal pattern localization strategy inspired by human attention. Our artificial intelligence system uses an offline reinforcement learning (RL) framework as its central component, which solves a highly abstracted Markov decision process (MDP) problem via offline training on human-labeled historical data. The RL agent uses top-down reasoning to determine the location of target signal fragments by deforming a bounding window using simple transformation actions. To overcome distribution shifts between logged data and real and ensure generalization, we propose a discrete distributionally robust soft actor-critic (SAC) RL framework (DRSAC-Discrete) to solve the MDP problem under uncertainty. By exploring unfamiliar environments in a restrictive manner, the DRSAC-Discrete algorithm provides a safe solution that can be used when data is limited during the early stage of this industrial application. We evaluated the RL-based localization system on augmented field Gamma-ray well-log datasets, and the results showed promising localization capability. Furthermore, the DRSAC-Discrete algorithm demonstrated relatively robust performance guarantees when facing data shortage.",
        "keywords": []
      },
      "file_name": "839395c4823ac8fff990485e7ce54e53c94bae6b.pdf"
    },
    {
      "success": true,
      "doc_id": "d6895b1e690ea9b6c2b4ce135f0bde33",
      "summary": "Reinforcement learning (RL) can obtain the supervisory controller for discrete-event systems modeled by finite automata and temporal logic. The published methods often have two limitations. First, a large number of training data are required to learn the RL controller. Second, the RL algorithms do not consider uncontrollable events, which are essential for supervisory control theory (SCT). To address the limitations, we first apply SCT to find the supervisors for the specifications modeled by automata. These supervisors remove illegal training data violating these specifications and hence reduce the exploration space of the RL algorithm. For the remaining specifications modeled by temporal logic, the RL algorithm is applied to search for the optimal control decision within the confined exploration space. Uncontrollable events are considered by the RL algorithm as uncertainties in the plant model. The proposed method can obtain a nonblocking supervisor for all specifications with less learning time than the published methods.",
      "intriguing_abstract": "Reinforcement learning (RL) can obtain the supervisory controller for discrete-event systems modeled by finite automata and temporal logic. The published methods often have two limitations. First, a large number of training data are required to learn the RL controller. Second, the RL algorithms do not consider uncontrollable events, which are essential for supervisory control theory (SCT). To address the limitations, we first apply SCT to find the supervisors for the specifications modeled by automata. These supervisors remove illegal training data violating these specifications and hence reduce the exploration space of the RL algorithm. For the remaining specifications modeled by temporal logic, the RL algorithm is applied to search for the optimal control decision within the confined exploration space. Uncontrollable events are considered by the RL algorithm as uncertainties in the plant model. The proposed method can obtain a nonblocking supervisor for all specifications with less learning time than the published methods.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/7825ea27ec1762f6ac41347603535500bcd121f7.pdf",
      "citation_key": "yang2023w3h",
      "metadata": {
        "title": "Reducing the Learning Time of Reinforcement Learning for the Supervisory Control of Discrete Event Systems",
        "authors": [
          "Junjun Yang",
          "Kaige Tan",
          "Lei Feng",
          "Ahmed M. El-Sherbeeny",
          "Zhiwu Li"
        ],
        "published_date": "2023",
        "abstract": "Reinforcement learning (RL) can obtain the supervisory controller for discrete-event systems modeled by finite automata and temporal logic. The published methods often have two limitations. First, a large number of training data are required to learn the RL controller. Second, the RL algorithms do not consider uncontrollable events, which are essential for supervisory control theory (SCT). To address the limitations, we first apply SCT to find the supervisors for the specifications modeled by automata. These supervisors remove illegal training data violating these specifications and hence reduce the exploration space of the RL algorithm. For the remaining specifications modeled by temporal logic, the RL algorithm is applied to search for the optimal control decision within the confined exploration space. Uncontrollable events are considered by the RL algorithm as uncertainties in the plant model. The proposed method can obtain a nonblocking supervisor for all specifications with less learning time than the published methods.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/7825ea27ec1762f6ac41347603535500bcd121f7.pdf",
        "venue": "IEEE Access",
        "citationCount": 5,
        "score": 2.5,
        "summary": "Reinforcement learning (RL) can obtain the supervisory controller for discrete-event systems modeled by finite automata and temporal logic. The published methods often have two limitations. First, a large number of training data are required to learn the RL controller. Second, the RL algorithms do not consider uncontrollable events, which are essential for supervisory control theory (SCT). To address the limitations, we first apply SCT to find the supervisors for the specifications modeled by automata. These supervisors remove illegal training data violating these specifications and hence reduce the exploration space of the RL algorithm. For the remaining specifications modeled by temporal logic, the RL algorithm is applied to search for the optimal control decision within the confined exploration space. Uncontrollable events are considered by the RL algorithm as uncertainties in the plant model. The proposed method can obtain a nonblocking supervisor for all specifications with less learning time than the published methods.",
        "keywords": []
      },
      "file_name": "7825ea27ec1762f6ac41347603535500bcd121f7.pdf"
    },
    {
      "success": true,
      "doc_id": "7cbf4d2d5963fa394740464183b503cb",
      "summary": "A fundamental challenge for reinforcement learning (RL) is how to achieve efficient exploration in initially unknown environments. Most state-of-the-art RL algorithms leverage action space noise to drive exploration. The classical strategies are computationally efficient and straightforward to implement. However, these methods may fail to perform effectively in complex environments. To address this issue, we propose a novel strategy named reward space noise (RSN) for farsighted and consistent exploration in RL. By introducing the stochasticity from reward space, we are able to change agents understanding about environment and perturb its behaviors. We find that the simple RSN can achieve consistent exploration and scale to complex domains without intensive computational cost. To demonstrate the effectiveness and scalability of the proposed method, we implement a deep Q-learning agent with reward noise and evaluate its exploratory performance on a set of Atari games which are challenging for the naive [Formula: see text]-greedy strategy. The results show that reward noise outperforms action noise in most games and performs comparably in others. Concretely, we found that in the early training, the best exploratory performance of reward noise is obviously better than action noise, which demonstrates that the reward noise can quickly explore the valuable states and aid in finding the optimal policy. Moreover, the average scores and learning efficiency of reward noise are also higher than action noise through the whole training, which indicates that the reward noise can generate more stable and consistent performance.",
      "intriguing_abstract": "A fundamental challenge for reinforcement learning (RL) is how to achieve efficient exploration in initially unknown environments. Most state-of-the-art RL algorithms leverage action space noise to drive exploration. The classical strategies are computationally efficient and straightforward to implement. However, these methods may fail to perform effectively in complex environments. To address this issue, we propose a novel strategy named reward space noise (RSN) for farsighted and consistent exploration in RL. By introducing the stochasticity from reward space, we are able to change agents understanding about environment and perturb its behaviors. We find that the simple RSN can achieve consistent exploration and scale to complex domains without intensive computational cost. To demonstrate the effectiveness and scalability of the proposed method, we implement a deep Q-learning agent with reward noise and evaluate its exploratory performance on a set of Atari games which are challenging for the naive [Formula: see text]-greedy strategy. The results show that reward noise outperforms action noise in most games and performs comparably in others. Concretely, we found that in the early training, the best exploratory performance of reward noise is obviously better than action noise, which demonstrates that the reward noise can quickly explore the valuable states and aid in finding the optimal policy. Moreover, the average scores and learning efficiency of reward noise are also higher than action noise through the whole training, which indicates that the reward noise can generate more stable and consistent performance.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/859172d8cb31ddebd9b18e3a7a7d982fab1d1341.pdf",
      "citation_key": "sun20219kr",
      "metadata": {
        "title": "Reward Space Noise for Exploration in Deep Reinforcement Learning",
        "authors": [
          "Chuxiong Sun",
          "Rui Wang",
          "Qian Li",
          "Xiaohui Hu"
        ],
        "published_date": "2021",
        "abstract": "A fundamental challenge for reinforcement learning (RL) is how to achieve efficient exploration in initially unknown environments. Most state-of-the-art RL algorithms leverage action space noise to drive exploration. The classical strategies are computationally efficient and straightforward to implement. However, these methods may fail to perform effectively in complex environments. To address this issue, we propose a novel strategy named reward space noise (RSN) for farsighted and consistent exploration in RL. By introducing the stochasticity from reward space, we are able to change agents understanding about environment and perturb its behaviors. We find that the simple RSN can achieve consistent exploration and scale to complex domains without intensive computational cost. To demonstrate the effectiveness and scalability of the proposed method, we implement a deep Q-learning agent with reward noise and evaluate its exploratory performance on a set of Atari games which are challenging for the naive [Formula: see text]-greedy strategy. The results show that reward noise outperforms action noise in most games and performs comparably in others. Concretely, we found that in the early training, the best exploratory performance of reward noise is obviously better than action noise, which demonstrates that the reward noise can quickly explore the valuable states and aid in finding the optimal policy. Moreover, the average scores and learning efficiency of reward noise are also higher than action noise through the whole training, which indicates that the reward noise can generate more stable and consistent performance.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/859172d8cb31ddebd9b18e3a7a7d982fab1d1341.pdf",
        "venue": "International journal of pattern recognition and artificial intelligence",
        "citationCount": 7,
        "score": 1.75,
        "summary": "A fundamental challenge for reinforcement learning (RL) is how to achieve efficient exploration in initially unknown environments. Most state-of-the-art RL algorithms leverage action space noise to drive exploration. The classical strategies are computationally efficient and straightforward to implement. However, these methods may fail to perform effectively in complex environments. To address this issue, we propose a novel strategy named reward space noise (RSN) for farsighted and consistent exploration in RL. By introducing the stochasticity from reward space, we are able to change agents understanding about environment and perturb its behaviors. We find that the simple RSN can achieve consistent exploration and scale to complex domains without intensive computational cost. To demonstrate the effectiveness and scalability of the proposed method, we implement a deep Q-learning agent with reward noise and evaluate its exploratory performance on a set of Atari games which are challenging for the naive [Formula: see text]-greedy strategy. The results show that reward noise outperforms action noise in most games and performs comparably in others. Concretely, we found that in the early training, the best exploratory performance of reward noise is obviously better than action noise, which demonstrates that the reward noise can quickly explore the valuable states and aid in finding the optimal policy. Moreover, the average scores and learning efficiency of reward noise are also higher than action noise through the whole training, which indicates that the reward noise can generate more stable and consistent performance.",
        "keywords": []
      },
      "file_name": "859172d8cb31ddebd9b18e3a7a7d982fab1d1341.pdf"
    },
    {
      "success": true,
      "doc_id": "329b7ab665ec7fd75cbcd256f17c82ab",
      "summary": "At present, task planning is mainly solved by rules or operational research methods. The time complexity and space complexity of these methods increase exponentially with the growth of problem size. Thus, it is challenging to solve large-scale problems. Moreover, it is helpless to solve dynamic task planning problems. Reinforcement learning (RL) is often used to solve the dynamic planning problem of continuous decision making. RL agent constantly interacts with the environment to achieve the expected goal. However, the RL method meets challenges when handling the problem with ample state space. More sampling and exploration are needed to update the strategy gradient, and the convergence speed is slow. Humans ensure a quick start of learning by using prior knowledge, which reduces the exploration time of problems. Thus, we review the methods which combine human prior knowledge with RL through temporal node classification of human prior knowledge combined with RL. In this way, agents effectively reduce the sampling and exploration of the environment and eventually get the optimal strategy faster. Finally, we propose the development direction.",
      "intriguing_abstract": "At present, task planning is mainly solved by rules or operational research methods. The time complexity and space complexity of these methods increase exponentially with the growth of problem size. Thus, it is challenging to solve large-scale problems. Moreover, it is helpless to solve dynamic task planning problems. Reinforcement learning (RL) is often used to solve the dynamic planning problem of continuous decision making. RL agent constantly interacts with the environment to achieve the expected goal. However, the RL method meets challenges when handling the problem with ample state space. More sampling and exploration are needed to update the strategy gradient, and the convergence speed is slow. Humans ensure a quick start of learning by using prior knowledge, which reduces the exploration time of problems. Thus, we review the methods which combine human prior knowledge with RL through temporal node classification of human prior knowledge combined with RL. In this way, agents effectively reduce the sampling and exploration of the environment and eventually get the optimal strategy faster. Finally, we propose the development direction.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/f58ada4680d374f5dba96b5e7fcb5b7be11a6acc.pdf",
      "citation_key": "guo2022y6b",
      "metadata": {
        "title": "Survey of Reinforcement Learning based on Human Prior Knowledge",
        "authors": [
          "Zijing Guo",
          "Chendie Yao",
          "Yanghe Feng",
          "Yue Xu"
        ],
        "published_date": "2022",
        "abstract": "At present, task planning is mainly solved by rules or operational research methods. The time complexity and space complexity of these methods increase exponentially with the growth of problem size. Thus, it is challenging to solve large-scale problems. Moreover, it is helpless to solve dynamic task planning problems. Reinforcement learning (RL) is often used to solve the dynamic planning problem of continuous decision making. RL agent constantly interacts with the environment to achieve the expected goal. However, the RL method meets challenges when handling the problem with ample state space. More sampling and exploration are needed to update the strategy gradient, and the convergence speed is slow. Humans ensure a quick start of learning by using prior knowledge, which reduces the exploration time of problems. Thus, we review the methods which combine human prior knowledge with RL through temporal node classification of human prior knowledge combined with RL. In this way, agents effectively reduce the sampling and exploration of the environment and eventually get the optimal strategy faster. Finally, we propose the development direction.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/f58ada4680d374f5dba96b5e7fcb5b7be11a6acc.pdf",
        "venue": "Journal of Uncertain Systems",
        "citationCount": 5,
        "score": 1.6666666666666665,
        "summary": "At present, task planning is mainly solved by rules or operational research methods. The time complexity and space complexity of these methods increase exponentially with the growth of problem size. Thus, it is challenging to solve large-scale problems. Moreover, it is helpless to solve dynamic task planning problems. Reinforcement learning (RL) is often used to solve the dynamic planning problem of continuous decision making. RL agent constantly interacts with the environment to achieve the expected goal. However, the RL method meets challenges when handling the problem with ample state space. More sampling and exploration are needed to update the strategy gradient, and the convergence speed is slow. Humans ensure a quick start of learning by using prior knowledge, which reduces the exploration time of problems. Thus, we review the methods which combine human prior knowledge with RL through temporal node classification of human prior knowledge combined with RL. In this way, agents effectively reduce the sampling and exploration of the environment and eventually get the optimal strategy faster. Finally, we propose the development direction.",
        "keywords": []
      },
      "file_name": "f58ada4680d374f5dba96b5e7fcb5b7be11a6acc.pdf"
    },
    {
      "success": true,
      "doc_id": "47d4ad00e6d492fae92f653c40d07c4a",
      "summary": "This paper proposes a new method to drastically speed up deep reinforcement learning (deep RL) training for problems that have the property of state-action permissibility (SAP). Two types of permissibility are defined under SAP. The first type says that after an action $a_t$ is performed in a state $s_t$ and the agent has reached the new state $s_{t+1}$, the agent can decide whether $a_t$ is permissible or not permissible in $s_t$. The second type says that even without performing $a_t$ in $s_t$, the agent can already decide whether $a_t$ is permissible or not in $s_t$. An action is not permissible in a state if the action can never lead to an optimal solution and thus should not be tried (over and over again). We incorporate the proposed SAP property and encode action permissibility knowledge into two state-of-the-art deep RL algorithms to guide their state-action exploration together with a virtual stopping strategy. Results show that the SAP-based guidance can markedly speed up RL training.",
      "intriguing_abstract": "This paper proposes a new method to drastically speed up deep reinforcement learning (deep RL) training for problems that have the property of state-action permissibility (SAP). Two types of permissibility are defined under SAP. The first type says that after an action $a_t$ is performed in a state $s_t$ and the agent has reached the new state $s_{t+1}$, the agent can decide whether $a_t$ is permissible or not permissible in $s_t$. The second type says that even without performing $a_t$ in $s_t$, the agent can already decide whether $a_t$ is permissible or not in $s_t$. An action is not permissible in a state if the action can never lead to an optimal solution and thus should not be tried (over and over again). We incorporate the proposed SAP property and encode action permissibility knowledge into two state-of-the-art deep RL algorithms to guide their state-action exploration together with a virtual stopping strategy. Results show that the SAP-based guidance can markedly speed up RL training.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/aa65704a16138790678e2b9b59ae679b6c9353d7.pdf",
      "citation_key": "mazumder2022deb",
      "metadata": {
        "title": "Knowledge-Guided Exploration in Deep Reinforcement Learning",
        "authors": [
          "Sahisnu Mazumder",
          "Bing Liu",
          "Shuai Wang",
          "Yingxuan Zhu",
          "Xiaotian Yin",
          "Lifeng Liu",
          "Jian Li"
        ],
        "published_date": "2022",
        "abstract": "This paper proposes a new method to drastically speed up deep reinforcement learning (deep RL) training for problems that have the property of state-action permissibility (SAP). Two types of permissibility are defined under SAP. The first type says that after an action $a_t$ is performed in a state $s_t$ and the agent has reached the new state $s_{t+1}$, the agent can decide whether $a_t$ is permissible or not permissible in $s_t$. The second type says that even without performing $a_t$ in $s_t$, the agent can already decide whether $a_t$ is permissible or not in $s_t$. An action is not permissible in a state if the action can never lead to an optimal solution and thus should not be tried (over and over again). We incorporate the proposed SAP property and encode action permissibility knowledge into two state-of-the-art deep RL algorithms to guide their state-action exploration together with a virtual stopping strategy. Results show that the SAP-based guidance can markedly speed up RL training.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/aa65704a16138790678e2b9b59ae679b6c9353d7.pdf",
        "venue": "arXiv.org",
        "citationCount": 5,
        "score": 1.6666666666666665,
        "summary": "This paper proposes a new method to drastically speed up deep reinforcement learning (deep RL) training for problems that have the property of state-action permissibility (SAP). Two types of permissibility are defined under SAP. The first type says that after an action $a_t$ is performed in a state $s_t$ and the agent has reached the new state $s_{t+1}$, the agent can decide whether $a_t$ is permissible or not permissible in $s_t$. The second type says that even without performing $a_t$ in $s_t$, the agent can already decide whether $a_t$ is permissible or not in $s_t$. An action is not permissible in a state if the action can never lead to an optimal solution and thus should not be tried (over and over again). We incorporate the proposed SAP property and encode action permissibility knowledge into two state-of-the-art deep RL algorithms to guide their state-action exploration together with a virtual stopping strategy. Results show that the SAP-based guidance can markedly speed up RL training.",
        "keywords": []
      },
      "file_name": "aa65704a16138790678e2b9b59ae679b6c9353d7.pdf"
    },
    {
      "success": true,
      "doc_id": "53fe6e7264e99637eb94aea4e4326d8f",
      "summary": "Distributional reinforcement learning demonstrates state-of-the-art performance in continuous and discrete control settings with the features of variance and risk, which can be used to explore. However, the exploration method employing the risk property is hard to find, although numerous exploration methods in Distributional RL employ the variance of return distribution per action. In this paper, we present risk scheduling approaches that explore risk levels and optimistic behaviors from a risk perspective. We demonstrate the performance enhancement of the DMIX algorithm using risk scheduling in a multi-agent setting with comprehensive experiments.",
      "intriguing_abstract": "Distributional reinforcement learning demonstrates state-of-the-art performance in continuous and discrete control settings with the features of variance and risk, which can be used to explore. However, the exploration method employing the risk property is hard to find, although numerous exploration methods in Distributional RL employ the variance of return distribution per action. In this paper, we present risk scheduling approaches that explore risk levels and optimistic behaviors from a risk perspective. We demonstrate the performance enhancement of the DMIX algorithm using risk scheduling in a multi-agent setting with comprehensive experiments.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/bd8aaab29fa16f40ef016393ba7ca30127abab58.pdf",
      "citation_key": "oh2022cei",
      "metadata": {
        "title": "Risk Perspective Exploration in Distributional Reinforcement Learning",
        "authors": [
          "Ji-Yun Oh",
          "Joonkee Kim",
          "Se-Young Yun"
        ],
        "published_date": "2022",
        "abstract": "Distributional reinforcement learning demonstrates state-of-the-art performance in continuous and discrete control settings with the features of variance and risk, which can be used to explore. However, the exploration method employing the risk property is hard to find, although numerous exploration methods in Distributional RL employ the variance of return distribution per action. In this paper, we present risk scheduling approaches that explore risk levels and optimistic behaviors from a risk perspective. We demonstrate the performance enhancement of the DMIX algorithm using risk scheduling in a multi-agent setting with comprehensive experiments.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/bd8aaab29fa16f40ef016393ba7ca30127abab58.pdf",
        "venue": "arXiv.org",
        "citationCount": 5,
        "score": 1.6666666666666665,
        "summary": "Distributional reinforcement learning demonstrates state-of-the-art performance in continuous and discrete control settings with the features of variance and risk, which can be used to explore. However, the exploration method employing the risk property is hard to find, although numerous exploration methods in Distributional RL employ the variance of return distribution per action. In this paper, we present risk scheduling approaches that explore risk levels and optimistic behaviors from a risk perspective. We demonstrate the performance enhancement of the DMIX algorithm using risk scheduling in a multi-agent setting with comprehensive experiments.",
        "keywords": []
      },
      "file_name": "bd8aaab29fa16f40ef016393ba7ca30127abab58.pdf"
    },
    {
      "success": true,
      "doc_id": "776aa4c8e3fb7870187bce330b25fbc4",
      "summary": "Learning from demonstration provides ways to transfer knowledge and skills from humans to robots. Models based solely on learning from demonstration often have very good generalization capabilities but are not completely accurate when adapting to new scenarios. This happens especially when learning stochastic tasks because of the correspondence problem and unmodeled physical properties of tasks. On the other hand, reinforcement learning (RL) methods such as policy search have the capability to refine an initial skill through exploration, where the learning process is often very dependent on the initialization strategy and is efficient in finding only local solutions. These two approaches are, therefore, frequently combined. In this paper, we present how the iterative learning of tasks can be accelerated by a learning from demonstration (LfD) method based on the extraction of via-points. The paper provides an evaluation of the approach on two different primitive motion tasks.",
      "intriguing_abstract": "Learning from demonstration provides ways to transfer knowledge and skills from humans to robots. Models based solely on learning from demonstration often have very good generalization capabilities but are not completely accurate when adapting to new scenarios. This happens especially when learning stochastic tasks because of the correspondence problem and unmodeled physical properties of tasks. On the other hand, reinforcement learning (RL) methods such as policy search have the capability to refine an initial skill through exploration, where the learning process is often very dependent on the initialization strategy and is efficient in finding only local solutions. These two approaches are, therefore, frequently combined. In this paper, we present how the iterative learning of tasks can be accelerated by a learning from demonstration (LfD) method based on the extraction of via-points. The paper provides an evaluation of the approach on two different primitive motion tasks.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/a6c5e49425ac01c534d9663a4453b28f6dc76f7c.pdf",
      "citation_key": "vidakovi2020q23",
      "metadata": {
        "title": "Accelerating Robot Trajectory Learning for Stochastic Tasks",
        "authors": [
          "J. Vidakovi",
          "B. Jerbi",
          "B. ekoranja",
          "M. vaco",
          "F. uligoj"
        ],
        "published_date": "2020",
        "abstract": "Learning from demonstration provides ways to transfer knowledge and skills from humans to robots. Models based solely on learning from demonstration often have very good generalization capabilities but are not completely accurate when adapting to new scenarios. This happens especially when learning stochastic tasks because of the correspondence problem and unmodeled physical properties of tasks. On the other hand, reinforcement learning (RL) methods such as policy search have the capability to refine an initial skill through exploration, where the learning process is often very dependent on the initialization strategy and is efficient in finding only local solutions. These two approaches are, therefore, frequently combined. In this paper, we present how the iterative learning of tasks can be accelerated by a learning from demonstration (LfD) method based on the extraction of via-points. The paper provides an evaluation of the approach on two different primitive motion tasks.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/a6c5e49425ac01c534d9663a4453b28f6dc76f7c.pdf",
        "venue": "IEEE Access",
        "citationCount": 6,
        "score": 1.2000000000000002,
        "summary": "Learning from demonstration provides ways to transfer knowledge and skills from humans to robots. Models based solely on learning from demonstration often have very good generalization capabilities but are not completely accurate when adapting to new scenarios. This happens especially when learning stochastic tasks because of the correspondence problem and unmodeled physical properties of tasks. On the other hand, reinforcement learning (RL) methods such as policy search have the capability to refine an initial skill through exploration, where the learning process is often very dependent on the initialization strategy and is efficient in finding only local solutions. These two approaches are, therefore, frequently combined. In this paper, we present how the iterative learning of tasks can be accelerated by a learning from demonstration (LfD) method based on the extraction of via-points. The paper provides an evaluation of the approach on two different primitive motion tasks.",
        "keywords": []
      },
      "file_name": "a6c5e49425ac01c534d9663a4453b28f6dc76f7c.pdf"
    },
    {
      "success": true,
      "doc_id": "be313f64b585276443e673d0f011220c",
      "summary": "This paper proposes a human-in-the-loop deep reinforcement learning (HL-DRL)-based VVC strategy to simultaneously reduce power losses, mitigate voltage violations and compensate for voltage unbalance in three-phase unbalanced distribution networks. Instead of fully trusting DRL actions made by deep neural networks, a human intervention module is proposed to modify dangerous actions that violate operation constraints during offline training. This module refers to well-designed human guidance rules based on voltage-reactive power sensitivities, which regulate PV reactive power to sequentially address local voltage violation and unbalance issues to obtain safe transitions. To efficiently and safely learn the optimal control policy from these training samples, a human-in-the-loop soft actor-critic (HL-SAC) solution method is then developed. Different from the standard SAC algorithm, an online switch mechanism between action exploration and human intervention is designed. The actor network loss function is modified to incorporate human guidance terms, which alleviates the inconsistency of the updating direction of actor and critic networks. A hybrid experience replay buffer including both dangerous and safe transitions is also used to facilitate the learning process towards human actions. Comparative simulation results on a modified IEEE 123-bus unbalanced distribution system demonstrate the effectiveness and superiority of the proposed method in voltage control.",
      "intriguing_abstract": "This paper proposes a human-in-the-loop deep reinforcement learning (HL-DRL)-based VVC strategy to simultaneously reduce power losses, mitigate voltage violations and compensate for voltage unbalance in three-phase unbalanced distribution networks. Instead of fully trusting DRL actions made by deep neural networks, a human intervention module is proposed to modify dangerous actions that violate operation constraints during offline training. This module refers to well-designed human guidance rules based on voltage-reactive power sensitivities, which regulate PV reactive power to sequentially address local voltage violation and unbalance issues to obtain safe transitions. To efficiently and safely learn the optimal control policy from these training samples, a human-in-the-loop soft actor-critic (HL-SAC) solution method is then developed. Different from the standard SAC algorithm, an online switch mechanism between action exploration and human intervention is designed. The actor network loss function is modified to incorporate human guidance terms, which alleviates the inconsistency of the updating direction of actor and critic networks. A hybrid experience replay buffer including both dangerous and safe transitions is also used to facilitate the learning process towards human actions. Comparative simulation results on a modified IEEE 123-bus unbalanced distribution system demonstrate the effectiveness and superiority of the proposed method in voltage control.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/d8bc75fbcfdecd5cd1952524d3e07db13722f5ff.pdf",
      "citation_key": "sun2024kxq",
      "metadata": {
        "title": "Optimal Volt/Var Control for Unbalanced Distribution Networks With Human-in-the-Loop Deep Reinforcement Learning",
        "authors": [
          "Xianzhuo Sun",
          "Zhao Xu",
          "Jing Qiu",
          "Huichuan Liu",
          "Huayi Wu",
          "Yuechuan Tao"
        ],
        "published_date": "2024",
        "abstract": "This paper proposes a human-in-the-loop deep reinforcement learning (HL-DRL)-based VVC strategy to simultaneously reduce power losses, mitigate voltage violations and compensate for voltage unbalance in three-phase unbalanced distribution networks. Instead of fully trusting DRL actions made by deep neural networks, a human intervention module is proposed to modify dangerous actions that violate operation constraints during offline training. This module refers to well-designed human guidance rules based on voltage-reactive power sensitivities, which regulate PV reactive power to sequentially address local voltage violation and unbalance issues to obtain safe transitions. To efficiently and safely learn the optimal control policy from these training samples, a human-in-the-loop soft actor-critic (HL-SAC) solution method is then developed. Different from the standard SAC algorithm, an online switch mechanism between action exploration and human intervention is designed. The actor network loss function is modified to incorporate human guidance terms, which alleviates the inconsistency of the updating direction of actor and critic networks. A hybrid experience replay buffer including both dangerous and safe transitions is also used to facilitate the learning process towards human actions. Comparative simulation results on a modified IEEE 123-bus unbalanced distribution system demonstrate the effectiveness and superiority of the proposed method in voltage control.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/d8bc75fbcfdecd5cd1952524d3e07db13722f5ff.pdf",
        "venue": "IEEE Transactions on Smart Grid",
        "citationCount": 21,
        "score": 21.0,
        "summary": "This paper proposes a human-in-the-loop deep reinforcement learning (HL-DRL)-based VVC strategy to simultaneously reduce power losses, mitigate voltage violations and compensate for voltage unbalance in three-phase unbalanced distribution networks. Instead of fully trusting DRL actions made by deep neural networks, a human intervention module is proposed to modify dangerous actions that violate operation constraints during offline training. This module refers to well-designed human guidance rules based on voltage-reactive power sensitivities, which regulate PV reactive power to sequentially address local voltage violation and unbalance issues to obtain safe transitions. To efficiently and safely learn the optimal control policy from these training samples, a human-in-the-loop soft actor-critic (HL-SAC) solution method is then developed. Different from the standard SAC algorithm, an online switch mechanism between action exploration and human intervention is designed. The actor network loss function is modified to incorporate human guidance terms, which alleviates the inconsistency of the updating direction of actor and critic networks. A hybrid experience replay buffer including both dangerous and safe transitions is also used to facilitate the learning process towards human actions. Comparative simulation results on a modified IEEE 123-bus unbalanced distribution system demonstrate the effectiveness and superiority of the proposed method in voltage control.",
        "keywords": []
      },
      "file_name": "d8bc75fbcfdecd5cd1952524d3e07db13722f5ff.pdf"
    },
    {
      "success": true,
      "doc_id": "34f4604de8c6b7a1b114d9d8522e4ac5",
      "summary": "The large-scale integration of intermittent renewable energy resources introduces increased uncertainty and volatility to the supply side of power systems, thereby complicating system operation and control. Recently, data-driven approaches, particularly reinforcement learning (RL), have shown significant promise in addressing complex control challenges in power systems, because RL can learn from interactive feedback without needing prior knowledge of the system model. However, the training process of model-free RL methods relies heavily on random decisions for exploration, which may result in ``bad\"decisions that violate critical safety constraints and lead to catastrophic control outcomes. Due to the inability of RL methods to theoretically ensure decision safety in power systems, directly deploying traditional RL algorithms in the real world is deemed unacceptable. Consequently, the safety issue in RL applications, known as safe RL, has garnered considerable attention in recent years, leading to numerous important developments. This paper provides a comprehensive review of the state-of-the-art safe RL techniques and discusses how these techniques can be applied to power system control problems such as frequency regulation, voltage control, and energy management. We then present discussions on key challenges and future research directions, related to convergence and optimality, training efficiency, universality, and real-world deployment.",
      "intriguing_abstract": "The large-scale integration of intermittent renewable energy resources introduces increased uncertainty and volatility to the supply side of power systems, thereby complicating system operation and control. Recently, data-driven approaches, particularly reinforcement learning (RL), have shown significant promise in addressing complex control challenges in power systems, because RL can learn from interactive feedback without needing prior knowledge of the system model. However, the training process of model-free RL methods relies heavily on random decisions for exploration, which may result in ``bad\"decisions that violate critical safety constraints and lead to catastrophic control outcomes. Due to the inability of RL methods to theoretically ensure decision safety in power systems, directly deploying traditional RL algorithms in the real world is deemed unacceptable. Consequently, the safety issue in RL applications, known as safe RL, has garnered considerable attention in recent years, leading to numerous important developments. This paper provides a comprehensive review of the state-of-the-art safe RL techniques and discusses how these techniques can be applied to power system control problems such as frequency regulation, voltage control, and energy management. We then present discussions on key challenges and future research directions, related to convergence and optimality, training efficiency, universality, and real-world deployment.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/17682d6f13dcac703092e0f1500a4197e46bbf2c.pdf",
      "citation_key": "yu2024x53",
      "metadata": {
        "title": "Safe Reinforcement Learning for Power System Control: A Review",
        "authors": [
          "Peipei Yu",
          "Zhen-yu Wang",
          "Hongcai Zhang",
          "Yonghua Song"
        ],
        "published_date": "2024",
        "abstract": "The large-scale integration of intermittent renewable energy resources introduces increased uncertainty and volatility to the supply side of power systems, thereby complicating system operation and control. Recently, data-driven approaches, particularly reinforcement learning (RL), have shown significant promise in addressing complex control challenges in power systems, because RL can learn from interactive feedback without needing prior knowledge of the system model. However, the training process of model-free RL methods relies heavily on random decisions for exploration, which may result in ``bad\"decisions that violate critical safety constraints and lead to catastrophic control outcomes. Due to the inability of RL methods to theoretically ensure decision safety in power systems, directly deploying traditional RL algorithms in the real world is deemed unacceptable. Consequently, the safety issue in RL applications, known as safe RL, has garnered considerable attention in recent years, leading to numerous important developments. This paper provides a comprehensive review of the state-of-the-art safe RL techniques and discusses how these techniques can be applied to power system control problems such as frequency regulation, voltage control, and energy management. We then present discussions on key challenges and future research directions, related to convergence and optimality, training efficiency, universality, and real-world deployment.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/17682d6f13dcac703092e0f1500a4197e46bbf2c.pdf",
        "venue": "arXiv.org",
        "citationCount": 8,
        "score": 8.0,
        "summary": "The large-scale integration of intermittent renewable energy resources introduces increased uncertainty and volatility to the supply side of power systems, thereby complicating system operation and control. Recently, data-driven approaches, particularly reinforcement learning (RL), have shown significant promise in addressing complex control challenges in power systems, because RL can learn from interactive feedback without needing prior knowledge of the system model. However, the training process of model-free RL methods relies heavily on random decisions for exploration, which may result in ``bad\"decisions that violate critical safety constraints and lead to catastrophic control outcomes. Due to the inability of RL methods to theoretically ensure decision safety in power systems, directly deploying traditional RL algorithms in the real world is deemed unacceptable. Consequently, the safety issue in RL applications, known as safe RL, has garnered considerable attention in recent years, leading to numerous important developments. This paper provides a comprehensive review of the state-of-the-art safe RL techniques and discusses how these techniques can be applied to power system control problems such as frequency regulation, voltage control, and energy management. We then present discussions on key challenges and future research directions, related to convergence and optimality, training efficiency, universality, and real-world deployment.",
        "keywords": []
      },
      "file_name": "17682d6f13dcac703092e0f1500a4197e46bbf2c.pdf"
    },
    {
      "success": true,
      "doc_id": "73291de6c6457e2ec5aa12d310dac12f",
      "summary": "The existing scheduling methods of wafer fabs focus on single area, achieving local optimisation while failing to realise global optimisation due to neglecting the coordination of multi-area. Therefore, it is necessary to consider the complex opposing relationships between multi-area caused by constraints such as batch processing, re-entrance, and multiple residency times within and between areas to conduct integrated scheduling and shorten the production cycle time. For this issue, this paper proposes a cooperative multi-agent reinforcement learning for multi-area integrated scheduling. Aiming at the dynamic batching and scheduling considering the dynamic arrival lots in multi-area, a multi-agent reinforcement learning algorithm is presented to learn the optimal dynamic batching and scheduling policy firstly. Subsequently, a cooperative multi-agent framework is raised to achieve the global optimisation and coordination of multi-area. Furthermore, an adaptive exploration strategy is constructed to enhance the global exploration capability of the complex solution space caused by residency time constraints and re-entrant property. Moreover, a policy share enhanced Double DQN is employed to improve the generalisation and adaptability of the multi-agent. Finally, the experiments demonstrate that the proposed integrated scheduling method has better comprehensive performance compared to the previous area-separated scheduling methods.",
      "intriguing_abstract": "The existing scheduling methods of wafer fabs focus on single area, achieving local optimisation while failing to realise global optimisation due to neglecting the coordination of multi-area. Therefore, it is necessary to consider the complex opposing relationships between multi-area caused by constraints such as batch processing, re-entrance, and multiple residency times within and between areas to conduct integrated scheduling and shorten the production cycle time. For this issue, this paper proposes a cooperative multi-agent reinforcement learning for multi-area integrated scheduling. Aiming at the dynamic batching and scheduling considering the dynamic arrival lots in multi-area, a multi-agent reinforcement learning algorithm is presented to learn the optimal dynamic batching and scheduling policy firstly. Subsequently, a cooperative multi-agent framework is raised to achieve the global optimisation and coordination of multi-area. Furthermore, an adaptive exploration strategy is constructed to enhance the global exploration capability of the complex solution space caused by residency time constraints and re-entrant property. Moreover, a policy share enhanced Double DQN is employed to improve the generalisation and adaptability of the multi-agent. Finally, the experiments demonstrate that the proposed integrated scheduling method has better comprehensive performance compared to the previous area-separated scheduling methods.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/11c34b84c3ad6587529517c32923c446797c63e6.pdf",
      "citation_key": "wang2024htz",
      "metadata": {
        "title": "Cooperative multi-agent reinforcement learning for multi-area integrated scheduling in wafer fabs",
        "authors": [
          "Ming Wang",
          "Jie Zhang",
          "Peng Zhang",
          "Mengyu Jin"
        ],
        "published_date": "2024",
        "abstract": "The existing scheduling methods of wafer fabs focus on single area, achieving local optimisation while failing to realise global optimisation due to neglecting the coordination of multi-area. Therefore, it is necessary to consider the complex opposing relationships between multi-area caused by constraints such as batch processing, re-entrance, and multiple residency times within and between areas to conduct integrated scheduling and shorten the production cycle time. For this issue, this paper proposes a cooperative multi-agent reinforcement learning for multi-area integrated scheduling. Aiming at the dynamic batching and scheduling considering the dynamic arrival lots in multi-area, a multi-agent reinforcement learning algorithm is presented to learn the optimal dynamic batching and scheduling policy firstly. Subsequently, a cooperative multi-agent framework is raised to achieve the global optimisation and coordination of multi-area. Furthermore, an adaptive exploration strategy is constructed to enhance the global exploration capability of the complex solution space caused by residency time constraints and re-entrant property. Moreover, a policy share enhanced Double DQN is employed to improve the generalisation and adaptability of the multi-agent. Finally, the experiments demonstrate that the proposed integrated scheduling method has better comprehensive performance compared to the previous area-separated scheduling methods.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/11c34b84c3ad6587529517c32923c446797c63e6.pdf",
        "venue": "International Journal of Production Research",
        "citationCount": 7,
        "score": 7.0,
        "summary": "The existing scheduling methods of wafer fabs focus on single area, achieving local optimisation while failing to realise global optimisation due to neglecting the coordination of multi-area. Therefore, it is necessary to consider the complex opposing relationships between multi-area caused by constraints such as batch processing, re-entrance, and multiple residency times within and between areas to conduct integrated scheduling and shorten the production cycle time. For this issue, this paper proposes a cooperative multi-agent reinforcement learning for multi-area integrated scheduling. Aiming at the dynamic batching and scheduling considering the dynamic arrival lots in multi-area, a multi-agent reinforcement learning algorithm is presented to learn the optimal dynamic batching and scheduling policy firstly. Subsequently, a cooperative multi-agent framework is raised to achieve the global optimisation and coordination of multi-area. Furthermore, an adaptive exploration strategy is constructed to enhance the global exploration capability of the complex solution space caused by residency time constraints and re-entrant property. Moreover, a policy share enhanced Double DQN is employed to improve the generalisation and adaptability of the multi-agent. Finally, the experiments demonstrate that the proposed integrated scheduling method has better comprehensive performance compared to the previous area-separated scheduling methods.",
        "keywords": []
      },
      "file_name": "11c34b84c3ad6587529517c32923c446797c63e6.pdf"
    },
    {
      "success": true,
      "doc_id": "8bd09fb15c521e322c7620e08d3448d4",
      "summary": "To overcome the problems of traditional distributed target allocation algorithms in terms of lack of target strategic priority, poor scalability, and robustness, this paper proposes a proximal strategy optimization algorithm that combines threat assessment and attention mechanism (TAPPO). Based on the distributed training framework, the algorithm integrates a threat assessment and dynamic attention strategy and designs a dynamic reward function based on the current hit rate of the drone and the missile benefit ratio to improve the algorithms exploration ability and scalability. Through an 8vs8 multi-UAV confrontation experiment in a digital twin simulation environment, the results show that the agent using the TAPPO algorithm for target allocation defeats the state machine with an 85% winning rate and is significantly better than other current mainstream target allocation algorithms, verifying the effectiveness of the algorithm.",
      "intriguing_abstract": "To overcome the problems of traditional distributed target allocation algorithms in terms of lack of target strategic priority, poor scalability, and robustness, this paper proposes a proximal strategy optimization algorithm that combines threat assessment and attention mechanism (TAPPO). Based on the distributed training framework, the algorithm integrates a threat assessment and dynamic attention strategy and designs a dynamic reward function based on the current hit rate of the drone and the missile benefit ratio to improve the algorithms exploration ability and scalability. Through an 8vs8 multi-UAV confrontation experiment in a digital twin simulation environment, the results show that the agent using the TAPPO algorithm for target allocation defeats the state machine with an 85% winning rate and is significantly better than other current mainstream target allocation algorithms, verifying the effectiveness of the algorithm.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/d929e0bc4e30910527a714f239b5e5b37a7ec6ad.pdf",
      "citation_key": "ding20246hx",
      "metadata": {
        "title": "Multi-UAV Cooperative Target Assignment Method Based on Reinforcement Learning",
        "authors": [
          "Yunlong Ding",
          "Minchi Kuang",
          "Heng Shi",
          "Jiazhan Gao"
        ],
        "published_date": "2024",
        "abstract": "To overcome the problems of traditional distributed target allocation algorithms in terms of lack of target strategic priority, poor scalability, and robustness, this paper proposes a proximal strategy optimization algorithm that combines threat assessment and attention mechanism (TAPPO). Based on the distributed training framework, the algorithm integrates a threat assessment and dynamic attention strategy and designs a dynamic reward function based on the current hit rate of the drone and the missile benefit ratio to improve the algorithms exploration ability and scalability. Through an 8vs8 multi-UAV confrontation experiment in a digital twin simulation environment, the results show that the agent using the TAPPO algorithm for target allocation defeats the state machine with an 85% winning rate and is significantly better than other current mainstream target allocation algorithms, verifying the effectiveness of the algorithm.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/d929e0bc4e30910527a714f239b5e5b37a7ec6ad.pdf",
        "venue": "Drones",
        "citationCount": 6,
        "score": 6.0,
        "summary": "To overcome the problems of traditional distributed target allocation algorithms in terms of lack of target strategic priority, poor scalability, and robustness, this paper proposes a proximal strategy optimization algorithm that combines threat assessment and attention mechanism (TAPPO). Based on the distributed training framework, the algorithm integrates a threat assessment and dynamic attention strategy and designs a dynamic reward function based on the current hit rate of the drone and the missile benefit ratio to improve the algorithms exploration ability and scalability. Through an 8vs8 multi-UAV confrontation experiment in a digital twin simulation environment, the results show that the agent using the TAPPO algorithm for target allocation defeats the state machine with an 85% winning rate and is significantly better than other current mainstream target allocation algorithms, verifying the effectiveness of the algorithm.",
        "keywords": []
      },
      "file_name": "d929e0bc4e30910527a714f239b5e5b37a7ec6ad.pdf"
    },
    {
      "success": true,
      "doc_id": "1f6c5b7dc7c3f4bbba308a9ba4a0aef4",
      "summary": "Uncertain emergency events are inevitable and occur unpredictably on the highway. Emergencies with lane capacity drops cause local congestion and can even cause a second accident if the response is not timely. To address this problem, a self-triggered variable speed limit (VSL) intelligent decision-making control strategy based on the improved deep deterministic policy gradient (DDPG) algorithm is proposed, which can eliminate or alleviate congestion in a timely manner. The action noise parameter is introduced to improve exploration efficiency and stability in the early stage of the algorithm training and then maximizes differential traffic flow as the control objective, taking the real-time traffic state as the input. The reward function is constructed to explore the values of the speed limit. The results show that in terms of safety, under different traffic flow levels, the proposed strategy has improved by over 28.30% compared to other methods. In terms of efficiency, except for being inferior to the no-control condition during low-traffic-flow conditions, our strategy has improved over 7.21% compared to the others. The proposed strategy greatly benefits traffic sustainability in Intelligent Transport Systems (ITSs).",
      "intriguing_abstract": "Uncertain emergency events are inevitable and occur unpredictably on the highway. Emergencies with lane capacity drops cause local congestion and can even cause a second accident if the response is not timely. To address this problem, a self-triggered variable speed limit (VSL) intelligent decision-making control strategy based on the improved deep deterministic policy gradient (DDPG) algorithm is proposed, which can eliminate or alleviate congestion in a timely manner. The action noise parameter is introduced to improve exploration efficiency and stability in the early stage of the algorithm training and then maximizes differential traffic flow as the control objective, taking the real-time traffic state as the input. The reward function is constructed to explore the values of the speed limit. The results show that in terms of safety, under different traffic flow levels, the proposed strategy has improved by over 28.30% compared to other methods. In terms of efficiency, except for being inferior to the no-control condition during low-traffic-flow conditions, our strategy has improved over 7.21% compared to the others. The proposed strategy greatly benefits traffic sustainability in Intelligent Transport Systems (ITSs).",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/516c6ab3feab17bc158f12ef6768b26c603566b8.pdf",
      "citation_key": "yang2024yh9",
      "metadata": {
        "title": "Variable Speed Limit Intelligent Decision-Making Control Strategy Based on Deep Reinforcement Learning under Emergencies",
        "authors": [
          "Jingwen Yang",
          "Ping Wang",
          "Yongfeng Ju"
        ],
        "published_date": "2024",
        "abstract": "Uncertain emergency events are inevitable and occur unpredictably on the highway. Emergencies with lane capacity drops cause local congestion and can even cause a second accident if the response is not timely. To address this problem, a self-triggered variable speed limit (VSL) intelligent decision-making control strategy based on the improved deep deterministic policy gradient (DDPG) algorithm is proposed, which can eliminate or alleviate congestion in a timely manner. The action noise parameter is introduced to improve exploration efficiency and stability in the early stage of the algorithm training and then maximizes differential traffic flow as the control objective, taking the real-time traffic state as the input. The reward function is constructed to explore the values of the speed limit. The results show that in terms of safety, under different traffic flow levels, the proposed strategy has improved by over 28.30% compared to other methods. In terms of efficiency, except for being inferior to the no-control condition during low-traffic-flow conditions, our strategy has improved over 7.21% compared to the others. The proposed strategy greatly benefits traffic sustainability in Intelligent Transport Systems (ITSs).",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/516c6ab3feab17bc158f12ef6768b26c603566b8.pdf",
        "venue": "Sustainability",
        "citationCount": 5,
        "score": 5.0,
        "summary": "Uncertain emergency events are inevitable and occur unpredictably on the highway. Emergencies with lane capacity drops cause local congestion and can even cause a second accident if the response is not timely. To address this problem, a self-triggered variable speed limit (VSL) intelligent decision-making control strategy based on the improved deep deterministic policy gradient (DDPG) algorithm is proposed, which can eliminate or alleviate congestion in a timely manner. The action noise parameter is introduced to improve exploration efficiency and stability in the early stage of the algorithm training and then maximizes differential traffic flow as the control objective, taking the real-time traffic state as the input. The reward function is constructed to explore the values of the speed limit. The results show that in terms of safety, under different traffic flow levels, the proposed strategy has improved by over 28.30% compared to other methods. In terms of efficiency, except for being inferior to the no-control condition during low-traffic-flow conditions, our strategy has improved over 7.21% compared to the others. The proposed strategy greatly benefits traffic sustainability in Intelligent Transport Systems (ITSs).",
        "keywords": []
      },
      "file_name": "516c6ab3feab17bc158f12ef6768b26c603566b8.pdf"
    },
    {
      "success": true,
      "doc_id": "276cffd7a87aba767b7c99a97cfc13e6",
      "summary": "This research addresses the pressing need for enhanced energy management in smart homes, motivated by the inefficiencies of current methods in balancing power usage optimization with user comfort. By integrating reinforcement learning and a unique columnandconstraint generation strategy, the study aims to fill this gap and offer a comprehensive solution. Furthermore, the increasing adoption of renewable energy sources like solar panels underscores the importance of developing advanced energy management techniques, driving the exploration of innovative approaches such as the one proposed herein. The constraint coordination game (CCG) method is designed to efficiently manage the power usage of each appliance, including the charging and discharging of the energy storage system. Additionally, a deep learning model, specifically a deep neural network, is employed to forecast indoor temperatures, which significantly influence the energy demands of the air conditioning system. The synergistic combination of the CCG method with deep learningbased indoor temperature forecasting promises significant reductions in homeowner energy expenses while maintaining optimal appliance performance and user satisfaction. Testing conducted in simulated environments demonstrates promising results, showcasing a 12% reduction in energy costs compared to conventional energy management strategies.",
      "intriguing_abstract": "This research addresses the pressing need for enhanced energy management in smart homes, motivated by the inefficiencies of current methods in balancing power usage optimization with user comfort. By integrating reinforcement learning and a unique columnandconstraint generation strategy, the study aims to fill this gap and offer a comprehensive solution. Furthermore, the increasing adoption of renewable energy sources like solar panels underscores the importance of developing advanced energy management techniques, driving the exploration of innovative approaches such as the one proposed herein. The constraint coordination game (CCG) method is designed to efficiently manage the power usage of each appliance, including the charging and discharging of the energy storage system. Additionally, a deep learning model, specifically a deep neural network, is employed to forecast indoor temperatures, which significantly influence the energy demands of the air conditioning system. The synergistic combination of the CCG method with deep learningbased indoor temperature forecasting promises significant reductions in homeowner energy expenses while maintaining optimal appliance performance and user satisfaction. Testing conducted in simulated environments demonstrates promising results, showcasing a 12% reduction in energy costs compared to conventional energy management strategies.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/f5e09973834f852237a7d9db6583c7e6615a907d.pdf",
      "citation_key": "afroosheh2024id4",
      "metadata": {
        "title": "Reinforcement learning layoutbased optimal energy management in smart home: AIbased approach",
        "authors": [
          "Sajjad Afroosheh",
          "Khodakhast Esapour",
          "Reza KhorramNia",
          "Mazaher Karimi"
        ],
        "published_date": "2024",
        "abstract": "This research addresses the pressing need for enhanced energy management in smart homes, motivated by the inefficiencies of current methods in balancing power usage optimization with user comfort. By integrating reinforcement learning and a unique columnandconstraint generation strategy, the study aims to fill this gap and offer a comprehensive solution. Furthermore, the increasing adoption of renewable energy sources like solar panels underscores the importance of developing advanced energy management techniques, driving the exploration of innovative approaches such as the one proposed herein. The constraint coordination game (CCG) method is designed to efficiently manage the power usage of each appliance, including the charging and discharging of the energy storage system. Additionally, a deep learning model, specifically a deep neural network, is employed to forecast indoor temperatures, which significantly influence the energy demands of the air conditioning system. The synergistic combination of the CCG method with deep learningbased indoor temperature forecasting promises significant reductions in homeowner energy expenses while maintaining optimal appliance performance and user satisfaction. Testing conducted in simulated environments demonstrates promising results, showcasing a 12% reduction in energy costs compared to conventional energy management strategies.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/f5e09973834f852237a7d9db6583c7e6615a907d.pdf",
        "venue": "IET Generation, Transmission &amp; Distribution",
        "citationCount": 5,
        "score": 5.0,
        "summary": "This research addresses the pressing need for enhanced energy management in smart homes, motivated by the inefficiencies of current methods in balancing power usage optimization with user comfort. By integrating reinforcement learning and a unique columnandconstraint generation strategy, the study aims to fill this gap and offer a comprehensive solution. Furthermore, the increasing adoption of renewable energy sources like solar panels underscores the importance of developing advanced energy management techniques, driving the exploration of innovative approaches such as the one proposed herein. The constraint coordination game (CCG) method is designed to efficiently manage the power usage of each appliance, including the charging and discharging of the energy storage system. Additionally, a deep learning model, specifically a deep neural network, is employed to forecast indoor temperatures, which significantly influence the energy demands of the air conditioning system. The synergistic combination of the CCG method with deep learningbased indoor temperature forecasting promises significant reductions in homeowner energy expenses while maintaining optimal appliance performance and user satisfaction. Testing conducted in simulated environments demonstrates promising results, showcasing a 12% reduction in energy costs compared to conventional energy management strategies.",
        "keywords": []
      },
      "file_name": "f5e09973834f852237a7d9db6583c7e6615a907d.pdf"
    },
    {
      "success": true,
      "doc_id": "d5472dca6da9f78a6b50d03e557523c4",
      "summary": "Reinforcement Learning with Verifiable Reward (RLVR) has significantly advanced the complex reasoning abilities of Large Language Models (LLMs). However, it struggles to break through the inherent capability boundaries of the base LLM, due to its essentially on-policy strategy coupled with LLM's immense action space and sparse reward. Critically, RLVR can lead to the capability boundary collapse, narrowing the LLM's problem-solving scope. To address this problem, we propose RL-PLUS, a novel hybrid-policy optimization approach for LLMs that synergizes internal exploitation with external data to achieve stronger reasoning capabilities and surpass the boundaries of base models. RL-PLUS integrates two core components, i.e., Multiple Importance Sampling to address distributional mismatch from external data, and Exploration-Based Advantage Function to guide the model towards high-value, unexplored reasoning paths. We provide both theoretical analysis and extensive experiments to demonstrate the superiority and generalizability of our approach. Compared with existing RLVR methods, RL-PLUS achieves 1) state-of-the-art performance on six math reasoning benchmarks; 2) superior performance on six out-of-distribution reasoning tasks; 3) consistent and significant gains across diverse model families, with average relative improvements up to 69.2\\%. Moreover, the analysis of Pass@k curves indicates that RL-PLUS effectively resolves the capability boundary collapse problem.",
      "intriguing_abstract": "Reinforcement Learning with Verifiable Reward (RLVR) has significantly advanced the complex reasoning abilities of Large Language Models (LLMs). However, it struggles to break through the inherent capability boundaries of the base LLM, due to its essentially on-policy strategy coupled with LLM's immense action space and sparse reward. Critically, RLVR can lead to the capability boundary collapse, narrowing the LLM's problem-solving scope. To address this problem, we propose RL-PLUS, a novel hybrid-policy optimization approach for LLMs that synergizes internal exploitation with external data to achieve stronger reasoning capabilities and surpass the boundaries of base models. RL-PLUS integrates two core components, i.e., Multiple Importance Sampling to address distributional mismatch from external data, and Exploration-Based Advantage Function to guide the model towards high-value, unexplored reasoning paths. We provide both theoretical analysis and extensive experiments to demonstrate the superiority and generalizability of our approach. Compared with existing RLVR methods, RL-PLUS achieves 1) state-of-the-art performance on six math reasoning benchmarks; 2) superior performance on six out-of-distribution reasoning tasks; 3) consistent and significant gains across diverse model families, with average relative improvements up to 69.2\\%. Moreover, the analysis of Pass@k curves indicates that RL-PLUS effectively resolves the capability boundary collapse problem.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/b43f8bacd80f32734cdff4f9d8c79e397872aed6.pdf",
      "citation_key": "dong2025887",
      "metadata": {
        "title": "RL-PLUS: Countering Capability Boundary Collapse of LLMs in Reinforcement Learning with Hybrid-policy Optimization",
        "authors": [
          "Yihong Dong",
          "Xue Jiang",
          "Yongding Tao",
          "Huanyu Liu",
          "Kechi Zhang",
          "Lili Mou",
          "Rongyu Cao",
          "Yingwei Ma",
          "Jue Chen",
          "Binhua Li",
          "Zhi Jin",
          "Fei Huang",
          "Yongbin Li",
          "Ge Li"
        ],
        "published_date": "2025",
        "abstract": "Reinforcement Learning with Verifiable Reward (RLVR) has significantly advanced the complex reasoning abilities of Large Language Models (LLMs). However, it struggles to break through the inherent capability boundaries of the base LLM, due to its essentially on-policy strategy coupled with LLM's immense action space and sparse reward. Critically, RLVR can lead to the capability boundary collapse, narrowing the LLM's problem-solving scope. To address this problem, we propose RL-PLUS, a novel hybrid-policy optimization approach for LLMs that synergizes internal exploitation with external data to achieve stronger reasoning capabilities and surpass the boundaries of base models. RL-PLUS integrates two core components, i.e., Multiple Importance Sampling to address distributional mismatch from external data, and Exploration-Based Advantage Function to guide the model towards high-value, unexplored reasoning paths. We provide both theoretical analysis and extensive experiments to demonstrate the superiority and generalizability of our approach. Compared with existing RLVR methods, RL-PLUS achieves 1) state-of-the-art performance on six math reasoning benchmarks; 2) superior performance on six out-of-distribution reasoning tasks; 3) consistent and significant gains across diverse model families, with average relative improvements up to 69.2\\%. Moreover, the analysis of Pass@k curves indicates that RL-PLUS effectively resolves the capability boundary collapse problem.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/b43f8bacd80f32734cdff4f9d8c79e397872aed6.pdf",
        "venue": "arXiv.org",
        "citationCount": 4,
        "score": 4.0,
        "summary": "Reinforcement Learning with Verifiable Reward (RLVR) has significantly advanced the complex reasoning abilities of Large Language Models (LLMs). However, it struggles to break through the inherent capability boundaries of the base LLM, due to its essentially on-policy strategy coupled with LLM's immense action space and sparse reward. Critically, RLVR can lead to the capability boundary collapse, narrowing the LLM's problem-solving scope. To address this problem, we propose RL-PLUS, a novel hybrid-policy optimization approach for LLMs that synergizes internal exploitation with external data to achieve stronger reasoning capabilities and surpass the boundaries of base models. RL-PLUS integrates two core components, i.e., Multiple Importance Sampling to address distributional mismatch from external data, and Exploration-Based Advantage Function to guide the model towards high-value, unexplored reasoning paths. We provide both theoretical analysis and extensive experiments to demonstrate the superiority and generalizability of our approach. Compared with existing RLVR methods, RL-PLUS achieves 1) state-of-the-art performance on six math reasoning benchmarks; 2) superior performance on six out-of-distribution reasoning tasks; 3) consistent and significant gains across diverse model families, with average relative improvements up to 69.2\\%. Moreover, the analysis of Pass@k curves indicates that RL-PLUS effectively resolves the capability boundary collapse problem.",
        "keywords": []
      },
      "file_name": "b43f8bacd80f32734cdff4f9d8c79e397872aed6.pdf"
    },
    {
      "success": true,
      "doc_id": "f1e693a1fb32caef6bfcdf922652ccf7",
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n### Focused Summary for Literature Review: Two-Stage Evolutionary Reinforcement Learning for Enhancing Exploration and Exploitation \\cite{zhu2024sb0}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem:** Deep Reinforcement Learning (DRL) often suffers from challenges such as sparse rewards, insufficient exploration, and premature convergence to local optima. Existing Evolutionary Reinforcement Learning (ERL) methods, which typically use only actor networks as individuals for Evolutionary Algorithms (EAs), limit exploratory capabilities because the entire actor population's evolution can be constrained by a single critic network falling into local optima.\n    *   **Importance and Challenge:** Addressing these issues is crucial for developing more robust and efficient DRL agents capable of solving complex tasks. The challenge lies in effectively integrating EAs and RL to leverage their respective strengths (EA for global exploration, RL for sample efficiency and local refinement) without incurring excessive computational costs or sacrificing exploration for exploitation.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches:** This work builds upon the foundation of Evolutionary Reinforcement Learning (ERL) algorithms, which combine EAs (like GA, CEM, ES) with RL methods (like DDPG, TD3, SAC).\n    *   **Limitations of Previous Solutions:**\n        *   Most ERLs primarily employ *actor networks* as individuals for the EA component (e.g., ERL, PDERL, CEM-RL, ESAC).\n        *   This approach curtails the exploration potential of individuals, as their updates are heavily reliant on a single RL critic network, making them susceptible to local optima if the critic gets stuck.\n        *   Concurrently training multiple complete RL agents (actor-critic pairs) has been computationally prohibitive for many ERL approaches.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper introduces Two-stage Evolutionary Reinforcement Learning (TERL) \\cite{zhu2024sb0}, a framework that maintains a population of *complete RL agents* (both actor and critic networks) and divides the learning process into two distinct stages:\n        *   **Stage 1 (Exploration Stage):** All individuals (actor-critic pairs) independently learn and explore. They are optimized alternatively by RL (gradient-based updates) and Particle Swarm Optimization (PSO). Information sharing is facilitated through a common replay buffer and periodic PSO updates, which significantly mitigates the computational load of training multiple agents.\n        *   **Stage 2 (Exploitation Stage):** The focus shifts to refining the best-performing individual. Only this best individual undergoes further RL-based refinement, while the remaining individuals continue PSO-based optimization to provide diverse experiences to the shared replay buffer. This allocates more computational resources to the most promising agent for superior final performance.\n    *   **Novelty/Differentiation:**\n        *   **Population of Full RL Agents:** Unlike most ERLs, TERL maintains a population of *both actor and critic networks* for each individual, allowing for more independent and diverse exploration by each agent.\n        *   **Two-Stage Learning Process:** Explicitly separates exploration and exploitation phases, optimizing resource allocation and learning strategies for each.\n        *   **Dual Optimization with PSO and RL:** Integrates PSO for population-level information sharing and global search with RL for local gradient-based refinement within individuals.\n        *   **Efficient Information Sharing:** Utilizes a common replay buffer and PSO updates to enable multi-agent training without excessive computational burden.\n\n4.  **Key Technical Contributions**\n    *   **Novel Method TERL:** A new ERL framework that maintains a population of complete RL agents (actor-critic pairs) and employs a two-stage learning process (exploration-focused initial stage, exploitation-focused latter stage).\n    *   **Information Sharing Strategy:** Proposes an effective strategy using PSO and a shared replay buffer to facilitate information exchange among individuals, enabling multiple agents to learn efficiently.\n    *   **Hybrid Optimization:** Combines gradient-based RL updates with population-based PSO updates for both individual and population-level optimization, balancing exploration and exploitation.\n    *   **Resource Allocation Strategy:** Dynamically allocates computational resources, initially distributing them for broad exploration and then concentrating them on the best individual for refined exploitation.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted:** Empirical assessments were performed across a range of continuous control problems.\n    *   **Environments:** Popular Mujoco simulation environments (e.g., HalfCheetah, Ant, Walker2d) from OpenAI Gym, along with three classic control problems.\n    *   **Key Performance Metrics:** The sum of rewards obtained in one episode. For population-based algorithms, the individual with the highest fitness within the population is selected and tested. Performance is evaluated every five thousand steps.\n    *   **Comparison Results:** TERL \\cite{zhu2024sb0} was implemented using the Twin Delayed Deep Deterministic Policy Gradient (TD3) algorithm as its base RL component. Comprehensive evaluations demonstrated that TERL consistently outperforms both state-of-the-art ERLs and standalone RL algorithms on the tested continuous control tasks.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:**\n        *   While information sharing reduces the burden, training multiple complete agents still consumes more resources than training a single agent, necessitating the two-stage approach.\n        *   In the exploitation stage, the \"best\" individual is not replaced even if other individuals achieve higher rewards, because their critic networks are no longer updated and cannot effectively guide actor updates. This design choice might limit adaptability if the environment changes drastically or if a non-best individual finds a significantly better policy later.\n        *   The specific choice of TD3 as the underlying RL algorithm and PSO as the EA for population updates might limit generalizability to other RL/EA combinations without further investigation.\n    *   **Scope of Applicability:** Primarily validated on continuous control problems in simulated environments (Mujoco, classic control). Its applicability to discrete action spaces, real-world robotics, or more complex, high-dimensional observation spaces would require further testing.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art:** TERL \\cite{zhu2024sb0} significantly advances ERL by addressing the critical limitation of actor-only populations, enabling more robust exploration and mitigating susceptibility to local optima through a population of full actor-critic agents. The two-stage approach provides a principled way to balance exploration and exploitation.\n    *   **Potential Impact on Future Research:**\n        *   Opens avenues for developing more sophisticated ERL frameworks that leverage populations of complete RL agents.\n        *   Encourages further research into dynamic resource allocation strategies in multi-agent learning.\n        *   Could inspire new methods for efficient information sharing and hybrid optimization in complex DRL settings, potentially leading to more generalizable and high-performing agents for challenging real-world problems.",
      "intriguing_abstract": "Deep Reinforcement Learning (DRL) agents frequently struggle with insufficient exploration, sparse rewards, and premature convergence to local optima. While Evolutionary Reinforcement Learning (ERL) offers a promising avenue, existing methods often limit exploration by evolving only actor networks, leaving the entire population susceptible to a single critic's local minima. We introduce **Two-stage Evolutionary Reinforcement Learning (TERL)**, a novel framework that fundamentally redefines ERL by maintaining a population of *complete actor-critic agents*.\n\nTERL employs a unique two-stage learning process. The initial **exploration stage** fosters diverse learning among all agents through independent RL updates and population-level **Particle Swarm Optimization (PSO)**, efficiently sharing experiences via a common replay buffer. The subsequent **exploitation stage** then strategically focuses computational resources on refining the best-performing agent, while others continue to enrich the shared experience. This hybrid optimization strategy, combining gradient-based RL with global PSO search, enables unprecedented robust exploration and effective exploitation. Empirical evaluations on challenging **continuous control** tasks demonstrate that TERL consistently outperforms state-of-the-art ERLs and standalone DRL algorithms, offering a significant leap towards more resilient and high-performing agents. This work paves the way for developing advanced DRL systems capable of tackling complex, real-world problems.",
      "keywords": [
        "Two-stage Evolutionary Reinforcement Learning (TERL)",
        "Deep Reinforcement Learning (DRL)",
        "Evolutionary Reinforcement Learning (ERL)",
        "exploration-exploitation balance",
        "population of full RL agents",
        "actor-critic networks",
        "Particle Swarm Optimization (PSO)",
        "hybrid optimization",
        "shared replay buffer",
        "dynamic resource allocation",
        "continuous control problems",
        "local optima",
        "state-of-the-art performance"
      ],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/8357670aac3c98a71b454ab5bca89558f265369d.pdf",
      "citation_key": "zhu2024sb0",
      "metadata": {
        "title": "Two-Stage Evolutionary Reinforcement Learning for Enhancing Exploration and Exploitation",
        "authors": [
          "Qingling Zhu",
          "Xiaoqiang Wu",
          "Qiuzhen Lin",
          "Weineng Chen"
        ],
        "published_date": "2024",
        "abstract": "The integration of Evolutionary Algorithm (EA) and Reinforcement Learning (RL) has emerged as a promising approach for tackling some challenges in RL, such as sparse rewards, lack of exploration, and brittle convergence properties. However, existing methods often employ actor networks as individuals of EA, which may constrain their exploratory capabilities, as the entire actor population will stop evolution when the critic network in RL falls into local optimal. To alleviate this issue, this paper introduces a Two-stage Evolutionary Reinforcement Learning (TERL) framework that maintains a population containing both actor and critic networks. TERL divides the learning process into two stages. In the initial stage, individuals independently learn actor-critic networks, which are optimized alternatively by RL and Particle Swarm Optimization (PSO). This dual optimization fosters greater exploration, curbing susceptibility to local optima. Shared information from a common replay buffer and PSO algorithm substantially mitigates the computational load of training multiple agents. In the subsequent stage, TERL shifts to a refined exploitation phase. Here, only the best individual undergoes further refinement, while the rest individuals continue PSO-based optimization. This allocates more computational resources to the best individual for yielding superior performance. Empirical assessments, conducted across a range of continuous control problems, validate the efficacy of the proposed TERL paradigm.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/8357670aac3c98a71b454ab5bca89558f265369d.pdf",
        "venue": "AAAI Conference on Artificial Intelligence",
        "citationCount": 4,
        "score": 4.0,
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n### Focused Summary for Literature Review: Two-Stage Evolutionary Reinforcement Learning for Enhancing Exploration and Exploitation \\cite{zhu2024sb0}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem:** Deep Reinforcement Learning (DRL) often suffers from challenges such as sparse rewards, insufficient exploration, and premature convergence to local optima. Existing Evolutionary Reinforcement Learning (ERL) methods, which typically use only actor networks as individuals for Evolutionary Algorithms (EAs), limit exploratory capabilities because the entire actor population's evolution can be constrained by a single critic network falling into local optima.\n    *   **Importance and Challenge:** Addressing these issues is crucial for developing more robust and efficient DRL agents capable of solving complex tasks. The challenge lies in effectively integrating EAs and RL to leverage their respective strengths (EA for global exploration, RL for sample efficiency and local refinement) without incurring excessive computational costs or sacrificing exploration for exploitation.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches:** This work builds upon the foundation of Evolutionary Reinforcement Learning (ERL) algorithms, which combine EAs (like GA, CEM, ES) with RL methods (like DDPG, TD3, SAC).\n    *   **Limitations of Previous Solutions:**\n        *   Most ERLs primarily employ *actor networks* as individuals for the EA component (e.g., ERL, PDERL, CEM-RL, ESAC).\n        *   This approach curtails the exploration potential of individuals, as their updates are heavily reliant on a single RL critic network, making them susceptible to local optima if the critic gets stuck.\n        *   Concurrently training multiple complete RL agents (actor-critic pairs) has been computationally prohibitive for many ERL approaches.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper introduces Two-stage Evolutionary Reinforcement Learning (TERL) \\cite{zhu2024sb0}, a framework that maintains a population of *complete RL agents* (both actor and critic networks) and divides the learning process into two distinct stages:\n        *   **Stage 1 (Exploration Stage):** All individuals (actor-critic pairs) independently learn and explore. They are optimized alternatively by RL (gradient-based updates) and Particle Swarm Optimization (PSO). Information sharing is facilitated through a common replay buffer and periodic PSO updates, which significantly mitigates the computational load of training multiple agents.\n        *   **Stage 2 (Exploitation Stage):** The focus shifts to refining the best-performing individual. Only this best individual undergoes further RL-based refinement, while the remaining individuals continue PSO-based optimization to provide diverse experiences to the shared replay buffer. This allocates more computational resources to the most promising agent for superior final performance.\n    *   **Novelty/Differentiation:**\n        *   **Population of Full RL Agents:** Unlike most ERLs, TERL maintains a population of *both actor and critic networks* for each individual, allowing for more independent and diverse exploration by each agent.\n        *   **Two-Stage Learning Process:** Explicitly separates exploration and exploitation phases, optimizing resource allocation and learning strategies for each.\n        *   **Dual Optimization with PSO and RL:** Integrates PSO for population-level information sharing and global search with RL for local gradient-based refinement within individuals.\n        *   **Efficient Information Sharing:** Utilizes a common replay buffer and PSO updates to enable multi-agent training without excessive computational burden.\n\n4.  **Key Technical Contributions**\n    *   **Novel Method TERL:** A new ERL framework that maintains a population of complete RL agents (actor-critic pairs) and employs a two-stage learning process (exploration-focused initial stage, exploitation-focused latter stage).\n    *   **Information Sharing Strategy:** Proposes an effective strategy using PSO and a shared replay buffer to facilitate information exchange among individuals, enabling multiple agents to learn efficiently.\n    *   **Hybrid Optimization:** Combines gradient-based RL updates with population-based PSO updates for both individual and population-level optimization, balancing exploration and exploitation.\n    *   **Resource Allocation Strategy:** Dynamically allocates computational resources, initially distributing them for broad exploration and then concentrating them on the best individual for refined exploitation.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted:** Empirical assessments were performed across a range of continuous control problems.\n    *   **Environments:** Popular Mujoco simulation environments (e.g., HalfCheetah, Ant, Walker2d) from OpenAI Gym, along with three classic control problems.\n    *   **Key Performance Metrics:** The sum of rewards obtained in one episode. For population-based algorithms, the individual with the highest fitness within the population is selected and tested. Performance is evaluated every five thousand steps.\n    *   **Comparison Results:** TERL \\cite{zhu2024sb0} was implemented using the Twin Delayed Deep Deterministic Policy Gradient (TD3) algorithm as its base RL component. Comprehensive evaluations demonstrated that TERL consistently outperforms both state-of-the-art ERLs and standalone RL algorithms on the tested continuous control tasks.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:**\n        *   While information sharing reduces the burden, training multiple complete agents still consumes more resources than training a single agent, necessitating the two-stage approach.\n        *   In the exploitation stage, the \"best\" individual is not replaced even if other individuals achieve higher rewards, because their critic networks are no longer updated and cannot effectively guide actor updates. This design choice might limit adaptability if the environment changes drastically or if a non-best individual finds a significantly better policy later.\n        *   The specific choice of TD3 as the underlying RL algorithm and PSO as the EA for population updates might limit generalizability to other RL/EA combinations without further investigation.\n    *   **Scope of Applicability:** Primarily validated on continuous control problems in simulated environments (Mujoco, classic control). Its applicability to discrete action spaces, real-world robotics, or more complex, high-dimensional observation spaces would require further testing.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art:** TERL \\cite{zhu2024sb0} significantly advances ERL by addressing the critical limitation of actor-only populations, enabling more robust exploration and mitigating susceptibility to local optima through a population of full actor-critic agents. The two-stage approach provides a principled way to balance exploration and exploitation.\n    *   **Potential Impact on Future Research:**\n        *   Opens avenues for developing more sophisticated ERL frameworks that leverage populations of complete RL agents.\n        *   Encourages further research into dynamic resource allocation strategies in multi-agent learning.\n        *   Could inspire new methods for efficient information sharing and hybrid optimization in complex DRL settings, potentially leading to more generalizable and high-performing agents for challenging real-world problems.",
        "keywords": [
          "Two-stage Evolutionary Reinforcement Learning (TERL)",
          "Deep Reinforcement Learning (DRL)",
          "Evolutionary Reinforcement Learning (ERL)",
          "exploration-exploitation balance",
          "population of full RL agents",
          "actor-critic networks",
          "Particle Swarm Optimization (PSO)",
          "hybrid optimization",
          "shared replay buffer",
          "dynamic resource allocation",
          "continuous control problems",
          "local optima",
          "state-of-the-art performance"
        ],
        "paper_type": "the paper should be classified as **technical**.\n\n**reasoning:**\n\n1.  **abstract analysis:**\n    *   it explicitly states: \"this paper introduces a two-stage evolutionary reinforcement learning (terl) framework\". this is a direct indicator of presenting a new method or system.\n    *   it describes the components and stages of this new framework (\"terl divides the learning process into two stages,\" \"in the initial stage...\", \"in the subsequent stage...\").\n    *   it mentions \"proposed terl paradigm,\" further emphasizing the novelty of the approach.\n    *   while it includes \"empirical assessments... validate the efficacy of the proposed terl paradigm,\" the empirical work is presented as validation *of the proposed method*, not as the primary research question itself.\n\n2.  **introduction analysis:**\n    *   it sets up a technical problem: challenges in drl like sparse rewards and premature convergence, and limitations of existing eas and erls.\n    *   it discusses existing algorithms and their shortcomings, which is typical for introducing the need for a new technical solution.\n\n3.  **classification criteria match:**\n    *   **technical**: \"abstract mentions: 'propose', 'develop', 'present', 'algorithm', 'method'. introduction discusses: technical problem, proposed solution.\" this aligns perfectly with the content. the paper proposes a new framework (terl) to solve specific technical challenges in rl."
      },
      "file_name": "8357670aac3c98a71b454ab5bca89558f265369d.pdf"
    },
    {
      "success": true,
      "doc_id": "6f80fbf8180c0e816ec288e7da414cdf",
      "summary": "Precise modeling of power systems is vital to ensure stability, reliability, and secure operations. In power industrial settings, model parameters can become skewed over time due to prolonged device usage or modifications made to the control systems. Doubly-Fed Induction Generator (DFIG), one of the most prevalent generators in wind farms, is sensitive to transient occurrences. Consequently, parameter calibration of DFIG becomes a crucial focal point in power system planning and operational studies. In this paper, two baseline approaches are first developed to identify the potentially harmful parameters of the DFIG system, including the Particle Swarm Optimization (PSO) method and the state-of-the-art off-policy Reinforcement Learning (RL) method, Soft Actor-Critic (SAC). The outcomes demonstrated that the SAC method outperformed PSO, resulting in an impressive reduction of 74.67% Mean Squared Error (MSE) and a more efficient testing period. In further exploration, a novel hybrid approach called SAC-PSO is developed, with SAC being the teacher of PSO to tackle scenarios with multiple potential solutions. The results exhibited an even greater enhancement over using SAC alone, leading to a remarkable reduction of 87.84% MSE during the testing phase. The proposed method can also effectively apply to a power plant incorporating multiple wind generators.",
      "intriguing_abstract": "Precise modeling of power systems is vital to ensure stability, reliability, and secure operations. In power industrial settings, model parameters can become skewed over time due to prolonged device usage or modifications made to the control systems. Doubly-Fed Induction Generator (DFIG), one of the most prevalent generators in wind farms, is sensitive to transient occurrences. Consequently, parameter calibration of DFIG becomes a crucial focal point in power system planning and operational studies. In this paper, two baseline approaches are first developed to identify the potentially harmful parameters of the DFIG system, including the Particle Swarm Optimization (PSO) method and the state-of-the-art off-policy Reinforcement Learning (RL) method, Soft Actor-Critic (SAC). The outcomes demonstrated that the SAC method outperformed PSO, resulting in an impressive reduction of 74.67% Mean Squared Error (MSE) and a more efficient testing period. In further exploration, a novel hybrid approach called SAC-PSO is developed, with SAC being the teacher of PSO to tackle scenarios with multiple potential solutions. The results exhibited an even greater enhancement over using SAC alone, leading to a remarkable reduction of 87.84% MSE during the testing phase. The proposed method can also effectively apply to a power plant incorporating multiple wind generators.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/58db2247187ac01acabc1c2fa02f9b189772729e.pdf",
      "citation_key": "xiang2024qhz",
      "metadata": {
        "title": "An Intelligent Parameter Identification Method of DFIG Systems Using Hybrid Particle Swarm Optimization and Reinforcement Learning",
        "authors": [
          "Xuanchen Xiang",
          "Ruisheng Diao",
          "S. Bernadin",
          "Simon Y. Foo",
          "Fangyuan Sun",
          "Ayodeji S. Ogundana"
        ],
        "published_date": "2024",
        "abstract": "Precise modeling of power systems is vital to ensure stability, reliability, and secure operations. In power industrial settings, model parameters can become skewed over time due to prolonged device usage or modifications made to the control systems. Doubly-Fed Induction Generator (DFIG), one of the most prevalent generators in wind farms, is sensitive to transient occurrences. Consequently, parameter calibration of DFIG becomes a crucial focal point in power system planning and operational studies. In this paper, two baseline approaches are first developed to identify the potentially harmful parameters of the DFIG system, including the Particle Swarm Optimization (PSO) method and the state-of-the-art off-policy Reinforcement Learning (RL) method, Soft Actor-Critic (SAC). The outcomes demonstrated that the SAC method outperformed PSO, resulting in an impressive reduction of 74.67% Mean Squared Error (MSE) and a more efficient testing period. In further exploration, a novel hybrid approach called SAC-PSO is developed, with SAC being the teacher of PSO to tackle scenarios with multiple potential solutions. The results exhibited an even greater enhancement over using SAC alone, leading to a remarkable reduction of 87.84% MSE during the testing phase. The proposed method can also effectively apply to a power plant incorporating multiple wind generators.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/58db2247187ac01acabc1c2fa02f9b189772729e.pdf",
        "venue": "IEEE Access",
        "citationCount": 4,
        "score": 4.0,
        "summary": "Precise modeling of power systems is vital to ensure stability, reliability, and secure operations. In power industrial settings, model parameters can become skewed over time due to prolonged device usage or modifications made to the control systems. Doubly-Fed Induction Generator (DFIG), one of the most prevalent generators in wind farms, is sensitive to transient occurrences. Consequently, parameter calibration of DFIG becomes a crucial focal point in power system planning and operational studies. In this paper, two baseline approaches are first developed to identify the potentially harmful parameters of the DFIG system, including the Particle Swarm Optimization (PSO) method and the state-of-the-art off-policy Reinforcement Learning (RL) method, Soft Actor-Critic (SAC). The outcomes demonstrated that the SAC method outperformed PSO, resulting in an impressive reduction of 74.67% Mean Squared Error (MSE) and a more efficient testing period. In further exploration, a novel hybrid approach called SAC-PSO is developed, with SAC being the teacher of PSO to tackle scenarios with multiple potential solutions. The results exhibited an even greater enhancement over using SAC alone, leading to a remarkable reduction of 87.84% MSE during the testing phase. The proposed method can also effectively apply to a power plant incorporating multiple wind generators.",
        "keywords": []
      },
      "file_name": "58db2247187ac01acabc1c2fa02f9b189772729e.pdf"
    },
    {
      "success": true,
      "doc_id": "397cd5c646f68f89bad64e7dd771a0f5",
      "summary": "In surface exploration missions, wheeled planetary vehicles have difficulty traveling on asteroids due to their weak gravitational fields. With the rapid development of hardware performance and control methods, quadruped robots have great potential to serve in asteroid exploration. When deploying or controlling the jumping motions of a quadruped robot on an asteroid, landing stability must be guaranteed. Under the weak and irregular gravitational fields of asteroids, the robot should be reoriented to an appropriate attitude for a smooth and stable landing. To achieve this objective, a model-free control method based on reinforcement learning (RL) was proposed. Sim-to-real transfer methods including domain randomization and transfer learning were proposed to address the sim-to-real problem. The original control model trained by RL was transferred to a new version that could be applied and tested on a real quadruped robot. An equivalent asteroid's weak gravitational environment experimental platform was for the first time designed and employed to test the performance of the proposed control scheme. Both the simulation and experimental results validated the effectiveness of the proposed controller training and sim-to-real transfer methods.",
      "intriguing_abstract": "In surface exploration missions, wheeled planetary vehicles have difficulty traveling on asteroids due to their weak gravitational fields. With the rapid development of hardware performance and control methods, quadruped robots have great potential to serve in asteroid exploration. When deploying or controlling the jumping motions of a quadruped robot on an asteroid, landing stability must be guaranteed. Under the weak and irregular gravitational fields of asteroids, the robot should be reoriented to an appropriate attitude for a smooth and stable landing. To achieve this objective, a model-free control method based on reinforcement learning (RL) was proposed. Sim-to-real transfer methods including domain randomization and transfer learning were proposed to address the sim-to-real problem. The original control model trained by RL was transferred to a new version that could be applied and tested on a real quadruped robot. An equivalent asteroid's weak gravitational environment experimental platform was for the first time designed and employed to test the performance of the proposed control scheme. Both the simulation and experimental results validated the effectiveness of the proposed controller training and sim-to-real transfer methods.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/e401ba782c2da93959582295089d3f04a051d6c1.pdf",
      "citation_key": "qi2024hxq",
      "metadata": {
        "title": "Reinforcement Learning and Sim-to-Real Transfer of Reorientation and Landing Control for Quadruped Robots on Asteroids",
        "authors": [
          "Ji Qi",
          "Haibo Gao",
          "Huanli Su",
          "M. Huo",
          "Haitao Yu",
          "Zongquan Deng"
        ],
        "published_date": "2024",
        "abstract": "In surface exploration missions, wheeled planetary vehicles have difficulty traveling on asteroids due to their weak gravitational fields. With the rapid development of hardware performance and control methods, quadruped robots have great potential to serve in asteroid exploration. When deploying or controlling the jumping motions of a quadruped robot on an asteroid, landing stability must be guaranteed. Under the weak and irregular gravitational fields of asteroids, the robot should be reoriented to an appropriate attitude for a smooth and stable landing. To achieve this objective, a model-free control method based on reinforcement learning (RL) was proposed. Sim-to-real transfer methods including domain randomization and transfer learning were proposed to address the sim-to-real problem. The original control model trained by RL was transferred to a new version that could be applied and tested on a real quadruped robot. An equivalent asteroid's weak gravitational environment experimental platform was for the first time designed and employed to test the performance of the proposed control scheme. Both the simulation and experimental results validated the effectiveness of the proposed controller training and sim-to-real transfer methods.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/e401ba782c2da93959582295089d3f04a051d6c1.pdf",
        "venue": "IEEE transactions on industrial electronics (1982. Print)",
        "citationCount": 4,
        "score": 4.0,
        "summary": "In surface exploration missions, wheeled planetary vehicles have difficulty traveling on asteroids due to their weak gravitational fields. With the rapid development of hardware performance and control methods, quadruped robots have great potential to serve in asteroid exploration. When deploying or controlling the jumping motions of a quadruped robot on an asteroid, landing stability must be guaranteed. Under the weak and irregular gravitational fields of asteroids, the robot should be reoriented to an appropriate attitude for a smooth and stable landing. To achieve this objective, a model-free control method based on reinforcement learning (RL) was proposed. Sim-to-real transfer methods including domain randomization and transfer learning were proposed to address the sim-to-real problem. The original control model trained by RL was transferred to a new version that could be applied and tested on a real quadruped robot. An equivalent asteroid's weak gravitational environment experimental platform was for the first time designed and employed to test the performance of the proposed control scheme. Both the simulation and experimental results validated the effectiveness of the proposed controller training and sim-to-real transfer methods.",
        "keywords": []
      },
      "file_name": "e401ba782c2da93959582295089d3f04a051d6c1.pdf"
    },
    {
      "success": true,
      "doc_id": "5428bafbe4cbf86441af331c1dfaf63f",
      "summary": "Edge servers, which are located in close proximity to mobile users, have become key components for providing augmented computation and bandwidth. As the resources of edge servers are limited and shared, it is critical for the decentralized mobile users to determine the amount of offloaded workload, to avoid competition or waste of the public resources at the edge servers. Reinforcement learning (RL) methods, which are sequential and model-free, have been widely considered as a promising approach. However, directly deploying RL in edge computing remains elusive, since arbitrary exploration in real online environments often leads to poor user experience. To avoid the costly interactions, in this paper, we propose an offline RL framework which can be optimized by using a static offline dataset only. In essence, our method first trains a supervised offline model to simulate the edge computing environment dynamics, and then optimize the offloading policy in the offline environment with cost-free interactions. As the offloading requests are mostly asynchronous, we adopt a mean-field approach that treats all neighboring users as a single agent. The problem can then be simplified and reduced to a game between only two players. Moreover, we limit the length of the offline model rollout to ensure the simulated trajectories are accurate, so that the trained offloading policies can be generalized to unseen online environments. Theoretical analyses are conducted to validate the accuracy and convergence of our algorithm. In the experiments, we first train the offline simulation environment with a real historical data set, and then optimize the offloading policy in this environment model. The results show that our algorithm can converge very fast during training. In the execution, the algorithm still achieves high performance in the online environment.",
      "intriguing_abstract": "Edge servers, which are located in close proximity to mobile users, have become key components for providing augmented computation and bandwidth. As the resources of edge servers are limited and shared, it is critical for the decentralized mobile users to determine the amount of offloaded workload, to avoid competition or waste of the public resources at the edge servers. Reinforcement learning (RL) methods, which are sequential and model-free, have been widely considered as a promising approach. However, directly deploying RL in edge computing remains elusive, since arbitrary exploration in real online environments often leads to poor user experience. To avoid the costly interactions, in this paper, we propose an offline RL framework which can be optimized by using a static offline dataset only. In essence, our method first trains a supervised offline model to simulate the edge computing environment dynamics, and then optimize the offloading policy in the offline environment with cost-free interactions. As the offloading requests are mostly asynchronous, we adopt a mean-field approach that treats all neighboring users as a single agent. The problem can then be simplified and reduced to a game between only two players. Moreover, we limit the length of the offline model rollout to ensure the simulated trajectories are accurate, so that the trained offloading policies can be generalized to unseen online environments. Theoretical analyses are conducted to validate the accuracy and convergence of our algorithm. In the experiments, we first train the offline simulation environment with a real historical data set, and then optimize the offloading policy in this environment model. The results show that our algorithm can converge very fast during training. In the execution, the algorithm still achieves high performance in the online environment.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/afa538f59cf2996837863be60a34eef5271a5ee9.pdf",
      "citation_key": "zhang2024wgo",
      "metadata": {
        "title": "Offline Reinforcement Learning for Asynchronous Task Offloading in Mobile Edge Computing",
        "authors": [
          "Bolei Zhang",
          "Fu Xiao",
          "Lifa Wu"
        ],
        "published_date": "2024",
        "abstract": "Edge servers, which are located in close proximity to mobile users, have become key components for providing augmented computation and bandwidth. As the resources of edge servers are limited and shared, it is critical for the decentralized mobile users to determine the amount of offloaded workload, to avoid competition or waste of the public resources at the edge servers. Reinforcement learning (RL) methods, which are sequential and model-free, have been widely considered as a promising approach. However, directly deploying RL in edge computing remains elusive, since arbitrary exploration in real online environments often leads to poor user experience. To avoid the costly interactions, in this paper, we propose an offline RL framework which can be optimized by using a static offline dataset only. In essence, our method first trains a supervised offline model to simulate the edge computing environment dynamics, and then optimize the offloading policy in the offline environment with cost-free interactions. As the offloading requests are mostly asynchronous, we adopt a mean-field approach that treats all neighboring users as a single agent. The problem can then be simplified and reduced to a game between only two players. Moreover, we limit the length of the offline model rollout to ensure the simulated trajectories are accurate, so that the trained offloading policies can be generalized to unseen online environments. Theoretical analyses are conducted to validate the accuracy and convergence of our algorithm. In the experiments, we first train the offline simulation environment with a real historical data set, and then optimize the offloading policy in this environment model. The results show that our algorithm can converge very fast during training. In the execution, the algorithm still achieves high performance in the online environment.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/afa538f59cf2996837863be60a34eef5271a5ee9.pdf",
        "venue": "IEEE Transactions on Network and Service Management",
        "citationCount": 4,
        "score": 4.0,
        "summary": "Edge servers, which are located in close proximity to mobile users, have become key components for providing augmented computation and bandwidth. As the resources of edge servers are limited and shared, it is critical for the decentralized mobile users to determine the amount of offloaded workload, to avoid competition or waste of the public resources at the edge servers. Reinforcement learning (RL) methods, which are sequential and model-free, have been widely considered as a promising approach. However, directly deploying RL in edge computing remains elusive, since arbitrary exploration in real online environments often leads to poor user experience. To avoid the costly interactions, in this paper, we propose an offline RL framework which can be optimized by using a static offline dataset only. In essence, our method first trains a supervised offline model to simulate the edge computing environment dynamics, and then optimize the offloading policy in the offline environment with cost-free interactions. As the offloading requests are mostly asynchronous, we adopt a mean-field approach that treats all neighboring users as a single agent. The problem can then be simplified and reduced to a game between only two players. Moreover, we limit the length of the offline model rollout to ensure the simulated trajectories are accurate, so that the trained offloading policies can be generalized to unseen online environments. Theoretical analyses are conducted to validate the accuracy and convergence of our algorithm. In the experiments, we first train the offline simulation environment with a real historical data set, and then optimize the offloading policy in this environment model. The results show that our algorithm can converge very fast during training. In the execution, the algorithm still achieves high performance in the online environment.",
        "keywords": []
      },
      "file_name": "afa538f59cf2996837863be60a34eef5271a5ee9.pdf"
    },
    {
      "success": true,
      "doc_id": "a33da1b711751920a49fa9a18ab782d0",
      "summary": "Autonomous region protection is a significant research area in multi-agent systems, aiming to empower defenders in preventing intruders from accessing specific regions. This paper presents a Multi-agent Region Protection Environment (MRPE) featuring fewer defenders, defender damages, and intruder evasion strategies targeting defenders. MRPE poses challenges for traditional protection methods due to its high nonstationarity and limited interception time window. To surmount these hurdles, we modify evolutionary reinforcement learning, giving rise to the corresponding multi-agent region protection method (MRPM). MRPM amalgamates the merits of evolutionary algorithms and deep reinforcement learning, specifically leveraging Differential Evolution (DE) and Multi-Agent Deep Deterministic Policy Gradient (MADDPG). DE facilitates diverse sample exploration and overcomes sparse rewards, while MADDPG trains defenders and expedites the DE convergence process. Additionally, an elite selection strategy tailored for multi-agent systems is devised to enhance defender collaboration. The paper also presents ingenious designs for the fitness and reward functions to effectively drive policy optimizations. Finally, extensive numerical simulations are conducted to validate the effectiveness of MRPM.",
      "intriguing_abstract": "Autonomous region protection is a significant research area in multi-agent systems, aiming to empower defenders in preventing intruders from accessing specific regions. This paper presents a Multi-agent Region Protection Environment (MRPE) featuring fewer defenders, defender damages, and intruder evasion strategies targeting defenders. MRPE poses challenges for traditional protection methods due to its high nonstationarity and limited interception time window. To surmount these hurdles, we modify evolutionary reinforcement learning, giving rise to the corresponding multi-agent region protection method (MRPM). MRPM amalgamates the merits of evolutionary algorithms and deep reinforcement learning, specifically leveraging Differential Evolution (DE) and Multi-Agent Deep Deterministic Policy Gradient (MADDPG). DE facilitates diverse sample exploration and overcomes sparse rewards, while MADDPG trains defenders and expedites the DE convergence process. Additionally, an elite selection strategy tailored for multi-agent systems is devised to enhance defender collaboration. The paper also presents ingenious designs for the fitness and reward functions to effectively drive policy optimizations. Finally, extensive numerical simulations are conducted to validate the effectiveness of MRPM.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/88f98e3629a7aef2312f9e214ed2a75672113e4f.pdf",
      "citation_key": "sun2024edc",
      "metadata": {
        "title": "A modified evolutionary reinforcement learning for multi-agent region protection with fewer defenders",
        "authors": [
          "Siqing Sun",
          "Huachao Dong",
          "Tianbo Li"
        ],
        "published_date": "2024",
        "abstract": "Autonomous region protection is a significant research area in multi-agent systems, aiming to empower defenders in preventing intruders from accessing specific regions. This paper presents a Multi-agent Region Protection Environment (MRPE) featuring fewer defenders, defender damages, and intruder evasion strategies targeting defenders. MRPE poses challenges for traditional protection methods due to its high nonstationarity and limited interception time window. To surmount these hurdles, we modify evolutionary reinforcement learning, giving rise to the corresponding multi-agent region protection method (MRPM). MRPM amalgamates the merits of evolutionary algorithms and deep reinforcement learning, specifically leveraging Differential Evolution (DE) and Multi-Agent Deep Deterministic Policy Gradient (MADDPG). DE facilitates diverse sample exploration and overcomes sparse rewards, while MADDPG trains defenders and expedites the DE convergence process. Additionally, an elite selection strategy tailored for multi-agent systems is devised to enhance defender collaboration. The paper also presents ingenious designs for the fitness and reward functions to effectively drive policy optimizations. Finally, extensive numerical simulations are conducted to validate the effectiveness of MRPM.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/88f98e3629a7aef2312f9e214ed2a75672113e4f.pdf",
        "venue": "Complex &amp; Intelligent Systems",
        "citationCount": 4,
        "score": 4.0,
        "summary": "Autonomous region protection is a significant research area in multi-agent systems, aiming to empower defenders in preventing intruders from accessing specific regions. This paper presents a Multi-agent Region Protection Environment (MRPE) featuring fewer defenders, defender damages, and intruder evasion strategies targeting defenders. MRPE poses challenges for traditional protection methods due to its high nonstationarity and limited interception time window. To surmount these hurdles, we modify evolutionary reinforcement learning, giving rise to the corresponding multi-agent region protection method (MRPM). MRPM amalgamates the merits of evolutionary algorithms and deep reinforcement learning, specifically leveraging Differential Evolution (DE) and Multi-Agent Deep Deterministic Policy Gradient (MADDPG). DE facilitates diverse sample exploration and overcomes sparse rewards, while MADDPG trains defenders and expedites the DE convergence process. Additionally, an elite selection strategy tailored for multi-agent systems is devised to enhance defender collaboration. The paper also presents ingenious designs for the fitness and reward functions to effectively drive policy optimizations. Finally, extensive numerical simulations are conducted to validate the effectiveness of MRPM.",
        "keywords": []
      },
      "file_name": "88f98e3629a7aef2312f9e214ed2a75672113e4f.pdf"
    },
    {
      "success": true,
      "doc_id": "ab8ee0f96e665ea65f91d8d600f0ec75",
      "summary": "This research focused on enhancing post-incident malware forensic investigation using reinforcement learning RL. We proposed an advanced MDP post incident malware forensics investigation model and framework to expedite post incident forensics. We then implement our RL Malware Investigation Model based on structured MDP within the proposed framework. To identify malware artefacts, the RL agent acquires and examines forensics evidence files, iteratively improving its capabilities using Q Table and temporal difference learning. The Q learning algorithm significantly improved the agent ability to identify malware. An epsilon greedy exploration strategy and Q learning updates enabled efficient learning and decision making. Our experimental testing revealed that optimal learning rates depend on the MDP environment complexity, with simpler environments benefiting from higher rates for quicker convergence and complex ones requiring lower rates for stability. Our model performance in identifying and classifying malware reduced malware analysis time compared to human experts, demonstrating robustness and adaptability. The study highlighted the significance of hyper parameter tuning and suggested adaptive strategies for complex environments. Our RL based approach produced promising results and is validated as an alternative to traditional methods notably by offering continuous learning and adaptation to new and evolving malware threats which ultimately enhance the post incident forensics investigations.",
      "intriguing_abstract": "This research focused on enhancing post-incident malware forensic investigation using reinforcement learning RL. We proposed an advanced MDP post incident malware forensics investigation model and framework to expedite post incident forensics. We then implement our RL Malware Investigation Model based on structured MDP within the proposed framework. To identify malware artefacts, the RL agent acquires and examines forensics evidence files, iteratively improving its capabilities using Q Table and temporal difference learning. The Q learning algorithm significantly improved the agent ability to identify malware. An epsilon greedy exploration strategy and Q learning updates enabled efficient learning and decision making. Our experimental testing revealed that optimal learning rates depend on the MDP environment complexity, with simpler environments benefiting from higher rates for quicker convergence and complex ones requiring lower rates for stability. Our model performance in identifying and classifying malware reduced malware analysis time compared to human experts, demonstrating robustness and adaptability. The study highlighted the significance of hyper parameter tuning and suggested adaptive strategies for complex environments. Our RL based approach produced promising results and is validated as an alternative to traditional methods notably by offering continuous learning and adaptation to new and evolving malware threats which ultimately enhance the post incident forensics investigations.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/c4aafb184f285d004d8c8072b5d6408e876428e1.pdf",
      "citation_key": "dunsin2024e5w",
      "metadata": {
        "title": "Reinforcement Learning for an Efficient and Effective Malware Investigation during Cyber Incident Response",
        "authors": [
          "Dipo Dunsin",
          "Mohamed Chahine Ghanem",
          "Karim Ouazzane",
          "Vassil T. Vassilev"
        ],
        "published_date": "2024",
        "abstract": "This research focused on enhancing post-incident malware forensic investigation using reinforcement learning RL. We proposed an advanced MDP post incident malware forensics investigation model and framework to expedite post incident forensics. We then implement our RL Malware Investigation Model based on structured MDP within the proposed framework. To identify malware artefacts, the RL agent acquires and examines forensics evidence files, iteratively improving its capabilities using Q Table and temporal difference learning. The Q learning algorithm significantly improved the agent ability to identify malware. An epsilon greedy exploration strategy and Q learning updates enabled efficient learning and decision making. Our experimental testing revealed that optimal learning rates depend on the MDP environment complexity, with simpler environments benefiting from higher rates for quicker convergence and complex ones requiring lower rates for stability. Our model performance in identifying and classifying malware reduced malware analysis time compared to human experts, demonstrating robustness and adaptability. The study highlighted the significance of hyper parameter tuning and suggested adaptive strategies for complex environments. Our RL based approach produced promising results and is validated as an alternative to traditional methods notably by offering continuous learning and adaptation to new and evolving malware threats which ultimately enhance the post incident forensics investigations.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/c4aafb184f285d004d8c8072b5d6408e876428e1.pdf",
        "venue": "arXiv.org",
        "citationCount": 4,
        "score": 4.0,
        "summary": "This research focused on enhancing post-incident malware forensic investigation using reinforcement learning RL. We proposed an advanced MDP post incident malware forensics investigation model and framework to expedite post incident forensics. We then implement our RL Malware Investigation Model based on structured MDP within the proposed framework. To identify malware artefacts, the RL agent acquires and examines forensics evidence files, iteratively improving its capabilities using Q Table and temporal difference learning. The Q learning algorithm significantly improved the agent ability to identify malware. An epsilon greedy exploration strategy and Q learning updates enabled efficient learning and decision making. Our experimental testing revealed that optimal learning rates depend on the MDP environment complexity, with simpler environments benefiting from higher rates for quicker convergence and complex ones requiring lower rates for stability. Our model performance in identifying and classifying malware reduced malware analysis time compared to human experts, demonstrating robustness and adaptability. The study highlighted the significance of hyper parameter tuning and suggested adaptive strategies for complex environments. Our RL based approach produced promising results and is validated as an alternative to traditional methods notably by offering continuous learning and adaptation to new and evolving malware threats which ultimately enhance the post incident forensics investigations.",
        "keywords": []
      },
      "file_name": "c4aafb184f285d004d8c8072b5d6408e876428e1.pdf"
    },
    {
      "success": true,
      "doc_id": "194b44241c71b30d83da304a7b216f87",
      "summary": "Offline reinforcement learning (RL) is crucial for real-world applications where exploration can be costly or unsafe. However, offline learned policies are often suboptimal, and further online fine-tuning is required. In this paper, we tackle the fundamental dilemma of offline-to-online fine-tuning: if the agent remains pessimistic, it may fail to learn a better policy, while if it becomes optimistic directly, performance may suffer from a sudden drop. We show that Bayesian design principles are crucial in solving such a dilemma. Instead of adopting optimistic or pessimistic policies, the agent should act in a way that matches its belief in optimal policies. Such a probability-matching agent can avoid a sudden performance drop while still being guaranteed to find the optimal policy. Based on our theoretical findings, we introduce a novel algorithm that outperforms existing methods on various benchmarks, demonstrating the efficacy of our approach. Overall, the proposed approach provides a new perspective on offline-to-online RL that has the potential to enable more effective learning from offline data.",
      "intriguing_abstract": "Offline reinforcement learning (RL) is crucial for real-world applications where exploration can be costly or unsafe. However, offline learned policies are often suboptimal, and further online fine-tuning is required. In this paper, we tackle the fundamental dilemma of offline-to-online fine-tuning: if the agent remains pessimistic, it may fail to learn a better policy, while if it becomes optimistic directly, performance may suffer from a sudden drop. We show that Bayesian design principles are crucial in solving such a dilemma. Instead of adopting optimistic or pessimistic policies, the agent should act in a way that matches its belief in optimal policies. Such a probability-matching agent can avoid a sudden performance drop while still being guaranteed to find the optimal policy. Based on our theoretical findings, we introduce a novel algorithm that outperforms existing methods on various benchmarks, demonstrating the efficacy of our approach. Overall, the proposed approach provides a new perspective on offline-to-online RL that has the potential to enable more effective learning from offline data.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/e825e6325dfb5b93066817e7cc6b226ba7d3b799.pdf",
      "citation_key": "hu2024085",
      "metadata": {
        "title": "Bayesian Design Principles for Offline-to-Online Reinforcement Learning",
        "authors": [
          "Haotian Hu",
          "Yiqin Yang",
          "Jianing Ye",
          "Chengjie Wu",
          "Ziqing Mai",
          "Yujing Hu",
          "Tangjie Lv",
          "Changjie Fan",
          "Qianchuan Zhao",
          "Chongjie Zhang"
        ],
        "published_date": "2024",
        "abstract": "Offline reinforcement learning (RL) is crucial for real-world applications where exploration can be costly or unsafe. However, offline learned policies are often suboptimal, and further online fine-tuning is required. In this paper, we tackle the fundamental dilemma of offline-to-online fine-tuning: if the agent remains pessimistic, it may fail to learn a better policy, while if it becomes optimistic directly, performance may suffer from a sudden drop. We show that Bayesian design principles are crucial in solving such a dilemma. Instead of adopting optimistic or pessimistic policies, the agent should act in a way that matches its belief in optimal policies. Such a probability-matching agent can avoid a sudden performance drop while still being guaranteed to find the optimal policy. Based on our theoretical findings, we introduce a novel algorithm that outperforms existing methods on various benchmarks, demonstrating the efficacy of our approach. Overall, the proposed approach provides a new perspective on offline-to-online RL that has the potential to enable more effective learning from offline data.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/e825e6325dfb5b93066817e7cc6b226ba7d3b799.pdf",
        "venue": "International Conference on Machine Learning",
        "citationCount": 4,
        "score": 4.0,
        "summary": "Offline reinforcement learning (RL) is crucial for real-world applications where exploration can be costly or unsafe. However, offline learned policies are often suboptimal, and further online fine-tuning is required. In this paper, we tackle the fundamental dilemma of offline-to-online fine-tuning: if the agent remains pessimistic, it may fail to learn a better policy, while if it becomes optimistic directly, performance may suffer from a sudden drop. We show that Bayesian design principles are crucial in solving such a dilemma. Instead of adopting optimistic or pessimistic policies, the agent should act in a way that matches its belief in optimal policies. Such a probability-matching agent can avoid a sudden performance drop while still being guaranteed to find the optimal policy. Based on our theoretical findings, we introduce a novel algorithm that outperforms existing methods on various benchmarks, demonstrating the efficacy of our approach. Overall, the proposed approach provides a new perspective on offline-to-online RL that has the potential to enable more effective learning from offline data.",
        "keywords": []
      },
      "file_name": "e825e6325dfb5b93066817e7cc6b226ba7d3b799.pdf"
    },
    {
      "success": true,
      "doc_id": "07a5163f7b6018a423970b036141cb0d",
      "summary": "Resting-state functional magnetic resonance imaging (rs-fMRI) has gained attention as a reliable technique for investigating the intrinsic function patterns of the brain. It facilitates the extraction of functional connectivity networks (FCNs) that capture synchronized activity patterns among regions of interest (ROIs). Analyzing FCNs enables the identification of distinctive connectivity patterns associated with mild cognitive impairment (MCI). For MCI diagnosis, various sparse representation techniques have been introduced, including statistical- and deep learning-based methods. However, these methods face limitations due to their reliance on supervised learning schemes, which restrict the exploration necessary for probing novel solutions. To overcome such limitation, prior work has incorporated reinforcement learning (RL) to dynamically select ROIs, but effective exploration remains challenging due to the vast search space during training. To tackle this issue, in this study, we propose an advanced RL-based framework that utilizes a divide-and-conquer approach to decompose the FCN construction task into smaller sub-problems in a subject-specific manner, enabling efficient exploration under each sub-problem condition. Additionally, we leverage the learned value function to determine the sparsity level of FCNs, considering individual characteristics of FCNs. We validate the effectiveness of our proposed framework by demonstrating its superior performance in MCI diagnosis on publicly available cohort datasets.",
      "intriguing_abstract": "Resting-state functional magnetic resonance imaging (rs-fMRI) has gained attention as a reliable technique for investigating the intrinsic function patterns of the brain. It facilitates the extraction of functional connectivity networks (FCNs) that capture synchronized activity patterns among regions of interest (ROIs). Analyzing FCNs enables the identification of distinctive connectivity patterns associated with mild cognitive impairment (MCI). For MCI diagnosis, various sparse representation techniques have been introduced, including statistical- and deep learning-based methods. However, these methods face limitations due to their reliance on supervised learning schemes, which restrict the exploration necessary for probing novel solutions. To overcome such limitation, prior work has incorporated reinforcement learning (RL) to dynamically select ROIs, but effective exploration remains challenging due to the vast search space during training. To tackle this issue, in this study, we propose an advanced RL-based framework that utilizes a divide-and-conquer approach to decompose the FCN construction task into smaller sub-problems in a subject-specific manner, enabling efficient exploration under each sub-problem condition. Additionally, we leverage the learned value function to determine the sparsity level of FCNs, considering individual characteristics of FCNs. We validate the effectiveness of our proposed framework by demonstrating its superior performance in MCI diagnosis on publicly available cohort datasets.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/3416214ca1d4f790a048ece4229829333e836b4d.pdf",
      "citation_key": "ji2024gkw",
      "metadata": {
        "title": "Sparse Graph Representation Learning Based on Reinforcement Learning for Personalized Mild Cognitive Impairment (MCI) Diagnosis",
        "authors": [
          "Chang-Hoon Ji",
          "Dong-Hee Shin",
          "Young-Han Son",
          "Tae-Eui Kam"
        ],
        "published_date": "2024",
        "abstract": "Resting-state functional magnetic resonance imaging (rs-fMRI) has gained attention as a reliable technique for investigating the intrinsic function patterns of the brain. It facilitates the extraction of functional connectivity networks (FCNs) that capture synchronized activity patterns among regions of interest (ROIs). Analyzing FCNs enables the identification of distinctive connectivity patterns associated with mild cognitive impairment (MCI). For MCI diagnosis, various sparse representation techniques have been introduced, including statistical- and deep learning-based methods. However, these methods face limitations due to their reliance on supervised learning schemes, which restrict the exploration necessary for probing novel solutions. To overcome such limitation, prior work has incorporated reinforcement learning (RL) to dynamically select ROIs, but effective exploration remains challenging due to the vast search space during training. To tackle this issue, in this study, we propose an advanced RL-based framework that utilizes a divide-and-conquer approach to decompose the FCN construction task into smaller sub-problems in a subject-specific manner, enabling efficient exploration under each sub-problem condition. Additionally, we leverage the learned value function to determine the sparsity level of FCNs, considering individual characteristics of FCNs. We validate the effectiveness of our proposed framework by demonstrating its superior performance in MCI diagnosis on publicly available cohort datasets.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/3416214ca1d4f790a048ece4229829333e836b4d.pdf",
        "venue": "IEEE journal of biomedical and health informatics",
        "citationCount": 4,
        "score": 4.0,
        "summary": "Resting-state functional magnetic resonance imaging (rs-fMRI) has gained attention as a reliable technique for investigating the intrinsic function patterns of the brain. It facilitates the extraction of functional connectivity networks (FCNs) that capture synchronized activity patterns among regions of interest (ROIs). Analyzing FCNs enables the identification of distinctive connectivity patterns associated with mild cognitive impairment (MCI). For MCI diagnosis, various sparse representation techniques have been introduced, including statistical- and deep learning-based methods. However, these methods face limitations due to their reliance on supervised learning schemes, which restrict the exploration necessary for probing novel solutions. To overcome such limitation, prior work has incorporated reinforcement learning (RL) to dynamically select ROIs, but effective exploration remains challenging due to the vast search space during training. To tackle this issue, in this study, we propose an advanced RL-based framework that utilizes a divide-and-conquer approach to decompose the FCN construction task into smaller sub-problems in a subject-specific manner, enabling efficient exploration under each sub-problem condition. Additionally, we leverage the learned value function to determine the sparsity level of FCNs, considering individual characteristics of FCNs. We validate the effectiveness of our proposed framework by demonstrating its superior performance in MCI diagnosis on publicly available cohort datasets.",
        "keywords": []
      },
      "file_name": "3416214ca1d4f790a048ece4229829333e836b4d.pdf"
    },
    {
      "success": true,
      "doc_id": "e442688a5873d7fee7b49db7bedde260",
      "summary": "Exploration in reinforcement learning (RL) remains an open challenge. RL algorithms rely on observing rewards to train the agent, and if informative rewards are sparse the agent learns slowly or may not learn at all. To improve exploration and reward discovery, popular algorithms rely on optimism. But what if sometimes rewards are unobservable, e.g., situations of partial monitoring in bandits and the recent formalism of monitored Markov decision process? In this case, optimism can lead to suboptimal behavior that does not explore further to collapse uncertainty. With this paper, we present a novel exploration strategy that overcomes the limitations of existing methods and guarantees convergence to an optimal policy even when rewards are not always observable. We further propose a collection of tabular environments for benchmarking exploration in RL (with and without unobservable rewards) and show that our method outperforms existing ones.",
      "intriguing_abstract": "Exploration in reinforcement learning (RL) remains an open challenge. RL algorithms rely on observing rewards to train the agent, and if informative rewards are sparse the agent learns slowly or may not learn at all. To improve exploration and reward discovery, popular algorithms rely on optimism. But what if sometimes rewards are unobservable, e.g., situations of partial monitoring in bandits and the recent formalism of monitored Markov decision process? In this case, optimism can lead to suboptimal behavior that does not explore further to collapse uncertainty. With this paper, we present a novel exploration strategy that overcomes the limitations of existing methods and guarantees convergence to an optimal policy even when rewards are not always observable. We further propose a collection of tabular environments for benchmarking exploration in RL (with and without unobservable rewards) and show that our method outperforms existing ones.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/6bf550ce7f10a4347d448eb810a92e1a5cfffa4b.pdf",
      "citation_key": "parisi2024u3o",
      "metadata": {
        "title": "Beyond Optimism: Exploration With Partially Observable Rewards",
        "authors": [
          "Simone Parisi",
          "Alireza Kazemipour",
          "Michael Bowling"
        ],
        "published_date": "2024",
        "abstract": "Exploration in reinforcement learning (RL) remains an open challenge. RL algorithms rely on observing rewards to train the agent, and if informative rewards are sparse the agent learns slowly or may not learn at all. To improve exploration and reward discovery, popular algorithms rely on optimism. But what if sometimes rewards are unobservable, e.g., situations of partial monitoring in bandits and the recent formalism of monitored Markov decision process? In this case, optimism can lead to suboptimal behavior that does not explore further to collapse uncertainty. With this paper, we present a novel exploration strategy that overcomes the limitations of existing methods and guarantees convergence to an optimal policy even when rewards are not always observable. We further propose a collection of tabular environments for benchmarking exploration in RL (with and without unobservable rewards) and show that our method outperforms existing ones.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/6bf550ce7f10a4347d448eb810a92e1a5cfffa4b.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 3,
        "score": 3.0,
        "summary": "Exploration in reinforcement learning (RL) remains an open challenge. RL algorithms rely on observing rewards to train the agent, and if informative rewards are sparse the agent learns slowly or may not learn at all. To improve exploration and reward discovery, popular algorithms rely on optimism. But what if sometimes rewards are unobservable, e.g., situations of partial monitoring in bandits and the recent formalism of monitored Markov decision process? In this case, optimism can lead to suboptimal behavior that does not explore further to collapse uncertainty. With this paper, we present a novel exploration strategy that overcomes the limitations of existing methods and guarantees convergence to an optimal policy even when rewards are not always observable. We further propose a collection of tabular environments for benchmarking exploration in RL (with and without unobservable rewards) and show that our method outperforms existing ones.",
        "keywords": []
      },
      "file_name": "6bf550ce7f10a4347d448eb810a92e1a5cfffa4b.pdf"
    },
    {
      "success": true,
      "doc_id": "d800e8784b0e81f79c39359deb16d5ea",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/54cb726595cd8c03f59f49c9a97b76b4d3f932f2.pdf",
      "citation_key": "wang2024anu",
      "metadata": {
        "title": "Rethinking Exploration in Reinforcement Learning with Effective Metric-Based Exploration Bonus",
        "authors": [
          "Yiming Wang",
          "Kaiyan Zhao",
          "Furui Liu",
          "Leong Hou"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/54cb726595cd8c03f59f49c9a97b76b4d3f932f2.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 3,
        "score": 3.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "54cb726595cd8c03f59f49c9a97b76b4d3f932f2.pdf"
    },
    {
      "success": true,
      "doc_id": "b81cf8fbdd467baf0b60f00b66161977",
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **CITATION**: \\cite{wu2024mak}\n\n### Technical Paper Analysis:\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Model-based offline Reinforcement Learning (MBRL) algorithms suffer from *biased exploration* during policy optimization within the learned dynamics model. Existing k-step rollout methods, often relying on maximum entropy exploration, fail to effectively explore the state space, leading to biased training data distributions and impaired algorithm performance and stability.\n    *   **Importance & Challenge**: Offline RL is crucial for real-world applications where agent-environment interaction is costly or risky. MBRL aims to address limited data by learning environment dynamics. However, cumulative model errors during synthetic trajectory rollouts make exploration challenging. The problem is important because effective exploration is fundamental for learning optimal policies, but in an offline setting, direct exploration can be detrimental due to model inaccuracies.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: The work focuses on model-based offline RL, similar to methods that use learned dynamics models as proxies for the real environment. It builds upon the k-step rollout method commonly adopted from MBPO \\cite{wu2024mak}.\n    *   **Limitations of Previous Solutions**:\n        *   Previous MBRL studies (e.g., MOPO, COMBO, RAMBO) have not explicitly investigated or adequately addressed the exploration mechanism in k-step rollouts.\n        *   Approaches like COMBO and RAMBO used random rollout policies but did not emphasize the exploration problem itself \\cite{wu2024mak}.\n        *   TATU (Zhang et al. 2023) introduced trajectory truncation based on uncertainty but also overlooked the core exploration problem \\cite{wu2024mak}.\n        *   Existing maximum entropy exploration mechanisms (e.g., from SAC) in MBRL are shown to introduce bias, failing to explore the state space comprehensively (as illustrated in Figure 1 of the paper) \\cite{wu2024mak}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes **Offline Conservative ExplorAtioN (OCEAN)**, a novel plug-in rollout approach for model-based offline RL. OCEAN introduces additional exploration during trajectory rollouts while ensuring this exploration is *conservative* to mitigate the impact of model errors.\n    *   **Novelty/Difference**:\n        *   **Decoupling Exploration and Exploitation**: Unlike previous methods where maximizing reward/minimizing uncertainty and maximizing entropy have antagonistic effects leading to biased exploration, OCEAN explicitly introduces an exploration module.\n        *   **Conservative Exploration with Three Constraints**: This is the core innovation. OCEAN ensures exploration occurs only in regions where the model is reliable and selects conservative actions within those regions, while also limiting rollout length.\n        *   **Plug-in Nature**: OCEAN is designed to be seamlessly integrated with existing state-of-the-art MBRL algorithms like MOPO, COMBO, and RAMBO \\cite{wu2024mak}.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   **State Evaluation Constraint**: At each rollout step, OCEAN determines if a state is suitable for exploration by estimating its uncertainty (using an ensemble of dynamics models and sampling exploration actions with added Gaussian noise). Exploration is only allowed in states with low uncertainty, defined by a penalty threshold `uT` \\cite{wu2024mak}.\n        *   **Exploration Range Constraint**: For states deemed suitable, OCEAN selects a *conservative* transition from a set of explored actions. This can be done by choosing the action leading to minimum uncertainty, median uncertainty, or a random sample from the exploration actions \\cite{wu2024mak}.\n        *   **Trajectory Truncation Constraint**: Similar to TATU, but integrated with the conservative exploration strategy, rollouts are automatically truncated if the uncertainty of a state-action pair exceeds a threshold (` * umax`, where `umax` is the maximum uncertainty in the dataset) \\cite{wu2024mak}.\n    *   **System Design/Architectural Innovations**: OCEAN is presented as a modular \"plug-in\" method that modifies the policy rollout phase of existing MBRL frameworks, making it broadly applicable.\n    *   **Theoretical Insights/Analysis**: The paper highlights the antagonistic effect of reward/uncertainty maximization and entropy maximization in standard MBRL objectives, leading to biased exploration. OCEAN's design aims to address this by decoupling and conservatively guiding exploration.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: The authors incorporated OCEAN with MOPO (MOPO+OCEAN) and conducted extensive experiments on the D4RL MuJoCo benchmark. They compared MOPO+OCEAN against the original MOPO and several strong model-free offline RL baselines (CQL, TD3+BC, IQL) \\cite{wu2024mak}.\n    *   **Key Performance Metrics & Comparison Results**:\n        *   **Metrics**: Normalized average score on various D4RL MuJoCo \"-v2\" tasks (halfcheetah, hopper, walker2d) across different data qualities (medium, medium-replay, medium-expert).\n        *   **Results**: MOPO+OCEAN significantly improved performance over the original MOPO across most tasks.\n            *   It achieved the highest average score (92.5) across all tasks, outperforming MOPO (77.4), CQL (80.35), TD3+BC (75.71), and IQL (79.13) \\cite{wu2024mak}.\n            *   Notable improvements were observed on challenging tasks like `hopper-m` (MOPO: 31.4, MOPO+OCEAN: 87.4), `halfcheetah-m-e` (MOPO: 79.3, MOPO+OCEAN: 99.3), and `hopper-m-e` (MOPO: 82.5, MOPO+OCEAN: 110.0) \\cite{wu2024mak}.\n            *   The standard deviations for MOPO+OCEAN were generally lower, indicating improved stability of the learned policies \\cite{wu2024mak}.\n        *   Ablation experiments were also conducted to further validate the efficacy of the proposed method, though specific details are not provided in the given text \\cite{wu2024mak}.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**:\n        *   The effectiveness of OCEAN relies on the accuracy of the uncertainty estimation provided by the ensemble dynamics models.\n        *   The method introduces several hyperparameters (e.g., `uT`, ``, number of exploration actions `n`, variance `2` for exploration policy) that require careful tuning.\n        *   While mitigating model errors, direct exploration in offline settings inherently carries risks of encountering regions with high model uncertainty.\n    *   **Scope of Applicability**: Primarily applicable to model-based offline RL algorithms that utilize k-step rollouts for policy optimization. Validated on continuous control tasks in the D4RL MuJoCo benchmark.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: OCEAN is presented as the first work to specifically address the critical problem of *biased exploration* in model-based offline RL. By introducing a principled, conservative exploration strategy, it significantly enhances the performance and stability of existing MBRL algorithms, pushing the state-of-the-art in this domain \\cite{wu2024mak}.\n    *   **Potential Impact on Future Research**: This work opens new avenues for research into more sophisticated and adaptive conservative exploration strategies in offline settings. It highlights the importance of explicit exploration mechanisms even when learning from static datasets, potentially leading to more robust and generalizable offline RL agents for real-world applications where data collection is constrained.",
      "intriguing_abstract": "Model-based offline Reinforcement Learning (MBRL) holds immense promise for real-world applications, yet a critical, often overlooked, bottleneck hinders its potential: **biased exploration** during policy optimization within learned dynamics models. Existing k-step rollout methods, even with maximum entropy objectives, fail to comprehensively explore the state space, leading to suboptimal policies and instability.\n\nThis paper introduces **Offline Conservative ExplorAtioN (OCEAN)**, a pioneering plug-in approach that fundamentally redefines exploration in MBRL. OCEAN addresses the antagonistic effects of reward maximization and entropy by explicitly decoupling exploration and exploitation. It introduces a novel conservative exploration module guided by three constraints: state evaluation based on **uncertainty estimation**, conservative action selection within reliable model regions, and adaptive **trajectory truncation**. This ensures exploration is both effective and safe, mitigating cumulative model errors.\n\nIntegrated seamlessly with state-of-the-art MBRL algorithms, OCEAN achieves unprecedented performance and stability on the **D4RL MuJoCo benchmark**, significantly outperforming baselines like MOPO, CQL, and IQL. By being the first to explicitly tackle biased exploration in MBRL, OCEAN sets a new standard, paving the way for more robust and generalizable **offline RL** agents crucial for real-world deployment.",
      "keywords": [
        "Model-based offline Reinforcement Learning (MBRL)",
        "biased exploration",
        "Offline Conservative ExplorAtioN (OCEAN)",
        "conservative exploration",
        "decoupling exploration and exploitation",
        "dynamics model uncertainty",
        "k-step rollouts",
        "State Evaluation Constraint",
        "Exploration Range Constraint",
        "Trajectory Truncation Constraint",
        "plug-in rollout approach",
        "D4RL MuJoCo benchmark",
        "improved performance and stability"
      ],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/b44b0e8222021b51783c62b0bce071ef6fb3f12c.pdf",
      "citation_key": "wu2024mak",
      "metadata": {
        "title": "OCEAN-MBRL: Offline Conservative Exploration for Model-Based Offline Reinforcement Learning",
        "authors": [
          "Fan Wu",
          "Rui Zhang",
          "Qi Yi",
          "Yunkai Gao",
          "Jiaming Guo",
          "Shaohui Peng",
          "Siming Lan",
          "Husheng Han",
          "Yansong Pan",
          "Kaizhao Yuan",
          "Pengwei Jin",
          "Rui Chen",
          "Yunji Chen",
          "Ling Li"
        ],
        "published_date": "2024",
        "abstract": "Model-based offline reinforcement learning (RL) algorithms have emerged as a promising paradigm for offline RL.\nThese algorithms usually learn a dynamics model from a static dataset of transitions, use the model to generate synthetic trajectories, and perform conservative policy optimization within these trajectories. \nHowever, our observations indicate that policy optimization methods used in these model-based offline RL algorithms are not effective at exploring the learned model and induce biased exploration, which ultimately impairs the performance of the algorithm.\nTo address this issue, we propose Offline Conservative ExplorAtioN (OCEAN), a novel rollout approach to model-based offline RL.\nIn our method, we incorporate additional exploration techniques and introduce three conservative constraints based on uncertainty estimation to mitigate the potential impact of significant dynamic errors resulting from exploratory transitions. \nOur work is a plug-in method and can be combined with classical model-based RL algorithms, such as MOPO, COMBO, and RAMBO.\nExperiment results of our method on the D4RL MuJoCo benchmark show that OCEAN significantly improves the performance of existing algorithms.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/b44b0e8222021b51783c62b0bce071ef6fb3f12c.pdf",
        "venue": "AAAI Conference on Artificial Intelligence",
        "citationCount": 3,
        "score": 3.0,
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **CITATION**: \\cite{wu2024mak}\n\n### Technical Paper Analysis:\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Model-based offline Reinforcement Learning (MBRL) algorithms suffer from *biased exploration* during policy optimization within the learned dynamics model. Existing k-step rollout methods, often relying on maximum entropy exploration, fail to effectively explore the state space, leading to biased training data distributions and impaired algorithm performance and stability.\n    *   **Importance & Challenge**: Offline RL is crucial for real-world applications where agent-environment interaction is costly or risky. MBRL aims to address limited data by learning environment dynamics. However, cumulative model errors during synthetic trajectory rollouts make exploration challenging. The problem is important because effective exploration is fundamental for learning optimal policies, but in an offline setting, direct exploration can be detrimental due to model inaccuracies.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: The work focuses on model-based offline RL, similar to methods that use learned dynamics models as proxies for the real environment. It builds upon the k-step rollout method commonly adopted from MBPO \\cite{wu2024mak}.\n    *   **Limitations of Previous Solutions**:\n        *   Previous MBRL studies (e.g., MOPO, COMBO, RAMBO) have not explicitly investigated or adequately addressed the exploration mechanism in k-step rollouts.\n        *   Approaches like COMBO and RAMBO used random rollout policies but did not emphasize the exploration problem itself \\cite{wu2024mak}.\n        *   TATU (Zhang et al. 2023) introduced trajectory truncation based on uncertainty but also overlooked the core exploration problem \\cite{wu2024mak}.\n        *   Existing maximum entropy exploration mechanisms (e.g., from SAC) in MBRL are shown to introduce bias, failing to explore the state space comprehensively (as illustrated in Figure 1 of the paper) \\cite{wu2024mak}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes **Offline Conservative ExplorAtioN (OCEAN)**, a novel plug-in rollout approach for model-based offline RL. OCEAN introduces additional exploration during trajectory rollouts while ensuring this exploration is *conservative* to mitigate the impact of model errors.\n    *   **Novelty/Difference**:\n        *   **Decoupling Exploration and Exploitation**: Unlike previous methods where maximizing reward/minimizing uncertainty and maximizing entropy have antagonistic effects leading to biased exploration, OCEAN explicitly introduces an exploration module.\n        *   **Conservative Exploration with Three Constraints**: This is the core innovation. OCEAN ensures exploration occurs only in regions where the model is reliable and selects conservative actions within those regions, while also limiting rollout length.\n        *   **Plug-in Nature**: OCEAN is designed to be seamlessly integrated with existing state-of-the-art MBRL algorithms like MOPO, COMBO, and RAMBO \\cite{wu2024mak}.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   **State Evaluation Constraint**: At each rollout step, OCEAN determines if a state is suitable for exploration by estimating its uncertainty (using an ensemble of dynamics models and sampling exploration actions with added Gaussian noise). Exploration is only allowed in states with low uncertainty, defined by a penalty threshold `uT` \\cite{wu2024mak}.\n        *   **Exploration Range Constraint**: For states deemed suitable, OCEAN selects a *conservative* transition from a set of explored actions. This can be done by choosing the action leading to minimum uncertainty, median uncertainty, or a random sample from the exploration actions \\cite{wu2024mak}.\n        *   **Trajectory Truncation Constraint**: Similar to TATU, but integrated with the conservative exploration strategy, rollouts are automatically truncated if the uncertainty of a state-action pair exceeds a threshold (` * umax`, where `umax` is the maximum uncertainty in the dataset) \\cite{wu2024mak}.\n    *   **System Design/Architectural Innovations**: OCEAN is presented as a modular \"plug-in\" method that modifies the policy rollout phase of existing MBRL frameworks, making it broadly applicable.\n    *   **Theoretical Insights/Analysis**: The paper highlights the antagonistic effect of reward/uncertainty maximization and entropy maximization in standard MBRL objectives, leading to biased exploration. OCEAN's design aims to address this by decoupling and conservatively guiding exploration.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: The authors incorporated OCEAN with MOPO (MOPO+OCEAN) and conducted extensive experiments on the D4RL MuJoCo benchmark. They compared MOPO+OCEAN against the original MOPO and several strong model-free offline RL baselines (CQL, TD3+BC, IQL) \\cite{wu2024mak}.\n    *   **Key Performance Metrics & Comparison Results**:\n        *   **Metrics**: Normalized average score on various D4RL MuJoCo \"-v2\" tasks (halfcheetah, hopper, walker2d) across different data qualities (medium, medium-replay, medium-expert).\n        *   **Results**: MOPO+OCEAN significantly improved performance over the original MOPO across most tasks.\n            *   It achieved the highest average score (92.5) across all tasks, outperforming MOPO (77.4), CQL (80.35), TD3+BC (75.71), and IQL (79.13) \\cite{wu2024mak}.\n            *   Notable improvements were observed on challenging tasks like `hopper-m` (MOPO: 31.4, MOPO+OCEAN: 87.4), `halfcheetah-m-e` (MOPO: 79.3, MOPO+OCEAN: 99.3), and `hopper-m-e` (MOPO: 82.5, MOPO+OCEAN: 110.0) \\cite{wu2024mak}.\n            *   The standard deviations for MOPO+OCEAN were generally lower, indicating improved stability of the learned policies \\cite{wu2024mak}.\n        *   Ablation experiments were also conducted to further validate the efficacy of the proposed method, though specific details are not provided in the given text \\cite{wu2024mak}.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**:\n        *   The effectiveness of OCEAN relies on the accuracy of the uncertainty estimation provided by the ensemble dynamics models.\n        *   The method introduces several hyperparameters (e.g., `uT`, ``, number of exploration actions `n`, variance `2` for exploration policy) that require careful tuning.\n        *   While mitigating model errors, direct exploration in offline settings inherently carries risks of encountering regions with high model uncertainty.\n    *   **Scope of Applicability**: Primarily applicable to model-based offline RL algorithms that utilize k-step rollouts for policy optimization. Validated on continuous control tasks in the D4RL MuJoCo benchmark.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: OCEAN is presented as the first work to specifically address the critical problem of *biased exploration* in model-based offline RL. By introducing a principled, conservative exploration strategy, it significantly enhances the performance and stability of existing MBRL algorithms, pushing the state-of-the-art in this domain \\cite{wu2024mak}.\n    *   **Potential Impact on Future Research**: This work opens new avenues for research into more sophisticated and adaptive conservative exploration strategies in offline settings. It highlights the importance of explicit exploration mechanisms even when learning from static datasets, potentially leading to more robust and generalizable offline RL agents for real-world applications where data collection is constrained.",
        "keywords": [
          "Model-based offline Reinforcement Learning (MBRL)",
          "biased exploration",
          "Offline Conservative ExplorAtioN (OCEAN)",
          "conservative exploration",
          "decoupling exploration and exploitation",
          "dynamics model uncertainty",
          "k-step rollouts",
          "State Evaluation Constraint",
          "Exploration Range Constraint",
          "Trajectory Truncation Constraint",
          "plug-in rollout approach",
          "D4RL MuJoCo benchmark",
          "improved performance and stability"
        ],
        "paper_type": "based on the abstract and introduction:\n\n1.  **\"propose offline conservative exploration (ocean), a novel rollout approach to model-based offline rl.\"** - this is a direct indicator of presenting a new method or system.\n2.  **\"in our method, we incorporate additional exploration techniques and introduce three conservative constraints...\"** - describes the components and design of the proposed method.\n3.  **\"our work is a plug-in method and can be combined with classical model-based rl algorithms...\"** - highlights the technical nature and applicability of the proposed solution.\n4.  **\"experiment results of our method on the d4rl mujoco benchmark show that ocean significantly improves the performance of existing algorithms.\"** - while this indicates empirical evaluation, it is done to validate the *proposed* technical solution. the empirical results serve to demonstrate the effectiveness of the new method.\n5.  the introduction sets up a technical problem (limitations of existing policy optimization in model-based offline rl) that the proposed solution (ocean) aims to address. figure 1 also illustrates an observation that motivates the technical problem.\n\nthe primary focus is on proposing and describing a new algorithm/method (ocean) to solve an identified problem in model-based offline reinforcement learning. the experiments are conducted to demonstrate the efficacy of this new technical contribution.\n\ntherefore, this paper is best classified as **technical**."
      },
      "file_name": "b44b0e8222021b51783c62b0bce071ef6fb3f12c.pdf"
    },
    {
      "success": true,
      "doc_id": "1b5075113a8f516b73f88075d090c397",
      "summary": "The application of reinforcement learning (RL) to the field of autonomous robotics has high requirements about sample efficiency, since the agent expends for interaction with the environment. One method for sample efficiency is to extract knowledge from existing samples and used to exploration. Typical RL algorithms achieve exploration using task-specific knowledge or adding exploration noise. These methods are limited to current policy improvement level and lack of long-term planning. We propose a novel active exploration deep RL algorithm for the continuous action space problem named active exploration deep reinforcement learning (AEDRL). Our method uses the Gaussian process to model dynamic model, enabling the probability description of prediction sample. Action selection is formulated as the solution of the optimization problem. Thus, the optimization objective is specifically designed for selecting samples that can minimize the uncertainty of the dynamic model. Active exploration is achieved through long-term optimized action selection. This long-term considered action exploration method is more guidance for learning. Enable intelligent agents to explore more interesting action spaces. The proposed AEDRL algorithm is evaluated on several robotic control task including classic pendulum problem and five complex articulated robots. The AEDRL can learn a controller using fewer episodes and demonstrates performance and sample efficiency.",
      "intriguing_abstract": "The application of reinforcement learning (RL) to the field of autonomous robotics has high requirements about sample efficiency, since the agent expends for interaction with the environment. One method for sample efficiency is to extract knowledge from existing samples and used to exploration. Typical RL algorithms achieve exploration using task-specific knowledge or adding exploration noise. These methods are limited to current policy improvement level and lack of long-term planning. We propose a novel active exploration deep RL algorithm for the continuous action space problem named active exploration deep reinforcement learning (AEDRL). Our method uses the Gaussian process to model dynamic model, enabling the probability description of prediction sample. Action selection is formulated as the solution of the optimization problem. Thus, the optimization objective is specifically designed for selecting samples that can minimize the uncertainty of the dynamic model. Active exploration is achieved through long-term optimized action selection. This long-term considered action exploration method is more guidance for learning. Enable intelligent agents to explore more interesting action spaces. The proposed AEDRL algorithm is evaluated on several robotic control task including classic pendulum problem and five complex articulated robots. The AEDRL can learn a controller using fewer episodes and demonstrates performance and sample efficiency.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/5ac5f3827ee4c32cebadbc9de8a892853575efb9.pdf",
      "citation_key": "zhao2024714",
      "metadata": {
        "title": "Active Exploration Deep Reinforcement Learning for Continuous Action Space with Forward Prediction",
        "authors": [
          "Dongfang Zhao",
          "Huanshi Xu",
          "Zhang Xun"
        ],
        "published_date": "2024",
        "abstract": "The application of reinforcement learning (RL) to the field of autonomous robotics has high requirements about sample efficiency, since the agent expends for interaction with the environment. One method for sample efficiency is to extract knowledge from existing samples and used to exploration. Typical RL algorithms achieve exploration using task-specific knowledge or adding exploration noise. These methods are limited to current policy improvement level and lack of long-term planning. We propose a novel active exploration deep RL algorithm for the continuous action space problem named active exploration deep reinforcement learning (AEDRL). Our method uses the Gaussian process to model dynamic model, enabling the probability description of prediction sample. Action selection is formulated as the solution of the optimization problem. Thus, the optimization objective is specifically designed for selecting samples that can minimize the uncertainty of the dynamic model. Active exploration is achieved through long-term optimized action selection. This long-term considered action exploration method is more guidance for learning. Enable intelligent agents to explore more interesting action spaces. The proposed AEDRL algorithm is evaluated on several robotic control task including classic pendulum problem and five complex articulated robots. The AEDRL can learn a controller using fewer episodes and demonstrates performance and sample efficiency.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/5ac5f3827ee4c32cebadbc9de8a892853575efb9.pdf",
        "venue": "International Journal of Computational Intelligence Systems",
        "citationCount": 3,
        "score": 3.0,
        "summary": "The application of reinforcement learning (RL) to the field of autonomous robotics has high requirements about sample efficiency, since the agent expends for interaction with the environment. One method for sample efficiency is to extract knowledge from existing samples and used to exploration. Typical RL algorithms achieve exploration using task-specific knowledge or adding exploration noise. These methods are limited to current policy improvement level and lack of long-term planning. We propose a novel active exploration deep RL algorithm for the continuous action space problem named active exploration deep reinforcement learning (AEDRL). Our method uses the Gaussian process to model dynamic model, enabling the probability description of prediction sample. Action selection is formulated as the solution of the optimization problem. Thus, the optimization objective is specifically designed for selecting samples that can minimize the uncertainty of the dynamic model. Active exploration is achieved through long-term optimized action selection. This long-term considered action exploration method is more guidance for learning. Enable intelligent agents to explore more interesting action spaces. The proposed AEDRL algorithm is evaluated on several robotic control task including classic pendulum problem and five complex articulated robots. The AEDRL can learn a controller using fewer episodes and demonstrates performance and sample efficiency.",
        "keywords": []
      },
      "file_name": "5ac5f3827ee4c32cebadbc9de8a892853575efb9.pdf"
    },
    {
      "success": true,
      "doc_id": "267a9dc87b1a9e2a14d89971fa4e5035",
      "summary": "This paper presents an intelligent control scheme for multirotors, where accurate trajectory tracking, strong robustness and reliable generalization are guaranteed by the dual-feedback sliding-mode (DFSM) guided deep reinforcement learning (RL). Different from current solutions, the proposed method explores optimal learning strategy on the sliding surface according to the DFSM demonstrations, where the elegantly designed parallel evaluation takes full advantage of model knowledge and learning exploration. Specifically, the intelligent tracking control is achieved in a two-step design. First, the DFSM algorithm is designed for multirotors, where the linear and nonlinear feedback terms work cooperatively. Second, the DFSM-guided deep RL is put forward to achieve intelligent switching on the sliding surface, where position and velocity errors are both considered to generate accurate switching decisions. In the framework, explorations and the DFSM demonstrations are evaluated in parallel, where only the explorations that are better than the DFSM baseline, are kept for policy improvement. In this way, the DFSM algorithm keeps pushing the RL policy to explore better strategy, where the unavoidable bad experiences arisen from exploration are identified accurately. Practical comparative experimental results are included to verify the effectiveness of the proposed strategy. Note to PractitionersThis paper is motivated by the practical problem of controlling multirotor system in uncertain environments. Up until now, most existing approaches are proposed without taking full advantage of model knowledge and deep learning techniques simultaneously, which lacks of reliability in practical application. To deal with the problem, a new dual-feedback sliding-mode (DFSM) guided deep reinforcement learning (RL) strategy is proposed, where the dual feedback and guided RL are designed to achieve satisfactory tracking control and simultaneously handle uncertainties. Specifically, by introducing double-check framework, the RL strategy explores optimal switching policy on the sliding surface according to the DFSM demonstrations, guaranteeing strong robustness and reliable generalization of the obtained RL policy in uncertain environments. The key feature of the framework is that the DFSM-driven training guarantees practice-oriented tracking control in a DFSM-RL cooperative manner. Comparative experiments are implemented to verify the tracking performance of the proposed intelligent control strategy.",
      "intriguing_abstract": "This paper presents an intelligent control scheme for multirotors, where accurate trajectory tracking, strong robustness and reliable generalization are guaranteed by the dual-feedback sliding-mode (DFSM) guided deep reinforcement learning (RL). Different from current solutions, the proposed method explores optimal learning strategy on the sliding surface according to the DFSM demonstrations, where the elegantly designed parallel evaluation takes full advantage of model knowledge and learning exploration. Specifically, the intelligent tracking control is achieved in a two-step design. First, the DFSM algorithm is designed for multirotors, where the linear and nonlinear feedback terms work cooperatively. Second, the DFSM-guided deep RL is put forward to achieve intelligent switching on the sliding surface, where position and velocity errors are both considered to generate accurate switching decisions. In the framework, explorations and the DFSM demonstrations are evaluated in parallel, where only the explorations that are better than the DFSM baseline, are kept for policy improvement. In this way, the DFSM algorithm keeps pushing the RL policy to explore better strategy, where the unavoidable bad experiences arisen from exploration are identified accurately. Practical comparative experimental results are included to verify the effectiveness of the proposed strategy. Note to PractitionersThis paper is motivated by the practical problem of controlling multirotor system in uncertain environments. Up until now, most existing approaches are proposed without taking full advantage of model knowledge and deep learning techniques simultaneously, which lacks of reliability in practical application. To deal with the problem, a new dual-feedback sliding-mode (DFSM) guided deep reinforcement learning (RL) strategy is proposed, where the dual feedback and guided RL are designed to achieve satisfactory tracking control and simultaneously handle uncertainties. Specifically, by introducing double-check framework, the RL strategy explores optimal switching policy on the sliding surface according to the DFSM demonstrations, guaranteeing strong robustness and reliable generalization of the obtained RL policy in uncertain environments. The key feature of the framework is that the DFSM-driven training guarantees practice-oriented tracking control in a DFSM-RL cooperative manner. Comparative experiments are implemented to verify the tracking performance of the proposed intelligent control strategy.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/cce1245ba1ec154120b3b256faf7bf28f769b505.pdf",
      "citation_key": "hua2025fq5",
      "metadata": {
        "title": "A Novel Guided Deep Reinforcement Learning Tracking Control Strategy for Multirotors",
        "authors": [
          "Hean Hua",
          "Yaonan Wang",
          "Hang Zhong",
          "Hui Zhang",
          "Yongchun Fang"
        ],
        "published_date": "2025",
        "abstract": "This paper presents an intelligent control scheme for multirotors, where accurate trajectory tracking, strong robustness and reliable generalization are guaranteed by the dual-feedback sliding-mode (DFSM) guided deep reinforcement learning (RL). Different from current solutions, the proposed method explores optimal learning strategy on the sliding surface according to the DFSM demonstrations, where the elegantly designed parallel evaluation takes full advantage of model knowledge and learning exploration. Specifically, the intelligent tracking control is achieved in a two-step design. First, the DFSM algorithm is designed for multirotors, where the linear and nonlinear feedback terms work cooperatively. Second, the DFSM-guided deep RL is put forward to achieve intelligent switching on the sliding surface, where position and velocity errors are both considered to generate accurate switching decisions. In the framework, explorations and the DFSM demonstrations are evaluated in parallel, where only the explorations that are better than the DFSM baseline, are kept for policy improvement. In this way, the DFSM algorithm keeps pushing the RL policy to explore better strategy, where the unavoidable bad experiences arisen from exploration are identified accurately. Practical comparative experimental results are included to verify the effectiveness of the proposed strategy. Note to PractitionersThis paper is motivated by the practical problem of controlling multirotor system in uncertain environments. Up until now, most existing approaches are proposed without taking full advantage of model knowledge and deep learning techniques simultaneously, which lacks of reliability in practical application. To deal with the problem, a new dual-feedback sliding-mode (DFSM) guided deep reinforcement learning (RL) strategy is proposed, where the dual feedback and guided RL are designed to achieve satisfactory tracking control and simultaneously handle uncertainties. Specifically, by introducing double-check framework, the RL strategy explores optimal switching policy on the sliding surface according to the DFSM demonstrations, guaranteeing strong robustness and reliable generalization of the obtained RL policy in uncertain environments. The key feature of the framework is that the DFSM-driven training guarantees practice-oriented tracking control in a DFSM-RL cooperative manner. Comparative experiments are implemented to verify the tracking performance of the proposed intelligent control strategy.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/cce1245ba1ec154120b3b256faf7bf28f769b505.pdf",
        "venue": "IEEE Transactions on Automation Science and Engineering",
        "citationCount": 3,
        "score": 3.0,
        "summary": "This paper presents an intelligent control scheme for multirotors, where accurate trajectory tracking, strong robustness and reliable generalization are guaranteed by the dual-feedback sliding-mode (DFSM) guided deep reinforcement learning (RL). Different from current solutions, the proposed method explores optimal learning strategy on the sliding surface according to the DFSM demonstrations, where the elegantly designed parallel evaluation takes full advantage of model knowledge and learning exploration. Specifically, the intelligent tracking control is achieved in a two-step design. First, the DFSM algorithm is designed for multirotors, where the linear and nonlinear feedback terms work cooperatively. Second, the DFSM-guided deep RL is put forward to achieve intelligent switching on the sliding surface, where position and velocity errors are both considered to generate accurate switching decisions. In the framework, explorations and the DFSM demonstrations are evaluated in parallel, where only the explorations that are better than the DFSM baseline, are kept for policy improvement. In this way, the DFSM algorithm keeps pushing the RL policy to explore better strategy, where the unavoidable bad experiences arisen from exploration are identified accurately. Practical comparative experimental results are included to verify the effectiveness of the proposed strategy. Note to PractitionersThis paper is motivated by the practical problem of controlling multirotor system in uncertain environments. Up until now, most existing approaches are proposed without taking full advantage of model knowledge and deep learning techniques simultaneously, which lacks of reliability in practical application. To deal with the problem, a new dual-feedback sliding-mode (DFSM) guided deep reinforcement learning (RL) strategy is proposed, where the dual feedback and guided RL are designed to achieve satisfactory tracking control and simultaneously handle uncertainties. Specifically, by introducing double-check framework, the RL strategy explores optimal switching policy on the sliding surface according to the DFSM demonstrations, guaranteeing strong robustness and reliable generalization of the obtained RL policy in uncertain environments. The key feature of the framework is that the DFSM-driven training guarantees practice-oriented tracking control in a DFSM-RL cooperative manner. Comparative experiments are implemented to verify the tracking performance of the proposed intelligent control strategy.",
        "keywords": []
      },
      "file_name": "cce1245ba1ec154120b3b256faf7bf28f769b505.pdf"
    },
    {
      "success": true,
      "doc_id": "3053703bc2b2b8d5eeecbfcd60439504",
      "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) is a powerful paradigm for enhancing the reasoning ability of Large Language Models (LLMs). Yet current RLVR methods often explore poorly, leading to premature convergence and entropy collapse. To address this challenge, we introduce Curiosity-Driven Exploration (CDE), a framework that leverages the model's own intrinsic sense of curiosity to guide exploration. We formalize curiosity with signals from both the actor and the critic: for the actor, we use perplexity over its generated response, and for the critic, we use the variance of value estimates from a multi-head architecture. Both signals serve as an exploration bonus within the RLVR framework to guide the model. Our theoretical analysis shows that the actor-wise bonus inherently penalizes overconfident errors and promotes diversity among correct responses; moreover, we connect the critic-wise bonus to the well-established count-based exploration bonus in RL. Empirically, our method achieves an approximate +3 point improvement over standard RLVR using GRPO/PPO on AIME benchmarks. Further analysis identifies a calibration collapse mechanism within RLVR, shedding light on common LLM failure modes.",
      "intriguing_abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) is a powerful paradigm for enhancing the reasoning ability of Large Language Models (LLMs). Yet current RLVR methods often explore poorly, leading to premature convergence and entropy collapse. To address this challenge, we introduce Curiosity-Driven Exploration (CDE), a framework that leverages the model's own intrinsic sense of curiosity to guide exploration. We formalize curiosity with signals from both the actor and the critic: for the actor, we use perplexity over its generated response, and for the critic, we use the variance of value estimates from a multi-head architecture. Both signals serve as an exploration bonus within the RLVR framework to guide the model. Our theoretical analysis shows that the actor-wise bonus inherently penalizes overconfident errors and promotes diversity among correct responses; moreover, we connect the critic-wise bonus to the well-established count-based exploration bonus in RL. Empirically, our method achieves an approximate +3 point improvement over standard RLVR using GRPO/PPO on AIME benchmarks. Further analysis identifies a calibration collapse mechanism within RLVR, shedding light on common LLM failure modes.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/37fe2a997bf07a972473abd079d175335940e6bd.pdf",
      "citation_key": "dai2025h8g",
      "metadata": {
        "title": "CDE: Curiosity-Driven Exploration for Efficient Reinforcement Learning in Large Language Models",
        "authors": [
          "Runpeng Dai",
          "Linfeng Song",
          "Haolin Liu",
          "Zhenwen Liang",
          "Dian Yu",
          "Haitao Mi",
          "Zhaopeng Tu",
          "Rui Liu",
          "Tong Zheng",
          "Hongtu Zhu",
          "Dong Yu"
        ],
        "published_date": "2025",
        "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) is a powerful paradigm for enhancing the reasoning ability of Large Language Models (LLMs). Yet current RLVR methods often explore poorly, leading to premature convergence and entropy collapse. To address this challenge, we introduce Curiosity-Driven Exploration (CDE), a framework that leverages the model's own intrinsic sense of curiosity to guide exploration. We formalize curiosity with signals from both the actor and the critic: for the actor, we use perplexity over its generated response, and for the critic, we use the variance of value estimates from a multi-head architecture. Both signals serve as an exploration bonus within the RLVR framework to guide the model. Our theoretical analysis shows that the actor-wise bonus inherently penalizes overconfident errors and promotes diversity among correct responses; moreover, we connect the critic-wise bonus to the well-established count-based exploration bonus in RL. Empirically, our method achieves an approximate +3 point improvement over standard RLVR using GRPO/PPO on AIME benchmarks. Further analysis identifies a calibration collapse mechanism within RLVR, shedding light on common LLM failure modes.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/37fe2a997bf07a972473abd079d175335940e6bd.pdf",
        "venue": "",
        "citationCount": 3,
        "score": 3.0,
        "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) is a powerful paradigm for enhancing the reasoning ability of Large Language Models (LLMs). Yet current RLVR methods often explore poorly, leading to premature convergence and entropy collapse. To address this challenge, we introduce Curiosity-Driven Exploration (CDE), a framework that leverages the model's own intrinsic sense of curiosity to guide exploration. We formalize curiosity with signals from both the actor and the critic: for the actor, we use perplexity over its generated response, and for the critic, we use the variance of value estimates from a multi-head architecture. Both signals serve as an exploration bonus within the RLVR framework to guide the model. Our theoretical analysis shows that the actor-wise bonus inherently penalizes overconfident errors and promotes diversity among correct responses; moreover, we connect the critic-wise bonus to the well-established count-based exploration bonus in RL. Empirically, our method achieves an approximate +3 point improvement over standard RLVR using GRPO/PPO on AIME benchmarks. Further analysis identifies a calibration collapse mechanism within RLVR, shedding light on common LLM failure modes.",
        "keywords": []
      },
      "file_name": "37fe2a997bf07a972473abd079d175335940e6bd.pdf"
    },
    {
      "success": true,
      "doc_id": "02d5d6746325769b987768ec77f60faf",
      "summary": "The approach of directly combining clustering method and reinforcement learning (RL) will lead to encounter the issue where states may have different state transition processes under the same action, resulting in poor policy performance. To address this challenge with multi-dimensional continuous observation data, an improved reinforcement learning method based on unsupervised learning is proposed with a novel framework. Instead of dimensionality reduction methods, unsupervised clustering is employed to indirectly capture the underlying structure of the data. First, the proposed framework incorporates multi-dimensional information, including the current observation data, the next observation data and reward information, during the clustering process, leading to a more accurate and comprehensive low-dimensional discrete representation of the observation data while retaining preserving transition of Markov decision process. Second, by compressing the observation data into a well-defined state space, the resulting cluster labels serve as the low-dimensional discrete label-states for reinforcement learning to generate more effective and robust policies. Comparative analysis with state-of-the-art RL methods demonstrates that the improved RL methods base on framework achieves higher rewards, indicating its superior performance. Furthermore, the framework exhibits computational efficiency, as evidenced by its reasonable time complexity. This structural innovation allows for better exploration and exploitation of the transition, leading to improved policy performance in engineering applications.",
      "intriguing_abstract": "The approach of directly combining clustering method and reinforcement learning (RL) will lead to encounter the issue where states may have different state transition processes under the same action, resulting in poor policy performance. To address this challenge with multi-dimensional continuous observation data, an improved reinforcement learning method based on unsupervised learning is proposed with a novel framework. Instead of dimensionality reduction methods, unsupervised clustering is employed to indirectly capture the underlying structure of the data. First, the proposed framework incorporates multi-dimensional information, including the current observation data, the next observation data and reward information, during the clustering process, leading to a more accurate and comprehensive low-dimensional discrete representation of the observation data while retaining preserving transition of Markov decision process. Second, by compressing the observation data into a well-defined state space, the resulting cluster labels serve as the low-dimensional discrete label-states for reinforcement learning to generate more effective and robust policies. Comparative analysis with state-of-the-art RL methods demonstrates that the improved RL methods base on framework achieves higher rewards, indicating its superior performance. Furthermore, the framework exhibits computational efficiency, as evidenced by its reasonable time complexity. This structural innovation allows for better exploration and exploitation of the transition, leading to improved policy performance in engineering applications.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/e32e28a8a06739997957113b7fa1bd033f6801ba.pdf",
      "citation_key": "chang2024u7x",
      "metadata": {
        "title": "An Improved Reinforcement Learning Method Based on Unsupervised Learning",
        "authors": [
          "Xin Chang",
          "Yanbin Li",
          "Guanjie Zhang",
          "Donghui Liu",
          "Changjun Fu"
        ],
        "published_date": "2024",
        "abstract": "The approach of directly combining clustering method and reinforcement learning (RL) will lead to encounter the issue where states may have different state transition processes under the same action, resulting in poor policy performance. To address this challenge with multi-dimensional continuous observation data, an improved reinforcement learning method based on unsupervised learning is proposed with a novel framework. Instead of dimensionality reduction methods, unsupervised clustering is employed to indirectly capture the underlying structure of the data. First, the proposed framework incorporates multi-dimensional information, including the current observation data, the next observation data and reward information, during the clustering process, leading to a more accurate and comprehensive low-dimensional discrete representation of the observation data while retaining preserving transition of Markov decision process. Second, by compressing the observation data into a well-defined state space, the resulting cluster labels serve as the low-dimensional discrete label-states for reinforcement learning to generate more effective and robust policies. Comparative analysis with state-of-the-art RL methods demonstrates that the improved RL methods base on framework achieves higher rewards, indicating its superior performance. Furthermore, the framework exhibits computational efficiency, as evidenced by its reasonable time complexity. This structural innovation allows for better exploration and exploitation of the transition, leading to improved policy performance in engineering applications.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/e32e28a8a06739997957113b7fa1bd033f6801ba.pdf",
        "venue": "IEEE Access",
        "citationCount": 3,
        "score": 3.0,
        "summary": "The approach of directly combining clustering method and reinforcement learning (RL) will lead to encounter the issue where states may have different state transition processes under the same action, resulting in poor policy performance. To address this challenge with multi-dimensional continuous observation data, an improved reinforcement learning method based on unsupervised learning is proposed with a novel framework. Instead of dimensionality reduction methods, unsupervised clustering is employed to indirectly capture the underlying structure of the data. First, the proposed framework incorporates multi-dimensional information, including the current observation data, the next observation data and reward information, during the clustering process, leading to a more accurate and comprehensive low-dimensional discrete representation of the observation data while retaining preserving transition of Markov decision process. Second, by compressing the observation data into a well-defined state space, the resulting cluster labels serve as the low-dimensional discrete label-states for reinforcement learning to generate more effective and robust policies. Comparative analysis with state-of-the-art RL methods demonstrates that the improved RL methods base on framework achieves higher rewards, indicating its superior performance. Furthermore, the framework exhibits computational efficiency, as evidenced by its reasonable time complexity. This structural innovation allows for better exploration and exploitation of the transition, leading to improved policy performance in engineering applications.",
        "keywords": []
      },
      "file_name": "e32e28a8a06739997957113b7fa1bd033f6801ba.pdf"
    },
    {
      "success": true,
      "doc_id": "ee285fa6259a8b74719289fd3ccb76af",
      "summary": "Reinforcement Learning (RL) has been successful when the environment has specific objectives and boundaries. But with the emerging focus on open-world application which makes all or some of the rules or purpose go to naught, it makes traditional methods of RL a bit more difficult. This paper goes over various advancements and changes in Reinforcement Learning which can be employed for open-ended environments. Among the other strategies, hierarchical reinforcement learning, intrinsic motivation-based exploration, meta-learning and unsupervised skill acquisition are also among the ones that are examined. As a result, such a position based on the technology argues the promising future of open-ended methods for the management of complex problems and high level of uncertainty associated with the preset target or purpose. Also, we study cases in video games, robotics and autonomous systems, where RL is implemented in an open-ended and dynamic environment. We also outline existing limitations and perspectives, highlighting the need for more flexible methods and inter-scientific collaboration to fully realize RL's potential in open-ended contexts.",
      "intriguing_abstract": "Reinforcement Learning (RL) has been successful when the environment has specific objectives and boundaries. But with the emerging focus on open-world application which makes all or some of the rules or purpose go to naught, it makes traditional methods of RL a bit more difficult. This paper goes over various advancements and changes in Reinforcement Learning which can be employed for open-ended environments. Among the other strategies, hierarchical reinforcement learning, intrinsic motivation-based exploration, meta-learning and unsupervised skill acquisition are also among the ones that are examined. As a result, such a position based on the technology argues the promising future of open-ended methods for the management of complex problems and high level of uncertainty associated with the preset target or purpose. Also, we study cases in video games, robotics and autonomous systems, where RL is implemented in an open-ended and dynamic environment. We also outline existing limitations and perspectives, highlighting the need for more flexible methods and inter-scientific collaboration to fully realize RL's potential in open-ended contexts.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/c81a06446fa1df12cc043d9d9c8cfd5775090c59.pdf",
      "citation_key": "janjua2024yhk",
      "metadata": {
        "title": "Enhancing Scalability in Reinforcement Learning for Open Spaces",
        "authors": [
          "J. Janjua",
          "Shagufta Kousar",
          "Areeba Khan",
          "Anaum Ihsan",
          "Tahir Abbas",
          "Ali Q Saeed"
        ],
        "published_date": "2024",
        "abstract": "Reinforcement Learning (RL) has been successful when the environment has specific objectives and boundaries. But with the emerging focus on open-world application which makes all or some of the rules or purpose go to naught, it makes traditional methods of RL a bit more difficult. This paper goes over various advancements and changes in Reinforcement Learning which can be employed for open-ended environments. Among the other strategies, hierarchical reinforcement learning, intrinsic motivation-based exploration, meta-learning and unsupervised skill acquisition are also among the ones that are examined. As a result, such a position based on the technology argues the promising future of open-ended methods for the management of complex problems and high level of uncertainty associated with the preset target or purpose. Also, we study cases in video games, robotics and autonomous systems, where RL is implemented in an open-ended and dynamic environment. We also outline existing limitations and perspectives, highlighting the need for more flexible methods and inter-scientific collaboration to fully realize RL's potential in open-ended contexts.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/c81a06446fa1df12cc043d9d9c8cfd5775090c59.pdf",
        "venue": "2024 International Conference on Decision Aid Sciences and Applications (DASA)",
        "citationCount": 3,
        "score": 3.0,
        "summary": "Reinforcement Learning (RL) has been successful when the environment has specific objectives and boundaries. But with the emerging focus on open-world application which makes all or some of the rules or purpose go to naught, it makes traditional methods of RL a bit more difficult. This paper goes over various advancements and changes in Reinforcement Learning which can be employed for open-ended environments. Among the other strategies, hierarchical reinforcement learning, intrinsic motivation-based exploration, meta-learning and unsupervised skill acquisition are also among the ones that are examined. As a result, such a position based on the technology argues the promising future of open-ended methods for the management of complex problems and high level of uncertainty associated with the preset target or purpose. Also, we study cases in video games, robotics and autonomous systems, where RL is implemented in an open-ended and dynamic environment. We also outline existing limitations and perspectives, highlighting the need for more flexible methods and inter-scientific collaboration to fully realize RL's potential in open-ended contexts.",
        "keywords": []
      },
      "file_name": "c81a06446fa1df12cc043d9d9c8cfd5775090c59.pdf"
    },
    {
      "success": true,
      "doc_id": "851194f89730da48085cda11f89f3743",
      "summary": "Reinforcement learning (RL) is a widely used control technique that finds an optimal policy using the feedback of its actions. The search for the optimal policy requires that the system explores a broad region of the state space. This search puts at risk the safe operation, since some of the explored regions might be near the physical system limits. Implementing learning methods in industrial applications is limited because of its uncertain behavior when finding an optimal policy. This work proposes an RL control algorithm with a filter that supervises the safety of the exploration based on a nominal model. The performance of this safety filter is increased by modeling the uncertainty with a Gaussian process (GP) regression. This method is applied to the optimization of the management of a water distribution network (WDN) with an elevated reservoir; the management objectives are to regulate the tank filling while maintaining an adequate water turnover. The proposed method is validated in a laboratory setup that emulates the hydraulic features of a WDN.",
      "intriguing_abstract": "Reinforcement learning (RL) is a widely used control technique that finds an optimal policy using the feedback of its actions. The search for the optimal policy requires that the system explores a broad region of the state space. This search puts at risk the safe operation, since some of the explored regions might be near the physical system limits. Implementing learning methods in industrial applications is limited because of its uncertain behavior when finding an optimal policy. This work proposes an RL control algorithm with a filter that supervises the safety of the exploration based on a nominal model. The performance of this safety filter is increased by modeling the uncertainty with a Gaussian process (GP) regression. This method is applied to the optimization of the management of a water distribution network (WDN) with an elevated reservoir; the management objectives are to regulate the tank filling while maintaining an adequate water turnover. The proposed method is validated in a laboratory setup that emulates the hydraulic features of a WDN.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/66c5bed508f9dce55b2beeaaf36a29929c0bc2ac.pdf",
      "citation_key": "ledesma2024zzm",
      "metadata": {
        "title": "Water Age Control for Water Distribution Networks via Safe Reinforcement Learning",
        "authors": [
          "Jorge Val Ledesma",
          "Rafa Winiewski",
          "C. Kallese",
          "Agisilaos Tsouvalas"
        ],
        "published_date": "2024",
        "abstract": "Reinforcement learning (RL) is a widely used control technique that finds an optimal policy using the feedback of its actions. The search for the optimal policy requires that the system explores a broad region of the state space. This search puts at risk the safe operation, since some of the explored regions might be near the physical system limits. Implementing learning methods in industrial applications is limited because of its uncertain behavior when finding an optimal policy. This work proposes an RL control algorithm with a filter that supervises the safety of the exploration based on a nominal model. The performance of this safety filter is increased by modeling the uncertainty with a Gaussian process (GP) regression. This method is applied to the optimization of the management of a water distribution network (WDN) with an elevated reservoir; the management objectives are to regulate the tank filling while maintaining an adequate water turnover. The proposed method is validated in a laboratory setup that emulates the hydraulic features of a WDN.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/66c5bed508f9dce55b2beeaaf36a29929c0bc2ac.pdf",
        "venue": "IEEE Transactions on Control Systems Technology",
        "citationCount": 3,
        "score": 3.0,
        "summary": "Reinforcement learning (RL) is a widely used control technique that finds an optimal policy using the feedback of its actions. The search for the optimal policy requires that the system explores a broad region of the state space. This search puts at risk the safe operation, since some of the explored regions might be near the physical system limits. Implementing learning methods in industrial applications is limited because of its uncertain behavior when finding an optimal policy. This work proposes an RL control algorithm with a filter that supervises the safety of the exploration based on a nominal model. The performance of this safety filter is increased by modeling the uncertainty with a Gaussian process (GP) regression. This method is applied to the optimization of the management of a water distribution network (WDN) with an elevated reservoir; the management objectives are to regulate the tank filling while maintaining an adequate water turnover. The proposed method is validated in a laboratory setup that emulates the hydraulic features of a WDN.",
        "keywords": []
      },
      "file_name": "66c5bed508f9dce55b2beeaaf36a29929c0bc2ac.pdf"
    },
    {
      "success": true,
      "doc_id": "ebe467579bbddaf7e8bbaa927f890486",
      "summary": "This paper presents a new reinforcement learning approach that leverages failed experiences in sparse reward environments. Unlike traditional reinforcement learning methods that rely on successful experiences or expert demonstrations, the proposed approach utilizes failed experiences to guide the policy update during the learning process. The primary objective behind this work is to develop a method that can efficiently utilize failed experiences to guide the search direction as directional cues from successful experiences may be limited in sparse environments. To achieve this objective, we introduce a new objective function that aims to maximize the dissimilarity between the RL agent's actions and actions from failed experiences. This discrepancy serves as a valuable indicator to guide the agent's exploration. In other words, our method focuses on achieving the desired objectives by leveraging failed experiences to provide a significant opportunity for the agent to refine its policy. We further employ hindsight experience replay (HER) to enhance the directional search by creating and achieving potential subgoals that align with the primary objectives. To assess the effectiveness of our method, we conduct experiments on three sparse reward environments. Our findings demonstrate that the proposed approach significantly enhances the agent's learning efficiency and improves robustness to variations in demonstration quality compared to conventional reinforcement learning techniques.",
      "intriguing_abstract": "This paper presents a new reinforcement learning approach that leverages failed experiences in sparse reward environments. Unlike traditional reinforcement learning methods that rely on successful experiences or expert demonstrations, the proposed approach utilizes failed experiences to guide the policy update during the learning process. The primary objective behind this work is to develop a method that can efficiently utilize failed experiences to guide the search direction as directional cues from successful experiences may be limited in sparse environments. To achieve this objective, we introduce a new objective function that aims to maximize the dissimilarity between the RL agent's actions and actions from failed experiences. This discrepancy serves as a valuable indicator to guide the agent's exploration. In other words, our method focuses on achieving the desired objectives by leveraging failed experiences to provide a significant opportunity for the agent to refine its policy. We further employ hindsight experience replay (HER) to enhance the directional search by creating and achieving potential subgoals that align with the primary objectives. To assess the effectiveness of our method, we conduct experiments on three sparse reward environments. Our findings demonstrate that the proposed approach significantly enhances the agent's learning efficiency and improves robustness to variations in demonstration quality compared to conventional reinforcement learning techniques.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/2389fafc2a97e13fa810c4014babe73bd886c06f.pdf",
      "citation_key": "wu20248f9",
      "metadata": {
        "title": "Offline Reinforcement Learning with Failure Under Sparse Reward Environments",
        "authors": [
          "Mingkang Wu",
          "Umer Siddique",
          "Abhinav Sinha",
          "Yongcan Cao"
        ],
        "published_date": "2024",
        "abstract": "This paper presents a new reinforcement learning approach that leverages failed experiences in sparse reward environments. Unlike traditional reinforcement learning methods that rely on successful experiences or expert demonstrations, the proposed approach utilizes failed experiences to guide the policy update during the learning process. The primary objective behind this work is to develop a method that can efficiently utilize failed experiences to guide the search direction as directional cues from successful experiences may be limited in sparse environments. To achieve this objective, we introduce a new objective function that aims to maximize the dissimilarity between the RL agent's actions and actions from failed experiences. This discrepancy serves as a valuable indicator to guide the agent's exploration. In other words, our method focuses on achieving the desired objectives by leveraging failed experiences to provide a significant opportunity for the agent to refine its policy. We further employ hindsight experience replay (HER) to enhance the directional search by creating and achieving potential subgoals that align with the primary objectives. To assess the effectiveness of our method, we conduct experiments on three sparse reward environments. Our findings demonstrate that the proposed approach significantly enhances the agent's learning efficiency and improves robustness to variations in demonstration quality compared to conventional reinforcement learning techniques.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/2389fafc2a97e13fa810c4014babe73bd886c06f.pdf",
        "venue": "International Conference on Multimodal Interaction",
        "citationCount": 3,
        "score": 3.0,
        "summary": "This paper presents a new reinforcement learning approach that leverages failed experiences in sparse reward environments. Unlike traditional reinforcement learning methods that rely on successful experiences or expert demonstrations, the proposed approach utilizes failed experiences to guide the policy update during the learning process. The primary objective behind this work is to develop a method that can efficiently utilize failed experiences to guide the search direction as directional cues from successful experiences may be limited in sparse environments. To achieve this objective, we introduce a new objective function that aims to maximize the dissimilarity between the RL agent's actions and actions from failed experiences. This discrepancy serves as a valuable indicator to guide the agent's exploration. In other words, our method focuses on achieving the desired objectives by leveraging failed experiences to provide a significant opportunity for the agent to refine its policy. We further employ hindsight experience replay (HER) to enhance the directional search by creating and achieving potential subgoals that align with the primary objectives. To assess the effectiveness of our method, we conduct experiments on three sparse reward environments. Our findings demonstrate that the proposed approach significantly enhances the agent's learning efficiency and improves robustness to variations in demonstration quality compared to conventional reinforcement learning techniques.",
        "keywords": []
      },
      "file_name": "2389fafc2a97e13fa810c4014babe73bd886c06f.pdf"
    },
    {
      "success": true,
      "doc_id": "2531d5429b087ce3a071e48b74763e00",
      "summary": "Safe Reinforcement Learning (Safe RL) is one of the prevalently studied subcategories of trial-and-error-based methods with the intention to be deployed on real-world systems. In safe RL, the goal is to maximize reward performance while minimizing constraints, often achieved by setting bounds on constraint functions and utilizing the Lagrangian method. However, deploying Lagrangian-based safe RL in real-world scenarios is challenging due to the necessity of threshold fine-tuning, as imprecise adjustments may lead to suboptimal policy convergence. To mitigate this challenge, we propose a unified Lagrangian-based model-free architecture called Meta Soft Actor-Critic Lagrangian (Meta SAC-Lag). Meta SAC-Lag uses meta-gradient optimization to automatically update the safety-related hyperparameters. The proposed method is designed to address safe exploration and threshold adjustment with minimal hyperparameter tuning requirement. In our pipeline, the inner parameters are updated through the conventional formulation and the hyperparameters are adjusted using the meta-objectives which are defined based on the updated parameters. Our results show that the agent can reliably adjust the safety performance due to the relatively fast convergence rate of the safety threshold. We evaluate the performance of Meta SAC-Lag in five simulated environments against Lagrangian baselines, and the results demonstrate its capability to create synergy between parameters, yielding better or competitive results. Furthermore, we conduct a real-world experiment involving a robotic arm tasked with pouring coffee into a cup without spillage. Meta SAC-Lag is successfully trained to execute the task, while minimizing effort constraints. The success of Meta SAC-Lag in performing the experiment is intended to be a step toward practical deployment of safe RL algorithms to learn the control process of safety-critical real-world systems without explicit engineering.",
      "intriguing_abstract": "Safe Reinforcement Learning (Safe RL) is one of the prevalently studied subcategories of trial-and-error-based methods with the intention to be deployed on real-world systems. In safe RL, the goal is to maximize reward performance while minimizing constraints, often achieved by setting bounds on constraint functions and utilizing the Lagrangian method. However, deploying Lagrangian-based safe RL in real-world scenarios is challenging due to the necessity of threshold fine-tuning, as imprecise adjustments may lead to suboptimal policy convergence. To mitigate this challenge, we propose a unified Lagrangian-based model-free architecture called Meta Soft Actor-Critic Lagrangian (Meta SAC-Lag). Meta SAC-Lag uses meta-gradient optimization to automatically update the safety-related hyperparameters. The proposed method is designed to address safe exploration and threshold adjustment with minimal hyperparameter tuning requirement. In our pipeline, the inner parameters are updated through the conventional formulation and the hyperparameters are adjusted using the meta-objectives which are defined based on the updated parameters. Our results show that the agent can reliably adjust the safety performance due to the relatively fast convergence rate of the safety threshold. We evaluate the performance of Meta SAC-Lag in five simulated environments against Lagrangian baselines, and the results demonstrate its capability to create synergy between parameters, yielding better or competitive results. Furthermore, we conduct a real-world experiment involving a robotic arm tasked with pouring coffee into a cup without spillage. Meta SAC-Lag is successfully trained to execute the task, while minimizing effort constraints. The success of Meta SAC-Lag in performing the experiment is intended to be a step toward practical deployment of safe RL algorithms to learn the control process of safety-critical real-world systems without explicit engineering.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/b2d827c286e32dbf0739e8c796b119b1074809b4.pdf",
      "citation_key": "honari202473t",
      "metadata": {
        "title": "Meta SAC-Lag: Towards Deployable Safe Reinforcement Learning via MetaGradient-based Hyperparameter Tuning",
        "authors": [
          "Homayoun Honari",
          "Amir M. Soufi Enayati",
          "Mehran Ghafarian Tamizi",
          "Homayoun Najjaran"
        ],
        "published_date": "2024",
        "abstract": "Safe Reinforcement Learning (Safe RL) is one of the prevalently studied subcategories of trial-and-error-based methods with the intention to be deployed on real-world systems. In safe RL, the goal is to maximize reward performance while minimizing constraints, often achieved by setting bounds on constraint functions and utilizing the Lagrangian method. However, deploying Lagrangian-based safe RL in real-world scenarios is challenging due to the necessity of threshold fine-tuning, as imprecise adjustments may lead to suboptimal policy convergence. To mitigate this challenge, we propose a unified Lagrangian-based model-free architecture called Meta Soft Actor-Critic Lagrangian (Meta SAC-Lag). Meta SAC-Lag uses meta-gradient optimization to automatically update the safety-related hyperparameters. The proposed method is designed to address safe exploration and threshold adjustment with minimal hyperparameter tuning requirement. In our pipeline, the inner parameters are updated through the conventional formulation and the hyperparameters are adjusted using the meta-objectives which are defined based on the updated parameters. Our results show that the agent can reliably adjust the safety performance due to the relatively fast convergence rate of the safety threshold. We evaluate the performance of Meta SAC-Lag in five simulated environments against Lagrangian baselines, and the results demonstrate its capability to create synergy between parameters, yielding better or competitive results. Furthermore, we conduct a real-world experiment involving a robotic arm tasked with pouring coffee into a cup without spillage. Meta SAC-Lag is successfully trained to execute the task, while minimizing effort constraints. The success of Meta SAC-Lag in performing the experiment is intended to be a step toward practical deployment of safe RL algorithms to learn the control process of safety-critical real-world systems without explicit engineering.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/b2d827c286e32dbf0739e8c796b119b1074809b4.pdf",
        "venue": "IEEE/RJS International Conference on Intelligent RObots and Systems",
        "citationCount": 3,
        "score": 3.0,
        "summary": "Safe Reinforcement Learning (Safe RL) is one of the prevalently studied subcategories of trial-and-error-based methods with the intention to be deployed on real-world systems. In safe RL, the goal is to maximize reward performance while minimizing constraints, often achieved by setting bounds on constraint functions and utilizing the Lagrangian method. However, deploying Lagrangian-based safe RL in real-world scenarios is challenging due to the necessity of threshold fine-tuning, as imprecise adjustments may lead to suboptimal policy convergence. To mitigate this challenge, we propose a unified Lagrangian-based model-free architecture called Meta Soft Actor-Critic Lagrangian (Meta SAC-Lag). Meta SAC-Lag uses meta-gradient optimization to automatically update the safety-related hyperparameters. The proposed method is designed to address safe exploration and threshold adjustment with minimal hyperparameter tuning requirement. In our pipeline, the inner parameters are updated through the conventional formulation and the hyperparameters are adjusted using the meta-objectives which are defined based on the updated parameters. Our results show that the agent can reliably adjust the safety performance due to the relatively fast convergence rate of the safety threshold. We evaluate the performance of Meta SAC-Lag in five simulated environments against Lagrangian baselines, and the results demonstrate its capability to create synergy between parameters, yielding better or competitive results. Furthermore, we conduct a real-world experiment involving a robotic arm tasked with pouring coffee into a cup without spillage. Meta SAC-Lag is successfully trained to execute the task, while minimizing effort constraints. The success of Meta SAC-Lag in performing the experiment is intended to be a step toward practical deployment of safe RL algorithms to learn the control process of safety-critical real-world systems without explicit engineering.",
        "keywords": []
      },
      "file_name": "b2d827c286e32dbf0739e8c796b119b1074809b4.pdf"
    },
    {
      "success": true,
      "doc_id": "7aaa228fae7165bde943607ab75e0e7c",
      "summary": "Achieving fully autonomous driving in urban traffic scenarios is a significant challenge that necessitates balancing safety, efficiency, and compliance with traffic regulations. In this letter, we introduce a novel Curriculum Residual Hierarchical Reinforcement Learning (CR-HRL) framework. It integrates a rule-based planning model as a guiding mechanism, while a deep reinforcement learning algorithm generates supplementary residual strategies. This combination enables the RL agent to perform safe and efficient overtaking in complex traffic scenarios. Furthermore, we implement a detailed three-stage curriculum learning strategy that enhances the training process. By progressively increasing task complexity, the curriculum strategy effectively guides the exploration of autonomous vehicles and improves the reusability of sub-strategies. The effectiveness of the CR-HRL framework is confirmed through ablation experiments. Comparative experiments further highlight the superior efficiency and decision-making capabilities of our framework over traditional rule-based and RL baseline methods. Tests conducted with actual vehicles also demonstrate its practical applicability in real-world settings.",
      "intriguing_abstract": "Achieving fully autonomous driving in urban traffic scenarios is a significant challenge that necessitates balancing safety, efficiency, and compliance with traffic regulations. In this letter, we introduce a novel Curriculum Residual Hierarchical Reinforcement Learning (CR-HRL) framework. It integrates a rule-based planning model as a guiding mechanism, while a deep reinforcement learning algorithm generates supplementary residual strategies. This combination enables the RL agent to perform safe and efficient overtaking in complex traffic scenarios. Furthermore, we implement a detailed three-stage curriculum learning strategy that enhances the training process. By progressively increasing task complexity, the curriculum strategy effectively guides the exploration of autonomous vehicles and improves the reusability of sub-strategies. The effectiveness of the CR-HRL framework is confirmed through ablation experiments. Comparative experiments further highlight the superior efficiency and decision-making capabilities of our framework over traditional rule-based and RL baseline methods. Tests conducted with actual vehicles also demonstrate its practical applicability in real-world settings.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/fae722ae17483aeef3485f0177346ba3ce332ea9.pdf",
      "citation_key": "shi2024g6o",
      "metadata": {
        "title": "Task-Driven Autonomous Driving: Balanced Strategies Integrating Curriculum Reinforcement Learning and Residual Policy",
        "authors": [
          "Jiamin Shi",
          "Tangyike Zhang",
          "Ziqi Zong",
          "Shitao Chen",
          "Jingmin Xin",
          "Nanning Zheng"
        ],
        "published_date": "2024",
        "abstract": "Achieving fully autonomous driving in urban traffic scenarios is a significant challenge that necessitates balancing safety, efficiency, and compliance with traffic regulations. In this letter, we introduce a novel Curriculum Residual Hierarchical Reinforcement Learning (CR-HRL) framework. It integrates a rule-based planning model as a guiding mechanism, while a deep reinforcement learning algorithm generates supplementary residual strategies. This combination enables the RL agent to perform safe and efficient overtaking in complex traffic scenarios. Furthermore, we implement a detailed three-stage curriculum learning strategy that enhances the training process. By progressively increasing task complexity, the curriculum strategy effectively guides the exploration of autonomous vehicles and improves the reusability of sub-strategies. The effectiveness of the CR-HRL framework is confirmed through ablation experiments. Comparative experiments further highlight the superior efficiency and decision-making capabilities of our framework over traditional rule-based and RL baseline methods. Tests conducted with actual vehicles also demonstrate its practical applicability in real-world settings.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/fae722ae17483aeef3485f0177346ba3ce332ea9.pdf",
        "venue": "IEEE Robotics and Automation Letters",
        "citationCount": 3,
        "score": 3.0,
        "summary": "Achieving fully autonomous driving in urban traffic scenarios is a significant challenge that necessitates balancing safety, efficiency, and compliance with traffic regulations. In this letter, we introduce a novel Curriculum Residual Hierarchical Reinforcement Learning (CR-HRL) framework. It integrates a rule-based planning model as a guiding mechanism, while a deep reinforcement learning algorithm generates supplementary residual strategies. This combination enables the RL agent to perform safe and efficient overtaking in complex traffic scenarios. Furthermore, we implement a detailed three-stage curriculum learning strategy that enhances the training process. By progressively increasing task complexity, the curriculum strategy effectively guides the exploration of autonomous vehicles and improves the reusability of sub-strategies. The effectiveness of the CR-HRL framework is confirmed through ablation experiments. Comparative experiments further highlight the superior efficiency and decision-making capabilities of our framework over traditional rule-based and RL baseline methods. Tests conducted with actual vehicles also demonstrate its practical applicability in real-world settings.",
        "keywords": []
      },
      "file_name": "fae722ae17483aeef3485f0177346ba3ce332ea9.pdf"
    },
    {
      "success": true,
      "doc_id": "8d196bcb6f3ffa03e7b08722d8ee52b2",
      "summary": "In the realm of real-time environmental monitoring and hazard detection, multi-robot systems present a promising solution for exploring and mapping dynamic fields, particularly in scenarios where human intervention poses safety risks. This research introduces a strategy for path planning and control of a group of mobile sensing robots to efficiently explore and reconstruct a dynamic field consisting of multiple non-overlapping diffusion sources. Our approach integrates a reinforcement learning-based path planning algorithm to guide the multi-robot formation in identifying diffusion sources, with a clustering-based method for destination selection once a new source is detected, to enhance coverage and accelerate exploration in unknown environments. Simulation results and real-world laboratory experiments demonstrate the effectiveness of our approach in exploring and reconstructing dynamic fields. This study advances the field of multi-robot systems in environmental monitoring and has practical implications for rescue missions and field explorations.",
      "intriguing_abstract": "In the realm of real-time environmental monitoring and hazard detection, multi-robot systems present a promising solution for exploring and mapping dynamic fields, particularly in scenarios where human intervention poses safety risks. This research introduces a strategy for path planning and control of a group of mobile sensing robots to efficiently explore and reconstruct a dynamic field consisting of multiple non-overlapping diffusion sources. Our approach integrates a reinforcement learning-based path planning algorithm to guide the multi-robot formation in identifying diffusion sources, with a clustering-based method for destination selection once a new source is detected, to enhance coverage and accelerate exploration in unknown environments. Simulation results and real-world laboratory experiments demonstrate the effectiveness of our approach in exploring and reconstructing dynamic fields. This study advances the field of multi-robot systems in environmental monitoring and has practical implications for rescue missions and field explorations.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/dca6c5f8f6c64b4188212e16c2faf461ffd4e49a.pdf",
      "citation_key": "lu2025caz",
      "metadata": {
        "title": "Reinforcement learning-based dynamic field exploration and reconstruction using multi-robot systems for environmental monitoring",
        "authors": [
          "Thinh Lu",
          "Divyam Sobti",
          "Deepak Talwar",
          "Wencen Wu"
        ],
        "published_date": "2025",
        "abstract": "In the realm of real-time environmental monitoring and hazard detection, multi-robot systems present a promising solution for exploring and mapping dynamic fields, particularly in scenarios where human intervention poses safety risks. This research introduces a strategy for path planning and control of a group of mobile sensing robots to efficiently explore and reconstruct a dynamic field consisting of multiple non-overlapping diffusion sources. Our approach integrates a reinforcement learning-based path planning algorithm to guide the multi-robot formation in identifying diffusion sources, with a clustering-based method for destination selection once a new source is detected, to enhance coverage and accelerate exploration in unknown environments. Simulation results and real-world laboratory experiments demonstrate the effectiveness of our approach in exploring and reconstructing dynamic fields. This study advances the field of multi-robot systems in environmental monitoring and has practical implications for rescue missions and field explorations.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/dca6c5f8f6c64b4188212e16c2faf461ffd4e49a.pdf",
        "venue": "Frontiers Robotics AI",
        "citationCount": 3,
        "score": 3.0,
        "summary": "In the realm of real-time environmental monitoring and hazard detection, multi-robot systems present a promising solution for exploring and mapping dynamic fields, particularly in scenarios where human intervention poses safety risks. This research introduces a strategy for path planning and control of a group of mobile sensing robots to efficiently explore and reconstruct a dynamic field consisting of multiple non-overlapping diffusion sources. Our approach integrates a reinforcement learning-based path planning algorithm to guide the multi-robot formation in identifying diffusion sources, with a clustering-based method for destination selection once a new source is detected, to enhance coverage and accelerate exploration in unknown environments. Simulation results and real-world laboratory experiments demonstrate the effectiveness of our approach in exploring and reconstructing dynamic fields. This study advances the field of multi-robot systems in environmental monitoring and has practical implications for rescue missions and field explorations.",
        "keywords": []
      },
      "file_name": "dca6c5f8f6c64b4188212e16c2faf461ffd4e49a.pdf"
    },
    {
      "success": true,
      "doc_id": "6697a901cafc6f510e6362234e41cdda",
      "summary": "Reinforcement Learning (RL) has achieved great success in sequential decision-making problems but often requires extensive agent-environment interactions. To improve sample efficiency, methods like Reinforcement Learning from Expert Demonstrations (RLED) incorporate external expert demonstrations to aid agent exploration during the learning process. However, these demonstrations, typically collected from human users, are costly and thus often limited in quantity. Therefore, how to select the optimal set of human demonstrations that most effectively aids learning becomes a critical concern. This paper introduces EARLY (Episodic Active Learning from demonstration querY), an algorithm designed to enable a learning agent to generate optimized queries for expert demonstrations in a trajectory-based feature space. EARLY employs a trajectory-level estimate of uncertainty in the agents current policy to determine the optimal timing and content for feature-based queries. By querying episodic demonstrations instead of isolated state-action pairs, EARLY enhances the human teaching experience and achieves better learning performance. We validate the effectiveness of our method across three simulated navigation tasks of increasing difficulty. Results indicate that our method achieves expert-level performance in all three tasks, converging over 50% faster than other four baseline methods when demonstrations are generated by simulated oracle policies. A follow-up pilot user study (N = 18) further supports that our method maintains significantly better convergence with human expert demonstrators, while also providing a better user experience in terms of perceived task load and requiring significantly less human time.",
      "intriguing_abstract": "Reinforcement Learning (RL) has achieved great success in sequential decision-making problems but often requires extensive agent-environment interactions. To improve sample efficiency, methods like Reinforcement Learning from Expert Demonstrations (RLED) incorporate external expert demonstrations to aid agent exploration during the learning process. However, these demonstrations, typically collected from human users, are costly and thus often limited in quantity. Therefore, how to select the optimal set of human demonstrations that most effectively aids learning becomes a critical concern. This paper introduces EARLY (Episodic Active Learning from demonstration querY), an algorithm designed to enable a learning agent to generate optimized queries for expert demonstrations in a trajectory-based feature space. EARLY employs a trajectory-level estimate of uncertainty in the agents current policy to determine the optimal timing and content for feature-based queries. By querying episodic demonstrations instead of isolated state-action pairs, EARLY enhances the human teaching experience and achieves better learning performance. We validate the effectiveness of our method across three simulated navigation tasks of increasing difficulty. Results indicate that our method achieves expert-level performance in all three tasks, converging over 50% faster than other four baseline methods when demonstrations are generated by simulated oracle policies. A follow-up pilot user study (N = 18) further supports that our method maintains significantly better convergence with human expert demonstrators, while also providing a better user experience in terms of perceived task load and requiring significantly less human time.",
      "keywords": [],
      "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/9ac3f4e74e9837cb47f43211cf143d4c7115e7f1.pdf",
      "citation_key": "hou20248b2",
      "metadata": {
        "title": "``Give Me an Example Like This'': Episodic Active Reinforcement Learning from Demonstrations",
        "authors": [
          "Muhan Hou",
          "Koen V. Hindriks",
          "Guszti Eiben",
          "Kim Baraka"
        ],
        "published_date": "2024",
        "abstract": "Reinforcement Learning (RL) has achieved great success in sequential decision-making problems but often requires extensive agent-environment interactions. To improve sample efficiency, methods like Reinforcement Learning from Expert Demonstrations (RLED) incorporate external expert demonstrations to aid agent exploration during the learning process. However, these demonstrations, typically collected from human users, are costly and thus often limited in quantity. Therefore, how to select the optimal set of human demonstrations that most effectively aids learning becomes a critical concern. This paper introduces EARLY (Episodic Active Learning from demonstration querY), an algorithm designed to enable a learning agent to generate optimized queries for expert demonstrations in a trajectory-based feature space. EARLY employs a trajectory-level estimate of uncertainty in the agents current policy to determine the optimal timing and content for feature-based queries. By querying episodic demonstrations instead of isolated state-action pairs, EARLY enhances the human teaching experience and achieves better learning performance. We validate the effectiveness of our method across three simulated navigation tasks of increasing difficulty. Results indicate that our method achieves expert-level performance in all three tasks, converging over 50% faster than other four baseline methods when demonstrations are generated by simulated oracle policies. A follow-up pilot user study (N = 18) further supports that our method maintains significantly better convergence with human expert demonstrators, while also providing a better user experience in terms of perceived task load and requiring significantly less human time.",
        "file_path": "paper_data/Exploration_Methods_in_Reinforcement_Learning/info/9ac3f4e74e9837cb47f43211cf143d4c7115e7f1.pdf",
        "venue": "International Conference on Human-Agent Interaction",
        "citationCount": 3,
        "score": 3.0,
        "summary": "Reinforcement Learning (RL) has achieved great success in sequential decision-making problems but often requires extensive agent-environment interactions. To improve sample efficiency, methods like Reinforcement Learning from Expert Demonstrations (RLED) incorporate external expert demonstrations to aid agent exploration during the learning process. However, these demonstrations, typically collected from human users, are costly and thus often limited in quantity. Therefore, how to select the optimal set of human demonstrations that most effectively aids learning becomes a critical concern. This paper introduces EARLY (Episodic Active Learning from demonstration querY), an algorithm designed to enable a learning agent to generate optimized queries for expert demonstrations in a trajectory-based feature space. EARLY employs a trajectory-level estimate of uncertainty in the agents current policy to determine the optimal timing and content for feature-based queries. By querying episodic demonstrations instead of isolated state-action pairs, EARLY enhances the human teaching experience and achieves better learning performance. We validate the effectiveness of our method across three simulated navigation tasks of increasing difficulty. Results indicate that our method achieves expert-level performance in all three tasks, converging over 50% faster than other four baseline methods when demonstrations are generated by simulated oracle policies. A follow-up pilot user study (N = 18) further supports that our method maintains significantly better convergence with human expert demonstrators, while also providing a better user experience in terms of perceived task load and requiring significantly less human time.",
        "keywords": []
      },
      "file_name": "9ac3f4e74e9837cb47f43211cf143d4c7115e7f1.pdf"
    }
  ]
}