# A Comprehensive Literature Review with Self-Reflection

**Generated on:** 2025-10-07T12:31:38.574280
**Papers analyzed:** 240

## Papers Included:
1. c28ec2a40a2c77e20d64cf1c85dc931106df8e83.pdf [nair2017crs]
2. 0fcb2034e31a2bc2f12a2b1363d0d77baf445fdf.pdf [tang20166wr]
3. 45f573f302dc7e77cbc5d1a74ccbac3564bbebc8.pdf [lee2021qzk]
4. f8d8e192979ab5d8d593e76a0c4e2c7581778732.pdf [hu2020qwm]
5. 2470fcf0f89082de874ac9133ccb3a8667dd89a8.pdf [stadie20158af]
6. 68c108795deef06fa929d1f6e96b75dbf7ce8531.pdf [gupta2018rge]
7. 431dc05ac25510de6264084434254cca877f9ab3.pdf [thananjeyan2020d20]
8. 2c23085488337c4c1b5673b8d0f4ac95bda73529.pdf [wu2021r67]
9. 2064020586d5832b55f80a7dffea1fd90a5d94dd.pdf [conti2017cr2]
10. 1c35807e1a4c24e2013fa0a090cee9cc4716a5f5.pdf [seo2022cjf]
11. f3a63a840185fa8c5e0db4bbe12afa7d3de7d029.pdf [uchendu20221h1]
12. 52a6657cf1cb3cc847695a386bd65b5eea34bc13.pdf [li2020r8r]
13. 12075ea34f5fbe32ec5582786761ab34d401209b.pdf [yang2021ngm]
14. dc05886db1e6f17f4489d867477b38fe13e31783.pdf [lee2019hnz]
15. 6bf9b58b8f418fb2922762550fb78bb22c83c0f8.pdf [zhang2020o5t]
16. 6ce21379ffac786207632d16ea7d6e3eb150f910.pdf [chang20221gc]
17. 2e5dc4b028e670d1b9ac0b4e15580cdb7197cb68.pdf [yang2021psl]
18. f593dc96b20ce8427182e773e3b2192d707706a8.pdf [li2022ktf]
19. cc9f2fd320a279741403c4bfbeb91179803c428c.pdf [liang20226ix]
20. 3c3093f5f8e9601637612fcfb8f160f116fa30e4.pdf [hong20182pr]
21. b2004f4f19bc6ccae8e4afc554f39142870100f5.pdf [hansen2022jm2]
22. 61f371768cdc093828f432660e22f7a17f22e2af.pdf [pong2021i4o]
23. 1d6beec78e720beab5adb0a5fce6fa6b846aed1a.pdf [jia2021kxs]
24. ba44a95f1a8bc5765438d03c01137799e930c88d.pdf [zhang2022dgg]
25. d27edce419e1c731c0aeb9e1842d1e022b2cc6ab.pdf [dorfman20216nq]
26. 116fc10b798e3294b00e92f2b8053d0c89ad9182.pdf [tai2016bp8]
27. 0f810eb4777fd05317951ebaa7a3f5835ee84cf4.pdf [martin2017bgt]
28. 468a3e85a2da0908c6f371ad83f6bcf6b6fdf193.pdf [rckin2021yud]
29. 535d184eadf47fa17ce4073b6e2f180783e85300.pdf [zhelo2018wi8]
30. 0d82360a4da311a277607db355dda3f196e8eb3d.pdf [zhang2020bse]
31. f6b218453170edcbb51e49dd44ba2f83af53ef92.pdf [mavrin2019iqm]
32. 1f4484086d210a2c44efe5eef0a2b42647822abf.pdf [li2021w3q]
33. 04615a9955bce148aa7ba29e864389c26e10523a.pdf [schumacher2022x3f]
34. c64a99ec780d5e1c6ba242a82e72e96eb0c3ef8a.pdf [aubret2022inh]
35. 7d05987db045c56fa691da40e679cd328f0b68ef.pdf [yuan2020epo]
36. 399806e861a2ef960a81b37b593c2176a728c399.pdf [rezaeifar20211eu]
37. 174be0bacee04d9eb13a698d484ab5ae441c1100.pdf [talaat2022ywa]
38. 65587d4927fccc30788d3dfc9b639567721ff393.pdf [xin2022qcl]
39. 2fd42844445ec644c2c44c093c3522c08b59cb45.pdf [dang2022kwh]
40. 3e0925355554e3aeb99de8165c268582a82de3bb.pdf [raffin2020o1a]
41. 1fa2a04bffc18cfb46650a11ab0e5696d711f58f.pdf [liu2018jde]
42. 442e9f1e8f6218e68f944fd3028c5385691d4112.pdf [sun2022ul9]
43. a25b645c3d24f91164230a0ac5bb2d4ec88c1538.pdf [nikolov20184g9]
44. 2fc993c78cd84dafe4c36d2ac1cc25a6256aaa9d.pdf [qiao20220gx]
45. 0ba7f2e592dda172bc3d07f88cdcf2a57830deec.pdf [yu20222xi]
46. 09da56cd3bf72b632c43969be97874fa14a3765c.pdf [lambert202277x]
47. fc3b5dc5528e24dbbc8f4ec273e622ce40eec855.pdf [woczyk20220mn]
48. 46eb68c585bdb8a1051dfda98b4b35610301264f.pdf [qu2022uym]
49. 04efc9768a8e0c5f23b8c8504fb6db8803ffc071.pdf [sun2020zjg]
50. 4df0238d5bfe0ab4cb8eaba67f4388c4e02dc2c8.pdf [tao202294e]
51. fb3c6456708b0e143f545d77dc8ec804eb947395.pdf [houthooft2016yee]
52. 248a25d697fe0132840e9d03c00aefadf03408d8.pdf [shi20215fg]
53. 7d1722451bff3b95e1d082864bb9b8437a0ddf41.pdf [li2021l92]
54. b0d376434a528ee69d98174d75b4a571c53247ae.pdf [liu20220g4]
55. 2822e79cb293f54e05fd8a7cd2844cfcc683f5d8.pdf [hu20195n2]
56. fed0701afdfa6896057f7d04bd30ab1328eff110.pdf [wang2022boj]
57. 813f6e34feb3dc0346b6392d061af12ff186ba7e.pdf [yu20213c1]
58. 21828b3f05acffbfdf7de7259ad8b8ebd57fa351.pdf [zheng2022816]
59. 714b7c61d81687d093fd8b7d6d6737d582a4c9b7.pdf [yang2022mx5]
60. 1e650cb12aaad371a49b0c8c4514e1b988a5178c.pdf [yang2022fou]
61. 1d2acae1d631e932c7ebe96fad3c3b04909e5cee.pdf [liu2022uiv]
62. fe7382db243694c67c667cf2ec80072577d2372b.pdf [hou2021c2r]
63. ba0e111b711e9b577932a02c9e40bc44b56cb88d.pdf [lale2020xqs]
64. cf628a42ee56c8f1b858790822a2bc0a61a49110.pdf [otto2022qef]
65. fbcace16369032bb0292754bd78d03b68b554a95.pdf [lakhani2021217]
66. be33087668f98ac746e72999178d7641d27412f9.pdf [huang2020wll]
67. cae05340421a18c64ac0897d57bcdcc9a496a3b8.pdf [yuan2022hto]
68. cac7f83769836707b02adadb0cda8c791ca23c92.pdf [muzahid2022fyb]
69. f4712cdb025ba4ab879bb47d5cf693a4923f1532.pdf [zhang20229rg]
70. b1be7ce8c639291adf7663535f0451f9ac03ed55.pdf [sierragarca2020g35]
71. a064b8183d657178916ae21c43b5099bfef6804d.pdf [han20199g2]
72. bc98c81467ed3a6b21788f39c20cbe659014e551.pdf [wabersich2018t86]
73. 621d57c1243f055bc3850c1f3e38f351f53c947f.pdf [bourel2020tnm]
74. 9b5aa6a75d8e9f0e68d12e3b331acad6f32667d4.pdf [cheng20224w2]
75. 6c66fc8000da4d80bb57e60667e35a051016144a.pdf [ceusters2022drp]
76. 5fd3ce235f5fcebd3d2807f710b060add527183b.pdf [stanton20183fs]
77. a011901fd788fc5ad452dd3d88f9f0970ea547a8.pdf [cideron2020kdj]
78. 3efc894d0990faeb2f69194195d465ed64694104.pdf [liu2022nhx]
79. 46bd3c20e6f7a9b6e413ea9f3452965601fd6de9.pdf [cho2022o2c]
80. 1d4288c47a7802575d2a0d231d1283a4f225a85b.pdf [zhang2020xq9]
81. 1a39ad2d1553d07084b5e44a28878c8bb5018cef.pdf [song2021elb]
82. ecf5dc817fd6326e943b759c889d1285e673b24a.pdf [wang20229ce]
83. 02ad21eea9ec32783ba529487e74a76e85499a53.pdf [lin2022vqo]
84. a4a509d9019deac486087a0b10158ac115274de6.pdf [zhang2022egf]
85. a17a7256c04afee68f9aa0b7bfdc67fbca998b9c.pdf [zhou2022fny]
86. 69bdc99655204190697067c3da5296e544e6865d.pdf [yu2022bo5]
87. abaa1a3a8468473fd2827e49623eabc36ffaf8fe.pdf [xie2015vwy]
88. 2029ebd195491dd845e14866045225b238f6c392.pdf [zhang2019yjm]
89. c447bc7891c2a3a775af4f8fb94e34e7968c1cb7.pdf [wu2021mht]
90. 70e1d6b227fdd605fe61239a953e803df97e521d.pdf [fu20220cl]
91. a45df3efbd472d4d43ddc4072c61f7f674981ac6.pdf [mndezmolina2022ec5]
92. 5c0b56d9440ea70c79d1d0032d46c14e06f541f5.pdf [steinparz20220nl]
93. 0cdbd90989e5f60b6a42dae76b23bd489fcf65cc.pdf [rahman2022p7b]
94. 5f3b337e74618a2364778222162b13bd55a15e27.pdf [xu2022cgd]
95. b0c40766974df3eae8ff500379e66e5566cd16c9.pdf [lee2020k9k]
96. 4c4d41a977e6d8d82b6cc19ac28e50de3c814fd8.pdf [li2022ec4]
97. 3d3b0704c61d47c7bafb70ae2670b2786b8e4d81.pdf [wang2022t55]
98. 9e5fe2ba652774ba3b1127f626c192668a907132.pdf [whitney2021xlu]
99. 678cd30fac3fcf215d0e714936f6907ecc6d4361.pdf [suri20226rr]
100. 103f1674121780097f896ffe525bab2c6ae0bcdc.pdf [xin2020y4j]
101. de93c8aed64229571b03e40b36499d4f07ce875d.pdf [matheron2020zmh]
102. 9d1445f1845a2880ff9c752845660e9c294aa7b5.pdf [yang2022j0z]
103. 83f1343500c9f0df62da0d61736738d8d7a9bba0.pdf [wu2022sot]
104. 2e1ca97d8f7604e3b334ff4903bfa67267379317.pdf [kessler202295l]
105. 7551b7f7420087de59ad36e445cb3bdef9c382ee.pdf [raffin2020ka2]
106. b6ffc15ab55281dcf08b6a19d6f8bfcb190765b8.pdf [yang20206wi]
107. 06318722ebbac1d9b59e2408ecd3994eadb0ec6b.pdf [liu20228r4]
108. 1f577f6f0785c7967bef4ac6d0ab0370c2815b4d.pdf [kamalova2022jpm]
109. 33e3f13087abd5241d55523140720f5e684b7bee.pdf [zhang2022p0b]
110. 23e3f39ede3c45cb8cf456de1f56d7a8b8d4bc2e.pdf [li20227ss]
111. 97fe089acbe9d1f55753cd6b2ad6070e89521f6c.pdf [huang2022or8]
112. 6d97b81b3473492cb9986a63886cbb128496010c.pdf [modi2019fs3]
113. 807f377de905eda62e4cd2f0797153a59296adbb.pdf [shi20215ek]
114. f14645d3a0740504ee632ab06f045cceaa5297bc.pdf [zhang2021qq6]
115. f715558b65fd4f3c6966505c237d9a622947010b.pdf [yang2020dxb]
116. e0d4af45fa3073dbfa9b6e464fa88d52e6f06928.pdf [bing2019py7]
117. f80432fa03c91283cadbf6c262a7bc6e45f4edd3.pdf [zhang20192ef]
118. 117cd9e1dafc577e53e2d46897a784ed1e65996f.pdf [hu2020yhq]
119. 46cbd8acfacca5fc74b8d05bb06264aff0d82a4e.pdf [kumar20216sy]
120. 48de0dab9255ededce3cdaeac84f039d726f7f3e.pdf [asiain2018wxr]
121. 071de005741bd666c7a9ccf40d5ed1d502f5282b.pdf [li2019tj1]
122. 3f673101c2cac3b47639056e2988e018546c3c90.pdf [sun2020c1p]
123. 24107405a96a53d4c292b08608300a6c7e457ffe.pdf [su2020k2m]
124. 57f2811d8d8da81f2b4ed80097f5e47a49d76d19.pdf [liu2020o0c]
125. bd38cbbb346a347cb5b60ac4a133b3d73cb44e07.pdf [ball20235zm]
126. 1b1efa2f9731ab3801c46bfc877695d41e437406.pdf [meng2025l1q]
127. 08e84c939b88fc50aaa74ef76e202e61a1ad940b.pdf [dou2024kjg]
128. 5bac7d00035bc1e246a34f9ee3152b290f97bb92.pdf [lee202337c]
129. 043582a1ed2d2b7d08a804bafe9db188e9a65d96.pdf [ma2024r2p]
130. 139a84c6fc4887ce2374489d79af0df9e1e7e4d6.pdf [matthews20241yx]
131. 26662adf92cacf0810a14faa514360f270e97b53.pdf [xi2024e2i]
132. 914eaadede7a95116362cd6982321f93044b3b19.pdf [zhang20242te]
133. 7d6168fbd3ed72f9098573007f4b8c2ec9e576b9.pdf [xi2024tj9]
134. a694a3fb49f3c6e63e2fa5ddb72fe3aa490592f1.pdf [cheng2024vjq]
135. 25db1b77bc330476c3cf6ce43236404c578b4372.pdf [sun20238u5]
136. 4e98282f5f3f1a388b8d95380473d4ef4878266e.pdf [xu2023t6r]
137. abeb46288d537f98f76b979040a547ee81216377.pdf [zhang2025wku]
138. 4a3e88d203564e547f5fb3f3d816a0b381492eae.pdf [lu2025j7f]
139. 1a73038804052a40c12aae696848ece2168f6da7.pdf [jiang2023qmw]
140. 79f923d6575bd8253e2f0b70813caa61a870ccee.pdf [zhang20244ty]
141. ff87e30cb969d40b1d5c8ddc8932427793467f29.pdf [gu2024fu3]
142. aedd5c4bc11d8faf01e8456c91a183f2f76e5778.pdf [ma2024b33]
143. e0bf76edd8e6b793b81c16a915ede48e377ebab6.pdf [yan2024p3y]
144. 5f5e9ec8bcc5e83eefecc0565bdc004f929f4723.pdf [chen2023ymk]
145. d60df0754df6ccb14c563f07f865f391da3cba2d.pdf [li2024drs]
146. 03c7f9a6b743eb1c9127b8d4552147b2b1a0c1e1.pdf [huang202366f]
147. 7bd4edf878976d329f326f3a12675a66cbc075e9.pdf [jin2024035]
148. c90aa0f206c6fd41c490c142f63f7ba046cae6b7.pdf [ishfaq20235fo]
149. 53db22a9d4ae77dd8218ba867184898adc84d1d1.pdf [guo2024sba]
150. fbc0b5e1b822796d7ae97268def2e0993b5da644.pdf [surina2025smk]
151. 97c7066b53b208b24001ad0dcc49a851fcf13bfd.pdf [celik202575j]
152. 81fdf2b3db2b3e7be90b51866a31b73587eecd30.pdf [hua2023omp]
153. e4fef8d5864c5468100ca167639ef3fa374c0442.pdf [sukhija2024zz8]
154. 7da3eebf5c2f481bc38ae6115cec3fc71dc87f2b.pdf [hsu2024tqd]
155. 2e6534fbeb932fc058b9f865c0b25a3f201a0704.pdf [wang20248rm]
156. 01936f6df3c760d23df237d8d15cb7faadce9520.pdf [ghamari2024bbm]
157. c225c6e24526c160bbceaaa36b447df9d8e95f13.pdf [zhang2024ppn]
158. c734971c6000e3f2769ab5165d00816af80dd76f.pdf [dai2024x3l]
159. ed8ea3d06c173849f02ee8afcf8db07df0f31261.pdf [shuai2025fq3]
160. dcbafb5ec43572d6ca170d86569d09a5dd40a8f8.pdf [stolz20240y2]
161. 492f441bc6fdbb5f4b9273197ae563126439abeb.pdf [tappler2024nm1]
162. 4d3ffd8feca81d750aa30122b49cc1e874e70c1a.pdf [rimon20243o6]
163. 40d1e0a1e8a861305f9354be747620782fc203ce.pdf [terven202599m]
164. 736c35ed3e8e86ce99e02ce117dba7ee77e60dda.pdf [hsiao2024wps]
165. a69adfee23dbc90b2292499ffb1bf9fbd7d656c5.pdf [kakooee2024w9m]
166. e5d6cca71ea0fb216a25f86e96d3480886fdba27.pdf [rafailov2024wtw]
167. 5ecf53ab083f72f10421225a7dde25eb51cb6b22.pdf [goldie2024cuf]
168. 4186f74da6d793c91c5ca4d8a10cf2f8638eade6.pdf [coelho2024oa6]
169. eb6b79b00d43fa03945ae2477c60e79edaf72d28.pdf [huang2024nh4]
170. b093a3fa79512c48524f81c754bddec7b16afb17.pdf [cho2023z4l]
171. 9a67ff1d46d691f7741822d7a13587a517b1be14.pdf [shi20258tu]
172. 134acf45a016fced57cc8f8e171eeb6e0015b0b8.pdf [li2024ge1]
173. a9c896060fa85f01f289baaad346e98e94dbed4c.pdf [ghasemi2024j43]
174. 68de2fdc9b516eba96b9726dfa0e77ee66336605.pdf [liu2023729]
175. 01f35fa70fc881ab80206121738380c57f8d2074.pdf [shang202305k]
176. 5dce3f2cfe018ada4bebeaa15aac6fe1d01b18ba.pdf [liu2024xkk]
177. 390ba2d438a33bf0add55e0e7b2831fc034db41f.pdf [kantaros2024sgn]
178. 085dddfa3105967d4b9f09b1cd0fa7725779faf1.pdf [li2024zix]
179. 99d0760520eaa1dc81b5531cd45101e00474bbd1.pdf [ang2024t27]
180. 3da8d2e6a0f8d5d62cbaae18235235483144c6a0.pdf [kim2024qde]
181. 086a4d45a9deef5a10ee4febcd4c92c95a6305de.pdf [ma2024jej]
182. 3dee83a4b0fadde414e00ff350940303eb859be1.pdf [ge20243g0]
183. 22c1ec46a81e9db6194b8784f4fe431f71953757.pdf [lu2024ush]
184. 9c7209f3d6b9bf362ea4d34730f34cf8511967f4.pdf [yuan2023m1m]
185. 200726cba07dec06a56ff46aa38836e9730a23a2.pdf [zheng2023u9k]
186. 3c483c11f5fd9234576a93aea71a7ec1435b8514.pdf [wang20241f3]
187. d06737f9395e592f35ef251e09bea1c18037b096.pdf [mahankali20248dx]
188. 3ef01975a45bcf78120d76c1bc73e8ec12e213bf.pdf [yoon2024lff]
189. 39d2839aa4c3d8c0e64553891fe98ba261703154.pdf [ishfaq20245to]
190. cb7ea0176eec1f809f3bc3a34aeb279d44dda612.pdf [santi2024hct]
191. b31c76815615c16cc8505dbb38d2921f921c029d.pdf [pham2024j80]
192. c1844cda42b3732a5576d05bb6e007eb1db00919.pdf [ding2023whs]
193. 8af5e79310ec1d8529eba38705e5f29dce789b00.pdf [malaiyappan20245bh]
194. 5221ba291d5901f950220f50d289d5e01d81b0c4.pdf [gan2023o50]
195. 8ae6c7cff8bb2d766f5f9d3585cd262032378b33.pdf [zhao2023cay]
196. dad0fcbc6f2c9b7c4d7b7b1d4513d94d57ea63c6.pdf [xu2023m9r]
197. ae47e4e2a4b749543c4de8624049c1d368cbc859.pdf [khlif2023zg3]
198. c6ed1f7f478f9d004d5f6b9783df79f82d0d1464.pdf [sreedharan2023nae]
199. 2807f9c666335946113fb11dccadf36f8d78b772.pdf [guo20233sd]
200. 9eed36fb9b9bbb97579953d5e303dc0cf6c70a58.pdf [zhang2023wqi]
201. 9c9a527ad885e664a31f0b70ec5f838d0c6ebfa6.pdf [beikmohammadi2023v6w]
202. 8ca9a74503c240b2746e351995ee0415657f1cd0.pdf [yang2023n56]
203. 06ffbf40c29daa4f6b4c5025d96ab1c27c0ea117.pdf [alvarez2023v09]
204. 7f437f4af59ff994d97482ee1c12aaeb4b310e85.pdf [li2023kgk]
205. 839395c4823ac8fff990485e7ce54e53c94bae6b.pdf [zi20238ug]
206. 7825ea27ec1762f6ac41347603535500bcd121f7.pdf [yang2023w3h]
207. 859172d8cb31ddebd9b18e3a7a7d982fab1d1341.pdf [sun20219kr]
208. f58ada4680d374f5dba96b5e7fcb5b7be11a6acc.pdf [guo2022y6b]
209. aa65704a16138790678e2b9b59ae679b6c9353d7.pdf [mazumder2022deb]
210. bd8aaab29fa16f40ef016393ba7ca30127abab58.pdf [oh2022cei]
211. a6c5e49425ac01c534d9663a4453b28f6dc76f7c.pdf [vidakovi2020q23]
212. d8bc75fbcfdecd5cd1952524d3e07db13722f5ff.pdf [sun2024kxq]
213. 17682d6f13dcac703092e0f1500a4197e46bbf2c.pdf [yu2024x53]
214. 11c34b84c3ad6587529517c32923c446797c63e6.pdf [wang2024htz]
215. d929e0bc4e30910527a714f239b5e5b37a7ec6ad.pdf [ding20246hx]
216. 516c6ab3feab17bc158f12ef6768b26c603566b8.pdf [yang2024yh9]
217. f5e09973834f852237a7d9db6583c7e6615a907d.pdf [afroosheh2024id4]
218. b43f8bacd80f32734cdff4f9d8c79e397872aed6.pdf [dong2025887]
219. 8357670aac3c98a71b454ab5bca89558f265369d.pdf [zhu2024sb0]
220. 58db2247187ac01acabc1c2fa02f9b189772729e.pdf [xiang2024qhz]
221. e401ba782c2da93959582295089d3f04a051d6c1.pdf [qi2024hxq]
222. afa538f59cf2996837863be60a34eef5271a5ee9.pdf [zhang2024wgo]
223. 88f98e3629a7aef2312f9e214ed2a75672113e4f.pdf [sun2024edc]
224. c4aafb184f285d004d8c8072b5d6408e876428e1.pdf [dunsin2024e5w]
225. e825e6325dfb5b93066817e7cc6b226ba7d3b799.pdf [hu2024085]
226. 3416214ca1d4f790a048ece4229829333e836b4d.pdf [ji2024gkw]
227. 6bf550ce7f10a4347d448eb810a92e1a5cfffa4b.pdf [parisi2024u3o]
228. 54cb726595cd8c03f59f49c9a97b76b4d3f932f2.pdf [wang2024anu]
229. b44b0e8222021b51783c62b0bce071ef6fb3f12c.pdf [wu2024mak]
230. 5ac5f3827ee4c32cebadbc9de8a892853575efb9.pdf [zhao2024714]
231. cce1245ba1ec154120b3b256faf7bf28f769b505.pdf [hua2025fq5]
232. 37fe2a997bf07a972473abd079d175335940e6bd.pdf [dai2025h8g]
233. e32e28a8a06739997957113b7fa1bd033f6801ba.pdf [chang2024u7x]
234. c81a06446fa1df12cc043d9d9c8cfd5775090c59.pdf [janjua2024yhk]
235. 66c5bed508f9dce55b2beeaaf36a29929c0bc2ac.pdf [ledesma2024zzm]
236. 2389fafc2a97e13fa810c4014babe73bd886c06f.pdf [wu20248f9]
237. b2d827c286e32dbf0739e8c796b119b1074809b4.pdf [honari202473t]
238. fae722ae17483aeef3485f0177346ba3ce332ea9.pdf [shi2024g6o]
239. dca6c5f8f6c64b4188212e16c2faf461ffd4e49a.pdf [lu2025caz]
240. 9ac3f4e74e9837cb47f43211cf143d4c7115e7f1.pdf [hou20248b2]

## Literature Review

### Introduction to Exploration in Reinforcement Learning

\section{Introduction to Exploration in Reinforcement Learning}
\label{sec:introduction_to_exploration_in_reinforcement_learning}



\subsection{The Exploration-Exploitation Dilemma}
\label{sec:1_1_the_exploration-exploitation_dilemma}

The exploration-exploitation dilemma represents a foundational and ubiquitous challenge in Reinforcement Learning (RL), fundamentally shaping how an autonomous agent acquires knowledge and optimizes its behavior within an uncertain environment [sutton2018reinforcement]. At its core, this dilemma encapsulates the inherent tension between two conflicting objectives: an agent must judiciously decide whether to \textit{exploit} its current understanding to select actions that are known to yield high immediate rewards, or to \textit{explore} unknown actions and states, which, despite immediate uncertainty, may lead to the discovery of significantly greater long-term rewards [robbins1952some, sutton2018reinforcement]. This delicate balance is critical for the design of effective RL agents, as an imbalanced trade-off can profoundly impact learning efficiency and the ultimate optimality of the learned policy.

To formally illustrate this core dilemma, consider the classic multi-armed bandit (MAB) problem, a simplified yet powerful model for sequential decision-making under uncertainty [robbins1952some]. In a MAB setting, an agent faces $K$ distinct "arms," each associated with an unknown probability distribution over rewards. At each time step $t$, the agent selects an arm $a_t \in \{1, \dots, K\}$ and observes a reward $r_t \sim \mathcal{D}_{a_t}$. The objective is to maximize the cumulative reward over a sequence of $T$ pulls, or equivalently, to minimize "regret." Regret, formally defined as the difference between the expected cumulative reward of an optimal policy (always pulling the best arm) and the agent's actual cumulative reward, is given by $R_T = \sum_{t=1}^T (\mu^* - \mu_{a_t})$, where $\mu^*$ is the expected reward of the optimal arm and $\mu_{a_t}$ is the expected reward of the arm chosen at time $t$ [lai1985asymptotically]. Pulling an arm with a high estimated mean reward is an act of exploitation. Conversely, choosing an arm that has been sampled infrequently, or whose reward distribution is highly uncertain, constitutes exploration. Pure exploitation risks converging to a suboptimal arm if initial samples were misleading, while pure exploration, such as random arm selection, wastes opportunities to collect known rewards, leading to high regret [auer2002finite]. Modern approaches to MABs continue to refine this balance, often by maximizing information gain or balancing intrinsic and extrinsic rewards to achieve sublinear regret [sukhija2024zz8].

Extending from the simplified MAB framework to full Markov Decision Processes (MDPs), the exploration-exploitation dilemma becomes significantly more intricate. In MDPs, an agent's action not only yields an immediate reward but also transitions the agent to a new state, influencing future rewards. The state space can be high-dimensional or continuous, and the environmental dynamics are often unknown. This means that the value of an action is not independent but depends on the subsequent states it might lead to. Furthermore, rewards can be delayed, making it challenging to attribute positive or negative outcomes to specific exploratory actions taken much earlier in a sequence. Partial observability, where the agent does not have complete information about the true state of the environment, further exacerbates the challenge, as "unknown" can refer to truly unvisited states or merely unobserved aspects of the current state [parisi2024u3o]. These complexities necessitate more sophisticated, directed exploration strategies that move beyond simple random action selection.

Conceptual approaches to managing this dilemma in complex RL settings generally fall into several categories. One prominent principle is "optimism in the face of uncertainty" (OFU), where agents are incentivized to explore states or actions about which their knowledge is limited, by optimistically assuming these unknown options will yield maximal rewards [auer2002finite, brafman2002r]. This encourages the agent to gather sufficient data to reduce uncertainty, as exemplified by algorithms like UCB (Upper Confidence Bound) in MABs and R-Max in MDPs [auer2002finite, brafman2002r]. Another approach involves explicitly valuing the discovery of novel states or actions, often by assigning intrinsic rewards for visiting less-frequented regions of the state-action space. Furthermore, information-theoretic methods guide exploration by maximizing the expected reduction in uncertainty about the environment's dynamics or the optimal policy, thereby prioritizing experiences that yield the most significant knowledge gain [sukhija2024zz8]. These conceptual frameworks highlight the diverse ways researchers have sought to formalize and address the fundamental trade-off.

This subsection has established the exploration-exploitation dilemma as a central challenge in RL, defining its core tenets, illustrating it with the MAB problem, and extending its complexities to MDPs. It has also introduced foundational conceptual approaches to its management. The subsequent sections of this literature review will systematically delve into the diverse methodologies developed to tackle this challenge, tracing their evolution from foundational heuristic approaches and theoretically grounded algorithms to advanced intrinsic motivation techniques, adaptive strategies, and their specialized applications. Each method offers a unique perspective on how to navigate this central trade-off, collectively advancing the field towards more intelligent and autonomous learning agents.
\subsection{Historical Context and Evolution of Exploration Research}
\label{sec:1_2_historical_context__and__evolution_of_exploration_research}

The fundamental challenge of balancing exploration and exploitation, where an agent must gather sufficient information about its environment to make optimal decisions while simultaneously leveraging its current knowledge, has been a cornerstone of Reinforcement Learning (RL) since its inception [Sutton1998]. The historical trajectory of exploration research reflects a continuous effort to overcome the inherent complexities of unknown environments, evolving from rudimentary heuristics in simplified settings to sophisticated, scalable strategies for complex, high-dimensional domains. This evolution has been driven by both conceptual shifts and technological advancements, particularly the rise of deep learning.

The intellectual origins of principled exploration can be traced to the multi-armed bandit (MAB) problem, the simplest setting where the exploration-exploitation dilemma is starkly presented. In MABs, an agent chooses from a set of actions (arms) with unknown reward distributions, aiming to maximize cumulative reward. Foundational algorithms like Upper Confidence Bound (UCB) [Auer2002] emerged from the principle of "optimism in the face of uncertainty" (OFU), which encourages agents to explore actions whose true values are uncertain by optimistically assuming they might yield high rewards. Concurrently, Bayesian approaches, notably Thompson Sampling [Thompson1933], provided a probabilistic framework for exploration by sampling from a posterior distribution over action values, effectively balancing exploration and exploitation by favoring actions with high potential given current uncertainty. These early MAB solutions laid the theoretical bedrock for later exploration strategies in full Markov Decision Processes (MDPs) [parisi2024u3o].

Transitioning to full MDPs, early RL exploration strategies were often heuristic. The $\epsilon$-greedy policy, a direct extension of MAB ideas, randomly selects actions with a small probability ($\epsilon$) to discover new state-action values, while otherwise exploiting current knowledge [Sutton1998]. While simple, its undirected nature proved inefficient in larger state spaces. To address this, early research in tabular settings introduced explicit exploration bonuses, often count-based, which incentivized agents to visit less-frequented states or take less-tried actions by augmenting the reward signal. These methods aimed for broader state space coverage, but their direct reliance on explicit state-action enumeration rendered them impractical for environments with continuous or very large discrete state spaces, foreshadowing the pervasive "curse of dimensionality." The role of dynamic programming in this era was primarily to compute optimal policies *given* a known model, highlighting the critical need for effective exploration to *learn* such models in unknown environments.

A significant conceptual shift emerged with the development of theoretically grounded exploration methods for finite MDPs, aiming to provide provable guarantees on learning efficiency. The OFU principle, originating from MABs, became a cornerstone, leading to algorithms like UCRL2 [Strehl2009]. Within the PAC-MDP (Probably Approximately Correct-MDP) framework, these methods provided provable bounds on the number of samples required to learn a near-optimal policy. While offering robust theoretical guarantees on sample complexity, these approaches were computationally demanding and inherently limited by their reliance on explicit state-action enumeration, making them largely inapplicable to the high-dimensional problems prevalent in modern RL. Efforts continued to refine these theoretical methods, with works like UCRL3 [bourel2020tnm] introducing tighter concentration inequalities and adaptive computation of transition supports to improve practical efficiency within the theoretical paradigm. However, the fundamental trade-off persisted: rigorous theoretical guarantees often came at the cost of scalability, necessitating a paradigm shift for complex, real-world domains.

The advent of deep learning provided a new impetus for exploration research, shifting the focus towards scalable solutions for complex, high-dimensional observation spaces where traditional counting or explicit model-learning became intractable. This era saw the emergence of intrinsic motivation, a paradigm where agents generate internal reward signals for novel or surprising experiences, independent of external task rewards. The challenge was to generalize the notion of "visitation count" or "novelty" to continuous, high-dimensional state spaces. Breakthroughs included the concept of pseudo-counts [Bellemare2016], derived from density models, which allowed agents to quantify novelty in high-dimensional state spaces. Complementing this, [martin2017bgt] introduced $\phi$-pseudocounts, generalizing state visit-counts by exploiting the same feature representation used for value function approximation, thereby rewarding exploration in feature space rather than the untransformed state space. Similarly, [Tang2016] demonstrated that even a simple generalization of classic count-based methods, using hash codes to count state occurrences, could achieve competitive performance in deep RL benchmarks, underscoring the power of novelty-seeking in complex environments.

Despite their success, early intrinsic motivation methods faced challenges, such as the "noisy TV problem," where agents might be perpetually distracted by uncontrollable stochastic elements that offer no meaningful learning. To address this, [Pathak2017] introduced the Intrinsic Curiosity Module (ICM), which generates intrinsic rewards based on the agent's prediction error of its own actions' consequences in a learned feature space, thereby focusing exploration on controllable and learnable aspects of the environment. Further refining this, [Burda2019] proposed Random Network Distillation (RND), a simpler and more robust intrinsic reward mechanism that measures prediction error between a policy network and a fixed, randomly initialized target network. RND proved less susceptible to environmental stochasticity, providing a more reliable signal for true novelty and significantly improving exploration stability. Simultaneously, efforts were made to bridge the gap between theoretical rigor and deep RL scalability. For instance, [nikolov20184g9] introduced Information-Directed Sampling (IDS) for deep Q-learning, providing a tractable approximation that explicitly accounts for both parametric uncertainty and heteroscedastic observation noise, further enhancing the theoretical grounding of exploration in deep RL.

In conclusion, the evolution of exploration research in RL reflects a continuous effort to overcome the inherent challenges of unknown environments. This journey moved from foundational theoretical guarantees in simplified settings like MABs, through heuristic and theoretically-grounded methods for tabular MDPs, and ultimately to practical, scalable, and increasingly robust solutions for complex, high-dimensional domains enabled by deep learning. This historical trajectory, marked by shifts from heuristic to theoretically grounded, and then to intrinsically motivated and uncertainty-aware deep learning approaches, sets the stage for the detailed methodological discussions of advanced and adaptive strategies that follow.
\subsection{Motivation for Effective Exploration}
\label{sec:1_3_motivation_for_effective_exploration}


Effective exploration stands as a cornerstone for the successful application of Reinforcement Learning (RL) agents, particularly in the intricate and often unforgiving landscape of real-world environments. Its crucial role stems from the inherent challenges that frequently impede learning: the scarcity of informative reward signals, the vastness of high-dimensional state and action spaces, the prevalence of deceptive local optima that can trap agents in suboptimal behaviors, and the critical need for policies that generalize beyond training data while remaining sample-efficient. Without well-designed exploration strategies, RL agents risk converging to inferior policies, failing to discover optimal solutions, or even remaining inert in complex tasks, thereby underscoring the continuous drive for innovation in this research domain.

One of the primary motivations for robust exploration arises from the pervasive issue of **sparse reward signals** and the **curse of dimensionality**. In many practical scenarios, agents receive meaningful feedback only after achieving specific, often distant, goals. This sparsity makes naive trial-and-error exploration highly inefficient or even impossible. Early attempts to address this, such as model-based planning with Dyna-Q [Sutton90] and subsequent works [Kaelbling93,Singh2004], aimed to improve sample efficiency by leveraging learned environmental models to generate synthetic experiences. Similarly, count-based methods [Thrun92] offered explicit incentives for visiting less-known states. However, these foundational approaches often struggled to scale to high-dimensional or continuous state spaces, where explicit state enumeration or precise model learning becomes intractable. This limitation fundamentally motivated the development of **intrinsic motivation** techniques, which empower agents to generate their own internal reward signals, independent of external task rewards. Pioneering ideas of "curiosity" based on prediction error [Schmidhuber91,Schmidhuber97] and learning progress [Singh00] provided conceptual breakthroughs. These concepts were subsequently scaled to deep RL through methods like pseudo-counts for high-dimensional spaces [Bellemare16], exploration bonuses derived from deep predictive models [stadie20158af], and hash-based count methods [tang20166wr]. Such advancements have proven vital for tasks requiring extensive discovery in visually rich or complex environments, such as mapless navigation for mobile robots [zhelo2018wi8].

Beyond simply finding rewards, effective exploration is essential to overcome the **peril of deceptive local optima**. Many environments present reward landscapes with numerous suboptimal peaks, where a greedy agent might become trapped, never discovering the globally optimal policy. This challenge necessitates exploration strategies that actively encourage agents to venture beyond seemingly good but ultimately inferior solutions. Information-theoretic approaches, such as Variational Information Maximizing Exploration (VIME) [Houthooft2016], address this by guiding agents to states that maximize information gain about the environment's dynamics, thereby reducing uncertainty and facilitating escape from local traps. More recent intrinsic motivation methods, like the Intrinsic Curiosity Module (ICM) [Pathak17] and Random Network Distillation (RND) [Burda18], provide robust novelty signals by rewarding prediction errors in learned feature spaces or against random targets. These methods are crucial for preventing agents from being perpetually attracted to uninformative stochastic elements (the "noisy TV" problem) that could otherwise lead to spurious curiosity and suboptimal convergence. Furthermore, approaches like diversity-driven exploration [hong20182pr] and novelty-seeking in evolutionary strategies [conti2017cr2] explicitly aim to prevent policies from being trapped in local optima by encouraging a wide range of behaviors and exploring diverse solution spaces. Robust policy optimization techniques, such as Robust Policy Optimization (RPO) [rahman2022p7b], also contribute by maintaining sufficient policy entropy, ensuring continuous and broad exploration to avoid premature convergence.

The imperative for **sample efficiency** and **generalization** further underscores the critical need for sophisticated exploration. In real-world applications, data collection can be costly, time-consuming, or even risky, making inefficient exploration a significant bottleneck. Moreover, agents must often perform reliably in environments that differ subtly or significantly from their training conditions. This motivates exploration strategies that not only discover optimal policies quickly but also acquire knowledge transferable to unseen scenarios. For instance, [whitney2021xlu] highlights that simple policy entropy maximization is often insufficient for sample-efficient continuous control, advocating for decoupled exploration and exploitation policies. Leveraging existing data, such as expert demonstrations [nair2017crs] or large volumes of offline trajectories [ball20235zm], can dramatically accelerate learning by guiding exploration towards promising regions of the state-action space, thus improving sample efficiency. The importance of exploration for *generalization* itself is a key motivation for methods like EDE (Exploration via Distributional Ensemble) [jiang2023qmw], which encourages exploration of states with high epistemic uncertainty to acquire knowledge that aids decision-making in novel environments. Meta-learning exploration strategies [gupta2018rge] enable agents to learn *how* to explore effectively across a distribution of tasks, fostering rapid adaptation and generalization. Crucially, in safety-critical domains, exploration must be conducted within predefined safe boundaries or with learned recovery mechanisms, as explored by Recovery RL [thananjeyan2020d20], ensuring that the pursuit of optimal behavior does not lead to catastrophic outcomes.

In conclusion, the motivation for effective exploration in Reinforcement Learning is deeply rooted in the fundamental challenges of the field. It is indispensable for navigating sparse reward landscapes, conquering high-dimensional complexities, escaping deceptive local optima, and achieving both sample efficiency and robust generalization in dynamic, real-world settings. The continuous evolution of exploration strategies, from basic heuristics to advanced intrinsic motivation, diversity-driven methods, and meta-learning approaches, reflects its non-negotiable status as a core component for unlocking the full potential of intelligent agents. Addressing these challenges drives ongoing research to develop more robust, theoretically grounded, and computationally efficient exploration methods that can seamlessly integrate with the demands of practical applications.


### Foundational Concepts and Early Approaches to Exploration

\section{Foundational Concepts and Early Approaches to Exploration}
\label{sec:foundational_concepts__and__early_approaches_to_exploration}



\subsection{Basic Exploration Heuristics}
\label{sec:2_1_basic_exploration_heuristics}

The fundamental challenge of exploration in reinforcement learning (RL) lies in efficiently discovering optimal policies within an environment while simultaneously exploiting currently known good actions. The earliest and most straightforward attempts to address this exploration-exploitation dilemma centered around simple, yet widely adopted, heuristics, primarily the $\epsilon$-greedy policy. This approach serves as a crucial baseline from which more sophisticated and targeted exploration strategies have evolved, highlighting the initial attempts to balance this fundamental trade-off.

The $\epsilon$-greedy policy operates on a simple principle: with a small probability $\epsilon$ (epsilon), the agent selects an action uniformly at random, thereby exploring the environment. With a higher probability of $1-\epsilon$, the agent chooses the action that maximizes its current estimated value (the greedy action), thus exploiting its learned knowledge. This method's appeal lies in its conceptual simplicity and ease of implementation, making it a foundational component in many early RL algorithms [yang2021ngm, aubret2022inh]. It ensures that every action has a non-zero probability of being selected, preventing the agent from getting permanently stuck in suboptimal policies.

Despite its widespread use, basic $\epsilon$-greedy exploration suffers from several inherent limitations that have motivated the development of more advanced techniques. A primary drawback is its undirected nature [sukhija2024zz8]. The random actions taken during exploration are not guided by any sense of novelty, uncertainty, or potential for high reward. This leads to inefficient exploration, particularly in environments with large state-action spaces, sparse rewards, or deceptive local optima [stadie20158af, houthooft2016yee]. For instance, in complex domains requiring processing raw pixel inputs, simple $\epsilon$-greedy methods are often impractical due to their reliance on enumerating or uniformly sampling a vast, high-dimensional state-action space [stadie20158af]. The lack of direction means the agent might spend considerable time revisiting well-understood states or exploring unpromising regions, rather than focusing on truly unknown or potentially rewarding areas [stanton20183fs].

Furthermore, $\epsilon$-greedy policies struggle to distinguish between actions that are truly unknown and those that are known to be suboptimal [gupta2018rge]. Every action, regardless of how much is known about its outcomes, receives the same random exploration probability. This uniform randomness can be particularly problematic in real-world settings, where "random exploration, nevertheless, can result in disastrous outcomes and surprising performance" [ghamari2024bbm]. The method fails to leverage the agent's uncertainty about its value estimates, a critical piece of information for efficient exploration. This limitation highlighted the need for strategies that could generalize uncertainty and direct exploration towards states or actions with high epistemic uncertainty, as explored by methods like count-based exploration in feature space [martin2017bgt, tang20166wr]. These later approaches aimed to provide exploration bonuses based on how frequently states or features were visited, offering a more nuanced way to encourage novelty than simple uniform random action selection.

The inefficiency of $\epsilon$-greedy becomes even more pronounced in large or continuous state spaces, where the probability of revisiting any specific state becomes infinitesimally small, rendering simple visit counts ineffective [tang20166wr]. This "curse of dimensionality" necessitated methods that could generalize exploration across similar states or learn representations of novelty, moving beyond the direct, uninformative randomness of $\epsilon$-greedy.

However, the concept of $\epsilon$-greedy has not been entirely abandoned. Its simplicity has made it a foundational element that has been significantly refined and adapted. For example, in the context of Incremental Reinforcement Learning, where state and action spaces continually expand, a Dual-Adaptive $\epsilon$-greedy Exploration (DAE) method has been proposed [ding2023whs]. This advanced variant dynamically adjusts the exploration probability $\epsilon$ based on the convergence of value estimates for specific states (Meta Policy) and guides the agent to prioritize "least-tried" actions (Explorer). This evolution demonstrates how the core idea of balancing exploration and exploitation, first introduced by basic $\epsilon$-greedy, can be made significantly more sophisticated and targeted to address the challenges of dynamic and expanding environments, moving beyond its initial undirected and inefficient form.

In conclusion, while basic exploration heuristics like $\epsilon$-greedy provided a crucial initial framework for addressing the exploration-exploitation trade-off, their inherent limitations—undirected exploration, inefficiency in large state spaces, and inability to distinguish between truly unknown and well-understood but suboptimal actions—underscored the necessity for more sophisticated and targeted exploration strategies. These early methods laid the groundwork, serving as a fundamental baseline from which the rich and diverse landscape of modern exploration techniques has emerged.
\subsection{Model-Based Planning and Experience Replay}
\label{sec:2_2_model-based_planning__and__experience_replay}


Efficiently navigating and learning within complex environments is a fundamental challenge in reinforcement learning (RL), often exacerbated by the high cost of real-world interactions. Model-based planning and experience replay address this by making more efficient use of collected experience, implicitly aiding exploration by accelerating learning and propagating information more widely across the state space.

A foundational approach in this domain is the Dyna architecture, introduced by [Sutton1990]. Dyna-Q integrates direct reinforcement learning with planning by concurrently learning an environmental model (transitions and rewards) from real experiences. This learned model is then used to generate simulated experiences, allowing the agent to perform "mental rehearsals" and update its value function from both real and imagined interactions. This process significantly accelerates value function updates and propagates information more widely, making each real interaction more valuable and implicitly encouraging exploration by quickly refining the agent's understanding of the environment. Complementing this, [Lin1992] highlighted the importance of experience replay, a mechanism where past experiences are stored and re-used for learning. By replaying previously collected data, agents can learn more efficiently from a fixed set of interactions, reducing the need for extensive new exploration and improving sample efficiency, particularly in off-policy learning settings.

To further enhance the efficiency of model-based planning, [Sutton1993] and [Moore1993] introduced prioritized sweeping. This method refines the planning process by prioritizing updates to state-action pairs whose values are likely to change significantly, or which have a large impact on other states. By focusing computational resources on the most informative simulated experiences, prioritized sweeping dramatically accelerates learning and value propagation compared to uniform sweeping, ensuring that the agent's understanding of the environment is refined more quickly and effectively. Addressing the challenge of scaling model-based methods to larger state spaces, [Singh1992] proposed reinforcement learning with a hierarchy of abstract models. This approach leverages structural decomposition to manage complexity, allowing exploration and planning to occur at different levels of temporal abstraction, which can make the learning problem more tractable. Similarly, [Kaelbling1993] explored methods for blending planning and direct reinforcement learning, demonstrating how a learned model can be actively used to guide exploration by evaluating hypothetical scenarios and informing action selection, thereby making exploration more directed and less random.

Beyond direct model learning for planning, advancements in state representation also contribute to the efficiency of model-based approaches. [Dayan1993] introduced the successor representation, which models the expected future state occupancies rather than immediate transitions. While not a direct planning mechanism in the Dyna sense, this representation provides a more generalized understanding of state relationships, improving generalization for temporal difference learning and implicitly aiding exploration by making value estimates more robust and transferable across similar states.

In more recent deep reinforcement learning contexts, the principles of model-based planning continue to evolve. [stadie20158af] demonstrated how deep predictive models can be used to incentivize exploration by assigning exploration bonuses based on the uncertainty or novelty derived from the learned dynamics. This approach leverages the representational power of neural networks to build scalable models in high-dimensional domains, guiding exploration towards areas where the model's predictions are less confident. Extending this, [fu20220cl] proposed a model-based lifelong reinforcement learning approach that estimates a hierarchical Bayesian posterior to distill common structures across tasks. By combining this learned posterior with Bayesian exploration, their method significantly increases the sample efficiency of learning across related tasks, showcasing how sophisticated model learning can facilitate principled exploration and transfer. Furthermore, [ma2024r2p] integrated neural network models into an actor-critic architecture (ModelPPO) for AUV path-following control. Their neural network model learns the state transition function, allowing the agent to explore spatio-temporal patterns and achieve superior performance compared to traditional model predictive control and other RL methods, underscoring the enduring utility of learned models in complex control tasks.

Despite their significant advantages in sample efficiency and information propagation, model-based planning and experience replay methods face critical limitations. Their performance heavily relies on the accuracy and learnability of the environmental model. In complex, high-dimensional, or non-stationary domains, learning an accurate and robust model can be exceedingly challenging, and errors in the model can compound, leading to "model bias" and potentially suboptimal policies or misleading exploration. Nevertheless, these foundational and evolving model-based approaches remain crucial for accelerating learning and making efficient use of collected experience, thereby implicitly guiding agents towards more effective exploration strategies.
\subsection{Early Explicit Exploration Bonuses}
\label{sec:2_3_early_explicit_exploration_bonuses}

The fundamental challenge of exploration in reinforcement learning (RL) necessitates strategies that transcend purely random actions to efficiently discover optimal policies, particularly in environments characterized by sparse or delayed rewards. This subsection focuses on the pioneering methods that introduced explicit incentives for agents to explore novel or less-visited states, thereby laying the groundwork for more sophisticated intrinsic motivation techniques. These early approaches were crucial in demonstrating the power of directed exploration beyond mere stochasticity.

A seminal contribution to explicit exploration bonuses came from [Thrun1992], who introduced count-based exploration. In this paradigm, agents receive an additional, intrinsic reward for visiting states or taking actions less frequently encountered. The core idea is straightforward: by incentivizing novelty based on visitation frequency, the agent is directly encouraged to explore uncharted regions of the state space. This approach effectively transforms the problem of undirected search into a directed quest for new experiences, ensuring broader state space coverage in tabular settings. This principle aligns with the broader concept of "optimism in the face of uncertainty," where less-known options are optimistically valued higher to encourage their selection [SuttonBarto2018]. Such count-based mechanisms share conceptual roots with strategies employed in the multi-armed bandit problem, where algorithms like Upper Confidence Bound (UCB) leverage visitation counts (or estimates of uncertainty) to balance exploitation of known good options with exploration of less-tried ones, thereby providing a theoretical basis for directed exploration in simpler settings.

Complementing frequency-based methods, the concept of \textit{recency-based} exploration also emerged as a valuable heuristic in early RL. While less formally enshrined in a single seminal work compared to count-based methods, the underlying idea was to grant exploration bonuses based on the time elapsed since a state was last visited, or to prioritize states that were recently discovered but not yet thoroughly explored. These approaches aimed to prevent agents from getting stuck in local optima by encouraging them to refresh their knowledge about "stale" or neglected parts of the environment. For instance, an agent might receive a bonus inversely proportional to the number of timesteps since its last visit to a particular state, ensuring that even frequently visited states are eventually re-explored if they haven't been seen for a while. Both count-based and recency-based methods, while distinct in their temporal focus, shared the common goal of directing exploration by explicitly rewarding the agent for interacting with less familiar parts of the environment, moving beyond the undirected nature of $\epsilon$-greedy exploration.

Despite their groundbreaking nature and effectiveness in controlled, tabular, or low-dimensional environments, these early explicit exploration bonuses faced significant limitations, primarily due to the curse of dimensionality. Count-based methods, by their very definition, require maintaining an accurate count for each unique state-action pair. In environments with large, continuous, or high-dimensional state spaces (e.g., visual observations from images), enumerating and tracking every distinct state becomes computationally infeasible and memory-prohibitive. The notion of a "unique state" itself becomes ill-defined in continuous spaces, making direct counting impossible. Similarly, recency-based methods also struggle in such complex settings, as tracking the last visit time for an astronomically large or continuous state space is equally impractical. The inability of these foundational explicit bonuses to scale effectively to real-world complexity underscored the need for more generalized and robust intrinsic motivation techniques that could approximate novelty in high-dimensional settings without explicit state enumeration.

In conclusion, early explicit exploration bonuses, encompassing both count-based (frequency) and recency-based heuristics, provided a critical foundation for directed exploration in reinforcement learning. They successfully demonstrated the power of incentivizing novelty to overcome the limitations of purely random search in environments where states could be distinctly enumerated. However, their inherent reliance on explicit state representations severely limited their applicability to tabular or low-dimensional environments. These fundamental scalability challenges, driven by the curse of dimensionality, highlighted the need for two distinct paths forward: firstly, the development of theoretically grounded approaches that could offer provable guarantees on learning efficiency in tractable domains (as discussed in Section 3); and secondly, the creation of more advanced intrinsic motivation techniques that could generalize the notion of a "count" or "novelty" to complex, high-dimensional domains without explicit state enumeration (as will be explored in Section 4.2).


### Theoretically Grounded Exploration Strategies

\section{Theoretically Grounded Exploration Strategies}
\label{sec:theoretically_grounded_exploration_strategies}



\subsection{Optimism in the Face of Uncertainty (OFU) and PAC-MDP}
\label{sec:3_1_optimism_in_the_face_of_uncertainty_(ofu)__and__pac-mdp}


The principle of "optimism in the face of uncertainty" (OFU) stands as a foundational pillar for theoretically grounded exploration in reinforcement learning. This paradigm dictates that when an agent faces uncertainty about the true value of a state-action pair, it should optimistically assume the highest possible reward, thereby actively incentivizing exploration of unknown or poorly understood regions of the environment. This inherent bias towards unexplored options ensures that the agent gathers sufficient data to accurately estimate values, ultimately facilitating convergence to an optimal policy. OFU is intrinsically linked to the concept of Probably Approximately Correct (PAC-MDP) guarantees, which provide strong theoretical assurances that an agent can learn a near-optimal policy with high probability within a polynomial number of interactions [kearns2002near].

The historical development of OFU principles can be traced from the simpler multi-armed bandit (MAB) setting to full Markov Decision Processes (MDPs). In MABs, Upper Confidence Bound (UCB) algorithms, such as UCB1 [auer2002finite], exemplify OFU by selecting actions that maximize an upper confidence bound on their estimated value. This bound is typically a sum of the empirical mean reward and a bonus term that scales with the uncertainty (e.g., inversely proportional to the square root of the number of times the arm has been pulled). This strategy ensures that arms with potentially high, but uncertain, returns are sufficiently explored.

Extending this principle to the more complex MDP setting, algorithms like R-Max [brafman2002r] and UCRL (Upper Confidence Reinforcement Learning) [auer2009ucrl2] operationalize OFU to provide PAC-MDP guarantees. R-Max constructs an explicit model of the MDP and, for any state-action pair that has not been sampled a sufficient number of times, it optimistically assigns a maximal reward ($R_{max}$) and models a self-loop transition. This design effectively "forces" the planning algorithm to prioritize exploration of these unknown regions, as they appear maximally rewarding. Once a state-action pair has been visited enough times, its estimated reward and transition dynamics are considered reliable, and the optimism is removed. Similarly, UCRL algorithms maintain confidence intervals over the estimated transition probabilities and reward functions of the MDP. At each step, UCRL computes an "optimistic" MDP whose parameters lie within these confidence intervals and whose optimal policy yields the highest possible value. The agent then acts optimally with respect to this optimistic model, ensuring that actions leading to uncertain but potentially high-reward outcomes are chosen. Another notable algorithm, UCB-Value Iteration (UCB-VI), also leverages confidence bounds on value functions to guide optimistic exploration [kearns2002near].

These OFU-based algorithms are celebrated for their robust theoretical bounds on sample complexity, guaranteeing that an agent will find an $\epsilon$-optimal policy (a policy whose value is within $\epsilon$ of the optimal value) within a number of interactions that scales polynomially with the size of the state space, action space, and the desired accuracy. This makes them a strong foundation for efficient learning in environments where such guarantees are paramount. However, a critical limitation arises from their reliance on explicit state enumeration and accurate model estimation, which becomes intractable in high-dimensional or continuous state spaces. As highlighted by [stadie20158af], while "Bayesian and PAC-MDP approaches to the exploration problem offer strong formal guarantees," they often become "impractical in higher dimensions due to their reliance on enumerating the state-action space." This "curse of dimensionality" severely restricts their direct applicability to complex, real-world environments, a challenge reinforced by comprehensive surveys on deep reinforcement learning exploration [yang2021psl].

Despite these scalability challenges, the core tenets of OFU continue to inform contemporary research. Modern model-based RL algorithms still strive for similar guarantees, even if they employ approximations to handle larger state spaces. For instance, [song2021elb] introduces PC-MLP, a model-based RL algorithm that aims for polynomial sample complexity in both Kernelized Nonlinear Regulators and linear MDPs, demonstrating that the pursuit of theoretically efficient exploration remains active. This work, like its predecessors, relies on a planning oracle, a common assumption in algorithms with strong theoretical bounds. Furthermore, recent work by [sreedharan2023nae] explores optimistic exploration using symbolic model estimates, showcasing how OFU principles can be adapted to structured environments where symbolic representations can mitigate some of the dimensionality issues, thereby making optimistic planning more tractable.

In conclusion, the principle of optimism in the face of uncertainty, coupled with PAC-MDP guarantees, provides a robust theoretical framework for efficient exploration in reinforcement learning. These methods offer strong bounds on sample complexity and ensure convergence to near-optimal policies by systematically exploring uncertain but potentially rewarding avenues. However, their inherent reliance on explicit model construction and finite state-action spaces limits their direct applicability to the vast, high-dimensional environments common in modern deep RL. This fundamental trade-off between theoretical rigor and practical scalability has motivated the development of alternative exploration strategies, such as intrinsic motivation and approximate methods, which often sacrifice explicit PAC-MDP assurances for greater applicability in complex domains.
\subsection{Bayesian Approaches to Exploration}
\label{sec:3_2_bayesian_approaches_to_exploration}


Bayesian approaches offer a principled and theoretically grounded framework for tackling the exploration-exploitation dilemma in reinforcement learning by explicitly quantifying and managing uncertainty. These methods maintain a posterior distribution over possible models of the environment, value functions, or policies, leveraging this uncertainty to guide decision-making. The fundamental premise is that actions are not merely chosen based on their immediate expected reward, but also for their potential to reduce epistemic uncertainty, thereby leading to more informed and efficient learning over the long term. This subsection explores key techniques, from foundational posterior sampling methods like Thompson Sampling to scalable approximations using deep ensembles and Monte Carlo dropout, highlighting their mechanisms for balancing exploration and exploitation.

Historically, the concept of Bayesian reinforcement learning dates back to early theoretical works, where agents would explicitly maintain a posterior over the entire Markov Decision Process (MDP) parameters [strens2000bayesian]. While providing strong theoretical guarantees, the computational intractability of maintaining and updating exact posterior distributions, especially in high-dimensional state and action spaces or with complex, non-linear dynamics models common in deep reinforcement learning (DRL), severely limited their practical application. Exact Bayesian inference often requires complex computations over continuous or high-dimensional parameter spaces, making it prohibitive for real-world scenarios.

A cornerstone technique that exemplifies the Bayesian principle is Thompson Sampling. It operates by sampling a model (or a Q-function, or a policy) from the current posterior distribution and then acting optimally with respect to that sampled entity for a period. This mechanism inherently balances exploration and exploitation: models that are highly uncertain or have not been sufficiently explored are more likely to be sampled, leading to exploration, while well-understood models guide exploitation. The elegance of Thompson Sampling lies in its ability to implicitly direct exploration towards promising yet uncertain areas. Recent advancements have focused on making Thompson Sampling more scalable and provably efficient for DRL. For instance, \textcite{ishfaq20235fo} present a scalable Thompson Sampling strategy for RL that directly samples the Q-function from its posterior distribution using Langevin Monte Carlo, an efficient Markov Chain Monte Carlo (MCMC) method. This approach bypasses the need for restrictive Gaussian approximations, offering a more accurate representation of the posterior and demonstrating a regret bound of $\tilde{O}(d^{3/2}H^{3/2}\sqrt{T})$ in linear Markov Decision Processes (MDPs), making it deployable in deep RL with standard optimizers. Building on this, \textcite{ishfaq20245to} further enhance randomized exploration for RL by proposing an algorithmic framework that incorporates various approximate sampling methods with the computationally challenging Feel-Good Thompson Sampling (FGTS) approach. Their work yields improved regret bounds for linear MDPs and shows significant empirical gains in challenging deep exploration tasks within the Atari 57 suite, underscoring the potential of efficient approximate sampling to unlock the power of Thompson Sampling in complex environments.

Given the challenges of exact Bayesian inference, much research in DRL has focused on practical approximations for estimating epistemic uncertainty, which is crucial for effective Bayesian exploration. Deep ensembles have emerged as a prominent and effective method. By training multiple neural networks with different random initializations or data subsets, the disagreement among their predictions can serve as a proxy for epistemic uncertainty. This disagreement can then be used to generate intrinsic rewards, encouraging the agent to explore states where the ensemble's predictions diverge significantly. \textcite{jiang2023qmw} propose Exploration via Distributional Ensemble (EDE), a method that encourages exploration of states with high epistemic uncertainty through an ensemble of Q-value distributions. EDE demonstrates state-of-the-art performance on benchmarks like Procgen and Crafter, highlighting the importance of exploration for generalization and the efficacy of ensemble-based uncertainty. Similarly, \textcite{yang2022mx5} introduce Ensemble Proximal Policy Optimization (EPPO), which learns ensemble policies and incorporates a diversity enhancement regularization over the policy space. This regularization helps to generalize to unseen states and promotes exploration by encouraging the ensemble members to maintain diverse behaviors, thereby covering a broader range of the state-action space. In safety-critical applications, \textcite{zhang2024ppn} leverage deep ensembles to estimate epistemic uncertainty within a safe reinforcement learning framework. Their Uncertainty-augmented Lagrangian (Lag-U) algorithm uses this uncertainty to encourage exploration and adaptively modify safety constraints, enabling a better trade-off between efficiency and risk avoidance in autonomous driving.

Another practical method for approximating Bayesian uncertainty in deep neural networks is Monte Carlo dropout. By applying dropout during inference, multiple forward passes can be performed to obtain a distribution of predictions, from which uncertainty (e.g., variance) can be estimated. This technique provides a computationally efficient way to quantify epistemic uncertainty without training multiple separate models. \textcite{wu2021r67} utilize a practical and effective dropout-based uncertainty estimation method in their Uncertainty Weighted Actor-Critic (UWAC) algorithm. While primarily applied to offline reinforcement learning to detect and down-weight out-of-distribution state-action pairs, the underlying principle of using dropout to estimate uncertainty is directly applicable to guiding exploration in online settings by incentivizing visits to states where uncertainty is high.

Despite their theoretical elegance and principled approach, a common limitation of explicit Bayesian methods remains the computational complexity associated with maintaining and updating posterior distributions. While modern approximations like MCMC, deep ensembles, and Monte Carlo dropout significantly improve scalability, they introduce their own trade-offs. Deep ensembles require training and maintaining multiple neural networks, which can be computationally expensive and memory-intensive. Monte Carlo dropout, while efficient, relies on specific assumptions about the network architecture and may not always accurately capture the true posterior uncertainty. The accuracy of these approximations directly impacts the effectiveness of the exploration strategy and the theoretical guarantees. Future research continues to focus on developing more scalable, computationally efficient, and theoretically robust Bayesian approximations that can harness the full potential of uncertainty-driven exploration in complex, high-dimensional, and real-world reinforcement learning scenarios, moving beyond heuristic exploration towards more informed and adaptive learning.
\subsection{Information-Theoretic Exploration}
\label{sec:3_3_information-theoretic_exploration}


Information-theoretic exploration strategies offer a principled framework for addressing the exploration-exploitation dilemma in Reinforcement Learning (RL) by explicitly quantifying and maximizing the expected information gain. Unlike heuristic or purely novelty-seeking approaches, these methods guide agents towards experiences that are most likely to reduce uncertainty about the environment's dynamics, the optimal policy, or the value function. This section delineates various facets of information-theoretic exploration, emphasizing how they provide a sophisticated understanding of learning progress and model improvement.

A fundamental concept in this domain is the maximization of mutual information. A seminal work, Variational Information Maximizing Exploration (VIME) [Houthooft2016], exemplifies this by proposing an intrinsic reward signal derived from the mutual information between the agent's actions and the learned parameters of its environment dynamics model. VIME leverages variational inference to estimate this information gain, thereby incentivizing the agent to take actions that maximally reduce its uncertainty about how the environment functions. This approach moves beyond simple state visitation counts, actively driving the agent to improve its internal model of the world by seeking out states and actions that are most informative for model learning. The strength of VIME lies in its explicit link between exploration and model improvement, but its computational complexity, particularly in estimating mutual information and maintaining accurate posterior distributions over model parameters in high-dimensional settings, can be a significant challenge.

Beyond uncertainty about the environment model, information-theoretic approaches also focus on reducing uncertainty about the optimal policy or value function. Information-Directed Sampling (IDS) is a prominent example, which explicitly quantifies the value of information by maximizing the "information ratio" [russo2014learning]. This ratio balances the expected reduction in regret (or increase in reward) from gaining information against the cost of exploration. Unlike Thompson Sampling (which samples a policy from a posterior and acts greedily), IDS directly optimizes for the value of information, making it a more explicit information-theoretic strategy. While initially developed for bandit problems, IDS has been extended to Deep RL, as demonstrated by [nikolov20184g9]. This work proposes a tractable approximation of IDS for deep Q-learning, which explicitly accounts for both parametric uncertainty and heteroscedastic observation noise. By leveraging distributional reinforcement learning, this approach provides a robust exploration strategy that is particularly effective in environments with varying levels of uncertainty, outperforming traditional methods that struggle with non-uniform return variability. The application of IDS also extends beyond traditional RL control tasks, as seen in [sun2020zjg], where it was used in density-based structural topology optimization to efficiently direct the search towards optimal designs by maximizing the expected value of information in generative design problems.

Another significant information-theoretic concept is empowerment, which defines an intrinsic reward as the channel capacity between an agent's actions and its future states [salge2014empowerment, mohamed2015variational]. Maximizing empowerment encourages an agent to explore states where it has greater control or influence over its future, effectively driving it towards regions of the state space that offer more diverse and controllable outcomes. This perspective aligns with the broader idea of intrinsic motivation, where agents are driven by an innate desire to understand and control their environment. As highlighted by [aubret2022inh], information theory provides a rich taxonomy for intrinsic motivation, encompassing concepts like surprise (reduction in predictive uncertainty), novelty (information gain about unfamiliar states), and skill-learning (maximizing control over future states, i.e., empowerment). These different facets underscore how information-theoretic principles can be applied to various aspects of learning and exploration.

While Bayesian methods, discussed in Section 3.2, inherently align with information-theoretic principles by maintaining and reducing uncertainty, information-theoretic exploration distinguishes itself by explicitly formulating exploration as an optimization problem over information gain. For instance, Thompson Sampling implicitly reduces uncertainty by sampling from a posterior, but IDS or VIME directly compute or approximate the value of information. The computational overhead of precisely quantifying mutual information or channel capacity remains a primary challenge for information-theoretic methods, especially in complex, high-dimensional, and non-stationary environments. Approximations, such as those used in VIME or the Deep RL extension of IDS [nikolov20184g9], are crucial for scalability.

In conclusion, information-theoretic exploration offers a powerful and principled lens through which to design effective exploration strategies. By explicitly valuing information gain—whether about the environment's dynamics (VIME), the optimal policy (IDS), or the agent's control over its future (empowerment)—these methods move beyond simple heuristics to foster truly intelligent and directed discovery. Despite challenges related to computational tractability and the accurate estimation of information-theoretic quantities in complex settings, advancements in approximation techniques continue to enhance their practical applicability. Future research will likely focus on developing more efficient and robust approximations for information gain, potentially integrating with meta-learning to adaptively select optimal information-seeking strategies, and further exploring their utility in multi-agent and open-ended learning scenarios.


### Intrinsic Motivation: Novelty, Curiosity, and Prediction Error

\section{Intrinsic Motivation: Novelty, Curiosity, and Prediction Error}
\label{sec:intrinsic_motivation:_novelty,_curiosity,__and__prediction_error}



\subsection{Early Concepts of Intrinsic Curiosity}
\label{sec:4_1_early_concepts_of_intrinsic_curiosity}

The challenge of exploration in reinforcement learning, particularly in environments characterized by sparse or delayed extrinsic rewards, led to the development of intrinsic motivation. This paradigm shift moved beyond solely relying on external reward signals, proposing that agents could be driven by an internal 'curiosity' or 'novelty' derived from their own learning progress or model improvement. These foundational concepts laid the theoretical and conceptual groundwork for later, more sophisticated curiosity-driven and novelty-seeking exploration methods.

One of the earliest proponents of intrinsic curiosity was \textcite{Schmidhuber1997}, who introduced the idea of rewarding an agent for improving its world model's predictive accuracy. The agent is intrinsically motivated to explore states where its current model makes inaccurate predictions, thus seeking out "surprising" observations to reduce its uncertainty and improve its understanding of the environment. Building on this, \textcite{Singh2004} further formalized the notion of intrinsic motivation, comparing and contrasting different intrinsic signals such as novelty (unfamiliarity) and surprise (prediction error), providing a more theoretical framework for these internal drives.

As reinforcement learning moved towards more complex, high-dimensional domains, the challenge became scaling these intrinsic curiosity concepts. \textcite{Stadie2015} addressed this by proposing an exploration method that assigned bonuses from a concurrently learned deep predictive model of the system dynamics. This work demonstrated how the early ideas of prediction-error-based curiosity could be extended to tasks requiring raw pixel inputs, like Atari games, by leveraging deep neural networks to parameterize the world model. Further refining the theoretical underpinnings, \textcite{Houthooft2016} introduced Variational Information Maximizing Exploration (VIME), a principled, Bayesian approach that encourages agents to explore by maximizing the information gain about the environment's dynamics model. This method provides a more formal way to quantify and reduce epistemic uncertainty, guiding exploration towards states that are most informative for improving the agent's internal model.

Despite these advancements, prediction-error-based curiosity methods faced a challenge known as the "noisy TV problem," where agents could be perpetually distracted by unlearnable stochastic elements in the environment that constantly generated high prediction errors. To address this, \textcite{Pathak2017} proposed the Intrinsic Curiosity Module (ICM), which computes intrinsic rewards based on the prediction error of future states in a *learned feature space* rather than raw pixel space. By learning a feature representation that is invariant to factors beyond the agent's control, ICM effectively filters out unlearnable stochasticity, allowing curiosity to focus on aspects of the environment that the agent can influence. \textcite{Burda2018} further simplified and improved the robustness of curiosity-driven exploration with Random Network Distillation (RND). RND measures novelty as the prediction error of a fixed, randomly initialized target network's output by a trained prediction network, providing a highly effective intrinsic reward signal that is largely immune to the noisy TV problem because the prediction target is independent of the environment's true dynamics.

The practical utility of these curiosity-driven approaches has been demonstrated across various applications. For instance, \textcite{Li2019tj1} showed how a simplified Intrinsic Curiosity Module (S-ICM) could be effectively integrated with off-policy reinforcement learning methods, significantly improving exploration efficiency and learning performance in robotic manipulation tasks with sparse rewards. Similarly, \textcite{Zhelo2018wi8} applied curiosity-driven exploration to mapless navigation for mobile robots, validating its crucial role in improving deep reinforcement learning performance in tasks with challenging exploration requirements and enhancing generalization capabilities in unseen environments. More recently, \textcite{Sun2022ul9} utilized a similarity-based curiosity module to enable aggressive quadrotor flights, demonstrating how intrinsic motivation can accelerate training and improve the robustness of policies in complex control tasks.

In conclusion, the early concepts of intrinsic curiosity marked a fundamental shift in reinforcement learning, moving from external reward dependence to internal drives based on predictability, surprise, and learning progress. These pioneering ideas, from \textcite{Schmidhuber1997}'s initial formulation of prediction error as a motivator to the more robust and scalable deep learning-driven methods like ICM \textcite{Pathak2017} and RND \textcite{Burda2018}, have provided effective solutions for exploration in sparse-reward environments. While significant progress has been made in making these methods robust to irrelevant stochasticity, ongoing research continues to explore how to design intrinsic reward functions that consistently align with efficient and meaningful exploration across diverse, open-ended domains, and how to balance these internal drives with external task objectives.
\subsection{Count-Based and Density-Based Novelty}
\label{sec:4_2_count-based__and__density-based_novelty}

Effective exploration is paramount in reinforcement learning, particularly when agents operate in environments characterized by sparse extrinsic rewards or vast, high-dimensional state spaces. A prominent class of intrinsic motivation methods addresses this by quantifying the 'novelty' or 'unvisitedness' of states, generating internal reward signals that encourage agents to venture into less-frequented regions. This approach aims to foster broad state space coverage, which is often crucial for discovering optimal policies.

The foundational concept of count-based exploration, as pioneered by [thrun1992efficient], involves assigning an intrinsic bonus to states inversely proportional to their visitation frequency. In tabular or low-dimensional discrete environments, these methods provide a theoretically sound mechanism for directed exploration, ensuring that agents sufficiently explore all reachable states. However, as discussed in Section 2.3, traditional count-based approaches face a critical limitation: the curse of dimensionality. In high-dimensional or continuous state spaces, the probability of revisiting any exact state becomes infinitesimally small. This renders direct state counting impractical, as most states are encountered only once, leading to uniformly high novelty bonuses that fail to guide exploration effectively.

To bridge this gap and enable count-based exploration in deep reinforcement learning, researchers developed sophisticated techniques to approximate state visitation frequencies. Early efforts, such as those by [stadie20158af], demonstrated that deep predictive models could generate intrinsic exploration bonuses based on learned system dynamics in complex visual environments like Atari games. While not strictly count-based, this work highlighted the potential of neural networks to process high-dimensional observations and produce meaningful intrinsic signals, setting the stage for more direct approximations of novelty.

A pivotal breakthrough in scaling count-based exploration was the introduction of pseudo-counts and density models. [bellemare2016unifying] proposed a unified framework that generalizes count-based exploration by estimating state visitation frequencies using density models, thereby generating "pseudo-counts" for high-dimensional observations. Instead of exact state matching, this approach leverages the statistical likelihood of observing a state given past experiences. States that are less probable under the learned density model are considered more novel and receive higher intrinsic rewards. This effectively overcomes the limitations of exact state enumeration by providing a continuous and differentiable measure of novelty.

Building on this principle, various practical implementations emerged. [tang20166wr] introduced \texttt{\#Exploration}, a surprisingly effective yet simple method that maps high-dimensional states to hash codes. By counting the occurrences of these hash codes, the approach approximates state visitation frequencies, allowing for scalable pseudo-counting. This demonstrated that even a relatively crude approximation of state novelty, when combined with deep reinforcement learning, could yield near state-of-the-art performance on challenging benchmarks. However, the effectiveness of hash-based methods can be sensitive to the choice of hash function and the potential for hash collisions, which might conflate distinct states. Further refining the use of neural networks for density estimation, [ostrovski2017count] explicitly employed neural density models to compute pseudo-counts. This provided a more principled and robust statistical approach to estimate state novelty in complex visual environments, as the density model can learn more meaningful representations of states and their relationships. The challenge with such methods lies in the computational complexity of training accurate density models in high-dimensional spaces and ensuring that the learned density truly reflects meaningful novelty rather than irrelevant stochasticity.

While count-based and density-based methods primarily focus on the frequency of state visitation, other intrinsic motivation techniques, such as curiosity-driven exploration, leverage prediction error as a proxy for novelty. Methods like the Intrinsic Curiosity Module (ICM) by [pathak2017curiosity] and Random Network Distillation (RND) by [burda2018exploration] reward agents for encountering states where their internal predictive models are inaccurate or for states that lead to unpredictable outcomes. These approaches offer an alternative perspective on novelty, focusing on the agent's learning progress or uncertainty about environmental dynamics rather than mere visitation frequency. Although distinct in their underlying signals, both paradigms share the common goal of generating intrinsic rewards to drive exploration in sparse-reward, high-dimensional settings, with density-based methods providing a statistical measure of "unvisitedness" and prediction-error methods focusing on "unpredictability."

In summary, count-based and density-based novelty methods have undergone a significant evolution, transforming from simple heuristics for discrete environments into sophisticated deep learning techniques capable of scaling to complex, high-dimensional state spaces. The transition from direct state counting to pseudo-counts derived from neural density models has been critical for enabling robust exploration in deep reinforcement learning. These techniques provide a practical and often effective way to incentivize broad state space coverage and discover new areas. Nevertheless, challenges persist, including the computational overhead of training accurate density models, the sensitivity to state representation, and the difficulty of ensuring that the quantified novelty aligns with task-relevant exploration rather than being misled by uninformative stochasticity. Future research continues to refine these methods, often by integrating insights from both visitation statistics and predictive uncertainty, to develop more adaptive and robust novelty-seeking agents.
\subsection{Prediction Error and Self-Supervised Curiosity}
\label{sec:4_3_prediction_error__and__self-supervised_curiosity}

Effective exploration remains a cornerstone challenge in reinforcement learning (RL), particularly in environments characterized by sparse rewards and high-dimensional observations. To address this, intrinsic motivation methods have emerged, where agents generate their own reward signals to drive discovery. A prominent approach within this paradigm is self-supervised curiosity, which leverages the agent's ability to predict future states or features, using prediction error as an intrinsic reward to guide exploration. This strategy incentivizes agents to seek out situations where their internal models of the world are inaccurate, thereby driving learning about the environment's underlying dynamics.

The foundational concept of curiosity as a driver for learning can be traced back to early work by [schmidhuber1997], which proposed that agents could be intrinsically motivated to explore by optimizing the predictability of their sensory inputs. This early idea laid the groundwork for defining curiosity as a measure of surprise or novelty. Expanding on this, [stadie20158af] demonstrated that deep predictive models could be effectively used to assign exploration bonuses in complex domains like Atari games, by rewarding states where the agent's learned dynamics model exhibited high uncertainty. This represented an important step in scaling prediction-error-based curiosity to high-dimensional visual inputs, moving beyond simpler tabular settings.

A significant advancement in this direction was the Intrinsic Curiosity Module (ICM) proposed by [pathak2017]. ICM defines curiosity as the error in predicting the consequence of an agent's own actions within a learned feature space. Specifically, it trains a self-supervised forward dynamics model to predict the next latent state given the current latent state and action. The magnitude of this prediction error then serves as the intrinsic reward. This design incentivizes the agent to explore states where its internal model is inaccurate, thereby driving learning about the environment's dynamics. Crucially, by operating in a learned feature space rather than raw pixels, ICM made an initial attempt to mitigate the "noisy TV problem," where agents might be perpetually drawn to uncontrollable stochastic elements (like static on a TV screen) that generate high prediction error but offer no meaningful learning progress. This focus on learnable and controllable aspects of the environment represented a significant step towards scalable curiosity in high-dimensional visual environments.

Despite ICM's success, its reliance on learning an accurate forward dynamics model can still be problematic, particularly in highly stochastic environments. In such settings, genuine environmental noise or inherent unpredictability can lead to consistently high prediction errors, which are uninformative for learning and can still distract the agent, leading to inefficient exploration. This limitation highlights a critical distinction: prediction error can arise from either the agent's lack of knowledge (epistemic uncertainty) or from inherent environmental stochasticity (aleatoric uncertainty). ICM, by primarily measuring the error of a single forward model, struggles to differentiate between these two sources, potentially leading to spurious curiosity signals.

To address this challenge and provide more robust curiosity signals, alternative prediction-error-based approaches have emerged. One prominent direction involves leveraging ensembles of models to quantify epistemic uncertainty more explicitly. For instance, methods like Exploration via Distributional Ensemble (EDE) [jiang2023qmw] encourage exploration of states with high epistemic uncertainty by using an ensemble of Q-value distributions. The disagreement or variance among the predictions of these ensemble members provides a more reliable signal of what the agent truly "doesn't know," rather than simply what is unpredictable due to noise. This ensemble-based approach offers a principled way to direct exploration towards areas where the agent's understanding of the environment is weakest, promoting more efficient knowledge acquisition. The concept of using prediction error as a curiosity signal is also versatile, extending to domains like Large Language Models, where signals such as perplexity over generated responses or variance of value estimates from multi-head architectures can serve as intrinsic exploration bonuses [dai2025h8g]. From an information-theoretic perspective, these methods align with the idea of maximizing information gain, where surprise (prediction error) and novelty drive the building of abstract dynamics models and transferable skills [aubret2022inh].

The versatility of these prediction-error-based curiosity mechanisms has led to their integration into various RL frameworks. For instance, [li2019tj1] demonstrated how a simplified version of ICM could be effectively combined with off-policy RL methods, such as Deep Deterministic Policy Gradient (DDPG) and Hindsight Experience Replay (HER), significantly improving exploration efficiency and learning performance in robotic manipulation tasks with sparse rewards. Furthermore, the utility of curiosity-based intrinsic motivation extends to the challenging domain of offline reinforcement learning. [lambert202277x] investigated how such curiosity-driven methods could be used to collect informative datasets in a task-agnostic manner, which could then be leveraged by offline RL algorithms, highlighting their role in generating high-quality data for subsequent learning.

While prediction error and self-supervised curiosity have proven highly effective in driving exploration by incentivizing agents to learn about their environment's dynamics, challenges remain. The primary limitation lies in distinguishing between genuine uncertainty that can be resolved through exploration and inherent environmental stochasticity that offers no meaningful learning progress. This distinction is crucial for designing intrinsic reward functions that consistently align with meaningful exploration, especially in complex, hierarchical tasks. The balance between intrinsic and extrinsic rewards, and the potential for agents to get stuck in "perpetual exploration" loops without making tangible task progress, are also critical considerations. Addressing the robustness of these intrinsic signals against uninformative stochasticity is a key area of ongoing research, motivating the development of more sophisticated methods that will be discussed in the subsequent section.
\subsection{Robust Intrinsic Rewards: Addressing the Noisy TV Problem}
\label{sec:4_4_robust_intrinsic_rewards:_addressing_the_noisy_tv_problem}

The quest for effective exploration in reinforcement learning (RL) is profoundly challenged by environments offering sparse extrinsic rewards. While intrinsic motivation methods emerged as a promising avenue to generate internal curiosity signals and alleviate this sparsity, early prediction-error approaches frequently succumbed to the "noisy TV problem." This phenomenon describes scenarios where agents are perpetually drawn to unpredictable yet uninformative stochastic elements within the environment, such as random pixel noise on a screen or flickering lights. These elements generate consistently high prediction errors, leading to spurious curiosity that distracts the agent from truly novel and learnable aspects of the environment, thereby hindering focused exploration.

Initial efforts, such as those by [stadie20158af], explored incentivizing exploration through bonuses derived from concurrently learned deep predictive models of system dynamics. This aimed to reward agents for encountering states that challenged their current environmental understanding. A prominent example of this paradigm is the Intrinsic Curiosity Module (ICM) [Pathak2017]. ICM trains a forward dynamics model to predict the next state's features given the current state's features and the executed action. The magnitude of this prediction error then serves as an intrinsic reward, encouraging the agent to visit states where its internal model is inaccurate. While ICM provided a scalable solution for high-dimensional visual inputs by operating in a learned feature space, its reliance on predicting *future states* made it inherently susceptible to the noisy TV problem. In environments with uninformative stochasticity, the forward dynamics model would consistently fail to predict these truly random, irreducible changes, resulting in persistently high prediction errors. This spurious curiosity would then cause the agent to repeatedly visit these uninformative areas, diverting computational resources and hindering progress towards task-relevant exploration.

To overcome the critical limitations posed by the noisy TV problem, [Burda2018] introduced Random Network Distillation (RND), a pivotal innovation in robust intrinsic reward generation. RND fundamentally redefines novelty detection by decoupling the intrinsic reward from the learnability of the environment's true dynamics. Instead of predicting future states, RND employs two neural networks: a fixed, randomly initialized target network ($f$) and a predictor network ($\hat{f}$). Both networks receive the current state $s_t$ as input. The predictor network is trained to predict the output of the target network, i.e., $\hat{f}(s_t) \approx f(s_t)$. The intrinsic reward is then defined as the mean squared error between the outputs of these two networks: $r_i = ||\hat{f}(s_t) - f(s_t)||^2$.

The key insight of RND lies in its design: for any given state $s_t$, even one containing uninformative stochasticity (like a noisy TV screen), the randomly initialized target network $f(s_t)$ produces a *fixed and deterministic* output embedding. The predictor network $\hat{f}(s_t)$ is then trained to learn this fixed mapping. If an agent repeatedly visits a state $s_t$, the predictor $\hat{f}$ will eventually learn to accurately map $s_t$ to $f(s_t)$, causing the prediction error and thus the intrinsic reward to *decay*. This mechanism effectively filters out irrelevant stochasticity because the reward is based on the *novelty of the state itself* (i.e., how well the predictor has learned to map that specific state to its fixed target), rather than the unpredictability of state transitions. Consequently, even a noisy TV state, despite its visual randomness, yields a fixed target output from $f$. Once the agent has sufficiently explored this state, $\hat{f}$ learns the mapping, and the curiosity bonus diminishes, preventing perpetual attraction. This contrasts sharply with ICM, where the prediction error for a truly stochastic transition remains irreducible, leading to persistent curiosity.

RND's robust and simpler mechanism for novelty detection proved highly effective, leading to significantly more efficient and directed exploration in complex visual domains, such as Atari games, where it substantially outperformed prior methods on hard exploration tasks [Burda2018]. Its success underscored the importance of designing intrinsic reward signals that are resilient to environmental noise and focus on aspects of the environment truly conducive to learning. Subsequent works, such as [li2019tj1], further explored simplified variants of curiosity modules, demonstrating how architectural or methodological simplifications could enhance the practical utility and integration of prediction-error based intrinsic motivation, particularly for off-policy reinforcement learning methods.

While RND provided a powerful solution to the noisy TV problem, it is not the sole robust intrinsic motivation strategy. Other approaches also aim to mitigate the effects of uninformative stochasticity or enhance exploration in complementary ways. For instance, methods based on model disagreement or ensembles, which reward exploration in states where multiple learned dynamics models disagree, can implicitly discount uncontrollable noise by focusing on areas where the agent's *learnable* understanding is inconsistent. Information-theoretic perspectives, as surveyed by [aubret2022inh], often emphasize maximizing *useful* information gain, which aligns with RND's implicit discounting of unlearnable noise. Furthermore, diversity-driven exploration strategies, which incentivize agents to visit states that are distinct from previously encountered ones [hong20182pr], or Random Latent Exploration (RLE), which encourages agents to pursue randomly sampled goals in a latent space [mahankali20248dx], offer alternative mechanisms for robust and deep exploration without relying solely on prediction error. Deep Curiosity Search (DeepCS) [stanton20183fs] also introduced the concept of "intra-life novelty," rewarding exploration within a single episode, which can complement RND's "across-training novelty" by encouraging immediate discovery.

Despite these significant advancements, challenges persist. While RND effectively addresses the noisy TV problem by focusing on learnable novelty, the intrinsic reward signal can still be somewhat undirected, potentially leading to exploration of areas that are novel but not necessarily relevant to the extrinsic task. Future research continues to explore how to imbue intrinsic rewards with more task-relevant directionality, perhaps through goal-conditioned or hierarchical approaches, or how to combine them with other exploration strategies to achieve even more efficient and purposeful exploration in increasingly complex and open-ended environments. The integration of RND into advanced agents like Agent57 [Badia2020Agent57] further demonstrates its lasting impact as a foundational component for achieving state-of-the-art performance in diverse and challenging domains.


### Advanced and Adaptive Exploration Strategies

\section{Advanced and Adaptive Exploration Strategies}
\label{sec:advanced__and__adaptive_exploration_strategies}



\subsection{Hierarchical Reinforcement Learning for Exploration}
\label{sec:5_1_hierarchical_reinforcement_learning_for_exploration}


Effective exploration is a critical bottleneck in reinforcement learning (RL), particularly in complex environments characterized by vast state-action spaces, sparse rewards, and long task horizons. Hierarchical Reinforcement Learning (HRL) offers a powerful and principled framework to address these challenges by decomposing large, intractable problems into a hierarchy of more manageable sub-problems. This decomposition allows agents to learn and explore at different levels of temporal abstraction, significantly enhancing exploration efficiency and robustness.

The foundational concept underpinning HRL for exploration is the "Options Framework" introduced by [Sutton1999]. Building upon earlier notions of skill chaining by [Singh1995], the Options Framework formalizes "options" as temporally extended actions, or sub-policies, that execute for multiple time steps. An option consists of an initiation set (states where it can be taken), a policy (how to act while the option is active), and a termination condition (when the option ends). This framework allows a high-level policy to choose among options, while a low-level policy executes the primitive actions within a chosen option. This mechanism fundamentally aids exploration by enabling agents to traverse large portions of the state space more efficiently than with primitive actions alone. Instead of exploring individual steps, the agent explores sequences of actions (options), effectively reducing the effective search space and allowing for more directed movement towards relevant subgoals or novel regions. For instance, an agent might learn an "open door" option, which, once selected, reliably executes the necessary primitive actions to open a door, allowing the high-level policy to explore the consequences of being on the other side of the door, rather than stumbling upon the correct sequence of door-opening actions by chance.

In the era of deep reinforcement learning, various architectures have been proposed to implement hierarchical control and leverage its benefits for exploration. FeUdal Networks (FuN) [Vezhnevets2017] introduced a two-level hierarchy with a "Manager" module that sets goals in a latent space and a "Worker" module that executes primitive actions to achieve those goals. The Manager explores the space of goals, while the Worker explores the primitive action space conditioned on the current goal. This explicit goal-setting mechanism intrinsically guides exploration towards achieving meaningful sub-objectives. Similarly, Hierarchical Reinforcement Learning with Off-policy Correction (HIRO) [Nachum2018] enables efficient learning of both high-level and low-level policies by correcting for off-policy data, allowing the high-level policy to explore by setting goals in the state space, and the low-level policy to learn how to reach those goals. Hierarchical Actor-Critic (HAC) [Levy2019] further refines this by using multiple layers of actor-critic agents, where higher levels set goals for lower levels, and a "hindsight experience replay" mechanism allows agents to learn from failed attempts to reach goals, thereby improving exploration by making better use of suboptimal trajectories. These deep HRL methods demonstrate how learning goal-conditioned policies at different levels of abstraction can significantly accelerate exploration in complex, high-dimensional environments.

A critical aspect of HRL for exploration is the autonomous discovery of useful options or skills, often driven by intrinsic motivation. Rather than manually defining options, agents can learn them through self-supervision. Methods like Diversity is All You Need (DIAYN) [Eysenbach2018] and Variational Option Discovery (VALOR) [Gregor2017] learn a diverse set of skills by maximizing the mutual information between the skill executed and the resulting state trajectory, effectively rewarding the agent for discovering distinct behaviors. These learned skills then serve as valuable options for a higher-level policy, enabling more structured and efficient exploration. The survey by [aubret2022inh] highlights how information-theoretic intrinsic motivation, particularly novelty and surprise, can assist in building a hierarchy of transferable skills, making the exploration process more robust. Furthermore, [janjua2024yhk] emphasizes unsupervised skill acquisition as a key advancement for enhancing scalability in open-ended environments, where HRL provides a natural framework for organizing these learned behaviors.

HRL also facilitates temporally coordinated and goal-conditioned exploration. [zhang2022p0b] proposed Generative Planning Method (GPM), which generates multi-step action plans, effectively acting as temporally extended options. These plans guide exploration towards high-value regions more consistently than single-step perturbations, and the plan generator can adapt to the task, further benefiting future explorations. This aligns with HRL's ability to create intentional action sequences for reaching specific subgoals. Similarly, Random Latent Exploration (RLE) [mahankali20248dx], while not strictly HRL, encourages exploration by pursuing randomly sampled goals in a latent space. This goal-conditioned exploration paradigm is inherently compatible with HRL, where the high-level policy can sample latent goals for the low-level policy to achieve, fostering diverse and deep exploration.

The utility of hierarchical exploration extends significantly to multi-agent systems, where coordination and efficient search are paramount. [hu2020qwm] designed a cooperative exploration strategy for multiple mobile robots using a hierarchical control architecture, where a high-level decision-making layer coordinates exploration to minimize redundancy, and a low-level layer handles target tracking and collision avoidance. This demonstrates how HRL can structure complex multi-robot behaviors for efficient, coordinated exploration. More recently, [yu20213c1] tackled cooperative visual exploration for multiple agents with a Multi-agent Spatial Planner (MSP) leveraging a transformer-based architecture with hierarchical spatial self-attentions, enabling agents to capture spatial relations and plan cooperatively based on visual signals. [liu2024xkk] further advances multi-agent exploration with "Imagine, Initialize, and Explore" (IIE), which uses a transformer to imagine critical states and then initializes agents at these states for targeted exploration. This approach, while not explicitly called HRL, embodies hierarchical decomposition by first identifying high-level critical states (subgoals) and then focusing low-level exploration from those points, enhancing the discovery of successful joint action sequences in long-horizon tasks.

In summary, HRL provides a powerful framework for managing the complexity of exploration in large state-action spaces. By enabling agents to learn and compose temporally extended actions (options) or to decompose complex tasks into sub-problems, HRL significantly reduces the dimensionality of the exploration problem. This structured approach allows agents to focus on achieving meaningful subgoals, leading to more directed and sample-efficient discovery of optimal policies in long-horizon tasks. Despite these advancements, challenges persist, particularly in the autonomous discovery of optimal and diverse skill sets, the robust learning of high-level policies that effectively coordinate lower-level skills, and ensuring seamless communication and transfer of information across hierarchical levels. Future research will likely focus on developing more adaptive and autonomous methods for skill acquisition and composition, further integrating HRL with robust intrinsic motivation and meta-learning to create agents capable of truly open-ended and efficient exploration.
\subsection{Learning Exploration Policies (Meta-Exploration)}
\label{sec:5_2_learning_exploration_policies_(meta-exploration)}


A pivotal advancement in reinforcement learning (RL) exploration shifts the paradigm from relying on hand-crafted heuristics or fixed intrinsic reward functions to enabling agents to autonomously learn their own exploration strategies. This sophisticated approach, termed meta-exploration, involves an outer-loop optimization process that trains a meta-controller or a recurrent policy to generate adaptive exploration behaviors, aiming to maximize long-term returns across a distribution of tasks or episodes. By learning "how to explore," these methods can dynamically adjust the exploration-exploitation trade-off, leading to more efficient, task-relevant discovery and robust performance, particularly in novel and complex environments [duan2016rll, wang2016learning]. This represents a significant stride towards autonomous and intelligent exploration, where agents generalize their exploration capabilities rather than relearning them from scratch for each new task.

The foundational concept of meta-exploration was significantly advanced by works demonstrating that recurrent neural networks (RNNs) can serve as meta-learners. [duan2016rll] introduced RL$^2$ (Reinforcement Learning to Reinforce Learn), a seminal framework where an RNN-based agent is trained to solve a distribution of tasks. The RNN's hidden state effectively encodes task-specific information and past experience, allowing it to learn an exploration strategy that adapts within a single episode and across multiple episodes of a new, unseen task. This enables the agent to exhibit rapid adaptation and efficient exploration behaviors, such as directed search or uncertainty-driven probing, without explicit hand-engineered exploration bonuses. Similarly, [wang2016learning] explored the idea of "Learning to Reinforce Learn," where an RNN acts as a meta-learner to discover an entire RL algorithm, including its exploration component, by processing sequences of observations, actions, and rewards. These approaches highlight the power of recurrent architectures to implicitly capture and execute sophisticated exploration policies that generalize across related tasks.

Building on these foundations, subsequent research has focused on learning more structured and informed exploration strategies. [gupta2018rge] proposed Model Agnostic Exploration with Structured Noise (MAESN), a gradient-based meta-learning algorithm that learns exploration strategies from prior experience. MAESN leverages prior tasks to initialize a policy and acquire a latent exploration space, which injects structured stochasticity into the policy. This allows for exploration strategies that are informed by previous knowledge, moving beyond simple action-space noise and proving more effective than task-agnostic methods. This work underscores the benefit of meta-learning not just the policy, but the *mechanism* of exploration itself, enabling more targeted and efficient discovery.

Meta-learning has also been applied to the generation and refinement of intrinsic motivation signals for exploration. [zhang2020xq9] introduced MetaCURE (Meta Reinforcement Learning with Empowerment-Driven Exploration), which explicitly models an exploration policy learning problem separate from the exploitation policy. MetaCURE employs a novel empowerment-driven exploration objective that aims to maximize information gain for task identification, deriving a corresponding intrinsic reward. By learning separate, context-aware exploration and exploitation policies and sharing task inference knowledge, MetaCURE significantly enhances exploration efficiency in sparse-reward meta-RL tasks. This demonstrates how meta-learning can discover effective intrinsic reward functions that guide exploration towards truly informative experiences, addressing a key challenge in intrinsic motivation.

Furthermore, meta-exploration has been integrated with other advanced RL paradigms. [rimon20243o6] presented MAMBA, a model-based approach to meta-RL that leverages world models for efficient exploration. By learning an internal model of the environment, MAMBA can plan and explore more effectively, leading to greater return and significantly improved sample efficiency (up to 15x) compared to existing meta-RL algorithms, especially in higher-dimensional domains. This highlights the synergy between learning environmental models and meta-learning exploration strategies. Complementing this, [goldie2024cuf] introduced Learned Optimization for Plasticity, Exploration and Non-stationarity (OPEN), which meta-learns an update rule (an optimizer) whose input features and output structure are informed by solutions to common RL difficulties, including exploration. OPEN's parameterization is flexible enough to use stochasticity for exploration, demonstrating that meta-learning can discover effective policy update mechanisms that inherently promote efficient exploration.

It is crucial to distinguish meta-exploration from merely adaptive exploration, where exploration parameters are tuned within a single learning process. While methods that dynamically adjust exploration probabilities based on metrics like information entropy [hu2020yhq] or ensemble learning for balancing exploration-exploitation ratios [shuai2025fq3] are valuable, they typically do not involve an outer-loop meta-training process to learn a generalizable exploration *strategy* across tasks. Such adaptive approaches are better categorized under integrated and adaptive exploration frameworks (Section 5.3), which focus on dynamic parameter adjustment rather than learning the exploration algorithm itself.

In conclusion, the shift towards learning exploration policies through meta-exploration represents a profound step towards truly autonomous and intelligent agents. These approaches, ranging from recurrent policies learning exploration behaviors across episodes to meta-learning structured exploration noise, intrinsic motivation signals, or even entire optimization rules, empower agents to generalize their exploration capabilities and achieve robust performance across diverse problem settings. However, significant challenges persist, including the computational expense of meta-training, the difficulty of defining appropriate and diverse task distributions for meta-learning, ensuring the learned exploration strategies generalize to truly novel and out-of-distribution tasks, and designing effective meta-objectives that capture desirable exploration properties. Future research will likely focus on developing more robust, sample-efficient, and generalizable meta-learning algorithms that can discover truly novel and effective exploration strategies across a wide spectrum of tasks, pushing the boundaries of autonomous discovery.
\subsection{Integrated and Adaptive Exploration Frameworks}
\label{sec:5_3_integrated__and__adaptive_exploration_frameworks}


Effective exploration in complex, high-dimensional environments often transcends the capabilities of a single, static strategy, necessitating frameworks that dynamically combine multiple exploration techniques and adaptively select or blend them based on the current task, state, or learning progress. Moving beyond a 'one-size-fits-all' approach, these integrated frameworks aim to create highly versatile and robust agents capable of tackling a wide spectrum of exploration challenges, representing a key direction for developing general-purpose reinforcement learning (RL) agents. Robust intrinsic motivation, as discussed in Section 4, frequently serves as a foundational component within these integrated systems, providing internal reward signals (e.g., RND-style novelty [Burda2018] or episodic novelty [NGU]) that are then managed or orchestrated by higher-level adaptive mechanisms.

A prominent approach to achieving adaptive exploration involves the use of meta-controllers that explicitly orchestrate a portfolio of exploration-exploitation policies. The seminal work of [Badia2020] on Agent57 exemplifies this paradigm, achieving state-of-the-art performance across diverse Atari games. Agent57 integrates multiple intrinsic motivation signals, including Random Network Distillation (RND) for life-long novelty and value-discrepancy-based novelty, with an adaptive exploration strategy managed by a meta-controller. This meta-controller dynamically selects from a range of exploration-exploitation policies, allowing the agent to adjust its behavior on the fly to suit the specific demands of each game and phase of learning. The strength of this approach lies in its ability to explicitly learn *how* to explore by selecting appropriate behaviors from a predefined set, offering significant flexibility. However, a limitation is its reliance on a hand-designed portfolio of base policies and the complexity of meta-learning the controller, which can be computationally intensive and may struggle if the optimal strategy is not represented within the initial portfolio.

Beyond explicit meta-controllers, other integrated frameworks leverage ensemble methods or decoupled architectures to achieve robust and adaptive exploration. Ensemble-based approaches, for instance, integrate multiple policies or value functions to capture uncertainty or promote diversity. [jiang2023qmw] introduced Exploration via Distributional Ensemble (EDE), a method that encourages exploration of states with high epistemic uncertainty through an ensemble of Q-value distributions. This integration of multiple distributional estimates allows for a more nuanced understanding of uncertainty, leading to improved generalization in unseen environments. Similarly, [yang2022mx5] proposed Ensemble Proximal Policy Optimization (EPPO), which learns ensemble policies in an end-to-end manner, combining individual policies and the ensemble organically. EPPO adopts a diversity enhancement regularization over the policy space, which theoretically increases exploration efficacy and promotes generalization. In a different vein, [whitney2021xlu] proposed Decoupled Exploration and Exploitation Policies (DEEP), which structurally separates the task policy from the exploration policy. This decoupling allows for directed exploration to be highly effective for sample-efficient continuous control without incurring performance penalties in densely-rewarding environments. In contrast to Agent57's explicit switching mechanism, these ensemble and decoupled architectures offer a more implicit form of adaptation, either through aggregation of diverse perspectives or a clear separation of learning objectives, providing robustness but potentially less fine-grained, task-specific adaptation.

Further advancements in adaptive exploration leverage probabilistic, Bayesian, and reactive mechanisms to guide behavior based on uncertainty or environmental shifts. Bayesian approaches provide a principled framework for managing uncertainty, which can be directly used to drive exploration. [fu20220cl] introduced a model-based lifelong RL approach that estimates a hierarchical Bayesian posterior, which, combined with a sample-based Bayesian exploration procedure, adaptively increases sample efficiency across related tasks. Extending this, [li2023kgk] explored Bayesian exploration with Implicit Posteriori Parameter Distribution Optimization (IPPDO), modeling parameter uncertainty with an implicit distribution approximated by generative models, offering greater flexibility and improved sample efficiency. In the realm of randomized exploration, [ishfaq20235fo] and [ishfaq20245to] developed scalable Thompson sampling strategies using Langevin Monte Carlo and approximate sampling, respectively. These methods directly sample the Q-function from its posterior distribution, providing provably efficient and adaptive exploration by inherently balancing uncertainty and reward. For dynamic environments, [steinparz20220nl] proposed Reactive Exploration, designed to track and react to continual domain shifts in lifelong reinforcement learning, demonstrating adaptive policy updates in non-stationary settings. These probabilistic and reactive methods offer strong theoretical grounding for continuous adaptation but often face computational challenges in high-dimensional settings, requiring efficient approximation techniques.

Emerging trends also point towards exploration as an emergent property from learned optimizers or large pre-trained models. [goldie2024cuf] introduced Learned Optimization for Plasticity, Exploration and Non-stationarity (OPEN), a meta-learned update rule whose input features and output structure are informed by solutions to RL difficulties, including the ability to use stochasticity for exploration. This represents a shift towards learning the optimization process itself, which implicitly includes adaptive exploration. In the context of large foundation models, [lee202337c] demonstrated that Decision-Pretrained Transformers (DPT) can exhibit emergent online exploration capabilities in-context, without explicit training for it. Building on this, [dai2024x3l] proposed In-context Exploration-Exploitation (ICEE), which optimizes the efficiency of in-context policy learning by performing exploration-exploitation trade-offs at inference time within a Transformer model. These approaches suggest a future where exploration is less explicitly engineered and more implicitly learned or emergent, potentially leading to highly generalizable agents, but raise questions about control, interpretability, and the sample efficiency required for pre-training.

In conclusion, integrated and adaptive exploration frameworks represent a significant leap towards developing general-purpose RL agents. By combining robust intrinsic motivation signals, explicit meta-controllers, ensemble and decoupled architectures, principled Bayesian and probabilistic methods, and leveraging emergent capabilities from learned optimizers and large models, these frameworks move beyond static heuristics to dynamically adjust their exploration behaviors. While significant progress has been made, future directions include developing more theoretically grounded guarantees for these complex adaptive systems, enhancing their computational efficiency and scalability, and ensuring robust generalization to truly novel and open-ended environments where the optimal exploration strategy might not be easily predefined or learned from limited prior experience. A key unresolved challenge is the automated discovery of an optimal portfolio of exploration strategies for meta-controllers, as current approaches often rely on hand-designed components.
\subsection{Population-Based and Evolutionary Exploration}
\label{sec:5_4_population-based__and__evolutionary_exploration}


The inherent challenge of the exploration-exploitation dilemma in Reinforcement Learning (RL) often leads single agents to converge prematurely to sub-optimal policies, particularly in environments characterized by sparse rewards or complex, multi-modal reward landscapes. To overcome these limitations, a distinct paradigm has emerged that leverages populations of agents or evolutionary algorithms to foster broader, more robust exploration and facilitate global search. These approaches represent a meta-level solution, structuring the learning system itself for enhanced discovery.

Early precursors to population-based exploration can be found in Neuroevolution, where neural network architectures and weights are optimized using evolutionary algorithms. Methods like NEAT (NeuroEvolution of Augmenting Topologies) [stanley2002evolving] demonstrated the power of evolving diverse populations of networks to solve complex control tasks, implicitly performing exploration by searching a vast hypothesis space. More recently, Evolution Strategies (ES) have gained prominence as a scalable black-box optimization technique for deep RL, capable of training deep neural networks efficiently due to their high parallelizability [salimans2017evolution]. However, standard ES can struggle with sparse or deceptive reward landscapes, necessitating directed exploration. To address this, [conti2017cr2] introduced methods like Novelty Search with Evolution Strategies (NS-ES) and Quality Diversity (QD) algorithms hybridized with ES. These approaches maintain a population of novelty-seeking agents, rewarding exploration of novel behaviors rather than just high performance, thereby enabling ES to avoid local optima and achieve higher performance on challenging deep RL tasks like Atari games and simulated robot locomotion.

A significant advancement in population-based methods for deep RL is Population Based Training (PBT) [jaderberg2017population]. PBT concurrently trains a population of agents, each with its own set of hyperparameters and model weights. Unlike traditional grid search or random search, PBT dynamically adapts hyperparameters during training by periodically evaluating agents, exploiting well-performing ones (copying their weights and hyperparameters) and exploring new hyperparameter configurations for underperforming ones. This asynchronous "exploit-and-explore" strategy allows PBT to discover robust hyperparameter schedules and model weights simultaneously, leading to faster training and improved final performance across diverse tasks, including complex deep RL benchmarks. PBT's strength lies in its ability to adaptively tune both learning processes and agent policies, making it highly effective for complex, high-dimensional problems.

Building on the strengths of both evolutionary algorithms and gradient-based RL, Evolutionary Reinforcement Learning (ERL) frameworks have emerged as a powerful hybrid paradigm. These methods typically combine the global search capabilities of evolutionary algorithms with the local optimization efficiency of gradient-based RL. Early ERL approaches, such as those by [khadka2018evolutionary] and CEM-RL [pourchot2019cemrl], often involve evolving a population of actor networks, while a shared critic network (trained via gradient descent) provides value estimates to guide both the evolutionary process and individual actor updates. This hybrid approach aims to leverage the exploration benefits of evolution (e.g., escaping local optima, maintaining diversity) and the sample efficiency of RL. For instance, [sun2024edc] proposed a modified ERL method for multi-agent region protection, amalgamating Differential Evolution (DE) for diverse sample exploration and overcoming sparse rewards with Multi-Agent Deep Deterministic Policy Gradient (MADDPG) for training defenders and expediting DE convergence.

A more recent advancement in this line is the Two-Stage Evolutionary Reinforcement Learning (TERL) framework proposed by [zhu2024sb0]. TERL addresses a key limitation of prior ERL methods, which often evolve only actor networks, thereby constraining exploration if a single critic network falls into local optima. Instead, TERL maintains and optimizes a population of *complete RL agents*, each comprising both an actor and a critic network. This design enables more independent and diverse exploration by each individual, mitigating the risk of premature convergence to suboptimal policies dictated by a flawed shared critic. The TERL framework operates through a novel two-stage learning process: an initial "Exploration Stage" where all individuals learn independently, optimized by a hybrid approach combining gradient-based RL updates with meta-optimization techniques like Particle Swarm Optimization (PSO). This stage emphasizes diversification, with agents sharing information efficiently through a common replay buffer, which helps propagate beneficial experiences across the population. Following this, the "Exploitation Stage" focuses on refining the best-performing individual from the population through concentrated RL-based updates, while the remaining individuals continue to undergo PSO to further diversify the replay buffer. This dynamic allocation of computational resources and tailored optimization strategies across stages allows TERL to effectively balance the exploration-exploitation dilemma.

Despite their advantages, population-based and evolutionary methods introduce their own set of challenges. Computational cost is a primary concern, as maintaining and training multiple agents or evolving large populations can be resource-intensive, although parallelization strategies (like those in ES and PBT) mitigate this. Furthermore, the integration of diverse data from population optimization into off-policy RL algorithms, particularly through shared replay buffers, can introduce instability and even degrade performance, as highlighted by [zheng2023u9k]. This issue arises because population data, while diverse, might not align with the on-policy distribution expected by some RL algorithms, leading to an "overlooked error." To remedy this, [zheng2023u9k] proposed a double replay buffer design to provide more on-policy data, demonstrating the need for careful architectural considerations when combining these paradigms. The choice between PBT's hyperparameter evolution and ERL's policy evolution also presents a trade-off: PBT excels at finding robust training configurations, while ERL directly optimizes policy parameters, often leading to more direct policy improvement.

In conclusion, population-based and evolutionary exploration methods offer a compelling meta-level solution to the challenges of exploration in complex RL environments. By evolving populations of complete RL agents, dynamically adapting hyperparameters, or employing hybrid optimization strategies, these approaches enable more diverse learning trajectories and a more robust search for optimal policies, moving beyond the limitations of single-agent exploration heuristics. Future research could explore more sophisticated mechanisms for inter-agent information sharing, investigate adaptive intrinsic motivation signals within these population-based frameworks, or extend these concepts to multi-task and open-ended learning scenarios, further enhancing the adaptability and generalization capabilities of RL agents.


### Specialized Contexts and Applications of Exploration

\section{Specialized Contexts and Applications of Exploration}
\label{sec:specialized_contexts__and__applications_of_exploration}



\subsection{Exploration in Offline Reinforcement Learning}
\label{sec:6_1_exploration_in_offline_reinforcement_learning}


In offline Reinforcement Learning (RL), the agent is tasked with learning an optimal policy solely from a fixed, pre-collected dataset, fundamentally precluding any further active interaction with the environment. This paradigm shift introduces unique challenges for exploration, as traditional active exploration strategies, which involve generating new experiences, are inherently impossible. Instead, the core problem transforms into managing *distributional shift* and preventing the policy from querying actions that are out-of-distribution (OOD) relative to the dataset. The focus shifts from active exploration to ensuring 'conservative learning' within the boundaries of the existing data distribution, aiming to identify reliable regions for policy improvement while rigorously avoiding OOD actions that could lead to unreliable value estimates or unsafe behaviors due to extrapolation errors. This conservative approach is paramount for real-world applications where data collection is costly, risky, or simply not feasible during the learning process.

The foundational approaches to offline RL primarily address the distributional shift problem through two main mechanisms: policy constraints and value function pessimism. Seminal works like Batch-Constrained Q-Learning (BCQ) [fujimoto2019off] introduced explicit policy constraints, regularizing the learned policy to stay close to the behavior policy that generated the dataset. This is typically achieved by adding a regularization term (e.g., KL-divergence) to the policy objective, ensuring that the agent does not venture into unobserved state-action pairs. Similarly, Behavior Regularized Actor Critic (BRAC) [wu2019behavior] further explored various forms of behavior regularization to mitigate the distributional shift. While effective in keeping the policy close to the data, these methods can sometimes limit the discovery of truly optimal policies if the behavior policy was suboptimal.

A complementary and highly influential approach is Conservative Q-Learning (CQL) [kumar2020conservative]. CQL tackles the distributional shift by explicitly enforcing pessimism in the value function estimation. It achieves this by adding a penalty to the Q-values of OOD actions, ensuring that the learned Q-function provides a lower bound on the true Q-values. This prevents overestimation of action values for unseen actions, which is a common failure mode in offline RL. While BCQ primarily constrains the policy directly, CQL intervenes at the value function level, indirectly shaping the policy by making OOD actions less attractive. This distinction highlights different points of intervention for ensuring conservatism.

Building upon these foundations, subsequent methods have leveraged uncertainty estimation to guide conservative learning more explicitly. The Uncertainty Weighted Actor-Critic (UWAC) [wu2021r67] explicitly incorporates uncertainty treatment by detecting OOD state-action pairs and down-weighting their contribution in the training objectives. Utilizing a practical dropout-based uncertainty estimation, UWAC laid groundwork for robust learning by mitigating the impact of unreliable data points. Similarly, [rezaeifar20211eu] conceptualized offline RL as "anti-exploration," proposing to subtract a prediction-based exploration bonus from the reward. This innovative approach encourages the policy to remain within the support of the dataset by penalizing actions whose consequences cannot be reliably predicted, effectively extending pessimism-based offline RL methods to deep learning settings. Further, [zhang20244ty] introduced an Entropy-regularized Diffusion Policy with Q-Ensembles, where robust policy improvement is achieved by learning the lower confidence bound of Q-ensembles. This implicitly accounts for uncertainty in value estimates, mitigating the impact of inaccurate value functions from OOD data, while an entropy regularizer improves "exploration" within the offline dataset by encouraging diverse actions within the observed distribution.

Simpler yet effective mechanisms have also emerged to ensure in-sample learning. [ma2024jej] proposed Improving Offline Reinforcement Learning with in-Sample Advantage Regularization (ISAR), which adapts offline RL to robotic manipulation with minimal changes. ISAR learns the state-value function exclusively from dataset samples, then calculates the advantage function based on this in-sample estimation and adds a behavior cloning regularization term. This method effectively mitigates the impact of unseen actions without introducing complex hyperparameters, offering a straightforward approach to conservative learning that implicitly handles OOD issues.

A significant challenge, particularly in model-based offline RL (MBRL), is addressing biased exploration during the synthetic trajectory generation phase. Standard maximum entropy exploration mechanisms, often adopted from online RL, can lead to skewed data distributions and impaired performance when applied to learned dynamics models. To tackle this, [wu2024mak] introduced OCEAN-MBRL (Offline Conservative ExplorAtioN for Model-Based Offline Reinforcement Learning), a novel plug-in rollout approach. OCEAN explicitly decouples exploration from exploitation, introducing a principled, conservative exploration strategy guided by an ensemble of dynamics models for uncertainty estimation. It employs three key constraints: a state evaluation constraint to explore only in low-uncertainty regions, an exploration range constraint to select conservative actions, and a trajectory truncation constraint to limit rollouts in high-uncertainty areas. This comprehensive approach significantly enhances the stability and performance of existing MBRL algorithms by ensuring that exploration within the learned model remains reliable and does not generate new, potentially unsafe, out-of-distribution experiences.

The transition from offline learning to online fine-tuning presents another critical juncture for conservative exploration. While initial conservatism is vital for stable offline learning, a purely pessimistic policy might fail to discover better actions during online interaction. The Simple Unified uNcertainty-Guided (SUNG) framework for offline-to-online RL [guo20233sd] quantifies uncertainty via a VAE-based state-action visitation density estimator. SUNG's adaptive exploitation method applies conservative offline RL objectives to high-uncertainty samples and standard online RL objectives to low-uncertainty samples, demonstrating a sophisticated use of uncertainty to dynamically adjust the degree of conservatism. Building on this, [guo2024sba] proposed Optimistic Exploration and Meta Adaptation (OEMA) for sample-efficient offline-to-online RL. OEMA employs an optimistic exploration strategy, adhering to the principle of optimism in the face of uncertainty, allowing agents to sufficiently explore while reducing distributional shift through meta-learning. Providing a theoretical underpinning for such adaptive strategies, [hu2024085] showed that Bayesian design principles are crucial for offline-to-online fine-tuning, suggesting that a probability-matching agent, rather than purely optimistic or pessimistic ones, can avoid sudden performance drops while being guaranteed to find the optimal policy.

The literature on exploration in offline RL has thus evolved from foundational methods that strictly constrain policies or penalize OOD value estimates to sophisticated, explicit conservative exploration strategies, particularly in model-based settings and during the offline-to-online transition. While significant progress has been made in ensuring learning remains robust and effective without active interaction, a persistent challenge lies in the accuracy, reliability, and computational efficiency of uncertainty estimation itself. Future research directions could focus on developing more robust and scalable uncertainty quantification methods, as well as adaptive mechanisms that dynamically adjust conservative exploration constraints based on the evolving confidence in the learned policy and model, effectively balancing the need for safety with the potential for performance improvement.
\subsection{Expert-Guided and Demonstration-Based Exploration}
\label{sec:6_2_expert-guided__and__demonstration-based_exploration}

Efficient exploration remains a formidable challenge in reinforcement learning (RL), particularly in complex, high-dimensional, and sparse-reward environments. To mitigate this, a significant body of research focuses on leveraging expert knowledge or demonstrations to guide and accelerate the exploration process, thereby improving sample efficiency and policy robustness. This paradigm, often termed Reinforcement Learning from Demonstrations (RLfD), seeks to bridge the gap between purely autonomous trial-and-error and the wealth of human or simulated expertise.

Early efforts established the foundational utility of demonstrations in overcoming exploration hurdles. [nair2017crs] demonstrated that even a small set of expert demonstrations, when integrated with RL algorithms like Deep Deterministic Policy Gradients (DDPG) and Hindsight Experience Replay (HER), could provide an order of magnitude speedup in learning complex, continuous control robotics tasks with sparse rewards. These methods often relied on static, \textit{offline} datasets of expert trajectories, which provided initial guidance but presented inherent limitations. Building upon this, [uchendu20221h1] proposed Jump-Start Reinforcement Learning (JSRL), which uses a "guide-policy" derived from offline data or demonstrations to form a curriculum of starting states for an "exploration-policy," significantly improving sample complexity, especially in data-scarce regimes. Similarly, [hansen2022jm2] highlighted key ingredients for accelerating visual model-based RL with demonstrations, including policy pretraining, targeted exploration, and oversampling demonstration data, leading to substantial performance gains in sparse reward tasks.

Beyond direct trajectory imitation, expert knowledge can also be integrated through symbolic rules or domain-specific insights. For instance, [hou2021c2r] introduced Rule-Aware Reinforcement Learning (RARL) for knowledge graph reasoning, injecting high-quality symbolic rules into the model's reasoning process to alleviate sparse rewards and prevent spurious paths. [mazumder2022deb] proposed using "state-action permissibility" knowledge to guide exploration, drastically speeding up deep RL training by identifying and avoiding impermissible actions. In a similar vein, [liu20228r4] combined human knowledge-based rule bases with imitation learning pre-training (ILDN) and safe RL to enhance efficiency and generalization in large-scale adversarial scenarios. Furthermore, theoretical frameworks have explored how exploration itself can be framed as a utility to be optimized, which demonstrations can implicitly help achieve [zhang2020o5t, santi2024hct].

Despite the benefits of offline demonstrations, a persistent challenge is the "distribution gap" [coelho2024oa6]. This occurs when the agent's policy deviates from the expert's, leading it into states not covered by the static demonstration dataset, thereby hindering generalization and robustness. To address this, research has shifted towards more dynamic and interactive integration of expert knowledge. [ball20235zm] showed that with minimal modifications, existing off-policy RL algorithms could effectively leverage offline data (including demonstrations) for online learning, achieving significant performance improvements. This offline-to-online fine-tuning paradigm is crucial for real-world applications [rafailov2024wtw, hu2024085], where policies pre-trained on demonstrations need to adapt to novel online experiences. For example, [lu2025j7f] presented VLA-RL, an approach that uses online reinforcement learning to improve pretrained Vision-Language-Action (VLA) models, specifically addressing out-of-distribution failures that arise from limited offline data in robotic manipulation. Even leveraging "negative demonstrations" or failed experiences can guide exploration by showing what \textit{not} to do, as demonstrated by [wu20248f9] in sparse reward environments.

A significant advancement in bridging the distribution gap and enhancing exploration efficiency comes from dynamically interacting with experts. [hou20248b2] introduced EARLY, an active RL from demonstrations algorithm where the agent intelligently queries for episodic demonstrations based on its trajectory-level uncertainty. This approach makes expert guidance more targeted and resource-efficient by requesting help only when needed. The pinnacle of this dynamic interaction is exemplified by [coelho2024oa6] with RLfOLD (Reinforcement Learning from Online Demonstrations) for urban autonomous driving. RLfOLD introduces the novel concept of \textit{online demonstrations}, which are dynamically collected from a simulator's privileged information during the agent's active exploration. These demonstrations are seamlessly integrated into a single replay buffer alongside agent experiences, directly addressing the distribution gap by providing contextually relevant expert guidance. The framework utilizes a modified Soft Actor-Critic (SAC) algorithm with a dual standard deviation policy network, outputting distinct $\sigma_{RL}$ for exploration and $\sigma_{IL}$ for imitation, allowing for adaptive balancing of these learning objectives. Furthermore, an uncertainty-based mechanism selectively invokes the online expert to guide the agent in challenging situations, making exploration more targeted and efficient. RLfOLD demonstrated superior performance in the CARLA NoCrash benchmark with significantly fewer resources, highlighting its effectiveness and efficiency in complex, real-time domains.

In conclusion, the intellectual trajectory of expert-guided exploration has evolved from static, offline demonstration datasets to dynamic, interactive, and online expert guidance. This progression effectively addresses critical challenges such as the distribution gap and sample inefficiency, particularly in safety-critical applications like autonomous driving and robotics. While methods leveraging offline demonstrations provide crucial initial boosts, the trend towards online and active expert interaction, exemplified by frameworks like RLfOLD, represents a significant step towards more robust, adaptive, and generalizable RL systems. Future research will likely focus on refining the mechanisms for generating and integrating online expert guidance, especially in real-world scenarios where "privileged information" is unavailable, and developing more sophisticated expert models that can provide nuanced and context-aware interventions.
\subsection{Exploration in Dynamic and Expanding Environments}
\label{sec:6_3_exploration_in_dynamic__and__exp_and_ing_environments}


The challenge of efficient exploration intensifies significantly in Reinforcement Learning (RL) when agents must operate in environments where the state and action spaces are not static but continually expand or evolve [yang2021ngm]. Unlike traditional Markov Decision Processes (MDPs) that assume fixed state and action sets, real-world systems often undergo updates, introducing novel states, actions, or even entire sub-environments. This dynamic nature necessitates exploration strategies that can efficiently discover and integrate new information without incurring computationally prohibitive retraining costs or suffering from catastrophic forgetting of previously acquired knowledge. This specialized context forms a crucial subset of lifelong and continual learning, where agents must adapt to an unending stream of tasks or environmental changes [janjua2024yhk, fu20220cl].

The concept of Incremental Reinforcement Learning (Incremental RL) has emerged to specifically address this challenge, focusing on how agents can efficiently adapt their policies to newly introduced states and actions. While lifelong learning broadly concerns sequential task learning and knowledge transfer [fu20220cl, woczyk20220mn], Incremental RL distinguishes itself by tackling the explicit *expansion* of the underlying MDP structure. A seminal contribution by [ding2023whs] formally defines Incremental RL and proposes the Dual-Adaptive $\epsilon$-greedy Exploration (DAE) algorithm. This approach confronts the inherent inefficiency of standard exploration methods and the strong inductive biases that can arise from extensive prior learning, which often hinder adaptation to expanding environments. DAE employs a Meta Policy ($\Psi$) to adaptively determine a state-dependent $\epsilon$ by assessing the exploration convergence of a state (via TD-Error rate), thereby deciding *when* to explore. Concurrently, an Explorer ($\Phi$) guides the agent to prioritize "least-tried" actions by estimating their relative frequencies, addressing *what* to explore. Crucially, DAE also incorporates strategies for incrementally adapting deep Q-networks by reusing trained policies and intelligently initializing new neurons and Q-values. This architectural flexibility, combined with the dual-adaptive exploration mechanism, significantly reduces training overhead and enables robust adaptation to expanding search spaces without retraining from scratch.

The need for adaptive exploration in dynamic settings is also highlighted by research into non-stationary environments, which share some conceptual overlaps with expanding environments, though they differ mechanistically. For instance, [steinparz20220nl] introduces Reactive Exploration to cope with continual domain shifts in lifelong reinforcement learning. This work demonstrates that policy-gradient methods benefit from strategies that track and react to non-stationarities, such as changes in reward functions or environmental dynamics, within an otherwise fixed state-action space. While Reactive Exploration focuses on adapting to *changes* in existing elements, DAE specifically addresses the *addition* of new states and actions. However, both underscore the broader necessity for exploration strategies that can actively adapt to environmental changes, rather than relying on static or pre-defined exploration schedules. Similarly, the importance of exploration for generalization to new, unseen environments, as explored by [jiang2023qmw], aligns with the goals of Incremental RL. Their Exploration via Distributional Ensemble (EDE) method encourages exploration of states with high epistemic uncertainty, which is crucial for acquiring knowledge that aids decision-making in novel situations. While EDE aims to generalize within a potentially vast but fixed environment, DAE's focus is on efficiently integrating entirely new components into the agent's operational space, a distinction critical for truly open-ended learning systems [janjua2024yhk].

Other advanced exploration techniques, such as those leveraging intrinsic motivation [houthooft2016yee] or information gain maximization [aubret2022inh], aim to improve exploration efficiency by incentivizing agents to visit novel states or reduce uncertainty about the environment dynamics. For example, Variational Information Maximizing Exploration (VIME) [houthooft2016yee] uses Bayesian neural networks to maximize information gain about environment dynamics. While powerful in static high-dimensional environments, their direct applicability and scalability to *continually expanding* state and action spaces present unique challenges. Prediction-error-based methods, like those underlying many intrinsic motivation approaches, may struggle when the underlying dynamics model requires continuous architectural restructuring rather than just parameter updates. The sudden introduction of entirely new states or actions can render existing prediction models inaccurate or incomplete, requiring significant re-learning or architectural modifications that are not inherently handled by these methods. Count-based or density-based novelty methods, while effective for discovering unvisited regions, would need robust mechanisms to distinguish truly *new* states/actions from merely *unvisited* ones within the previously known space, and to efficiently update their density estimations for an ever-growing space. DAE's explicit focus on identifying and prioritizing newly available actions and states, overcoming the inductive bias from extensive prior learning, offers a more targeted solution to these architectural and knowledge-transfer challenges.

The integration of such adaptive exploration strategies with broader lifelong learning frameworks is a critical direction. Lifelong RL methods, such as model-based Bayesian approaches that estimate a hierarchical posterior to distill common task structures [fu20220cl], offer mechanisms for backward transfer and efficient learning across related tasks. However, these often assume a fixed set of potential tasks or a stable underlying model structure. The challenge of Incremental RL lies in the dynamic *growth* of this structure, requiring not just transfer but also efficient architectural expansion and robust exploration of truly novel elements. Learned optimization methods, which meta-learn update rules to handle non-stationarity, plasticity loss, and exploration [goldie2024cuf], offer a promising avenue by building adaptability directly into the learning process, potentially complementing DAE's specific exploration mechanisms.

In conclusion, the progression towards "Exploration in Dynamic and Expanding Environments" marks a crucial intellectual shift in Reinforcement Learning, moving beyond the static MDP assumption towards more realistic, evolving systems. While foundational exploration methods provide general tools, the work on Incremental RL, particularly the Dual-Adaptive $\epsilon$-greedy Exploration [ding2023whs], offers a targeted solution for environments where state and action spaces continually grow. Future research in this area will likely focus on extending these adaptive exploration strategies to more complex, partially observable, or even multi-agent expanding environments, further enhancing the lifelong learning capabilities of RL agents in truly dynamic real-world scenarios, and integrating them more deeply with meta-learning and continual learning paradigms to address catastrophic forgetting and efficient knowledge transfer in ever-growing systems.
\subsection{Safety-Aware Exploration}
\label{sec:6_4_safety-aware_exploration}

The exploration phase in reinforcement learning (RL) is critical for discovering optimal policies, yet in real-world, safety-critical applications, unconstrained exploration can lead to catastrophic outcomes and raise significant ethical concerns regarding accountability, fairness in decision-making under risk, and the potential for unforeseen negative side-effects in human-inhabited environments. This subsection delves into methods designed to ensure safety during exploration, navigating the inherent tension between the need for aggressive exploration to achieve optimal performance and the imperative to maintain safe operation. The problem is often formally cast within the framework of Constrained Markov Decision Processes (CMDPs), where an agent aims to maximize cumulative reward while simultaneously satisfying constraints on cumulative costs, such as safety violations [altman1999constrained]. This formalism provides a robust theoretical foundation for developing algorithms that provide safety guarantees during the learning process.

Early efforts to integrate safety into RL exploration focused on establishing explicit boundaries and constraints. A foundational approach involves "safety layers" or "shielding," which act as guardians, restricting the agent's actions or states to predefined safe regions, thereby preventing the agent from entering hazardous situations during learning [Stachurski2008]. While early works laid the groundwork, modern deep RL has seen significant advancements, notably with methods like those proposed by [alshiekh2018safe], which formally integrate shielding into deep RL agents. These methods enforce explicit safety constraints, ensuring exploration is guided within a permissible operational envelope, effectively mitigating the risk of catastrophic failures. However, a key limitation of static safety layers is their potential to be overly conservative, which can severely restrict the agent's exploration capabilities and prevent the discovery of truly optimal, yet initially unknown, safe behaviors. This conservativeness often stems from the difficulty of accurately predefining safe regions in complex, high-dimensional environments, or from worst-case assumptions made to guarantee safety.

Addressing the limitations of static constraints and the inherent conservativeness, more recent research has explored dynamic and learned safety mechanisms, often decoupling the concerns of task performance and safety. [yu20222xi] introduced SEditor, a two-policy approach that learns a "safety editor policy" to transform potentially unsafe actions proposed by a utility-maximizing policy into safe ones. SEditor represents a conceptual shift from static, predefined shields to a learned, dynamic safety filter, allowing for more nuanced and state-dependent safety interventions. This method moves beyond simplified safety models, enabling the safety editor to learn complex safety functions, effectively acting as a dynamic shield. While SEditor demonstrates significantly lower constraint violation rates and maintains high utility, its effectiveness relies on the ability to train an accurate safety editor policy, which can be challenging in highly dynamic or unpredictable environments.

Further advancing dynamic safety, [thananjeyan2020d20] introduced Recovery RL, which first leverages offline data to learn about constraint-violating zones. It then employs a task policy for reward maximization and a dedicated recovery policy that activates to guide the agent back to safety when constraint violation is likely. This dual-policy structure allows for more aggressive exploration by the task policy, relying on the learned recovery mechanism to prevent unsafe outcomes. Unlike SEditor, which modifies actions *before* execution, Recovery RL focuses on *recovering* from potentially unsafe trajectories, offering a different trade-off between proactive prevention and reactive correction. Recovery RL demonstrates superior efficiency in balancing task success and constraint adherence in complex, contact-rich manipulation tasks and image-based navigation, even on physical robots, by allowing the task policy greater freedom. Similarly, [zhang2023wqi] proposed a method for safe RL with dead-ends avoidance and recovery. This approach constructs a boundary to discriminate between safe and unsafe states, equivalent to distinguishing dead-end states, thereby ensuring maximum safe exploration with minimal limitation. Like Recovery RL, it utilizes a decoupled framework with a task policy and a pre-trained recovery policy, along with a safety critic, to ensure safe actions during online training. This strategy aims to achieve better task performance with fewer safety violations by carefully delineating the extent of guaranteed safe exploration, offering a more precise definition of "safe" exploration boundaries.

Another powerful paradigm for guaranteeing safety, particularly in continuous control systems, draws from control theory: Lyapunov stability and Control Barrier Functions (CBFs). These methods provide formal guarantees that a system's state will remain within a predefined safe set. Control Barrier Functions (CBFs) are functions of the state that define a safe set and whose derivatives can be constrained to render this set forward-invariant, thus preventing the agent from leaving it. [zhang2022dgg] proposed a Barrier Lyapunov Function-based safe RL (BLF-SRL) algorithm for autonomous vehicles. This approach integrates BLF items into an optimized backstepping control method, constraining state variables within a designed safety region during learning. By decomposing optimal control with BLF items, it achieves safe exploration while learning adaptive uncertain items, ensuring both safety and performance optimization in safety-critical domains. While control-theoretic methods like BLF-SRL offer strong, often deterministic, safety guarantees, they typically require an accurate model of the system dynamics and specific assumptions about the environment, a limitation not shared by model-free, data-driven approaches like Recovery RL, which in turn provide only probabilistic safety assurances. Addressing this model dependency, [cheng20224w2] presented a method using Disturbance-Observer-Based Control Barrier Functions (DOB-CBFs). This approach avoids explicit model learning by leveraging disturbance observers to accurately estimate the pointwise value of uncertainty, which is then incorporated into a robust CBF condition. This allows for less conservative safety filters, especially during early learning, by effectively handling unknown disturbances without requiring extensive model training.

The inherent uncertainty in exploration necessitates risk-sensitive approaches. Building on Bayesian exploration principles (as discussed in Section 3.2), some approaches use uncertainty to define credible intervals for constraint satisfaction, leading to more principled conservative exploration. [zhang2024ppn] introduced Lag-U, an uncertainty-augmented Lagrangian safe RL algorithm for autonomous driving. This method uses deep ensembles to estimate epistemic uncertainty, which is then used to encourage exploration and learn a risk-sensitive policy by adaptively modifying safety constraints. Furthermore, it incorporates an intervention assurance mechanism based on quantified uncertainty to select safer actions during deployment. This allows for a better trade-off between efficiency and risk avoidance, preventing overly conservative policies by making safety decisions based on the agent's confidence in its predictions. Complementing this, [yu2022bo5] proposed a distributional reachability certificate (DRC) to address model uncertainties and characterize robust persistently safe states. Their framework builds a shield policy based on DRC to minimize constraint violations, especially during training, by considering the distribution of potential long-term constraint violations, thereby enhancing safety robustness against model uncertainty. Beyond explicit uncertainty estimation, some methods directly manipulate the learning process to balance reward and safety. [gu2024fu3] addressed the conflict between reward and safety gradients, proposing a soft switching policy optimization method. By analyzing and manipulating these gradients, their framework aims to achieve a better balance between optimizing for rewards and adhering to safety constraints, offering a more direct way to mitigate the inherent conflict compared to simply adding penalty terms in CMDPs.

In scenarios where some safety signals are available in a controlled environment, "guided safe exploration" can be employed to facilitate safe transfer learning. [yang2023n56] proposed a method where a "guide" agent learns to explore safely without external rewards in a controlled environment where safety signals are available. This guide then helps compose a safe behavior policy for a "student" agent in a target task where safety violations are prohibited. The student policy is regularized towards the guide while it is unreliable, gradually reducing the guide's influence as it learns. This approach enables safe transfer learning and faster problem-solving in the target task, highlighting the utility of leveraging prior safety knowledge to bootstrap safe exploration in novel, safety-critical settings.

In summary, safety-aware exploration has evolved significantly, moving from static, predefined safety layers and formal constraint satisfaction (CMDPs) to dynamic, learned recovery mechanisms, control-theoretic guarantees (BLFs, CBFs), and risk-sensitive approaches leveraging uncertainty quantification. The field continues to grapple with the fundamental tension between encouraging sufficient exploration for optimal learning and guaranteeing safety in real-world, safety-critical applications. Future directions will likely involve integrating more sophisticated formal verification techniques, developing robust and scalable uncertainty quantification for proactive safety prediction, designing intrinsically safe exploration strategies that adhere to ethical guidelines from the outset, and further refining gradient manipulation techniques to optimally balance conflicting objectives. The goal remains to enable autonomous agents to learn effectively without compromising human or environmental safety, ensuring responsible deployment in increasingly complex and sensitive domains.


### Conclusion and Future Directions

\section{Conclusion and Future Directions}
\label{sec:conclusion__and__future_directions}



\subsection{Summary of Key Advancements}
\label{sec:7_1_summary_of_key_advancements}


The journey of exploration in reinforcement learning (RL) reflects a continuous and sophisticated evolution, driven by the persistent challenge of balancing the acquisition of new information with the exploitation of known optimal actions. This review has traced a narrative arc from foundational heuristics to theoretically grounded algorithms, and subsequently to scalable intrinsic motivation techniques, culminating in adaptive, learned exploration strategies tailored for specialized contexts. This progression underscores the field's relentless innovation in addressing the increasing complexity of environments and the inherent difficulties of the exploration-exploitation dilemma.

Early attempts to navigate the exploration-exploitation trade-off, as discussed in Section 2, centered on basic heuristics like $\epsilon$-greedy and the implicit exploration benefits of model-based planning architectures such as Dyna-Q. Concurrently, explicit exploration bonuses, often count-based, provided direct incentives for visiting novel states. While effective in tabular settings, these methods faced significant scalability challenges in high-dimensional or continuous state spaces. For instance, early attempts to generalize count-based exploration to deep RL, such as mapping states to hash codes [tang20166wr], provided a surprisingly strong baseline but highlighted the need for more robust, scalable novelty detection mechanisms.

A pivotal shift occurred with the development of theoretically grounded exploration strategies (Section 3), which sought to provide provable guarantees on learning efficiency. Principles like "optimism in the face of uncertainty" (OFU), embodied in algorithms like R-Max, offered PAC-MDP guarantees by optimistically valuing unexplored regions. Bayesian approaches, notably Thompson Sampling, provided a principled framework for managing uncertainty by maintaining posterior distributions over models or value functions. Further advancing this theoretical rigor, information-theoretic methods emerged, guiding exploration by maximizing knowledge gain about the environment or optimal policy. Techniques such as Variational Information Maximizing Exploration (VIME) [houthooft2016yee] and MaxInfoRL [sukhija2024zz8] exemplify this, rewarding agents for transitions that significantly reduce uncertainty or improve the agent's internal model. This information-centric view, as surveyed by [aubret2022inh], moved beyond simple novelty to a more sophisticated understanding of learning progress. However, the computational complexity of maintaining accurate uncertainty estimates and the limitations of optimism in scenarios with partially observable rewards [parisi2024u3o] often restricted their applicability to simpler environments.

The advent of deep reinforcement learning necessitated a paradigm shift, leading to the widespread adoption of intrinsic motivation techniques (Section 4). These methods generate internal reward signals to drive exploration, proving crucial in environments with sparse external rewards. Initial concepts of curiosity and novelty-seeking evolved into scalable approaches. Count-based methods were adapted for high-dimensional spaces using pseudo-counts and density models. A significant breakthrough came with prediction-error based curiosity, where agents are rewarded for encountering surprising or unpredictable observations, as seen in the Intrinsic Curiosity Module (ICM) [li2019tj1, zhelo2018wi8]. This directed exploration towards aspects of the environment that improve the agent's internal dynamics model. To address the "noisy TV" problem, where agents are perpetually attracted to uninformative stochasticity, robust intrinsic reward mechanisms like Random Network Distillation (RND) were developed, effectively filtering out irrelevant noise and focusing exploration on truly learnable novelty. The introduction of Random Latent Exploration (RLE) [mahankali20248dx] further simplified this, offering deep exploration benefits by pursuing randomly sampled goals in a latent space without complex bonus calculations.

The field has continued to push towards more advanced and adaptive exploration strategies (Section 5). Hierarchical Reinforcement Learning (HRL) enabled structured exploration by decomposing tasks into sub-problems, allowing agents to explore at different levels of temporal abstraction. A particularly impactful direction is meta-learning for exploration, where agents learn *how* to explore effectively across diverse tasks. Algorithms like MAESN [gupta2018rge] demonstrate this by learning structured exploration strategies and latent exploration spaces from prior experience, injecting informed stochasticity into policies and outperforming task-agnostic methods. Furthermore, the development of decoupled exploration and exploitation policies (DEEP) [whitney2021xlu] highlighted that separating these concerns can significantly boost sample efficiency, particularly in sparse reward settings. Integrated frameworks, such as Agent57, combine multiple techniques like RND, episodic memory, and adaptive meta-controllers to achieve state-of-the-art performance across a wide range of challenging environments. Diversity-driven exploration strategies [hong20182pr] also contribute to preventing policies from being trapped in local optima by encouraging varied behaviors. Population-based and evolutionary methods offer a meta-level solution, leveraging multiple agents or meta-optimization to achieve more robust and global exploration in complex reward landscapes.

Finally, the application of these sophisticated exploration methods has expanded into specialized contexts (Section 6), demonstrating their versatility and practical impact. In offline reinforcement learning, where active exploration is impossible, the focus shifted to conservative exploration within fixed datasets, employing uncertainty estimation to avoid out-of-distribution actions [wu2021r67, zi20238ug]. Expert demonstrations and human feedback have been leveraged to guide exploration, significantly improving sample efficiency and overcoming sparse reward challenges in domains like robotics [nair2017crs, lee2021qzk]. Safety-aware exploration has become critical for real-world applications, incorporating constraints and recovery policies to prevent hazardous actions [thananjeyan2020d20]. The challenges of dynamic and open-ended environments, which demand continuous adaptation and robust discovery, are also being addressed [janjua2024yhk, matthews20241yx]. Emerging trends, such as the use of Decision-Pretrained Transformers (DPT) for in-context learning and adaptive exploration [lee202337c], hint at a future where powerful foundation models might inherently possess sophisticated exploration capabilities.

In summary, the field has progressed from simple, often undirected, exploration heuristics to theoretically grounded methods, then to scalable intrinsic motivation for deep RL, and finally to highly adaptive, learned, and integrated strategies. This continuous innovation has enabled RL agents to achieve unprecedented performance and robustness in increasingly complex and diverse tasks, while also addressing critical concerns like safety and data efficiency. The trajectory reflects a deep commitment to overcoming the persistent challenges of the exploration-exploitation dilemma, paving the way for more intelligent and autonomous learning systems.
\subsection{Open Challenges and Theoretical Gaps}
\label{sec:7_2_open_challenges__and__theoretical_gaps}

Despite significant advancements in reinforcement learning (RL) exploration, several fundamental challenges persist, particularly concerning scalability, robustness, sample efficiency, and the enduring gap between theoretical guarantees and practical applicability in complex, real-world settings. Addressing these issues is crucial for enabling RL agents to operate effectively in high-dimensional, stochastic, and open-ended environments.

A primary challenge lies in scaling exploration strategies to extremely high-dimensional or continuous state-action spaces, where traditional methods struggle due to the curse of dimensionality. Early count-based exploration, while effective in tabular settings [Thrun1992], quickly becomes infeasible. Efforts to bridge this gap include methods that generalize counts to high-dimensional spaces, such as using hash codes [Tang20166wr] or pseudo-counts derived from density models [Bellemare2016]. Concurrently, intrinsic motivation, often based on prediction error, emerged as a scalable heuristic. For instance, [Stadie20158af] utilized deep predictive models to generate exploration bonuses in Atari games, demonstrating the potential of learned models to guide exploration in complex visual domains. However, these prediction-error methods, like the Intrinsic Curiosity Module (ICM) [Pathak2017], often suffer from the "noisy TV problem," where uninformative stochasticity in the environment can generate spurious intrinsic rewards, leading to inefficient exploration. [Burda2018] addressed this by proposing Random Network Distillation (RND), which uses the prediction error of a fixed random network, proving more robust to environmental stochasticity and focusing exploration on learnable aspects of the environment. More recently, [Li2023kgk] proposed Implicit Posteriori Parameter Distribution Optimization (IPPDO) to improve exploration by modeling parameter uncertainty with implicit distributions, aiming for more flexible and efficient exploration in deep RL. Similarly, [Rahman2022p7b] introduced Robust Policy Optimization (RPO) to maintain high policy entropy throughout training, preventing premature convergence and ensuring sustained exploration in continuous action spaces.

Another persistent gap exists between methods offering strong theoretical guarantees and those providing practical scalability. Algorithms like R-Max [Strehl2008] provide provable bounds on sample complexity and regret, but their reliance on explicit model learning or tabular representations limits their application to simpler, finite Markov Decision Processes (MDPs). Bridging this gap requires developing principled yet adaptable solutions for real-world complexity. [Song2021elb] attempts this by proposing a computationally and statistically efficient model-based RL algorithm for specific model classes (Kernelized Nonlinear Regulators and linear MDPs) with polynomial sample complexity guarantees. In practical control applications, where accurate models are hard to obtain, methods like ModelPPO [Ma2024r2p] integrate neural network models into actor-critic architectures for AUV path following, demonstrating improved performance over traditional and model-free RL by learning state transition functions. For dynamic systems like microgrids, [Meng2025l1q] employs online RL with SARSA to adapt to uncertainties without relying on traditional mathematical models, prioritizing computational efficiency and real-time adaptability. Similarly, [Xi2024e2i] proposes a lightweight, adaptive SAC algorithm for UAV path planning, which adjusts exploration probability dynamically to balance efficiency and generalization in resource-constrained environments. These works highlight the ongoing tension between theoretical optimality and the need for practical, robust solutions in complex engineering domains.

The challenge of designing exploration strategies that are both sample-efficient and theoretically optimal across diverse tasks, especially in open-ended learning scenarios, remains largely unresolved. Meta-reinforcement learning (meta-RL) offers a promising avenue by learning exploration strategies from prior experience. [Gupta2018rge] introduced MAESN (Model Agnostic Exploration with Structured Noise) to learn structured exploration strategies, which are more effective than task-agnostic noise. [Zhang2020xq9] further developed MetaCURE, an empowerment-driven exploration method for meta-RL that maximizes information gain for task identification in sparse-reward settings. Leveraging offline data can also significantly boost sample efficiency. [Nair2017crs] demonstrated that incorporating demonstrations can overcome exploration difficulties in sparse-reward robotics tasks, while [Ball20235zm] showed how to effectively integrate offline data into online RL with minimal modifications. Offline RL itself faces challenges with out-of-distribution actions, which [Wu2021r67] addresses with Uncertainty Weighted Actor-Critic (UWAC) by down-weighting contributions from uncertain state-action pairs. More recent work explores in-context learning with large transformer models: [Lee202337c] introduced Decision-Pretrained Transformer (DPT), which can learn in-context exploration and conservatism from diverse datasets, generalizing to new tasks. Building on this, [Dai2024x3l] proposed In-context Exploration-Exploitation (ICEE) to optimize this trade-off at inference time, enhancing efficiency. The development of benchmarks like Craftax [Matthews20241yx] further underscores the current limitations of existing methods in achieving deep exploration, long-term planning, and continual adaptation required for truly open-ended learning. Cooperative exploration in multi-agent systems [Hu2020qwm, Yu20213c1] and autonomous navigation [Li2020r8r, Zhelo2018wi8, Kamalova2022jpm] also represent significant real-world complexities demanding robust and adaptable exploration.

Finally, integrating safety constraints into exploration is a critical, yet often conflicting, requirement for real-world deployment. Extensive exploration, while necessary for learning, can lead to dangerous situations. [Thananjeyan2020d20] proposed Recovery RL, which decouples task and recovery policies and learns constraint-violating zones from offline data to safely navigate this trade-off. [Yu20222xi] introduced SEditor, a two-policy approach where a safety editor policy transforms potentially unsafe actions into safe ones, achieving extremely low constraint violation rates. [Zhang2023wqi] advanced safe RL by identifying and avoiding dead-end states, providing a minimal limitation on safe exploration. In reward-free settings, [Yang2023n56] proposed a "guide" agent to learn safe exploration which then regularizes a "student" policy. For deployable safe RL, [Honari202473t] developed Meta SAC-Lag, using meta-gradient optimization to automatically tune safety-related hyperparameters. Addressing model uncertainties for robust safety, [Yu2022bo5] introduced a distributional reachability certificate for safe model-based RL. Furthermore, [Zi20238ug] applied distributionally robust RL for active signal pattern localization, enabling safe exploration in unfamiliar environments with limited data. These efforts highlight the ongoing challenge of balancing the need for exploration with stringent safety requirements, often requiring complex architectural designs or meta-learning approaches.

In conclusion, while the field has made substantial progress from tabular, theoretically-grounded methods to scalable, deep learning-based intrinsic motivation, significant open challenges remain. The persistent gap between methods with strong theoretical guarantees (often for simpler settings) and those providing practical scalability (often heuristic-driven) underscores the critical need for principled yet adaptable solutions. Future research must focus on developing exploration strategies that are robust to environmental stochasticity, sample-efficient across diverse tasks, capable of deep exploration in open-ended environments, and inherently safe for real-world deployment, potentially through hybrid approaches that combine the strengths of model-based reasoning, intrinsic motivation, and meta-learning with strong theoretical foundations.
\subsection{Emerging Trends and Ethical Considerations}
\label{sec:7_3_emerging_trends__and__ethical_considerations}


The frontier of reinforcement learning (RL) exploration is characterized by a dual pursuit: developing increasingly sophisticated agents capable of understanding and navigating complex, open-ended environments, and simultaneously ensuring these autonomous systems operate ethically and safely, particularly in human-interactive or safety-critical domains. This subsection explores cutting-edge research directions, including the transformative integration of large foundation models, the development of truly general-purpose and adaptive exploration agents, the increasing focus on learning better representations, and the critical ethical implications of deploying such intelligent systems.

A pivotal emerging trend is the integration of **large foundation models (LFMs)**, such as Large Language Models (LLMs) and Vision-Language Models (VLMs), to imbue RL agents with more sophisticated world understanding, high-level planning capabilities, and common-sense priors. Traditional RL often struggles with extensive exploration in complex, semantically rich environments due to its limited grasp of underlying decision dynamics. LLMs, with their vast domain-specific knowledge, can serve as powerful prior action distributions, significantly reducing exploration and optimization complexity when integrated into RL frameworks through Bayesian inference methods [yan2024p3y]. This approach can decrease the number of required samples by over 90\% in offline learning scenarios, demonstrating the immense potential of leveraging pre-trained knowledge to guide exploration. Furthermore, LLMs are being directly utilized to generate intrinsic curiosity signals. For instance, Curiosity-Driven Exploration (CDE) for LLMs in Reinforcement Learning with Verifiable Rewards (RLVR) leverages the model's own perplexity (from the actor) and the variance of value estimates (from the critic) as intrinsic bonuses [dai2025h8g]. This framework guides exploration by penalizing overconfident errors and promoting diversity, addressing issues like premature convergence and entropy collapse common in LLM-based RL. Beyond direct policy guidance, LLMs can act as adaptive search operators within meta-learning frameworks, where evolutionary search discovers improved algorithms, and RL fine-tunes the LLM policy based on these discoveries, accelerating algorithm design for complex combinatorial optimization tasks [surina2025smk]. These advancements highlight LFMs' capacity to elevate exploration from low-level state space coverage to high-level conceptual discovery and informed decision-making.

Complementing the rise of LFMs, there is an increasing focus on **learning better representations** to facilitate more informed and efficient exploration. Robust representations are crucial for defining novelty, quantifying uncertainty, and building accurate world models in high-dimensional observation spaces. While earlier methods like the simplified Intrinsic Curiosity Module (S-ICM) [li2019tj1] and its predecessor ICM [pathak2017] leveraged prediction error in learned feature spaces to incentivize novelty, contemporary research pushes for more sophisticated self-supervised techniques that disentangle factors of variation and capture epistemic uncertainty. For example, the Actor-Model-Critic (AMC) architecture for Autonomous Underwater Vehicle (AUV) path-following explicitly learns the state transition function via a neural network, enabling the agent to anticipate environmental dynamics and guide exploration towards informative regions [ma2024r2p]. Beyond explicit model learning, methods like Exploration via Distributional Ensemble (EDE) emphasize the importance of exploration for generalization, not just optimal policy finding [jiang2023qmw]. EDE encourages exploration of states with high epistemic uncertainty using an ensemble of Q-value distributions, implicitly relying on robust representations to quantify this uncertainty effectively. Similarly, Decoupled Exploration and Exploitation Policies (DEEP) demonstrate that separating the task policy from the exploration policy can yield significant sample efficiency improvements in sparse environments, a benefit often amplified by representations that allow for meaningful novelty detection and uncertainty estimation [whitney2021xlu]. These approaches underscore that the quality of learned representations directly impacts an agent's ability to discern truly novel or uncertain aspects of an environment, leading to more directed and less wasteful exploration.

The drive towards **truly general-purpose exploration agents** capable of tackling open-ended problems is leading to more adaptive, robust, and scalable strategies. Rather than relying on fixed heuristics, recent work focuses on agents that can dynamically adjust their exploration behavior. Adaptive exploration strategies, such as those using multi-attribute decision-making based on information entropy and task decomposition, allow for more flexible and context-aware exploration [hu2020yhq]. Further advancing this, ensemble learning schemes with explicit "exploration-to-exploitation (E2E) ratio control" via multiple Q-learning agents and adaptive decay functions enable more nuanced balancing of exploration and exploitation, crucial for real-world applications requiring continuous adaptation [shuai2025fq3]. The scalability and theoretical guarantees of exploration are also paramount for such agents. Thompson sampling-based methods, employing Langevin Monte Carlo (LMC) and approximate sampling, offer provably efficient and scalable exploration in deep RL with theoretical regret bounds, ensuring reliability in autonomous systems [ishfaq20235fo, ishfaq20245to]. This extends to collaborative settings, where randomized exploration in cooperative multi-agent RL (MARL) with methods like CoopTS-PHE and CoopTS-LMC provides theoretical guarantees for regret bounds and communication complexity, essential for complex multi-agent environments [hsu2024tqd]. Moreover, simple yet effective strategies like Random Latent Exploration (RLE), which pursues randomly sampled goals in a latent space, demonstrate that deep exploration can be achieved without complex bonus calculations, promoting broader applicability as a general plug-in for existing RL algorithms [mahankali20248dx]. The concept of meta-learning how to explore is also gaining traction, with approaches like Learned Optimization for Plasticity, Exploration and Non-stationarity (OPEN) meta-learning update rules that incorporate stochasticity for exploration, showing strong generalization across diverse environments and agent architectures [goldie2024cuf]. These advancements, alongside broader discussions on open-ended RL emphasizing hierarchical learning, intrinsic motivation, and unsupervised skill acquisition [janjua2024yhk], signify a shift towards agents that can autonomously learn and adapt their exploration strategies across a wide spectrum of tasks.

Alongside these advancements in exploration capabilities, the **ethical considerations** surrounding autonomous exploration are gaining increasing prominence, especially in safety-critical or human-interactive environments. The inherent trial-and-error nature of RL exploration can lead to "bad decisions" that violate critical safety constraints, as highlighted in reviews of safe RL for power system control [yu2024x53]. This necessitates responsible development and deployment, emphasizing alignment with human values and safety standards. A direct response to this challenge is the "human-in-the-loop deep reinforcement learning (HL-DRL)" approach for optimal Volt/Var control in unbalanced distribution networks [sun2024kxq]. This method allows human intervention to modify dangerous actions during offline training and integrates human guidance into the actor network's loss function, ensuring the learned policy adheres to operational constraints and human safety guidelines. Broader advancements in RL for autonomous systems also explicitly identify safety, dependability, and explainability as critical constraints that limit wide adoption [malaiyappan20245bh]. The imperative is to develop exploration strategies that not only discover optimal behaviors but do so within predefined safe regions, learn to recover from unsafe situations, and provide transparent decision-making processes. This ensures that the learning process does not lead to catastrophic outcomes and adheres to ethical considerations, bridging the gap between autonomous learning and responsible societal impact.

In conclusion, the field is rapidly advancing towards more intelligent, adaptive, and generalizable exploration strategies. This progress is driven by the transformative potential of large foundation models for high-level understanding and goal generation, the continuous refinement of learned representations for informed low-level novelty detection and uncertainty quantification, and the development of meta-learning approaches for truly general-purpose agents. Simultaneously, the increasing power and autonomy of these systems amplify the imperative to address ethical implications, particularly in safety-critical domains. Future research must continue to bridge the gap between theoretical guarantees and practical deployment in highly dynamic real-world scenarios, further integrating human oversight, value alignment, and explainability into the design of autonomous exploration systems to ensure their beneficial and responsible societal impact.


