\subsection{Safety-Aware Exploration}
The exploration phase in reinforcement learning (RL) is critical for discovering optimal policies, yet in real-world, safety-critical applications, unconstrained exploration can lead to catastrophic outcomes and raise significant ethical concerns regarding accountability, fairness in decision-making under risk, and the potential for unforeseen negative side-effects in human-inhabited environments. This subsection delves into methods designed to ensure safety during exploration, navigating the inherent tension between the need for aggressive exploration to achieve optimal performance and the imperative to maintain safe operation. The problem is often formally cast within the framework of Constrained Markov Decision Processes (CMDPs), where an agent aims to maximize cumulative reward while simultaneously satisfying constraints on cumulative costs, such as safety violations \cite{altman1999constrained}. This formalism provides a robust theoretical foundation for developing algorithms that provide safety guarantees during the learning process.

Early efforts to integrate safety into RL exploration focused on establishing explicit boundaries and constraints. A foundational approach involves "safety layers" or "shielding," which act as guardians, restricting the agent's actions or states to predefined safe regions, thereby preventing the agent from entering hazardous situations during learning \cite{Stachurski2008}. While early works laid the groundwork, modern deep RL has seen significant advancements, notably with methods like those proposed by \cite{alshiekh2018safe}, which formally integrate shielding into deep RL agents. These methods enforce explicit safety constraints, ensuring exploration is guided within a permissible operational envelope, effectively mitigating the risk of catastrophic failures. However, a key limitation of static safety layers is their potential to be overly conservative, which can severely restrict the agent's exploration capabilities and prevent the discovery of truly optimal, yet initially unknown, safe behaviors. This conservativeness often stems from the difficulty of accurately predefining safe regions in complex, high-dimensional environments, or from worst-case assumptions made to guarantee safety.

Addressing the limitations of static constraints and the inherent conservativeness, more recent research has explored dynamic and learned safety mechanisms, often decoupling the concerns of task performance and safety. \cite{yu20222xi} introduced SEditor, a two-policy approach that learns a "safety editor policy" to transform potentially unsafe actions proposed by a utility-maximizing policy into safe ones. SEditor represents a conceptual shift from static, predefined shields to a learned, dynamic safety filter, allowing for more nuanced and state-dependent safety interventions. This method moves beyond simplified safety models, enabling the safety editor to learn complex safety functions, effectively acting as a dynamic shield. While SEditor demonstrates significantly lower constraint violation rates and maintains high utility, its effectiveness relies on the ability to train an accurate safety editor policy, which can be challenging in highly dynamic or unpredictable environments.

Further advancing dynamic safety, \cite{thananjeyan2020d20} introduced Recovery RL, which first leverages offline data to learn about constraint-violating zones. It then employs a task policy for reward maximization and a dedicated recovery policy that activates to guide the agent back to safety when constraint violation is likely. This dual-policy structure allows for more aggressive exploration by the task policy, relying on the learned recovery mechanism to prevent unsafe outcomes. Unlike SEditor, which modifies actions *before* execution, Recovery RL focuses on *recovering* from potentially unsafe trajectories, offering a different trade-off between proactive prevention and reactive correction. Recovery RL demonstrates superior efficiency in balancing task success and constraint adherence in complex, contact-rich manipulation tasks and image-based navigation, even on physical robots, by allowing the task policy greater freedom. Similarly, \cite{zhang2023wqi} proposed a method for safe RL with dead-ends avoidance and recovery. This approach constructs a boundary to discriminate between safe and unsafe states, equivalent to distinguishing dead-end states, thereby ensuring maximum safe exploration with minimal limitation. Like Recovery RL, it utilizes a decoupled framework with a task policy and a pre-trained recovery policy, along with a safety critic, to ensure safe actions during online training. This strategy aims to achieve better task performance with fewer safety violations by carefully delineating the extent of guaranteed safe exploration, offering a more precise definition of "safe" exploration boundaries.

Another powerful paradigm for guaranteeing safety, particularly in continuous control systems, draws from control theory: Lyapunov stability and Control Barrier Functions (CBFs). These methods provide formal guarantees that a system's state will remain within a predefined safe set. Control Barrier Functions (CBFs) are functions of the state that define a safe set and whose derivatives can be constrained to render this set forward-invariant, thus preventing the agent from leaving it. \cite{zhang2022dgg} proposed a Barrier Lyapunov Function-based safe RL (BLF-SRL) algorithm for autonomous vehicles. This approach integrates BLF items into an optimized backstepping control method, constraining state variables within a designed safety region during learning. By decomposing optimal control with BLF items, it achieves safe exploration while learning adaptive uncertain items, ensuring both safety and performance optimization in safety-critical domains. While control-theoretic methods like BLF-SRL offer strong, often deterministic, safety guarantees, they typically require an accurate model of the system dynamics and specific assumptions about the environment, a limitation not shared by model-free, data-driven approaches like Recovery RL, which in turn provide only probabilistic safety assurances. Addressing this model dependency, \cite{cheng20224w2} presented a method using Disturbance-Observer-Based Control Barrier Functions (DOB-CBFs). This approach avoids explicit model learning by leveraging disturbance observers to accurately estimate the pointwise value of uncertainty, which is then incorporated into a robust CBF condition. This allows for less conservative safety filters, especially during early learning, by effectively handling unknown disturbances without requiring extensive model training.

The inherent uncertainty in exploration necessitates risk-sensitive approaches. Building on Bayesian exploration principles (as discussed in Section 3.2), some approaches use uncertainty to define credible intervals for constraint satisfaction, leading to more principled conservative exploration. \cite{zhang2024ppn} introduced Lag-U, an uncertainty-augmented Lagrangian safe RL algorithm for autonomous driving. This method uses deep ensembles to estimate epistemic uncertainty, which is then used to encourage exploration and learn a risk-sensitive policy by adaptively modifying safety constraints. Furthermore, it incorporates an intervention assurance mechanism based on quantified uncertainty to select safer actions during deployment. This allows for a better trade-off between efficiency and risk avoidance, preventing overly conservative policies by making safety decisions based on the agent's confidence in its predictions. Complementing this, \cite{yu2022bo5} proposed a distributional reachability certificate (DRC) to address model uncertainties and characterize robust persistently safe states. Their framework builds a shield policy based on DRC to minimize constraint violations, especially during training, by considering the distribution of potential long-term constraint violations, thereby enhancing safety robustness against model uncertainty. Beyond explicit uncertainty estimation, some methods directly manipulate the learning process to balance reward and safety. \cite{gu2024fu3} addressed the conflict between reward and safety gradients, proposing a soft switching policy optimization method. By analyzing and manipulating these gradients, their framework aims to achieve a better balance between optimizing for rewards and adhering to safety constraints, offering a more direct way to mitigate the inherent conflict compared to simply adding penalty terms in CMDPs.

In scenarios where some safety signals are available in a controlled environment, "guided safe exploration" can be employed to facilitate safe transfer learning. \cite{yang2023n56} proposed a method where a "guide" agent learns to explore safely without external rewards in a controlled environment where safety signals are available. This guide then helps compose a safe behavior policy for a "student" agent in a target task where safety violations are prohibited. The student policy is regularized towards the guide while it is unreliable, gradually reducing the guide's influence as it learns. This approach enables safe transfer learning and faster problem-solving in the target task, highlighting the utility of leveraging prior safety knowledge to bootstrap safe exploration in novel, safety-critical settings.

In summary, safety-aware exploration has evolved significantly, moving from static, predefined safety layers and formal constraint satisfaction (CMDPs) to dynamic, learned recovery mechanisms, control-theoretic guarantees (BLFs, CBFs), and risk-sensitive approaches leveraging uncertainty quantification. The field continues to grapple with the fundamental tension between encouraging sufficient exploration for optimal learning and guaranteeing safety in real-world, safety-critical applications. Future directions will likely involve integrating more sophisticated formal verification techniques, developing robust and scalable uncertainty quantification for proactive safety prediction, designing intrinsically safe exploration strategies that adhere to ethical guidelines from the outset, and further refining gradient manipulation techniques to optimally balance conflicting objectives. The goal remains to enable autonomous agents to learn effectively without compromising human or environmental safety, ensuring responsible deployment in increasingly complex and sensitive domains.