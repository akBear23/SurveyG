\subsection*{Exploration in Offline Reinforcement Learning}

In offline Reinforcement Learning (RL), the agent is tasked with learning an optimal policy solely from a fixed, pre-collected dataset, fundamentally precluding any further active interaction with the environment. This paradigm shift introduces unique challenges for exploration, as traditional active exploration strategies, which involve generating new experiences, are inherently impossible. Instead, the core problem transforms into managing *distributional shift* and preventing the policy from querying actions that are out-of-distribution (OOD) relative to the dataset. The focus shifts from active exploration to ensuring 'conservative learning' within the boundaries of the existing data distribution, aiming to identify reliable regions for policy improvement while rigorously avoiding OOD actions that could lead to unreliable value estimates or unsafe behaviors due to extrapolation errors. This conservative approach is paramount for real-world applications where data collection is costly, risky, or simply not feasible during the learning process.

The foundational approaches to offline RL primarily address the distributional shift problem through two main mechanisms: policy constraints and value function pessimism. Seminal works like Batch-Constrained Q-Learning (BCQ) \cite{fujimoto2019off} introduced explicit policy constraints, regularizing the learned policy to stay close to the behavior policy that generated the dataset. This is typically achieved by adding a regularization term (e.g., KL-divergence) to the policy objective, ensuring that the agent does not venture into unobserved state-action pairs. Similarly, Behavior Regularized Actor Critic (BRAC) \cite{wu2019behavior} further explored various forms of behavior regularization to mitigate the distributional shift. While effective in keeping the policy close to the data, these methods can sometimes limit the discovery of truly optimal policies if the behavior policy was suboptimal.

A complementary and highly influential approach is Conservative Q-Learning (CQL) \cite{kumar2020conservative}. CQL tackles the distributional shift by explicitly enforcing pessimism in the value function estimation. It achieves this by adding a penalty to the Q-values of OOD actions, ensuring that the learned Q-function provides a lower bound on the true Q-values. This prevents overestimation of action values for unseen actions, which is a common failure mode in offline RL. While BCQ primarily constrains the policy directly, CQL intervenes at the value function level, indirectly shaping the policy by making OOD actions less attractive. This distinction highlights different points of intervention for ensuring conservatism.

Building upon these foundations, subsequent methods have leveraged uncertainty estimation to guide conservative learning more explicitly. The Uncertainty Weighted Actor-Critic (UWAC) \cite{wu2021r67} explicitly incorporates uncertainty treatment by detecting OOD state-action pairs and down-weighting their contribution in the training objectives. Utilizing a practical dropout-based uncertainty estimation, UWAC laid groundwork for robust learning by mitigating the impact of unreliable data points. Similarly, \cite{rezaeifar20211eu} conceptualized offline RL as "anti-exploration," proposing to subtract a prediction-based exploration bonus from the reward. This innovative approach encourages the policy to remain within the support of the dataset by penalizing actions whose consequences cannot be reliably predicted, effectively extending pessimism-based offline RL methods to deep learning settings. Further, \cite{zhang20244ty} introduced an Entropy-regularized Diffusion Policy with Q-Ensembles, where robust policy improvement is achieved by learning the lower confidence bound of Q-ensembles. This implicitly accounts for uncertainty in value estimates, mitigating the impact of inaccurate value functions from OOD data, while an entropy regularizer improves "exploration" within the offline dataset by encouraging diverse actions within the observed distribution.

Simpler yet effective mechanisms have also emerged to ensure in-sample learning. \cite{ma2024jej} proposed Improving Offline Reinforcement Learning with in-Sample Advantage Regularization (ISAR), which adapts offline RL to robotic manipulation with minimal changes. ISAR learns the state-value function exclusively from dataset samples, then calculates the advantage function based on this in-sample estimation and adds a behavior cloning regularization term. This method effectively mitigates the impact of unseen actions without introducing complex hyperparameters, offering a straightforward approach to conservative learning that implicitly handles OOD issues.

A significant challenge, particularly in model-based offline RL (MBRL), is addressing biased exploration during the synthetic trajectory generation phase. Standard maximum entropy exploration mechanisms, often adopted from online RL, can lead to skewed data distributions and impaired performance when applied to learned dynamics models. To tackle this, \cite{wu2024mak} introduced OCEAN-MBRL (Offline Conservative ExplorAtioN for Model-Based Offline Reinforcement Learning), a novel plug-in rollout approach. OCEAN explicitly decouples exploration from exploitation, introducing a principled, conservative exploration strategy guided by an ensemble of dynamics models for uncertainty estimation. It employs three key constraints: a state evaluation constraint to explore only in low-uncertainty regions, an exploration range constraint to select conservative actions, and a trajectory truncation constraint to limit rollouts in high-uncertainty areas. This comprehensive approach significantly enhances the stability and performance of existing MBRL algorithms by ensuring that exploration within the learned model remains reliable and does not generate new, potentially unsafe, out-of-distribution experiences.

The transition from offline learning to online fine-tuning presents another critical juncture for conservative exploration. While initial conservatism is vital for stable offline learning, a purely pessimistic policy might fail to discover better actions during online interaction. The Simple Unified uNcertainty-Guided (SUNG) framework for offline-to-online RL \cite{guo20233sd} quantifies uncertainty via a VAE-based state-action visitation density estimator. SUNG's adaptive exploitation method applies conservative offline RL objectives to high-uncertainty samples and standard online RL objectives to low-uncertainty samples, demonstrating a sophisticated use of uncertainty to dynamically adjust the degree of conservatism. Building on this, \cite{guo2024sba} proposed Optimistic Exploration and Meta Adaptation (OEMA) for sample-efficient offline-to-online RL. OEMA employs an optimistic exploration strategy, adhering to the principle of optimism in the face of uncertainty, allowing agents to sufficiently explore while reducing distributional shift through meta-learning. Providing a theoretical underpinning for such adaptive strategies, \cite{hu2024085} showed that Bayesian design principles are crucial for offline-to-online fine-tuning, suggesting that a probability-matching agent, rather than purely optimistic or pessimistic ones, can avoid sudden performance drops while being guaranteed to find the optimal policy.

The literature on exploration in offline RL has thus evolved from foundational methods that strictly constrain policies or penalize OOD value estimates to sophisticated, explicit conservative exploration strategies, particularly in model-based settings and during the offline-to-online transition. While significant progress has been made in ensuring learning remains robust and effective without active interaction, a persistent challenge lies in the accuracy, reliability, and computational efficiency of uncertainty estimation itself. Future research directions could focus on developing more robust and scalable uncertainty quantification methods, as well as adaptive mechanisms that dynamically adjust conservative exploration constraints based on the evolving confidence in the learned policy and model, effectively balancing the need for safety with the potential for performance improvement.