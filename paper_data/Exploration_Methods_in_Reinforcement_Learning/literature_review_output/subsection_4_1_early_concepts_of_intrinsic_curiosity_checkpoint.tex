\subsection{Early Concepts of Intrinsic Curiosity}
The challenge of exploration in reinforcement learning, particularly in environments characterized by sparse or delayed extrinsic rewards, led to the development of intrinsic motivation. This paradigm shift moved beyond solely relying on external reward signals, proposing that agents could be driven by an internal 'curiosity' or 'novelty' derived from their own learning progress or model improvement. These foundational concepts laid the theoretical and conceptual groundwork for later, more sophisticated curiosity-driven and novelty-seeking exploration methods.

One of the earliest proponents of intrinsic curiosity was \textcite{Schmidhuber1997}, who introduced the idea of rewarding an agent for improving its world model's predictive accuracy. The agent is intrinsically motivated to explore states where its current model makes inaccurate predictions, thus seeking out "surprising" observations to reduce its uncertainty and improve its understanding of the environment. Building on this, \textcite{Singh2004} further formalized the notion of intrinsic motivation, comparing and contrasting different intrinsic signals such as novelty (unfamiliarity) and surprise (prediction error), providing a more theoretical framework for these internal drives.

As reinforcement learning moved towards more complex, high-dimensional domains, the challenge became scaling these intrinsic curiosity concepts. \textcite{Stadie2015} addressed this by proposing an exploration method that assigned bonuses from a concurrently learned deep predictive model of the system dynamics. This work demonstrated how the early ideas of prediction-error-based curiosity could be extended to tasks requiring raw pixel inputs, like Atari games, by leveraging deep neural networks to parameterize the world model. Further refining the theoretical underpinnings, \textcite{Houthooft2016} introduced Variational Information Maximizing Exploration (VIME), a principled, Bayesian approach that encourages agents to explore by maximizing the information gain about the environment's dynamics model. This method provides a more formal way to quantify and reduce epistemic uncertainty, guiding exploration towards states that are most informative for improving the agent's internal model.

Despite these advancements, prediction-error-based curiosity methods faced a challenge known as the "noisy TV problem," where agents could be perpetually distracted by unlearnable stochastic elements in the environment that constantly generated high prediction errors. To address this, \textcite{Pathak2017} proposed the Intrinsic Curiosity Module (ICM), which computes intrinsic rewards based on the prediction error of future states in a *learned feature space* rather than raw pixel space. By learning a feature representation that is invariant to factors beyond the agent's control, ICM effectively filters out unlearnable stochasticity, allowing curiosity to focus on aspects of the environment that the agent can influence. \textcite{Burda2018} further simplified and improved the robustness of curiosity-driven exploration with Random Network Distillation (RND). RND measures novelty as the prediction error of a fixed, randomly initialized target network's output by a trained prediction network, providing a highly effective intrinsic reward signal that is largely immune to the noisy TV problem because the prediction target is independent of the environment's true dynamics.

The practical utility of these curiosity-driven approaches has been demonstrated across various applications. For instance, \textcite{Li2019tj1} showed how a simplified Intrinsic Curiosity Module (S-ICM) could be effectively integrated with off-policy reinforcement learning methods, significantly improving exploration efficiency and learning performance in robotic manipulation tasks with sparse rewards. Similarly, \textcite{Zhelo2018wi8} applied curiosity-driven exploration to mapless navigation for mobile robots, validating its crucial role in improving deep reinforcement learning performance in tasks with challenging exploration requirements and enhancing generalization capabilities in unseen environments. More recently, \textcite{Sun2022ul9} utilized a similarity-based curiosity module to enable aggressive quadrotor flights, demonstrating how intrinsic motivation can accelerate training and improve the robustness of policies in complex control tasks.

In conclusion, the early concepts of intrinsic curiosity marked a fundamental shift in reinforcement learning, moving from external reward dependence to internal drives based on predictability, surprise, and learning progress. These pioneering ideas, from \textcite{Schmidhuber1997}'s initial formulation of prediction error as a motivator to the more robust and scalable deep learning-driven methods like ICM \textcite{Pathak2017} and RND \textcite{Burda2018}, have provided effective solutions for exploration in sparse-reward environments. While significant progress has been made in making these methods robust to irrelevant stochasticity, ongoing research continues to explore how to design intrinsic reward functions that consistently align with efficient and meaningful exploration across diverse, open-ended domains, and how to balance these internal drives with external task objectives.