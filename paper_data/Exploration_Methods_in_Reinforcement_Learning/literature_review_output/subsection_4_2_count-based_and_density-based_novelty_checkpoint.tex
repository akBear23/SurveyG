\subsection{Count-Based and Density-Based Novelty}
Effective exploration is paramount in reinforcement learning, particularly when agents operate in environments characterized by sparse extrinsic rewards or vast, high-dimensional state spaces. A prominent class of intrinsic motivation methods addresses this by quantifying the 'novelty' or 'unvisitedness' of states, generating internal reward signals that encourage agents to venture into less-frequented regions. This approach aims to foster broad state space coverage, which is often crucial for discovering optimal policies.

The foundational concept of count-based exploration, as pioneered by \cite{thrun1992efficient}, involves assigning an intrinsic bonus to states inversely proportional to their visitation frequency. In tabular or low-dimensional discrete environments, these methods provide a theoretically sound mechanism for directed exploration, ensuring that agents sufficiently explore all reachable states. However, as discussed in Section 2.3, traditional count-based approaches face a critical limitation: the curse of dimensionality. In high-dimensional or continuous state spaces, the probability of revisiting any exact state becomes infinitesimally small. This renders direct state counting impractical, as most states are encountered only once, leading to uniformly high novelty bonuses that fail to guide exploration effectively.

To bridge this gap and enable count-based exploration in deep reinforcement learning, researchers developed sophisticated techniques to approximate state visitation frequencies. Early efforts, such as those by \cite{stadie20158af}, demonstrated that deep predictive models could generate intrinsic exploration bonuses based on learned system dynamics in complex visual environments like Atari games. While not strictly count-based, this work highlighted the potential of neural networks to process high-dimensional observations and produce meaningful intrinsic signals, setting the stage for more direct approximations of novelty.

A pivotal breakthrough in scaling count-based exploration was the introduction of pseudo-counts and density models. \cite{bellemare2016unifying} proposed a unified framework that generalizes count-based exploration by estimating state visitation frequencies using density models, thereby generating "pseudo-counts" for high-dimensional observations. Instead of exact state matching, this approach leverages the statistical likelihood of observing a state given past experiences. States that are less probable under the learned density model are considered more novel and receive higher intrinsic rewards. This effectively overcomes the limitations of exact state enumeration by providing a continuous and differentiable measure of novelty.

Building on this principle, various practical implementations emerged. \cite{tang20166wr} introduced \texttt{\#Exploration}, a surprisingly effective yet simple method that maps high-dimensional states to hash codes. By counting the occurrences of these hash codes, the approach approximates state visitation frequencies, allowing for scalable pseudo-counting. This demonstrated that even a relatively crude approximation of state novelty, when combined with deep reinforcement learning, could yield near state-of-the-art performance on challenging benchmarks. However, the effectiveness of hash-based methods can be sensitive to the choice of hash function and the potential for hash collisions, which might conflate distinct states. Further refining the use of neural networks for density estimation, \cite{ostrovski2017count} explicitly employed neural density models to compute pseudo-counts. This provided a more principled and robust statistical approach to estimate state novelty in complex visual environments, as the density model can learn more meaningful representations of states and their relationships. The challenge with such methods lies in the computational complexity of training accurate density models in high-dimensional spaces and ensuring that the learned density truly reflects meaningful novelty rather than irrelevant stochasticity.

While count-based and density-based methods primarily focus on the frequency of state visitation, other intrinsic motivation techniques, such as curiosity-driven exploration, leverage prediction error as a proxy for novelty. Methods like the Intrinsic Curiosity Module (ICM) by \cite{pathak2017curiosity} and Random Network Distillation (RND) by \cite{burda2018exploration} reward agents for encountering states where their internal predictive models are inaccurate or for states that lead to unpredictable outcomes. These approaches offer an alternative perspective on novelty, focusing on the agent's learning progress or uncertainty about environmental dynamics rather than mere visitation frequency. Although distinct in their underlying signals, both paradigms share the common goal of generating intrinsic rewards to drive exploration in sparse-reward, high-dimensional settings, with density-based methods providing a statistical measure of "unvisitedness" and prediction-error methods focusing on "unpredictability."

In summary, count-based and density-based novelty methods have undergone a significant evolution, transforming from simple heuristics for discrete environments into sophisticated deep learning techniques capable of scaling to complex, high-dimensional state spaces. The transition from direct state counting to pseudo-counts derived from neural density models has been critical for enabling robust exploration in deep reinforcement learning. These techniques provide a practical and often effective way to incentivize broad state space coverage and discover new areas. Nevertheless, challenges persist, including the computational overhead of training accurate density models, the sensitivity to state representation, and the difficulty of ensuring that the quantified novelty aligns with task-relevant exploration rather than being misled by uninformative stochasticity. Future research continues to refine these methods, often by integrating insights from both visitation statistics and predictive uncertainty, to develop more adaptive and robust novelty-seeking agents.