\subsection*{Exploration in Dynamic and Expanding Environments}

The challenge of efficient exploration intensifies significantly in Reinforcement Learning (RL) when agents must operate in environments where the state and action spaces are not static but continually expand or evolve \cite{yang2021ngm}. Unlike traditional Markov Decision Processes (MDPs) that assume fixed state and action sets, real-world systems often undergo updates, introducing novel states, actions, or even entire sub-environments. This dynamic nature necessitates exploration strategies that can efficiently discover and integrate new information without incurring computationally prohibitive retraining costs or suffering from catastrophic forgetting of previously acquired knowledge. This specialized context forms a crucial subset of lifelong and continual learning, where agents must adapt to an unending stream of tasks or environmental changes \cite{janjua2024yhk, fu20220cl}.

The concept of Incremental Reinforcement Learning (Incremental RL) has emerged to specifically address this challenge, focusing on how agents can efficiently adapt their policies to newly introduced states and actions. While lifelong learning broadly concerns sequential task learning and knowledge transfer \cite{fu20220cl, woczyk20220mn}, Incremental RL distinguishes itself by tackling the explicit *expansion* of the underlying MDP structure. A seminal contribution by \cite{ding2023whs} formally defines Incremental RL and proposes the Dual-Adaptive $\epsilon$-greedy Exploration (DAE) algorithm. This approach confronts the inherent inefficiency of standard exploration methods and the strong inductive biases that can arise from extensive prior learning, which often hinder adaptation to expanding environments. DAE employs a Meta Policy ($\Psi$) to adaptively determine a state-dependent $\epsilon$ by assessing the exploration convergence of a state (via TD-Error rate), thereby deciding *when* to explore. Concurrently, an Explorer ($\Phi$) guides the agent to prioritize "least-tried" actions by estimating their relative frequencies, addressing *what* to explore. Crucially, DAE also incorporates strategies for incrementally adapting deep Q-networks by reusing trained policies and intelligently initializing new neurons and Q-values. This architectural flexibility, combined with the dual-adaptive exploration mechanism, significantly reduces training overhead and enables robust adaptation to expanding search spaces without retraining from scratch.

The need for adaptive exploration in dynamic settings is also highlighted by research into non-stationary environments, which share some conceptual overlaps with expanding environments, though they differ mechanistically. For instance, \cite{steinparz20220nl} introduces Reactive Exploration to cope with continual domain shifts in lifelong reinforcement learning. This work demonstrates that policy-gradient methods benefit from strategies that track and react to non-stationarities, such as changes in reward functions or environmental dynamics, within an otherwise fixed state-action space. While Reactive Exploration focuses on adapting to *changes* in existing elements, DAE specifically addresses the *addition* of new states and actions. However, both underscore the broader necessity for exploration strategies that can actively adapt to environmental changes, rather than relying on static or pre-defined exploration schedules. Similarly, the importance of exploration for generalization to new, unseen environments, as explored by \cite{jiang2023qmw}, aligns with the goals of Incremental RL. Their Exploration via Distributional Ensemble (EDE) method encourages exploration of states with high epistemic uncertainty, which is crucial for acquiring knowledge that aids decision-making in novel situations. While EDE aims to generalize within a potentially vast but fixed environment, DAE's focus is on efficiently integrating entirely new components into the agent's operational space, a distinction critical for truly open-ended learning systems \cite{janjua2024yhk}.

Other advanced exploration techniques, such as those leveraging intrinsic motivation \cite{houthooft2016yee} or information gain maximization \cite{aubret2022inh}, aim to improve exploration efficiency by incentivizing agents to visit novel states or reduce uncertainty about the environment dynamics. For example, Variational Information Maximizing Exploration (VIME) \cite{houthooft2016yee} uses Bayesian neural networks to maximize information gain about environment dynamics. While powerful in static high-dimensional environments, their direct applicability and scalability to *continually expanding* state and action spaces present unique challenges. Prediction-error-based methods, like those underlying many intrinsic motivation approaches, may struggle when the underlying dynamics model requires continuous architectural restructuring rather than just parameter updates. The sudden introduction of entirely new states or actions can render existing prediction models inaccurate or incomplete, requiring significant re-learning or architectural modifications that are not inherently handled by these methods. Count-based or density-based novelty methods, while effective for discovering unvisited regions, would need robust mechanisms to distinguish truly *new* states/actions from merely *unvisited* ones within the previously known space, and to efficiently update their density estimations for an ever-growing space. DAE's explicit focus on identifying and prioritizing newly available actions and states, overcoming the inductive bias from extensive prior learning, offers a more targeted solution to these architectural and knowledge-transfer challenges.

The integration of such adaptive exploration strategies with broader lifelong learning frameworks is a critical direction. Lifelong RL methods, such as model-based Bayesian approaches that estimate a hierarchical posterior to distill common task structures \cite{fu20220cl}, offer mechanisms for backward transfer and efficient learning across related tasks. However, these often assume a fixed set of potential tasks or a stable underlying model structure. The challenge of Incremental RL lies in the dynamic *growth* of this structure, requiring not just transfer but also efficient architectural expansion and robust exploration of truly novel elements. Learned optimization methods, which meta-learn update rules to handle non-stationarity, plasticity loss, and exploration \cite{goldie2024cuf}, offer a promising avenue by building adaptability directly into the learning process, potentially complementing DAE's specific exploration mechanisms.

In conclusion, the progression towards "Exploration in Dynamic and Expanding Environments" marks a crucial intellectual shift in Reinforcement Learning, moving beyond the static MDP assumption towards more realistic, evolving systems. While foundational exploration methods provide general tools, the work on Incremental RL, particularly the Dual-Adaptive $\epsilon$-greedy Exploration \cite{ding2023whs}, offers a targeted solution for environments where state and action spaces continually grow. Future research in this area will likely focus on extending these adaptive exploration strategies to more complex, partially observable, or even multi-agent expanding environments, further enhancing the lifelong learning capabilities of RL agents in truly dynamic real-world scenarios, and integrating them more deeply with meta-learning and continual learning paradigms to address catastrophic forgetting and efficient knowledge transfer in ever-growing systems.