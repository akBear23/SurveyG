\subsection{Robust Intrinsic Rewards: Addressing the Noisy TV Problem}
The quest for effective exploration in reinforcement learning (RL) is profoundly challenged by environments offering sparse extrinsic rewards. While intrinsic motivation methods emerged as a promising avenue to generate internal curiosity signals and alleviate this sparsity, early prediction-error approaches frequently succumbed to the "noisy TV problem." This phenomenon describes scenarios where agents are perpetually drawn to unpredictable yet uninformative stochastic elements within the environment, such as random pixel noise on a screen or flickering lights. These elements generate consistently high prediction errors, leading to spurious curiosity that distracts the agent from truly novel and learnable aspects of the environment, thereby hindering focused exploration.

Initial efforts, such as those by \cite{stadie20158af}, explored incentivizing exploration through bonuses derived from concurrently learned deep predictive models of system dynamics. This aimed to reward agents for encountering states that challenged their current environmental understanding. A prominent example of this paradigm is the Intrinsic Curiosity Module (ICM) \cite{Pathak2017}. ICM trains a forward dynamics model to predict the next state's features given the current state's features and the executed action. The magnitude of this prediction error then serves as an intrinsic reward, encouraging the agent to visit states where its internal model is inaccurate. While ICM provided a scalable solution for high-dimensional visual inputs by operating in a learned feature space, its reliance on predicting *future states* made it inherently susceptible to the noisy TV problem. In environments with uninformative stochasticity, the forward dynamics model would consistently fail to predict these truly random, irreducible changes, resulting in persistently high prediction errors. This spurious curiosity would then cause the agent to repeatedly visit these uninformative areas, diverting computational resources and hindering progress towards task-relevant exploration.

To overcome the critical limitations posed by the noisy TV problem, \cite{Burda2018} introduced Random Network Distillation (RND), a pivotal innovation in robust intrinsic reward generation. RND fundamentally redefines novelty detection by decoupling the intrinsic reward from the learnability of the environment's true dynamics. Instead of predicting future states, RND employs two neural networks: a fixed, randomly initialized target network ($f$) and a predictor network ($\hat{f}$). Both networks receive the current state $s_t$ as input. The predictor network is trained to predict the output of the target network, i.e., $\hat{f}(s_t) \approx f(s_t)$. The intrinsic reward is then defined as the mean squared error between the outputs of these two networks: $r_i = ||\hat{f}(s_t) - f(s_t)||^2$.

The key insight of RND lies in its design: for any given state $s_t$, even one containing uninformative stochasticity (like a noisy TV screen), the randomly initialized target network $f(s_t)$ produces a *fixed and deterministic* output embedding. The predictor network $\hat{f}(s_t)$ is then trained to learn this fixed mapping. If an agent repeatedly visits a state $s_t$, the predictor $\hat{f}$ will eventually learn to accurately map $s_t$ to $f(s_t)$, causing the prediction error and thus the intrinsic reward to *decay*. This mechanism effectively filters out irrelevant stochasticity because the reward is based on the *novelty of the state itself* (i.e., how well the predictor has learned to map that specific state to its fixed target), rather than the unpredictability of state transitions. Consequently, even a noisy TV state, despite its visual randomness, yields a fixed target output from $f$. Once the agent has sufficiently explored this state, $\hat{f}$ learns the mapping, and the curiosity bonus diminishes, preventing perpetual attraction. This contrasts sharply with ICM, where the prediction error for a truly stochastic transition remains irreducible, leading to persistent curiosity.

RND's robust and simpler mechanism for novelty detection proved highly effective, leading to significantly more efficient and directed exploration in complex visual domains, such as Atari games, where it substantially outperformed prior methods on hard exploration tasks \cite{Burda2018}. Its success underscored the importance of designing intrinsic reward signals that are resilient to environmental noise and focus on aspects of the environment truly conducive to learning. Subsequent works, such as \cite{li2019tj1}, further explored simplified variants of curiosity modules, demonstrating how architectural or methodological simplifications could enhance the practical utility and integration of prediction-error based intrinsic motivation, particularly for off-policy reinforcement learning methods.

While RND provided a powerful solution to the noisy TV problem, it is not the sole robust intrinsic motivation strategy. Other approaches also aim to mitigate the effects of uninformative stochasticity or enhance exploration in complementary ways. For instance, methods based on model disagreement or ensembles, which reward exploration in states where multiple learned dynamics models disagree, can implicitly discount uncontrollable noise by focusing on areas where the agent's *learnable* understanding is inconsistent. Information-theoretic perspectives, as surveyed by \cite{aubret2022inh}, often emphasize maximizing *useful* information gain, which aligns with RND's implicit discounting of unlearnable noise. Furthermore, diversity-driven exploration strategies, which incentivize agents to visit states that are distinct from previously encountered ones \cite{hong20182pr}, or Random Latent Exploration (RLE), which encourages agents to pursue randomly sampled goals in a latent space \cite{mahankali20248dx}, offer alternative mechanisms for robust and deep exploration without relying solely on prediction error. Deep Curiosity Search (DeepCS) \cite{stanton20183fs} also introduced the concept of "intra-life novelty," rewarding exploration within a single episode, which can complement RND's "across-training novelty" by encouraging immediate discovery.

Despite these significant advancements, challenges persist. While RND effectively addresses the noisy TV problem by focusing on learnable novelty, the intrinsic reward signal can still be somewhat undirected, potentially leading to exploration of areas that are novel but not necessarily relevant to the extrinsic task. Future research continues to explore how to imbue intrinsic rewards with more task-relevant directionality, perhaps through goal-conditioned or hierarchical approaches, or how to combine them with other exploration strategies to achieve even more efficient and purposeful exploration in increasingly complex and open-ended environments. The integration of RND into advanced agents like Agent57 \cite{Badia2020Agent57} further demonstrates its lasting impact as a foundational component for achieving state-of-the-art performance in diverse and challenging domains.