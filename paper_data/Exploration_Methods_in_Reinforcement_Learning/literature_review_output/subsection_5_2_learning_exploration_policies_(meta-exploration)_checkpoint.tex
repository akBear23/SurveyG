\subsection{Learning Exploration Policies (Meta-Exploration)}

A pivotal advancement in reinforcement learning (RL) exploration shifts the paradigm from relying on hand-crafted heuristics or fixed intrinsic reward functions to enabling agents to autonomously learn their own exploration strategies. This sophisticated approach, termed meta-exploration, involves an outer-loop optimization process that trains a meta-controller or a recurrent policy to generate adaptive exploration behaviors, aiming to maximize long-term returns across a distribution of tasks or episodes. By learning "how to explore," these methods can dynamically adjust the exploration-exploitation trade-off, leading to more efficient, task-relevant discovery and robust performance, particularly in novel and complex environments \cite{duan2016rll, wang2016learning}. This represents a significant stride towards autonomous and intelligent exploration, where agents generalize their exploration capabilities rather than relearning them from scratch for each new task.

The foundational concept of meta-exploration was significantly advanced by works demonstrating that recurrent neural networks (RNNs) can serve as meta-learners. \cite{duan2016rll} introduced RL$^2$ (Reinforcement Learning to Reinforce Learn), a seminal framework where an RNN-based agent is trained to solve a distribution of tasks. The RNN's hidden state effectively encodes task-specific information and past experience, allowing it to learn an exploration strategy that adapts within a single episode and across multiple episodes of a new, unseen task. This enables the agent to exhibit rapid adaptation and efficient exploration behaviors, such as directed search or uncertainty-driven probing, without explicit hand-engineered exploration bonuses. Similarly, \cite{wang2016learning} explored the idea of "Learning to Reinforce Learn," where an RNN acts as a meta-learner to discover an entire RL algorithm, including its exploration component, by processing sequences of observations, actions, and rewards. These approaches highlight the power of recurrent architectures to implicitly capture and execute sophisticated exploration policies that generalize across related tasks.

Building on these foundations, subsequent research has focused on learning more structured and informed exploration strategies. \cite{gupta2018rge} proposed Model Agnostic Exploration with Structured Noise (MAESN), a gradient-based meta-learning algorithm that learns exploration strategies from prior experience. MAESN leverages prior tasks to initialize a policy and acquire a latent exploration space, which injects structured stochasticity into the policy. This allows for exploration strategies that are informed by previous knowledge, moving beyond simple action-space noise and proving more effective than task-agnostic methods. This work underscores the benefit of meta-learning not just the policy, but the *mechanism* of exploration itself, enabling more targeted and efficient discovery.

Meta-learning has also been applied to the generation and refinement of intrinsic motivation signals for exploration. \cite{zhang2020xq9} introduced MetaCURE (Meta Reinforcement Learning with Empowerment-Driven Exploration), which explicitly models an exploration policy learning problem separate from the exploitation policy. MetaCURE employs a novel empowerment-driven exploration objective that aims to maximize information gain for task identification, deriving a corresponding intrinsic reward. By learning separate, context-aware exploration and exploitation policies and sharing task inference knowledge, MetaCURE significantly enhances exploration efficiency in sparse-reward meta-RL tasks. This demonstrates how meta-learning can discover effective intrinsic reward functions that guide exploration towards truly informative experiences, addressing a key challenge in intrinsic motivation.

Furthermore, meta-exploration has been integrated with other advanced RL paradigms. \cite{rimon20243o6} presented MAMBA, a model-based approach to meta-RL that leverages world models for efficient exploration. By learning an internal model of the environment, MAMBA can plan and explore more effectively, leading to greater return and significantly improved sample efficiency (up to 15x) compared to existing meta-RL algorithms, especially in higher-dimensional domains. This highlights the synergy between learning environmental models and meta-learning exploration strategies. Complementing this, \cite{goldie2024cuf} introduced Learned Optimization for Plasticity, Exploration and Non-stationarity (OPEN), which meta-learns an update rule (an optimizer) whose input features and output structure are informed by solutions to common RL difficulties, including exploration. OPEN's parameterization is flexible enough to use stochasticity for exploration, demonstrating that meta-learning can discover effective policy update mechanisms that inherently promote efficient exploration.

It is crucial to distinguish meta-exploration from merely adaptive exploration, where exploration parameters are tuned within a single learning process. While methods that dynamically adjust exploration probabilities based on metrics like information entropy \cite{hu2020yhq} or ensemble learning for balancing exploration-exploitation ratios \cite{shuai2025fq3} are valuable, they typically do not involve an outer-loop meta-training process to learn a generalizable exploration *strategy* across tasks. Such adaptive approaches are better categorized under integrated and adaptive exploration frameworks (Section 5.3), which focus on dynamic parameter adjustment rather than learning the exploration algorithm itself.

In conclusion, the shift towards learning exploration policies through meta-exploration represents a profound step towards truly autonomous and intelligent agents. These approaches, ranging from recurrent policies learning exploration behaviors across episodes to meta-learning structured exploration noise, intrinsic motivation signals, or even entire optimization rules, empower agents to generalize their exploration capabilities and achieve robust performance across diverse problem settings. However, significant challenges persist, including the computational expense of meta-training, the difficulty of defining appropriate and diverse task distributions for meta-learning, ensuring the learned exploration strategies generalize to truly novel and out-of-distribution tasks, and designing effective meta-objectives that capture desirable exploration properties. Future research will likely focus on developing more robust, sample-efficient, and generalizable meta-learning algorithms that can discover truly novel and effective exploration strategies across a wide spectrum of tasks, pushing the boundaries of autonomous discovery.