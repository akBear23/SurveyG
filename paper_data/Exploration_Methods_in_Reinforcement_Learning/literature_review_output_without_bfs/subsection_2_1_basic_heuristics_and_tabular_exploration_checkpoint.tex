\subsection{Basic Heuristics and Tabular Exploration}

Early reinforcement learning (RL) research primarily confronted the exploration-exploitation dilemma within tabular settings, where the finite and discrete nature of state-action spaces allowed for explicit enumeration and tracking of environmental knowledge. This context facilitated the development of straightforward, yet effective, heuristics designed to ensure adequate coverage of the state-action space, thereby laying conceptual groundwork for more advanced exploration strategies \cite{sutton2018reinforcement}.

One of the most widely adopted and fundamental heuristics is $\epsilon$-greedy exploration. This strategy operates by typically selecting the action with the highest estimated value (exploitation), but with a small probability $\epsilon$, it chooses an action uniformly at random (exploration). This mechanism is crucial for preventing premature convergence to suboptimal policies, as it guarantees that the agent will occasionally deviate from its current best-known path, potentially discovering more rewarding actions or states. The effectiveness of $\epsilon$-greedy can be refined through a decaying $\epsilon$ schedule, where the probability of random action gradually diminishes as the agent accumulates more experience, thereby transitioning from a phase of aggressive exploration to one of increased exploitation \cite{sutton2018reinforcement}. However, a significant limitation of $\epsilon$-greedy is its undirected nature; random actions may be highly inefficient in large state spaces or environments characterized by sparse rewards, as they do not prioritize informative exploration.

Boltzmann (or softmax) exploration offers a more nuanced approach to balancing exploration and exploitation. Instead of making uniform random choices, this method selects actions probabilistically, with the probability of choosing an action being proportional to its estimated value, scaled by a temperature parameter. A higher temperature encourages more diverse, random exploration by making action probabilities more uniform, while a lower temperature leads to more deterministic behavior, favoring actions with higher estimated values. This graded level of exploration can be more effective than purely random choices in certain tabular scenarios, as it allows actions with slightly lower but still promising values a reasonable chance of selection \cite{sutton2018reinforcement}.

Building on the principle of directly incentivizing novelty, early work also introduced explicit count-based methods. For instance, \cite{Thrun1992} proposed augmenting rewards for visiting less-frequented states. This approach directly leverages state visitation frequencies: states or state-action pairs that have been encountered fewer times receive an additional exploration bonus, effectively encouraging the agent to explore unknown or rarely visited regions of the state space. By prioritizing areas where its knowledge is limited, the agent is intrinsically motivated to gather more information, systematically reducing uncertainty about the environment's true dynamics and reward structure. This direct encouragement of novelty in tabular settings served as a crucial precursor to later, more sophisticated intrinsic motivation concepts, where information gain or prediction error becomes the basis for internal reward signals \cite{aubret2022inh, dai2025h8g}.

In the simplified context of multi-armed bandits, the Upper Confidence Bound (UCB) algorithm emerged as a powerful heuristic that embodies the principle of "optimism in the face of uncertainty." UCB selects actions by adding an exploration bonus to the estimated reward, where this bonus is inversely proportional to the number of times an action has been chosen. This design ensures that actions that are less explored or have higher associated uncertainty are optimistically valued, making them more attractive for selection \cite{auer2002finite}. While initially developed for bandit problems, UCB's underlying principle of balancing estimated reward with an uncertainty bonus provided a critical conceptual foundation for more advanced, theoretically grounded exploration strategies in full Markov Decision Processes (MDPs).

These basic heuristics—$\epsilon$-greedy, Boltzmann exploration, count-based bonuses, and UCB for bandits—collectively established the fundamental idea of directly encouraging novelty and systematically reducing uncertainty within tabular environments. They provided practical initial solutions for environments with discrete and manageable state spaces, demonstrating that even simple mechanisms could significantly improve learning efficiency by ensuring sufficient state-action space coverage \cite{ghasemi2024j43}.

Despite their foundational importance, these early tabular exploration methods faced severe limitations, primarily their inherent lack of scalability. Their reliance on explicit state enumeration, precise visitation counts, or tractable model learning in finite spaces meant they quickly became impractical for larger or continuous state and action spaces. This issue, often termed the "curse of dimensionality," arises because the computational and memory requirements for maintaining explicit counts, value tables, or accurate tabular models grow exponentially with the number of states and actions. Consequently, for real-world problems characterized by high-dimensional observations—such as those encountered in robotics or complex game environments—these approaches become infeasible. The need to generalize knowledge across states and to approximate visitation counts in high-dimensional spaces, for instance through pseudo-counts \cite{tang20166wr, martin2017bgt}, highlighted the critical need for more scalable exploration mechanisms, paving the way for subsequent research into model-based planning and other advanced techniques discussed in later sections.