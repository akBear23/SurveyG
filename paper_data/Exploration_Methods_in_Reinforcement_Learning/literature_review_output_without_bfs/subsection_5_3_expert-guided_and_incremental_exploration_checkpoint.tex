\subsection{Expert-Guided and Incremental Exploration}
Beyond purely autonomous exploration, specialized strategies in reinforcement learning (RL) are increasingly leveraging external guidance or adapting to dynamic environments to enhance efficiency and robustness. This subsection delves into two critical paradigms: expert-guided exploration, which integrates domain-specific knowledge or demonstrations to steer the agent, and incremental exploration, designed to efficiently adapt to continually expanding state and action spaces. These approaches collectively signify a growing trend towards more realistic and adaptable RL systems capable of operating in complex, dynamic scenarios.

The fundamental challenge of exploration, particularly in environments with sparse rewards, has long been a bottleneck for RL algorithms \cite{nair2017crs}. Expert demonstrations offer a powerful means to overcome this, providing valuable initial guidance. Early methods focused on leveraging *offline* demonstrations or pre-existing policies to jump-start RL training \cite{uchendu20221h1, hansen2022jm2}. For instance, \cite{uchendu20221h1} proposed Jump-Start Reinforcement Learning (JSRL), using a guide-policy to form a curriculum of starting states for an exploration-policy, significantly improving sample complexity. Similarly, \cite{hansen2022jm2} introduced MoDem, accelerating visual model-based RL by incorporating demonstrations through policy pretraining, targeted exploration, and oversampling, thereby addressing the exploration bottleneck in model-based settings. Further works explored integrating human knowledge bases \cite{liu20228r4} or simple modifications to off-policy methods for effective offline data utilization \cite{ball20235zm}. While effective, these methods often contend with a "distribution gap" between the static demonstration data and the agent's evolving online experiences, limiting generalization \cite{rafailov2024wtw}. Approaches like Demonstration-guided EXploration (DEX) by \cite{huang202366f} attempted to mitigate this by estimating expert-like behaviors and using non-parametric regression for guidance at unobserved states, yet the core reliance on pre-collected data remained. The challenge of efficiently selecting these costly demonstrations was addressed by \cite{hou20248b2}'s EARLY algorithm, which actively queries episodic demonstrations based on trajectory-level uncertainty.

A significant advancement in expert-guided exploration is presented by \cite{coelho2024oa6} with RLfOLD, a novel Reinforcement Learning from Demonstrations (RLfD) framework for urban autonomous driving. RLfOLD directly tackles the distribution gap by leveraging *online demonstrations* collected dynamically from a simulator's privileged information, integrating them into a single replay buffer alongside agent experiences. A key innovation is its dual standard deviation policy network, which outputs distinct $\sigma_{RL}$ for exploration and $\sigma_{IL}$ for imitation, allowing for adaptive balancing of these learning objectives. Furthermore, exploration is enhanced by an uncertainty-based mechanism that selectively invokes the online expert to guide the agent in challenging situations, making exploration more targeted and efficient in safety-critical domains. This online, adaptive guidance contrasts with methods that rely on static rule bases \cite{hou2021c2r} or pre-defined state-action permissibility knowledge \cite{mazumder2022deb}, offering a more dynamic and responsive form of expert intervention. The concept of using demonstrations to guide exploration has also been extended to training large language models for reasoning through reverse curriculum reinforcement learning \cite{xi2024tj9} and for robotic manipulation with vision-language-action models \cite{lu2025j7f}, showing the versatility of expert guidance.

Concurrently, the paradigm of incremental exploration addresses the challenge of environments where the state and action spaces are not static but continually expanding. This scenario, formally defined as Incremental Reinforcement Learning by \cite{ding2023whs}, is a departure from traditional RL assumptions and is highly relevant for real-world applications where systems evolve. \cite{ding2023whs} introduces Dual-Adaptive $\epsilon$-Greedy Exploration (DAE), a method designed to efficiently adapt to these expanding spaces. DAE operates with a Meta Policy ($\Psi$) that adaptively determines a state-dependent $\epsilon$ by assessing the exploration convergence of a state (via TD-Error rate), deciding *when* to explore. Simultaneously, an Explorer ($\Phi$) guides the agent to prioritize "least-tried" actions by estimating their relative frequencies, addressing *what* to explore. This dual-adaptive mechanism specifically handles new states and actions, mitigating the strong inductive bias from prior learning that would otherwise hinder exploration in an expanding environment.

DAE's approach to exploration builds upon and refines general exploration strategies. The Meta Policy's use of TD-Error rate to gauge uncertainty aligns with uncertainty-oriented exploration methods that leverage distributional RL to estimate value function variance \cite{mavrin2019iqm, oh2022cei} or information gain \cite{houthooft2016yee, nikolov20184g9, sukhija2024zz8, jiang2023qmw}. Similarly, the Explorer's focus on "least-tried" actions resonates with count-based and novelty-seeking exploration strategies \cite{tang20166wr, martin2017bgt, conti2017cr2, hong20182pr, stanton20183fs}. However, DAE's innovation lies in adapting these principles to the unique challenge of *expanding* environments, ensuring that newly introduced states and actions are efficiently discovered without excessive retraining overhead. This is crucial for lifelong learning settings where environments are non-stationary \cite{steinparz20220nl} or resources are restricted \cite{wang2022t55}.

In conclusion, the intellectual trajectory of exploration methods is clearly shifting towards more adaptive and context-aware strategies. Expert-guided exploration, particularly with online demonstrations as demonstrated by \cite{coelho2024oa6}, effectively bridges the distribution gap and offers targeted guidance in complex, safety-critical domains. Concurrently, incremental exploration, exemplified by \cite{ding2023whs}'s DAE, provides a robust framework for agents to adapt to continually expanding environments, a more realistic scenario for many real-world applications. While significant progress has been made, challenges remain in the practical deployment of online experts (especially without privileged simulator information), the scalability of incremental methods to highly complex and rapidly changing architectures, and the seamless integration of these two powerful paradigms to create truly masterful and general RL agents. Future research will likely explore hybrid approaches that combine dynamic expert interaction with robust incremental learning capabilities, pushing the boundaries of autonomous intelligence in evolving systems.