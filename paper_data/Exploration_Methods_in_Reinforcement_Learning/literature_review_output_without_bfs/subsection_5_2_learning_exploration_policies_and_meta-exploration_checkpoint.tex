\subsection*{Learning Exploration Policies and Meta-Exploration}

Effective exploration is paramount for success in reinforcement learning, particularly in complex environments with sparse rewards. Moving beyond fixed heuristics or hand-crafted intrinsic reward functions, a significant advancement involves training agents to learn *how* to explore, dynamically adapting their strategy to the task and environment. This paradigm, often leveraging meta-learning, enables agents to acquire sophisticated exploration policies that maximize future returns over multiple episodes or tasks, representing a crucial step towards more intelligent and adaptive learning systems.

**Core Meta-Learning of Exploration Policies**
A foundational step in this direction was taken by \cite{Pathak2017}, who introduced a meta-policy gradient approach to learn an exploration policy. This method trains an agent to generate intrinsic rewards that guide exploration, effectively learning a meta-policy that optimizes the discovery of novel and rewarding experiences across a distribution of tasks. This contrasts with fixed intrinsic motivation methods by making the *generation* of the exploration signal itself a learned process. Building on this, \cite{gupta2018rge} further refined meta-exploration by learning structured exploration strategies. Their Model Agnostic Exploration with Structured Noise (MAESN) algorithm leverages prior experience to acquire a latent exploration space, injecting structured stochasticity into the policy. This allows for more effective exploration informed by past tasks, moving beyond simple random noise to a learned, task-aware exploration behavior. Addressing the specific challenge of sparse-reward tasks within meta-RL, \cite{zhang2020xq9} proposed MetaCURE, which explicitly separates exploration and exploitation policies and introduces an empowerment-driven objective. This objective maximizes information gain for task identification, enabling efficient discovery of informative experiences in challenging settings by actively seeking out states that best disambiguate the current task. A distinct approach to learning exploration policies is presented by \cite{goldie2024cuf} with their Learned Optimization for Plasticity, Exploration and Non-stationarity (OPEN) method. Instead of learning a policy directly, OPEN meta-learns an update rule (an optimizer) that inherently incorporates mechanisms for exploration, plasticity, and coping with non-stationarity. This demonstrates how the very process of learning can be optimized to facilitate exploration, offering a meta-learning perspective on the optimization landscape itself.

**Adaptive Ensembles and Model-Based Meta-Exploration**
A pinnacle of learned exploration is \cite{Badia2020}'s Agent57, which achieved state-of-the-art performance across a wide range of tasks by adaptively combining multiple intrinsic motivation techniques, such as Random Network Distillation (RND) and Value Discrepancy Networks (VDN), with UCB-like bonuses. The key innovation lies in a meta-controller that dynamically adjusts the weighting of these diverse intrinsic signals. This allows the agent to learn an optimal, context-dependent exploration strategy for diverse environments, demonstrating the power of integrating and adaptively managing various exploration mechanisms rather than relying on a single, static approach. This adaptive weighting mechanism is crucial for balancing different types of novelty and uncertainty. Complementing this, \cite{rimon20243o6} introduced MAMBA, a model-based approach for meta-reinforcement learning that significantly improves sample efficiency in meta-exploration. By learning an effective world model, MAMBA can generate synthetic experiences and plan more efficiently, allowing the agent to explore hypothetical scenarios and propagate value information more rapidly across tasks. This integration of model-based reasoning within a meta-RL framework offers a powerful mechanism for accelerating the learning of exploration policies, particularly in high-dimensional domains.

**Meta-Exploration in Constrained Data and Lifelong Settings**
As meta-learning for exploration matured, researchers also tackled practical challenges like data efficiency and generalization in more constrained settings. \cite{pong2021i4o} addressed offline meta-reinforcement learning, where meta-training occurs on a fixed dataset. They proposed using additional unsupervised online data collection to bridge the distribution shift that arises when the learned exploration strategy gathers data systematically different from the offline dataset, thereby improving adaptive capabilities. This highlights the challenge of applying learned exploration in data-scarce scenarios. In a related vein, \cite{fu20220cl} introduced a model-based lifelong reinforcement learning approach that employs Bayesian exploration. By estimating a hierarchical Bayesian posterior, their method distills common structure across tasks, enhancing sample efficiency and enabling backward transfer by learning a more generalizable exploration model that adapts over a lifetime of tasks. Furthering this, \cite{steinparz20220nl} proposed Reactive Exploration to cope with non-stationarity in lifelong reinforcement learning. Their method focuses on tracking and reacting to continual domain shifts, adapting the exploration strategy dynamically to changing environment dynamics and rewards. This is critical for agents operating in real-world, constantly evolving environments where fixed exploration policies would quickly become suboptimal. Finally, \cite{guo2024sba} tackled sample-efficient offline-to-online reinforcement learning by proposing Optimistic Exploration and Meta Adaptation (OEMA). This method combines an optimistic exploration strategy, learned to sufficiently explore the environment, with meta-learning-based adaptation to reduce distribution shift and accelerate the fine-tuning process when transitioning from offline pre-training to online interaction.

**Emerging Paradigms: Implicit Exploration via In-Context Learning**
Beyond explicit meta-training of exploration policies, a more recent paradigm leverages large-scale pretraining to enable implicit, in-context adaptation without task-specific fine-tuning. \cite{wu2022sot} aimed for zero-shot policy transfer by learning a disentangled task representation within a meta-RL framework. While not directly learning an exploration policy, this approach optimizes the *need* for exploration by enabling agents to infer unseen compositional task representations and generalize effectively, thereby reducing the exploration burden in new tasks. More recently, the paradigm of in-context learning, particularly with large transformer models, has shown promise for implicit exploration. \cite{lee202337c} demonstrated that Decision-Pretrained Transformers (DPTs) can learn in-context reinforcement learning, exhibiting online exploration and offline conservatism without explicit training for these behaviors. The model implicitly learns to adapt its decision-making strategies, including exploration, from diverse pretraining data, essentially "pattern-matching" optimal exploration from its vast knowledge base. Building on this, \cite{dai2024x3l} proposed In-context Exploration-Exploitation (ICEE), which optimizes the efficiency of in-context policy learning by performing exploration-exploitation trade-offs at inference time within a Transformer model, significantly reducing the episodes needed to solve new tasks. This represents a fundamental shift, where exploration is not an explicitly optimized policy but an emergent property of a highly generalizable model.

**Advanced Bayesian Approaches for Learned Exploration**
The concept of learning exploration strategies also draws heavily from principled Bayesian approaches. \cite{houthooft2016yee}'s Variational Information Maximizing Exploration (VIME), while discussed earlier as an intrinsic motivation method, is also a foundational example of *learning* an exploration strategy. VIME explicitly learns an intrinsic reward that maximizes information gain about the environment's dynamics, effectively teaching the agent to seek out experiences that reduce its uncertainty. This principle of leveraging uncertainty for learned exploration is further extended by \cite{li2023kgk}, who explored implicit posterior parameter distribution optimization in reinforcement learning. By modeling parameter uncertainty with an implicit distribution approximated by generative models, their method provides structured stochasticity for exploration. This enhances flexibility and sample efficiency compared to explicit Bayesian neural networks, offering a sophisticated way to generate diverse and informative exploratory actions based on learned uncertainty.

In conclusion, the evolution of exploration in reinforcement learning has moved from fixed heuristics to sophisticated, learned, and adaptive strategies. Meta-learning has been instrumental in training agents to learn *how* to explore, dynamically adjusting their approach based on task and environment specifics, as exemplified by \cite{Pathak2017}'s meta-policy gradient and the adaptive capabilities of \cite{Badia2020}'s Agent57. These explicit meta-learning approaches are being refined to address practical challenges like data efficiency in offline and lifelong learning settings \cite{pong2021i4o, fu20220cl, steinparz20220nl, guo2024sba}, and are increasingly complemented by model-based methods like MAMBA \cite{rimon20243o6} for greater sample efficiency. A significant emerging trend is the implicit learning of exploration within large, pre-trained models, where in-context learning enables adaptive behaviors without explicit meta-training \cite{lee202337c, dai2024x3l}. While explicit meta-learning offers direct control and interpretability over the exploration policy, implicit methods promise broader generalization and reduced engineering effort, albeit with higher pre-training costs and less transparent mechanisms. Challenges remain in scaling these learned strategies to truly open-ended domains, ensuring robustness against pathological exploration, and managing the computational overhead of meta-learning and large-scale model pretraining, pointing towards a future where hybrid approaches may combine the strengths of both paradigms.