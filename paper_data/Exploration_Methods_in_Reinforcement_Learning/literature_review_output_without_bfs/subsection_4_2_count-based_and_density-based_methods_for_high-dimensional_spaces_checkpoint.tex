\subsection{Count-Based and Density-Based Methods for High-Dimensional Spaces}

The challenge of effective exploration in reinforcement learning (RL) is particularly acute in environments characterized by high-dimensional or continuous state spaces, where traditional count-based methods become intractable. While early work by \cite{Thrun1992} demonstrated the efficacy of providing exploration bonuses based on state visitation counts to incentivize novelty, this approach was inherently limited to tabular or low-dimensional discrete state spaces. The core difficulty lay in accurately quantifying "novelty" when states are rarely revisited exactly, necessitating a generalization of the concept of a "count."

A significant advancement in addressing this limitation was the introduction of pseudo-counts by \cite{Bellemare2016}. This method generalized the idea of visitation counts by constructing a density model of observed states and deriving a pseudo-count for a given state based on its likelihood under this model. States that are statistically less probable given past experience are assigned higher pseudo-counts, effectively generating intrinsic rewards for regions of the state space that have been less explored in a statistical sense. This approach provided a principled way to extend the benefits of count-based exploration to complex, high-dimensional observation spaces, such as those encountered in Atari games.

Building upon the need for scalable novelty estimation, \cite{Tang2017} proposed the \textit{\#Exploration} method, which employed feature hashing to map high-dimensional observations into a countable, lower-dimensional space. By applying a hash function, the method could approximate state visitation counts in this compressed space, offering a computationally efficient alternative to explicit density modeling. This technique allowed for the generation of intrinsic rewards proportional to the inverse square root of the estimated visitation count, thereby encouraging the agent to visit states that have been infrequently encountered within the hashed representation. While simpler than density models, feature hashing provided a practical means to generalize count-based exploration in deep RL settings.

Further refining the use of density models for novelty, \cite{Ostrovski2017} introduced a method that leveraged neural density models to estimate state novelty more robustly. By training a generative model, such as an autoencoder with a density estimator in its latent space, the approach could assign intrinsic rewards based on the novelty of states in a learned, meaningful representation. This allowed for a more sophisticated and flexible approximation of state density, particularly beneficial in environments with rich perceptual inputs where simple hashing might struggle with perceptual aliasing or fine-grained distinctions. The integration of neural networks enabled these density-based methods to scale effectively to deep reinforcement learning environments, providing a crucial bridge for applying novelty-driven exploration to complex tasks.

Collectively, these advancements provided a vital mechanism for quantifying and incentivizing the discovery of new states in environments previously inaccessible to traditional count-based methods. By approximating state novelty through pseudo-counts, feature hashing, or neural density models, these techniques enabled the generation of intrinsic rewards for statistically less-visited regions. This has proven instrumental in improving performance in sparse reward settings, allowing agents to explore effectively and discover optimal policies in complex deep reinforcement learning environments where extrinsic rewards are rare or delayed.

Despite their success, these methods face ongoing challenges. The effectiveness of density-based approaches heavily relies on the quality of the learned state representation or the robustness of the density model itself. Poor representations can lead to inaccurate novelty signals, while complex density models can be computationally expensive to train and prone to issues like the "noisy TV problem," where the agent is attracted to unpredictable but uninformative regions. Future work continues to explore more robust and efficient ways to estimate novelty, potentially by combining these insights with other forms of intrinsic motivation or by developing adaptive mechanisms for tuning exploration strategies.