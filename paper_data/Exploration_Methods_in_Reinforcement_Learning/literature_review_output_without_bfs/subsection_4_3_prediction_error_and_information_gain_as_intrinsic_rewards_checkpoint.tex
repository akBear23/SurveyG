\subsection{Prediction Error and Information Gain as Intrinsic Rewards}

Intrinsic motivation, particularly through the lens of prediction error and information gain, offers a powerful paradigm for driving exploration in reinforcement learning agents, especially in environments characterized by sparse extrinsic rewards and high-dimensional observations. This approach posits that an agent's internal drive to improve its understanding of the world can serve as a potent signal for discovering novel and informative states. Early conceptualizations of this idea can be traced back to the pioneering work by \textcite{schmidhuber1991}, who proposed that agents could be intrinsically motivated to explore states where their internal models of the world were inaccurate or where predictability could be improved. This foundational concept laid the groundwork for agents to seek out experiences that reduce their uncertainty or surprise, independent of external task rewards \cite{aubret2022inh}.

Building on this, modern intrinsic curiosity methods often define curiosity as the prediction error of a learned forward dynamics model. A prominent example is the Intrinsic Curiosity Module (ICM) proposed by \textcite{pathak2017}. ICM rewards the agent for encountering states where its learned forward dynamics model, which predicts the next state (or a feature representation thereof) given the current state and action, is inaccurate. A key design choice in ICM was the use of an inverse dynamics model to learn a feature space for observations. The intention behind this was to filter out irrelevant stochasticity from the raw pixel observations, ensuring that the prediction error for the forward model was computed in a more abstract, action-relevant space, thereby making the intrinsic reward more robust to uninformative visual noise. By driving the agent towards states that are hard to predict in this learned feature space, ICM encourages exploration of novel dynamics and state transitions, making it particularly effective in visually rich environments where traditional count-based exploration is impractical. Extensions and simplifications of ICM, such as the Simplified Intrinsic Curiosity Module (S-ICM) by \textcite{li2019tj1}, further demonstrate the practical utility of this prediction-error paradigm in improving exploration efficiency for off-policy reinforcement learning methods, particularly in robotic manipulation tasks with sparse rewards.

Complementing prediction error, other methods leverage information theory to quantify the value of exploration. The Variational Information Maximizing Exploration (VIME) framework, introduced by \textcite{houthooft2016, houthooft2016yee}, exemplifies this approach. Building on the Bayesian principles of uncertainty reduction discussed in Section 3.3, VIME operationalizes this by encouraging actions that maximize the information gain about the environment's dynamics model. Specifically, VIME uses Bayesian neural networks to maintain a posterior distribution over possible environment dynamics. The intrinsic reward is then defined as the Kullback-Leibler divergence between the posterior distributions of the model parameters before and after observing a new transition, effectively incentivizing actions that lead to the greatest reduction in uncertainty about how the world works. By intrinsically motivating the agent to perform experiments that are most informative for its internal model, VIME improves its predictive capabilities and facilitates more effective exploration. More recent frameworks, such as MaxInfoRL by \textcite{sukhija2024zz8}, further explore information gain maximization, steering exploration towards informative transitions and demonstrating how this can be naturally balanced with extrinsic rewards, achieving sublinear regret in simplified settings and superior performance in complex visual control tasks.

While prediction error and information gain methods offer significant advantages in sparse-reward settings, they are susceptible to a critical limitation known as the "noisy TV problem." This problem arises when unpredictable but uninformative elements in the environment (e.g., static on a TV screen, random background noise) consistently generate high prediction error or information gain. An agent driven purely by these signals might repeatedly visit these uninteresting states, as they continuously appear "novel" or "surprising" without contributing to any meaningful understanding of the environment or progress towards a task. For instance, even with ICM's inverse dynamics model, if the environment contains highly stochastic but irrelevant elements, the forward model's prediction error can remain high, leading to spurious curiosity. This pathological behavior diverts the agent's attention from learnable and task-relevant dynamics, leading to inefficient exploration.

Despite this challenge, the core principles of curiosity and information gain continue to inspire advanced exploration techniques. For example, in the context of large language models (LLMs), \textcite{dai2025h8g} introduce Curiosity-Driven Exploration (CDE), leveraging both actor-wise perplexity (a form of prediction error) and critic-wise value estimate variance (a measure of uncertainty) as intrinsic rewards. This demonstrates the broad applicability of these concepts, even while highlighting new challenges like "calibration collapse" within LLMs, where overconfident errors can still lead to suboptimal exploration. Furthermore, the utility of such curiosity-driven exploration extends beyond online learning; \textcite{lambert202277x} investigate the challenges of exploration for offline reinforcement learning, demonstrating how curiosity-based intrinsic motivation can be effectively used to collect informative datasets in a task-agnostic manner. Their work underscores that the quality of data collected through curiosity directly impacts the performance of downstream offline RL algorithms, emphasizing the importance of robust intrinsic reward signals, even if the methods themselves have inherent vulnerabilities.

In summary, prediction error and information gain provide powerful mechanisms for intrinsic motivation, enabling agents to explore and learn in complex, sparse-reward environments where extrinsic signals are scarce. Methods like ICM and VIME effectively drive agents to improve their understanding of environmental dynamics by seeking out states that are hard to predict or highly informative. However, the 'noisy TV problem' represents a significant hurdle, as these methods can be misled by uninformative stochasticity, leading to inefficient or pathological exploration. Addressing this and other challenges, such as ensuring intrinsic exploration remains aligned with eventual task objectives and balancing the computational cost of information-theoretic approaches, necessitates more robust and sophisticated intrinsic reward designs, which will be discussed in the subsequent section.