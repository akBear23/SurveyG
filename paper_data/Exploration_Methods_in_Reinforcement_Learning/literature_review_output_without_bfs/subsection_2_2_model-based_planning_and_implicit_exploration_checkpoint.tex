\subsection*{Model-Based Planning and Implicit Exploration}

Model-based planning represents a foundational paradigm in reinforcement learning (RL) that significantly accelerates learning and implicitly aids exploration by leveraging an explicit model of the environment's dynamics and rewards. This approach allows an agent to perform "mental rehearsals" by generating simulated experiences, thereby propagating value information more efficiently throughout the state space without requiring additional real-world interactions. While not directly designed as an explicit exploration strategy, this process accelerates learning and indirectly encourages the agent to explore states relevant for model improvement and policy refinement.

A seminal contribution in this area is the Dyna-Q architecture proposed by \cite{Sutton1990}. Dyna-Q integrates model-free learning (typically Q-learning) with model-based planning. The agent continuously learns both a value function from real experiences and an explicit model of the environment's transitions and rewards. This learned model is then used to generate numerous simulated experiences, allowing the agent to update its value function and policy through planning steps. This process of generating synthetic data and updating value functions accelerates learning and efficiently propagates value information throughout the state space. By rapidly updating value estimates, Dyna-Q implicitly encourages the agent to revisit states and actions that lead to improved model accuracy or refined policy performance, as better models lead to more effective planning and thus better policies. Further empirical evidence supporting the benefits of learning and utilizing explicit models for faster learning and improved sample efficiency was provided by \cite{Singh1992}, highlighting how such models make each real interaction more valuable by allowing for extensive offline updates. \cite{Kaelbling1993} further refined these concepts by proposing methods to blend planning with direct reinforcement learning, demonstrating how models could be used to inform action selection and implicitly manage the exploration-exploitation trade-off by simulating hypothetical scenarios to evaluate potential actions.

Building upon the core idea of efficient value propagation, subsequent early model-based algorithms focused on optimizing the planning process. Prioritized Sweeping, introduced by \cite{Moore1993}, enhanced Dyna-Q by prioritizing which state-action pairs to update during planning. Instead of uniformly sampling simulated experiences, Prioritized Sweeping focuses computational effort on states whose values are most likely to change significantly, often due to recent real-world experiences. This targeted update mechanism further accelerates the propagation of value information, particularly in sparse reward settings, by ensuring that the most impactful updates are performed first. While it improved the efficiency of planning, Prioritized Sweeping, like Dyna-Q, still relied on the existing learned model and primarily facilitated *implicit* exploration by making the agent's value function more accurate and up-to-date across the *known* parts of the state space. The exploration was a side-effect of efficient learning and value propagation, not an explicit drive towards novelty or uncertainty reduction in truly unknown regions.

Despite their foundational importance in demonstrating the power of model-based learning, a key limitation of these early approaches was their heavy reliance on the accuracy and learnability of the environmental model. In complex, high-dimensional, or stochastic environments, learning a perfectly accurate model from limited data was, and remains, a significant challenge. Errors in the learned model can propagate during planning, leading to suboptimal value estimates and potentially poor policy performance. For instance, if the model inaccurately predicts transitions or rewards in an unexplored region, planning based on this flawed model might reinforce existing biases or even propagate misinformation, hindering the discovery of truly optimal paths. Before the advent of deep learning, representing and learning accurate models for environments with large or continuous state spaces was often intractable, limiting the scalability of these implicit exploration strategies. This meant that while model-based planning was highly sample-efficient *within* the regions where the model was accurate, it struggled to guide effective exploration into genuinely unknown or poorly modeled parts of the environment. The implicit nature of exploration in these methods meant they lacked mechanisms to actively seek out novelty or reduce uncertainty, which would later become the focus of more explicit exploration strategies.

In conclusion, early model-based planning, exemplified by Dyna-Q \cite{Sutton1990} and enhanced by techniques like Prioritized Sweeping \cite{Moore1993}, established a powerful paradigm for accelerating reinforcement learning through efficient value propagation. By leveraging learned models for simulated experiences, these methods implicitly aided exploration by making the agent's understanding of the environment more comprehensive and its policy more refined. However, their primary limitation lay in the challenges of accurately learning and representing environmental models, particularly in complex, high-dimensional, or stochastic settings, where implicit exploration alone proved insufficient for robust discovery. These limitations highlighted the need for more explicit and theoretically grounded exploration mechanisms, paving the way for the development of strategies that directly address uncertainty and novelty, as discussed in subsequent sections.