\subsection{Early Concepts of Curiosity and Novelty}

In reinforcement learning (RL) environments characterized by sparse, delayed, or non-existent external rewards, agents often struggle to acquire sufficient experience for effective policy learning. This fundamental challenge spurred the development of intrinsic motivation, a powerful paradigm that encourages agents to explore their environment based on internal signals, independent of immediate task-specific rewards. This approach posits that agents can be driven by an innate desire for novelty, surprise, or learning progress, fostering a more robust and autonomous exploration strategy than purely extrinsic reward-driven methods.

One of the earliest and most influential contributions to this paradigm came from \textcite{Schmidhuber91}, who pioneered the concept of an internal "curiosity" drive. This drive was fundamentally based on minimizing the prediction error of a learned world model. The agent was intrinsically motivated to seek out states or experiences that were surprising or difficult for its internal model to predict accurately. By doing so, the agent actively pursued information that would improve its understanding of the environment's dynamics, thereby fostering learning progress. This mechanism provided a foundational framework for agents to actively seek information, akin to how humans and animals exhibit epistemic curiosity \cite{Berlyne60} or strive to resolve cognitive disequilibrium \cite{Piaget52}. Schmidhuber's work on curiosity evolved throughout the 1990s, with subsequent contributions further developing the idea of agents learning to generate novel inputs to improve their predictive models \cite{Schmidhuber97}. From an information-theoretic perspective, this form of curiosity can be understood as an agent's implicit drive to reduce its uncertainty about the environment or maximize the information gain about its dynamics \cite{aubret2022inh}. By focusing on states where its model's predictions are inaccurate, the agent effectively targets regions of high uncertainty, thereby gathering the most informative data to refine its internal representation of the world.

Concurrently, another critical early approach to intrinsic motivation emerged through novelty-seeking mechanisms. As introduced in Section 2.1, \textcite{Thrun92} laid crucial groundwork by proposing explicit count-based exploration bonuses. While initially framed as a basic heuristic for tabular settings, this method inherently functions as an intrinsic motivation by assigning higher internal rewards to states or state-action pairs that have been visited less frequently. This systematic drive towards novel regions of the state space ensures breadth of experience, as the agent is incentivized to discover new environmental dynamics, potential rewards, or escape local optima, thereby building a more comprehensive understanding of the environment. The underlying algorithmic principle is straightforward: rarity implies novelty, and novelty is intrinsically rewarding.

While both \textcite{Schmidhuber91}'s prediction-error-based curiosity and \textcite{Thrun92}'s count-based novelty mechanisms were instrumental in establishing the intrinsic motivation paradigm, they operated on distinct mechanistic approaches. Schmidhuber's work focused on improving an internal predictive model, where novelty was a byproduct of learning progress and uncertainty reduction. In contrast, Thrun's method directly incentivized state novelty, treating visitation counts as a direct proxy for how "interesting" a state might be. Despite their differences, these early contributions shared a critical limitation: their practical application was largely confined to tabular or low-dimensional environments.

A significant challenge arose in defining and measuring "meaningful novelty" within complex, high-dimensional observation spaces, such as raw pixel inputs from video games or real-world sensor data. In such settings, traditional explicit state counting, as proposed by \textcite{Thrun92}, becomes intractable due to the vastness and often continuous nature of the state space. Each slight variation in pixel values could be considered a "new" state, leading to an explosion of counts that are neither meaningful nor computationally feasible. The very notion of a "state" becomes ambiguous, making direct visitation counts impractical. While later work would attempt to generalize count-based methods to high-dimensional spaces using feature representations or density models (e.g., \textcite{martin2017bgt}), the original formulation faced this severe scalability barrier.

Similarly, for prediction-error-based curiosity, simple prediction errors in high-dimensional spaces can be easily generated by stochastic but uninteresting phenomena. An agent might become perpetually curious about a randomly flickering pixel on a screen, which offers no true learning progress about the environment's underlying dynamics. This leads to a vulnerability to stochastic, uninformative phenomena, a challenge that would later be famously dubbed the "noisy TV problem" in the context of deep reinforcement learning. This issue can lead to inefficient or spurious exploration, diverting the agent's focus from genuinely informative novelties and hindering the acquisition of useful skills.

These inherent limitations in scalability and robustness highlighted the need for more sophisticated mechanisms to quantify and leverage novelty in complex environments. Nevertheless, the foundational ideas introduced by \textcite{Schmidhuber91} and \textcite{Thrun92} were pivotal. They shifted the focus of exploration from purely external reward signals to internally generated drives, laying the groundwork for subsequent advancements that would adapt and extend these concepts to the deep reinforcement learning era, where challenges of high-dimensionality and sparse rewards became even more pronounced.