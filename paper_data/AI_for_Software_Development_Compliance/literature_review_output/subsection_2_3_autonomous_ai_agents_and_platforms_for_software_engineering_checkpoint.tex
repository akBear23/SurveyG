\subsection{Autonomous AI Agents and Platforms for Software Engineering}

The landscape of software engineering is being reshaped by the emergence of autonomous AI agents and sophisticated platforms designed to perform complex development tasks with increasing independence. This shift moves beyond mere AI assistance towards systems capable of understanding, planning, executing, and verifying software changes, paving the way for more autonomous compliance-related tasks.

The conceptual foundation for evaluating generalist agents was laid by early work such as the Arcade Learning Environment (ALE) \cite{bellemare2012sge}, which provided a diverse platform for assessing AI algorithms across numerous tasks, emphasizing the need for domain-independent competency. Complementing this, the development of large-scale, diverse datasets like CodeNet \cite{puri2021d34} became crucial, offering over 14 million code samples in 55 programming languages with rich metadata, providing the necessary raw material for training AI models capable of understanding and generating code.

Recent advancements have focused on empowering Large Language Models (LLMs) to not only use but also create their own tools, significantly enhancing their autonomy. The LLMs AsToolMakers (LATM) framework \cite{sauvola2024zw7} exemplifies this by introducing a two-phase process where a powerful LLM acts as a "tool maker" to craft reusable Python functions from demonstrations, while a lightweight LLM serves as a "tool user" to apply these verified tools. This innovative approach, coupled with a functional cache, optimizes cost and performance by strategically dividing labor and allowing agents to extend their capabilities dynamically.

Building upon these foundational capabilities, platforms are emerging to provide comprehensive environments for generalist AI agents. OpenHands \cite{wang20241va} stands out as an open platform designed for AI software developers, offering a secure Docker-sandboxed runtime environment for each task session. This platform provides a comprehensive, programming language-based action space, allowing agents to interact with operating systems via arbitrary Python code (`IPythonRunCellAction`), execute bash commands (`CmdRunAction`), and browse the web using a domain-specific language (`BrowserInteractiveAction`). OpenHands further enhances agent capabilities through an extensible `AgentSkills` library for specialized tools and a multi-agent delegation mechanism, providing an "ideal interface" for agents to engage in complex software-related tasks.

Beyond generalist platforms, specialized agents are demonstrating advanced capabilities in specific software engineering domains. MarsCode Agent \cite{liu2024uqj}, for instance, focuses on AI-native automated bug fixing through a novel multi-agent collaborative framework. This agent integrates LLMs with advanced code analysis techniques like Code Knowledge Graphs (CKG) and Language Server Protocols (LSP) for deep code understanding across large codebases, utilizing a containerized sandbox for dynamic debugging and accurate patch generation. MarsCode Agent represents a significant step in overcoming LLM limitations in handling real-world bug fixing complexity by structuring the process into distinct agent roles with specialized toolsets.

As these autonomous agents become more sophisticated, rigorous evaluation of their generalization capabilities becomes paramount. SWE-bench Multimodal (SWE-bench M) \cite{yang20244xg} addresses this critical need by introducing a novel benchmark dataset that evaluates AI systems on visual, user-facing JavaScript software issues. Unlike prior text-based, Python-centric benchmarks, SWE-bench M explicitly requires multimodal reasoning to interpret images and videos in problem statements, alongside cross-language generalization to JavaScript. Evaluations on SWE-bench M have revealed significant limitations in existing AI systems, with even adapted agents like `SWE-agent M` achieving modest success rates, underscoring the substantial research required for AI systems to generalize across diverse languages and visual domains.

In conclusion, the progression from foundational general agent evaluation and large-scale code datasets to platforms enabling autonomous tool creation and comprehensive interaction environments marks a significant leap in AI for software engineering. While specialized agents like MarsCode Agent demonstrate impressive capabilities in targeted tasks, benchmarks like SWE-bench Multimodal highlight the persistent challenge of achieving true generalization across complex, multimodal, and multi-language software domains. Bridging this gap remains a key unresolved issue, requiring future research to integrate advanced LLM reasoning with robust, language-agnostic tools and multimodal perception, which is essential for these agents to reliably perform autonomous compliance-related tasks in real-world software development.