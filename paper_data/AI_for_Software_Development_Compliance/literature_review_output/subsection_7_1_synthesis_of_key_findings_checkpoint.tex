\subsection*{Synthesis of Key Findings}

The extensive literature on AI for software development compliance reveals a dynamic field characterized by a compelling dual trajectory: the relentless advancement of AI capabilities to augment and automate software engineering tasks, and the simultaneous, urgent imperative to ensure these AI systems are developed and deployed responsibly, ethically, and compliantly. This review has traced the intellectual progression from foundational AI capabilities and initial productivity gains to the identification of critical risks, the development of conceptual frameworks for responsible AI, and the emergence of advanced AI-driven detection and proactive design solutions. The field is rapidly maturing, demonstrating the interconnectedness of its various research streams and underscoring the comprehensive scope of AI's impact on the entire software development lifecycle.

Initially, research focused on establishing the bedrock for "AI for Code," addressing data scarcity through large-scale datasets and demonstrating AI's capacity to significantly enhance developer productivity. Early empirical studies consistently showed that AI assistants could accelerate task completion and democratize access to advanced coding practices, with less experienced developers often benefiting disproportionately. This initial optimism, however, quickly gave way to a more nuanced understanding of human-AI interaction, revealing that while AI boosts individual productivity, it also reshapes team dynamics, information-seeking behaviors, and introduces user experience challenges related to validating AI-generated content \cite{haque20246hg, russo2023kua, li2024voc, ulfsnes2024pib}. Concurrently, the technical frontier pushed towards increasingly autonomous AI agents and platforms capable of complex software engineering tasks, including self-tooling and multimodal reasoning.

Crucially, as AI integration deepened, the focus broadened from mere efficiency to the profound trustworthiness and compliance challenges inherent in AI-assisted development. A pivotal finding, empirically validated by multiple studies, highlighted a significant tension: developers, when assisted by AI, often produce less secure code and exhibit increased overconfidence in its security, despite generally mistrusting AI suggestions for critical tasks \cite{perry2022cq5, klemmer20246zk}. This exposed a severe, counterintuitive compliance risk. Beyond security, the environmental footprint of AI systems emerged as a critical concern, with research providing holistic frameworks for quantifying carbon emissions across the AI lifecycle and advocating for "green AI" architectures \cite{wu2021t2c, dodge2022uqb}. Furthermore, empirical investigations consistently revealed a notable gap between abstract AI ethics guidelines and practical industry implementation, particularly concerning societal well-being, diversity, non-discrimination, and fairness \cite{vakkuri2020co9, vakkuri2022wjr, sanderson2022zra, pant2022dlh}. These findings underscored that the "compliance" aspect extends not only to the software being built but also to the inherent qualities and development processes of the AI systems themselves.

In response to these identified problems, the field has progressed towards developing more robust conceptual frameworks and technical solutions for proactive design and governance. Visionary paradigms, such as "AI-native Software Engineering," articulate a future where AI acts as an intelligent, goal-driven teammate, emphasizing intent-first development and knowledge-driven models to ensure consistency and quality across the SDLC \cite{hassan2024pqx, hassan2024hq8, terragni2025ltf}. However, a critical gap remains between these aspirational visions and the current reality of practical, validated solutions, which often address point problems rather than holistic, end-to-end compliance. To bridge this, research has proposed concrete frameworks for moving beyond abstract AI ethics to verifiable claims, supported by mechanisms like audit trails and interpretability \cite{brundage2020dn4}. This shift is echoed in the call for revised AI lifecycle models that explicitly incorporate critical stages like data collection, feasibility studies, robust documentation, continuous model monitoring, and comprehensive risk assessment, particularly for heavily regulated domains like fintech \cite{haakman2020xky}. Such models emphasize the need for legal mandates and external oversight to ensure compliance with human rights standards and broader societal values \cite{truby2020xrk, yeung20205sw}.

The application of AI for automated compliance detection has evolved from traditional AI/ML techniques for code and artifact analysis to leveraging Large Language Models (LLMs) for complex regulatory interpretation and policy-to-code mapping. Generative AI is increasingly explored for compliance-aware design, aiming to embed compliance proactively into software components rather than merely detecting violations post-factum. The integration of these AI-driven mechanisms into DevOps and Agile workflows signifies a move towards continuous compliance throughout the entire development lifecycle. Yet, the trustworthiness of these AI-driven compliance systems themselves remains paramount. Explainable AI (XAI) techniques are critical for providing human-understandable justifications for AI decisions, essential for auditing and building trust. Similarly, blockchain technology offers a promising avenue for immutable compliance evidence and traceability, bolstering accountability in regulated environments \cite{Li2021}. Furthermore, addressing human factors, such as developer overconfidence and the need for rigorous scrutiny of AI-generated code, is crucial for mitigating risks and ensuring the auditability of AI-assisted secure development \cite{fernandez20241ee}.

In conclusion, the journey of AI for software development compliance reflects a rapid maturation, transitioning from initial explorations of AI's utility to a sophisticated engagement with its profound implications. The field has successfully identified the dual nature of AI's impact, moving from problem identification—such as security vulnerabilities, ethical gaps, and environmental concerns—to the development of conceptual frameworks, technical solutions, and empirical validations. However, a persistent tension exists between the pursuit of efficiency and the imperative for transparency, auditability, and human interpretability. While significant progress has been made in automating detection and envisioning proactive design, the overarching challenge lies in seamlessly integrating these advancements to create inherently compliant and trustworthy AI-driven SDLCs that not only leverage AI's technical capabilities but also robustly address ethical, regulatory, and human demands. This necessitates continuous interdisciplinary research, robust validation in real-world settings, and a commitment to responsible innovation to navigate the complexities of this evolving landscape.