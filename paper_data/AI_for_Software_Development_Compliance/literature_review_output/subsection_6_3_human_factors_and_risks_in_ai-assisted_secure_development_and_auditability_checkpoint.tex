\subsection{Human Factors and Risks in AI-Assisted Secure Development and Auditability}

The integration of Artificial Intelligence (AI) into software development fundamentally alters developer workflows, introducing a complex interplay of human factors that critically impact the security, compliance, and, crucially, the auditability of the entire development process. While Section 4.2 addresses the direct introduction of vulnerabilities by AI systems and developer overconfidence in code security, this subsection delves deeper into how human-AI interaction patterns, trust dynamics, and scrutiny practices influence the *trustworthiness* and *verifiability* of AI-assisted development artifacts and processes. It underscores the imperative for usable security research, developer education, and AI assistant designs that actively mitigate human-induced vulnerabilities, ensuring that AI tools enhance, rather than compromise, overall software security and compliance, and that the resulting artifacts remain auditable.

Empirical studies reveal a nuanced landscape of human-AI collaboration. Developers often exhibit a general mistrust in the security of AI suggestions due to overall quality concerns, yet paradoxically, AI assistants are widely used for security-critical tasks like code generation, threat modeling, and vulnerability detection \cite{klemmer20246zk}. This highlights a significant gap between perceived risk and actual usage, where the convenience and productivity gains often outweigh explicit security concerns in practice. For instance, large-scale telemetry analysis of GitHub Copilot users demonstrates substantial productivity increases, particularly for less experienced developers \cite{dohmke2023tpd}. While beneficial for efficiency, this finding poses a heightened security risk: less experienced developers, who may possess less inherent security expertise, are leveraging tools that can generate insecure code, increasing the likelihood of vulnerabilities being introduced. This dynamic complicates auditability, as the human review layer, intended as a safeguard, may be less effective due to a lack of expertise or over-reliance on AI.

The challenge is further exacerbated by the cognitive biases and trust dynamics inherent in human-AI interaction. \cite{perry2022cq5} demonstrated that developers using AI assistants wrote significantly less secure code and, critically, exhibited increased overconfidence in their code's security. This overconfidence directly impacts auditability: if developers *believe* their AI-assisted code is secure, they may not apply the rigorous scrutiny or document the comprehensive review steps necessary for a robust audit trail. \cite{sabouri2025g21} further explored trust dynamics, finding that developers primarily evaluate AI code suggestions based on comprehensibility and perceived correctness, but identified a significant lack of real-time support for robust trust evaluation. This results in developers frequently altering AI suggestions, accepting only about 52\% of original outputs, indicating a continuous need for human oversight and validation. However, this constant human intervention, if not systematically logged and justified, can obscure the provenance of code and decision-making, making it difficult to reconstruct the development process for compliance audits. The burden of validation is also noted by \cite{haque20246hg}, who found that AI tools introduce UX issues like inconsistent responses, requiring developers to cross-reference multiple AI systems or traditional sources, which can indirectly impact security by leading to the use of incomplete or incorrect information. Similarly, \cite{pandey2024dcu}'s real-world evaluation of GitHub Copilot highlighted variable code quality and the *necessity* of thorough human review, especially for complex tasks and specific languages, reinforcing the human validation burden.

These human factors present profound challenges for establishing trustworthy AI-driven compliance systems and ensuring auditability. The "black box" nature of many AI models, combined with human overconfidence and inconsistent scrutiny, can lead to a lack of transparency regarding *why* certain code was generated or accepted, *who* is accountable for its security, and *how* compliance was verified. This necessitates a shift towards designing AI-assisted development processes that inherently support verifiable claims and robust audit trails.

To address these issues, several approaches are proposed to enhance trustworthiness and auditability. \cite{brundage2020dn4} advocate for mechanisms supporting verifiable claims in AI development, emphasizing the need for comprehensive audit trails, interpretability methods tailored for risk assessment and auditing, and secure hardware for machine learning to enhance accountability and transparency. Such audit trails must capture not only the final code but also the AI prompts, AI responses, human modifications, and the rationale behind acceptance or rejection, creating an immutable record of the human-AI collaboration. Building on this, \cite{lu2021m0b} propose operationalized patterns for Responsible AI, integrating continuous monitoring and system-level ethical considerations throughout the AI system lifecycle. This process-oriented view is crucial for embedding auditability from inception, rather than as an afterthought. Furthermore, \cite{cho2023v8k} introduced a maturity model (AI-MM) for trustworthy AI software development, providing practical guidelines for enhancing maturity levels in AI projects, which implicitly leads to better-defined processes and improved auditability.

Beyond process-level changes, technical solutions within AI tools themselves are vital. \cite{terragni20245xq} envisions future AI-driven software engineering where Explainable AI (XAI) provides justifications for design suggestions, aiding human decision-making and building trust. They also highlight Metamorphic Testing as a critical technique for verifying AI-generated code, especially for addressing the oracle problem, thereby enhancing the verifiability of AI outputs. The concept of "security-aware" AI assistants, actively designed to mitigate vulnerabilities and user overconfidence, as implied by \cite{perry2022cq5}, is paramount. This could involve AI flagging potential security risks in its own suggestions or prompting developers for explicit security review.

Finally, developer education remains a critical component, though its efficacy against deep-seated cognitive biases requires careful consideration. \cite{fernandez20241ee} advocate for integrating basic AI knowledge and traditional software verification steps into early computer science curricula, equipping future developers with the skills to critically evaluate AI-generated code. However, education alone may not be sufficient to counteract automation bias or overconfidence; technical guardrails and robust process frameworks are equally essential to ensure that the human element in AI-assisted development consistently contributes to, rather than detracts from, software security and auditability.

In conclusion, the human element in AI-assisted secure development presents a double-edged sword: while human oversight is indispensable for validating AI outputs, inherent biases like overconfidence, coupled with a lack of robust support for trust evaluation and audit trails, can undermine the trustworthiness and auditability of the entire process. Addressing this requires a multi-faceted approach, combining transparent AI designs, comprehensive developer education, and structured frameworks for verifiable claims and continuous monitoring throughout the software development lifecycle.