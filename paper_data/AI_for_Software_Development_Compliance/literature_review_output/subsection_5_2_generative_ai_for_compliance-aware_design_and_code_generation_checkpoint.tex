\subsection{Generative AI for Compliance-Aware Design and Code Generation}

The increasing complexity of regulatory landscapes necessitates a paradigm shift from reactive compliance detection to proactive compliance-by-design, where adherence to rules and policies is embedded from the initial stages of software development. This subsection explores the innovative application of Generative AI (GenAI) to achieve this goal, focusing on approaches where AI actively guides developers or auto-generates design artifacts and code snippets that inherently adhere to compliance rules, security policies, or architectural guidelines. This proactive stance aims to prevent non-compliance from the outset, rather than merely detecting it post-factum, driven by stringent regulations such as the General Data Protection Regulation (GDPR) \cite{amugongo2023vwb}.

Foundational work in AI for software engineering has broadly categorized AI's potential across the Software Development Life Cycle (SDLC) \cite{barenkamp2020w3b}. Building upon this, recent reviews specifically on Generative AI in software architecture underscore its application in architectural decision support, reconstruction, and the transformation from requirements to architecture \cite{esposito20252vd}. However, these reviews also point to a critical gap in rigorous evaluation methodologies for GenAI outputs in architectural contexts, a limitation that becomes even more pronounced when considering compliance. Addressing this, \textcite{eisenreich20243sq} propose an iterative, semi-automatic process leveraging Large Language Models (LLMs) to generate and evaluate multiple software architecture candidates directly from textual requirements. This approach is pivotal for compliance-aware design, as architectural decisions often embed fundamental compliance principles, allowing for proactive adherence to security or data privacy guidelines from the blueprint stage. However, the effectiveness of such LLM-driven architectural generation in fully capturing and enforcing complex, non-functional compliance requirements, which are often ambiguous and context-dependent, remains a significant challenge, requiring extensive prompt engineering and human refinement \cite{eisenreich20243sq}. The inherent "black-box" nature of LLMs also complicates the auditability and explainability of why a particular architectural decision was made, a critical aspect for regulatory compliance \cite{belani20194yc}.

The application of GenAI extends significantly into code generation, moving beyond mere assistance to autonomous creation. A comprehensive review of AI techniques for Automated Code Generation (ACG) highlights advancements in Machine Learning (ML), Natural Language Processing (NLP), Deep Learning (DL), and Evolutionary Algorithms (EAs) in translating requirements into executable code \cite{odeh2024b0w}. While these techniques demonstrate potential for efficiency, ensuring the correctness, quality, and, crucially, the compliance of generated code remains a significant challenge. General GenAI coding tools, such as GitHub Copilot and ChatGPT, despite offering productivity gains, frequently generate code with security vulnerabilities, logical flaws, and "hallucinations" (incorrect or nonsensical outputs) \cite{aarti2024abq, haque2025vb3, kholoosi2024mh2}. This necessitates thorough developer review and testing, as AI-generated code often requires significant correction to meet quality and security standards \cite{aarti2024abq}. Empirical studies have even shown that developers using AI assistants can produce *less secure code* and exhibit increased overconfidence in its security, highlighting a critical human factor in the compliance chain \cite{perry2022cq5, klemmer20246zk, sabouri2025g21}. This human-AI interaction dynamic poses a direct threat to the 'compliance-by-design' paradigm if not properly managed, as the ultimate responsibility for compliant code still rests with the human developer.

A significant advancement in embedding proactive compliance comes from frameworks where LLMs can autonomously create and verify their own programmatic tools. \textcite{sauvola2024zw7} introduce the LLMs AsToolMakers (LATM) framework, a closed-loop system where LLMs generate, verify (through unit tests and self-correction), and manage reusable Python-based tools. This innovative architecture, which separates a powerful "tool maker" LLM from a lightweight "tool user" LLM, directly addresses the limitations of high inference costs for repetitive tasks and the lack of reusability in LLM-generated code. For compliance-aware design and code generation, LATM represents a transformative capability: an LLM could autonomously generate a specific programmatic tool to check for adherence to a particular security policy, a data minimization principle, or an architectural guideline. This self-generated and verified tool could then be applied to generated designs or code, or even provided to developers, ensuring that compliance checks are not only automated but also dynamically created and validated by the AI itself, fostering true 'compliance-by-design'. However, the robustness of LATM's self-correction mechanism against complex, ambiguous, or evolving compliance rules, which often lack clear, deterministic test cases, needs further rigorous validation. The framework's reliance on unit tests for verification might also fall short for non-functional compliance properties that require broader system-level analysis.

To address the inherent limitations of probabilistic GenAI outputs for compliance-critical systems, researchers are exploring integrations with more deterministic approaches. The concept of "correct-by-construction" from formal methods and program synthesis offers a contrast to GenAI's heuristic nature. While GenAI excels at generating diverse solutions, formal methods aim for provably correct outputs based on specifications. \textcite{fernandez20241ee} advocate for teaching software verification alongside GenAI, emphasizing the critical need for developers to understand and verify AI-generated code for security and correctness. This suggests that even with "compliance-aware" generation, a robust verification step is indispensable. Furthermore, Model-Driven Engineering (MDE) and Domain-Specific Languages (DSLs) provide structured ways to embed compliance rules directly into models, which can then be used to generate code with stronger guarantees \cite{rdler202361h}. Integrating GenAI with MDE, where LLMs generate initial models or transformations that are then formally verified or processed by MDE tools, could offer a hybrid approach to achieve more reliably compliant designs and code. This would leverage GenAI's creativity for initial design exploration while employing MDE's rigor for rule enforcement and traceability.

In conclusion, Generative AI is rapidly evolving from a tool for general software development assistance to a powerful enabler of compliance-aware design and code generation. The ability of LLMs to semi-automatically generate software architectures from requirements \cite{eisenreich20243sq} and, more profoundly, to autonomously create and verify their own programmatic tools for specific compliance checks \cite{sauvola2024zw7}, marks a significant step towards embedding compliance from the outset. However, the probabilistic nature of GenAI, coupled with challenges such as hallucinations, security vulnerabilities, and human overconfidence, necessitates a critical approach. Future advancements must focus on enhancing the verifiability and explainability of AI-generated artifacts, integrating GenAI with formal methods and model-driven approaches for stronger compliance guarantees, and designing AI assistants that actively mitigate human-induced security risks. The trajectory towards 'compliance-by-design' through autonomous, verifiable, and proactive AI-driven solutions is clear, promising more secure and compliant software systems, but requires continuous innovation in evaluation, integration, and human-AI collaboration.