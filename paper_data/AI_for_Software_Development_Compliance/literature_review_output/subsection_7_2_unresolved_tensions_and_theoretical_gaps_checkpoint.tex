\subsection{Unresolved Tensions and Theoretical Gaps}
The rapid advancement of AI in software engineering, particularly in compliance-related tasks, has unveiled significant unresolved tensions and theoretical gaps that demand further inquiry. While AI models offer unprecedented efficiency and power, their application in regulated environments often clashes with fundamental imperatives for transparency, auditability, and human interpretability.

A primary tension lies in the inherent trade-offs between the efficiency and power of complex AI models and the critical need for transparency and auditability. Advanced AI agents, such as those capable of autonomously creating their own tools \cite{sauvola2024zw7} or operating as generalist software developers in sandboxed environments \cite{wang20241va}, demonstrate immense potential for productivity gains \cite{peng2023uj3}. However, this sophistication often comes at the cost of explainability. The "black box" nature of many powerful AI models, particularly Large Language Models (LLMs) used for tasks like regulatory interpretation \cite{Wang2022} or proactive compliance design \cite{Li2024}, introduces challenges in providing clear, human-understandable justifications for their decisions. This directly conflicts with the need for verifiable claims in trustworthy AI development \cite{brundage2020dn4}, where external scrutiny and accountability are paramount. Empirical evidence further exacerbates this tension, as studies show that AI assistants can lead to significantly less secure code and increased user overconfidence \cite{perry2022cq5}, directly undermining the auditability and safety required in regulated software development. While Explainable AI (XAI) techniques are proposed for compliance auditing \cite{Chen2023}, their practical integration into complex, real-time systems and ensuring the legal sufficiency of explanations remain significant hurdles.

Another critical gap concerns the scalability and generalizability of specific AI models across diverse compliance domains. While foundational platforms like the Arcade Learning Environment \cite{bellemare2012sge} and large datasets like CodeNet \cite{puri2021d34} aim for general AI capabilities, their focus often remains on functional performance rather than comprehensive compliance implications. Automated compliance detection and enforcement systems \cite{Siddiqui2020, Al-Hajj2021, Gupta2023} aim to reduce manual effort, yet they frequently struggle with high false positive rates and adapting to new or nuanced regulatory contexts without extensive retraining. For instance, while environmental compliance tools measure the carbon intensity of AI workloads \cite{wu2021t2c, dodge2022uqb}, their methodologies are highly specific and do not readily generalize to other compliance areas like data privacy or security. The introduction of benchmarks like SWE-bench Multimodal \cite{yang20244xg} highlights the current limitations of AI systems in generalizing across visual and multi-language software domains, suggesting that generalizability for compliance, which often involves complex, domain-specific rules, is an even more formidable challenge.

The dynamic nature of regulatory environments presents a significant challenge for AI-driven compliance solutions. Regulations evolve rapidly, often outpacing the development and validation cycles of complex AI models. While LLMs show promise in interpreting regulatory texts \cite{Wang2022}, their potential for "hallucinations" and the computational cost of continuous adaptation make reliable, real-time responses to evolving legal frameworks difficult without substantial human oversight. The existing literature on automated compliance often overlooks the mechanisms for rapid model adaptation and validation in response to regulatory shifts, leaving a critical gap in ensuring continuous compliance.

Finally, there is a pronounced need for more robust, long-term empirical validation of proposed frameworks and solutions in real-world industrial settings. Many frameworks, such as POLARIS for trustworthy AI development \cite{baldassarre2024v2c}, offer actionable guidelines across the Software Development Life Cycle (SDLC) but are currently supported by only "initial validation." A systematic review of Responsible AI frameworks empirically confirms a severe lack of actionable tools and comprehensive SDLC coverage, particularly in later phases like design, development, testing, and deployment \cite{barletta202346k}. This indicates that while conceptual solutions are emerging, their practical effectiveness and long-term impact in diverse, complex industrial environments remain largely unproven. Even controlled experiments demonstrating AI's productivity benefits \cite{peng2023uj3} or security risks \cite{perry2022cq5} often operate within specific task contexts or participant pools, limiting their generalizability to the full spectrum of real-world industrial compliance scenarios.

These unresolved tensions and theoretical gaps collectively represent fertile ground for future theoretical and empirical inquiry. Addressing them demands innovative solutions that can effectively balance the power of AI with the non-negotiable requirements of transparency, auditability, and human interpretability, while also ensuring scalability, adaptability to evolving regulations, and rigorous, long-term validation in complex industrial settings.