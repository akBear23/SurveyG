{
  "nodes": [
    {
      "id": "0d1c76d45afa012ded7ab741194baf142117c495",
      "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model",
      "abstract": "While large-scale unsupervised language models (LMs) learn broad world knowledge and some reasoning skills, achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training. Existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine-tune the unsupervised LM to align with these preferences, often with reinforcement learning from human feedback (RLHF). However, RLHF is a complex and often unstable procedure, first fitting a reward model that reflects the human preferences, and then fine-tuning the large unsupervised LM using reinforcement learning to maximize this estimated reward without drifting too far from the original model. In this paper we introduce a new parameterization of the reward model in RLHF that enables extraction of the corresponding optimal policy in closed form, allowing us to solve the standard RLHF problem with only a simple classification loss. The resulting algorithm, which we call Direct Preference Optimization (DPO), is stable, performant, and computationally lightweight, eliminating the need for sampling from the LM during fine-tuning or performing significant hyperparameter tuning. Our experiments show that DPO can fine-tune LMs to align with human preferences as well as or better than existing methods. Notably, fine-tuning with DPO exceeds PPO-based RLHF in ability to control sentiment of generations, and matches or improves response quality in summarization and single-turn dialogue while being substantially simpler to implement and train.",
      "authors": [
        "Rafael Rafailov",
        "Archit Sharma",
        "E. Mitchell",
        "Stefano Ermon",
        "Christopher D. Manning",
        "Chelsea Finn"
      ],
      "year": 2023,
      "citation_count": 5332,
      "layer": 1,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/0d1c76d45afa012ded7ab741194baf142117c495",
      "pdf_link": "",
      "venue": "Neural Information Processing Systems",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "f2d0f3d47ae850f49a58f4977393bd0025af4bec",
      "title": "HybridFlow: A Flexible and Efficient RLHF Framework",
      "abstract": "Reinforcement Learning from Human Feedback (RLHF) is widely used in Large Language Model (LLM) alignment. Traditional RL can be modeled as a dataflow, where each node represents computation of a neural network (NN) and each edge denotes data dependencies between the NNs. RLHF complicates the dataflow by expanding each node into a distributed LLM training or generation program, and each edge into a many-to-many multicast. Traditional RL frameworks execute the dataflow using a single controller to instruct both intra-node computation and inter-node communication, which can be inefficient in RLHF due to large control dispatch overhead for distributed intra-node computation. Existing RLHF systems adopt a multi-controller paradigm, which can be inflexible due to nesting distributed computation and data communication. We propose HybridFlow, which combines single-controller and multi-controller paradigms in a hybrid manner to enable flexible representation and efficient execution of the RLHF data flow. We carefully design a set of hierarchical APIs that decouple and encapsulate computation and data dependencies in the complex RLHF dataflow, allowing efficient operation orchestration to implement RLHF algorithms and flexible mapping of the computation onto various devices. We further design a 3D-HybridEngine for efficient actor model resharding between training and generation phases, with zero memory redundancy and significantly reduced communication overhead. Our experimental results demonstrate 1.53x~20.57× throughput improvement when running various RLHF algorithms using HybridFlow, as compared with state-of-the-art baselines. HybridFlow source code is available at https://github.com/volcengine/verl",
      "authors": [
        "Guangming Sheng",
        "Chi Zhang",
        "Zilingfeng Ye",
        "Xibin Wu",
        "Wang Zhang",
        "Ru Zhang",
        "Yanghua Peng",
        "Haibin Lin",
        "Chuan Wu"
      ],
      "year": 2024,
      "citation_count": 551,
      "layer": 1,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/f2d0f3d47ae850f49a58f4977393bd0025af4bec",
      "pdf_link": "",
      "venue": "European Conference on Computer Systems",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "600ff4c4ae9fc506c86673c5ecce4fa90803e987",
      "title": "RLAIF vs. RLHF: Scaling Reinforcement Learning from Human Feedback with AI Feedback",
      "abstract": "Reinforcement learning from human feedback (RLHF) has proven effective in aligning large language models (LLMs) with human preferences, but gathering high-quality preference labels is expensive. RL from AI Feedback (RLAIF), introduced in Bai et al., offers a promising alternative that trains the reward model (RM) on preferences generated by an off-the-shelf LLM. Across the tasks of summarization, helpful dialogue generation, and harmless dialogue generation, we show that RLAIF achieves comparable performance to RLHF. Furthermore, we take a step towards\"self-improvement\"by demonstrating that RLAIF can outperform a supervised fine-tuned baseline even when the AI labeler is the same size as the policy, or even the exact same checkpoint as the initial policy. Finally, we introduce direct-RLAIF (d-RLAIF) - a technique that circumvents RM training by obtaining rewards directly from an off-the-shelf LLM during RL, which achieves superior performance to canonical RLAIF. Our results suggest that RLAIF can achieve performance on-par with using human feedback, offering a potential solution to the scalability limitations of RLHF.",
      "authors": [
        "Harrison Lee",
        "Samrat Phatale",
        "Hassan Mansoor",
        "Kellie Lu",
        "Thomas Mesnard",
        "Colton Bishop",
        "Victor Carbune",
        "Abhinav Rastogi"
      ],
      "year": 2023,
      "citation_count": 408,
      "layer": 1,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/600ff4c4ae9fc506c86673c5ecce4fa90803e987",
      "pdf_link": "",
      "venue": "International Conference on Machine Learning",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "6ca16c1c2c60ceda87242c8f8e522d12cc4a13bc",
      "title": "Eureka: Human-Level Reward Design via Coding Large Language Models",
      "abstract": "Large Language Models (LLMs) have excelled as high-level semantic planners for sequential decision-making tasks. However, harnessing them to learn complex low-level manipulation tasks, such as dexterous pen spinning, remains an open problem. We bridge this fundamental gap and present Eureka, a human-level reward design algorithm powered by LLMs. Eureka exploits the remarkable zero-shot generation, code-writing, and in-context improvement capabilities of state-of-the-art LLMs, such as GPT-4, to perform evolutionary optimization over reward code. The resulting rewards can then be used to acquire complex skills via reinforcement learning. Without any task-specific prompting or pre-defined reward templates, Eureka generates reward functions that outperform expert human-engineered rewards. In a diverse suite of 29 open-source RL environments that include 10 distinct robot morphologies, Eureka outperforms human experts on 83% of the tasks, leading to an average normalized improvement of 52%. The generality of Eureka also enables a new gradient-free in-context learning approach to reinforcement learning from human feedback (RLHF), readily incorporating human inputs to improve the quality and the safety of the generated rewards without model updating. Finally, using Eureka rewards in a curriculum learning setting, we demonstrate for the first time, a simulated Shadow Hand capable of performing pen spinning tricks, adeptly manipulating a pen in circles at rapid speed.",
      "authors": [
        "Yecheng Jason Ma",
        "William Liang",
        "Guanzhi Wang",
        "De-An Huang",
        "Osbert Bastani",
        "Dinesh Jayaraman",
        "Yuke Zhu",
        "Linxi Fan",
        "Anima Anandkumar"
      ],
      "year": 2023,
      "citation_count": 393,
      "layer": 1,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/6ca16c1c2c60ceda87242c8f8e522d12cc4a13bc",
      "pdf_link": "",
      "venue": "International Conference on Learning Representations",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "e01515c6138bc525f7aec30fc85f2adf028d4156",
      "title": "Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision",
      "abstract": "Recent AI-assistant agents, such as ChatGPT, predominantly rely on supervised fine-tuning (SFT) with human annotations and reinforcement learning from human feedback (RLHF) to align the output of large language models (LLMs) with human intentions, ensuring they are helpful, ethical, and reliable. However, this dependence can significantly constrain the true potential of AI-assistant agents due to the high cost of obtaining human supervision and the related issues on quality, reliability, diversity, self-consistency, and undesirable biases. To address these challenges, we propose a novel approach called SELF-ALIGN, which combines principle-driven reasoning and the generative power of LLMs for the self-alignment of AI agents with minimal human supervision. Our approach encompasses four stages: first, we use an LLM to generate synthetic prompts, and a topic-guided method to augment the prompt diversity; second, we use a small set of human-written principles for AI models to follow, and guide the LLM through in-context learning from demonstrations (of principles application) to produce helpful, ethical, and reliable responses to user's queries; third, we fine-tune the original LLM with the high-quality self-aligned responses so that the resulting model can generate desirable responses for each query directly without the principle set and the demonstrations anymore; and finally, we offer a refinement step to address the issues of overly-brief or indirect responses. Applying SELF-ALIGN to the LLaMA-65b base language model, we develop an AI assistant named Dromedary. With fewer than 300 lines of human annotations (including<200 seed prompts, 16 generic principles, and 5 exemplars for in-context learning). Dromedary significantly surpasses the performance of several state-of-the-art AI systems, including Text-Davinci-003 and Alpaca, on benchmark datasets with various settings.",
      "authors": [
        "Zhiqing Sun",
        "Yikang Shen",
        "Qinhong Zhou",
        "Hongxin Zhang",
        "Zhenfang Chen",
        "David D. Cox",
        "Yiming Yang",
        "Chuang Gan"
      ],
      "year": 2023,
      "citation_count": 366,
      "layer": 1,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/e01515c6138bc525f7aec30fc85f2adf028d4156",
      "pdf_link": "",
      "venue": "Neural Information Processing Systems",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "182c7b40ff7560a5545764814338f55a2098e441",
      "title": "Reinforced Self-Training (ReST) for Language Modeling",
      "abstract": "Reinforcement learning from human feedback (RLHF) can improve the quality of large language model's (LLM) outputs by aligning them with human preferences. We propose a simple algorithm for aligning LLMs with human preferences inspired by growing batch reinforcement learning (RL), which we call Reinforced Self-Training (ReST). Given an initial LLM policy, ReST produces a dataset by generating samples from the policy, which are then used to improve the LLM policy using offline RL algorithms. ReST is more efficient than typical online RLHF methods because the training dataset is produced offline, which allows data reuse. While ReST is a general approach applicable to all generative learning settings, we focus on its application to machine translation. Our results show that ReST can substantially improve translation quality, as measured by automated metrics and human evaluation on machine translation benchmarks in a compute and sample-efficient manner.",
      "authors": [
        "Caglar Gulcehre",
        "T. Paine",
        "S. Srinivasan",
        "Ksenia Konyushkova",
        "L. Weerts",
        "Abhishek Sharma",
        "Aditya Siddhant",
        "Alexa Ahern",
        "Miaosen Wang",
        "Chenjie Gu",
        "Wolfgang Macherey",
        "A. Doucet",
        "Orhan Firat",
        "Nando de Freitas"
      ],
      "year": 2023,
      "citation_count": 338,
      "layer": 1,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/182c7b40ff7560a5545764814338f55a2098e441",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "44a9d8b0314d34aff91ccff9207d38eed37216ed",
      "title": "Iterative Preference Learning from Human Feedback: Bridging Theory and Practice for RLHF under KL-constraint",
      "abstract": "This paper studies the alignment process of generative models with Reinforcement Learning from Human Feedback (RLHF). We first identify the primary challenges of existing popular methods like offline PPO and offline DPO as lacking in strategical exploration of the environment. Then, to understand the mathematical principle of RLHF, we consider a standard mathematical formulation, the reverse-KL regularized contextual bandit for RLHF. Despite its widespread practical application, a rigorous theoretical analysis of this formulation remains open. We investigate its behavior in three distinct settings -- offline, online, and hybrid -- and propose efficient algorithms with finite-sample theoretical guarantees. Moving towards practical applications, our framework, with a robust approximation of the information-theoretical policy improvement oracle, naturally gives rise to several novel RLHF algorithms. This includes an iterative version of the Direct Preference Optimization (DPO) algorithm for online settings, and a multi-step rejection sampling strategy for offline scenarios. Our empirical evaluations on real-world alignment experiment of large language model demonstrate that these proposed methods significantly surpass existing strong baselines, such as DPO and Rejection Sampling Optimization (RSO), showcasing the connections between solid theoretical foundations and their potent practical implementations.",
      "authors": [
        "Wei Xiong",
        "Hanze Dong",
        "Chen Ye",
        "Han Zhong",
        "Nan Jiang",
        "Tong Zhang"
      ],
      "year": 2023,
      "citation_count": 249,
      "layer": 1,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/44a9d8b0314d34aff91ccff9207d38eed37216ed",
      "pdf_link": "",
      "venue": "International Conference on Machine Learning",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "c78350e81298ca87bc1d59b466fa40081232caaa",
      "title": "Teaching Large Language Models to Reason with Reinforcement Learning",
      "abstract": "Reinforcement Learning from Human Feedback (\\textbf{RLHF}) has emerged as a dominant approach for aligning LLM outputs with human preferences. Inspired by the success of RLHF, we study the performance of multiple algorithms that learn from feedback (Expert Iteration, Proximal Policy Optimization (\\textbf{PPO}), Return-Conditioned RL) on improving LLM reasoning capabilities. We investigate both sparse and dense rewards provided to the LLM both heuristically and via a learned reward model. We additionally start from multiple model sizes and initializations both with and without supervised fine-tuning (\\textbf{SFT}) data. Overall, we find all algorithms perform comparably, with Expert Iteration performing best in most cases. Surprisingly, we find the sample complexity of Expert Iteration is similar to that of PPO, requiring at most on the order of $10^6$ samples to converge from a pretrained checkpoint. We investigate why this is the case, concluding that during RL training models fail to explore significantly beyond solutions already produced by SFT models. Additionally, we discuss a trade off between maj@1 and pass@96 metric performance during SFT training and how conversely RL training improves both simultaneously. We then conclude by discussing the implications of our findings for RLHF and the future role of RL in LLM fine-tuning.",
      "authors": [
        "Alex Havrilla",
        "Yuqing Du",
        "S. Raparthy",
        "Christoforos Nalmpantis",
        "Jane Dwivedi-Yu",
        "Maksym Zhuravinskyi",
        "Eric Hambro",
        "Sainbayar Sukhbaatar",
        "R. Raileanu"
      ],
      "year": 2024,
      "citation_count": 115,
      "layer": 1,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/c78350e81298ca87bc1d59b466fa40081232caaa",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "6f9dbae279fa0c3a90d12f3b0f271dc8e6274817",
      "title": "A Survey of Reinforcement Learning from Human Feedback",
      "abstract": "Reinforcement learning from human feedback (RLHF) is a variant of reinforcement learning (RL) that learns from human feedback instead of relying on an engineered reward function. Building on prior work on the related setting of preference-based reinforcement learning (PbRL), it stands at the intersection of artificial intelligence and human-computer interaction. This positioning offers a promising avenue to enhance the performance and adaptability of intelligent systems while also improving the alignment of their objectives with human values. The training of large language models (LLMs) has impressively demonstrated this potential in recent years, where RLHF played a decisive role in directing the model's capabilities toward human objectives. This article provides a comprehensive overview of the fundamentals of RLHF, exploring the intricate dynamics between RL agents and human input. While recent focus has been on RLHF for LLMs, our survey adopts a broader perspective, examining the diverse applications and wide-ranging impact of the technique. We delve into the core principles that underpin RLHF, shedding light on the symbiotic relationship between algorithms and human feedback, and discuss the main research trends in the field. By synthesizing the current landscape of RLHF research, this article aims to provide researchers as well as practitioners with a comprehensive understanding of this rapidly growing field of research.",
      "authors": [
        "Timo Kaufmann",
        "Paul Weng",
        "Viktor Bengs",
        "Eyke Hüllermeier"
      ],
      "year": 2023,
      "citation_count": 206,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/6f9dbae279fa0c3a90d12f3b0f271dc8e6274817",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "cb3968152f7d93f53d24b00279a90d5071ddc85a",
      "title": "Understanding the Effects of RLHF on LLM Generalisation and Diversity",
      "abstract": "Large language models (LLMs) fine-tuned with reinforcement learning from human feedback (RLHF) have been used in some of the most widely deployed AI models to date, such as OpenAI's ChatGPT or Anthropic's Claude. While there has been significant work developing these methods, our understanding of the benefits and downsides of each stage in RLHF is still limited. To fill this gap, we present an extensive analysis of how each stage of the process (i.e. supervised fine-tuning (SFT), reward modelling, and RLHF) affects two key properties: out-of-distribution (OOD) generalisation and output diversity. OOD generalisation is crucial given the wide range of real-world scenarios in which these models are being used, while output diversity refers to the model's ability to generate varied outputs and is important for a variety of use cases. We perform our analysis across two base models on both summarisation and instruction following tasks, the latter being highly relevant for current LLM use cases. We find that RLHF generalises better than SFT to new inputs, particularly as the distribution shift between train and test becomes larger. However, RLHF significantly reduces output diversity compared to SFT across a variety of measures, implying a tradeoff in current LLM fine-tuning methods between generalisation and diversity. Our results provide guidance on which fine-tuning method should be used depending on the application, and show that more research is needed to improve the tradeoff between generalisation and diversity.",
      "authors": [
        "Robert Kirk",
        "Ishita Mediratta",
        "Christoforos Nalmpantis",
        "Jelena Luketina",
        "Eric Hambro",
        "Edward Grefenstette",
        "R. Raileanu"
      ],
      "year": 2023,
      "citation_count": 204,
      "layer": 1,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/cb3968152f7d93f53d24b00279a90d5071ddc85a",
      "pdf_link": "",
      "venue": "International Conference on Learning Representations",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "548278897d46a54958909bb23bcaecf63e24fadf",
      "title": "Secrets of RLHF in Large Language Models Part I: PPO",
      "abstract": "Large language models (LLMs) have formulated a blueprint for the advancement of artificial general intelligence. Its primary objective is to function as a human-centric (helpful, honest, and harmless) assistant. Alignment with humans assumes paramount significance, and reinforcement learning with human feedback (RLHF) emerges as the pivotal technological paradigm underpinning this pursuit. Current technical routes usually include \\textbf{reward models} to measure human preferences, \\textbf{Proximal Policy Optimization} (PPO) to optimize policy model outputs, and \\textbf{process supervision} to improve step-by-step reasoning capabilities. However, due to the challenges of reward design, environment interaction, and agent training, coupled with huge trial and error cost of large language models, there is a significant barrier for AI researchers to motivate the development of technical alignment and safe landing of LLMs. The stable training of RLHF has still been a puzzle. In the first report, we dissect the framework of RLHF, re-evaluate the inner workings of PPO, and explore how the parts comprising PPO algorithms impact policy agent training. We identify policy constraints being the key factor for the effective implementation of the PPO algorithm. Therefore, we explore the PPO-max, an advanced version of PPO algorithm, to efficiently improve the training stability of the policy model. Based on our main results, we perform a comprehensive analysis of RLHF abilities compared with SFT models and ChatGPT. The absence of open-source implementations has posed significant challenges to the investigation of LLMs alignment. Therefore, we are eager to release technical reports, reward models and PPO codes, aiming to make modest contributions to the advancement of LLMs.",
      "authors": [
        "Rui Zheng",
        "Shihan Dou",
        "Songyang Gao",
        "Wei Shen",
        "Wei-Yuan Shen",
        "Bing Wang",
        "Yan Liu",
        "Senjie Jin",
        "Qin Liu",
        "Limao Xiong",
        "Luyao Chen",
        "Zhiheng Xi",
        "Yuhao Zhou",
        "Nuo Xu",
        "Wen-De Lai",
        "Minghao Zhu",
        "Rongxiang Weng",
        "Wen-Chun Cheng",
        "Cheng Chang",
        "Zhangyue Yin",
        "Yuan Hua",
        "Haoran Huang",
        "Tianxiang Sun",
        "Hang Yan",
        "Tao Gui",
        "Qi Zhang",
        "Xipeng Qiu",
        "Xuanjing Huang"
      ],
      "year": 2023,
      "citation_count": 201,
      "layer": 1,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/548278897d46a54958909bb23bcaecf63e24fadf",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "59a2203ef6ea159bb41540bd282e29e80a8ad579",
      "title": "A Long Way to Go: Investigating Length Correlations in RLHF",
      "abstract": "Great success has been reported using Reinforcement Learning from Human Feedback (RLHF) to align large language models, with open preference datasets enabling wider experimentation, particularly for\"helpfulness\"in tasks like dialogue and web question answering. Alongside these improvements, however, RLHF also often drives models to produce longer outputs. This paper demonstrates, on three diverse settings, that optimizing for response length is, much more than previously thought, a significant factor behind RLHF. Studying the strategies RL optimization uses to maximize reward, we find improvements in reward to largely be driven by increasing response length, instead of other features. Indeed, we find that even a purely length-based reward reproduces most downstream RLHF improvements over supervised fine-tuned models. Testing a comprehensive set of length-countering interventions, we identify the dominant source of these biases to be reward models, which, by studying training dynamics, we find are non-robust and easily influenced by length biases in preference data.",
      "authors": [
        "Prasann Singhal",
        "Tanya Goyal",
        "Jiacheng Xu",
        "Greg Durrett"
      ],
      "year": 2023,
      "citation_count": 179,
      "layer": 1,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/59a2203ef6ea159bb41540bd282e29e80a8ad579",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "900cd128482bbab4d2752d01ce80c55498b78dd2",
      "title": "SWE-RL: Advancing LLM Reasoning via Reinforcement Learning on Open Software Evolution",
      "abstract": "The recent DeepSeek-R1 release has demonstrated the immense potential of reinforcement learning (RL) in enhancing the general reasoning capabilities of large language models (LLMs). While DeepSeek-R1 and other follow-up work primarily focus on applying RL to competitive coding and math problems, this paper introduces SWE-RL, the first approach to scale RL-based LLM reasoning for real-world software engineering. Leveraging a lightweight rule-based reward (e.g., the similarity score between ground-truth and LLM-generated solutions), SWE-RL enables LLMs to autonomously recover a developer's reasoning processes and solutions by learning from extensive open-source software evolution data -- the record of a software's entire lifecycle, including its code snapshots, code changes, and events such as issues and pull requests. Trained on top of Llama 3, our resulting reasoning model, Llama3-SWE-RL-70B, achieves a 41.0% solve rate on SWE-bench Verified -- a human-verified collection of real-world GitHub issues. To our knowledge, this is the best performance reported for medium-sized (<100B) LLMs to date, even comparable to leading proprietary LLMs like GPT-4o. Surprisingly, despite performing RL solely on software evolution data, Llama3-SWE-RL has even emerged with generalized reasoning skills. For example, it shows improved results on five out-of-domain tasks, namely, function coding, library use, code reasoning, mathematics, and general language understanding, whereas a supervised-finetuning baseline even leads to performance degradation on average. Overall, SWE-RL opens up a new direction to improve the reasoning capabilities of LLMs through reinforcement learning on massive software engineering data.",
      "authors": [
        "Yuxiang Wei",
        "Olivier Duchenne",
        "Jade Copet",
        "Quentin Carbonneaux",
        "Lingming Zhang",
        "Daniel Fried",
        "Gabriele Synnaeve",
        "Rishabh Singh",
        "Sida Wang"
      ],
      "year": 2025,
      "citation_count": 87,
      "layer": 1,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/900cd128482bbab4d2752d01ce80c55498b78dd2",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "d0c48d3b80efb9c170e7100cb9d78d1e7f7710bf",
      "title": "Leveraging Generative AI and Large Language Models: A Comprehensive Roadmap for Healthcare Integration",
      "abstract": "Generative artificial intelligence (AI) and large language models (LLMs), exemplified by ChatGPT, are promising for revolutionizing data and information management in healthcare and medicine. However, there is scant literature guiding their integration for non-AI professionals. This study conducts a scoping literature review to address the critical need for guidance on integrating generative AI and LLMs into healthcare and medical practices. It elucidates the distinct mechanisms underpinning these technologies, such as Reinforcement Learning from Human Feedback (RLFH), including few-shot learning and chain-of-thought reasoning, which differentiates them from traditional, rule-based AI systems. It requires an inclusive, collaborative co-design process that engages all pertinent stakeholders, including clinicians and consumers, to achieve these benefits. Although global research is examining both opportunities and challenges, including ethical and legal dimensions, LLMs offer promising advancements in healthcare by enhancing data management, information retrieval, and decision-making processes. Continued innovation in data acquisition, model fine-tuning, prompt strategy development, evaluation, and system implementation is imperative for realizing the full potential of these technologies. Organizations should proactively engage with these technologies to improve healthcare quality, safety, and efficiency, adhering to ethical and legal guidelines for responsible application.",
      "authors": [
        "Ping Yu",
        "Hua Xu",
        "Xia Hu",
        "C. Deng"
      ],
      "year": 2023,
      "citation_count": 172,
      "layer": 1,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/d0c48d3b80efb9c170e7100cb9d78d1e7f7710bf",
      "pdf_link": "",
      "venue": "Healthcare",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "16d68add6cbad736e6a75b00d2bf8bd49e4dbd40",
      "title": "DPO Meets PPO: Reinforced Token Optimization for RLHF",
      "abstract": "In the classical Reinforcement Learning from Human Feedback (RLHF) framework, Proximal Policy Optimization (PPO) is employed to learn from sparse, sentence-level rewards -- a challenging scenario in traditional deep reinforcement learning. Despite the great successes of PPO in the alignment of large language models, its open-source implementation is still largely sub-optimal. To address these issues, we introduce a framework that models RLHF problems as a Markov decision process (MDP), enabling the capture of fine-grained token-wise information. Under this framework, we introduce an algorithm Reinforced Token Optimization (\\texttt{RTO}), which learns the token-wise reward function from preference data and performs policy optimization based on this learned token-wise reward signal. Theoretically, \\texttt{RTO} is proven to have the capability of finding the near-optimal policy sample-efficiently. For its practical implementation, \\texttt{RTO} innovatively integrates Direct Preference Optimization (DPO) and PPO. DPO, originally derived from sparse sentence rewards, surprisingly provides us with a token-wise characterization of response quality, which is seamlessly incorporated into our subsequent PPO training stage. Extensive experiments demonstrate that \\texttt{RTO} performs better than PPO and other direct preference learning algorithms. In particular, RTO outperforms PPO by 7.5 points on the AlpacaEval 2 benchmark and by 4.1 points on Arena-Hard. Our code and models are available at \\href{https://github.com/zkshan2002/RTO}{https://github.com/zkshan2002/RTO}.",
      "authors": [
        "Han Zhong",
        "Guhao Feng",
        "Wei Xiong",
        "Li Zhao",
        "Di He",
        "Jiang Bian",
        "Liwei Wang"
      ],
      "year": 2024,
      "citation_count": 84,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/16d68add6cbad736e6a75b00d2bf8bd49e4dbd40",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "0c43750030198dbe7fe164e1ce743ec64427bca1",
      "title": "Scaling Laws for Reward Model Overoptimization in Direct Alignment Algorithms",
      "abstract": "Reinforcement Learning from Human Feedback (RLHF) has been crucial to the recent success of Large Language Models (LLMs), however, it is often a complex and brittle process. In the classical RLHF framework, a reward model is first trained to represent human preferences, which is in turn used by an online reinforcement learning (RL) algorithm to optimize the LLM. A prominent issue with such methods is reward over-optimization or reward hacking, where performance as measured by the learned proxy reward model increases, but true quality plateaus or even deteriorates. Direct Alignment Algorithms (DDAs) like Direct Preference Optimization have emerged as alternatives to the classical RLHF pipeline by circumventing the reward modeling phase. However, although DAAs do not use a separate proxy reward model, they still commonly deteriorate from over-optimization. While the so-called reward hacking phenomenon is not well-defined for DAAs, we still uncover similar trends: at higher KL budgets, DAA algorithms exhibit similar degradation patterns to their classic RLHF counterparts. In particular, we find that DAA methods deteriorate not only across a wide range of KL budgets but also often before even a single epoch of the dataset is completed. Through extensive empirical experimentation, this work formulates and formalizes the reward over-optimization or hacking problem for DAAs and explores its consequences across objectives, training regimes, and model scales.",
      "authors": [
        "Rafael Rafailov",
        "Yaswanth Chittepu",
        "Ryan Park",
        "Harshit S. Sikchi",
        "Joey Hejna",
        "Bradley Knox",
        "Chelsea Finn",
        "S. Niekum"
      ],
      "year": 2024,
      "citation_count": 84,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/0c43750030198dbe7fe164e1ce743ec64427bca1",
      "pdf_link": "",
      "venue": "Neural Information Processing Systems",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "550006bea81e4ccb67743dd1b82a70b86b48d93a",
      "title": "RL-VLM-F: Reinforcement Learning from Vision Language Foundation Model Feedback",
      "abstract": "Reward engineering has long been a challenge in Reinforcement Learning (RL) research, as it often requires extensive human effort and iterative processes of trial-and-error to design effective reward functions. In this paper, we propose RL-VLM-F, a method that automatically generates reward functions for agents to learn new tasks, using only a text description of the task goal and the agent's visual observations, by leveraging feedbacks from vision language foundation models (VLMs). The key to our approach is to query these models to give preferences over pairs of the agent's image observations based on the text description of the task goal, and then learn a reward function from the preference labels, rather than directly prompting these models to output a raw reward score, which can be noisy and inconsistent. We demonstrate that RL-VLM-F successfully produces effective rewards and policies across various domains - including classic control, as well as manipulation of rigid, articulated, and deformable objects - without the need for human supervision, outperforming prior methods that use large pretrained models for reward generation under the same assumptions. Videos can be found on our project website: https://rlvlmf2024.github.io/",
      "authors": [
        "Yufei Wang",
        "Zhanyi Sun",
        "Jesse Zhang",
        "Zhou Xian",
        "Erdem Biyik",
        "David Held",
        "Zackory Erickson"
      ],
      "year": 2024,
      "citation_count": 81,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/550006bea81e4ccb67743dd1b82a70b86b48d93a",
      "pdf_link": "",
      "venue": "International Conference on Machine Learning",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "ec97a1565dff9d2fab1ef489e47296bbef68b680",
      "title": "Using Human Feedback to Fine-tune Diffusion Models without Any Reward Model",
      "abstract": "Using reinforcement learning with human feedback (RLHF) has shown significant promise in fine-tuning diffusion models. Previous methods start by training a reward model that aligns with human preferences, then leverage RL techniques to fine-tune the underlying models. However, crafting an efficient reward model demands extensive datasets, optimal architecture, and manual hyperparameter tuning, making the process both time and cost-intensive. The direct preference optimization (DPO) method, effective in fine-tuning large language models, eliminates the necessity for a reward model. However, the extensive GPU memory requirement of the diffusion model's denoising process hinders the direct application of the DPO method. To address this issue, we introduce the Direct Preference for De-noising Diffusion Policy Optimization (D3PO) method to directly fine-tune diffusion models. The theoretical analysis demonstrates that although D3PO omits training a reward model, it effectively functions as the optimal re-ward model trained using human feedback data to guide the learning process. This approach requires no training of a reward model, proving to be more direct, cost-effective, and minimizing computational overhead. In experiments, our method uses the relative scale of objectives as a proxy for human preference, delivering comparable results to methods using ground-truth rewards. Moreover, D3PO demonstrates the ability to reduce image distortion rates and generate safer images, overcoming challenges lacking robust reward models. Our code is publicly available at https://github.com/yk7333/D3PO.",
      "authors": [
        "Kai Yang",
        "Jian Tao",
        "Jiafei Lyu",
        "Chunjiang Ge",
        "Jiaxin Chen",
        "Qimai Li",
        "Weihan Shen",
        "Xiaolong Zhu",
        "Xiu Li"
      ],
      "year": 2023,
      "citation_count": 143,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/ec97a1565dff9d2fab1ef489e47296bbef68b680",
      "pdf_link": "",
      "venue": "Computer Vision and Pattern Recognition",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "db32da8f3b075d566a73512f4ccc2c95449c75a1",
      "title": "MaxMin-RLHF: Alignment with Diverse Human Preferences",
      "abstract": "Reinforcement Learning from Human Feedback (RLHF) aligns language models to human preferences by employing a singular reward model derived from preference data. However, such an approach overlooks the rich diversity of human preferences inherent in data collected from multiple users. In this work, we first derive an impossibility result of alignment with single reward RLHF, thereby highlighting its insufficiency in representing diverse human preferences. To provide an equitable solution to the problem, we learn a mixture of preference distributions via an expectation-maximization algorithm and propose a MaxMin alignment objective for policy learning inspired by the Egalitarian principle in social choice theory to better represent diverse human preferences. We elucidate the connection of our proposed approach to distributionally robust optimization and general utility RL, thereby highlighting the generality and robustness of our proposed solution. We present comprehensive experimental results on small-scale (GPT-2) and large-scale language models (with Tulu2-7B) and show the efficacy of the proposed approach in the presence of diversity among human preferences. Our algorithm achieves an average improvement of more than 16% in win-rates over conventional RLHF algorithms and improves the win-rate (accuracy) for minority groups by over 33% without compromising the performance of majority groups, showcasing the robustness and fairness of our approach. We remark that our findings in this work are not only limited to language models but also extend to reinforcement learning in general.",
      "authors": [
        "Souradip Chakraborty",
        "Jiahao Qiu",
        "Hui Yuan",
        "Alec Koppel",
        "Furong Huang",
        "Dinesh Manocha",
        "A. S. Bedi",
        "Mengdi Wang"
      ],
      "year": 2024,
      "citation_count": 67,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/db32da8f3b075d566a73512f4ccc2c95449c75a1",
      "pdf_link": "",
      "venue": "International Conference on Machine Learning",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "ea75117f34b168a20f2a4309ac2eb685ca6b1436",
      "title": "Chain-of-Thought Hub: A Continuous Effort to Measure Large Language Models' Reasoning Performance",
      "abstract": "As large language models (LLMs) are continuously being developed, their evaluation becomes increasingly important yet challenging. This work proposes Chain-of-Thought Hub, an open-source evaluation suite on the multi-step reasoning capabilities of large language models. We are interested in this setting for two reasons: (1) from the behavior of GPT and PaLM model family, we observe that complex reasoning is likely to be a key differentiator between weaker and stronger LLMs; (2) we envisage large language models to become the next-generation computational platform and foster an ecosystem of LLM-based new applications, this naturally requires the foundation models to perform complex tasks that often involve the composition of linguistic and logical operations. Our approach is to compile a suite of challenging reasoning benchmarks to track the progress of LLMs. Our current results show that: (1) model scale clearly correlates with reasoning capabilities; (2) As of May 2023, Claude-v1.3 and PaLM-2 are the only two models that are comparable with GPT-4, while open-sourced models still lag behind; (3) LLaMA-65B performs closely to code-davinci-002, indicating that with successful further development such as reinforcement learning from human feedback (RLHF), it has great potential to be close to GPT-3.5-Turbo. Our results also suggest that for the open-source efforts to catch up, the community may focus more on building better base models and exploring RLHF.",
      "authors": [
        "Yao Fu",
        "Litu Ou",
        "Mingyu Chen",
        "Yuhao Wan",
        "Hao-Chun Peng",
        "Tushar Khot"
      ],
      "year": 2023,
      "citation_count": 122,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/ea75117f34b168a20f2a4309ac2eb685ca6b1436",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "fb09b581589e1195ff018179c6a11668587c6d64",
      "title": "Vision-Language Models are Zero-Shot Reward Models for Reinforcement Learning",
      "abstract": "Reinforcement learning (RL) requires either manually specifying a reward function, which is often infeasible, or learning a reward model from a large amount of human feedback, which is often very expensive. We study a more sample-efficient alternative: using pretrained vision-language models (VLMs) as zero-shot reward models (RMs) to specify tasks via natural language. We propose a natural and general approach to using VLMs as reward models, which we call VLM-RMs. We use VLM-RMs based on CLIP to train a MuJoCo humanoid to learn complex tasks without a manually specified reward function, such as kneeling, doing the splits, and sitting in a lotus position. For each of these tasks, we only provide a single sentence text prompt describing the desired task with minimal prompt engineering. We provide videos of the trained agents at: https://sites.google.com/view/vlm-rm. We can improve performance by providing a second\"baseline\"prompt and projecting out parts of the CLIP embedding space irrelevant to distinguish between goal and baseline. Further, we find a strong scaling effect for VLM-RMs: larger VLMs trained with more compute and data are better reward models. The failure modes of VLM-RMs we encountered are all related to known capability limitations of current VLMs, such as limited spatial reasoning ability or visually unrealistic environments that are far off-distribution for the VLM. We find that VLM-RMs are remarkably robust as long as the VLM is large enough. This suggests that future VLMs will become more and more useful reward models for a wide range of RL applications.",
      "authors": [
        "Juan Rocamonde",
        "Victoriano Montesinos",
        "Elvis Nava",
        "Ethan Perez",
        "David Lindner"
      ],
      "year": 2023,
      "citation_count": 106,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/fb09b581589e1195ff018179c6a11668587c6d64",
      "pdf_link": "",
      "venue": "International Conference on Learning Representations",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "9732c864d1d4161fcb106f2961d9a80dd4fffc9a",
      "title": "Dense Reward for Free in Reinforcement Learning from Human Feedback",
      "abstract": "Reinforcement Learning from Human Feedback (RLHF) has been credited as the key advance that has allowed Large Language Models (LLMs) to effectively follow instructions and produce useful assistance. Classically, this involves generating completions from the LLM in response to a query before using a separate reward model to assign a score to the full completion. As an auto-regressive process, the LLM has to take many\"actions\"(selecting individual tokens) and only receives a single, sparse reward at the end of an episode, a setup that is known to be difficult to optimise in traditional reinforcement learning. In this work we leverage the fact that the reward model contains more information than just its scalar output, in particular, it calculates an attention map over tokens as part of the transformer architecture. We use these attention weights to redistribute the reward along the whole completion, effectively densifying the signal and highlighting the most important tokens, all without incurring extra computational cost or requiring any additional modelling. We demonstrate that, theoretically, this approach is equivalent to potential-based reward shaping, ensuring that the optimal policy remains unchanged. Empirically, we show that it stabilises training, accelerates the rate of learning, and, in practical cases, may lead to better local optima.",
      "authors": [
        "Alex J. Chan",
        "Hao Sun",
        "Samuel Holt",
        "M. Schaar"
      ],
      "year": 2024,
      "citation_count": 50,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/9732c864d1d4161fcb106f2961d9a80dd4fffc9a",
      "pdf_link": "",
      "venue": "International Conference on Machine Learning",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "6366cb50a5e2043b2bca11a8f03005c42b036c3e",
      "title": "Multi-turn Reinforcement Learning from Preference Human Feedback",
      "abstract": "Reinforcement Learning from Human Feedback (RLHF) has become the standard approach for aligning Large Language Models (LLMs) with human preferences, allowing LLMs to demonstrate remarkable abilities in various tasks. Existing methods work by emulating the preferences at the single decision (turn) level, limiting their capabilities in settings that require planning or multi-turn interactions to achieve a long-term goal. In this paper, we address this issue by developing novel methods for Reinforcement Learning (RL) from preference feedback between two full multi-turn conversations. In the tabular setting, we present a novel mirror-descent-based policy optimization algorithm for the general multi-turn preference-based RL problem, and prove its convergence to Nash equilibrium. To evaluate performance, we create a new environment, Education Dialogue, where a teacher agent guides a student in learning a random topic, and show that a deep RL variant of our algorithm outperforms RLHF baselines. Finally, we show that in an environment with explicit rewards, our algorithm recovers the same performance as a reward-based RL baseline, despite relying solely on a weaker preference signal.",
      "authors": [
        "Lior Shani",
        "Aviv Rosenberg",
        "Asaf B. Cassel",
        "Oran Lang",
        "Daniele Calandriello",
        "Avital Zipori",
        "Hila Noga",
        "Orgad Keller",
        "Bilal Piot",
        "Idan Szpektor",
        "A. Hassidim",
        "Yossi Matias",
        "Rémi Munos"
      ],
      "year": 2024,
      "citation_count": 48,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/6366cb50a5e2043b2bca11a8f03005c42b036c3e",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "a43c2ba35e16d2828ab9b27a92edc68b6af8846d",
      "title": "Value-Incentivized Preference Optimization: A Unified Approach to Online and Offline RLHF",
      "abstract": "Reinforcement learning from human feedback (RLHF) has demonstrated great promise in aligning large language models (LLMs) with human preference. Depending on the availability of preference data, both online and offline RLHF are active areas of investigation. A key bottleneck is understanding how to incorporate uncertainty estimation in the reward function learned from the preference data for RLHF, regardless of how the preference data is collected. While the principles of optimism or pessimism under uncertainty are well-established in standard reinforcement learning (RL), a practically-implementable and theoretically-grounded form amenable to large language models is not yet available, as standard techniques for constructing confidence intervals become intractable under arbitrary policy parameterizations. In this paper, we introduce a unified approach to online and offline RLHF -- value-incentivized preference optimization (VPO) -- which regularizes the maximum-likelihood estimate of the reward function with the corresponding value function, modulated by a $\\textit{sign}$ to indicate whether the optimism or pessimism is chosen. VPO also directly optimizes the policy with implicit reward modeling, and therefore shares a simpler RLHF pipeline similar to direct preference optimization. Theoretical guarantees of VPO are provided for both online and offline settings, matching the rates of their standard RL counterparts. Moreover, experiments on text summarization and dialog verify the practicality and effectiveness of VPO.",
      "authors": [
        "Shicong Cen",
        "Jincheng Mei",
        "Katayoon Goshvadi",
        "Hanjun Dai",
        "Tong Yang",
        "Sherry Yang",
        "D. Schuurmans",
        "Yuejie Chi",
        "Bo Dai"
      ],
      "year": 2024,
      "citation_count": 44,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/a43c2ba35e16d2828ab9b27a92edc68b6af8846d",
      "pdf_link": "",
      "venue": "International Conference on Learning Representations",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "96d6bb5d6abdeda9b2db9af6296527200ba7aa32",
      "title": "Boosting Theory-of-Mind Performance in Large Language Models via Prompting",
      "abstract": "Large language models (LLMs) excel in many tasks in 2023, but they still face challenges in complex reasoning. Theory-of-mind (ToM) tasks, which require understanding agents' beliefs, goals, and mental states, are essential for common-sense reasoning involving humans, making it crucial to enhance LLM performance in this area. This study measures the ToM performance of GPT-4 and three GPT-3.5 variants (Davinci-2, Davinci-3, GPT-3.5-Turbo), and investigates the effectiveness of in-context learning in improving their ToM comprehension. We evaluated prompts featuring two-shot chain of thought reasoning and step-by-step thinking instructions. We found that LLMs trained with Reinforcement Learning from Human Feedback (RLHF) (all models excluding Davinci-2) improved their ToM accuracy via in-context learning. GPT-4 performed best in zero-shot settings, reaching nearly 80% ToM accuracy, but still fell short of the 87% human accuracy on the test set. However, when supplied with prompts for in-context learning, all RLHF-trained LLMs exceeded 80% ToM accuracy, with GPT-4 reaching 100%. These results demonstrate that appropriate prompting enhances LLM ToM reasoning, and they underscore the context-dependent nature of LLM cognitive capacities.",
      "authors": [
        "Shima Rahimi Moghaddam",
        "C. Honey"
      ],
      "year": 2023,
      "citation_count": 86,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/96d6bb5d6abdeda9b2db9af6296527200ba7aa32",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "420af69e5ae3686b709c14a8cec7dc9f90a85681",
      "title": "ChatGPT: perspectives from human–computer interaction and psychology",
      "abstract": "The release of GPT-4 has garnered widespread attention across various fields, signaling the impending widespread adoption and application of Large Language Models (LLMs). However, previous research has predominantly focused on the technical principles of ChatGPT and its social impact, overlooking its effects on human–computer interaction and user psychology. This paper explores the multifaceted impacts of ChatGPT on human–computer interaction, psychology, and society through a literature review. The author investigates ChatGPT’s technical foundation, including its Transformer architecture and RLHF (Reinforcement Learning from Human Feedback) process, enabling it to generate human-like responses. In terms of human–computer interaction, the author studies the significant improvements GPT models bring to conversational interfaces. The analysis extends to psychological impacts, weighing the potential of ChatGPT to mimic human empathy and support learning against the risks of reduced interpersonal connections. In the commercial and social domains, the paper discusses the applications of ChatGPT in customer service and social services, highlighting the improvements in efficiency and challenges such as privacy issues. Finally, the author offers predictions and recommendations for ChatGPT’s future development directions and its impact on social relationships.",
      "authors": [
        "Jiaxi Liu"
      ],
      "year": 2024,
      "citation_count": 37,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/420af69e5ae3686b709c14a8cec7dc9f90a85681",
      "pdf_link": "",
      "venue": "Frontiers Artif. Intell.",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "2d906cda427cb2c4a71069423312e57ba4cd5445",
      "title": "Reinforcement Learning Enhanced LLMs: A Survey",
      "abstract": "Reinforcement learning (RL) enhanced large language models (LLMs), particularly exemplified by DeepSeek-R1, have exhibited outstanding performance. Despite the effectiveness in improving LLM capabilities, its implementation remains highly complex, requiring complex algorithms, reward modeling strategies, and optimization techniques. This complexity poses challenges for researchers and practitioners in developing a systematic understanding of RL-enhanced LLMs. Moreover, the absence of a comprehensive survey summarizing existing research on RL-enhanced LLMs has limited progress in this domain, hindering further advancements. In this work, we are going to make a systematic review of the most up-to-date state of knowledge on RL-enhanced LLMs, attempting to consolidate and analyze the rapidly growing research in this field, helping researchers understand the current challenges and advancements. Specifically, we (1) detail the basics of RL; (2) introduce popular RL-enhanced LLMs; (3) review researches on two widely-used reward model-based RL techniques: Reinforcement Learning from Human Feedback (RLHF) and Reinforcement Learning from AI Feedback (RLAIF); and (4) explore Direct Preference Optimization (DPO), a set of methods that bypass the reward model to directly use human preference data for aligning LLM outputs with human expectations. We will also point out current challenges and deficiencies of existing methods and suggest some avenues for further improvements. Project page of this work can be found at https://github.com/ShuheWang1998/Reinforcement-Learning-Enhanced-LLMs-A-Survey.",
      "authors": [
        "Shuhe Wang",
        "Shengyu Zhang",
        "Jie Zhang",
        "Runyi Hu",
        "Xiaoya Li",
        "Tianwei Zhang",
        "Jiwei Li",
        "Fei Wu",
        "Guoyin Wang",
        "Eduard H. Hovy"
      ],
      "year": 2024,
      "citation_count": 35,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/2d906cda427cb2c4a71069423312e57ba4cd5445",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "b6b7a7fe30623f06a627d3ddbe34f40cb96a982f",
      "title": "Self-Generated Critiques Boost Reward Modeling for Language Models",
      "abstract": "Reward modeling is crucial for aligning large language models (LLMs) with human preferences, especially in reinforcement learning from human feedback (RLHF). However, current reward models mainly produce scalar scores and struggle to incorporate critiques in a natural language format. We hypothesize that predicting both critiques and the scalar reward would improve reward modeling ability. Motivated by this, we propose Critic-RM, a framework that improves reward models using self-generated critiques without extra supervision. Critic-RM employs a two-stage process: generating and filtering high-quality critiques, followed by joint fine-tuning on reward prediction and critique generation. Experiments across benchmarks show that Critic-RM improves reward modeling accuracy by 3.7%-7.3% compared to standard reward models and LLM judges, demonstrating strong performance and data efficiency. Additional studies further validate the effectiveness of generated critiques in rectifying flawed reasoning steps with 2.5%-3.2% gains in improving reasoning accuracy.",
      "authors": [
        "Yue Yu",
        "Zhengxing Chen",
        "Aston Zhang",
        "Liang Tan",
        "Chenguang Zhu",
        "Richard Yuanzhe Pang",
        "Yundi Qian",
        "Xuewei Wang",
        "Suchin Gururangan",
        "Chao Zhang",
        "M. Kambadur",
        "Dhruv Mahajan",
        "Rui Hou"
      ],
      "year": 2024,
      "citation_count": 34,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/b6b7a7fe30623f06a627d3ddbe34f40cb96a982f",
      "pdf_link": "",
      "venue": "North American Chapter of the Association for Computational Linguistics",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "386cebdba39d2d5f2862a9ab43a8d807f3863dae",
      "title": "Contrastive Preference Learning: Learning from Human Feedback without RL",
      "abstract": "Reinforcement Learning from Human Feedback (RLHF) has emerged as a popular paradigm for aligning models with human intent. Typically RLHF algorithms operate in two phases: first, use human preferences to learn a reward function and second, align the model by optimizing the learned reward via reinforcement learning (RL). This paradigm assumes that human preferences are distributed according to reward, but recent work suggests that they instead follow the regret under the user's optimal policy. Thus, learning a reward function from feedback is not only based on a flawed assumption of human preference, but also leads to unwieldy optimization challenges that stem from policy gradients or bootstrapping in the RL phase. Because of these optimization challenges, contemporary RLHF methods restrict themselves to contextual bandit settings (e.g., as in large language models) or limit observation dimensionality (e.g., state-based robotics). We overcome these limitations by introducing a new family of algorithms for optimizing behavior from human feedback using the regret-based model of human preferences. Using the principle of maximum entropy, we derive Contrastive Preference Learning (CPL), an algorithm for learning optimal policies from preferences without learning reward functions, circumventing the need for RL. CPL is fully off-policy, uses only a simple contrastive objective, and can be applied to arbitrary MDPs. This enables CPL to elegantly scale to high-dimensional and sequential RLHF problems while being simpler than prior methods.",
      "authors": [
        "Joey Hejna",
        "Rafael Rafailov",
        "Harshit S. Sikchi",
        "Chelsea Finn",
        "S. Niekum",
        "W. B. Knox",
        "Dorsa Sadigh"
      ],
      "year": 2023,
      "citation_count": 65,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/386cebdba39d2d5f2862a9ab43a8d807f3863dae",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "af7669dc48c70d8cf6fccdf1322d6056a6b39dc8",
      "title": "Confronting Reward Model Overoptimization with Constrained RLHF",
      "abstract": "Large language models are typically aligned with human preferences by optimizing $\\textit{reward models}$ (RMs) fitted to human feedback. However, human preferences are multi-faceted, and it is increasingly common to derive reward from a composition of simpler reward models which each capture a different aspect of language quality. This itself presents a challenge, as it is difficult to appropriately weight these component RMs when combining them. Compounding this difficulty, because any RM is only a proxy for human evaluation, this process is vulnerable to $\\textit{overoptimization}$, wherein past a certain point, accumulating higher reward is associated with worse human ratings. In this paper, we perform, to our knowledge, the first study on overoptimization in composite RMs, showing that correlation between component RMs has a significant effect on the locations of these points. We then introduce an approach to solve this issue using constrained reinforcement learning as a means of preventing the agent from exceeding each RM's threshold of usefulness. Our method addresses the problem of weighting component RMs by learning dynamic weights, naturally expressed by Lagrange multipliers. As a result, each RM stays within the range at which it is an effective proxy, improving evaluation performance. Finally, we introduce an adaptive method using gradient-free optimization to identify and optimize towards these points during a single run.",
      "authors": [
        "Ted Moskovitz",
        "Aaditya K. Singh",
        "DJ Strouse",
        "T. Sandholm",
        "Ruslan Salakhutdinov",
        "Anca D. Dragan",
        "S. McAleer"
      ],
      "year": 2023,
      "citation_count": 62,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/af7669dc48c70d8cf6fccdf1322d6056a6b39dc8",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "dd951242ebc94bf633eecc4994c64f46146a1413",
      "title": "Reward Shaping to Mitigate Reward Hacking in RLHF",
      "abstract": "Reinforcement Learning from Human Feedback (RLHF) is essential for aligning large language models (LLMs) with human values. However, RLHF is susceptible to \\emph{reward hacking}, where the agent exploits flaws in the reward function rather than learning the intended behavior, thus degrading alignment. Although reward shaping helps stabilize RLHF and partially mitigate reward hacking, a systematic investigation into shaping techniques and their underlying principles remains lacking. To bridge this gap, we present a comprehensive study of the prevalent reward shaping methods. Our analysis suggests two key design principles: (1) the RL reward should be bounded, and (2) the RL reward benefits from rapid initial growth followed by gradual convergence. Guided by these insights, we propose Preference As Reward (PAR), a novel approach that leverages the latent preferences embedded within the reward model as the signal for reinforcement learning. We evaluated PAR on two base models, Gemma2-2B, and Llama3-8B, using two datasets, Ultrafeedback-Binarized and HH-RLHF. Experimental results demonstrate PAR's superior performance over other reward shaping methods. On the AlpacaEval 2.0 benchmark, PAR achieves a win rate of at least 5 percentage points higher than competing approaches. Furthermore, PAR exhibits remarkable data efficiency, requiring only a single reference reward for optimal performance, and maintains robustness against reward hacking even after two full epochs of training. The code is available at https://github.com/PorUna-byte/PAR, and the Work done during the internship at StepFun by Jiayi Fu.",
      "authors": [
        "Jiayi Fu",
        "Xuandong Zhao",
        "Chengyuan Yao",
        "Heng Wang",
        "Qi Han",
        "Yanghua Xiao"
      ],
      "year": 2025,
      "citation_count": 30,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/dd951242ebc94bf633eecc4994c64f46146a1413",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "f2fde6be4b074f509cf974d1aac24019247473ae",
      "title": "How to Evaluate Reward Models for RLHF",
      "abstract": "We introduce a new benchmark for reward models that quantifies their ability to produce strong language models through RLHF (Reinforcement Learning from Human Feedback). The gold-standard approach is to run a full RLHF training pipeline and directly probe downstream LLM performance. However, this process is prohibitively expensive. To address this, we build a predictive model of downstream LLM performance by evaluating the reward model on proxy tasks. These proxy tasks consist of a large-scale human preference and a verifiable correctness preference dataset, in which we measure 12 metrics across 12 domains. To investigate which reward model metrics are most correlated to gold-standard RLHF outcomes, we launch an end-to-end RLHF experiment on a large-scale crowdsourced human preference platform to view real reward model downstream performance as ground truth. Ultimately, we compile our data and findings into Preference Proxy Evaluations (PPE), the first reward model benchmark explicitly linked to post-RLHF real-world human preference performance, which we open-source for public use and further development. Our code and evaluations can be found at https://github.com/lmarena/PPE .",
      "authors": [
        "Evan Frick",
        "Tianle Li",
        "Connor Chen",
        "Wei-Lin Chiang",
        "Anastasios Nikolas Angelopoulos",
        "Jiantao Jiao",
        "Banghua Zhu",
        "Joseph Gonzalez",
        "I. Stoica"
      ],
      "year": 2024,
      "citation_count": 30,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/f2fde6be4b074f509cf974d1aac24019247473ae",
      "pdf_link": "",
      "venue": "International Conference on Learning Representations",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "eff0410f7d5d78ea6874596a0a77b184d03ecca5",
      "title": "On the Algorithmic Bias of Aligning Large Language Models with RLHF: Preference Collapse and Matching Regularization",
      "abstract": "Accurately aligning large language models (LLMs) with human preferences is crucial for informing fair, economically sound, and statistically efficient decision-making processes. However, we argue that the predominant approach for aligning LLMs with human preferences through a reward model -- reinforcement learning from human feedback (RLHF) -- suffers from an inherent algorithmic bias due to its Kullback--Leibler-based regularization in optimization. In extreme cases, this bias could lead to a phenomenon we term preference collapse, where minority preferences are virtually disregarded. To mitigate this algorithmic bias, we introduce preference matching (PM) RLHF, a novel approach that provably aligns LLMs with the preference distribution of the reward model under the Bradley--Terry--Luce/Plackett--Luce model. Central to our approach is a PM regularizer that takes the form of the negative logarithm of the LLM's policy probability distribution over responses, which helps the LLM balance response diversification and reward maximization. Notably, we obtain this regularizer by solving an ordinary differential equation that is necessary for the PM property. For practical implementation, we introduce a conditional variant of PM RLHF that is tailored to natural language generation. Finally, we empirically validate the effectiveness of conditional PM RLHF through experiments on the OPT and Llama-family models, demonstrating a 29% to 41% improvement in alignment with human preferences, as measured by a certain metric, compared to standard RLHF.",
      "authors": [
        "Jiancong Xiao",
        "Ziniu Li",
        "Xingyu Xie",
        "E. Getzen",
        "Cong Fang",
        "Qi Long",
        "Weijie J. Su"
      ],
      "year": 2024,
      "citation_count": 29,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/eff0410f7d5d78ea6874596a0a77b184d03ecca5",
      "pdf_link": "",
      "venue": "Journal of the American Statistical Association",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "302065b71e09783cab30eed17e85eb437e279ae3",
      "title": "Big-Math: A Large-Scale, High-Quality Math Dataset for Reinforcement Learning in Language Models",
      "abstract": "Increasing interest in reasoning models has led math to become a prominent testing ground for algorithmic and methodological improvements. However, existing open math datasets either contain a small collection of high-quality, human-written problems or a large corpus of machine-generated problems of uncertain quality, forcing researchers to choose between quality and quantity. In this work, we present Big-Math, a dataset of over 250,000 high-quality math questions with verifiable answers, purposefully made for reinforcement learning (RL). To create Big-Math, we rigorously filter, clean, and curate openly available datasets, extracting questions that satisfy our three desiderata: (1) problems with uniquely verifiable solutions, (2) problems that are open-ended, (3) and problems with a closed-form solution. To ensure the quality of Big-Math, we manually verify each step in our filtering process. Based on the findings from our filtering process, we introduce 47,000 new questions with verified answers, Big-Math-Reformulated: closed-ended questions (i.e. multiple choice questions) that have been reformulated as open-ended questions through a systematic reformulation algorithm. Compared to the most commonly used existing open-source datasets for math reasoning, GSM8k and MATH, Big-Math is an order of magnitude larger, while our rigorous filtering ensures that we maintain the questions most suitable for RL. We also provide a rigorous analysis of the dataset, finding that Big-Math contains a high degree of diversity across problem domains, and incorporates a wide range of problem difficulties, enabling a wide range of downstream uses for models of varying capabilities and training requirements. By bridging the gap between data quality and quantity, Big-Math establish a robust foundation for advancing reasoning in LLMs.",
      "authors": [
        "Alon Albalak",
        "Duy Phung",
        "nathan lile",
        "Rafael Rafailov",
        "Kanishk Gandhi",
        "Louis Castricato",
        "Anikait Singh",
        "Chase Blagden",
        "Violet Xiang",
        "Dakota Mahan",
        "Nick Haber"
      ],
      "year": 2025,
      "citation_count": 28,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/302065b71e09783cab30eed17e85eb437e279ae3",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "78a2943fd2424a5515d595d6bdc54b9a4dbb4389",
      "title": "WPO: Enhancing RLHF with Weighted Preference Optimization",
      "abstract": "Reinforcement learning from human feedback (RLHF) is a promising solution to align large language models (LLMs) more closely with human values. Off-policy preference optimization, where the preference data is obtained from other models, is widely adopted due to its cost efficiency and scalability. However, off-policy preference optimization often suffers from a distributional gap between the policy used for data collection and the target policy, leading to suboptimal optimization. In this paper, we propose a novel strategy to mitigate this problem by simulating on-policy learning with off-policy preference data. Our Weighted Preference Optimization (WPO) method adapts off-policy data to resemble on-policy data more closely by reweighting preference pairs according to their probability under the current policy. This method not only addresses the distributional gap problem but also enhances the optimization process without incurring additional costs. We validate our method on instruction following benchmarks including Alpaca Eval 2 and MT-bench. WPO not only outperforms Direct Preference Optimization (DPO) by up to 5.6% on Alpaca Eval 2 but also establishes a remarkable length-controlled winning rate against GPT-4-turbo of 76.7% based on Gemma-2-9b-it. We release the code and models at https://github.com/wzhouad/WPO.",
      "authors": [
        "Wenxuan Zhou",
        "Ravi Agrawal",
        "Shujian Zhang",
        "Sathish Indurthi",
        "Sanqiang Zhao",
        "Kaiqiang Song",
        "Silei Xu",
        "Chenguang Zhu"
      ],
      "year": 2024,
      "citation_count": 28,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/78a2943fd2424a5515d595d6bdc54b9a4dbb4389",
      "pdf_link": "",
      "venue": "Conference on Empirical Methods in Natural Language Processing",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "911e9915df23c4bc59f10608af2aee8335e7a4a5",
      "title": "Large language models for biomedicine: foundations, opportunities, challenges, and best practices",
      "abstract": "OBJECTIVES\nGenerative large language models (LLMs) are a subset of transformers-based neural network architecture models. LLMs have successfully leveraged a combination of an increased number of parameters, improvements in computational efficiency, and large pre-training datasets to perform a wide spectrum of natural language processing (NLP) tasks. Using a few examples (few-shot) or no examples (zero-shot) for prompt-tuning has enabled LLMs to achieve state-of-the-art performance in a broad range of NLP applications. This article by the American Medical Informatics Association (AMIA) NLP Working Group characterizes the opportunities, challenges, and best practices for our community to leverage and advance the integration of LLMs in downstream NLP applications effectively. This can be accomplished through a variety of approaches, including augmented prompting, instruction prompt tuning, and reinforcement learning from human feedback (RLHF).\n\n\nTARGET AUDIENCE\nOur focus is on making LLMs accessible to the broader biomedical informatics community, including clinicians and researchers who may be unfamiliar with NLP. Additionally, NLP practitioners may gain insight from the described best practices.\n\n\nSCOPE\nWe focus on 3 broad categories of NLP tasks, namely natural language understanding, natural language inferencing, and natural language generation. We review the emerging trends in prompt tuning, instruction fine-tuning, and evaluation metrics used for LLMs while drawing attention to several issues that impact biomedical NLP applications, including falsehoods in generated text (confabulation/hallucinations), toxicity, and dataset contamination leading to overfitting. We also review potential approaches to address some of these current challenges in LLMs, such as chain of thought prompting, and the phenomena of emergent capabilities observed in LLMs that can be leveraged to address complex NLP challenge in biomedical applications.",
      "authors": [
        "S. Sahoo",
        "Joseph M. Plasek",
        "Hua Xu",
        "Özlem Uzuner",
        "Trevor Cohen",
        "Meliha Yetisgen-Yildiz",
        "Hongfang Liu",
        "Stéphane Meystre",
        "Yanshan Wang"
      ],
      "year": 2024,
      "citation_count": 27,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/911e9915df23c4bc59f10608af2aee8335e7a4a5",
      "pdf_link": "",
      "venue": "J. Am. Medical Informatics Assoc.",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "521c2905e667ad6d2162ac369cf3f85d70e0f477",
      "title": "Best-of-Venom: Attacking RLHF by Injecting Poisoned Preference Data",
      "abstract": "Reinforcement Learning from Human Feedback (RLHF) is a popular method for aligning Language Models (LM) with human values and preferences. RLHF requires a large number of preference pairs as training data, which are often used in both the Supervised Fine-Tuning and Reward Model training and therefore publicly available datasets are commonly used. In this work, we study to what extent a malicious actor can manipulate the LMs generations by poisoning the preferences, i.e., injecting poisonous preference pairs into these datasets and the RLHF training process. We propose strategies to build poisonous preference pairs and test their performance by poisoning two widely used preference datasets. Our results show that preference poisoning is highly effective: injecting a small amount of poisonous data (1-5\\% of the original dataset), we can effectively manipulate the LM to generate a target entity in a target sentiment (positive or negative). The findings from our experiments also shed light on strategies to defend against the preference poisoning attack.",
      "authors": [
        "Tim Baumgärtner",
        "Yang Gao",
        "Dana Alon",
        "Donald Metzler"
      ],
      "year": 2024,
      "citation_count": 27,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/521c2905e667ad6d2162ac369cf3f85d70e0f477",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "8115ffbbadd1055424d18369dba66ce32a572800",
      "title": "ChatGLM-RLHF: Practices of Aligning Large Language Models with Human Feedback",
      "abstract": "ChatGLM is a free-to-use AI service powered by the ChatGLM family of large language models (LLMs). In this paper, we present the ChatGLM-RLHF pipeline -- a reinforcement learning from human feedback (RLHF) system -- designed to enhance ChatGLM's alignment with human preferences. ChatGLM-RLHF encompasses three major components: the collection of human preference data, the training of the reward model, and the optimization of policies. Throughout the process of integrating ChatGLM-RLHF into production, we encountered and addressed several unprecedented challenges. We introduce the strategies to mitigate reward variance for stabilized large-scale training, implement model parallelism with fused gradient-descent, and design regularization constraints to avoid catastrophic forgetting in LLMs. Experiments show that ChatGLM-RLHF brings significant improvements in alignment tasks compared to the supervised fine-tuned (SFT) version of ChatGLM. For instance, it achieves on average 15\\% more wins against ChatGLM-SFT in Chinese alignment tasks. The work presents our practices of aligning LLMs with human preferences, offering insights into the challenges and solutions in RLHF implementations.",
      "authors": [
        "Zhenyu Hou",
        "Yiin Niu",
        "Zhengxiao Du",
        "Xiaohan Zhang",
        "Xiao Liu",
        "Aohan Zeng",
        "Qinkai Zheng",
        "Minlie Huang",
        "Hongning Wang",
        "Jie Tang",
        "Yuxiao Dong"
      ],
      "year": 2024,
      "citation_count": 23,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/8115ffbbadd1055424d18369dba66ce32a572800",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "7f96bb27a8fca35b1f7d02ee319a64be04114809",
      "title": "LLM-Assisted Light: Leveraging Large Language Model Capabilities for Human-Mimetic Traffic Signal Control in Complex Urban Environments",
      "abstract": "Traffic congestion in metropolitan areas presents a formidable challenge with far-reaching economic, environmental, and societal ramifications. Therefore, effective congestion management is imperative, with traffic signal control (TSC) systems being pivotal in this endeavor. Conventional TSC systems, designed upon rule-based algorithms or reinforcement learning (RL), frequently exhibit deficiencies in managing the complexities and variabilities of urban traffic flows, constrained by their limited capacity for adaptation to unfamiliar scenarios. In response to these limitations, this work introduces an innovative approach that integrates Large Language Models (LLMs) into TSC, harnessing their advanced reasoning and decision-making faculties. Specifically, a hybrid framework that augments LLMs with a suite of perception and decision-making tools is proposed, facilitating the interrogation of both the static and dynamic traffic information. This design places the LLM at the center of the decision-making process, combining external traffic data with established TSC methods. Moreover, a simulation platform is developed to corroborate the efficacy of the proposed framework. The findings from our simulations attest to the system's adeptness in adjusting to a multiplicity of traffic environments without the need for additional training. Notably, in cases of Sensor Outage (SO), our approach surpasses conventional RL-based systems by reducing the average waiting time by $20.4\\%$. This research signifies a notable advance in TSC strategies and paves the way for the integration of LLMs into real-world, dynamic scenarios, highlighting their potential to revolutionize traffic management. The related code is available at https://github.com/Traffic-Alpha/LLM-Assisted-Light.",
      "authors": [
        "Maonan Wang",
        "Aoyu Pang",
        "Yuheng Kan",
        "Man-On Pun",
        "Chung Shue Chen",
        "Bo Huang"
      ],
      "year": 2024,
      "citation_count": 23,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/7f96bb27a8fca35b1f7d02ee319a64be04114809",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "c085e88a0351e393609a95305afc1db792d1db0f",
      "title": "The History and Risks of Reinforcement Learning and Human Feedback",
      "abstract": "Reinforcement learning from human feedback (RLHF) has emerged as a powerful technique to make large language models (LLMs) easier to use and more effective. A core piece of the RLHF process is the training and utilization of a model of human preferences that acts as a reward function for optimization. This approach, which operates at the intersection of many stakeholders and academic disciplines, remains poorly understood. RLHF reward models are often cited as being central to achieving performance, yet very few descriptors of capabilities, evaluations, training methods, or open-source models exist. Given this lack of information, further study and transparency is needed for learned RLHF reward models. In this paper, we illustrate the complex history of optimizing preferences, and articulate lines of inquiry to understand the sociotechnical context of reward models. In particular, we highlight the ontological differences between costs, rewards, and preferences at stake in RLHF's foundations, related methodological tensions, and possible research directions to improve general understanding of how reward models function.",
      "authors": [
        "Nathan Lambert",
        "Thomas Krendl Gilbert",
        "T. Zick"
      ],
      "year": 2023,
      "citation_count": 45,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/c085e88a0351e393609a95305afc1db792d1db0f",
      "pdf_link": "",
      "venue": "",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "777d4ec0148c34b0bfab91e9ac3a902e420b891e",
      "title": "Verbosity Bias in Preference Labeling by Large Language Models",
      "abstract": "In recent years, Large Language Models (LLMs) have witnessed a remarkable surge in prevalence, altering the landscape of natural language processing and machine learning. One key factor in improving the performance of LLMs is alignment with humans achieved with Reinforcement Learning from Human Feedback (RLHF), as for many LLMs such as GPT-4, Bard, etc. In addition, recent studies are investigating the replacement of human feedback with feedback from other LLMs named Reinforcement Learning from AI Feedback (RLAIF). We examine the biases that come along with evaluating LLMs with other LLMs and take a closer look into verbosity bias -- a bias where LLMs sometimes prefer more verbose answers even if they have similar qualities. We see that in our problem setting, GPT-4 prefers longer answers more than humans. We also propose a metric to measure this bias.",
      "authors": [
        "Keita Saito",
        "Akifumi Wachi",
        "Koki Wataoka",
        "Youhei Akimoto"
      ],
      "year": 2023,
      "citation_count": 45,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/777d4ec0148c34b0bfab91e9ac3a902e420b891e",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "57451ce18f3035fcadf64db38420434f9299b7f3",
      "title": "Optimizing Autonomous Driving for Safety: A Human-Centric Approach with LLM-Enhanced RLHF",
      "abstract": "Reinforcement Learning from Human Feedback (RLHF) is popular in large language models (LLMs), whereas traditional Reinforcement Learning (RL) often falls short. Current autonomous driving methods typically utilize either human feedback in machine learning, including RL, or LLMs. Most feedback guides the car agent's learning process (e.g., controlling the car). RLHF is usually applied in the fine-tuning step, requiring direct human \"preferences,\" which are not commonly used in optimizing autonomous driving models. In this research, we innovatively combine RLHF and LLMs to enhance autonomous driving safety. Training a model with human guidance from scratch is inefficient. Our framework starts with a pre-trained autonomous car agent model and implements multiple human-controlled agents, such as cars and pedestrians, to simulate real-life road environments. The autonomous car model is not directly controlled by humans. We integrate both physical and physiological feedback to fine-tune the model, optimizing this process using LLMs. This multi-agent interactive environment ensures safe, realistic interactions before real-world application. Finally, we will validate our model using data gathered from real-life testbeds located in New Jersey and New York City.",
      "authors": [
        "Yuan Sun",
        "Navid Salami Pargoo",
        "Peter J. Jin",
        "Jorge Ortiz"
      ],
      "year": 2024,
      "citation_count": 22,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/57451ce18f3035fcadf64db38420434f9299b7f3",
      "pdf_link": "",
      "venue": "Companion of the 2024 on ACM International Joint Conference on Pervasive and Ubiquitous Computing",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "0a922b4fdbe923b5161b5c6f5adfe586bf7304c3",
      "title": "Large language models will not replace healthcare professionals: curbing popular fears and hype",
      "abstract": "Following the release of ChatGPT, large language models (LLMs) have entered the mainstream. ChatGPT and GPT-4 recently garnered particular attention for attaining expert-level performance in United States Medical Licensing Examinations. However, performance is not perfect, and has not been as impressive in more specialised tests, such as the Membership of the Royal College of General Practitioners Applied Knowledge Test. ChatGPT frequently ‘hallucinates’, providing false, unverified information in the same manner as which it delivers facts. While performance in clinical tasks is expected to improve dramatically with the release of GPT-4, remaining inaccuracy and lack of an uncertainty indicator preclude autonomous deployment of ChatGPT and LLM chatbots like it in clinical settings. LLM applications may nevertheless revolutionise cognitive work – tools such as ChatGPT excel in tasks where specialist knowledge is not required, or is provided by the user prompt: examples include correcting language and rephrasing information for different audiences or within other constraints (e.g. word limits), and it has already been proposed as a tool for administrative tasks, clinical work and patient education. While this does represent an impressive advance in natural language processing, and benefits may be manifold across fields including medicine, these limited use-cases do not live up to the hype surrounding LLMs and artificial intelligence (AI) more generally in 2023. This is due to a fundamental misunderstanding about the form of AI represented by LLMs. Do LLMs represent artificial generalised intelligence (AGI)? The answer is currently probably not, despite emergence of interactive conversational interfaces and few-shot or zero-shot properties – where models execute tasks that they have previously been exposed to only a few times before, or never before, respectively. This is demonstrated by observing how these models are trained, and the composition of their architecture. The backend LLM (GPT-3, from which GPT-3.5 was developed) underpinning older versions of ChatGPT was initially trained on a dataset of billions of words taken from books, Wikipedia and the wider internet. Through a process of machine learning, the GPT-3 accurately encoded the association between individual words in the training dataset. Through ‘reinforcement learning from human feedback’, GPT-3 was subsequently finetuned to provide appropriate responses to users’ queries – producing GPT-3.5. Through these processes, ChatGPT has developed an impressive ability to respond appropriately to diverse prompts, albeit equally lucidly with accurate and inaccurate statements. This lucidity, responsiveness and flexibility have led to sensational claims regarding attainment of AGI that could feasibly replace professionals in cognitive roles. The performance of GPT-4 – which powers newer versions of ChatGPT – dwarfs that of GPT-3.5 across tasks including logical reasoning and medical aptitude tests. Moreover, GPT-4 can be prompted to adopt different roles on demand, and will accept multimodal input, processing images as well as text. Prominent figures in industry and academia have advocated for a moratorium on development of more advanced AI systems in response to concerns regarding safety, ethics and fears of replacement. Despite these fears and hype, the barriers to implementation of LLMs replacing healthcare professionals in any capacity still look out of reach. Although GPT-4’s architecture and training are confidential, it likely relies on similar schemata to its predecessor as it exhibits similar (albeit fewer) hallucinations and reasoning errors, including in medicine. None of ChatGPT’s published autonomous training involved actual comprehension of language in context; the meaning (as we understand it) of words in the dataset was immaterial throughout. While this brute force linguistic processing may prove sufficient to develop a form of AGI, it appears that these LLMs will continue to be afflicted by mistakes and errors. Journal of the Royal Society of Medicine; 2023, Vol. 116(5) 181–182",
      "authors": [
        "A. J. Thirunavukarasu"
      ],
      "year": 2023,
      "citation_count": 43,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/0a922b4fdbe923b5161b5c6f5adfe586bf7304c3",
      "pdf_link": "",
      "venue": "Journal of the Royal Society of Medicine",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "519d5ccbd5aec517ba987209e17afd4741ac9b8a",
      "title": "The Perfect Blend: Redefining RLHF with Mixture of Judges",
      "abstract": "Reinforcement learning from human feedback (RLHF) has become the leading approach for fine-tuning large language models (LLM). However, RLHF has limitations in multi-task learning (MTL) due to challenges of reward hacking and extreme multi-objective optimization (i.e., trade-off of multiple and/or sometimes conflicting objectives). Applying RLHF for MTL currently requires careful tuning of the weights for reward model and data combinations. This is often done via human intuition and does not generalize. In this work, we introduce a novel post-training paradigm which we called Constrained Generative Policy Optimization (CGPO). The core of CGPO is Mixture of Judges (MoJ) with cost-efficient constrained policy optimization with stratification, which can identify the perfect blend in RLHF in a principled manner. It shows strong empirical results with theoretical guarantees, does not require extensive hyper-parameter tuning, and is plug-and-play in common post-training pipelines. Together, this can detect and mitigate reward hacking behaviors while reaching a pareto-optimal point across an extremely large number of objectives. Our empirical evaluations demonstrate that CGPO significantly outperforms standard RLHF algorithms like PPO and DPO across various tasks including general chat, STEM questions, instruction following, and coding. Specifically, CGPO shows improvements of 7.4% in AlpacaEval-2 (general chat), 12.5% in Arena-Hard (STEM&reasoning), and consistent gains in other domains like math and coding. Notably, PPO, while commonly used, is prone to severe reward hacking in popular coding benchmarks, which CGPO successfully addresses. This breakthrough in RLHF not only tackles reward hacking and extreme multi-objective optimization challenges but also advances the state-of-the-art in aligning general-purpose LLMs for diverse applications.",
      "authors": [
        "Tengyu Xu",
        "Eryk Helenowski",
        "Karthik Abinav Sankararaman",
        "Di Jin",
        "Kaiyan Peng",
        "Eric Han",
        "Shaoliang Nie",
        "Chen Zhu",
        "Hejia Zhang",
        "Wenxuan Zhou",
        "Zhouhao Zeng",
        "Yun He",
        "Karishma Mandyam",
        "Arya Talabzadeh",
        "Madian Khabsa",
        "Gabriel Cohen",
        "Yuandong Tian",
        "Hao Ma",
        "Si-Yuan Wang",
        "Han Fang"
      ],
      "year": 2024,
      "citation_count": 21,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/519d5ccbd5aec517ba987209e17afd4741ac9b8a",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "25709a50df5eb8e40dce7ffe6ecd3bfa79969c7a",
      "title": "Exploring Data Scaling Trends and Effects in Reinforcement Learning from Human Feedback",
      "abstract": "Reinforcement Learning from Human Feedback (RLHF) is crucial for aligning large language models with human preferences. While recent research has focused on algorithmic improvements, the importance of prompt-data construction has been overlooked. This paper addresses this gap by exploring data-driven bottlenecks in RLHF performance scaling, particularly reward hacking and decreasing response diversity. We introduce a hybrid reward system combining reasoning task verifiers (RTV) and a generative reward model (GenRM) to mitigate reward hacking. We also propose a novel prompt-selection method, Pre-PPO, to maintain response diversity and enhance learning effectiveness. Additionally, we find that prioritizing mathematical and coding tasks early in RLHF training significantly improves performance. Experiments across two model sizes validate our methods' effectiveness and scalability. Results show that RTV is most resistant to reward hacking, followed by GenRM with ground truth, and then GenRM with SFT Best-of-N responses. Our strategies enable rapid capture of subtle task-specific distinctions, leading to substantial improvements in overall RLHF performance. This work highlights the importance of careful data construction and provides practical methods to overcome performance barriers in RLHF.",
      "authors": [
        "Wei Shen",
        "Guanlin Liu",
        "Zheng Wu",
        "Ruofei Zhu",
        "Qingping Yang",
        "Chao Xin",
        "Yu Yue",
        "Lin Yan"
      ],
      "year": 2025,
      "citation_count": 20,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/25709a50df5eb8e40dce7ffe6ecd3bfa79969c7a",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "9cb7f7415fb0590186a3d903a8d5d7044b7a3fdc",
      "title": "The Alignment Ceiling: Objective Mismatch in Reinforcement Learning from Human Feedback",
      "abstract": "Reinforcement learning from human feedback (RLHF) has emerged as a powerful technique to make large language models (LLMs) more capable in complex settings. RLHF proceeds as collecting human preference data, training a reward model on said data, and optimizing a base ML model with respect to said reward for extrinsic evaluation metrics (e.g. MMLU, GSM8k). RLHF relies on many assumptions about how the various pieces fit together, such as a reward model capturing human preferences and an RL optimizer extracting the right signal from a reward model. As the RLHF process involves many distinct design decisions, it is easy to assume that multiple processes are correlated and therefore numerically linked. This apparent correlation is often not true, where reward models are easily overoptimized or RL optimizers can reduce performance on tasks not modeled in the data. Notable manifestations of models trained with imperfect RLHF systems are those that are prone to refusing basic requests for safety reasons or appearing lazy in generations. As chat model evaluation becomes increasingly nuanced, the reliance on a perceived link between reward model training, RL scores, and downstream performance drives these issues, which we describe as an objective mismatch. In this paper, we illustrate the causes of this issue, reviewing relevant literature from model-based reinforcement learning, and argue for solutions. By solving objective mismatch in RLHF, the ML models of the future will be more precisely aligned to user instructions for both safety and helpfulness.",
      "authors": [
        "Nathan Lambert",
        "Roberto Calandra"
      ],
      "year": 2023,
      "citation_count": 38,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/9cb7f7415fb0590186a3d903a8d5d7044b7a3fdc",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "cdd0e94e51a02bac22ca5e94fa95daa18f36e226",
      "title": "Uncertainty-Penalized Reinforcement Learning from Human Feedback with Diverse Reward LoRA Ensembles",
      "abstract": "Reinforcement learning from human feedback (RLHF) emerges as a promising paradigm for aligning large language models (LLMs). However, a notable challenge in RLHF is overoptimization, where beyond a certain threshold, the pursuit of higher rewards leads to a decline in human preferences. In this paper, we observe the weakness of KL regularization which is commonly employed in existing RLHF methods to address overoptimization. To mitigate this limitation, we scrutinize the RLHF objective in the offline dataset and propose uncertainty-penalized RLHF (UP-RLHF), which incorporates uncertainty regularization during RL-finetuning. To enhance the uncertainty quantification abilities for reward models, we first propose a diverse low-rank adaptation (LoRA) ensemble by maximizing the nuclear norm of LoRA matrix concatenations. Then we optimize policy models utilizing penalized rewards, determined by both rewards and uncertainties provided by the diverse reward LoRA ensembles. Our experimental results, based on two real human preference datasets, showcase the effectiveness of diverse reward LoRA ensembles in quantifying reward uncertainty. Additionally, uncertainty regularization in UP-RLHF proves to be pivotal in mitigating overoptimization, thereby contributing to the overall performance.",
      "authors": [
        "Yuanzhao Zhai",
        "Han Zhang",
        "Yu Lei",
        "Yue Yu",
        "Kele Xu",
        "Dawei Feng",
        "Bo Ding",
        "Huaimin Wang"
      ],
      "year": 2023,
      "citation_count": 36,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/cdd0e94e51a02bac22ca5e94fa95daa18f36e226",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "c243df958269bf501f874ef213ba6cc904f24ea9",
      "title": "Jailbreak Open-Sourced Large Language Models via Enforced Decoding",
      "abstract": "Large Language Models (LLMs) have achieved unprecedented performance in Natural Language Generation (NLG) tasks. However, many existing studies have shown that they could be misused to generate undesired content. In response, before releasing LLMs for public access, model developers usually align those language models through Supervised Fine-Tuning (SFT) or Reinforcement Learning with Human Feedback (RLHF). Consequently, those aligned large language models refuse to generate undesired content when facing potentially harmful/unethical requests. A natural question is “ could alignment really prevent those open-sourced large language models from being mis-used to generate undesired content? ”. In this work, we provide a negative answer to this question. In particular, we show those open-sourced, aligned large language models could be easily misguided to generate undesired content without heavy computations or careful prompt designs. Our key idea is to directly manipulate the generation process of open-sourced LLMs to misguide it to generate undesired content including harmful or biased information and even private data. We evaluate our method on 4 open-sourced LLMs accessible publicly and our finding highlights the need for more advanced mitigation strategies for open-sourced LLMs.",
      "authors": [
        "Hangfan Zhang",
        "Zhimeng Guo",
        "Huaisheng Zhu",
        "Bochuan Cao",
        "Lu Lin",
        "Jinyuan Jia",
        "Jinghui Chen",
        "Di Wu"
      ],
      "year": 2024,
      "citation_count": 18,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/c243df958269bf501f874ef213ba6cc904f24ea9",
      "pdf_link": "",
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "eb6ef63df104c1b35bbc2400f00285b3414400b2",
      "title": "SimpleTIR: End-to-End Reinforcement Learning for Multi-Turn Tool-Integrated Reasoning",
      "abstract": "Large Language Models (LLMs) can significantly improve their reasoning capabilities by interacting with external tools, a paradigm known as Tool-Integrated Reasoning (TIR). However, extending TIR to multi-turn scenarios using Reinforcement Learning (RL) is often hindered by training instability and performance collapse. We identify that such instability is primarily caused by a distributional drift from external tool feedback, leading to the generation of low-probability tokens. This issue compounds over successive turns, causing catastrophic gradient norm explosions that derail the training process. To address this challenge, we introduce SimpleTIR , a plug-and-play algorithm that stabilizes multi-turn TIR training. Its core strategy is to identify and filter out trajectories containing void turns, i.e., turns that yield neither a code block nor a final answer. By removing these problematic trajectories from the policy update, SimpleTIR effectively blocks the harmful, high-magnitude gradients, thus stabilizing the learning dynamics. Extensive experiments show that SimpleTIR achieves state-of-the-art performance on challenging math reasoning benchmarks, notably elevating the AIME24 score from a text-only baseline of 22.1 to 50.5 when starting from the Qwen2.5-7B base model. Furthermore, by avoiding the constraints of supervised fine-tuning, SimpleTIR encourages the model to discover diverse and sophisticated reasoning patterns, such as self-correction and cross-validation.",
      "authors": [
        "Zhenghai Xue",
        "Longtao Zheng",
        "Qian Liu",
        "Yingru Li",
        "Xiaosen Zheng",
        "Zejun Ma",
        "Bo An"
      ],
      "year": 2025,
      "citation_count": 17,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/eb6ef63df104c1b35bbc2400f00285b3414400b2",
      "pdf_link": "",
      "venue": "",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "2044ab82dcb2c11ef660bd51d40130fe182f98d3",
      "title": "Zeroth-Order Optimization Meets Human Feedback: Provable Learning via Ranking Oracles",
      "abstract": "In this study, we delve into an emerging optimization challenge involving a black-box objective function that can only be gauged via a ranking oracle-a situation frequently encountered in real-world scenarios, especially when the function is evaluated by human judges. Such challenge is inspired from Reinforcement Learning with Human Feedback (RLHF), an approach recently employed to enhance the performance of Large Language Models (LLMs) using human guidance. We introduce ZO-RankSGD, an innovative zeroth-order optimization algorithm designed to tackle this optimization problem, accompanied by theoretical assurances. Our algorithm utilizes a novel rank-based random estimator to determine the descent direction and guarantees convergence to a stationary point. Moreover, ZO-RankSGD is readily applicable to policy optimization problems in Reinforcement Learning (RL), particularly when only ranking oracles for the episode reward are available. Last but not least, we demonstrate the effectiveness of ZO-RankSGD in a novel application: improving the quality of images generated by a diffusion generative model with human ranking feedback. Throughout experiments, we found that ZO-RankSGD can significantly enhance the detail of generated images with only a few rounds of human feedback. Overall, our work advances the field of zeroth-order optimization by addressing the problem of optimizing functions with only ranking feedback, and offers a new and effective approach for aligning Artificial Intelligence (AI) with human intentions.",
      "authors": [
        "Zhiwei Tang",
        "Dmitry Rybin",
        "Tsung-Hui Chang"
      ],
      "year": 2023,
      "citation_count": 33,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/2044ab82dcb2c11ef660bd51d40130fe182f98d3",
      "pdf_link": "",
      "venue": "International Conference on Learning Representations",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "9ddfb1583ce7f5370ace2751bb5f260fa4af1961",
      "title": "Balancing Enhancement, Harmlessness, and General Capabilities: Enhancing Conversational LLMs with Direct RLHF",
      "abstract": "In recent advancements in Conversational Large Language Models (LLMs), a concerning trend has emerged, showing that many new base LLMs experience a knowledge reduction in their foundational capabilities following Supervised Fine-Tuning (SFT). This process often leads to issues such as forgetting or a decrease in the base model's abilities. Moreover, fine-tuned models struggle to align with user preferences, inadvertently increasing the generation of toxic outputs when specifically prompted. To overcome these challenges, we adopted an innovative approach by completely bypassing SFT and directly implementing Harmless Reinforcement Learning from Human Feedback (RLHF). Our method not only preserves the base model's general capabilities but also significantly enhances its conversational abilities, while notably reducing the generation of toxic outputs. Our approach holds significant implications for fields that demand a nuanced understanding and generation of responses, such as customer service. We applied this methodology to Mistral, the most popular base model, thereby creating Mistral-Plus. Our validation across 11 general tasks demonstrates that Mistral-Plus outperforms similarly sized open-source base models and their corresponding instruct versions. Importantly, the conversational abilities of Mistral-Plus were significantly improved, indicating a substantial advancement over traditional SFT models in both safety and user preference alignment.",
      "authors": [
        "Chen Zheng",
        "Ke Sun",
        "Hang Wu",
        "Chenguang Xi",
        "Xun Zhou"
      ],
      "year": 2024,
      "citation_count": 15,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/9ddfb1583ce7f5370ace2751bb5f260fa4af1961",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "a09521156c5d2efd40e5ca8f0d1e0116f0d41ce4",
      "title": "Affective Computing in the Era of Large Language Models: A Survey from the NLP Perspective",
      "abstract": "Affective Computing (AC) integrates computer science, psychology, and cognitive science to enable machines to recognize, interpret, and simulate human emotions across domains such as social media, finance, healthcare, and education. AC commonly centers on two task families: Affective Understanding (AU) and Affective Generation (AG). While fine-tuned pre-trained language models (PLMs) have achieved solid AU performance, they often generalize poorly across tasks and remain limited for AG, especially in producing diverse, emotionally appropriate responses. The advent of Large Language Models (LLMs) (e.g., ChatGPT and LLaMA) has catalyzed a paradigm shift by offering in-context learning, broader world knowledge, and stronger sequence generation. This survey presents an NLP-oriented overview of AC in the LLM era. We (i) consolidate traditional AC tasks and preliminary LLM-based studies; (ii) review adaptation techniques that improve AU/AG, including Instruction Tuning (full and parameter-efficient methods such as LoRA, P-/Prompt-Tuning), Prompt Engineering (zero/few-shot, chain-of-thought, agent-based prompting), and Reinforcement Learning. For the latter, we summarize RL from human preferences (RLHF), verifiable/programmatic rewards (RLVR), and AI feedback (RLAIF), which provide preference- or rule-grounded optimization signals that can help steer AU/AG toward empathy, safety, and planning, achieving finer-grained or multi-objective control. To assess progress, we compile benchmarks and evaluation practices for both AU and AG. We also discuss open challenges-from ethics, data quality, and safety to robust evaluation and resource efficiency-and outline research directions. We hope this survey clarifies the landscape and offers practical guidance for building affect-aware, reliable, and responsible LLM systems.",
      "authors": [
        "Yiqun Zhang",
        "Xiaocui Yang",
        "Xingle Xu",
        "Zeran Gao",
        "Yijie Huang",
        "Shiyi Mu",
        "Shi Feng",
        "Daling Wang",
        "Yifei Zhang",
        "Kaisong Song",
        "Ge Yu"
      ],
      "year": 2024,
      "citation_count": 15,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/a09521156c5d2efd40e5ca8f0d1e0116f0d41ce4",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "208fdbf3ac095740a53230523db3828a52414da6",
      "title": "PREDILECT: Preferences Delineated with Zero-Shot Language-based Reasoning in Reinforcement Learning",
      "abstract": "Preference-based reinforcement learning (RL) has emerged as a new field in robot learning, where humans play a pivotal role in shaping robot behavior by expressing preferences on different sequences of state-action pairs. However, formulating realistic policies for robots demands responses from humans to an extensive array of queries. In this work, we approach the sample-efficiency challenge by expanding the information collected per query to contain both preferences and optional text prompting. To accomplish this, we leverage the zero-shot capabilities of a large language model (LLM) to reason from the text provided by humans. To accommodate the additional query information, we reformulate the reward learning objectives to contain flexible highlights – state-action pairs that contain relatively high information and are related to the features processed in a zero-shot fashion from a pretrained LLM. In both a simulated scenario and a user study, we reveal the effectiveness of our work by analyzing the feedback and its implications. Additionally, the collective feedback collected serves to train a robot on socially compliant trajectories in a simulated social navigation landscape. We provide video examples of the trained policies at https://sites.google.com/view/rl-predilectCCS CONCEPTS• Computing methodologies → Inverse reinforcement learning; Learning to rank.",
      "authors": [
        "Simon Holk",
        "Daniel Marta",
        "Iolanda Leite"
      ],
      "year": 2024,
      "citation_count": 14,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/208fdbf3ac095740a53230523db3828a52414da6",
      "pdf_link": "",
      "venue": "IEEE/ACM International Conference on Human-Robot Interaction",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "ba015c5d3f5b44e36363b90070bb3301d21ae57e",
      "title": "On the Safety of Open-Sourced Large Language Models: Does Alignment Really Prevent Them From Being Misused?",
      "abstract": "Large Language Models (LLMs) have achieved unprecedented performance in Natural Language Generation (NLG) tasks. However, many existing studies have shown that they could be misused to generate undesired content. In response, before releasing LLMs for public access, model developers usually align those language models through Supervised Fine-Tuning (SFT) or Reinforcement Learning with Human Feedback (RLHF). Consequently, those aligned large language models refuse to generate undesired content when facing potentially harmful/unethical requests. A natural question is\"could alignment really prevent those open-sourced large language models from being misused to generate undesired content?''. In this work, we provide a negative answer to this question. In particular, we show those open-sourced, aligned large language models could be easily misguided to generate undesired content without heavy computations or careful prompt designs. Our key idea is to directly manipulate the generation process of open-sourced LLMs to misguide it to generate undesired content including harmful or biased information and even private data. We evaluate our method on 4 open-sourced LLMs accessible publicly and our finding highlights the need for more advanced mitigation strategies for open-sourced LLMs.",
      "authors": [
        "Hangfan Zhang",
        "Zhimeng Guo",
        "Huaisheng Zhu",
        "Bochuan Cao",
        "Lu Lin",
        "Jinyuan Jia",
        "Jinghui Chen",
        "Di Wu"
      ],
      "year": 2023,
      "citation_count": 27,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/ba015c5d3f5b44e36363b90070bb3301d21ae57e",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "983c01d00102075dae128b8ef9f01abef98720b5",
      "title": "Does RLHF Scale? Exploring the Impacts From Data, Model, and Method",
      "abstract": "This study explores the scaling properties of Reinforcement Learning from Human Feedback (RLHF) in Large Language Models (LLMs). Although RLHF is considered an important step in post-training of LLMs, its scaling potential is still largely unknown. We systematically analyze key components in the RLHF framework--model size, data composition, and inference budget--and their impacts on performance. Our findings show that increasing data diversity and volume improves reward model performance, helping process-supervision models scale better. For policy training, more response samples per prompt boost performance initially but quickly plateau. And larger reward models offer modest gains in policy training. In addition, larger policy models benefit less from RLHF with a fixed reward model. Overall, RLHF scales less efficiently than pretraining, with diminishing returns from additional computational resources. Based on these observations, we propose strategies to optimize RLHF performance within computational limits.",
      "authors": [
        "Zhenyu Hou",
        "Pengfan Du",
        "Yilin Niu",
        "Zhengxiao Du",
        "Aohan Zeng",
        "Xiao Liu",
        "Minlie Huang",
        "Hongning Wang",
        "Jie Tang",
        "Yuxiao Dong"
      ],
      "year": 2024,
      "citation_count": 13,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/983c01d00102075dae128b8ef9f01abef98720b5",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "faae9de3d314e8731b0505607298fd826e3de1a7",
      "title": "Beyond Sparse Rewards: Enhancing Reinforcement Learning with Language Model Critique in Text Generation",
      "abstract": "Reinforcement learning (RL) can align language models with non-differentiable reward signals, such as human preferences. However, a major challenge arises from the sparsity of these reward signals - typically, there is only a single reward for an entire output. This sparsity of rewards can lead to inefficient and unstable learning. To address this challenge, our paper introduces an novel framework that utilizes the critique capability of Large Language Models (LLMs) to produce intermediate-step rewards during RL training. Our method involves coupling a policy model with a critic language model, which is responsible for providing comprehensive feedback of each part of the output. This feedback is then translated into token or span-level rewards that can be used to guide the RL training process. We investigate this approach under two different settings: one where the policy model is smaller and is paired with a more powerful critic model, and another where a single language model fulfills both roles. We assess our approach on three text generation tasks: sentiment control, language model detoxification, and summarization. Experimental results show that incorporating artificial intrinsic rewards significantly improve both sample efficiency and the overall performance of the policy model, supported by both automatic and human evaluation.",
      "authors": [
        "Meng Cao",
        "Lei Shu",
        "Lei Yu",
        "Yun Zhu",
        "Nevan Wichers",
        "Yinxiao Liu",
        "Lei Meng"
      ],
      "year": 2024,
      "citation_count": 12,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/faae9de3d314e8731b0505607298fd826e3de1a7",
      "pdf_link": "",
      "venue": "",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "0425c47e19b5f1fcc680967ebd6c6e7cebc0b768",
      "title": "Aligning as Debiasing: Causality-Aware Alignment via Reinforcement Learning with Interventional Feedback",
      "abstract": "Large language models (LLMs) often generate biased outputs containing offensive, toxic, or stereotypical text. Existing LLM alignment methods such as reinforcement learning from human feedback (RLHF) alleviate biases primarily based on reward signals from current model outputs without considering the source of biases. In this work, to explore how biases are formed, we revisit LLMs’ text generation from a causal perspective. We identify pretraining data and input prompts, which contain semantic correlations of textual phrases, as two confounders between LLMs and model outputs causing biases. Inspired by our causal view, we leverage the reward model in RL alignment as an instrumental variable to perform causal intervention on LLMs. Utilizing the reward difference between an initial LLM and intervened LLM as interventional feedback to guide RL finetuning, we propose Causality-Aware Alignment (CAA) for LLM debiasing. Experiments on two text generation tasks with three different alignment objectives demonstrate the advantages of our method in aligning LLMs to generate less biased and safer outputs.",
      "authors": [
        "Yu Xia",
        "Tong Yu",
        "Zhankui He",
        "Handong Zhao",
        "Julian J. McAuley",
        "Shuai Li"
      ],
      "year": 2024,
      "citation_count": 12,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/0425c47e19b5f1fcc680967ebd6c6e7cebc0b768",
      "pdf_link": "",
      "venue": "North American Chapter of the Association for Computational Linguistics",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "f782d5f569eaa4eb2d66e0cb95bdba0941dbb8b0",
      "title": "Reinforcement Learning in the Era of LLMs: What is Essential? What is needed? An RL Perspective on RLHF, Prompting, and Beyond",
      "abstract": "Recent advancements in Large Language Models (LLMs) have garnered wide attention and led to successful products such as ChatGPT and GPT-4. Their proficiency in adhering to instructions and delivering harmless, helpful, and honest (3H) responses can largely be attributed to the technique of Reinforcement Learning from Human Feedback (RLHF). In this paper, we aim to link the research in conventional RL to RL techniques used in LLM research. Demystify this technique by discussing why, when, and how RL excels. Furthermore, we explore potential future avenues that could either benefit from or contribute to RLHF research. Highlighted Takeaways: 1. RLHF is Online Inverse RL with Offline Demonstration Data. 2. RLHF $>$ SFT because Imitation Learning (and Inverse RL) $>$ Behavior Cloning (BC) by alleviating the problem of compounding error. 3. The RM step in RLHF generates a proxy of the expensive human feedback, such an insight can be generalized to other LLM tasks such as prompting evaluation and optimization where feedback is also expensive. 4. The policy learning in RLHF is more challenging than conventional problems studied in IRL due to their high action dimensionality and feedback sparsity. 5. The main superiority of PPO over off-policy value-based methods is its stability gained from (almost) on-policy data and conservative policy updates.",
      "authors": [
        "Hao Sun"
      ],
      "year": 2023,
      "citation_count": 24,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/f782d5f569eaa4eb2d66e0cb95bdba0941dbb8b0",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "40cc085a2608985b753c38dc245ac21be592ed08",
      "title": "HRLAIF: Improvements in Helpfulness and Harmlessness in Open-domain Reinforcement Learning From AI Feedback",
      "abstract": "Reinforcement Learning from AI Feedback (RLAIF) has the advantages of shorter annotation cycles and lower costs over Reinforcement Learning from Human Feedback (RLHF), making it highly efficient during the rapid strategy iteration periods of large language model (LLM) training. Using ChatGPT as a labeler to provide feedback on open-domain prompts in RLAIF training, we observe an increase in human evaluators' preference win ratio for model responses, but a decrease in evaluators' satisfaction rate. Analysis suggests that the decrease in satisfaction rate is mainly due to some responses becoming less helpful, particularly in terms of correctness and truthfulness, highlighting practical limitations of basic RLAIF. In this paper, we propose Hybrid Reinforcement Learning from AI Feedback (HRLAIF). This method enhances the accuracy of AI annotations for responses, making the model's helpfulness more robust in training process. Additionally, it employs AI for Red Teaming, further improving the model's harmlessness. Human evaluation results show that HRLAIF inherits the ability of RLAIF to enhance human preference for outcomes at a low cost while also improving the satisfaction rate of responses. Compared to the policy model before Reinforcement Learning (RL), it achieves an increase of 2.08\\% in satisfaction rate, effectively addressing the issue of a decrease of 4.58\\% in satisfaction rate after basic RLAIF.",
      "authors": [
        "Ang Li",
        "Qiugen Xiao",
        "Peng Cao",
        "Jian Tang",
        "Yi Yuan",
        "Zijie Zhao",
        "Xiaoyuan Chen",
        "Liang Zhang",
        "Xiangyang Li",
        "Kaitong Yang",
        "Weidong Guo",
        "Yukang Gan",
        "Jeffrey Xu Yu",
        "D. Wang",
        "Ying Shan"
      ],
      "year": 2024,
      "citation_count": 11,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/40cc085a2608985b753c38dc245ac21be592ed08",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "e7c9478b9dab56b6113a85d1c53723eb5d09e58f",
      "title": "ReaL: Efficient RLHF Training of Large Language Models with Parameter Reallocation",
      "abstract": "Reinforcement Learning from Human Feedback (RLHF) is a pivotal technique for empowering large language model (LLM) applications. Compared with the supervised training process of LLMs, the RLHF training process is much more sophisticated, requiring a diverse range of computation workloads with intricate dependencies between multiple LLM instances. Therefore, simply adopting the fixed parallelization strategies from supervised training for LLMs can be insufficient for RLHF and result in low training efficiency. To overcome this limitation, we propose a novel technique named parameter ReaLlocation, which dynamically adapts the parallelization strategies for different workloads during training by redistributing LLM parameters across the training cluster. Building upon this idea, we introduce ReaL, a pioneering system for efficient RLHF training. ReaL introduces the concept of an execution plan, which defines a fine-grained resource allocation and parallelization strategy particularly designed for RLHF training. Based on this concept, ReaL employs a tailored search algorithm with a lightweight run-time estimator to automatically discover an efficient execution plan for an instance of RLHF experiment. Subsequently, the runtime engine deploys the selected plan by effectively parallelizing computations and redistributing parameters. We evaluate ReaL on the LLaMA models with up to 70 billion parameters and 128 GPUs. The experimental results demonstrate that ReaL achieves speedups of up to $3.58\\times$ compared to baseline methods. Furthermore, the execution plans generated by ReaL exhibit an average of $81\\%$ performance improvement over heuristic approaches based on Megatron-LM in the long-context scenario. The source code of ReaL is publicly available at https://github.com/openpsi-project/ReaLHF .",
      "authors": [
        "Zhiyu Mei",
        "Wei Fu",
        "Kaiwei Li",
        "Guangju Wang",
        "Huanchen Zhang",
        "Yi Wu"
      ],
      "year": 2024,
      "citation_count": 11,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/e7c9478b9dab56b6113a85d1c53723eb5d09e58f",
      "pdf_link": "",
      "venue": "",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "e6fac5811e260466366f3a905076c33e252405ef",
      "title": "Optimal Design for Reward Modeling in RLHF",
      "abstract": "Reinforcement Learning from Human Feedback (RLHF) has become a popular approach to align language models (LMs) with human preferences. This method involves collecting a large dataset of human pairwise preferences across various text generations and using it to infer (implicitly or explicitly) a reward model. Numerous methods have been proposed to learn the reward model and align a LM with it. However, the costly process of collecting human preferences has received little attention and could benefit from theoretical insights. This paper addresses this issue and aims to formalize the reward training model in RLHF. We frame the selection of an effective dataset as a simple regret minimization task, using a linear contextual dueling bandit method. Given the potentially large number of arms, this approach is more coherent than the best-arm identification setting. We then propose an offline framework for solving this problem. Under appropriate assumptions - linearity of the reward model in the embedding space, and boundedness of the reward parameter - we derive bounds on the simple regret. Finally, we provide a lower bound that matches our upper bound up to constant and logarithmic terms. To our knowledge, this is the first theoretical contribution in this area to provide an offline approach as well as worst-case guarantees.",
      "authors": [
        "Antoine Scheid",
        "Etienne Boursier",
        "A. Durmus",
        "Michael I. Jordan",
        "Pierre M'enard",
        "Éric Moulines",
        "Michal Valko"
      ],
      "year": 2024,
      "citation_count": 11,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/e6fac5811e260466366f3a905076c33e252405ef",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "aece81d7dcbf2929e650a6094af63666e95a0c83",
      "title": "Equilibrate RLHF: Towards Balancing Helpfulness-Safety Trade-off in Large Language Models",
      "abstract": "Fine-tuning large language models (LLMs) based on human preferences, commonly achieved through reinforcement learning from human feedback (RLHF), has been effective in improving their performance. However, maintaining LLM safety throughout the fine-tuning process remains a significant challenge, as resolving conflicts between safety and helpfulness can be non-trivial. Typically, the safety alignment of LLM is trained on data with safety-related categories. However, our experiments find that naively increasing the scale of safety training data usually leads the LLMs to an ``overly safe'' state rather than a ``truly safe'' state, boosting the refusal rate through extensive safety-aligned data without genuinely understanding the requirements for safe responses. Such an approach can inadvertently diminish the models' helpfulness. To understand the phenomenon, we first investigate the role of safety data by categorizing them into three different groups, and observe that each group behaves differently as training data scales up. To boost the balance between safety and helpfulness, we propose an Equilibrate RLHF framework including a Fine-grained Data-centric (FDC) approach that achieves better safety alignment even with fewer training data, and an Adaptive Message-wise Alignment (AMA) approach, which selectively highlight the key segments through a gradient masking strategy. Extensive experimental results demonstrate that our approach significantly enhances the safety alignment of LLMs while balancing safety and helpfulness.",
      "authors": [
        "Yingshui Tan",
        "Yilei Jiang",
        "Yanshi Li",
        "Jiaheng Liu",
        "Xingyuan Bu",
        "Wenbo Su",
        "Xiangyu Yue",
        "Xiaoyong Zhu",
        "Bo Zheng"
      ],
      "year": 2025,
      "citation_count": 10,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/aece81d7dcbf2929e650a6094af63666e95a0c83",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "4376e282954ec59eaeca345ce4ec99219a075670",
      "title": "A Unified Pairwise Framework for RLHF: Bridging Generative Reward Modeling and Policy Optimization",
      "abstract": "Reinforcement Learning from Human Feedback (RLHF) has emerged as a important paradigm for aligning large language models (LLMs) with human preferences during post-training. This framework typically involves two stages: first, training a reward model on human preference data, followed by optimizing the language model using reinforcement learning algorithms. However, current RLHF approaches may constrained by two limitations. First, existing RLHF frameworks often rely on Bradley-Terry models to assign scalar rewards based on pairwise comparisons of individual responses. However, this approach imposes significant challenges on reward model (RM), as the inherent variability in prompt-response pairs across different contexts demands robust calibration capabilities from the RM. Second, reward models are typically initialized from generative foundation models, such as pre-trained or supervised fine-tuned models, despite the fact that reward models perform discriminative tasks, creating a mismatch. This paper introduces Pairwise-RL, a RLHF framework that addresses these challenges through a combination of generative reward modeling and a pairwise proximal policy optimization (PPO) algorithm. Pairwise-RL unifies reward model training and its application during reinforcement learning within a consistent pairwise paradigm, leveraging generative modeling techniques to enhance reward model performance and score calibration. Experimental evaluations demonstrate that Pairwise-RL outperforms traditional RLHF frameworks across both internal evaluation datasets and standard public benchmarks, underscoring its effectiveness in improving alignment and model behavior.",
      "authors": [
        "Wenyuan Xu",
        "Xiaochen Zuo",
        "Chao Xin",
        "Yu Yue",
        "Lin Yan",
        "Yong-Xu Wu"
      ],
      "year": 2025,
      "citation_count": 10,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/4376e282954ec59eaeca345ce4ec99219a075670",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "3e8b2bed3e3f78ca67be9064c3866d8fbd94bca3",
      "title": "Step-wise Adaptive Integration of Supervised Fine-tuning and Reinforcement Learning for Task-Specific LLMs",
      "abstract": "Large language models (LLMs) excel at mathematical reasoning and logical problem-solving. The current popular training paradigms primarily use supervised fine-tuning (SFT) and reinforcement learning (RL) to enhance the models'reasoning abilities. However, when using SFT or RL alone, there are respective challenges: SFT may suffer from overfitting, while RL is prone to mode collapse. The state-of-the-art methods have proposed hybrid training schemes. However, static switching faces challenges such as poor generalization across different tasks and high dependence on data quality. In response to these challenges, inspired by the curriculum learning-quiz mechanism in human reasoning cultivation, We propose SASR, a step-wise adaptive hybrid training framework that theoretically unifies SFT and RL and dynamically balances the two throughout optimization. SASR uses SFT for initial warm-up to establish basic reasoning skills, and then uses an adaptive dynamic adjustment algorithm based on gradient norm and divergence relative to the original distribution to seamlessly integrate SFT with the online RL method GRPO. By monitoring the training status of LLMs and adjusting the training process in sequence, SASR ensures a smooth transition between training schemes, maintaining core reasoning abilities while exploring different paths. Experimental results demonstrate that SASR outperforms SFT, RL, and static hybrid training methods.",
      "authors": [
        "Jack Chen",
        "Fazhong Liu",
        "Naruto Liu",
        "Yuhan Luo",
        "Erqu Qin",
        "Harry Zheng",
        "Tian Dong",
        "Haojin Zhu",
        "Yan Meng",
        "Xiao Wang"
      ],
      "year": 2025,
      "citation_count": 10,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/3e8b2bed3e3f78ca67be9064c3866d8fbd94bca3",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "85a1f32e4794b4c176f3330364bc39977a50d258",
      "title": "Aligning Neural Machine Translation Models: Human Feedback in Training and Inference",
      "abstract": "Reinforcement learning from human feedback (RLHF) is a recent technique to improve the quality of the text generated by a language model, making it closer to what humans would generate.A core ingredient in RLHF’s success in aligning and improving large language models (LLMs) is its \\textit{reward model}, trained using human feedback on model outputs. In machine translation (MT), where metrics trained from human annotations can readily be used as reward models, recent methods using \\textit{minimum Bayes risk} decoding and reranking have succeeded in improving the final quality of translation.In this study, we comprehensively explore and compare techniques for integrating quality metrics as reward models into the MT pipeline. This includes using the reward model for data filtering, during the training phase through RL, and at inference time by employing reranking techniques, and we assess the effects of combining these in a unified approach.Our experimental results, conducted across multiple translation tasks, underscore the crucial role of effective data filtering, based on estimated quality, in harnessing the full potential of RL in enhancing MT quality.Furthermore, our findings demonstrate the effectiveness of combining RL training with reranking techniques, showcasing substantial improvements in translation quality.",
      "authors": [
        "Miguel Moura Ramos",
        "Patrick Fernandes",
        "António Farinhas",
        "Andr'e F. T. Martins"
      ],
      "year": 2023,
      "citation_count": 19,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/85a1f32e4794b4c176f3330364bc39977a50d258",
      "pdf_link": "",
      "venue": "European Association for Machine Translation Conferences/Workshops",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "ee3c57d53327c5f84a8f3988f592c6e2479c1924",
      "title": "Inverse-Q*: Token Level Reinforcement Learning for Aligning Large Language Models Without Preference Data",
      "abstract": "Reinforcement Learning from Human Feedback (RLHF) has proven effective in aligning large language models with human intentions, yet it often relies on complex methodologies like Proximal Policy Optimization (PPO) that require extensive hyper-parameter tuning and present challenges in sample efficiency and stability. In this paper, we introduce Inverse-Q*, an innovative framework that transcends traditional RL methods by optimizing token-level reinforcement learning without the need for additional reward or value models. Inverse-Q* leverages direct preference optimization techniques but extends them by estimating the conditionally optimal policy directly from the model's responses, facilitating more granular and flexible policy shaping. Our approach reduces reliance on human annotation and external supervision, making it especially suitable for low-resource settings. We present extensive experimental results demonstrating that Inverse-Q* not only matches but potentially exceeds the effectiveness of PPO in terms of convergence speed and the alignment of model responses with human preferences. Our findings suggest that Inverse-Q* offers a practical and robust alternative to conventional RLHF approaches, paving the way for more efficient and adaptable model training approaches.",
      "authors": [
        "Han Xia",
        "Songyang Gao",
        "Qiming Ge",
        "Zhiheng Xi",
        "Qi Zhang",
        "Xuanjing Huang"
      ],
      "year": 2024,
      "citation_count": 9,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/ee3c57d53327c5f84a8f3988f592c6e2479c1924",
      "pdf_link": "",
      "venue": "Conference on Empirical Methods in Natural Language Processing",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "cd6b87d6f746bd572a79c4f77cc60cf6a92fc015",
      "title": "Adaptive Preference Scaling for Reinforcement Learning with Human Feedback",
      "abstract": "Reinforcement learning from human feedback (RLHF) is a prevalent approach to align AI systems with human values by learning rewards from human preference data. Due to various reasons, however, such data typically takes the form of rankings over pairs of trajectory segments, which fails to capture the varying strengths of preferences across different pairs. In this paper, we propose a novel adaptive preference loss, underpinned by distributionally robust optimization (DRO), designed to address this uncertainty in preference strength. By incorporating an adaptive scaling parameter into the loss for each pair, our method increases the flexibility of the reward function. Specifically, it assigns small scaling parameters to pairs with ambiguous preferences, leading to more comparable rewards, and large scaling parameters to those with clear preferences for more distinct rewards. Computationally, our proposed loss function is strictly convex and univariate with respect to each scaling parameter, enabling its efficient optimization through a simple second-order algorithm. Our method is versatile and can be readily adapted to various preference optimization frameworks, including direct preference optimization (DPO). Our experiments with robotic control and natural language generation with large language models (LLMs) show that our method not only improves policy performance but also aligns reward function selection more closely with policy optimization, simplifying the hyperparameter tuning process.",
      "authors": [
        "Ilgee Hong",
        "Zichong Li",
        "Alexander Bukharin",
        "Yixiao Li",
        "Haoming Jiang",
        "Tianbao Yang",
        "Tuo Zhao"
      ],
      "year": 2024,
      "citation_count": 9,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/cd6b87d6f746bd572a79c4f77cc60cf6a92fc015",
      "pdf_link": "",
      "venue": "Neural Information Processing Systems",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "5b6f58a8b0098dbc7bfa735359a2f6ae7ea4cbe6",
      "title": "A Survey On Enhancing Reinforcement Learning in Complex Environments: Insights from Human and LLM Feedback",
      "abstract": "Reinforcement learning (RL) is one of the active fields in machine learning, demonstrating remarkable potential in tackling real-world challenges. Despite its promising prospects, this methodology has encountered with issues and challenges, hindering it from achieving the best performance. In particular, these approaches lack decent performance when navigating environments and solving tasks with large observation space, often resulting in sample-inefficiency and prolonged learning times. This issue, commonly referred to as the curse of dimensionality, complicates decision-making for RL agents, necessitating a careful balance between attention and decision-making. RL agents, when augmented with human or large language models' (LLMs) feedback, may exhibit resilience and adaptability, leading to enhanced performance and accelerated learning. Such feedback, conveyed through various modalities or granularities including natural language, serves as a guide for RL agents, aiding them in discerning relevant environmental cues and optimizing decision-making processes. In this survey paper, we mainly focus on problems of two-folds: firstly, we focus on humans or an LLMs assistance, investigating the ways in which these entities may collaborate with the RL agent in order to foster optimal behavior and expedite learning; secondly, we delve into the research papers dedicated to addressing the intricacies of environments characterized by large observation space.",
      "authors": [
        "Alireza Rashidi Laleh",
        "M. N. Ahmadabadi"
      ],
      "year": 2024,
      "citation_count": 8,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/5b6f58a8b0098dbc7bfa735359a2f6ae7ea4cbe6",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "57e959b74f36a30cd62d0abd4204f08907b42e87",
      "title": "RL of Thoughts: Navigating LLM Reasoning with Inference-time Reinforcement Learning",
      "abstract": "Despite rapid advancements in large language models (LLMs), the token-level autoregressive nature constrains their complex reasoning capabilities. To enhance LLM reasoning, inference-time techniques, including Chain/Tree/Graph-of-Thought(s), successfully improve the performance, as they are fairly cost-effective by guiding reasoning through sophisticated logical structures without modifying LLMs'parameters. However, these manually predefined, task-agnostic frameworks are applied uniformly across diverse tasks, lacking adaptability. To improve this, we propose RL-of-Thoughts (RLoT), where we train a lightweight navigator model with reinforcement learning (RL) to adaptively enhance LLM reasoning at inference time. Specifically, we design five basic logic blocks from the perspective of human cognition. During the reasoning process, the trained RL navigator dynamically selects the suitable logic blocks and combines them into task-specific logical structures according to problem characteristics. Experiments across multiple reasoning benchmarks (AIME, MATH, GPQA, etc.) with multiple LLMs (GPT, Llama, Qwen, and DeepSeek) illustrate that RLoT outperforms established inference-time techniques by up to 13.4%. Remarkably, with less than 3K parameters, our RL navigator is able to make sub-10B LLMs comparable to 100B-scale counterparts. Moreover, the RL navigator demonstrates strong transferability: a model trained on one specific LLM-task pair can effectively generalize to unseen LLMs and tasks. Our code is open-source at https://anonymous.4open.science/r/RL-LLM-Reasoning-1A30 for reproducibility.",
      "authors": [
        "Qianyue Hao",
        "Sibo Li",
        "Jian Yuan",
        "Yong Li"
      ],
      "year": 2025,
      "citation_count": 8,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/57e959b74f36a30cd62d0abd4204f08907b42e87",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "0940c04de5a9f5dbea57aa0c7953e3fe4a052422",
      "title": "The Energy Loss Phenomenon in RLHF: A New Perspective on Mitigating Reward Hacking",
      "abstract": "This work identifies the Energy Loss Phenomenon in Reinforcement Learning from Human Feedback (RLHF) and its connection to reward hacking. Specifically, energy loss in the final layer of a Large Language Model (LLM) gradually increases during the RL process, with an excessive increase in energy loss characterizing reward hacking. Beyond empirical analysis, we further provide a theoretical foundation by proving that, under mild conditions, the increased energy loss reduces the upper bound of contextual relevance in LLMs, which is a critical aspect of reward hacking as the reduced contextual relevance typically indicates overfitting to reward model-favored patterns in RL. To address this issue, we propose an Energy loss-aware PPO algorithm (EPPO) which penalizes the increase in energy loss in the LLM's final layer during reward calculation to prevent excessive energy loss, thereby mitigating reward hacking. We theoretically show that EPPO can be conceptually interpreted as an entropy-regularized RL algorithm, which provides deeper insights into its effectiveness. Extensive experiments across various LLMs and tasks demonstrate the commonality of the energy loss phenomenon, as well as the effectiveness of EPPO in mitigating reward hacking and improving RLHF performance.",
      "authors": [
        "Yuchun Miao",
        "Sen Zhang",
        "Liang Ding",
        "Yuqi Zhang",
        "Lefei Zhang",
        "D. Tao"
      ],
      "year": 2025,
      "citation_count": 8,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/0940c04de5a9f5dbea57aa0c7953e3fe4a052422",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "4a081d1e1ea87ffcbf64cfe1c8a4a1da24c2c1b8",
      "title": "Dual Active Learning for Reinforcement Learning from Human Feedback",
      "abstract": "Aligning large language models (LLMs) with human preferences is critical to recent advances in generative artificial intelligence. Reinforcement learning from human feedback (RLHF) is widely applied to achieve this objective. A key step in RLHF is to learn the reward function from human feedback. However, human feedback is costly and time-consuming, making it essential to collect high-quality conversation data for human teachers to label. Additionally, different human teachers have different levels of expertise. It is thus critical to query the most appropriate teacher for their opinions. In this paper, we use offline reinforcement learning (RL) to formulate the alignment problem. Motivated by the idea of $D$-optimal design, we first propose a dual active reward learning algorithm for the simultaneous selection of conversations and teachers. Next, we apply pessimistic RL to solve the alignment problem, based on the learned reward estimator. Theoretically, we show that the reward estimator obtained through our proposed adaptive selection strategy achieves minimal generalized variance asymptotically, and prove that the sub-optimality of our pessimistic policy scales as $O(1/\\sqrt{T})$ with a given sample budget $T$. Through simulations and experiments on LLMs, we demonstrate the effectiveness of our algorithm and its superiority over state-of-the-arts.",
      "authors": [
        "Pangpang Liu",
        "Chengchun Shi",
        "Will Wei Sun"
      ],
      "year": 2024,
      "citation_count": 7,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/4a081d1e1ea87ffcbf64cfe1c8a4a1da24c2c1b8",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "58f614941629541c8c04acdb8acb9e3fb350ac5a",
      "title": "CodePMP: Scalable Preference Model Pretraining for Large Language Model Reasoning",
      "abstract": "Large language models (LLMs) have made significant progress in natural language understanding and generation, driven by scalable pretraining and advanced finetuning. However, enhancing reasoning abilities in LLMs, particularly via reinforcement learning from human feedback (RLHF), remains challenging due to the scarcity of high-quality preference data, which is labor-intensive to annotate and crucial for reward model (RM) finetuning. To alleviate this issue, we introduce CodePMP, a scalable preference model pretraining (PMP) pipeline that utilizes a large corpus of synthesized code-preference pairs from publicly available high-quality source code. CodePMP improves RM finetuning efficiency by pretraining preference models on large-scale synthesized code-preference pairs. We evaluate CodePMP on mathematical reasoning tasks (GSM8K, MATH) and logical reasoning tasks (ReClor, LogiQA2.0), consistently showing significant improvements in reasoning performance of LLMs and highlighting the importance of scalable preference model pretraining for efficient reward modeling.",
      "authors": [
        "Huimu Yu",
        "Xing Wu",
        "Weidong Yin",
        "Debing Zhang",
        "Songlin Hu"
      ],
      "year": 2024,
      "citation_count": 7,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/58f614941629541c8c04acdb8acb9e3fb350ac5a",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "54c4f894d41095b18294932c0ee8c39ffe3c0ac1",
      "title": "REvolve: Reward Evolution with Large Language Models using Human Feedback",
      "abstract": "Designing effective reward functions is crucial to training reinforcement learning (RL) algorithms. However, this design is non-trivial, even for domain experts, due to the subjective nature of certain tasks that are hard to quantify explicitly. In recent works, large language models (LLMs) have been used for reward generation from natural language task descriptions, leveraging their extensive instruction tuning and commonsense understanding of human behavior. In this work, we hypothesize that LLMs, guided by human feedback, can be used to formulate reward functions that reflect human implicit knowledge. We study this in three challenging settings -- autonomous driving, humanoid locomotion, and dexterous manipulation -- wherein notions of ``good\"behavior are tacit and hard to quantify. To this end, we introduce REvolve, a truly evolutionary framework that uses LLMs for reward design in RL. REvolve generates and refines reward functions by utilizing human feedback to guide the evolution process, effectively translating implicit human knowledge into explicit reward functions for training (deep) RL agents. Experimentally, we demonstrate that agents trained on REvolve-designed rewards outperform other state-of-the-art baselines.",
      "authors": [
        "Rishi Hazra",
        "Alkis Sygkounas",
        "A. Persson",
        "Amy Loutfi",
        "Pedro Zuidberg Dos Martires"
      ],
      "year": 2024,
      "citation_count": 7,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/54c4f894d41095b18294932c0ee8c39ffe3c0ac1",
      "pdf_link": "",
      "venue": "International Conference on Learning Representations",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "1df7dc4002ec7f8a8dfbf8329f32485f2e96ed6b",
      "title": "Online Learning from Strategic Human Feedback in LLM Fine-Tuning",
      "abstract": "Reinforcement learning from human feedback (RLHF) has become an essential step in fine-tuning large language models (LLMs) to align them with human preferences. However, human labelers are selfish and have diverse preferences. They may strategically misreport their online feedback to influence the system's aggregation towards their own preferences. Current practice simply averages labelers' feedback per time and fails to identify the most accurate human labeler, leading to linear regret $\\mathcal{O}(T)$ for $T$ time slots. To our best knowledge, we are the first to study online learning mechanisms against strategic human labelers in the LLM fine-tuning process. We formulate a new dynamic Bayesian game and dynamically adjust human labelers' weights in the preference aggregation, ensuring their truthful feedback and sublinear regret $\\mathcal{O}(T^{1/2})$. Simulation results demonstrate our mechanism's great advantages over the existing benchmark schemes.",
      "authors": [
        "Shugang Hao",
        "Lingjie Duan"
      ],
      "year": 2024,
      "citation_count": 7,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/1df7dc4002ec7f8a8dfbf8329f32485f2e96ed6b",
      "pdf_link": "",
      "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "1ab303435946a859620ca334556ca3b0e53464fc",
      "title": "MedAide: Leveraging Large Language Models for On-Premise Medical Assistance on Edge Devices",
      "abstract": "Large language models (LLMs) are revolutionizing various domains with their remarkable natural language processing (NLP) abilities. However, deploying LLMs in resource-constrained edge computing and embedded systems presents significant challenges. Another challenge lies in delivering medical assistance in remote areas with limited healthcare facilities and infrastructure. To address this, we introduce MedAide, an on-premise healthcare chatbot. It leverages tiny-LLMs integrated with LangChain, providing efficient edge-based preliminary medical diagnostics and support. MedAide employs model optimizations for minimal memory footprint and latency on embedded edge devices without server infrastructure. The training process is optimized using low-rank adaptation (LoRA). Additionally, the model is trained on diverse medical datasets, employing reinforcement learning from human feedback (RLHF) to enhance its domain-specific capabilities. The system is implemented on various consumer GPUs and Nvidia Jetson development board. MedAide achieves 77\\% accuracy in medical consultations and scores 56 in USMLE benchmark, enabling an energy-efficient healthcare assistance platform that alleviates privacy concerns due to edge-based deployment, thereby empowering the community.",
      "authors": [
        "Abdul Basit",
        "Khizar Hussain",
        "M. Hanif",
        "Muhammad Shafique"
      ],
      "year": 2024,
      "citation_count": 7,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/1ab303435946a859620ca334556ca3b0e53464fc",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "fde0ffe77186561497ce15e4faca82db11dacd64",
      "title": "Trust, But Verify: A Self-Verification Approach to Reinforcement Learning with Verifiable Rewards",
      "abstract": "Large Language Models (LLMs) show great promise in complex reasoning, with Reinforcement Learning with Verifiable Rewards (RLVR) being a key enhancement strategy. However, a prevalent issue is ``superficial self-reflection'', where models fail to robustly verify their own outputs. We introduce RISE (Reinforcing Reasoning with Self-Verification), a novel online RL framework designed to tackle this. RISE explicitly and simultaneously trains an LLM to improve both its problem-solving and self-verification abilities within a single, integrated RL process. The core mechanism involves leveraging verifiable rewards from an outcome verifier to provide on-the-fly feedback for both solution generation and self-verification tasks. In each iteration, the model generates solutions, then critiques its own on-policy generated solutions, with both trajectories contributing to the policy update. Extensive experiments on diverse mathematical reasoning benchmarks show that RISE consistently improves model's problem-solving accuracy while concurrently fostering strong self-verification skills. Our analyses highlight the advantages of online verification and the benefits of increased verification compute. Additionally, RISE models exhibit more frequent and accurate self-verification behaviors during reasoning. These advantages reinforce RISE as a flexible and effective path towards developing more robust and self-aware reasoners.",
      "authors": [
        "Xiaoyuan Liu",
        "Tian Liang",
        "Zhiwei He",
        "Jiahao Xu",
        "Wenxuan Wang",
        "Pinjia He",
        "Zhaopeng Tu",
        "Haitao Mi",
        "Dong Yu"
      ],
      "year": 2025,
      "citation_count": 7,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/fde0ffe77186561497ce15e4faca82db11dacd64",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "32fdc2cf900e1af1acc4264f312a55c1de5879d3",
      "title": "Measuring memorization in RLHF for code completion",
      "abstract": "Reinforcement learning with human feedback (RLHF) has become the dominant method to align large models to user preferences. Unlike fine-tuning, for which there are many studies regarding training data memorization, it is not clear how memorization is affected by or introduced in the RLHF alignment process. Understanding this relationship is important as real user data may be collected and used to align large models; if user data is memorized during RLHF and later regurgitated, this could raise privacy concerns. In addition to RLHF, other methods such as Direct Preference Optimization (DPO) and $\\Psi$PO have gained popularity for learning directly from human preferences, removing the need for optimizing intermediary reward models with reinforcement learning. In this work, we analyze how training data memorization can surface and propagate through each phase of RLHF and direct preference learning. We focus our study on code completion models, as code completion is one of the most popular use cases for large language models. We find that RLHF significantly decreases the chance that data used for reward modeling and reinforcement learning is memorized in comparison to directly fine-tuning on this data, but that examples already memorized during the fine-tuning stage of RLHF, will, in the majority of cases, remain memorized after RLHF. In contrast, we find that aligning by learning directly from human preference data via a special case of $\\Psi$PO, Identity Preference Optimization (IPO), increases the likelihood that training data is regurgitated compared to RLHF. Our work suggests that RLHF, as opposed to direct preference learning, is a safer way to mitigate the risk of regurgitating sensitive preference data when aligning large language models. We find our conclusions are robust across multiple code completion datasets, tasks, and model scales.",
      "authors": [
        "Aneesh Pappu",
        "Billy Porter",
        "Ilia Shumailov",
        "Jamie Hayes"
      ],
      "year": 2024,
      "citation_count": 7,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/32fdc2cf900e1af1acc4264f312a55c1de5879d3",
      "pdf_link": "",
      "venue": "International Conference on Learning Representations",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "85c961d5b3fea95b48f94c0461782e887a8b3b0f",
      "title": "Zeroth-Order Policy Gradient for Reinforcement Learning from Human Feedback without Reward Inference",
      "abstract": "Reward inference (learning a reward model from human preferences) is a critical intermediate step in the Reinforcement Learning from Human Feedback (RLHF) pipeline for fine-tuning Large Language Models (LLMs). In practice, RLHF faces fundamental challenges such as distribution shift, reward model overfitting, and problem misspecification. An alternative approach is direct policy optimization without reward inference, such as Direct Preference Optimization (DPO), which provides a much simpler pipeline and has shown empirical success in LLM applications. However, DPO utilizes the closed-form expression between the optimal policy and the reward function, which is only suitable under the bandit setting or deterministic MDPs. This paper develops two RLHF algorithms without reward inference for general RL problems beyond bandits and deterministic MDPs, and general preference models beyond the Bradley-Terry model. The key idea is to estimate the local value function difference from human preferences and then approximate the policy gradient with a zeroth-order gradient approximator. For both algorithms, we establish polynomial convergence rates in terms of the number of policy gradient iterations, the number of trajectory samples, and human preference queries per iteration. Numerical experiments in stochastic environments validate the performance of our proposed algorithms, outperforming popular RLHF baselines such as DPO and PPO. Our paper shows there exist provably efficient methods to solve general RLHF problems without reward inference.",
      "authors": [
        "Qining Zhang",
        "Lei Ying"
      ],
      "year": 2024,
      "citation_count": 6,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/85c961d5b3fea95b48f94c0461782e887a8b3b0f",
      "pdf_link": "",
      "venue": "International Conference on Learning Representations",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "2530e6ecbd0198012bb8ee4359acb9241cefec95",
      "title": "Seeing Eye to AI: Human Alignment via Gaze-Based Response Rewards for Large Language Models",
      "abstract": "Advancements in Natural Language Processing (NLP), have led to the emergence of Large Language Models (LLMs) such as GPT, Llama, Claude, and Gemini, which excel across a range of tasks but require extensive fine-tuning to align their outputs with human expectations. A widely used method for achieving this alignment is Reinforcement Learning from Human Feedback (RLHF), which, despite its success, faces challenges in accurately modelling human preferences. In this paper, we introduce GazeReward, a novel framework that integrates implicit feedback -- and specifically eye-tracking (ET) data -- into the Reward Model (RM). In addition, we explore how ET-based features can provide insights into user preferences. Through ablation studies we test our framework with different integration methods, LLMs, and ET generator models, demonstrating that our approach significantly improves the accuracy of the RM on established human preference datasets. This work advances the ongoing discussion on optimizing AI alignment with human values, exploring the potential of cognitive data for shaping future NLP research.",
      "authors": [
        "Ángela López-Cardona",
        "Carlos Segura",
        "Alexandros Karatzoglou",
        "Sergi Abadal",
        "Ioannis Arapakis"
      ],
      "year": 2024,
      "citation_count": 6,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/2530e6ecbd0198012bb8ee4359acb9241cefec95",
      "pdf_link": "",
      "venue": "International Conference on Learning Representations",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "ff5b0cc250d93b97fe60e1b0c2048708d6875595",
      "title": "Helpful, harmless, honest? Sociotechnical limits of AI alignment and safety through Reinforcement Learning from Human Feedback",
      "abstract": "This paper critically evaluates the attempts to align Artificial Intelligence (AI) systems, especially Large Language Models (LLMs), with human values and intentions through Reinforcement Learning from Feedback methods, involving either human feedback (RLHF) or AI feedback (RLAIF). Specifically, we show the shortcomings of the broadly pursued alignment goals of honesty, harmlessness, and helpfulness. Through a multidisciplinary sociotechnical critique, we examine both the theoretical underpinnings and practical implementations of RLHF techniques, revealing significant limitations in their approach to capturing the complexities of human ethics, and contributing to AI safety. We highlight tensions inherent in the goals of RLHF, as captured in the HHH principle (helpful, harmless and honest). In addition, we discuss ethically-relevant issues that tend to be neglected in discussions about alignment and RLHF, among which the trade-offs between user-friendliness and deception, flexibility and interpretability, and system safety. We offer an alternative vision for AI safety and ethics which positions RLHF approaches within a broader context of comprehensive design across institutions, processes and technological systems, and suggest the establishment of AI safety as a sociotechnical discipline that is open to the normative and political dimensions of artificial intelligence.",
      "authors": [
        "Adam Dahlgren Lindström",
        "Leila Methnani",
        "Lea Krause",
        "Petter Ericson",
        "Íñigo Martinez de Rituerto de Troya",
        "Dimitri Coelho Mollo",
        "Roel Dobbe"
      ],
      "year": 2025,
      "citation_count": 6,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/ff5b0cc250d93b97fe60e1b0c2048708d6875595",
      "pdf_link": "",
      "venue": "Ethics and Information Technology",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "77f0687571a213c784f0901a821f22b2a03f3ddd",
      "title": "Self-Play with Adversarial Critic: Provable and Scalable Offline Alignment for Language Models",
      "abstract": "This work studies the challenge of aligning large language models (LLMs) with offline preference data. We focus on alignment by Reinforcement Learning from Human Feedback (RLHF) in particular. While popular preference optimization methods exhibit good empirical performance in practice, they are not theoretically guaranteed to converge to the optimal policy and can provably fail when the data coverage is sparse by classical offline reinforcement learning (RL) results. On the other hand, a recent line of work has focused on theoretically motivated preference optimization methods with provable guarantees, but these are not computationally efficient for large-scale applications like LLM alignment. To bridge this gap, we propose SPAC, a new offline preference optimization method with self-play, inspired by the on-average pessimism technique from the offline RL literature, to be the first provable and scalable approach to LLM alignment. We both provide theoretical analysis for its convergence under single-policy concentrability for the general function approximation setting and demonstrate its competitive empirical performance for LLM alignment on a 7B Mistral model with Open LLM Leaderboard evaluations.",
      "authors": [
        "Xiang Ji",
        "Sanjeev Kulkarni",
        "Mengdi Wang",
        "Tengyang Xie"
      ],
      "year": 2024,
      "citation_count": 6,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/77f0687571a213c784f0901a821f22b2a03f3ddd",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "612ec1fbb54cfe61de62bc5922346d20f15f5023",
      "title": "Parameter Efficient Reinforcement Learning from Human Feedback",
      "abstract": "While Reinforcement Learning from Human Feedback (RLHF) effectively aligns pretrained Large Language and Vision-Language Models (LLMs, and VLMs) with human preferences, its computational cost and complexity hamper its wider adoption. To alleviate some of the computational burden of fine-tuning, parameter efficient methods, like LoRA were introduced. In this work, we empirically evaluate the setup of Parameter Efficient Reinforcement Learning from Human Feedback (PE-RLHF) that leverages LoRA fine-tuning for Reward Modeling, and Reinforcement Learning. We benchmark the PE-RLHF setup on six diverse datasets spanning summarization, harmless/helpful response generation, UI automation, and visual question answering in terms of effectiveness of the trained models, and the training resources required. Our findings show, for the first time, that PE-RLHF achieves comparable performance to RLHF, while significantly reducing training time (up to 90% faster for reward models, and 30% faster for RL), and memory footprint (up to 50% reduction for reward models, and 27% for RL). We provide comprehensive ablations across LoRA ranks, and model sizes for both reward modeling and reinforcement learning. By mitigating the computational burden associated with RLHF, we push for a broader adoption of PE-RLHF as an alignment technique for LLMs and VLMs.",
      "authors": [
        "Hakim Sidahmed",
        "Samrat Phatale",
        "Alex Hutcheson",
        "Zhuonan Lin",
        "Zhan Chen",
        "Zac Yu",
        "Jarvis Jin",
        "Simral Chaudhary",
        "Roman Komarytsia",
        "Christiane Ahlheim",
        "Yonghao Zhu",
        "Bowen Li",
        "Saravanan Ganesh",
        "Bill Byrne",
        "Jessica Hoffmann",
        "Hassan Mansoor",
        "Wei Li",
        "Abhinav Rastogi",
        "Lucas Dixon"
      ],
      "year": 2024,
      "citation_count": 5,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/612ec1fbb54cfe61de62bc5922346d20f15f5023",
      "pdf_link": "",
      "venue": "",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "32608b3b06793a9b453fa742756b34c82afdb9d7",
      "title": "ESRL: Efficient Sampling-based Reinforcement Learning for Sequence Generation",
      "abstract": "Applying Reinforcement Learning (RL) to sequence generation models enables the direct optimization of long-term rewards (e.g., BLEU and human feedback), but typically requires large-scale sampling over a space of action sequences. This is a computational challenge as presented by the practice of sequence generation problems, such as machine translation, where we often deal with a large action space (e.g., a vocabulary) and a long action sequence (e.g., a translation). In this work, we introduce two-stage sampling and dynamic sampling approaches to improve the sampling efficiency during training sequence generation models via RL. We experiment with our approaches on the traditional sequence generation tasks, including machine translation and abstractive summarization. Furthermore, we evaluate our approaches in RL from human feedback (RLHF) through training a large language model using the reward model. Experimental results show that the efficient sampling-based RL, referred to as ESRL, can outperform all baselines in terms of both training efficiency and memory consumption. Notably, ESRL yields consistent performance gains over the strong REINFORCE, minimum risk training, and proximal policy optimization methods. The code is available at https://github.com/wangclnlp/DeepSpeed-Chat-Extension/examples/esrl.",
      "authors": [
        "Chenglong Wang",
        "Hang Zhou",
        "Yimin Hu",
        "Yi Huo",
        "Bei Li",
        "Tongran Liu",
        "Tong Xiao",
        "Jingbo Zhu"
      ],
      "year": 2023,
      "citation_count": 10,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/32608b3b06793a9b453fa742756b34c82afdb9d7",
      "pdf_link": "",
      "venue": "AAAI Conference on Artificial Intelligence",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "103436cbc7509b306c2fe82e62c9e63d29064c95",
      "title": "Hallucination Reduction and Optimization for Large Language Model-Based Autonomous Driving",
      "abstract": "Large language models (LLMs) are widely integrated into autonomous driving systems to enhance their operational intelligence and responsiveness and improve self-driving vehicles’ overall performance. Despite these advances, LLMs still struggle between hallucinations—when models either misinterpret the environment or generate imaginary parts for downstream use cases—and taxing computational overhead that relegates their performance to strictly non-real-time operations. These are essential problems to solve to make autonomous driving as safe and efficient as possible. This work is thus focused on symmetrical trade-offs between the reduction of hallucination and optimization, leading to a framework for these two combined and at least specifically motivated by these limitations. This framework intends to generate a symmetry of mapping between real and virtual worlds. It helps in minimizing hallucinations and optimizing computational resource consumption reasonably. In autonomous driving tasks, we use multimodal LLMs that combine an image-encoding Visual Transformer (ViT) and a decoding GPT-2 with responses generated by the powerful new sequence generator from OpenAI known as GPT4. Our hallucination reduction and optimization framework leverages iterative refinement loops, RLHF—reinforcement learning from human feedback (RLHF)—along with symmetric performance metrics, e.g., BLEU, ROUGE, and CIDEr similarity scores between machine-generated answers specific to other human reference answers. This ensures that improvements in model accuracy are not overused to the detriment of increased computational overhead. Experimental results show a twofold improvement in decision-maker error rate and processing efficiency, resulting in an overall decrease of 30% for the model and a 25% improvement in processing efficiency across diverse driving scenarios. Not only does this symmetrical approach reduce hallucination, but it also better aligns the virtual and real-world representations.",
      "authors": [
        "Jue Wang"
      ],
      "year": 2024,
      "citation_count": 5,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/103436cbc7509b306c2fe82e62c9e63d29064c95",
      "pdf_link": "",
      "venue": "Symmetry",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "ba7fe3757a5343e73b7961b29fe5d65dbb0ef971",
      "title": "Solver-Informed RL: Grounding Large Language Models for Authentic Optimization Modeling",
      "abstract": "Optimization modeling is fundamental to decision-making across diverse domains. Despite progress in automating optimization formulation from natural language descriptions, Large Language Models (LLMs) often struggle to generate formally correct and usable models against hallucinations, posing a challenge for reliable automation. Inspired by the success of Reinforcement Learning (RL) in enhancing Large Reasoning Models, we present Solver-Informed Reinforcement Learning (SIRL), a novel framework that significantly improves the authenticity of LLMs for optimization modeling using Reinforcement Learning with Verifiable Reward by leveraging external optimization solvers as verifiers. These verifiers automatically assess the executable code and the instance-level mathematical model represented by the associated LP file, yielding precise and comprehensive feedback signals -- including syntax, feasibility, and solution quality, serving as direct rewards for the RL process. This automated verification process, particularly from classic optimization solvers, also underpins our instance-enhanced self-consistency method to synthesize high-quality training data. Extensive experiments on diverse public benchmarks demonstrate that SIRL achieves state-of-the-art performance, substantially outperforming existing methods in generating accurate and executable optimization models. Our code is publicly available at https://github.com/Cardinal-Operations/SIRL.",
      "authors": [
        "Yitian Chen",
        "Jingfan Xia",
        "Siyu Shao",
        "Dongdong Ge",
        "Yinyu Ye"
      ],
      "year": 2025,
      "citation_count": 5,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/ba7fe3757a5343e73b7961b29fe5d65dbb0ef971",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "c9e4efa58fd42a07da27ae70254981715cc257d5",
      "title": "Mitigating Reward Over-Optimization in RLHF via Behavior-Supported Regularization",
      "abstract": "Reinforcement learning from human feedback (RLHF) is an effective method for aligning large language models (LLMs) with human values. However, reward over-optimization remains an open challenge leading to discrepancies between the performance of LLMs under the reward model and the true human objectives. A primary contributor to reward over-optimization is the extrapolation error that arises when the reward model evaluates out-of-distribution (OOD) responses. However, current methods still fail to prevent the increasing frequency of OOD response generation during the reinforcement learning (RL) process and are not effective at handling extrapolation errors from OOD responses. In this work, we propose the Behavior-Supported Policy Optimization (BSPO) method to mitigate the reward over-optimization issue. Specifically, we define behavior policy as the next token distribution of the reward training dataset to model the in-distribution (ID) region of the reward model. Building on this, we introduce the behavior-supported Bellman operator to regularize the value function, penalizing all OOD values without impacting the ID ones. Consequently, BSPO reduces the generation of OOD responses during the RL process, thereby avoiding overestimation caused by the reward model's extrapolation errors. Theoretically, we prove that BSPO guarantees a monotonic improvement of the supported policy until convergence to the optimal behavior-supported policy. Empirical results from extensive experiments show that BSPO outperforms baselines in preventing reward over-optimization due to OOD evaluation and finding the optimal ID policy.",
      "authors": [
        "Juntao Dai",
        "Taiye Chen",
        "Yaodong Yang",
        "Qian Zheng",
        "Gang Pan"
      ],
      "year": 2025,
      "citation_count": 5,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/c9e4efa58fd42a07da27ae70254981715cc257d5",
      "pdf_link": "",
      "venue": "International Conference on Learning Representations",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "33c611e6b1c071dee7a928b5263e5baf3b23ead6",
      "title": "Privately Aligning Language Models with Reinforcement Learning",
      "abstract": "Positioned between pre-training and user deployment, aligning large language models (LLMs) through reinforcement learning (RL) has emerged as a prevailing strategy for training instruction following-models such as ChatGPT. In this work, we initiate the study of privacy-preserving alignment of LLMs through Differential Privacy (DP) in conjunction with RL. Following the influential work of Ziegler et al. (2020), we study two dominant paradigms: (i) alignment via RL without human in the loop (e.g., positive review generation) and (ii) alignment via RL from human feedback (RLHF) (e.g., summarization in a human-preferred way). We give a new DP framework to achieve alignment via RL, and prove its correctness. Our experimental results validate the effectiveness of our approach, offering competitive utility while ensuring strong privacy protections.",
      "authors": [
        "Fan Wu",
        "Huseyin A. Inan",
        "A. Backurs",
        "Varun Chandrasekaran",
        "Janardhan Kulkarni",
        "Robert Sim"
      ],
      "year": 2023,
      "citation_count": 9,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/33c611e6b1c071dee7a928b5263e5baf3b23ead6",
      "pdf_link": "",
      "venue": "International Conference on Learning Representations",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "adc8c71591ee5b043447a7d7db8ae09a8a9f1251",
      "title": "Interpretable Preferences via Multi-Objective Reward Modeling and Mixture-of-Experts",
      "abstract": "Reinforcement learning from human feedback (RLHF) has emerged as the primary method for aligning large language models (LLMs) with human preferences. The RLHF process typically starts by training a reward model (RM) using human preference data. Conventional RMs are trained on pairwise responses to the same user request, with relative ratings indicating which response humans prefer. The trained RM serves as a proxy for human preferences. However, due to the black-box nature of RMs, their outputs lack interpretability, as humans cannot intuitively understand why an RM thinks a response is good or not. As RMs act as human preference proxies, we believe they should be human-interpretable to ensure that their internal decision processes are consistent with human preferences and to prevent reward hacking in LLM alignment. To build RMs with interpretable preferences, we propose a two-stage approach: i) train an Absolute-Rating Multi-Objective Reward Model (ArmoRM) with multi-dimensional absolute-rating data, each dimension corresponding to a human-interpretable objective (e.g., honesty, verbosity, safety); ii) employ a Mixture-of-Experts (MoE) strategy with a gating network that automatically selects the most suitable reward objectives based on the context. We efficiently trained an ArmoRM with Llama-3 8B and a gating network consisting of a shallow MLP on top of the ArmoRM. Our trained model, ArmoRM-Llama3-8B, obtains state-of-the-art performance on RewardBench, a benchmark evaluating RMs for language modeling. Notably, the performance of our model surpasses the LLM-as-a-judge method with GPT-4 judges by a margin, and approaches the performance of the much larger Nemotron-4 340B reward model.",
      "authors": [
        "Haoxiang Wang",
        "Wei Xiong",
        "Tengyang Xie",
        "Han Zhao",
        "Tong Zhang"
      ],
      "year": 2024,
      "citation_count": 234,
      "layer": 1,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/adc8c71591ee5b043447a7d7db8ae09a8a9f1251",
      "pdf_link": "",
      "venue": "Conference on Empirical Methods in Natural Language Processing",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "7d4ea31ec7a6a693dc15f69ac464b1d01d3e5745",
      "title": "Understanding the performance gap between online and offline alignment algorithms",
      "abstract": "Reinforcement learning from human feedback (RLHF) is the canonical framework for large language model alignment. However, rising popularity in offline alignment algorithms challenge the need for on-policy sampling in RLHF. Within the context of reward over-optimization, we start with an opening set of experiments that demonstrate the clear advantage of online methods over offline methods. This prompts us to investigate the causes to the performance discrepancy through a series of carefully designed experimental ablations. We show empirically that hypotheses such as offline data coverage and data quality by itself cannot convincingly explain the performance difference. We also find that while offline algorithms train policy to become good at pairwise classification, it is worse at generations; in the meantime the policies trained by online algorithms are good at generations while worse at pairwise classification. This hints at a unique interplay between discriminative and generative capabilities, which is greatly impacted by the sampling process. Lastly, we observe that the performance discrepancy persists for both contrastive and non-contrastive loss functions, and appears not to be addressed by simply scaling up policy networks. Taken together, our study sheds light on the pivotal role of on-policy sampling in AI alignment, and hints at certain fundamental challenges of offline alignment algorithms.",
      "authors": [
        "Yunhao Tang",
        "Daniel Guo",
        "Zeyu Zheng",
        "Daniele Calandriello",
        "Yuan Cao",
        "Eugene Tarassov",
        "Rémi Munos",
        "B. '. Pires",
        "Michal Valko",
        "Yong Cheng",
        "Will Dabney"
      ],
      "year": 2024,
      "citation_count": 84,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/7d4ea31ec7a6a693dc15f69ac464b1d01d3e5745",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "d084517f14ee247883de0f4dd58bb923e418157d",
      "title": "LLaMA-Berry: Pairwise Optimization for O1-like Olympiad-Level Mathematical Reasoning",
      "abstract": "This paper presents an advanced mathematical problem-solving framework, LLaMA-Berry, for enhancing the mathematical reasoning ability of Large Language Models (LLMs). The framework combines Monte Carlo Tree Search (MCTS) with iterative Self-Refine to optimize the reasoning path and utilizes a pairwise reward model to evaluate different paths globally. By leveraging the self-critic and rewriting capabilities of LLMs, Self-Refine applied to MCTS (SR-MCTS) overcomes the inefficiencies and limitations of conventional step-wise and greedy search algorithms by fostering a more efficient exploration of solution spaces. Pairwise Preference Reward Model~(PPRM), inspired by Reinforcement Learning from Human Feedback (RLHF), is then used to model pairwise preferences between solutions, utilizing an Enhanced Borda Count (EBC) method to synthesize these preferences into a global ranking score to find better answers. This approach addresses the challenges of scoring variability and non-independent distributions in mathematical reasoning tasks. The framework has been tested on general and advanced benchmarks, showing superior performance in terms of search efficiency and problem-solving capability compared to existing methods like ToT and rStar, particularly in complex Olympiad-level benchmarks, including GPQA, AIME24 and AMC23.",
      "authors": [
        "Di Zhang",
        "Jianbo Wu",
        "Jingdi Lei",
        "Tong Che",
        "Jiatong Li",
        "Tong Xie",
        "Xiaoshui Huang",
        "Shufei Zhang",
        "Marco Pavone",
        "Yuqiang Li",
        "Wanli Ouyang",
        "Dongzhan Zhou"
      ],
      "year": 2024,
      "citation_count": 73,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/d084517f14ee247883de0f4dd58bb923e418157d",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "f21d0177e9374bb8579c1d9c71319f212f62b3d5",
      "title": "InferAligner: Inference-Time Alignment for Harmlessness through Cross-Model Guidance",
      "abstract": "As large language models (LLMs) rapidly evolve, they are increasingly being customized through fine-tuning to suit the specific needs of various applications. A critical aspect of this advancement is the alignment process, which ensures that these models perform tasks in ways that align with human values and expectations. Current alignment methods, such as direct preference optimization (DPO) and reinforcement learning from human feedback (RLHF), focus primarily on alignment during training phase. However, these methods often involve complex and resource-intensive training processes, posing significant challenge for their implementation. Therefore, we propose InferAligner, a simple yet effective method for harmlessness alignment during inference phase. InferAligner decouples harmlessness from helpfulness. During the training phase, it focuses solely on enhancing the target model’s capabilities on downstream tasks. In the inference phase, it utilizes safety steering vectors extracted from the aligned model to guide the target model towards harmlessness alignment. Experimental results show that our method can be very effectively applied to domain-specific models in finance, medicine, and mathematics, as well as to multimodal large language models (MLLMs) such as LLaVA. It significantly diminishes the attack success rate (ASR) of both harmful instructions and jailbreak instructions, while maintaining almost unchanged performance in downstream tasks.",
      "authors": [
        "Pengyu Wang",
        "Dong Zhang",
        "Linyang Li",
        "Chenkun Tan",
        "Xinghao Wang",
        "Ke Ren",
        "Botian Jiang",
        "Xipeng Qiu"
      ],
      "year": 2024,
      "citation_count": 59,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/f21d0177e9374bb8579c1d9c71319f212f62b3d5",
      "pdf_link": "",
      "venue": "Conference on Empirical Methods in Natural Language Processing",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "830c277b2992f59ec2f21982e245bd1e17dd85ca",
      "title": "Step-level Value Preference Optimization for Mathematical Reasoning",
      "abstract": "Direct Preference Optimization (DPO) using an implicit reward model has proven to be an effective alternative to reinforcement learning from human feedback (RLHF) for fine-tuning preference aligned large language models (LLMs). However, the overall preference annotations of responses do not fully capture the fine-grained quality of model outputs in complex multi-step reasoning tasks, such as mathematical reasoning. To address this limitation, we introduce a novel algorithm called Step-level Value Preference Optimization (SVPO). Our approach employs Monte Carlo Tree Search (MCTS) to automatically annotate step-level preferences for multi-step reasoning. Furthermore, from the perspective of learning-to-rank, we train an explicit value model to replicate the behavior of the implicit reward model, complementing standard preference optimization. This value model enables the LLM to generate higher reward responses with minimal cost during inference. Experimental results demonstrate that our method achieves state-of-the-art performance on both in-domain and out-of-domain mathematical reasoning benchmarks. Our code is available at \\url{https://github.com/MARIO-Math-Reasoning/Super_MARIO}.",
      "authors": [
        "Guoxin Chen",
        "Minpeng Liao",
        "Chengxi Li",
        "Kai Fan"
      ],
      "year": 2024,
      "citation_count": 54,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/830c277b2992f59ec2f21982e245bd1e17dd85ca",
      "pdf_link": "",
      "venue": "Conference on Empirical Methods in Natural Language Processing",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "44162aa2763c88a384d9c51d60eafcc59277a1c9",
      "title": "Decoding-time Realignment of Language Models",
      "abstract": "Aligning language models with human preferences is crucial for reducing errors and biases in these models. Alignment techniques, such as reinforcement learning from human feedback (RLHF), are typically cast as optimizing a tradeoff between human preference rewards and a proximity regularization term that encourages staying close to the unaligned model. Selecting an appropriate level of regularization is critical: insufficient regularization can lead to reduced model capabilities due to reward hacking, whereas excessive regularization hinders alignment. Traditional methods for finding the optimal regularization level require retraining multiple models with varying regularization strengths. This process, however, is resource-intensive, especially for large models. To address this challenge, we propose decoding-time realignment (DeRa), a simple method to explore and evaluate different regularization strengths in aligned models without retraining. DeRa enables control over the degree of alignment, allowing users to smoothly transition between unaligned and aligned models. It also enhances the efficiency of hyperparameter tuning by enabling the identification of effective regularization strengths using a validation dataset.",
      "authors": [
        "Tianlin Liu",
        "Shangmin Guo",
        "Leonardo Bianco",
        "Daniele Calandriello",
        "Quentin Berthet",
        "Felipe Llinares-López",
        "Jessica Hoffmann",
        "Lucas Dixon",
        "Michal Valko",
        "Mathieu Blondel"
      ],
      "year": 2024,
      "citation_count": 53,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/44162aa2763c88a384d9c51d60eafcc59277a1c9",
      "pdf_link": "",
      "venue": "International Conference on Machine Learning",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "30ef6a82ac5aa80f2eea02aebeee2b98ca8ba290",
      "title": "Self-Exploring Language Models: Active Preference Elicitation for Online Alignment",
      "abstract": "Preference optimization, particularly through Reinforcement Learning from Human Feedback (RLHF), has achieved significant success in aligning Large Language Models (LLMs) to adhere to human intentions. Unlike offline alignment with a fixed dataset, online feedback collection from humans or AI on model generations typically leads to more capable reward models and better-aligned LLMs through an iterative process. However, achieving a globally accurate reward model requires systematic exploration to generate diverse responses that span the vast space of natural language. Random sampling from standard reward-maximizing LLMs alone is insufficient to fulfill this requirement. To address this issue, we propose a bilevel objective optimistically biased towards potentially high-reward responses to actively explore out-of-distribution regions. By solving the inner-level problem with the reparameterized reward function, the resulting algorithm, named Self-Exploring Language Models (SELM), eliminates the need for a separate RM and iteratively updates the LLM with a straightforward objective. Compared to Direct Preference Optimization (DPO), the SELM objective reduces indiscriminate favor of unseen extrapolations and enhances exploration efficiency. Our experimental results demonstrate that when fine-tuned on Zephyr-7B-SFT and Llama-3-8B-Instruct models, SELM significantly boosts the performance on instruction-following benchmarks such as MT-Bench and AlpacaEval 2.0, as well as various standard academic benchmarks in different settings. Our code and models are available at https://github.com/shenao-zhang/SELM.",
      "authors": [
        "Shenao Zhang",
        "Donghan Yu",
        "Hiteshi Sharma",
        "Zhihan Liu",
        "Ziyi Yang",
        "Shuohang Wang",
        "Hany Hassan",
        "Zhaoran Wang"
      ],
      "year": 2024,
      "citation_count": 43,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/30ef6a82ac5aa80f2eea02aebeee2b98ca8ba290",
      "pdf_link": "",
      "venue": "Trans. Mach. Learn. Res.",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "613a32f18388958cc60dbb906d87fc7f206c0e66",
      "title": "Self-Play Fine-Tuning of Diffusion Models for Text-to-Image Generation",
      "abstract": "Fine-tuning Diffusion Models remains an underexplored frontier in generative artificial intelligence (GenAI), especially when compared with the remarkable progress made in fine-tuning Large Language Models (LLMs). While cutting-edge diffusion models such as Stable Diffusion (SD) and SDXL rely on supervised fine-tuning, their performance inevitably plateaus after seeing a certain volume of data. Recently, reinforcement learning (RL) has been employed to fine-tune diffusion models with human preference data, but it requires at least two images (\"winner\"and\"loser\"images) for each text prompt. In this paper, we introduce an innovative technique called self-play fine-tuning for diffusion models (SPIN-Diffusion), where the diffusion model engages in competition with its earlier versions, facilitating an iterative self-improvement process. Our approach offers an alternative to conventional supervised fine-tuning and RL strategies, significantly improving both model performance and alignment. Our experiments on the Pick-a-Pic dataset reveal that SPIN-Diffusion outperforms the existing supervised fine-tuning method in aspects of human preference alignment and visual appeal right from its first iteration. By the second iteration, it exceeds the performance of RLHF-based methods across all metrics, achieving these results with less data.",
      "authors": [
        "Huizhuo Yuan",
        "Zixiang Chen",
        "Kaixuan Ji",
        "Quanquan Gu"
      ],
      "year": 2024,
      "citation_count": 43,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/613a32f18388958cc60dbb906d87fc7f206c0e66",
      "pdf_link": "",
      "venue": "Neural Information Processing Systems",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "c08fa8d84104ec1a8304f75b72bed411100aaf5c",
      "title": "OpenThinkIMG: Learning to Think with Images via Visual Tool Reinforcement Learning",
      "abstract": "While humans can flexibly leverage interactive visual cognition for complex problem-solving, enabling Large Vision-Language Models (LVLMs) to learn similarly adaptive behaviors with visual tools remains challenging. A significant hurdle is the current lack of standardized infrastructure, which hinders integrating diverse tools, generating rich interaction data, and training robust agents effectively. To address these gaps, we introduce OpenThinkIMG, the first open-source, comprehensive end-to-end framework for tool-augmented LVLMs. It features standardized vision tool interfaces, scalable trajectory generation for policy initialization, and a flexible training environment. Furthermore, considering supervised fine-tuning (SFT) on static demonstrations offers limited policy generalization for dynamic tool invocation, we propose a novel reinforcement learning (RL) framework V-ToolRL to train LVLMs to learn adaptive policies for invoking external vision tools. V-ToolRL enables LVLMs to autonomously discover optimal tool-usage strategies by directly optimizing for task success using feedback from tool interactions. We empirically validate V-ToolRL on challenging chart reasoning tasks. Our RL-trained agent, built upon a Qwen2-VL-2B, significantly outperforms its SFT-initialized counterpart (+28.83 points) and surpasses established supervised tool-learning baselines like Taco and CogCom by an average of +12.7 points. Notably, it also surpasses prominent closed-source models like GPT-4.1 by +8.68 accuracy points. We hope OpenThinkIMG can serve as a foundational framework for advancing dynamic, tool-augmented visual reasoning, helping the community develop AI agents that can genuinely\"think with images\".",
      "authors": [
        "Zhao-yu Su",
        "Linjie Li",
        "Mingyang Song",
        "Yunzhuo Hao",
        "Zhengyuan Yang",
        "Jun Zhang",
        "Guanjie Chen",
        "Jiawei Gu",
        "Juntao Li",
        "Xiaoye Qu",
        "Yu Cheng"
      ],
      "year": 2025,
      "citation_count": 36,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/c08fa8d84104ec1a8304f75b72bed411100aaf5c",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "d3dd08e86a6c9a175385a3b4d282c5c754f4f51d",
      "title": "Bootstrapping Language Models with DPO Implicit Rewards",
      "abstract": "Human alignment in large language models (LLMs) is an active area of research. A recent groundbreaking work, direct preference optimization (DPO), has greatly simplified the process from past work in reinforcement learning from human feedback (RLHF) by bypassing the reward learning stage in RLHF. DPO, after training, provides an implicit reward model. In this work, we make a novel observation that this implicit reward model can by itself be used in a bootstrapping fashion to further align the LLM. Our approach is to use the rewards from a current LLM to construct a preference dataset, which is then used in subsequent DPO rounds. We incorporate two refinements to further improve our approach: 1) length-regularized reward shaping to make the preference dataset length-unbiased; 2) experience replay to enhance the quality of the preference dataset. Our approach, named self-alignment with DPO ImpliCit rEwards (DICE), shows great improvements in alignment. It achieves an increase of more than 8$\\\\%$ in lengthcontrolled win rate on AlpacaEval 2 for all the different base models that we tried, without relying on external feedback. Our code is available at https://github.com/sail-sg/dice.",
      "authors": [
        "Changyu Chen",
        "Zi-Yan Liu",
        "Chao Du",
        "Tianyu Pang",
        "Qian Liu",
        "Arunesh Sinha",
        "Pradeep Varakantham",
        "Min Lin"
      ],
      "year": 2024,
      "citation_count": 36,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/d3dd08e86a6c9a175385a3b4d282c5c754f4f51d",
      "pdf_link": "",
      "venue": "International Conference on Learning Representations",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "33c445469aa9688837b0f76a2e55bcabe29dce47",
      "title": "How Likely Do LLMs with CoT Mimic Human Reasoning?",
      "abstract": "Chain-of-thought emerges as a promising technique for eliciting reasoning capabilities from Large Language Models (LLMs). However, it does not always improve task performance or accurately represent reasoning processes, leaving unresolved questions about its usage. In this paper, we diagnose the underlying mechanism by comparing the reasoning process of LLMs with humans, using causal analysis to understand the relationships between the problem instruction, reasoning, and the answer in LLMs. Our empirical study reveals that LLMs often deviate from the ideal causal chain, resulting in spurious correlations and potential consistency errors (inconsistent reasoning and answers). We also examine various factors influencing the causal structure, finding that in-context learning with examples strengthens it, while post-training techniques like supervised fine-tuning and reinforcement learning on human feedback weaken it. To our surprise, the causal structure cannot be strengthened by enlarging the model size only, urging research on new techniques. We hope that this preliminary study will shed light on understanding and improving the reasoning process in LLM.",
      "authors": [
        "Guangsheng Bao",
        "Hongbo Zhang",
        "Linyi Yang",
        "Cunxiang Wang",
        "Yue Zhang"
      ],
      "year": 2024,
      "citation_count": 28,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/33c445469aa9688837b0f76a2e55bcabe29dce47",
      "pdf_link": "",
      "venue": "International Conference on Computational Linguistics",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "4ed96712afa0d0e82cddb3d669d4e9f60195aecb",
      "title": "CheatAgent: Attacking LLM-Empowered Recommender Systems via LLM Agent",
      "abstract": "Recently, Large Language Model (LLM)-empowered recommender systems (RecSys) have brought significant advances in personalized user experience and have attracted considerable attention. Despite the impressive progress, the research question regarding the safety vulnerability of LLM-empowered RecSys still remains largely under-investigated. Given the security and privacy concerns, it is more practical to focus on attacking the black-box RecSys, where attackers can only observe the system's inputs and outputs. However, traditional attack approaches employing reinforcement learning (RL) agents are not effective for attacking LLM-empowered RecSys due to the limited capabilities in processing complex textual inputs, planning, and reasoning. On the other hand, LLMs provide unprecedented opportunities to serve as attack agents to attack RecSys because of their impressive capability in simulating human-like decision-making processes. Therefore, in this paper, we propose a novel attack framework called CheatAgent by harnessing the human-like capabilities of LLMs, where an LLM-based agent is developed to attack LLM-Empowered RecSys. Specifically, our method first identifies the insertion position for maximum impact with minimal input modification. After that, the LLM agent is designed to generate adversarial perturbations to insert at target positions. To further improve the quality of generated perturbations, we utilize the prompt tuning technique to improve attacking strategies via feedback from the victim RecSys iteratively. Extensive experiments across three real-world datasets demonstrate the effectiveness of our proposed attacking method.",
      "authors": [
        "Liang-bo Ning",
        "Shijie Wang",
        "Wenqi Fan",
        "Qing Li",
        "Xin Xu",
        "Hao Chen",
        "Feiran Huang"
      ],
      "year": 2024,
      "citation_count": 27,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/4ed96712afa0d0e82cddb3d669d4e9f60195aecb",
      "pdf_link": "",
      "venue": "Knowledge Discovery and Data Mining",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "9c9ca3a8320c0babd9fc331cc376ffff32fe1f67",
      "title": "Online Merging Optimizers for Boosting Rewards and Mitigating Tax in Alignment",
      "abstract": "Effectively aligning Large Language Models (LLMs) with human-centric values while preventing the degradation of abilities acquired through Pre-training and Supervised Fine-tuning (SFT) poses a central challenge in Reinforcement Learning from Human Feedback (RLHF). In this paper, we first discover that interpolating RLHF and SFT model parameters can adjust the trade-off between human preference and basic capabilities, thereby reducing the alignment tax at the cost of alignment reward. Inspired by this, we propose integrating the RL policy and SFT models at each optimization step in RLHF to continuously regulate the training direction, introducing the Online Merging Optimizer. Specifically, we merge gradients with the parameter differences between SFT and pretrained models, effectively steering the gradient towards maximizing rewards in the direction of SFT optimization. We demonstrate that our optimizer works well with different LLM families, such as Qwen and LLaMA, across various model sizes ranging from 1.8B to 8B, various RLHF algorithms like DPO and KTO, and existing model merging methods. It significantly enhances alignment reward while mitigating alignment tax, achieving higher overall performance across 14 benchmarks.",
      "authors": [
        "Keming Lu",
        "Bowen Yu",
        "Fei Huang",
        "Yang Fan",
        "Runji Lin",
        "Chang Zhou"
      ],
      "year": 2024,
      "citation_count": 23,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/9c9ca3a8320c0babd9fc331cc376ffff32fe1f67",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "9123ec44f0026e70f8398b904e97a4224866bb36",
      "title": "Bi-Factorial Preference Optimization: Balancing Safety-Helpfulness in Language Models",
      "abstract": "Fine-tuning large language models (LLMs) on human preferences, typically through reinforcement learning from human feedback (RLHF), has proven successful in enhancing their capabilities. However, ensuring the safety of LLMs during fine-tuning remains a critical concern, and mitigating the potential conflicts in safety and helpfulness is costly in RLHF. To address this issue, we propose a supervised learning framework called Bi-Factorial Preference Optimization (BFPO), which re-parameterizes a joint RLHF objective of both safety and helpfulness into a single supervised learning objective. In supervised optimization, a labeling function is used to capture the global preferences ranking to balance both safety and helpfulness. To evaluate BFPO, we develop a benchmark that includes comprehensive discriminative and generative tasks for helpfulness and harmlessness. The results indicate that our method significantly outperforms existing approaches in both safety and helpfulness. Moreover, BFPO achieves the same level of safety as methods that heavily rely on human labor with less than 10\\% of the computational resources and human prompting and annotation process. The training recipes can be found here: https://github.com/wx-zhang/bfpo.",
      "authors": [
        "Wenxuan Zhang",
        "Philip H. S. Torr",
        "Mohamed Elhoseiny",
        "Adel Bibi"
      ],
      "year": 2024,
      "citation_count": 18,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/9123ec44f0026e70f8398b904e97a4224866bb36",
      "pdf_link": "",
      "venue": "International Conference on Learning Representations",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "3d43594804af065c89d4f5be5d0a17957b633092",
      "title": "Overcoming Reward Overoptimization via Adversarial Policy Optimization with Lightweight Uncertainty Estimation",
      "abstract": "We introduce Adversarial Policy Optimization (AdvPO), a novel solution to the pervasive issue of reward over-optimization in Reinforcement Learning from Human Feedback (RLHF) for Large Language Models (LLMs). Over-optimization occurs when a reward model serves as an imperfect proxy for human preference, and RL-driven policy optimization erroneously exploits reward inaccuracies. In this paper, we begin by introducing a lightweight way to quantify uncertainties in rewards, relying solely on the last layer embeddings of the reward model, without the need for computationally expensive reward ensembles. AdvPO then addresses a distributionally robust optimization problem centred around the confidence interval of the reward model's predictions for policy improvement. Through comprehensive experiments on the Anthropic HH and TL;DR summarization datasets, we illustrate the efficacy of AdvPO in mitigating the overoptimization issue, consequently resulting in enhanced performance as evaluated through human-assisted evaluation.",
      "authors": [
        "Xiaoying Zhang",
        "Jean-François Ton",
        "Wei Shen",
        "Hongning Wang",
        "Yang Liu"
      ],
      "year": 2024,
      "citation_count": 18,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/3d43594804af065c89d4f5be5d0a17957b633092",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "ea9809331f53bd9b3013f49cc10ac79965e40b2e",
      "title": "Creativity Has Left the Chat: The Price of Debiasing Language Models",
      "abstract": "Large Language Models (LLMs) have revolutionized natural language processing but can exhibit biases and may generate toxic content. While alignment techniques like Reinforcement Learning from Human Feedback (RLHF) reduce these issues, their impact on creativity, defined as syntactic and semantic diversity, remains unexplored. We investigate the unintended consequences of RLHF on the creativity of LLMs through three experiments focusing on the Llama-2 series. Our findings reveal that aligned models exhibit lower entropy in token predictions, form distinct clusters in the embedding space, and gravitate towards\"attractor states\", indicating limited output diversity. Our findings have significant implications for marketers who rely on LLMs for creative tasks such as copywriting, ad creation, and customer persona generation. The trade-off between consistency and creativity in aligned models should be carefully considered when selecting the appropriate model for a given application. We also discuss the importance of prompt engineering in harnessing the creative potential of base models.",
      "authors": [
        "Behnam Mohammadi"
      ],
      "year": 2024,
      "citation_count": 17,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/ea9809331f53bd9b3013f49cc10ac79965e40b2e",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "5695f983699b36af61851d8025aab9caea970eae",
      "title": "No Free Lunch: Rethinking Internal Feedback for LLM Reasoning",
      "abstract": "Reinforcement learning has emerged as a powerful paradigm for post-training large language models (LLMs) to improve reasoning. Approaches like Reinforcement Learning from Human Feedback (RLHF) and Reinforcement Learning with Verifiable Rewards (RLVR) have shown strong results, but they require extensive external supervision. We investigate an alternative class of methods, Reinforcement Learning from Internal Feedback (RLIF), which relies solely on intrinsic model-derived signals instead of external rewards. In particular, we leverage unsupervised reward proxies such as token-level entropy, trajectory-level entropy, and self-certainty. Our theoretical analysis shows these internal objectives are partially equivalent, and we empirically evaluate various RLIF strategies on challenging math reasoning benchmarks. Experimental results demonstrate that RLIF can boost the reasoning performance of base LLMs at the beginning phase of the training, matching or surpassing RLVR techniques on these tasks. However, when training progresses, performance degrades even below the model before training. Moreover, we find that RLIF yields little improvement for instruction-tuned models, indicating diminishing returns of intrinsic feedback once an LLM is already instruction-tuned. We further analyze this limitation by mixing model weights and explain the reason of RLIF's training behaviors, providing practical guidelines for integrating internal feedback signals into LLM training. We hope our analysis of internal feedback will inform more principled and effective strategies for LLM post-training.",
      "authors": [
        "Yanzhi Zhang",
        "Zhaoxi Zhang",
        "Haoxiang Guan",
        "Yilin Cheng",
        "Yitong Duan",
        "Chen Wang",
        "Yue Wang",
        "Shuxin Zheng",
        "Jiyan He"
      ],
      "year": 2025,
      "citation_count": 8,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/5695f983699b36af61851d8025aab9caea970eae",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "b8f435d3b8202f1086be9d791857c20cb3a4a90a",
      "title": "Pareto-Optimal Learning from Preferences with Hidden Context",
      "abstract": "Ensuring AI models align with human values is essential for their safety and functionality. Reinforcement learning from human feedback (RLHF) leverages human preferences to achieve this alignment. However, when preferences are sourced from diverse populations, point estimates of reward can result in suboptimal performance or be unfair to specific groups. We propose Pareto Optimal Preference Learning (POPL), which enables pluralistic alignment by framing discrepant group preferences as objectives with potential trade-offs, aiming for policies that are Pareto-optimal on the preference dataset. POPL utilizes lexicase selection, an iterative process that selects diverse and Pareto-optimal solutions. Our theoretical and empirical evaluations demonstrate that POPL surpasses baseline methods in learning sets of reward functions and policies, effectively catering to distinct groups without access to group numbers or membership labels. We verify the performance of POPL on a stateless preference learning setting, a Minigrid RL domain, Metaworld robotics benchmarks, as well as large language model (LLM) fine-tuning. We illustrate that POPL can also serve as a foundation for techniques optimizing specific notions of group fairness, ensuring safe and equitable AI model alignment.",
      "authors": [
        "Ryan Boldi",
        "Lijie Ding",
        "Lee Spector",
        "S. Niekum"
      ],
      "year": 2024,
      "citation_count": 6,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/b8f435d3b8202f1086be9d791857c20cb3a4a90a",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "280598d6613a071db232422b914f613a37cf13d1",
      "title": "Simplify RLHF as Reward-Weighted SFT: A Variational Method",
      "abstract": "Reinforcement Learning from Human Feedback (RLHF) is crucial for aligning Large Language Models (LLMs) with human values. However, RLHF has been continuously challenged by its high complexity in implementation and computation consumption. Even with recent simplifications, such as Direct Preference Optimization (DPO) and Advantage Leftover Lunch (A-LoL), the problems of over-fitting and training instability remain hindering the alignment process from the expected optimal performance. To address the existing challenges, we propose a novel simplification of RLHF from the perspective of variational inference, called $\\textbf{V}$ariational $\\textbf{A}$lignment with $\\textbf{R}$e-weighting ($\\textbf{VAR}$). More specifically, by directly minimizing the distribution gap between the learning LLM policy and the optimal solution of RLHF, we transform the alignment objective into a reward-driven re-weighted supervised fine-tuning (SFT) form, which only requires minor adjustment on the SFT loss to obtain noticeable improvement on training stability and effectiveness. On comprehensive alignment and generation benchmarks, our VAR method has numerically achieved competitive performance in LLM alignment helpfulness and harmlessness.",
      "authors": [
        "Yuhao Du",
        "Zhuo Li",
        "Pengyu Cheng",
        "Zhihong Chen",
        "Yuejiao Xie",
        "Xiang Wan",
        "Anningzhe Gao"
      ],
      "year": 2025,
      "citation_count": 6,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/280598d6613a071db232422b914f613a37cf13d1",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "4146b447187e1a09b736564854007c403f986c69",
      "title": "ALaRM: Align Language Models via Hierarchical Rewards Modeling",
      "abstract": "We introduce ALaRM, the first framework modeling hierarchical rewards in reinforcement learning from human feedback (RLHF), which is designed to enhance the alignment of large language models (LLMs) with human preferences. The framework addresses the limitations of current alignment approaches, which often struggle with the inconsistency and sparsity of human supervision signals, by integrating holistic rewards with aspect-specific rewards. This integration enables more precise and consistent guidance of language models towards desired outcomes, particularly in complex and open text generation tasks. By employing a methodology that filters and combines multiple rewards based on their consistency, the framework provides a reliable mechanism for improving model alignment. We validate our approach through applications in long-form question answering and machine translation tasks, employing gpt-3.5-turbo for pairwise comparisons, and demonstrate improvements over existing baselines. Our work underscores the effectiveness of hierarchical rewards modeling in refining LLM training processes for better human preference alignment. We release our code at https://ALaRM-fdu.github.io.",
      "authors": [
        "Yuhang Lai",
        "Siyuan Wang",
        "Shujun Liu",
        "Xuanjing Huang",
        "Zhongyu Wei"
      ],
      "year": 2024,
      "citation_count": 6,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/4146b447187e1a09b736564854007c403f986c69",
      "pdf_link": "",
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "68981715a1e37c955329fc1a278aef59c9be4764",
      "title": "Dynamic Rewarding with Prompt Optimization Enables Tuning-free Self-Alignment of Language Models",
      "abstract": "Aligning Large Language Models (LLMs) traditionally relies on complex and costly training processes like supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF). To address the challenge of achieving alignment without these extensive tuning costs and expensive annotations, we present a novel, tuning-free approach for self-alignment called Dynamic Rewarding with Prompt Optimization (DRPO). Our approach enables self-alignment through a search-based prompt optimization framework, allowing the model to self-improve and generate optimized prompts without additional training or human supervision. The core of DRPO leverages a dynamic rewarding mechanism to identify and rectify model-specific alignment weaknesses, enabling LLMs to adapt quickly to various alignment challenges. Empirical evaluations on eight recent LLMs, including both open- and closed-source, reveal that DRPO significantly enhances alignment performance, enabling base models to outperform their SFT/RLHF-tuned counterparts. Moreover, DRPO’s automatically optimized prompts surpass those curated by human experts, demonstrating its superior alignment capabilities. Our findings envision a highly cost-effective and adaptable solution for future alignment research to be further explored.",
      "authors": [
        "Somanshu Singla",
        "Zhen Wang",
        "Tianyang Liu",
        "Abdullah Ashfaq",
        "Zhiting Hu",
        "Eric P. Xing"
      ],
      "year": 2024,
      "citation_count": 5,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/68981715a1e37c955329fc1a278aef59c9be4764",
      "pdf_link": "",
      "venue": "Conference on Empirical Methods in Natural Language Processing",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "1271cc5f6eaecebecd0489c23c727b30ee7f6089",
      "title": "Aligning LLMs through Multi-perspective User Preference Ranking-based Feedback for Programming Question Answering",
      "abstract": "Code Community Question Answering (CCQA) seeks to tackle programming-related issues, thereby boosting productivity in both software engineering and academic research. Recent advancements in Reinforcement Learning from Human Feedback (RLHF) have transformed the fine-tuning process of Large Language Models (LLMs) to produce responses that closely mimic human behavior. Leveraging LLMs with RLHF for practical CCQA applications has thus emerged as a promising area of study. Unlike standard code question-answering tasks, CCQA involves multiple possible answers, with varying user preferences for each response. Additionally, code communities often show a preference for new APIs. These challenges prevent LLMs from generating responses that cater to the diverse preferences of users in CCQA tasks. To address these issues, we propose a novel framework called Aligning LLMs through Multi-perspective User Preference Ranking-based Feedback for Programming Question Answering (ALMupQA) to create user-focused responses. Our approach starts with Multi-perspective Preference Ranking Alignment (MPRA), which synthesizes varied user preferences based on the characteristics of answers from code communities. We then introduce a Retrieval-augmented In-context Learning (RIL) module to mitigate the problem of outdated answers by retrieving responses to similar questions from a question bank. Due to the limited availability of high-quality, multi-answer CCQA datasets, we also developed a dataset named StaCCQA from real code communities. Extensive experiments demonstrated the effectiveness of the ALMupQA framework in terms of accuracy and user preference. Compared to the base model, ALMupQA showed nearly an 11% improvement in BLEU, with increases of 20% and 17.5% in BERTScore and CodeBERTScore, respectively.",
      "authors": [
        "Hongyu Yang",
        "Liyang He",
        "Min Hou",
        "Shuanghong Shen",
        "Rui Li",
        "Jiahui Hou",
        "Jianhui Ma",
        "Junda Zhao"
      ],
      "year": 2024,
      "citation_count": 5,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/1271cc5f6eaecebecd0489c23c727b30ee7f6089",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "0d49552b54a1c2e064047d332018a898fcf6d9cb",
      "title": "MA-RLHF: Reinforcement Learning from Human Feedback with Macro Actions",
      "abstract": "Reinforcement learning from human feedback (RLHF) has demonstrated effectiveness in aligning large language models (LLMs) with human preferences. However, token-level RLHF suffers from the credit assignment problem over long sequences, where delayed rewards make it challenging for the model to discern which actions contributed to preferred outcomes. This hinders learning efficiency and slows convergence.In this paper, we propose MA-RLHF, a simple yet effective RLHF framework that incorporates macro actions -- sequences of tokens or higher-level language constructs -- into the learning process. By operating at higher level of abstraction, our approach reduces the temporal distance between actions and rewards, facilitating faster and more accurate credit assignment. This results in more stable policy gradient estimates and enhances learning efficiency within each episode, all without increasing computational complexity during training or inference. We validate our approach through extensive experiments across various model sizes and tasks, including text summarization, dialogue generation, question answering, and program synthesis. Our method achieves substantial performance improvements over standard RLHF, with performance gains of up to 30% in text summarization and code generation, 18% in dialogue, and 8% in question answering tasks. Notably, our approach reaches parity with vanilla RLHF 1.7 ~ 2 times faster in terms of training time and continues to outperform it with further training. We make our code and data publicly available at https://github.com/ernie-research/MA-RLHF.",
      "authors": [
        "Yekun Chai",
        "Haoran Sun",
        "Huang Fang",
        "Shuohuan Wang",
        "Yu Sun",
        "Hua Wu"
      ],
      "year": 2024,
      "citation_count": 4,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/0d49552b54a1c2e064047d332018a898fcf6d9cb",
      "pdf_link": "",
      "venue": "International Conference on Learning Representations",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "197c91461c4f0bfc19d775f329607492ac80912f",
      "title": "Reusing Embeddings: Reproducible Reward Model Research in Large Language Model Alignment without GPUs",
      "abstract": "Large Language Models (LLMs) have made substantial strides in structured tasks through Reinforcement Learning (RL), demonstrating proficiency in mathematical reasoning and code generation. However, applying RL in broader domains like chatbots and content generation -- through the process known as Reinforcement Learning from Human Feedback (RLHF) -- presents unique challenges. Reward models in RLHF are critical, acting as proxies that evaluate the alignment of LLM outputs with human intent. Despite advancements, the development of reward models is hindered by challenges such as computational heavy training, costly evaluation, and therefore poor reproducibility. We advocate for using embedding-based input in reward model research as an accelerated solution to those challenges. By leveraging embeddings for reward modeling, we can enhance reproducibility, reduce computational demands on hardware, improve training stability, and significantly reduce training and evaluation costs, hence facilitating fair and efficient comparisons in this active research area. We then show a case study of reproducing existing reward model ensemble research using embedding-based reward models. We discussed future avenues for research, aiming to contribute to safer and more effective LLM deployments.",
      "authors": [
        "Hao Sun",
        "Yunyi Shen",
        "Jean-Franccois Ton",
        "M. Schaar"
      ],
      "year": 2025,
      "citation_count": 4,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/197c91461c4f0bfc19d775f329607492ac80912f",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "503c85a9df91de5dace92d6c5ade8627701f08ac",
      "title": "Large language model empowered participatory urban planning",
      "abstract": "Participatory urban planning is the mainstream of modern urban planning and involves the active engagement of different stakeholders. However, the traditional participatory paradigm encounters challenges in time and manpower, while the generative planning tools fail to provide adjustable and inclusive solutions. This research introduces an innovative urban planning approach integrating Large Language Models (LLMs) within the participatory process. The framework, based on the crafted LLM agent, consists of role-play, collaborative generation, and feedback iteration, solving a community-level land-use task catering to 1000 distinct interests. Empirical experiments in diverse urban communities exhibit LLM's adaptability and effectiveness across varied planning scenarios. The results were evaluated on four metrics, surpassing human experts in satisfaction and inclusion, and rivaling state-of-the-art reinforcement learning methods in service and ecology. Further analysis shows the advantage of LLM agents in providing adjustable and inclusive solutions with natural language reasoning and strong scalability. While implementing the recent advancements in emulating human behavior for planning, this work envisions both planners and citizens benefiting from low-cost, efficient LLM agents, which is crucial for enhancing participation and realizing participatory urban planning.",
      "authors": [
        "Zhilun Zhou",
        "Yuming Lin",
        "Yong Li"
      ],
      "year": 2024,
      "citation_count": 4,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/503c85a9df91de5dace92d6c5ade8627701f08ac",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "365b5f8439ccd558de22c7fbb229a380c8ea423f",
      "title": "Learning Only with Images: Visual Reinforcement Learning with Reasoning, Rendering, and Visual Feedback",
      "abstract": "Multimodal Large Language Models (MLLMs) exhibit impressive performance across various visual tasks. Subsequent investigations into enhancing their visual reasoning abilities have significantly expanded their performance envelope. However, a critical bottleneck in the advancement of MLLMs toward deep visual reasoning is their heavy reliance on curated image-text supervision. To solve this problem, we introduce a novel framework, ``Reasoning-Rendering-Visual-Feedback''(RRVF), that enables MLLMs to learn complex visual reasoning from only raw images. This framework builds on the ``Asymmetry of Verification''principle, i.e., verifying the rendered output against the source image is substantially easier than performing deep visual reasoning to generate a faithful, structured representation such as code. We demonstrate that this relative ease provides an ideal reward signal for optimization via Reinforcement Learning (RL), thereby reducing reliance on image-text supervision. RRVF implements a closed-loop iterative process encompassing reasoning, rendering, and visual feedback components, enabling the model to perform complex reasoning, including self-correction through multi-turn interactions. This process is optimized end-to-end using the GRPO algorithm. Extensive evaluations are conducted on image-to-code generation across two diverse domains: data charts and web interfaces. The RRVF-trained model not only outperforms existing similarly sized open-source MLLMs and supervised fine-tuning baselines but also exhibits superior generalization. Notably, the model outperforms the more advanced MLLM used to generate visual feedback during training. Code is available at https://github.com/L-O-I/RRVF.",
      "authors": [
        "Yang Chen",
        "Yufan Shen",
        "Wenxuan Huang",
        "Sheng Zhou",
        "Qunshu Lin",
        "Xinyu Cai",
        "Zhi Yu",
        "Jiajun Bu",
        "Botian Shi",
        "Yu Qiao"
      ],
      "year": 2025,
      "citation_count": 4,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/365b5f8439ccd558de22c7fbb229a380c8ea423f",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "72789bb011045e4230834b2df0e3922f2104f8fa",
      "title": "Insights from Verification: Training a Verilog Generation LLM with Reinforcement Learning with Testbench Feedback",
      "abstract": "Large language models (LLMs) have shown strong performance in Verilog generation from natural language description. However, ensuring the functional correctness of the generated code remains a significant challenge. This paper introduces a method that integrates verification insights from testbench into the training of Verilog generation LLMs, aligning the training with the fundamental goal of hardware design: functional correctness. The main obstacle in using LLMs for Verilog code generation is the lack of sufficient functional verification data, particularly testbenches paired with design specifications and code. To address this problem, we introduce an automatic testbench generation pipeline that decomposes the process and uses feedback from the Verilog compiler simulator (VCS) to reduce hallucination and ensure correctness. We then use the testbench to evaluate the generated codes and collect them for further training, where verification insights are introduced. Our method applies reinforcement learning (RL), specifically direct preference optimization (DPO), to align Verilog code generation with functional correctness by training preference pairs based on testbench outcomes. In evaluations on VerilogEval-Machine, VerilogEval-Human, RTLLM v1.1, RTLLM v2, and VerilogEval v2, our approach consistently outperforms state-of-the-art baselines in generating functionally correct Verilog code. We open source all training code, data, and models at https://anonymous.4open.science/r/VeriPrefer-E88B.",
      "authors": [
        "Ning Wang",
        "Bingkun Yao",
        "Jie Zhou",
        "Yuchen Hu",
        "Xi Wang",
        "Nan Guan",
        "Zhe Jiang"
      ],
      "year": 2025,
      "citation_count": 4,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/72789bb011045e4230834b2df0e3922f2104f8fa",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "c3bb8d030a47d28c7a965ee5112a453d86e098ad",
      "title": "Reinforcement Learning from Human Feedback without Reward Inference: Model-Free Algorithm and Instance-Dependent Analysis",
      "abstract": "In this paper, we study reinforcement learning from human feedback (RLHF) under an episodic Markov decision process with a general trajectory-wise reward model. We developed a model-free RLHF best policy identification algorithm, called $\\mathsf{BSAD}$, without explicit reward model inference, which is a critical intermediate step in the contemporary RLHF paradigms for training large language models (LLM). The algorithm identifies the optimal policy directly from human preference information in a backward manner, employing a dueling bandit sub-routine that constantly duels actions to identify the superior one. $\\mathsf{BSAD}$ adopts a reward-free exploration and best-arm-identification-like adaptive stopping criteria to equalize the visitation among all states in the same decision step while moving to the previous step as soon as the optimal action is identifiable, leading to a provable, instance-dependent sample complexity $\\tilde{\\mathcal{O}}(c_{\\mathcal{M}}SA^3H^3M\\log\\frac{1}{\\delta})$ which resembles the result in classic RL, where $c_{\\mathcal{M}}$ is the instance-dependent constant and $M$ is the batch size. Moreover, $\\mathsf{BSAD}$ can be transformed into an explore-then-commit algorithm with logarithmic regret and generalized to discounted MDPs using a frame-based approach. Our results show: (i) sample-complexity-wise, RLHF is not significantly harder than classic RL and (ii) end-to-end RLHF may deliver improved performance by avoiding pitfalls in reward inferring such as overfit and distribution shift.",
      "authors": [
        "Qining Zhang",
        "Honghao Wei",
        "Lei Ying"
      ],
      "year": 2024,
      "citation_count": 3,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/c3bb8d030a47d28c7a965ee5112a453d86e098ad",
      "pdf_link": "",
      "venue": "RLJ",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "914d93e9b0d5a035f5cee473bd1e7d3fb6407e8c",
      "title": "Proxy-RLHF: Decoupling Generation and Alignment in Large Language Model with Proxy",
      "abstract": "Reinforcement Learning from Human Feedback (RLHF) is the prevailing approach to ensure Large Language Models (LLMs) align with human values. However, existing RLHF methods require a high computational cost, one main reason being that RLHF assigns both the generation and alignment tasks to the LLM simultaneously. In this paper, we introduce Proxy-RLHF, which decouples the generation and alignment processes of LLMs, achieving alignment with human values at a much lower computational cost. We start with a novel Markov Decision Process (MDP) designed for the alignment process and employ Reinforcement Learning (RL) to train a streamlined proxy model that oversees the token generation of the LLM, without altering the LLM itself. Experiments show that our method achieves a comparable level of alignment with only 1\\% of the training parameters of other methods.",
      "authors": [
        "Yu Zhu",
        "Chuxiong Sun",
        "Wenfei Yang",
        "Wenqiang Wei",
        "Bo Tang",
        "Tianzhu Zhang",
        "Zhiyu Li",
        "Shifeng Zhang",
        "Feiyu Xiong",
        "Jie Hu",
        "Mingchuan Yang"
      ],
      "year": 2024,
      "citation_count": 3,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/914d93e9b0d5a035f5cee473bd1e7d3fb6407e8c",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "b467036844e26c96ee94c466d771f1a5bf617204",
      "title": "A Technical Survey of Reinforcement Learning Techniques for Large Language Models",
      "abstract": "Reinforcement Learning (RL) has emerged as a transformative approach for aligning and enhancing Large Language Models (LLMs), addressing critical challenges in instruction following, ethical alignment, and reasoning capabilities. This survey offers a comprehensive foundation on the integration of RL with language models, highlighting prominent algorithms such as Proximal Policy Optimization (PPO), Q-Learning, and Actor-Critic methods. Additionally, it provides an extensive technical overview of RL techniques specifically tailored for LLMs, including foundational methods like Reinforcement Learning from Human Feedback (RLHF) and AI Feedback (RLAIF), as well as advanced strategies such as Direct Preference Optimization (DPO) and Group Relative Policy Optimization (GRPO). We systematically analyze their applications across domains, i.e., from code generation to tool-augmented reasoning. We also present a comparative taxonomy based on reward modeling, feedback mechanisms, and optimization strategies. Our evaluation highlights key trends. RLHF remains dominant for alignment, and outcome-based RL such as RLVR significantly improves stepwise reasoning. However, persistent challenges such as reward hacking, computational costs, and scalable feedback collection underscore the need for continued innovation. We further discuss emerging directions, including hybrid RL algorithms, verifier-guided training, and multi-objective alignment frameworks. This survey serves as a roadmap for researchers advancing RL-driven LLM development, balancing capability enhancement with safety and scalability.",
      "authors": [
        "S. Srivastava",
        "Vaneet Aggarwal"
      ],
      "year": 2025,
      "citation_count": 3,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/b467036844e26c96ee94c466d771f1a5bf617204",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "5cb5453d2c54e1449f82fdf2e976cab04396b224",
      "title": "Distributionally Robust Reinforcement Learning with Human Feedback",
      "abstract": "Reinforcement learning from human feedback (RLHF) has evolved to be one of the main methods for fine-tuning large language models (LLMs). However, existing RLHF methods are non-robust, and their performance deteriorates if the downstream task differs significantly from the preference dataset used in fine-tuning. In order to mitigate this problem, we introduce a distributionally robust RLHF for fine-tuning LLMs. In particular, our goal is to ensure that a fine-tuned model retains its performance even when the distribution of prompts significantly differs from the distribution encountered during fine-tuning. We formulate distributionally robust optimization (DRO) version of two popular fine-tuning methods -- (1) reward-based RLHF and (2) reward-free DPO (direct preference optimization). We propose a minibatch gradient descent based algorithms for both of them, and theoretically prove convergence guarantees for the algorithms. Subsequently, we evaluate our algorithms on an out-of-distribution (OOD) task by first training the model on the Unified-Feedback dataset and evaluating its performance on two different datasets. The experimental results show that our robust training improves the accuracy of the learned reward models on average, and markedly on some tasks, such as reasoning. Furthermore, we show that the robust versions of policy optimization methods, similarly improve performance on OOD tasks.",
      "authors": [
        "Debmalya Mandal",
        "Paulius Sasnauskas",
        "Goran Radanovic"
      ],
      "year": 2025,
      "citation_count": 3,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/5cb5453d2c54e1449f82fdf2e976cab04396b224",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "4069e2f0eaf53a0e7086bb715e359e345e151abc",
      "title": "Logarithmic Regret for Online KL-Regularized Reinforcement Learning",
      "abstract": "Recent advances in Reinforcement Learning from Human Feedback (RLHF) have shown that KL-regularization plays a pivotal role in improving the efficiency of RL fine-tuning for large language models (LLMs). Despite its empirical advantage, the theoretical difference between KL-regularized RL and standard RL remains largely under-explored. While there is a recent line of work on the theoretical analysis of KL-regularized objective in decision making \\citep{xiong2024iterative, xie2024exploratory,zhao2024sharp}, these analyses either reduce to the traditional RL setting or rely on strong coverage assumptions. In this paper, we propose an optimism-based KL-regularized online contextual bandit algorithm, and provide a novel analysis of its regret. By carefully leveraging the benign optimization landscape induced by the KL-regularization and the optimistic reward estimation, our algorithm achieves an $\\mathcal{O}\\big(\\eta\\log (N_{\\mathcal R} T)\\cdot d_{\\mathcal R}\\big)$ logarithmic regret bound, where $\\eta, N_{\\mathcal R},T,d_{\\mathcal R}$ denote the KL-regularization parameter, the cardinality of the reward function class, number of rounds, and the complexity of the reward function class. Furthermore, we extend our algorithm and analysis to reinforcement learning by developing a novel decomposition over transition steps and also obtain a similar logarithmic regret bound.",
      "authors": [
        "Heyang Zhao",
        "Chen Ye",
        "Wei Xiong",
        "Quanquan Gu",
        "Tong Zhang"
      ],
      "year": 2025,
      "citation_count": 3,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/4069e2f0eaf53a0e7086bb715e359e345e151abc",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "8216a2d3d897f63b46e80211bccb0931ab9fbda9",
      "title": "Impact of Non-Standard Unicode Characters on Security and Comprehension in Large Language Models",
      "abstract": "The advancement of large language models has significantly improved natural language processing. However, challenges such as jailbreaks (prompt injections that cause an LLM to follow instructions contrary to its intended use), hallucinations (generating incorrect or misleading information), and comprehension errors remain prevalent. In this report, we present a comparative analysis of the performance of fifteen distinct models, with each model undergoing a standardized test comprising 38 queries across three key metrics: jailbreaks, hallucinations, and comprehension errors. The models are assessed based on the total occurrences of jailbreaks, hallucinations, and comprehension errors. Our work exposes these models' inherent vulnerabilities and challenges the notion of human-level language comprehension of these models. We have empirically analysed the impact of non-standard Unicode characters on LLMs and their safeguarding mechanisms on the best-performing LLMs, including GPT-4, Gemini 1.5 Pro, LlaMA-3-70B, and Claude 3 Opus. By incorporating alphanumeric symbols from Unicode outside the standard Latin block and variants of characters in other languages, we observed a reduction in the efficacy of guardrails implemented through Reinforcement Learning Human Feedback (RLHF). Consequently, these models exhibit heightened vulnerability to content policy breaches and prompt leakage. Our study also suggests a need to incorporate non-standard Unicode text in LLM training data to enhance the capabilities of these models.",
      "authors": [
        "Johan S Daniel",
        "Anand Pal"
      ],
      "year": 2024,
      "citation_count": 3,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/8216a2d3d897f63b46e80211bccb0931ab9fbda9",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "8171ff1da2605a410b99b82e2dbd0feb68d021ef",
      "title": "RED: Unleashing Token-Level Rewards from Holistic Feedback via Reward Redistribution",
      "abstract": "Reinforcement learning from human feedback (RLHF) offers a promising approach to aligning large language models (LLMs) with human preferences. Typically, a reward model is trained or supplied to act as a proxy for humans in evaluating generated responses during the reinforcement training phase. However, current reward models operate as sequence-to-one models, allocating a single, sparse, and delayed reward to an entire output sequence. This approach may overlook the significant contributions of individual tokens toward the desired outcome. To this end, we propose a more fine-grained, token-level guidance approach for RL training. Specifically, we introduce RED, a novel reward redistribition method that evaluates and assigns specific credit to each token using an off-the-shelf reward model. Utilizing these fine-grained rewards enhances the model's understanding of language nuances, leading to more precise performance improvements. Notably, our method does not require modifying the reward model or introducing additional training steps, thereby incurring minimal computational costs. Experimental results across diverse datasets and tasks demonstrate the superiority of our approach.",
      "authors": [
        "Jiahui Li",
        "Lin Li",
        "Tai-wei Chang",
        "Kun Kuang",
        "Long Chen",
        "Jun Zhou",
        "Cheng Yang"
      ],
      "year": 2024,
      "citation_count": 3,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/8171ff1da2605a410b99b82e2dbd0feb68d021ef",
      "pdf_link": "",
      "venue": "",
      "paper_type": "",
      "keywords": []
    }
  ],
  "edges": [
    {
      "source": "0d1c76d45afa012ded7ab741194baf142117c495",
      "target": "b467036844e26c96ee94c466d771f1a5bf617204",
      "weight": 0.2901854598805691
    },
    {
      "source": "f2d0f3d47ae850f49a58f4977393bd0025af4bec",
      "target": "eb6ef63df104c1b35bbc2400f00285b3414400b2",
      "weight": 0.2337213487326677
    },
    {
      "source": "f2d0f3d47ae850f49a58f4977393bd0025af4bec",
      "target": "5695f983699b36af61851d8025aab9caea970eae",
      "weight": 0.23866229561859226
    },
    {
      "source": "f2d0f3d47ae850f49a58f4977393bd0025af4bec",
      "target": "fde0ffe77186561497ce15e4faca82db11dacd64",
      "weight": 0.22334950937255024
    },
    {
      "source": "f2d0f3d47ae850f49a58f4977393bd0025af4bec",
      "target": "ba7fe3757a5343e73b7961b29fe5d65dbb0ef971",
      "weight": 0.23831500697494054
    },
    {
      "source": "f2d0f3d47ae850f49a58f4977393bd0025af4bec",
      "target": "e7c9478b9dab56b6113a85d1c53723eb5d09e58f",
      "weight": 0.3142383285013912
    },
    {
      "source": "600ff4c4ae9fc506c86673c5ecce4fa90803e987",
      "target": "b467036844e26c96ee94c466d771f1a5bf617204",
      "weight": 0.343826357659451
    },
    {
      "source": "600ff4c4ae9fc506c86673c5ecce4fa90803e987",
      "target": "b6b7a7fe30623f06a627d3ddbe34f40cb96a982f",
      "weight": 0.3115637633758733
    },
    {
      "source": "600ff4c4ae9fc506c86673c5ecce4fa90803e987",
      "target": "d3dd08e86a6c9a175385a3b4d282c5c754f4f51d",
      "weight": 0.06873067898113389
    },
    {
      "source": "600ff4c4ae9fc506c86673c5ecce4fa90803e987",
      "target": "ea9809331f53bd9b3013f49cc10ac79965e40b2e",
      "weight": 0.22969826957186315
    },
    {
      "source": "600ff4c4ae9fc506c86673c5ecce4fa90803e987",
      "target": "521c2905e667ad6d2162ac369cf3f85d70e0f477",
      "weight": 0.2558809473876974
    },
    {
      "source": "600ff4c4ae9fc506c86673c5ecce4fa90803e987",
      "target": "914d93e9b0d5a035f5cee473bd1e7d3fb6407e8c",
      "weight": 0.31290392651537907
    },
    {
      "source": "600ff4c4ae9fc506c86673c5ecce4fa90803e987",
      "target": "9732c864d1d4161fcb106f2961d9a80dd4fffc9a",
      "weight": 0.2398089074877974
    },
    {
      "source": "600ff4c4ae9fc506c86673c5ecce4fa90803e987",
      "target": "faae9de3d314e8731b0505607298fd826e3de1a7",
      "weight": 0.3021364129721736
    },
    {
      "source": "600ff4c4ae9fc506c86673c5ecce4fa90803e987",
      "target": "c085e88a0351e393609a95305afc1db792d1db0f",
      "weight": 0.27385183644997524
    },
    {
      "source": "600ff4c4ae9fc506c86673c5ecce4fa90803e987",
      "target": "777d4ec0148c34b0bfab91e9ac3a902e420b891e",
      "weight": 0.3138404735790954
    },
    {
      "source": "600ff4c4ae9fc506c86673c5ecce4fa90803e987",
      "target": "0425c47e19b5f1fcc680967ebd6c6e7cebc0b768",
      "weight": 0.26462418298719986
    },
    {
      "source": "6ca16c1c2c60ceda87242c8f8e522d12cc4a13bc",
      "target": "54c4f894d41095b18294932c0ee8c39ffe3c0ac1",
      "weight": 0.3498856150431463
    },
    {
      "source": "6ca16c1c2c60ceda87242c8f8e522d12cc4a13bc",
      "target": "550006bea81e4ccb67743dd1b82a70b86b48d93a",
      "weight": 0.3133539231263467
    },
    {
      "source": "6ca16c1c2c60ceda87242c8f8e522d12cc4a13bc",
      "target": "fb09b581589e1195ff018179c6a11668587c6d64",
      "weight": 0.2568545633254988
    },
    {
      "source": "e01515c6138bc525f7aec30fc85f2adf028d4156",
      "target": "68981715a1e37c955329fc1a278aef59c9be4764",
      "weight": 0.3764285144102367
    },
    {
      "source": "e01515c6138bc525f7aec30fc85f2adf028d4156",
      "target": "d3dd08e86a6c9a175385a3b4d282c5c754f4f51d",
      "weight": 0.09038310005150593
    },
    {
      "source": "e01515c6138bc525f7aec30fc85f2adf028d4156",
      "target": "30ef6a82ac5aa80f2eea02aebeee2b98ca8ba290",
      "weight": 0.32674534714480785
    },
    {
      "source": "e01515c6138bc525f7aec30fc85f2adf028d4156",
      "target": "9c9ca3a8320c0babd9fc331cc376ffff32fe1f67",
      "weight": 0.277536458495776
    },
    {
      "source": "e01515c6138bc525f7aec30fc85f2adf028d4156",
      "target": "ba015c5d3f5b44e36363b90070bb3301d21ae57e",
      "weight": 0.2764170246313353
    },
    {
      "source": "e01515c6138bc525f7aec30fc85f2adf028d4156",
      "target": "c243df958269bf501f874ef213ba6cc904f24ea9",
      "weight": 0.27687644505958453
    },
    {
      "source": "182c7b40ff7560a5545764814338f55a2098e441",
      "target": "b6b7a7fe30623f06a627d3ddbe34f40cb96a982f",
      "weight": 0.2656294181371559
    },
    {
      "source": "182c7b40ff7560a5545764814338f55a2098e441",
      "target": "58f614941629541c8c04acdb8acb9e3fb350ac5a",
      "weight": 0.23747562001114683
    },
    {
      "source": "182c7b40ff7560a5545764814338f55a2098e441",
      "target": "ee3c57d53327c5f84a8f3988f592c6e2479c1924",
      "weight": 0.2924592735307334
    },
    {
      "source": "182c7b40ff7560a5545764814338f55a2098e441",
      "target": "adc8c71591ee5b043447a7d7db8ae09a8a9f1251",
      "weight": 0.26018487307391036
    },
    {
      "source": "182c7b40ff7560a5545764814338f55a2098e441",
      "target": "30ef6a82ac5aa80f2eea02aebeee2b98ca8ba290",
      "weight": 0.2703850503145095
    },
    {
      "source": "182c7b40ff7560a5545764814338f55a2098e441",
      "target": "7d4ea31ec7a6a693dc15f69ac464b1d01d3e5745",
      "weight": 0.3295646812614202
    },
    {
      "source": "182c7b40ff7560a5545764814338f55a2098e441",
      "target": "16d68add6cbad736e6a75b00d2bf8bd49e4dbd40",
      "weight": 0.24739785993019614
    },
    {
      "source": "182c7b40ff7560a5545764814338f55a2098e441",
      "target": "521c2905e667ad6d2162ac369cf3f85d70e0f477",
      "weight": 0.26212784733690303
    },
    {
      "source": "182c7b40ff7560a5545764814338f55a2098e441",
      "target": "3d43594804af065c89d4f5be5d0a17957b633092",
      "weight": 0.25115885139988103
    },
    {
      "source": "182c7b40ff7560a5545764814338f55a2098e441",
      "target": "c78350e81298ca87bc1d59b466fa40081232caaa",
      "weight": 0.32278034389592297
    },
    {
      "source": "182c7b40ff7560a5545764814338f55a2098e441",
      "target": "cdd0e94e51a02bac22ca5e94fa95daa18f36e226",
      "weight": 0.26293329456746894
    },
    {
      "source": "182c7b40ff7560a5545764814338f55a2098e441",
      "target": "44a9d8b0314d34aff91ccff9207d38eed37216ed",
      "weight": 0.3265262469734471
    },
    {
      "source": "182c7b40ff7560a5545764814338f55a2098e441",
      "target": "85a1f32e4794b4c176f3330364bc39977a50d258",
      "weight": 0.4287156737628332
    },
    {
      "source": "44a9d8b0314d34aff91ccff9207d38eed37216ed",
      "target": "4069e2f0eaf53a0e7086bb715e359e345e151abc",
      "weight": 0.15750039248650374
    },
    {
      "source": "44a9d8b0314d34aff91ccff9207d38eed37216ed",
      "target": "1df7dc4002ec7f8a8dfbf8329f32485f2e96ed6b",
      "weight": 0.2569917834469916
    },
    {
      "source": "44a9d8b0314d34aff91ccff9207d38eed37216ed",
      "target": "2d906cda427cb2c4a71069423312e57ba4cd5445",
      "weight": 0.28321505524291274
    },
    {
      "source": "44a9d8b0314d34aff91ccff9207d38eed37216ed",
      "target": "85c961d5b3fea95b48f94c0461782e887a8b3b0f",
      "weight": 0.3467073295516548
    },
    {
      "source": "44a9d8b0314d34aff91ccff9207d38eed37216ed",
      "target": "adc8c71591ee5b043447a7d7db8ae09a8a9f1251",
      "weight": 0.24267197648556435
    },
    {
      "source": "44a9d8b0314d34aff91ccff9207d38eed37216ed",
      "target": "d3dd08e86a6c9a175385a3b4d282c5c754f4f51d",
      "weight": 0.10075831694199572
    },
    {
      "source": "44a9d8b0314d34aff91ccff9207d38eed37216ed",
      "target": "eff0410f7d5d78ea6874596a0a77b184d03ecca5",
      "weight": 0.25605475301636027
    },
    {
      "source": "44a9d8b0314d34aff91ccff9207d38eed37216ed",
      "target": "16d68add6cbad736e6a75b00d2bf8bd49e4dbd40",
      "weight": 0.3005776580588213
    },
    {
      "source": "44a9d8b0314d34aff91ccff9207d38eed37216ed",
      "target": "3d43594804af065c89d4f5be5d0a17957b633092",
      "weight": 0.25646306037322164
    },
    {
      "source": "c78350e81298ca87bc1d59b466fa40081232caaa",
      "target": "57e959b74f36a30cd62d0abd4204f08907b42e87",
      "weight": 0.2866746984585682
    },
    {
      "source": "c78350e81298ca87bc1d59b466fa40081232caaa",
      "target": "adc8c71591ee5b043447a7d7db8ae09a8a9f1251",
      "weight": 0.2598845921591791
    },
    {
      "source": "6f9dbae279fa0c3a90d12f3b0f271dc8e6274817",
      "target": "f2d0f3d47ae850f49a58f4977393bd0025af4bec",
      "weight": 0.24460700203399083
    },
    {
      "source": "6f9dbae279fa0c3a90d12f3b0f271dc8e6274817",
      "target": "85c961d5b3fea95b48f94c0461782e887a8b3b0f",
      "weight": 0.2990033224165954
    },
    {
      "source": "6f9dbae279fa0c3a90d12f3b0f271dc8e6274817",
      "target": "c3bb8d030a47d28c7a965ee5112a453d86e098ad",
      "weight": 0.25251171235365943
    },
    {
      "source": "cb3968152f7d93f53d24b00279a90d5071ddc85a",
      "target": "ff5b0cc250d93b97fe60e1b0c2048708d6875595",
      "weight": 0.051669705399260246
    },
    {
      "source": "cb3968152f7d93f53d24b00279a90d5071ddc85a",
      "target": "25709a50df5eb8e40dce7ffe6ecd3bfa79969c7a",
      "weight": 0.3328786210006711
    },
    {
      "source": "cb3968152f7d93f53d24b00279a90d5071ddc85a",
      "target": "57451ce18f3035fcadf64db38420434f9299b7f3",
      "weight": 0.2892101932473888
    },
    {
      "source": "cb3968152f7d93f53d24b00279a90d5071ddc85a",
      "target": "c78350e81298ca87bc1d59b466fa40081232caaa",
      "weight": 0.3107798268550384
    },
    {
      "source": "cb3968152f7d93f53d24b00279a90d5071ddc85a",
      "target": "9cb7f7415fb0590186a3d903a8d5d7044b7a3fdc",
      "weight": 0.2539970785984152
    },
    {
      "source": "548278897d46a54958909bb23bcaecf63e24fadf",
      "target": "b467036844e26c96ee94c466d771f1a5bf617204",
      "weight": 0.3888599456815789
    },
    {
      "source": "548278897d46a54958909bb23bcaecf63e24fadf",
      "target": "dd951242ebc94bf633eecc4994c64f46146a1413",
      "weight": 0.28404148073382346
    },
    {
      "source": "548278897d46a54958909bb23bcaecf63e24fadf",
      "target": "aece81d7dcbf2929e650a6094af63666e95a0c83",
      "weight": 0.2878882554582091
    },
    {
      "source": "548278897d46a54958909bb23bcaecf63e24fadf",
      "target": "519d5ccbd5aec517ba987209e17afd4741ac9b8a",
      "weight": 0.32997121215796527
    },
    {
      "source": "548278897d46a54958909bb23bcaecf63e24fadf",
      "target": "9123ec44f0026e70f8398b904e97a4224866bb36",
      "weight": 0.25617054960969526
    },
    {
      "source": "548278897d46a54958909bb23bcaecf63e24fadf",
      "target": "a09521156c5d2efd40e5ca8f0d1e0116f0d41ce4",
      "weight": 0.240433537427014
    },
    {
      "source": "548278897d46a54958909bb23bcaecf63e24fadf",
      "target": "9c9ca3a8320c0babd9fc331cc376ffff32fe1f67",
      "weight": 0.3257687816853525
    },
    {
      "source": "548278897d46a54958909bb23bcaecf63e24fadf",
      "target": "8115ffbbadd1055424d18369dba66ce32a572800",
      "weight": 0.29645932582015666
    },
    {
      "source": "548278897d46a54958909bb23bcaecf63e24fadf",
      "target": "9732c864d1d4161fcb106f2961d9a80dd4fffc9a",
      "weight": 0.2547758970360502
    },
    {
      "source": "548278897d46a54958909bb23bcaecf63e24fadf",
      "target": "59a2203ef6ea159bb41540bd282e29e80a8ad579",
      "weight": 0.2779346596797516
    },
    {
      "source": "548278897d46a54958909bb23bcaecf63e24fadf",
      "target": "0425c47e19b5f1fcc680967ebd6c6e7cebc0b768",
      "weight": 0.29622428596439687
    },
    {
      "source": "59a2203ef6ea159bb41540bd282e29e80a8ad579",
      "target": "c9e4efa58fd42a07da27ae70254981715cc257d5",
      "weight": 0.2660292240997822
    },
    {
      "source": "59a2203ef6ea159bb41540bd282e29e80a8ad579",
      "target": "dd951242ebc94bf633eecc4994c64f46146a1413",
      "weight": 0.282869484811744
    },
    {
      "source": "59a2203ef6ea159bb41540bd282e29e80a8ad579",
      "target": "0940c04de5a9f5dbea57aa0c7953e3fe4a052422",
      "weight": 0.23372290813902744
    },
    {
      "source": "59a2203ef6ea159bb41540bd282e29e80a8ad579",
      "target": "b6b7a7fe30623f06a627d3ddbe34f40cb96a982f",
      "weight": 0.27041664489892236
    },
    {
      "source": "59a2203ef6ea159bb41540bd282e29e80a8ad579",
      "target": "519d5ccbd5aec517ba987209e17afd4741ac9b8a",
      "weight": 0.2744679290620988
    },
    {
      "source": "59a2203ef6ea159bb41540bd282e29e80a8ad579",
      "target": "adc8c71591ee5b043447a7d7db8ae09a8a9f1251",
      "weight": 0.2604107879715319
    },
    {
      "source": "59a2203ef6ea159bb41540bd282e29e80a8ad579",
      "target": "0c43750030198dbe7fe164e1ce743ec64427bca1",
      "weight": 0.2916701386669035
    },
    {
      "source": "59a2203ef6ea159bb41540bd282e29e80a8ad579",
      "target": "521c2905e667ad6d2162ac369cf3f85d70e0f477",
      "weight": 0.2795157217511953
    },
    {
      "source": "59a2203ef6ea159bb41540bd282e29e80a8ad579",
      "target": "8115ffbbadd1055424d18369dba66ce32a572800",
      "weight": 0.2684305987626515
    },
    {
      "source": "59a2203ef6ea159bb41540bd282e29e80a8ad579",
      "target": "3d43594804af065c89d4f5be5d0a17957b633092",
      "weight": 0.2911770110949797
    },
    {
      "source": "59a2203ef6ea159bb41540bd282e29e80a8ad579",
      "target": "9cb7f7415fb0590186a3d903a8d5d7044b7a3fdc",
      "weight": 0.32451961343433466
    },
    {
      "source": "16d68add6cbad736e6a75b00d2bf8bd49e4dbd40",
      "target": "4069e2f0eaf53a0e7086bb715e359e345e151abc",
      "weight": 0.03286858847407321
    },
    {
      "source": "16d68add6cbad736e6a75b00d2bf8bd49e4dbd40",
      "target": "1df7dc4002ec7f8a8dfbf8329f32485f2e96ed6b",
      "weight": 0.22854597593647458
    },
    {
      "source": "16d68add6cbad736e6a75b00d2bf8bd49e4dbd40",
      "target": "8171ff1da2605a410b99b82e2dbd0feb68d021ef",
      "weight": 0.31290067896844675
    },
    {
      "source": "16d68add6cbad736e6a75b00d2bf8bd49e4dbd40",
      "target": "adc8c71591ee5b043447a7d7db8ae09a8a9f1251",
      "weight": 0.23875544535218657
    },
    {
      "source": "16d68add6cbad736e6a75b00d2bf8bd49e4dbd40",
      "target": "d3dd08e86a6c9a175385a3b4d282c5c754f4f51d",
      "weight": 0.12463789536868798
    },
    {
      "source": "16d68add6cbad736e6a75b00d2bf8bd49e4dbd40",
      "target": "a43c2ba35e16d2828ab9b27a92edc68b6af8846d",
      "weight": 0.2804737969006018
    },
    {
      "source": "16d68add6cbad736e6a75b00d2bf8bd49e4dbd40",
      "target": "30ef6a82ac5aa80f2eea02aebeee2b98ca8ba290",
      "weight": 0.27803762138925436
    },
    {
      "source": "0c43750030198dbe7fe164e1ce743ec64427bca1",
      "target": "0940c04de5a9f5dbea57aa0c7953e3fe4a052422",
      "weight": 0.3525764666981378
    },
    {
      "source": "0c43750030198dbe7fe164e1ce743ec64427bca1",
      "target": "519d5ccbd5aec517ba987209e17afd4741ac9b8a",
      "weight": 0.3687630227183527
    },
    {
      "source": "0c43750030198dbe7fe164e1ce743ec64427bca1",
      "target": "85c961d5b3fea95b48f94c0461782e887a8b3b0f",
      "weight": 0.3692486534345516
    },
    {
      "source": "0c43750030198dbe7fe164e1ce743ec64427bca1",
      "target": "78a2943fd2424a5515d595d6bdc54b9a4dbb4389",
      "weight": 0.27150099391647753
    },
    {
      "source": "ec97a1565dff9d2fab1ef489e47296bbef68b680",
      "target": "613a32f18388958cc60dbb906d87fc7f206c0e66",
      "weight": 0.5524796141934926
    },
    {
      "source": "db32da8f3b075d566a73512f4ccc2c95449c75a1",
      "target": "9123ec44f0026e70f8398b904e97a4224866bb36",
      "weight": 0.27574070483433827
    },
    {
      "source": "db32da8f3b075d566a73512f4ccc2c95449c75a1",
      "target": "b8f435d3b8202f1086be9d791857c20cb3a4a90a",
      "weight": 0.34294746067787646
    },
    {
      "source": "ea75117f34b168a20f2a4309ac2eb685ca6b1436",
      "target": "57e959b74f36a30cd62d0abd4204f08907b42e87",
      "weight": 0.3500905559340618
    },
    {
      "source": "fb09b581589e1195ff018179c6a11668587c6d64",
      "target": "550006bea81e4ccb67743dd1b82a70b86b48d93a",
      "weight": 0.4893663187764433
    },
    {
      "source": "9732c864d1d4161fcb106f2961d9a80dd4fffc9a",
      "target": "8171ff1da2605a410b99b82e2dbd0feb68d021ef",
      "weight": 0.3841255992889333
    },
    {
      "source": "9732c864d1d4161fcb106f2961d9a80dd4fffc9a",
      "target": "16d68add6cbad736e6a75b00d2bf8bd49e4dbd40",
      "weight": 0.27351189744201304
    },
    {
      "source": "a43c2ba35e16d2828ab9b27a92edc68b6af8846d",
      "target": "4069e2f0eaf53a0e7086bb715e359e345e151abc",
      "weight": 0.0934493115955786
    },
    {
      "source": "a43c2ba35e16d2828ab9b27a92edc68b6af8846d",
      "target": "1df7dc4002ec7f8a8dfbf8329f32485f2e96ed6b",
      "weight": 0.264548410123689
    },
    {
      "source": "a43c2ba35e16d2828ab9b27a92edc68b6af8846d",
      "target": "e6fac5811e260466366f3a905076c33e252405ef",
      "weight": 0.08877196985253062
    },
    {
      "source": "a43c2ba35e16d2828ab9b27a92edc68b6af8846d",
      "target": "30ef6a82ac5aa80f2eea02aebeee2b98ca8ba290",
      "weight": 0.3214959935674252
    },
    {
      "source": "2d906cda427cb2c4a71069423312e57ba4cd5445",
      "target": "b467036844e26c96ee94c466d771f1a5bf617204",
      "weight": 0.4469164642935396
    },
    {
      "source": "386cebdba39d2d5f2862a9ab43a8d807f3863dae",
      "target": "77f0687571a213c784f0901a821f22b2a03f3ddd",
      "weight": 0.3269768714227099
    },
    {
      "source": "af7669dc48c70d8cf6fccdf1322d6056a6b39dc8",
      "target": "c9e4efa58fd42a07da27ae70254981715cc257d5",
      "weight": 0.2736441038860292
    },
    {
      "source": "af7669dc48c70d8cf6fccdf1322d6056a6b39dc8",
      "target": "dd951242ebc94bf633eecc4994c64f46146a1413",
      "weight": 0.28367347794190667
    },
    {
      "source": "af7669dc48c70d8cf6fccdf1322d6056a6b39dc8",
      "target": "0940c04de5a9f5dbea57aa0c7953e3fe4a052422",
      "weight": 0.23683620770711
    },
    {
      "source": "af7669dc48c70d8cf6fccdf1322d6056a6b39dc8",
      "target": "519d5ccbd5aec517ba987209e17afd4741ac9b8a",
      "weight": 0.277241388148401
    },
    {
      "source": "af7669dc48c70d8cf6fccdf1322d6056a6b39dc8",
      "target": "0c43750030198dbe7fe164e1ce743ec64427bca1",
      "weight": 0.30419824449738303
    },
    {
      "source": "af7669dc48c70d8cf6fccdf1322d6056a6b39dc8",
      "target": "4146b447187e1a09b736564854007c403f986c69",
      "weight": 0.24965793792287916
    },
    {
      "source": "af7669dc48c70d8cf6fccdf1322d6056a6b39dc8",
      "target": "cdd0e94e51a02bac22ca5e94fa95daa18f36e226",
      "weight": 0.3109657328187829
    },
    {
      "source": "dd951242ebc94bf633eecc4994c64f46146a1413",
      "target": "b467036844e26c96ee94c466d771f1a5bf617204",
      "weight": 0.3137173266291557
    },
    {
      "source": "78a2943fd2424a5515d595d6bdc54b9a4dbb4389",
      "target": "a43c2ba35e16d2828ab9b27a92edc68b6af8846d",
      "weight": 0.3408911131805349
    },
    {
      "source": "8115ffbbadd1055424d18369dba66ce32a572800",
      "target": "2d906cda427cb2c4a71069423312e57ba4cd5445",
      "weight": 0.26556779772440453
    },
    {
      "source": "c085e88a0351e393609a95305afc1db792d1db0f",
      "target": "25709a50df5eb8e40dce7ffe6ecd3bfa79969c7a",
      "weight": 0.3009451908717913
    },
    {
      "source": "c085e88a0351e393609a95305afc1db792d1db0f",
      "target": "9cb7f7415fb0590186a3d903a8d5d7044b7a3fdc",
      "weight": 0.3647314408397937
    },
    {
      "source": "777d4ec0148c34b0bfab91e9ac3a902e420b891e",
      "target": "adc8c71591ee5b043447a7d7db8ae09a8a9f1251",
      "weight": 0.28690330542553055
    },
    {
      "source": "519d5ccbd5aec517ba987209e17afd4741ac9b8a",
      "target": "b6b7a7fe30623f06a627d3ddbe34f40cb96a982f",
      "weight": 0.28070609319421835
    },
    {
      "source": "9cb7f7415fb0590186a3d903a8d5d7044b7a3fdc",
      "target": "ff5b0cc250d93b97fe60e1b0c2048708d6875595",
      "weight": 0.116871284860582
    },
    {
      "source": "9cb7f7415fb0590186a3d903a8d5d7044b7a3fdc",
      "target": "ea9809331f53bd9b3013f49cc10ac79965e40b2e",
      "weight": 0.27135696077923294
    },
    {
      "source": "9cb7f7415fb0590186a3d903a8d5d7044b7a3fdc",
      "target": "0c43750030198dbe7fe164e1ce743ec64427bca1",
      "weight": 0.3304952102064588
    },
    {
      "source": "cdd0e94e51a02bac22ca5e94fa95daa18f36e226",
      "target": "0940c04de5a9f5dbea57aa0c7953e3fe4a052422",
      "weight": 0.24975241876471854
    },
    {
      "source": "cdd0e94e51a02bac22ca5e94fa95daa18f36e226",
      "target": "0c43750030198dbe7fe164e1ce743ec64427bca1",
      "weight": 0.3016473885049765
    },
    {
      "source": "cdd0e94e51a02bac22ca5e94fa95daa18f36e226",
      "target": "3d43594804af065c89d4f5be5d0a17957b633092",
      "weight": 0.3477085325506248
    },
    {
      "source": "2044ab82dcb2c11ef660bd51d40130fe182f98d3",
      "target": "85c961d5b3fea95b48f94c0461782e887a8b3b0f",
      "weight": 0.31816059326325313
    },
    {
      "source": "9ddfb1583ce7f5370ace2751bb5f260fa4af1961",
      "target": "57451ce18f3035fcadf64db38420434f9299b7f3",
      "weight": 0.2602805930768545
    },
    {
      "source": "983c01d00102075dae128b8ef9f01abef98720b5",
      "target": "25709a50df5eb8e40dce7ffe6ecd3bfa79969c7a",
      "weight": 0.387813222676788
    },
    {
      "source": "f782d5f569eaa4eb2d66e0cb95bdba0941dbb8b0",
      "target": "2d906cda427cb2c4a71069423312e57ba4cd5445",
      "weight": 0.34586848341463244
    },
    {
      "source": "f782d5f569eaa4eb2d66e0cb95bdba0941dbb8b0",
      "target": "57451ce18f3035fcadf64db38420434f9299b7f3",
      "weight": 0.2686360462756868
    },
    {
      "source": "40cc085a2608985b753c38dc245ac21be592ed08",
      "target": "2530e6ecbd0198012bb8ee4359acb9241cefec95",
      "weight": 0.2575758409307809
    },
    {
      "source": "ee3c57d53327c5f84a8f3988f592c6e2479c1924",
      "target": "8171ff1da2605a410b99b82e2dbd0feb68d021ef",
      "weight": 0.3299341253626684
    },
    {
      "source": "cd6b87d6f746bd572a79c4f77cc60cf6a92fc015",
      "target": "5cb5453d2c54e1449f82fdf2e976cab04396b224",
      "weight": 0.3520984990981491
    },
    {
      "source": "adc8c71591ee5b043447a7d7db8ae09a8a9f1251",
      "target": "c9e4efa58fd42a07da27ae70254981715cc257d5",
      "weight": 0.2835004456674205
    },
    {
      "source": "adc8c71591ee5b043447a7d7db8ae09a8a9f1251",
      "target": "2d906cda427cb2c4a71069423312e57ba4cd5445",
      "weight": 0.26468129286891656
    },
    {
      "source": "adc8c71591ee5b043447a7d7db8ae09a8a9f1251",
      "target": "b6b7a7fe30623f06a627d3ddbe34f40cb96a982f",
      "weight": 0.32827833664422046
    },
    {
      "source": "adc8c71591ee5b043447a7d7db8ae09a8a9f1251",
      "target": "f2fde6be4b074f509cf974d1aac24019247473ae",
      "weight": 0.32980533947546475
    },
    {
      "source": "adc8c71591ee5b043447a7d7db8ae09a8a9f1251",
      "target": "9123ec44f0026e70f8398b904e97a4224866bb36",
      "weight": 0.28199143255854653
    },
    {
      "source": "adc8c71591ee5b043447a7d7db8ae09a8a9f1251",
      "target": "ee3c57d53327c5f84a8f3988f592c6e2479c1924",
      "weight": 0.27969932789702984
    },
    {
      "source": "adc8c71591ee5b043447a7d7db8ae09a8a9f1251",
      "target": "78a2943fd2424a5515d595d6bdc54b9a4dbb4389",
      "weight": 0.2578601239610395
    },
    {
      "source": "7d4ea31ec7a6a693dc15f69ac464b1d01d3e5745",
      "target": "e6fac5811e260466366f3a905076c33e252405ef",
      "weight": 0.10425218574551949
    },
    {
      "source": "7d4ea31ec7a6a693dc15f69ac464b1d01d3e5745",
      "target": "0d49552b54a1c2e064047d332018a898fcf6d9cb",
      "weight": 0.23536946115327964
    },
    {
      "source": "7d4ea31ec7a6a693dc15f69ac464b1d01d3e5745",
      "target": "78a2943fd2424a5515d595d6bdc54b9a4dbb4389",
      "weight": 0.29125986124726594
    },
    {
      "source": "7d4ea31ec7a6a693dc15f69ac464b1d01d3e5745",
      "target": "d3dd08e86a6c9a175385a3b4d282c5c754f4f51d",
      "weight": 0.054569557305791126
    },
    {
      "source": "7d4ea31ec7a6a693dc15f69ac464b1d01d3e5745",
      "target": "0c43750030198dbe7fe164e1ce743ec64427bca1",
      "weight": 0.2861753029000453
    },
    {
      "source": "7d4ea31ec7a6a693dc15f69ac464b1d01d3e5745",
      "target": "30ef6a82ac5aa80f2eea02aebeee2b98ca8ba290",
      "weight": 0.2991252497892477
    },
    {
      "source": "7d4ea31ec7a6a693dc15f69ac464b1d01d3e5745",
      "target": "eff0410f7d5d78ea6874596a0a77b184d03ecca5",
      "weight": 0.24039024008386908
    },
    {
      "source": "d084517f14ee247883de0f4dd58bb923e418157d",
      "target": "57e959b74f36a30cd62d0abd4204f08907b42e87",
      "weight": 0.29730538430266074
    },
    {
      "source": "f21d0177e9374bb8579c1d9c71319f212f62b3d5",
      "target": "68981715a1e37c955329fc1a278aef59c9be4764",
      "weight": 0.3474377515398719
    },
    {
      "source": "830c277b2992f59ec2f21982e245bd1e17dd85ca",
      "target": "d084517f14ee247883de0f4dd58bb923e418157d",
      "weight": 0.4444479371590291
    },
    {
      "source": "30ef6a82ac5aa80f2eea02aebeee2b98ca8ba290",
      "target": "1df7dc4002ec7f8a8dfbf8329f32485f2e96ed6b",
      "weight": 0.25671474718582526
    },
    {
      "source": "30ef6a82ac5aa80f2eea02aebeee2b98ca8ba290",
      "target": "adc8c71591ee5b043447a7d7db8ae09a8a9f1251",
      "weight": 0.3147168465211669
    },
    {
      "source": "30ef6a82ac5aa80f2eea02aebeee2b98ca8ba290",
      "target": "a43c2ba35e16d2828ab9b27a92edc68b6af8846d",
      "weight": 0.3214959935674252
    },
    {
      "source": "c08fa8d84104ec1a8304f75b72bed411100aaf5c",
      "target": "365b5f8439ccd558de22c7fbb229a380c8ea423f",
      "weight": 0.39297601792831705
    },
    {
      "source": "9c9ca3a8320c0babd9fc331cc376ffff32fe1f67",
      "target": "2d906cda427cb2c4a71069423312e57ba4cd5445",
      "weight": 0.29167143161156633
    },
    {
      "source": "9123ec44f0026e70f8398b904e97a4224866bb36",
      "target": "aece81d7dcbf2929e650a6094af63666e95a0c83",
      "weight": 0.600929555432888
    },
    {
      "source": "3d43594804af065c89d4f5be5d0a17957b633092",
      "target": "c9e4efa58fd42a07da27ae70254981715cc257d5",
      "weight": 0.36542954450791615
    },
    {
      "source": "3d43594804af065c89d4f5be5d0a17957b633092",
      "target": "0940c04de5a9f5dbea57aa0c7953e3fe4a052422",
      "weight": 0.2887943835654736
    },
    {
      "source": "c3bb8d030a47d28c7a965ee5112a453d86e098ad",
      "target": "85c961d5b3fea95b48f94c0461782e887a8b3b0f",
      "weight": 0.35951880361738736
    }
  ]
}